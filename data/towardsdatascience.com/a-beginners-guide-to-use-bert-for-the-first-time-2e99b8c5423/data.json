{"url": "https://towardsdatascience.com/a-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423", "time": 1683016768.046171, "path": "towardsdatascience.com/a-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423/", "webpage": {"metadata": {"title": "A Beginner\u2019s Guide to Using BERT for the First Time | by Arfinda Ilmania | Towards Data Science", "h1": "A Beginner\u2019s Guide to Using BERT for the First Time", "description": "BERT has become a new standard for Natural Language Processing (NLP). It achieved a whole new state-of-the-art on eleven NLP task, including text classification, sequence labeling, question\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 0}, {"url": "https://huggingface.co/transformers/index.html", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://huggingface.co/transformers/installation.html", "anchor_text": "website", "paragraph_index": 3}, {"url": "https://huggingface.co/models", "anchor_text": "model hub", "paragraph_index": 8}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer", "anchor_text": "Trainer()", "paragraph_index": 12}, {"url": "https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained", "anchor_text": "from_pretrained(", "paragraph_index": 13}, {"url": "https://nijianmo.github.io/amazon/index.html", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 22}, {"url": "https://medium.com/@arfinda/membership", "anchor_text": "join Medium", "paragraph_index": 23}, {"url": "https://medium.com/subscribe/@arfinda", "anchor_text": "subscribe to my newsletter", "paragraph_index": 23}], "all_paragraphs": ["BERT has become a new standard for Natural Language Processing (NLP). It achieved a whole new state-of-the-art on eleven NLP task, including text classification, sequence labeling, question answering, and many more. Even better, it can also give incredible results using only a small amount of data. BERT was first released in 2018 by Google along with its paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.", "Now we can easily apply BERT to our model by using Huggingface (\ud83e\udd17) Transformers library. The library already provided complete documentation about other transformers models too. You can check it here. In this post, I will try to summarize some important points which we will likely use frequently. We will take a look at how to use and train models using BERT from \ud83e\udd17 Transformers. Later, you can also utilize other transformers models (such as XLM, RoBERTa, XLM RoBERTa (my favorite!), BART, and many others) by simply changing a single line of code.", "Text classification seems to be a pretty good start to get to know BERT. There are many kinds of text classification tasks, but we will choose sentiment analysis in this case. Here are 5 main points which we will be covered in this post:", "As stated on their website, to run \ud83e\udd17 Transformers you will need to have some requirement as follow:", "They also encourage us to use virtual environments to install them, so don\u2019t forget to activate it first.", "The installation is quite easy, when Tensorflow or Pytorch had been installed, you just need to type:", "In this post, we are going to use Pytorch. But it should be easy if you want to translate it into Tensorflow, just add \u2018TF\u2019 at the beginning of each model class name.", "When you just want to test or simply use it to predict some sentences, you can use pipeline(). Besides text classification, they already provided many different tasks such as text generation, question answering, summarization, and so on. To run sentiment analysis task, simply type:", "It uses a model named \u201cdistilbert-base-uncased-finetuned-sst-2-english\u201d by default. We can also change to other models that we can find in the model hub. For example, if we want to use nlptown/bert-base-multilingual-uncased-sentiment, then simply do the following:", "First thing first, we need a dataset. At this point, we are going to use the dataset provided by \ud83e\udd17 Datasets. They provide a wide range of task options, varying from text classification, token classification, language modeling, and many more. To install it, simply execute the following line:", "We are going to use sst2 dataset from GLUE task and bert-base-uncased pretrained. By runningload_dataset and load_metric, we are downloading dataset as well as metric. load_metricautomatically loads a metric associated with the chosen task.", "To preprocess, we need to instantiate our tokenizer using AutoTokenizer (or other tokenizer class associated with the model, eg: BertTokenizer). By calling from_pretrained(), we download the vocab used during pretraining the given model (in this case, bert-base-uncased). The vocab is useful so that the tokenization results are corresponding to the model\u2019s vocab.", "Fortunately, they also provide a simple interface called Trainer() which makes the training and evaluation process much easier without losing its flexibility to modify a wide range of training options.", "First, instantiate and download the model with from_pretrained(). Since our task is sequence classification, we can use AutoModelForSequenceClassification (or other model class associated to the pretrained, eg: BertForSequenceClassification).", "We need to define our own compute_metrics function if we want to have other metrics in addition to the loss. This function can be passed to the trainer.", "Now we just need to convert our dataset into the right format so that the model can work properly. We will use a small subset from Amazon review dataset in the fashion category. You can find the dataset here. The labels are still in the form of rating, so we need to change them into whether positive or negative. Reviews with 3 or more stars will be classified as positive, and the rest are negative. This is just for an example, feel free to change it the way you like.", "After that, we split them into train, validation, and test and tokenize them using AutoTokenizer. We also need to convert our data to dataset object by subclassing torch.utils.data.Dataset object and implementing __len__ and __getitem__. Take a look at AmazonDataset class below. For training, just repeat the steps in the previous section. But this time, we use DistilBert instead of BERT. It is a small version of BERT. Faster and lighter!", "As you can see, the evaluation is quite good (almost 100% accuracy!). Apparently, it\u2019s because there are a lot of repetitive data. Some reviews can appear more than three times in the dataset. So, make sure that your data is clear and good enough to represent the actual world.", "Even better, they also support hyperparameter search using Optuna or Ray tune (you can choose one). It will run the training process several times so it needs to have the model defined via a function (so it can be reinitialized at each new run). See model_init function below.", "Besides that, it will also take a very long time to run. Alternatively, you can do a hyperparameter search using only a portion of the training data to save time and resources. After getting the best configuration, we can rerun the training using full data with the best configuration. Just do something like this:", "This process will return a BestRun object containing information about the hyperparameter which is used for the best run. To use this configuration, just set the hyperparameter into TrainingArgument.", "That\u2019s it! If you want to try another task or another pretrained model or even use your own dataset, you can easily customize it to your needs by modifying a couple of lines, and BOOM! You already had your own transformers-powered NLP model!", "[2] Devlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)", "If you enjoyed reading this post and would like to hear more from me and other writers here, join Medium and subscribe to my newsletter. Or simply follow the links below. Thank you!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2e99b8c5423&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@arfinda?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arfinda?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "Arfinda Ilmania"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff5eb4b4b2023&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&user=Arfinda+Ilmania&userId=f5eb4b4b2023&source=post_page-f5eb4b4b2023----2e99b8c5423---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e99b8c5423&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e99b8c5423&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jamie452?utm_source=medium&utm_medium=referral", "anchor_text": "Jamie Street"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://huggingface.co/transformers/index.html", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/installation.html", "anchor_text": "website"}, {"url": "https://huggingface.co/models", "anchor_text": "model hub"}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer", "anchor_text": "Trainer()"}, {"url": "https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained", "anchor_text": "from_pretrained("}, {"url": "https://nijianmo.github.io/amazon/index.html", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/index.html", "anchor_text": "Huggingface Transformers"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://medium.com/@arfinda/membership", "anchor_text": "join Medium"}, {"url": "https://medium.com/subscribe/@arfinda", "anchor_text": "subscribe to my newsletter"}, {"url": "https://medium.com/@arfinda/membership", "anchor_text": "Join Medium with my referral link - Arfinda IlmaniaAs a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story\u2026medium.com"}, {"url": "https://medium.com/subscribe/@arfinda", "anchor_text": "Get an email whenever Arfinda Ilmania publishes.Get an email whenever Arfinda Ilmania publishes. By signing up, you will create a Medium account if you don't already\u2026medium.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----2e99b8c5423---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----2e99b8c5423---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2e99b8c5423---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----2e99b8c5423---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e99b8c5423&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&user=Arfinda+Ilmania&userId=f5eb4b4b2023&source=-----2e99b8c5423---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e99b8c5423&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&user=Arfinda+Ilmania&userId=f5eb4b4b2023&source=-----2e99b8c5423---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e99b8c5423&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2e99b8c5423&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2e99b8c5423---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2e99b8c5423--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2e99b8c5423--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2e99b8c5423--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arfinda?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arfinda?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arfinda Ilmania"}, {"url": "https://medium.com/@arfinda/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "103 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff5eb4b4b2023&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&user=Arfinda+Ilmania&userId=f5eb4b4b2023&source=post_page-f5eb4b4b2023--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F87129b17fb7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423&newsletterV3=f5eb4b4b2023&newsletterV3Id=87129b17fb7b&user=Arfinda+Ilmania&userId=f5eb4b4b2023&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}