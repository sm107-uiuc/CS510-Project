{"url": "https://towardsdatascience.com/model-free-prediction-reinforcement-learning-507297e8e2ad", "time": 1682994838.125241, "path": "towardsdatascience.com/model-free-prediction-reinforcement-learning-507297e8e2ad/", "webpage": {"metadata": {"title": "Model-Free Prediction: Reinforcement Learning | by Ryan Wong | Towards Data Science", "h1": "Model-Free Prediction: Reinforcement Learning", "description": "Previously, we looked at planning by dynamic programming to solve a known MDP. In this post, we will use model-free prediction to estimate the value function of an unknown MDP. i.e We will look at\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "anchor_text": "Introduction to Reinforcement Learning", "paragraph_index": 0}], "all_paragraphs": ["Previously, we looked at planning by dynamic programming to solve a known MDP. In this post, we will use model-free prediction to estimate the value function of an unknown MDP. i.e We will look at policy evaluation of an unknown MDP. This series of blog posts contain a summary of concepts explained in Introduction to Reinforcement Learning by David Silver.", "The three main methods that will be explained for model-free predictions are:", "This post will mainly look at evaluating a given policy in an unknown MDP and not finding the optimal policy.", "Monte Carlo methods are model-free which learn directly from episodes of experience. Monte Carlo learns from complete episodes with no bootstrapping. One drawback to MC is that it can only apply to episodic Markov Decision Processes where all episodes must terminate.", "Model-Free: no knowledge of MDP transitions / rewardsBootstrapping: update involves an estimate", "Goal: Given a policy \u03c0, learn v_\u03c0 (value for the policy) from episodes of experience.", "Monte-Carlo policy evaluation uses empirical mean return instead of expected return. Two approaches to evaluate the value function of a policy at a state is to use First-Visit Monte-Carlo Policy Evaluation or Every-Visit Monte-Carlo Policy Evaluation.", "In both of the above evaluation approaches, we had to track the statistics of our algorithm. i.e we can only compute the value after we have completed all the episodes. To solve this problem we can use the Incremental Mean equation to update the value incrementally.", "Incremental MeanThe mean \u00b5\u2081, \u00b5\u2082, \u2026 of a sequence x\u2081, x\u2082, \u2026 can be computed incrementally.", "Incremental Monte-Carlo UpdatesUpdate V(s) incrementally after episode S\u2081, A\u2081, R\u2082, \u2026, S\u209c. For each state S\u209c with return G\u209c :", "In non-stationary problems (where things are drifting around and you don\u2019t need to remember things that happened long time ago), we can use the running mean approach, i.e. forget old episodes.", "Temporal-Difference is model-free. Temporal Difference methods learn directly from experience / interaction with the environment. Temporal Difference learns from incomplete episodes, by bootstrapping (update the guess of the value function).", "In both MC and TD the goal is to learn v_\u03c0 online from experience under policy \u03c0. If we were to apply incremental every-visit Monte-Carlo we update the value V(S\u209c) towards the actual return G\u209c", "The simplest temporal-difference learning algorithm, TD(0) differs as we update the value V(S\u209c) towards an estimated return R\u209c\u208a\u2081 + \u03b3V(S\u209c\u208a\u2081)", "TD learning updates the value function immediately which allows it to learn before knowing the final outcome after every step unlike MC which must wait until the end of the episode before the return is known. TD works in continuing (non-terminating) environments while MC only works for episodic (terminating) environments / complete sequences.", "An example to illustrate the difference between TD and MC, is if we were to try to predict the how long it takes to drive home at each state along the way.  In MC, we would assign each state the value we receive at the end of the journey (actual outcome).  In TD, we would update the value along the way at each state using the impact that the next state has on the current state (estimated outcome).", "There is a trade-off between the bias and the variance. MC has a high variance and zero bias as it uses the return G\u209c which depends on many random actions, transitions and rewards. Therefore it has good convergence properties even with the function approximation and is not sensitive to initial value.", "TD has low variance and some bias as the TD target depends on one random action, transition and reward. It is usually more efficient than MC. TD(0) converges to v_\u03c0(s) but not always with function approximation. Unlike MC it is more sensitive to the initial value.", "So we have seen that MC and TD converge: V(s) \u2192 v_\u03c0(s) as experience \u2192 \u221eBut in practise we cannot go on forever, so how do these algorithms converge for batch solution for finite experience?", "Suppose we have two states A, B with no discounting and 8 episodes of experience.", "What is the value at state A. V(A) ?MC converges to solution which best fits the observed returns with minimum mean-squared error.", "Therefore V(A)=0. As the only time the state A appears in an episode is when the return is 0.", "TD(0) converges to solution of max likelihood Markov model. It is the solution to the MDP that best fits the data.", "Therefore V(A)=0.75. Since we got a reward 6 out of 8 episodes. TD exploits the Markov property unlike MC.", "Monte-Carlo Backup:Value of a state S\u209c can only be computed once a terminal state is reached", "Temporal-Difference TD(0) Backup: Value of a state S\u209c is computed using only one step look ahead.", "Dynamic Programming Backup:Value at S\u209c is computed with a one step look at every possible state and computes an expected value.", "n-Step ReturnAn approach between TD(0) and MC, where we have n-step temporal-difference learning. Therefore the value will be computed by looking ahead n-steps and apply the temporal-difference learning method.", "Instead of looking at each n-step return G\u209c\u207d\u207f\u207e, we can use a decaying weighted sum to combine all n-step returns called the \u03bb-return.", "The value at a state can now be computed using a forward-view TD(\u03bb)", "The forward-view looks into the future to compute the \u03bb-return and updates the value function towards it and can only be computed from complete episodes.", "The backward-view provides mechanism to update the value online, every step, from incomplete sequences. We keep an eligibility trace for every state s and update the V(s) for every state s in proportion to TD-error \u03b4\u209c and eligibility trace E\u209c(s).", "Eligibility TraceEligibility traces combine both frequency heuristic and recency heuristic.- Frequency heuristic: assign credit to most frequent states- Recency heuristic: assign credit to most recent states", "We have looked at various methods for model-free predictions such as Monte-Carlo Learning, Temporal-Difference Learning and TD(\u03bb). These methods allowed us to find the value of a state when given a policy. In the next post, we will look at finding the optimal policies using model-free methods.", "If you enjoyed this post and want to see more don\u2019t forget follow and/or leave a clap.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F507297e8e2ad&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@taggatle?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@taggatle?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "Ryan Wong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c034a2353ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&user=Ryan+Wong&userId=8c034a2353ca&source=post_page-8c034a2353ca----507297e8e2ad---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F507297e8e2ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F507297e8e2ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "anchor_text": "Introduction to Reinforcement Learning"}, {"url": "https://towardsdatascience.com/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d", "anchor_text": "1"}, {"url": "https://towardsdatascience.com/getting-started-with-markov-decision-processes-reinforcement-learning-ada7b4572ffb", "anchor_text": "2"}, {"url": "https://towardsdatascience.com/planning-by-dynamic-programming-reinforcement-learning-ed4924bbaa4c", "anchor_text": "3"}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf", "anchor_text": "UCL Course on RL \u2014 Lecture 4"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----507297e8e2ad---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----507297e8e2ad---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----507297e8e2ad---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----507297e8e2ad---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/predictions?source=post_page-----507297e8e2ad---------------predictions-----------------", "anchor_text": "Predictions"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F507297e8e2ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&user=Ryan+Wong&userId=8c034a2353ca&source=-----507297e8e2ad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F507297e8e2ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&user=Ryan+Wong&userId=8c034a2353ca&source=-----507297e8e2ad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F507297e8e2ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F507297e8e2ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----507297e8e2ad---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----507297e8e2ad--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----507297e8e2ad--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----507297e8e2ad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@taggatle?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@taggatle?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ryan Wong"}, {"url": "https://medium.com/@taggatle/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "347 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c034a2353ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&user=Ryan+Wong&userId=8c034a2353ca&source=post_page-8c034a2353ca--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F87d33b7279c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-free-prediction-reinforcement-learning-507297e8e2ad&newsletterV3=8c034a2353ca&newsletterV3Id=87d33b7279c4&user=Ryan+Wong&userId=8c034a2353ca&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}