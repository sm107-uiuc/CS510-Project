{"url": "https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28", "time": 1682995260.900501, "path": "towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28/", "webpage": {"metadata": {"title": "Using LDA Topic Models as a Classification Model Input | by Marc Kelechava | Towards Data Science", "h1": "Using LDA Topic Models as a Classification Model Input", "description": "Topic Modeling in NLP seeks to find hidden semantic structure in documents. They are probabilistic models that can help you comb through massive amounts of raw text and cluster similar groups of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised", "anchor_text": "the Repo", "paragraph_index": 7}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/blob/master/preprocess.py", "anchor_text": "preprocessing script", "paragraph_index": 7}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/tree/master/data", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/blob/master/notebooks/2-train_corpus_prep_and_LDA_train.ipynb", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/blob/master/notebooks/3-test_corpus_prep_and_apply_LDA_get_vectors.ipynb", "anchor_text": "here", "paragraph_index": 7}, {"url": "http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/", "anchor_text": "here", "paragraph_index": 19}, {"url": "http://gerin.perso.math.cnrs.fr/ChineseRestaurant.html", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://medium.com/@eduardo.coronado92", "anchor_text": "Eduardo Coronado", "paragraph_index": 21}, {"url": "http://gibbslda.sourceforge.net/fp224-phan.pdf", "anchor_text": "paper", "paragraph_index": 28}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised", "anchor_text": "here", "paragraph_index": 46}], "all_paragraphs": ["Topic Modeling in NLP seeks to find hidden semantic structure in documents. They are probabilistic models that can help you comb through massive amounts of raw text and cluster similar groups of documents together in an unsupervised way.", "This post specifically focuses on Latent Dirichlet Allocation (LDA), which was a technique proposed in 2000 for population genetics and re-discovered independently by ML-hero Andrew Ng et al. in 2003. LDA states that each document in a corpus is a combination of a fixed number of topics. A topic has a probability of generating various words, where the words are all the observed words in the corpus. These \u2018hidden\u2019 topics are then surfaced based on the likelihood of word co-occurrence. Formally, this is Bayesian Inference problem [1].", "Once LDA topic modeling is applied to set of documents, you\u2018re able to see the words that make up each hidden topic. In my case, I took 100,000 reviews from Yelp Restaurants in 2016 using the Yelp dataset [2]. Here are two examples of topics discovered via LDA:", "You can see the first topic group seems to have identified word co-occurrences for negative burger reviews, and the second topic group seems to have identified positive Italian restaurant experiences. The third topic isn\u2019t as clear-cut, but generally seems to touch on terrible, dry salty food.", "I was more interested to see if this hidden semantic structure (generated unsupervised) could be converted to be used in a supervised classification problem. Assume for a minute that I had only trained a LDA model to find 3 topics as above. After training, I could then take all 100,000 reviews and see the distribution of topics for every review. In other words, some documents might be 100% Topic 1, others might be 33%/33%/33% of Topic 1/2/3, etc. That output is just a vector for every review showing the distribution. The idea here is to test whether the distribution per review of hidden semantic information could predict positive and negative sentiment.", "With that intro out of the way, here was my goal:", "If the supervised F1-scores on the unseen data generalizes, then we can posit that the 2016 topic model has identified latent semantic structure that persists over time in this restaurant review domain.", "UPDATE (9/23/19): I\u2019ve added a README to the Repo which shows how to create a MongoDB using the source data. I\u2019ve also included a preprocessing script which will allow you to create the exact training and test DataFrames I use below. However, I realize that might be a lot of work, so I also included pickle files of my Train and Test DataFrames in the directory here. This will allow you to follow along with the notebooks in the repo directly, namely, here and then here. If you\u2019d rather just get the highlights/takeaways, I point out all the key bits in the rest of this blog post below with code snippets.", "I used the truly wonderful gensim library to create bi-gram representations of the reviews and to run LDA. Gensim\u2019s LDA implementation needs reviews as a sparse vector. Conveniently, gensim also provides convenience utilities to convert NumPy dense matrices or scipy sparse matrices into the required form.", "I\u2019ll show how I got to the requisite representation using gensim functions. I started with a pandas DataFrame containing the text of every review in a column named'text\u2019, which can be extracted to a list of list of strings, where each list represents a review. This is the object namedwords in my example below:", "I\u2019m omitting showing a few steps of additional pre-processing (punctuation, stripping newlines, etc) for brevity in this post.", "There are really 2 key items in this code block:", "\u201cThe function doc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. The sparse vector [(0, 1), (1, 1)] therefore reads: in the document \u201cHuman computer interaction\u201d, the words computer (id 0) and human (id 1) appear once; the other ten dictionary words appear (implicitly) zero times.\u201d", "Line 23 above then gives us the corpus in the representation needed for LDA.", "To give an example of the type of text we\u2019re dealing with, here\u2019s a snapshot of a Yelp review:", "In order to train a LDA model you need to provide a fixed assume number of topics across your corpus. There are a number of ways you could approach this:", "Of these: I don\u2019t trust #1 as a method at all. Who am I to say what\u2019s sensible or not in this context? I\u2019m relying on LDA to identify latent topic representations of 100,000 documents, and it\u2019s possible that it won\u2019t necessarily be intuitive. For #2: I talked to some former NLP professionals and they dissuaded me from relying on coherence scores based on their experience in industry. Approach #3 would be reasonable for my purpose but there\u2019s a reality that LDA takes a non-trivial of time to train even though I was using a 16GB 8-Core AWS instance.", "Thus I came up with an idea that I think is fairly novel \u2014 at the very least, I haven\u2019t seen anyone do this online or in papers:", "Gensim also provides a Hierarchical Dirichlet Process (HDP) class [5]. HDP is similar to LDA, except it seeks to learn the correct number of topics from the data; that is, you don\u2019t need to provide a fixed number of topics. I figured I would run HDP on my 100,000 reviews a few times and see the number of topics it was learning. In my case this was always 20 topics, so I went with that.", "To get an intuitive feel for HDP: I found a number of sources online that said it\u2019s most similar to a Chinese Restaurant Process. This is explained brilliantly by Edwin Chen here [6], and beautifully visualized in [7] here. Here\u2019s the visualization from [7]:", "In this example, we need to assign 8 to a topic. There\u2019s a 3/8 probability 8 will land in topic C1, a 4/8 probability 8 will land in topic C2, and a 1/8 probability a new topic C3 will be created. In this way a number of topics is discovered. So the bigger a cluster is, the more likely it is for someone to join that cluster. I felt this was just as reasonable as any other method to choose a fixed topic number for LDA. If anyone with a heavier Bayesian Inference background has thoughts on this please weigh in!", "UPDATE [4/13/2020] \u2014 Eduardo Coronado has kindly provided some more precise info on HDP, via the comments:", "\u201cIt is true that HDPs non-parametric property allow us to learn the topics from the data however Dirichlet Process mixtures do that already. The main advantage of HDPs is they allowing distinct corpora (groups) to share statistical strength when modeling \u2014 in this case share a common set of potentially infinite topics. So it is an extension of Dirichlet Process mixtures.\u201d", "Here\u2019s the code to run LDA with Gensim:", "By turning on the eval_every flag we\u2019re able to process the corpus in chunks: in my case chunks of 100 documents worked fairly well for convergence. The number of passes is separate passes over the entire corpus.", "Once that\u2019s done, you can view the words making up each topic as follows:", "With that code you\u2019ll see 10 of the 20 topics and the 15 top words for each.", "Now comes the interesting bit. We\u2019re going to use the LDA model to grab the distribution of these 20 topics for every review. This 20-vector will be our feature vector for supervised classification, with the supervised learning goal being to determine positive or negative sentiment.", "Note that I think this approach for supervised classification using topic model vectors is not very common. When I did it I wasn\u2019t aware of any example online of people trying this, though later on when I was done I discovered this paper where it was done in 2008 [8]. Please let me know if there are other examples out there!", "The ultimate goal is not only to see how this performs in a train/test CV split of the current data, but whether the topics have hit on something fundamental that translates to unseen test data in the future (in my case, data from a year later).", "Here\u2019s what I did to grab the feature vectors for every review:", "The key bit is using minimum_probability=0.0 in line 3. This ensures that we\u2019ll capture the instances where a review is presented with 0% in some topics, and the representation for each review will add up to 100%.", "Lines 5 and 6 are two hand-engineered features I added.", "Thus a single observation for a review for supervised classification now looks like this:", "The first 20 items represent the distribution for the 20 found topics for each review.", "We\u2019re now ready to train! Here I\u2019m using 100,000 2016 restaurant reviews and their topic-model distribution feature vector + two hand-engineered features:", "I\u2019ll first walk-through the lower .53 and .62 f1-scores using Logisitic Regression. When I initially started training I tried to predict the individual review ratings: 1,2,3,4 or 5 stars. As you can see this was not successful. I was a bit discouraged with the initial .53 score, so went back to examine my initial EDA charts to see if I could notice anything in the data. I had run this chart earlier on:", "This shows the word count IQR range by rating. Since the main IQR range was pretty compact, I decided to try re-running the LDA pre-processing and model restricted to just (roughly) the IQR range. Making this change increased my Logistic Regression score on f1 to .62 for 1,2,3,4,5 star classification. Still not great.", "At this point I decided to see what would happen if I got rid of the 3-stars, and grouped the 1,2 stars as \u2018bad\u2019 and 4,5 stars as \u2018good\u2019 sentiment scores. As you\u2019ll see in the graph above, this worked wonder! Now Logistic Regression yieled a .869 f1-score.", "When I was running these, I noticed a \u2018modified huber\u2019 loss option in SKLearn\u2019s Stochastic Gradient Descent implementation [10]. In this case, the penalty for being wrong is much worse than either Hinge (SVC) or Log Loss:", "I\u2019m still grappling with why this worked so well, but my initial thought is that these punishing penalties caused SGD (remember, 1 by 1 weight updates) to learn quickly. Regularization helped immensely here as well. The alpha in line 42 in the code above is a regularization parameter (think like in Ridge or Lasso regularization), and this helped in getting my f1-score up to .936.", "At this point I was pretty thrilled by these results, but wanted to further see what would happen on completely unseen data.", "All that\u2019s required is making bigrams for the test corpus, then throwing this into the test-vector extraction approach as before:", "Somewhat shockingly to me, this generalizes!", "I was thrilled with this result, as I believe this approach could work generally for any company trying to train a classifier in this way. I also did a hypothesis test at the end using mlxtend [11] and the final result is indeed statistically significant.", "I intend on extending this a bit in the future, and I\u2019ll leave you with that:", "I\u2019ve also hosted all the code for this and the trained LDA model on my GitHub here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcf8ee9e5cf28&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marckelechava?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marckelechava?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "Marc Kelechava"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa0cc3baa6435&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&user=Marc+Kelechava&userId=a0cc3baa6435&source=post_page-a0cc3baa6435----cf8ee9e5cf28---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf8ee9e5cf28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf8ee9e5cf28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised", "anchor_text": "the Repo"}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/blob/master/preprocess.py", "anchor_text": "preprocessing script"}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/tree/master/data", "anchor_text": "here"}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/blob/master/notebooks/2-train_corpus_prep_and_LDA_train.ipynb", "anchor_text": "here"}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised/blob/master/notebooks/3-test_corpus_prep_and_apply_LDA_get_vectors.ipynb", "anchor_text": "here"}, {"url": "https://radimrehurek.com/gensim/tut1.html", "anchor_text": "examples"}, {"url": "http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/", "anchor_text": "here"}, {"url": "http://gerin.perso.math.cnrs.fr/ChineseRestaurant.html", "anchor_text": "here"}, {"url": "https://medium.com/@eduardo.coronado92", "anchor_text": "Eduardo Coronado"}, {"url": "http://gibbslda.sourceforge.net/fp224-phan.pdf", "anchor_text": "paper"}, {"url": "https://github.com/marcmuon/nlp_yelp_review_unsupervised", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation", "anchor_text": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"}, {"url": "https://www.yelp.com/dataset", "anchor_text": "https://www.yelp.com/dataset"}, {"url": "https://radimrehurek.com/gensim/tut1.html", "anchor_text": "https://radimrehurek.com/gensim/tut1.html"}, {"url": "https://radimrehurek.com/gensim/models/coherencemodel.html", "anchor_text": "https://radimrehurek.com/gensim/models/coherencemodel.html"}, {"url": "https://radimrehurek.com/gensim/models/hdpmodel.html", "anchor_text": "https://radimrehurek.com/gensim/models/hdpmodel.html"}, {"url": "http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/", "anchor_text": "http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/"}, {"url": "http://gerin.perso.math.cnrs.fr/ChineseRestaurant.html", "anchor_text": "http://gerin.perso.math.cnrs.fr/ChineseRestaurant.html"}, {"url": "http://gibbslda.sourceforge.net/fp224-phan.pdf", "anchor_text": "http://gibbslda.sourceforge.net/fp224-phan.pdf"}, {"url": "http://blog.madhukaraphatak.com/class-imbalance-part-2/", "anchor_text": "http://blog.madhukaraphatak.com/class-imbalance-part-2/"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "anchor_text": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"}, {"url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/", "anchor_text": "http://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cf8ee9e5cf28---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/topic-modeling?source=post_page-----cf8ee9e5cf28---------------topic_modeling-----------------", "anchor_text": "Topic Modeling"}, {"url": "https://medium.com/tag/nlp?source=post_page-----cf8ee9e5cf28---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/classification?source=post_page-----cf8ee9e5cf28---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----cf8ee9e5cf28---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf8ee9e5cf28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&user=Marc+Kelechava&userId=a0cc3baa6435&source=-----cf8ee9e5cf28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf8ee9e5cf28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&user=Marc+Kelechava&userId=a0cc3baa6435&source=-----cf8ee9e5cf28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf8ee9e5cf28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcf8ee9e5cf28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cf8ee9e5cf28---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cf8ee9e5cf28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marckelechava?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marckelechava?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marc Kelechava"}, {"url": "https://medium.com/@marckelechava/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "432 Followers"}, {"url": "https://www.linkedin.com/in/marckelechava/", "anchor_text": "https://www.linkedin.com/in/marckelechava/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa0cc3baa6435&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&user=Marc+Kelechava&userId=a0cc3baa6435&source=post_page-a0cc3baa6435--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6e9afdb68ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28&newsletterV3=a0cc3baa6435&newsletterV3Id=6e9afdb68ae&user=Marc+Kelechava&userId=a0cc3baa6435&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}