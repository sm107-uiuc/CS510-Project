{"url": "https://towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3", "time": 1682997022.549718, "path": "towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3/", "webpage": {"metadata": {"title": "Support Vector Machines for Classification | by Oscar Contreras Carrasco | Towards Data Science", "h1": "Support Vector Machines for Classification", "description": "Classification in Machine Learning is the task of learning to distinguish points that belong to two or more categories in a dataset. In geometrical terms, associating a set of points to some category\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/math/0701907.pdf", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://bit.ly/2xBDJ2Q", "anchor_text": "https://bit.ly/2xBDJ2Q", "paragraph_index": 34}, {"url": "https://work.caltech.edu/telecourse", "anchor_text": "https://work.caltech.edu/telecourse", "paragraph_index": 50}], "all_paragraphs": ["Classification in Machine Learning is the task of learning to distinguish points that belong to two or more categories in a dataset. In geometrical terms, associating a set of points to some category involves finding the best possible separation between these. Let us suppose we have a dataset that looks like this:", "Here, we can clearly distinguish two categories, each identified by C1 = 1 and C2 = -1. We will also define this as a binary classification problem. For instance, if our data points each represent e-mail texts, and we wish to categorise these as either spam and non-spam, then we would have that each email is a data point x, whereas y = 1 would identify spam, and y = -1 non-spam. Likewise, we could have other similar scenarios, like dog-cat classification, credit / non-credit approval, etc.", "In Figure 1 we can also see that the linear function in between ensures the best possible separation between the two categories. The reason we have called it hyperplane on purpose is because the description we have here can be extended to multidimensional scenarios. For instance, if we were working in a 3D space, we would have a dividing plane instead of a line.", "From now on, we will identify the separation between the categorised datasets as margin, and it will be governed by the distance formed between the closest points and the separating hyperplane. So the question that arises immediately is, how can we ensure that this optimal hyperplane ensures the best possible margin for the two categories? This is exactly what Support Vector Machines, or SVM for short will do for us.", "Before moving on, it\u2019s worth pointing out that SVMs are among the most powerful machine learning algorithms for classification tasks and are used extensively for applications ranging from computer vision to NLP. Let\u2019s explore this method further.", "First, let\u2019s suppose our dataset is composed of N points. In Figure 1, we can see that the two categories involved can be perfectly separated by using a linear function (aka hyperplane) defined by:", "Here, xn denotes a data point, w is a vector of weights, b is the bias, and yn is the model\u2019s prediction. In this case each prediction can be either 1 or -1 for points that lie on both sides of the hyperplane and 0 for points that lie on the hyperplane itself. Given a dataset that contains several x\u2019s with their corresponding y\u2019s, what we want to do is to find values for w and b that give us the best possible margin.", "For further clarity, let us illustrate our model variables geometrically:", "Here, xn is one of the points closest to the hyperplane and it forms an orthogonal vector d that stems from it. As we can see, vector d is in the same direction as w. Besides, any point x0 that lies on the hyperplane will form a vector r with xn. Given these definitions, we can clearly see that d is the projection of r on w given by:", "Let\u2019s add b and -b to the numerator in this equation. The magnitude of d will give us the distance of x to the hyperplane and is defined by:", "Note that for x0 that lies on the hyperplane, its corresponding prediction y0 is zero, and that\u2019s why the expression inside of the parenthesis vanishes. Therefore, in order to find the optimal margin, what we want to do is to maximize d. With these considerations, we can finally state the optimization problem for the model:", "Well, from Calculus we know that in order to find a maximum value, we need to use derivatives. Given this scenario, it will be more convenient for us to work with expressions that do not involve the use of reciprocal terms. So how could we make the d equation easier for us to optimize? Yes, you guessed it right! Invert the reciprocal term inside of the parentheses and then turn it into a minimization problem. Let\u2019s do that:", "So now you may be asking, why did we raise w to the power 2 and then divide it by 2? This is just a mathematical trick we are applying to make things easier for us when computing derivatives. Basically, the main reason is that the derivative for this expression is simply w. However and most importantly, this is going to help us define a quadratic programming problem that we will be able to solve using popular optimization libraries as we will see later.", "Okay, so at this point we have part of our optimization problem defined. However, let\u2019s take a look at the restriction a bit further:", "We can see that there is an absolute value involved. Not looking great, right? It would be nice if we could get rid of that. How can we do it? Well, let\u2019s think a little bit about correctness of our predictions. We want our model to try its best to classify all points correctly. But how do we define a correctly classified point in mathematical terms? Let\u2019s first introduce some helpful notation.", "So we want our model to make predictions that are equal to the correct answer yn. This will happen when the given and predicted answers have the same sign. That is, when the multiplication of both is at least one. Let us state that as a mathematical expression:", "Great! With this restriction in place, as well as Equation 1, our optimization problem will be the following:", "This expression defines the primal problem for Support Vector Machines. Note that for each restriction formed by (xn, yn) there is an associated Langrange multipler \u03b1. The data points xn whose alpha value is greater than zero are also known as support vectors, since they influence the behaviour of the separating hyperplane.", "Additionally, if we take a look at Equation 3, we can see that it has a quadratic form on w. We can therefore say this is a quadratic programming problem. However, we have an issue here, this problem depends on three variables and not just one. Making it dependent on just one parameter set will definitely make things much easier for us, and this will motivate our derivation for the dual problem that we discuss in the next section.", "In the previous section, we formulated the primal problem for the SVM. However, as we discussed earlier, it will be much easier for us to just make it dependent on one parameter. Let\u2019s do that now.", "If we calculate the derivative of L in Equation 3 with respect to w, and b and then equate the derivatives to zero, we get:", "Replacing these two expressions in Equation 3, we get:", "Well, as we can see, this has become a quadratic programming problem that depends only on the Lagrange multipliers \u03b1 and not on w and b anymore. This is definitely a much easier scenario than the primal problem we discussed before. Besides, there is an interesting aspect about the production of xm and xn in Equation 4. It turns out to be that this product can actually be expressed as a kernel function. In general terms, kernel functions allow us to transform non-linearly separable spaces to linearly-separable ones and are a useful tool when we are attempting to perform diverse classification tasks. Kernels used commonly include but are not limited to, linear kernels and radial-basis functions (RBF). More on kernel methods can be found here [2].", "Finally, the dual problem can be stated as follows:", "Which is a quadratic programming problem for maximization. Here, k is a kernel function we define. Also, note the first restriction that enforces all alphas to be greater than or equal to zero. As we stated earlier, data points whose alpha value is greater than zero are the support vectors and influence the behaviour of the separating hyperplane. Furthermore, if we turn this into a minimization problem and convert these expressions to matrix form, we can easily use optimization libraries like CVXOPT to solve for the optimal parameters. We will see how to do this later.", "Our definition for the dual problem, as is, has an important limitation here. It only works well when the categories can be separated completely. That\u2019s why this default version is also called hard-margin SVM. But as you can imagine, it\u2019s very unlikely for us to find datasets that are completely separable in real life. Therefore, we will next discuss a variation that allows us to extend SVMs to scenarios where some points could be misclassified, also known as soft-margin SVM.", "As we said earlier, hard-margin SVMs have limited use in real life applications. Here we will show that by making a slight change to the original dual problem we can extend SVMs to scenarios where some points could be misclassified. Let\u2019s illustrate our idea by means of a graph:", "Here, we have defined a new tolerance variable \u03b5. Points that lie on the margin will have \u03b5 = 0. On the other hand, the farther we are from the correct boundary, the larger the value of \u03b5 will be. Ultimately, for points that are on the wrong side of the hyperplane, we expect \u03b5 to be greather than 1. What we need to do now is to plug this new variable into our initial definitions for the primal problem. Therefore, if we add \u03b5 to our restriction defined in Equation 2, we get:", "This means that our restriction will now allow for some points to be a bit farther from the margin boundary. \u03b5 itself now becomes another parameter we must account for in our primal problem and we must strive to minimize it. Given this scenario, our objective becomes:", "As we can see, the second term here is a product of a new variable C we define and the sum of all epsilons. This will enforce that when C is larger, the overall misclassification given by the epsilon values will be smaller. That means, the larger C is, the stricter the margin will become. By considering these new variables in the original primal problem, we get:", "Here, \u03bc is a Lagrange multiplier we have defined for the tolerance values \u03b5. We expect each \u03bc to be greater than or equal to zero. Let\u2019s now differentiate L with respect to w, b, and \u03b5.", "Let\u2019s turn our attention to the third expression a little bit. We stated earlier that both \u03bc and \u03b1 can be greater than or equal to zero, this implies the following:", "Then, by replacing all of these expressions in Equation 6 and rearranging like we did for the hard-margin SVM case, we get the following:", "Great! At this point we can see that everything is the same as in the hard-margin SVM case, except for the first restriction that now takes C into account. In the next section we will get to implement a soft-margin SVM by using a toy dataset and we will use CVXOPT for optimisation purposes.", "Note: The full implementation is available as a Jupyter notebook at https://bit.ly/2xBDJ2Q", "CVXOPT [3] is a Python library for convex optimisation. We will use it to solve the dual problem for soft-margin SVMs. However, before doing so we need to convert such a dual problem to a minimization objective and then turn all of our variables into matrices. To turn the dual problem into minimization, we just invert the sign of the Lagrangian in Equation 7 and then express it in matrix form:", "Here, we have used the * operator to denote the element-by-element multiplication in Python. Additionally, K is the Gram-matrix that results from calculating kernel values over the whole dataset X.", "The restrictions, now expressed in matrix form, are the following:", "Now, CVXOPT will require a particular notation to configure our dual problem, as well as our restrictions. Essentially, what we need to do is to configure the following matrices:", "The corresponding Python code using CVXOPT is the following:", "The tolerance parameters at the end define how much variation we will be allowing before declaring convergence. As you can see, the solvers.qp call receives all matrices we have configured, and it will print out the cost values at each epoch until convergence.", "Once our SVM is fully trained, we can easily obtain the parameters w and b by using the following expressions:", "Where S is the subset of support vectors. The corresponding implementation is as follows:", "We have tested this implementation using two self-generated datasets for illustrative purposes. The first dataset is linearly separable and the second one has categories that can be separated by a circle. Here are the graphs for the two datasets:", "In both cases, we can see that the points belonging to the first and second class are coloured red and blue, respectively. Also, the points in black are the support vectors we have found. By configuring the C parameter appropriately, we should see different behaviours for the margin.", "In addition, the second dataset is not linearly separable. What we do in these cases is to use the kernel trick. That is, we use a kernel function that takes us from a non-linearly separable dataset in some space to a counterpart that is linearly separable in another space. In particular, what we do in this example to address the second dataset is to use a RBF instead of a linear kernel.", "The linear kernel and the RBF can be computed by means of these expressions, respectively:", "Where \u03b3 is a hyperparameter we set for the RBF case. The corresponding implementation for these kernels is shown below:", "Support Vector Machines are a very powerful machine learning model. Whereas we focused our attention mainly on SVMs for binary classification, we can extend their use to multiclass scenarios by using techniques such as one-vs-one or one-vs-all, which would involve the creation of one SVM for each pair of classes. I highly encourage you to look further into the implementation as well as how we could train SVMs using popular libraries, such as sklearn. There you are expected to choose appropriate values for C as well as a suitable kernel for better classification results.", "[1] Bishop, Christopher M. Pattern Recognition and Machine Learning (2006) Springer-Verlag Berlin, Heidelberg.", "[4] California Institute of Technology. Learning from Data (2012). Available at https://work.caltech.edu/telecourse", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffc7c1565e3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@OscarContrerasC?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "Oscar Contreras Carrasco"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F91a848e356c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=post_page-91a848e356c8----fc7c1565e3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/math/0701907.pdf", "anchor_text": "here"}, {"url": "https://bit.ly/2xBDJ2Q", "anchor_text": "https://bit.ly/2xBDJ2Q"}, {"url": "https://cvxopt.org/examples/tutorial/qp.html", "anchor_text": "https://cvxopt.org/examples/tutorial/qp.html"}, {"url": "https://work.caltech.edu/telecourse", "anchor_text": "https://work.caltech.edu/telecourse"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fc7c1565e3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/support-vector-machine?source=post_page-----fc7c1565e3---------------support_vector_machine-----------------", "anchor_text": "Support Vector Machine"}, {"url": "https://medium.com/tag/svm?source=post_page-----fc7c1565e3---------------svm-----------------", "anchor_text": "Svm"}, {"url": "https://medium.com/tag/classification?source=post_page-----fc7c1565e3---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----fc7c1565e3---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----fc7c1565e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----fc7c1565e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fc7c1565e3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fc7c1565e3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fc7c1565e3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fc7c1565e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Oscar Contreras Carrasco"}, {"url": "https://medium.com/@OscarContrerasC/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "358 Followers"}, {"url": "https://www.linkedin.com/in/oscar-contreras/", "anchor_text": "https://www.linkedin.com/in/oscar-contreras/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F91a848e356c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=post_page-91a848e356c8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F36bc253d12d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&newsletterV3=91a848e356c8&newsletterV3Id=36bc253d12d7&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}