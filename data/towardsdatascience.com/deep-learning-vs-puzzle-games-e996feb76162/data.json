{"url": "https://towardsdatascience.com/deep-learning-vs-puzzle-games-e996feb76162", "time": 1683015041.915654, "path": "towardsdatascience.com/deep-learning-vs-puzzle-games-e996feb76162/", "webpage": {"metadata": {"title": "Deep Learning vs Puzzle Games. Is deep learning better suited to\u2026 | by Kabalan Gaspard | Towards Data Science", "h1": "Deep Learning vs Puzzle Games", "description": "We\u2019ve all been there. You\u2019re bored on your phone and have some time to kill, so you decide \u2014 against your better judgement \u2014 to visit the games section of the app store to see what\u2019s trending. You\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.bigduckgames.com/", "anchor_text": "Flow Free", "paragraph_index": 1}, {"url": "https://mzucker.github.io/2016/08/28/flow-solver.html", "anchor_text": "excellent blog post by Matt Zucker", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/solving-sudoku-with-convolution-neural-network-keras-655ba4be3b11", "anchor_text": "article by Shiva Verma in Towards Data Science", "paragraph_index": 19}, {"url": "https://flowfreesolutions.com/", "anchor_text": "www.flowfreesolutions.com", "paragraph_index": 22}, {"url": "https://github.com/kgaspard/flow-free-ai/blob/master/imageParser.py", "anchor_text": "Flow Free solution image processor", "paragraph_index": 23}, {"url": "https://mzucker.github.io/swarthmore/", "anchor_text": "Matt Zucker", "paragraph_index": 27}, {"url": "https://www.researchgate.net/profile/Youssef_Keyrouz", "anchor_text": "Youssef Keyrouz", "paragraph_index": 27}, {"url": "https://medium.com/@marksaroufim", "anchor_text": "Mark Saroufim", "paragraph_index": 27}, {"url": "https://github.com/kgaspard/flow-free-ai", "anchor_text": "https://github.com/kgaspard/flow-free-ai", "paragraph_index": 27}], "all_paragraphs": ["We\u2019ve all been there. You\u2019re bored on your phone and have some time to kill, so you decide \u2014 against your better judgement \u2014 to visit the games section of the app store to see what\u2019s trending. You see a puzzle app that looks fun, but it doesn\u2019t really matter, because you\u2019re only going to play this for half an hour then delete it and forget about it, right?", "2 months later, I had completed over 2,000 levels of Flow Free, insisting on getting a \u201cperfect\u201d rating on every level. The premise of the game \u2014 one of the most popular mobile games on both iOS and Android since its 2012 release \u2014 is stupidly simple: connect \u201cvalves\u201d of different colours on a 2D grid, without 2 lines ever crossing:", "The level in the screenshot may seem simple, but it does get harder. As the levels progressed, I found myself coming up with some tactics that would help me solve these advanced levels faster (e.g. sticking to the outermost unfilled boundary whenever possible, avoiding creating \u201cinlets\u201d of unfilled squares, etc.) Browsing online forums, I saw that other players had their own techniques, some similar and some different to mine. This begged the question \u2014 could a computer, not through brute force but through \u201cexperience\u201d, learn these techniques as well?", "Deep learning problems nowadays mostly reduce to deciding which algorithm to use. I started off with A* search. Even if it isn\u2019t deep learning per se, it gives a good idea of the inherent complexity of the problem, and gives us a chance to try out a few heuristics a more advanced algorithm could figure out on its own.", "The space complexity would be way too large to solve the whole board at once, so I started by solving recursively colour by colour (with a backtrack to a previous colour if a given path was \u201cblocked\u201d). As a heuristic, I used the trusty Manhattan distance. I tested my algorithm on 4 sizes of boards: tiny (2x4), small (5x5), medium (8x8), and large (14x14). I made sure the medium and large boards had fewer colours than average, as this actually makes the puzzle more difficult, both for humans and computers (more possible paths/options/states).", "This worked fine on the small board, but took a fair bit of time. I therefore added a few rules to the next state function, in the hope that reinforcement learning algorithms would figure out those rules themselves: prevent non-consecutive adjacent squares of the same colour, or empty \u201cinlets\u201d:", "The results on my 7-year-old PC were much more encouraging, but still needed improvement:", "I was playing around with optimising my A* search a bit more before moving on to reinforcement learning, when I discovered this excellent blog post by Matt Zucker, where he had already built an A* solver for Flow Free (glad to see I wasn\u2019t the only one with this obsession), and had thought much more carefully about the states to eliminate from his A* search. Even more, he had found that a simple SAT algorithm with just 6 rules outperformed his very advanced A* search, and was getting very good solving times with SAT (sub-second even for 14x14 boards).", "It seemed, so far, we had both arrived to the same discouraging conclusion: a simple brute force technique will outperform basic AI algorithms for this type of (non-adversarial, closed-ended) puzzle game.", "Still, there was no use stopping there. After all, the question that launched this exploration still remained: as human players, we discovered specific techniques to beat Flow Free puzzles more efficiently after playing a few levels. Why wouldn\u2019t a machine be able to learn the same techniques?", "I was very excited to try out Q-learning on Flow Free, as that was really where the \u201cI\u201d in \u201cAI\u201d would start to come into play. The work on the A* search was anything but a waste of time, as we can then use it as the state-action space for our Q learning agent. We take the state space to be a combination of which squares on the board are occupied by which colour, and which path (colour) is currently \u201cactive\u201d. An action is simply the next square to fill in that path.", "The learning agent learned quite quickly how to solve the small board (100 iterations in 1.5s) after making some basic errors in the beginning \u2014 looking good so far. On the medium board however, still no dice after 10,000 iterations, which took 10 minutes:", "To improve this, I started playing around with the standard Q-learning parameters (learning rate \u03b1, discount rate \u03b3, exploration/exploitation rate \u03b5) which didn\u2019t help much, so I turned my attention to the reward function. Aside from playing with the actual reward, there was essentially one parameter to toggle with the reward function (or risk becoming too prescriptive, which would go against the whole machine learning exercise): whether or not the agent gets a reward for solving a single path rather than the whole puzzle. Unfortunately, this didn\u2019t make much of a difference.", "In the end, it was becoming clear that the algorithm was struggling on larger boards simply because the option space was too large. Q-learning does indeed help in this situation vs A* search by making more clever choices, but there were still too many situations that, even after 10,000 iterations, the exact Q-learning agent hadn\u2019t seen yet. The next natural step was therefore:", "The applications of approximate Q-learning are fascinating, particularly in games. Rather than the agent deciding a best action in a given state, the idea is to give it some intuitive features that it can quickly calculate at each step, independently of the exact state (configuration of the board), and let it decide which ones are the most important. This can be a game-changer in games like Pacman (e.g. decide your next move based on the distance to the nearest pellet and to the nearest ghost, rather than an action for every single possible state), or of course Flow Free, where the number of possible states is simply too large for exact Q-learning to be effective.", "The trade-off is that it\u2019s now on the developer to pick the \u201cright\u201d features. I narrowed the list down to features I knew were important in many cases (e.g. total remaining Manhattan distance to close each path), and some I suspected were important (but had no way to prove), which I would just let the algorithm figure out. These include:", "Unfortunately, with these features, the Q-learning agent wasn\u2019t even able to solve the small board, even after varying the Q-learning parameters. However, it was an interesting exercise to see it in action. For instance, the algorithm attached a strong negative weight to \u201cboxes with no valves\u201d, meaning it was able to learn that having a box with no valves leads to the puzzle not being solved.", "Perhaps with a larger sample size of puzzles, it could better learn to actually solve them, but I was excited to see it actually picking up what was important. This is a fascinating phenomenon in AI which is quite common in game AI in particular: even if an algorithm can\u2019t win the game, it can find techniques to help a human play better.", "I was, initially, biased against the idea of a supervised approach to Flow Free. First of all, it would require a large sample of solved Flow Free games, which are difficult to find on the public internet. Second, the supervised approach that seemed to most closely match Flow Free \u2014 neural networks \u2014 is a notorious black box, which would preclude the most interesting part of the exercise: seeing the techniques the algorithm learns to solve the puzzle.", "I then however came across an article by Shiva Verma in Towards Data Science where he does something fairly similar with Sudoku: essentially treats a Sudoku board as an image and uses a convolutional neural network (CNN) to solve it. The author reached good results with Sudoku, which caused me to revisit my initial reservations and try this approach for Flow Free nonetheless.", "The first hurdle was, of course, getting the input data; solved Flow Free puzzles in parseable text format are a lot harder to find than Sudoku puzzles. The best way I found in the beginning was to look into the code of the Android app, which had just over a thousand puzzles stored in text format:", "The initial results of the CNN were disappointing: a flat 0% of puzzles from the test sample solved, although the CNN was learning that it should make paths, rather than just fill out colours in isolation.", "Tweaking layers, epochs, kernel size, and the other such usual suspects didn\u2019t help much. It was looking like we were back to a data scientist\u2019s most common problem: the best algorithm in the world is nothing without enough data. The best other source of data I found was www.flowfreesolutions.com, with thousands of Flow Free solutions to entirely new puzzles than the ones I had\u2026but in image format.", "Despite my numerous attempts to contact them through various channels (and even with a financial incentive offered), I was not able to get them to send me a parseable text version of their solutions. Well, this isn\u2019t an AI article for nothing \u2014 who needs the underlying solution matrix when one has modern image processing? Cue a sub-project to build a Flow Free solution image processor:", "This yielded a couple of thousand more data points to work with, but that still really wasn\u2019t much. But I then realised that, as far as the CNN is concerned, the exact value of the colour doesn\u2019t matter, just the fact that the colours are different. So we can permute the colours around and still have another non-redundant data point. Even for 5x5 boards which have up to 6 different colours, this gives our CNN as many as 6!=720 times more data points to work with (and of course even more combinations to choose from for larger board with more colours):", "A friend pointed out that this is in fact a common way to increase data points in game AI. With 720x as many data points, we were finally getting somewhere \u2014 12% accuracy on test data with a 20-epoch model with ~200k data points, that took 20 minutes to run on my 7-year old GPU. Note that we are using a strict criteria for success here: even if the board is off by one cell, we count this as a failure.", "However, the failures were much more interesting than the successes here. Out of almost all of the failures, the CNN solved most of the board correctly, enough so that it would be easy for a human to complete it. In this sense, the CNN was able to solve the original premise of the article: to intuitively learn human techniques:", "I asked some more experienced AI/ML experts (a big thank you to Matt Zucker, Youssef Keyrouz, and Mark Saroufim) to review this article and they suggested trying the following ideas to improve the CNN algorithm. This may be the subject of a Part 2 article, or you can feel free to try them yourselves (as well as the approaches detailed in this article) on https://github.com/kgaspard/flow-free-ai:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ex-mathematician exploring the intersection between tech and art"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe996feb76162&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e996feb76162--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e996feb76162--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kab.gaspard?source=post_page-----e996feb76162--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kab.gaspard?source=post_page-----e996feb76162--------------------------------", "anchor_text": "Kabalan Gaspard"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ccd545c47ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&user=Kabalan+Gaspard&userId=7ccd545c47ce&source=post_page-7ccd545c47ce----e996feb76162---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe996feb76162&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe996feb76162&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.bigduckgames.com/", "anchor_text": "Flow Free"}, {"url": "https://mzucker.github.io/2016/08/28/flow-solver.html", "anchor_text": "excellent blog post by Matt Zucker"}, {"url": "https://towardsdatascience.com/solving-sudoku-with-convolution-neural-network-keras-655ba4be3b11", "anchor_text": "article by Shiva Verma in Towards Data Science"}, {"url": "https://flowfreesolutions.com/", "anchor_text": "www.flowfreesolutions.com"}, {"url": "https://github.com/kgaspard/flow-free-ai/blob/master/imageParser.py", "anchor_text": "Flow Free solution image processor"}, {"url": "https://mzucker.github.io/swarthmore/", "anchor_text": "Matt Zucker"}, {"url": "https://www.researchgate.net/profile/Youssef_Keyrouz", "anchor_text": "Youssef Keyrouz"}, {"url": "https://medium.com/@marksaroufim", "anchor_text": "Mark Saroufim"}, {"url": "https://github.com/kgaspard/flow-free-ai", "anchor_text": "https://github.com/kgaspard/flow-free-ai"}, {"url": "https://medium.com/tag/ai?source=post_page-----e996feb76162---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e996feb76162---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e996feb76162---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/games?source=post_page-----e996feb76162---------------games-----------------", "anchor_text": "Games"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----e996feb76162---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe996feb76162&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&user=Kabalan+Gaspard&userId=7ccd545c47ce&source=-----e996feb76162---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe996feb76162&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&user=Kabalan+Gaspard&userId=7ccd545c47ce&source=-----e996feb76162---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe996feb76162&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e996feb76162--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe996feb76162&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e996feb76162---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e996feb76162--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e996feb76162--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e996feb76162--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e996feb76162--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e996feb76162--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e996feb76162--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e996feb76162--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e996feb76162--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kab.gaspard?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kab.gaspard?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kabalan Gaspard"}, {"url": "https://medium.com/@kab.gaspard/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ccd545c47ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&user=Kabalan+Gaspard&userId=7ccd545c47ce&source=post_page-7ccd545c47ce--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F322d1c6786ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-vs-puzzle-games-e996feb76162&newsletterV3=7ccd545c47ce&newsletterV3Id=322d1c6786ea&user=Kabalan+Gaspard&userId=7ccd545c47ce&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}