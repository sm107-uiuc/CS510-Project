{"url": "https://towardsdatascience.com/normalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e", "time": 1682994341.8016021, "path": "towardsdatascience.com/normalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e/", "webpage": {"metadata": {"title": "Normalization in Gradient`s Point of View [ Manual Back Prop in TF ] | by Jae Duk Seo | Towards Data Science", "h1": "Normalization in Gradient`s Point of View [ Manual Back Prop in TF ]", "description": "Normalization is a pre-processing technique in which, changes the property of a given distribution. Despite what the name suggests, batch normalization performs standardization, and it has been shown\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "Batch Normalization", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "Layer Normalization", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1607.08022.pdf", "anchor_text": "Instance Normalization", "paragraph_index": 4}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.boxcox1p.html#scipy.special.boxcox1p", "anchor_text": "Box-Cox Transformation", "paragraph_index": 4}, {"url": "https://cs.stanford.edu/~acoates/stl10/", "anchor_text": "STL 10 image data set", "paragraph_index": 9}, {"url": "https://colab.research.google.com/drive/1gDx0kwnbBFcdghA34h-sKUiPFZnXQGpA", "anchor_text": "click here", "paragraph_index": 42}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Normalization%20in%20Gradient%20Point%20of%20View/blog/a%20blog.ipynb", "anchor_text": "click here.", "paragraph_index": 42}, {"url": "https://jaedukseo.me/", "anchor_text": "website", "paragraph_index": 44}], "all_paragraphs": ["Normalization is a pre-processing technique in which, changes the property of a given distribution. Despite what the name suggests, batch normalization performs standardization, and it has been shown that normalization layers accelerate training of a deep neural network.", "But why is that? How does it change the gradients respect to each weight? Also what effects does other normalization schemes have? Such as layer normalization as well as instance normalization.", "Finally, if normalization aims to make the distribution more symmetrical (or normal), what would happen if we perform a simpler transformation such as box-cox transformation?", "Below is the list of methods that we are going to compare.", "a) Normal CNN (Base Line)b) Batch Normalizationc) Layer Normalizationd) Instance Normalizatione) Box-Cox Transformation", "In general, I understand normalization as a technique/process in which changes the properties of the given distribution. For example, the most basic usage of normalization would be limiting the range of the distribution into a certain range. In computer vision, we often see a lot of researchers normalizing pixel intensity between range 0 and 1.", "However, in deep learning, the exact usage of this term differs, specifically batch normalization performs standardization in which centers the distribution to zero and changes the standard deviation into one. This process is also known as the process of putting different data into the same scale.", "Despite their popular usage, I was not able to find what effect these normalizations have on the gradients, so I wanted to fix that.", "Blue Rectangle \u2192 Convolution Layer with ReLU ActivationYellow Rectangle \u2192 Normalization Layer, depending on the scheme this layer can change between batch/layer/instance, and box-cox transformation. In the case where we do not have any normalization scheme, this layer does not exist. Red Rectangle \u2192 Final output vector, softmax operation applied. Grey Sphere \u2192 input image dimensions of (20,96,96,3).", "Every single network is trained for 150 epoch, learning rate set to 0.0008, and mini-batch size of 20. Finally, I am going to use STL 10 image data set, without any data augmentation, please note that this means we have 5000 training images while testing accuracy is measured on 8000 testing images.", "Before showing the results, I want to cover how I placed results for each experiment. Every image contains six plots and each of the plots represent each layer in a sequential fashion. (in the sequence shown below.).", "So starting from the top, each box represents how the value in that layer changes over time. And I am going to display four of those images in the order written below.", "Top Left \u2192 Represents Gradient Respect to the Weight at each layerTop Right \u2192 Represents Gradient that gets passed along to previous layersBottom Left \u2192 Represents Weight at each layerBottom Right \u2192 Represents Subtraction between current weight and calculated Gradient", "Finally, after showing the gif I will display a static image of results from the final iteration.", "Top Left \u2192 Gradient Respect to the Weight at each layerTop Right \u2192 Gradient that gets passed along to previous layersBottom Left \u2192 Weight at each layerBottom Right \u2192 Subtraction between current weight and calculated Gradient", "When we focus our attention to top left gif, we can see that most of the gradient is centered at zero. This indicates that most of the weights are not being updated. The effect of having a lot of zero gradients can be seen in the gif located in the bottom left. Simply put after few iterations the weights are not changing much, especially layer two, three, and four.", "Top Left \u2192 Gradient Respect to the Weight at each layerTop Right \u2192 Gradient that gets passed along to previous layersBottom Left \u2192 Weight at each layerBottom Right \u2192 Subtraction between current weight and calculated Gradient", "Right away we can see one dramatic difference between the network that did not have any normalization scheme, Number of Non-Zero Gradients, for each layer.", "This property of batch normalization, which increases the number of non-zero gradients, is the reason why training accelerates for deep neural networks. And when we view how weights change over time, we can see that the histogram has more overall movement.", "Top Left \u2192 Gradient Respect to the Weight at each layerTop Right \u2192 Gradient that gets passed along to previous layersBottom Left \u2192 Weight at each layerBottom Right \u2192 Subtraction between current weight and calculated Gradient", "We can observe a similar phenomenon when we used layer normalization between every layer. As the number of non-zero gradients increases, the update of weights for each layer becomes more frequent.", "Top Left \u2192 Gradient Respect to the Weight at each layerTop Right \u2192 Gradient that gets passed along to previous layersBottom Left \u2192 Weight at each layerBottom Right \u2192 Subtraction between current weight and calculated Gradient", "Instance normalization standardizes every image or feature map, and due to this, I personally believe that the number of the non-zero gradient is maximized when compared to other normalization schemes.", "Top Left \u2192 Gradient Respect to the Weight at each layerTop Right \u2192 Gradient that gets passed along to previous layersBottom Left \u2192 Weight at each layerBottom Right \u2192 Subtraction between current weight and calculated Gradient", "When compared to the network that did not have any normalization scheme we can see that there are more non-zero elements in the gradient respect to each weight. However, when compared to any other normalization scheme we can see that we still have a lot of zeros in our gradient.", "One very important thing to remember is the fact that every single one of these networks (with/without) normalization schemes have exactly the same number of parameters.", "Meaning their learning capacity is exactly the same.", "The reason for this is because I did not add any alpha or beta parameter to batch/layer/instance normalization, so all of the data that gets passed through each layer has to be standardized. Knowing this let us see the accuracy plots.", "Orange \u2192 Batch NormalizationRed \u2192 Instance NormalizationGreen \u2192 Layer NormalizationPurple \u2192 Box-Cox Transformation Blue \u2192 No Normalization", "When we use a normalization scheme such as batch/layer/instance normalization we can achieve +95% accuracy on training images by 130th epoch. Meanwhile, the network with box-cox transformation as well as the network without any normalization scheme struggles even to pass +60% accuracy.", "Orange \u2192 Batch NormalizationRed \u2192 Instance NormalizationGreen \u2192 Layer NormalizationPurple \u2192 Box-Cox Transformation Blue \u2192 No Normalization", "From the above plot we can conclude that surprisingly, the network without any normalization scheme did the best. When taking into account that we have much more testing images compared to training images this is somewhat an impressive result. STL 10 dataset has 5000 training images and 8000 testing images, so 55 percent of 8000 images would mean 4400 images.", "Additionally, we can see a pattern emerging, as the number of parameters in which we calculate the mean and the standard deviation decreases, testing accuracy increases. More precisely, when we have a 4D tensor that has dimensions of (20,96,96,16), where each axis represents (batch size, width, height, channel), when we perform batch normalization we calculate the mean respect to the channel dimension so 16. Meanwhile, layer and instance normalization calculate the mean respect to batch size and both batch size and channel dimensions, respectively.", "Knowing all of the information above, we can see that the inverse pattern is clearly present. As the number of parameters, we are calculating the mean and standard deviation increases, the model tends to over-fit.", "Well, I am not an expert in machine learning, but clear speculation is the gradients. More precisely, non-zero gradients. In hindsight, this makes very clear sense, if the gradient respect to weights is composed with a lot of zero elements, then the weights are not going to change after the update operation.", "But I think, there is another reason, more mathematical one. (Note I am not an expert in math either lol.).", "Normalization schemes that puts a limit to the properties of a distribution simplify the problem by reducing the problem dimension space.", "What I mean by this, is essentially the same thing saying reducing internal covariate shift. In batch/layer/instance normalization we limit the mean and standard deviation to be zero and one. And the operations have a hard result, meaning the mean IS ZERO, and the standard deviation IS ONE.", "A softened version of this would be the box-cox transformation, where the operation aims to make a certain distribution more \u201cnormal\u201d or \u201csymmetrical\u201d.", "Both of these operations put certain restrictions on the distribution of feature map, making it easier for the network to do well on the training images. However, that does not mean that it will do well on testing images.", "Conclusion and Code for this Blog Post", "Normalization schemes accelerate training by increasing the frequency of non-zero gradients, however, in certain cases this limit in the change of distribution result in the network losing generalization power, leading to over-fitting.", "To access the code for this blog post in google collab please click here, to access the GitHub version please click here.", "In general, as the number of trainable parameters increases, it is more probable for the model to over-fit. (Without any regularization, techniques.). The future direction of this study can be furthered into two directions, 1) how will the network perform when we add the regularization parameter. 2) would transformation methods such as box-cox be more robust to adversary attacks.", "For more articles, please visit my website.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Exploring the intersection of AI, deep learning, and art. Passionate about pushing the boundaries of multi-media production and beyond. #AIArt"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2197dfa3497e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jdseo?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "Jae Duk Seo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70eb2d57a447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&user=Jae+Duk+Seo&userId=70eb2d57a447&source=post_page-70eb2d57a447----2197dfa3497e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2197dfa3497e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2197dfa3497e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://giphy.com/gifs/animation-cats-transformation-3oEjHIOJjuDBQkuwVy", "anchor_text": "website"}, {"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "Batch Normalization"}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "Layer Normalization"}, {"url": "https://arxiv.org/pdf/1607.08022.pdf", "anchor_text": "Instance Normalization"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.boxcox1p.html#scipy.special.boxcox1p", "anchor_text": "Box-Cox Transformation"}, {"url": "https://cs.stanford.edu/~acoates/stl10/", "anchor_text": "STL 10 image data set"}, {"url": "https://colab.research.google.com/drive/1gDx0kwnbBFcdghA34h-sKUiPFZnXQGpA", "anchor_text": "click here"}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Normalization%20in%20Gradient%20Point%20of%20View/blog/a%20blog.ipynb", "anchor_text": "click here."}, {"url": "https://jaedukseo.me/", "anchor_text": "website"}, {"url": "http://rohanvarma.me/", "anchor_text": "http://rohanvarma.me/"}, {"url": "https://github.com/rohan-varma/nn-init-demo/blob/master/batchnorm-demo.py", "anchor_text": "https://github.com/rohan-varma/nn-init-demo/blob/master/batchnorm-demo.py"}, {"url": "https://wiseodd.github.io/techblog/2016/07/04/batchnorm/", "anchor_text": "https://wiseodd.github.io/techblog/2016/07/04/batchnorm/"}, {"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "https://arxiv.org/pdf/1502.03167.pdf"}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "https://arxiv.org/abs/1607.06450"}, {"url": "https://arxiv.org/pdf/1607.08022.pdf", "anchor_text": "https://arxiv.org/pdf/1607.08022.pdf"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.boxcox1p.html#scipy.special.boxcox1p", "anchor_text": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.boxcox1p.html#scipy.special.boxcox1p"}, {"url": "https://cs.stanford.edu/~acoates/stl10/", "anchor_text": "https://cs.stanford.edu/~acoates/stl10/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2197dfa3497e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2197dfa3497e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----2197dfa3497e---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----2197dfa3497e---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/python?source=post_page-----2197dfa3497e---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2197dfa3497e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&user=Jae+Duk+Seo&userId=70eb2d57a447&source=-----2197dfa3497e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2197dfa3497e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&user=Jae+Duk+Seo&userId=70eb2d57a447&source=-----2197dfa3497e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2197dfa3497e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2197dfa3497e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2197dfa3497e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2197dfa3497e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2197dfa3497e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2197dfa3497e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jae Duk Seo"}, {"url": "https://medium.com/@jdseo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70eb2d57a447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&user=Jae+Duk+Seo&userId=70eb2d57a447&source=post_page-70eb2d57a447--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd9ea20dd433a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnormalization-in-gradient-s-point-of-view-manual-back-prop-in-tf-2197dfa3497e&newsletterV3=70eb2d57a447&newsletterV3Id=d9ea20dd433a&user=Jae+Duk+Seo&userId=70eb2d57a447&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}