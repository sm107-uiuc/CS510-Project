{"url": "https://towardsdatascience.com/oops-predicting-unintentional-action-in-video-87626aab3da3", "time": 1683009985.668276, "path": "towardsdatascience.com/oops-predicting-unintentional-action-in-video-87626aab3da3/", "webpage": {"metadata": {"title": "OOPS! Predicting Unintentional Action in Video | by Ching Lam Choi | Towards Data Science", "h1": "OOPS! Predicting Unintentional Action in Video", "description": "Realistically, humans are imperfect agents whose actions can be erratic and unpredictable. While prior research focuses largely on human activity recognition and prediction, Columbia University\u2026"}, "outgoing_paragraph_urls": [{"url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf", "anchor_text": "OOPS! Predicting unintentional action in video.", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1807.03748.pdf", "anchor_text": "arXiv preprint arXiv:1807.03748", "paragraph_index": 15}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Self-Supervised_Spatiotemporal_Learning_via_Video_Clip_Order_Prediction_CVPR_2019_paper.pdf", "anchor_text": "Self-supervised spatiotemporal learning via video clip order prediction.", "paragraph_index": 15}, {"url": "http://oops.cs.columbia.edu/", "anchor_text": "OOPS! Predicting Unintentional Action in Video.", "paragraph_index": 15}, {"url": "https://oops.cs.columbia.edu/", "anchor_text": "https://oops.cs.columbia.edu/", "paragraph_index": 15}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.pdf", "anchor_text": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?", "paragraph_index": 15}, {"url": "https://www.pnas.org/content/pnas/113/33/9250.full.pdf", "anchor_text": "Slow motion increases perceived intent.", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1602.02410.pdf", "anchor_text": "Exploring the limits of language modeling.", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1909.04656.pdf", "anchor_text": "Video representation learning by dense predictive coding.", "paragraph_index": 15}, {"url": "https://chinglamchoi.github.io/cchoi/", "anchor_text": "https://chinglamchoi.github.io/cchoi/", "paragraph_index": 17}], "all_paragraphs": ["Realistically, humans are imperfect agents whose actions can be erratic and unpredictable. While prior research focuses largely on human activity recognition and prediction, Columbia University researchers adopt a new approach \u2014 analysing goal-directed human action. Presented in CVPR 2020, Dave Epstein, Boyuan Chen, and Carl Vondrick make valuable contributions in \u201cOOPS! Predicting Unintentional Action in Video\u201d [1]:", "Numerous research has sought to model the physical and atomic consequences of human action, yet few try to understand the intentions behind motion. This paper distinguishes between intentional and unintentional motion, aiming to identify, localise and predict unintentional motions.", "The OOPS! Dataset [4] consists of 20,338 video clips (3\u201330 seconds long, totalling 50+ hours) from YouTube fail compilations \u2014 all verified to contain some unintentional, \u201cin-the-wild\u201d human action. As the authors propose a self-supervised approach to this task, the dataset is split into 3 subsets: 7,368 videos as the labelled training set, 6,739 labelled videos as the test set, and the rest being the unlabelled set for pre-training. For classification, actions in videos are labelled as \u201cintentional\u201d, \u201cunintentional\u201d or \u201ctransitional\u201d; for localisation, workers annotate timestamp markers at the temporal locations of failure (the moment when failure starts). Additional dataset annotations include optical flow and Natural Language descriptions. Within the dataset, 270 videos are designated as the diagnostic set, which see more fine-grained manual annotation. Such videos are categorised into 9 types of unintentional action: \u201cLimited skill\u201d, \u201cLimited knowledge\u201d, \u201cEnvironmental\u201d, \u201cUnexpected\u201d, \u201cLimited visibility\u201d, \u201cPlanning error\u201d, \u201cExecution error\u201d, \u201cSingle-agent\u201d, \u201cMulti-agent\u201d.", "The authors also report various dataset statistics, including the distribution of video clip lengths and failure time labels to illustrate the diversity of data; standard deviation of labels from different human annotators to demonstrate high human (annotator) agreement; distribution of action and scene categories (as predicted by their fully-supervised baseline).", "Self-Supervised Features from Mid-Level Perceptual Clues", "The authors investigate self-supervisory clues which are naturally present in videos (or requires minimal annotation), to learn deep, transferable representations of intentionality in human action. Specifically, video speed, video context and event order are examined. All ConvNets are implemented via the ResNet3D-18 model [5].", "Building on prior research [6], the authors point out that human judgement of intentionality is substantially affected by video speed. As video speed is intrinsic to each video, inference by speed requires minimal pre-processing. For training, the authors synthetically alter the speeds of videos, and train a self-supervised ConvNet to predict the true frame-rate. As noted by the authors, the features generated by this ConvNet are correlated to the expected duration of events (robustness derived from training on videos with synthetically altered speeds), and encode frame-by-frame motion information, thus constructing a useful representation of video speed information.", "Authors state that \u201cunintentional action is often a deviation from expectation\u201d, and explore the predictability of frames as a visual metric for intentionality. Guided by prior research [2], they consider frames x_{t-1} and x_{t+1} as surrounding video context, and incentivise the model to interpolate the feature map of the middle target frame x_t. Notably, they utilise concepts of noise-contrastive estimation [7] and contrastive predictive coding [2, 3, 8] to structure the objective function to be maximised:", "in order to maximise distance between target frame features and the contextual embedding, while minimising that of the target frame and non-contextual clip features.", "Authors present the rationale that \u201cunintentional motion often manifests as chaotic or irreversible motion\u201d, leading to a distinctive temporal event order. To generate a representation of event order [3], they permutate and shuffle subsampled clips from videos, and train a ConvNet to predict the applied permutation sequence. This is achieved via a 3-part model, consisting of a clip feature encoder, pairwise clip relation network (where features denote the similarity of clips), and event order predictor.", "Features extracted from the above self-supervised models are then used as input for a linear classifier, which performs 3-class classification with classes: \u201cintentional\u201d, \u201cunintentional\u201d, and \u201ctransitional\u201d motion.", "Performance is benchmarked on 3 tasks \u2014 classification, localisation (localising the temporal boundary of transition from intentional to unintentional motion), anticipation (predicting the onset of failure), and make 3 levels of comparison: Firstly, comparing between different self-supervisory incidental clues (video speed (newly proposed), video context, event order); secondly, comparing self-supervised models and fully-supervised baselines (pre-training on the Kinetics action recognition dataset plus fine-tuning, fine-grained annotations: motion magnitude, Scratch, Chance); lastly, comparing machine and human performance (human agreement).", "Across all 3 tasks, Kinetics supervision yields the best machine performances, while video-speed supervision consistently outperforms all other self-supervised and fully-supervised methods. The performance gap between fully-supervised and self-supervised methods is least in classification (Table 1), and greatest in temporal localisation (Table 2). To quantify localisation accuracy, predictions which overlap (results reported for within 1s and 0.25s) with any of the ground truth temporal locations (some videos have multiple ground truths) are considered correct.", "In particular, self-supervised models suffered more false positive boundary predictions than fully-supervised ones, where they conflate intentional motion with the start of failure (Fig. 2). Additionally, the authors conduct detailed error rate analysis (Fig. 3) for each of the 9 unintentional motion categories (in the aforementioned diagnostic set). They report that unintentional motion caused by unexpected (\u201csuch as a bird swooping in suddenly\u201d) or environmental (\u201csuch as slipping on ice\u201d) factors are the most difficult to detect, and hypothesise that multi-agent scenes record the lowest error rate due to their more obvious visual clues. Other challenges include limited video visibility (occluded objects) and limited knowledge (\u201csuch as understanding that fire is hot\u201d). As seen from the results, both self-supervised and fully-supervised methods lag significantly behind human performance.", "\u201cOOPS! Predicting Unintentional Action in Video\u201d introduces 3 new tasks for understanding intentionality in human actions, and presents a large benchmark dataset for future work. The authors propose a self-supervised approach, and report promising results using video speed as an incidental clue for video representation.", "[1] Dave Epstein, Boyuan Chen, and Carl Vondrick. OOPS! Predicting unintentional action in video. In CVPR, 2020.[2] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.[3] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In CVPR, 2019.[4] Dave Epstein, Boyuan Chen, and Carl Vondrick. OOPS! Predicting Unintentional Action in Video. Retrieved from https://oops.cs.columbia.edu/[5] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In CVPR, 2018.[6] Eugene M Caruso, Zachary C Burns, and Benjamin A Converse. Slow motion increases perceived intent. In PNAS, 2016.[7] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.[8] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCV Workshops, 2019.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "18 year old | Aspiring Computer Vision researcher | https://chinglamchoi.github.io/cchoi/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F87626aab3da3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----87626aab3da3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----87626aab3da3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@cchoi314?source=post_page-----87626aab3da3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cchoi314?source=post_page-----87626aab3da3--------------------------------", "anchor_text": "Ching Lam Choi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff717a4ab90e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&user=Ching+Lam+Choi&userId=f717a4ab90e8&source=post_page-f717a4ab90e8----87626aab3da3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F87626aab3da3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F87626aab3da3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf", "anchor_text": "OOPS! Predicting unintentional action in video."}, {"url": "https://arxiv.org/pdf/1807.03748.pdf", "anchor_text": "arXiv preprint arXiv:1807.03748"}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Self-Supervised_Spatiotemporal_Learning_via_Video_Clip_Order_Prediction_CVPR_2019_paper.pdf", "anchor_text": "Self-supervised spatiotemporal learning via video clip order prediction."}, {"url": "http://oops.cs.columbia.edu/", "anchor_text": "OOPS! Predicting Unintentional Action in Video."}, {"url": "https://oops.cs.columbia.edu/", "anchor_text": "https://oops.cs.columbia.edu/"}, {"url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.pdf", "anchor_text": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?"}, {"url": "https://www.pnas.org/content/pnas/113/33/9250.full.pdf", "anchor_text": "Slow motion increases perceived intent."}, {"url": "https://arxiv.org/pdf/1602.02410.pdf", "anchor_text": "Exploring the limits of language modeling."}, {"url": "https://arxiv.org/pdf/1909.04656.pdf", "anchor_text": "Video representation learning by dense predictive coding."}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----87626aab3da3---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----87626aab3da3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----87626aab3da3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----87626aab3da3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F87626aab3da3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&user=Ching+Lam+Choi&userId=f717a4ab90e8&source=-----87626aab3da3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F87626aab3da3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&user=Ching+Lam+Choi&userId=f717a4ab90e8&source=-----87626aab3da3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F87626aab3da3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----87626aab3da3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F87626aab3da3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----87626aab3da3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----87626aab3da3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----87626aab3da3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----87626aab3da3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----87626aab3da3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----87626aab3da3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----87626aab3da3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----87626aab3da3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----87626aab3da3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cchoi314?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cchoi314?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ching Lam Choi"}, {"url": "https://medium.com/@cchoi314/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://chinglamchoi.github.io/cchoi/", "anchor_text": "https://chinglamchoi.github.io/cchoi/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff717a4ab90e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&user=Ching+Lam+Choi&userId=f717a4ab90e8&source=post_page-f717a4ab90e8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3c8bfd6a3d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foops-predicting-unintentional-action-in-video-87626aab3da3&newsletterV3=f717a4ab90e8&newsletterV3Id=3c8bfd6a3d6d&user=Ching+Lam+Choi&userId=f717a4ab90e8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}