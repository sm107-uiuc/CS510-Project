{"url": "https://towardsdatascience.com/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184", "time": 1683002322.7454789, "path": "towardsdatascience.com/logistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184/", "webpage": {"metadata": {"title": "Logistic Regression for malignancy prediction in cancer | by Luca Zammataro | Towards Data Science", "h1": "Logistic Regression for malignancy prediction in cancer", "description": "In Linear Regression with one or more variables, we have introduced the concept of Linear Regression, a statistical model used in Machine Learning that belongs to the Supervised Learning class of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Linear Regression with one or more variables", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Linear Regression", "paragraph_index": 1}, {"url": "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29", "anchor_text": "\u201cBreast Cancer Wisconsin (Diagnostic)\u201d", "paragraph_index": 2}, {"url": "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)", "anchor_text": "UCI, Machine Learning Repository", "paragraph_index": 2}, {"url": "https://raw.githubusercontent.com/lucazammataro/bioinsteps/master/wdbc.data.csv", "anchor_text": "link", "paragraph_index": 11}, {"url": "https://jupyter.org/", "anchor_text": "Jupyter Notebook", "paragraph_index": 13}, {"url": "http://scipy.org/", "anchor_text": "SciPy.org", "paragraph_index": 14}, {"url": "http://seaborn.pydata.org/generated/seaborn.pairplot.html", "anchor_text": "seaborn", "paragraph_index": 14}, {"url": "https://pandas.pydata.org/", "anchor_text": "pandas", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/breast-cancer-cell-type-classifier-ace4e82f9a79", "anchor_text": "article", "paragraph_index": 18}, {"url": "https://datascience.stackexchange.com/a/10461", "anchor_text": "AN6U5", "paragraph_index": 22}, {"url": "https://rstudio-pubs-static.s3.amazonaws.com/344010_1f4d6691092d4544bfbddb092e7223d2.html", "anchor_text": "this post by Kalshtein Yael", "paragraph_index": 24}, {"url": "https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/", "anchor_text": "Chris Albon", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Linear Regression with one or more variables", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Equation 4 in the article dedicated to the Linear Regression", "paragraph_index": 39}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "post dedicated to Linear Regression", "paragraph_index": 49}, {"url": "https://www.coursera.org/learn/machine-learning/supplement/cmjIc/advanced-optimization", "anchor_text": "See the Scypy documentation page for more information", "paragraph_index": 67}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow,", "paragraph_index": 79}, {"url": "https://scikit-learn.org/stable/", "anchor_text": "Scikit-learn", "paragraph_index": 79}], "all_paragraphs": ["In Linear Regression with one or more variables, we have introduced the concept of Linear Regression, a statistical model used in Machine Learning that belongs to the Supervised Learning class of algorithms. Also, we have discussed the possible applications of the Linear Regression to biomedical data, in particular, in predicting continuous-valued outputs.", "In this article, which ideally represents a continuation of that dedicated to the Linear Regression, we will explain the Logistic Regression, another Supervised Learning model, used for predicting discrete-valued outputs.", "We will introduce the mathematical concepts underlying the Logistic Regression, and through Python, step by step, we will make a predictor for malignancy in breast cancer. We will use the \u201cBreast Cancer Wisconsin (Diagnostic)\u201d (WBCD) dataset, provided by the University of Wisconsin, and hosted by the UCI, Machine Learning Repository.", "Drs Mangasarian, Street, and Wolberg, the creators of the database, intended to utilize 30 characteristics of individual cells of breast cancer obtained from a minimally invasive fine needle aspirate (FNA), to discriminate benign from malignant lumps of a breast mass, using Machine Learning [1].", "Briefly, an FNA is a kind of biopsy, in which physicians use a needle attached to a syringe to withdraw a small number of cells from a suspicious area. The biopsy is then checked for cancer identification [2].", "Using an image analysis software called Xcyt based on a curve-fitting algorithm, the authors were able to determine the boundaries of the cell nuclei from a digitized 640\u00d7400, 8-bit-per-pixel grayscale image of the FNA.", "The 30 features describe characteristics of the cell nuclei present in the scanned images.", "As reported in their paper, the authors have used machine learning and image processing techniques to accomplish malignancy outcome prediction. Still, an in-depth on their computational approaches go beyond the aim of this article.", "In this post, we will not remake the work of Mangasarian, Street, and Wolberg. We will use their dataset to implement a Logistic Regression predictor based on some of the 30 features of the WBCD, in Python. We will use the outcome Bening/Malignant to predict if a new patient has a probability of developing malignancy or not, basing on the FNA data. Furthermore, our predictor will be an exciting occasion of exposing some basic concepts of Logistic Regression and implementing a code around the biomedical problem: what features are essential in predicting malignant outcomes.", "The first column of the dataset corresponds to the patient ID, while the last column represents the diagnosis (the outcome can be \u201cBenign\u201d or \u201cMalignant\u201d based on the type of diagnosis reported). The resulting dataset consists of 569 patients: 212 (37.2%) have an outcome of Malignancy, and 357 (62.7) are Benign. Figure 2 describes the dataset structure:", "In detail, the dataset consists of ten real-valued features computed for each cell nucleus. They are 1) Radius (mean of distances from center to points on the perimeter), 2) Texture (standard deviation of gray-scale values), 3) Perimeter, 4) Area, 5) Smoothness (local variation in radius lengths), 6) Compactness (perimeter\u00b2 / area \u2014 1.0), 7) Concavity (severity of concave portions of the contour), 8) Concave points (number of concave portions of the contour), 9) Symmetry and 10) Fractal Dimension (\u201ccoastline approximation\u201d \u2014 1). The ten real-valued features correspond to the Mean, (values from columns 2 to column 11), to the Standard Errors (values from columns 12 to 21), and the Worst or largest (mean of the three largest values), (columns from 22 to 31). Column 32 contains the Bening/Malignant outcome.", "For more simplicity, I have formatted the WBDC in a comma-separated-values file. You can download the formatted version following this link (filename: wdbc.data.csv).", "Before starting, I suggest readers following the interesting course in Machine Learning at Coursera by Andrew NG [3]. The course provides an excellent explanation of all the arguments treated in this post.", "All the code presented in this article is written in Python 2.7. For the implementation environment, I recommend the use of the Jupyter Notebook.", "Import all the packages required for the Python code of this article: Pandas, NumPy, matplotlib, and SciPy. These packages belong to SciPy.org, which is a Python-based ecosystem of open-source software for mathematics, science, and engineering. Also, we will import seaborn, which is a Python data visualization library based on matplotlib. Moreover, an object op from the scipy.optimize package will be created, to make optimizations of the Gradient.", "Once we have downloaded the wdbc.data.csv file, use it to create a DataFrame using pandas:", "With pandas we can visualize the first lines of a df content using the .head() method:", "Now that we have a DataFrame containing our data, we want to identify which of the 30 features are important for our prediction model. Selecting the best features of a dataset is a critical step in Machine Learning, to get a useful classification and to avoiding a predictive bias.", "One method is the Visualization Per Pairs. As suggested by Ahmed Qassim in his article, an elegant display of data is gettable using the pairplot set of functions provided by seaborn. The Python code that produces the complete features combination plot is the following:", "Here we have created the seaborn object sns; then, we have used the pandas DataFrame (df) produced in Step 2 as an argument for the sns pairplot method. Note that, specifying the argument \u201chue = Diagnosis,\u201d the pairplot method has access to the df column containing the Diagnosis values (0, 1). Moreover, pairplot will use the argument \u201cpalette\u201d to color points with b blue or r red basing on the Diagnosis values.", "For simplifying the visualization, the map resulting from Code 3 shows the combinations of the first five pairs of features. They represent mean values of parameters Radius, Texture, Perimeter, Area, and Smoothness. Depending on your computer capabilities, the production of the complete features map, running Code 3, could take time.", "Each tile represents a scatter plot of a couple of parameters. This visualization makes more accessible the identification of essential elements for the classification. Some of these pairs, like Radius vs. Texture or Perimeter vs. Smoothness, have a right separation level concerning the Diagnosis (blue points = Benign; red spots = Malignant). Visualizing features in plots of pairs represent an excellent tool, primarily because of its immediacy.", "Visualization apart, a more efficient method for selecting non-redundant features is the Bivariate Analysis, which is a method based on the correlation, that analyzes the relationship between pairs of elements. Here I propose a code that I have readapted from AN6U5, which implements a function for the Bivariate Analysis. This function, here called features_correlation_matrix, takes the WBCD df as an argument and returns a correlation matrix for the 30 features:", "Take a glance at the resulting plot in Figure 4: we aim to understand how the 30 characteristics of the dataset are related to each other. In calculating correlations between variables, we can observe that some of them result in being notably correlated (values greater than 0.9). The general assumption of the Bivariate Analysis is that features that are highly associated provide redundant information: for this reason, we want to eliminate them, avoiding a predictive bias.", "As suggested in this post by Kalshtein Yael, dedicated to the WBCD analysis with R, we should remove all features with a correlation higher than 0.9, keeping those with the lower mean. Code 5 is a readaptation of a Python code example, from the Chris Albon book [4]:", "The \u201cto_drop\u201d list contains ten redundant features with a correlation > 0.9. They are listed as follows:", "Now we want to eliminate these features from our DataFrame and re-plot the df with the remaining 20:", "Figure 5 shows the plot resulting from Code 6:", "We have now removed highly correlated feature pairs, so only features with low correlation, like Radius and Texture, will be kept. We can directly access to the correlation matrix values, typing in a new notebook cell:", "The resulting output is the following:", "Thanks to the Visualization Per Pairs and the Bivariate Analysis, now we know what features deserve to be considered in our analysis, then let\u2019s go ahead with the construction of our Logistic Regression outcome predictor. In the next Steps, we will write a code to predict the diagnosis outcome for a pair of features chosen on the base of their non-redundancy.", "Theoretically, for the Logistic Regression, we could use the same hypothesis model that we have used for the Linear Regression (see Linear Regression with one or more variables). Still, there are several reasons why we cannot use the linear hypothesis for the logistic, and now we are going to explain why. Take a glance at Figure 6:", "The plot in Figure 6A explains why we cannot apply the linear Hypothesis to binary classification. Imagine that we want to plot our samples with an outcome that can be Benign or Malignant (red circles). We could apply the Linear Hypothesis model to separate the samples into two distinct groups. In attempting to classifying binary values as 0 and 1, the Linear Regression tries to predict values greater than 0.5 as \u201c1\u201d and all less than 0.5 as \u201c0\u201d because the threshold classifier output for h\u03b8(x) is at 0.5.", "Theoretically, the Linear Regression model that we have chosen could work well also for the binary classification, as the blue line demonstrates. But let see what happens if we insert another sample with Malignant diagnosis (the green circle in Figure 6A). The Linear Regression Hypothesis adapts the line to include the new sample (the magenta line).", "Still, this Hypothesis model couldn\u2019t work correctly for all the new samples that we are going to upload, because the Linear Hypothesis seems not to add further information to our predictions. That happens because classification is not a linear function. What we need is a new Hypothesis h\u03b8(x) that can calculate the probability that the Diagnosis output can be 0 or 1: this new hypothesis model is the Logistic regression.", "The Logistic Regression Hypothesis model in Equation 1, looks similar to that of the Linear Regression. But the real difference is in the g function that uses the product of the translated \u03b8 vector with the x vector (we will call this product z) as an argument. The g function is defined as in Equation 2:", "The g(z) function, which is a sigmoid function (Logistic Function) is non-linear. It calculates the probability that the Diagnosis output can be 0 or 1(Figure 6B).", "The Python Code for the implementation of the Logistic Function is the following:", "This Step will answer to the question: How to choose parameters \u03b8 with Logistic Regression? Just like for the Linear Regression, the only way to select \u03b8 is by calculating its Cost Function, and then try to optimize the search using sophisticated algorithms for the Gradient descent optimization. (argument of Step 6).", "Again, we could use the same Cost Function that we used for Linear Regression, and it should look like the Cost Function of the Equation 4 in the article dedicated to the Linear Regression, except for a difference in the sigmoid function that characterizes the Hypothesis model h\u03b8(x).", "The y vector, as usual, represents the output with the difference that, here, y is a vector of binary outcomes (0/1, or Benign/Malignant), and not a vector of continuous-valued outputs:", "Concretely, we cannot use the Linear Regression Cost Function for the Logistic. The non-linearity of the sigmoid function, which handles h\u03b8(x), leads to a J(\u03b8) having a non-convex pattern, and it will look like the curve in the graph of Figure 7A:", "This non-convex J(\u03b8) is a function with many local optima. It\u2019s not guaranteed that the Gradient descent will converge to the global minimum. What we want is a convex J(\u03b8) like that in Figure 7B, which is a function that converges to the global minimum. So, we have to write the Cost Function in a way that guarantees a convex J(\u03b8):", "Implementing the Equation 4, the Cost Function that is calculated as log(z), looks like the red curve of Figure 8:", "Because we have two binary conditions for the Benign or Malignant outcome (y), the Cost Function in Equation 4 states the cost of our Hypothesis prediction concerning y is.", "If y = 1, but we predict h\u03b8(x) = 0, we will penalize the learning algorithm by a considerable cost (see the red curve in Figure 8) because, in this case, the cost will tend to infinite. Instead, if our prediction is h\u03b8(x) = 1, (thus equal to y), then the cost is going to be 0.", "In the case of y = 0, we have the opposite: if y = 0 and we predict h\u03b8(x) = 0, the cost is going to be 0, because our Hypothesis matches with y, while if our prediction is h\u03b8(x) = 1, we end up paying a very large cost.", "The simplified version of the Cost Function is the following:", "If y = 0, the Cost Function will be equal to -(log(1-h\u03b8(x)). If y = 1, the second part of Equation 5 will be 0, then the cost will be equal to -y * log(h\u03b8(x)). A Python implementation of the simplified and vectorized Cost Function is the following:", "In the post dedicated to Linear Regression, we have introduced the Gradient Descent, which is an algorithm that calculates the derivative of the Cost Function, updating the vector \u03b8 by mean of the parameter \u03b1, that is the learning rate.", "But in the Machine Learning scenario, the Gradient Descent is not the only algorithm for minimizing the Cost Function: Conjugate Gradient, BFGS, L-BGFS, TNC, for example, represent some of the more sophisticated algorithms for minimizing the Cost Function, in automatizing the search of \u03b8.", "Based on different statistics, these algorithms try to optimize the Gradient Function, which is the difference between the actual vector y of the dataset, and the h vector (the prediction), to learn how to find the minimum J. Generalizing, an optimization algorithm will repeat until it will converge. Importantly: the updating of \u03b8 always has to be simultaneous.", "Optimization algorithms are arguments of the Numerical Optimization field, and an in-depth study on the use of algorithms goes beyond this article. The Gradient function that we are going to implement looks identical to that used for the Linear Regression. But also here, as for the Cost Function, the difference is in the definition of h\u03b8(x) that needs the sigmoid function:", "The equation for the vectorized implementation of the Gradient Function is:", "and the Python code for calculating the Gradient is the following:", "In Step 9, we will see how to optimize the Gradient Function, using one of the algorithms provided by scipy.optimize.", "We could need to rescale and normalized our data. The Python code for calculating Feature Scaling and Normalization for two variables is the following:", "This function calculates the accuracy of our algorithm:", "In the previous steps, we have created all the fundamental functions for the implementation of the Logistic Regression. Let\u2019s briefly summarize all of them:", "Now we will write the code that wraps all of these functions, to predict outcomes of Malignancy based on two of the 20 not-redundant features of our dataset. For the choice of the features, we could refer to one of the pairs that we have found in Step 3, Radius, and Texture, which have a correlation score = 0.32. The following code produces the features numpy array X and the output numpy vector y, from the DataFrame df:", "In Figure 7, the resulting plot is shown:", "The yellow dots represent the Benigns, the blacks, the Malignants.", "Now let\u2019s Normalize and Scale, our data. Also, we need collecting the mu, which is the average values of X in our training set, and sigma that is the Standard Deviation. In a new notebook cell, let\u2019s type:", "Now, we have to update the array X adding a column of \u201cones,\u201d with the method .vstack:", "Let\u2019s do some tests: in order to test our code, let\u2019s try to calculate the Cost Function and the Gradient, starting with a \u03b8 = [0, 0, 0]:", "Also, we can try non-zero values of \u03b8, and see what happens:", "The optimization algorithm that we will use for finding \u03b8 is the BFGS, which is based on the quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno [5]. Code 19 will implement the function Scypy minimize, which internally will call the BFGS method:", "If we don\u2019t specify the type of Method we want to use in the argument \u201cmethod\u201d of the .minimize function, the BFGS algorithm is used as default. Another method, i.e. TNC, uses a truncated Newton algorithm for minimizing a function with variables subject to bounds. Users can experiment with trying the various types of optimizing algorithms available with the .minimize function in Scypy. See the Scypy documentation page for more information about all the optimizing methods, with the function .minimize. The output produced by Code 18, is the following:", "The BFGS algorithm has found \u03b8 = [-0.70755981, 3.72528774, 0.93824469], corresponding to a vector containing the Result.x argument, of the scypy.minimize function. In Step 3, we mentioned that, for Logistic Regression, the Hypothesis h\u03b8(x) calculates the probability that the output can be 0 or 1. To map this probability to a discrete class (Bening/Malignant), we select a threshold that is 0.5 above which we will classify values as \u201c1\u201d and below which we classify values as \u201c0\u201d. So, what we want to do is tracking the so-called Decision Boundary. A Decision Boundary is a property of the Hypothesis and its parameters \u03b8, and not a property of the dataset. Let\u2019s plot the Radius and Texture features again, but with the adding of a red line, that represents the Decision Boundary for the found \u03b8:", "I think that it\u2019s important to highlight that, surprisingly, in spite of Logistic Regression uses a non-linear (sigmoid) function into its Hypothesis model, the Decision Boundary is linear!", "Now we want to calculate the accuracy of our algorithm. The function CalcAccuracy described in Step 8 will do this job:", "The output of CalculateAccuracy is 89.1, that is a good accuracy score!", "Now that we have tested our algorithm and evaluated its accuracy, we want to make predictions. The following code represents a possible example of a Query: we want to know what is the outcome for a Radius = 18.00 and a Texture = 10.12:", "Note that we have to normalize the Query with mu and sigma for the Scaling and Normalization. The outcome predicted is 0.79, which means that for a Radius = 18 and a Texture =10.12, the risk of malignancy is nearby to 1.", "In this step, we will modify the previous code for the handling of multiple variables. There are few things to update: the first concerns the Feature Scaling and Normalization function:", "Other minor revisions concern the code for uploading the X vector and the code for the Query. The following code reassembles what we have done until now, extending all the functions to the use of the Logistic Regression with multiple variables. Copy and paste the following code in a new Jupyter Notebook cell:", "Code 24 will predict the risk of malignancy for: Radius = 5.00, Texture = 1.10, and W_Concave_points = 0.4. After all the calculations it will produce the following output:", "The prediction of malignancy for these input values is nearby 1. Modifying the Query, you can experiment by yourself how the probability changes.", "Logistic Regression is a powerful Machine Learning tool, and we can use it successfully for predicting categorical outputs of biomedical data. Data wrangling and data mining can benefit from excellent performances offered by Python and its libraries so well supported by the community. Linear Algebra programming has intrinsic advantages in avoiding, where possible, \u2018while\u2019 and \u2018for\u2019 loops. It is implementable by numpy, a package that vectorizes the matrixes. Numpy makes working on them more comfortable, and guarantees better control over the operations, especially for large arrays.", "Moreover, the Machine Learning scenario with Python is enriched by the presence of many powerful packages (i.e., Tensorflow, Scikit-learn, and other that, for technical reasons, we haven\u2019t mentioned in this post), which provide excellently optimized classifications and predictions on data.", "While Machine Learning, equipped with Python and all its accessories, can represent the path toward the future of preventive and diagnostic Medicine, limitations in comprehending biological variables could make this path an awkward road.", "The Wisconsin Breast Cancer (Diagnostic) Data Set, with its 569 patients and 30 features, offers an exhaustive assortment of parameters for classification and for this reason represents a perfect example for Machine Learning applications. Anyway, many of these features seem to be redundant, and a definite impact on classification and prediction by some of them remains still unknown.", "We have introduced a Bivariate Analysis in Step 3, to reduce the number of redundant features. An in-depth discussion on the role of these features in the prediction should deserve a dedicated article.", "I hope you find this post useful!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a data science passionate. My interests mainly encompass Machine Learning and Bioinformatics applications for Immuno-oncology and immunoinformatics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F27b1a1960184&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----27b1a1960184--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----27b1a1960184--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://luca-zammataro.medium.com/?source=post_page-----27b1a1960184--------------------------------", "anchor_text": ""}, {"url": "https://luca-zammataro.medium.com/?source=post_page-----27b1a1960184--------------------------------", "anchor_text": "Luca Zammataro"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ac72d0e7a58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&user=Luca+Zammataro&userId=5ac72d0e7a58&source=post_page-5ac72d0e7a58----27b1a1960184---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27b1a1960184&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27b1a1960184&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/ml-for-bio-data", "anchor_text": "MACHINE LEARNING FOR BIOMEDICAL DATA"}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Linear Regression with one or more variables"}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Linear Regression"}, {"url": "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29", "anchor_text": "\u201cBreast Cancer Wisconsin (Diagnostic)\u201d"}, {"url": "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)", "anchor_text": "UCI, Machine Learning Repository"}, {"url": "https://raw.githubusercontent.com/lucazammataro/bioinsteps/master/wdbc.data.csv", "anchor_text": "link"}, {"url": "https://jupyter.org/", "anchor_text": "Jupyter Notebook"}, {"url": "http://scipy.org/", "anchor_text": "SciPy.org"}, {"url": "http://seaborn.pydata.org/generated/seaborn.pairplot.html", "anchor_text": "seaborn"}, {"url": "https://pandas.pydata.org/", "anchor_text": "pandas"}, {"url": "https://towardsdatascience.com/breast-cancer-cell-type-classifier-ace4e82f9a79", "anchor_text": "article"}, {"url": "https://datascience.stackexchange.com/a/10461", "anchor_text": "AN6U5"}, {"url": "https://rstudio-pubs-static.s3.amazonaws.com/344010_1f4d6691092d4544bfbddb092e7223d2.html", "anchor_text": "this post by Kalshtein Yael"}, {"url": "https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/", "anchor_text": "Chris Albon"}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Linear Regression with one or more variables"}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "Equation 4 in the article dedicated to the Linear Regression"}, {"url": "https://towardsdatascience.com/machine-learning-for-biomedical-data-linear-regression-7d43461cdfa9", "anchor_text": "post dedicated to Linear Regression"}, {"url": "https://www.coursera.org/learn/machine-learning/supplement/cmjIc/advanced-optimization", "anchor_text": "See the Scypy documentation page for more information"}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow,"}, {"url": "https://scikit-learn.org/stable/", "anchor_text": "Scikit-learn"}, {"url": "https://pubsonline.informs.org/doi/pdf/10.1287/opre.43.4.570", "anchor_text": "10.1287/opre.43.4.570."}, {"url": "https://www.cancer.org/cancer/breast-cancer/screening-tests-and-early-detection/breast-biopsy/fine-needle-aspiration-biopsy-of-the-breast.html", "anchor_text": "https://www.cancer.org/cancer/breast-cancer/screening-tests-and-early-detection/breast-biopsy/fine-needle-aspiration-biopsy-of-the-breast.html#references"}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "Andrew NG, Machine Learning | Coursera."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----27b1a1960184---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/wisconsin-breast-cancer?source=post_page-----27b1a1960184---------------wisconsin_breast_cancer-----------------", "anchor_text": "Wisconsin Breast Cancer"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----27b1a1960184---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/python?source=post_page-----27b1a1960184---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/ml-for-bio-data?source=post_page-----27b1a1960184---------------ml_for_bio_data-----------------", "anchor_text": "Ml For Bio Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27b1a1960184&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&user=Luca+Zammataro&userId=5ac72d0e7a58&source=-----27b1a1960184---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27b1a1960184&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&user=Luca+Zammataro&userId=5ac72d0e7a58&source=-----27b1a1960184---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27b1a1960184&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----27b1a1960184--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F27b1a1960184&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----27b1a1960184---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----27b1a1960184--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----27b1a1960184--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----27b1a1960184--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----27b1a1960184--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----27b1a1960184--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----27b1a1960184--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----27b1a1960184--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----27b1a1960184--------------------------------", "anchor_text": ""}, {"url": "https://luca-zammataro.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://luca-zammataro.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Luca Zammataro"}, {"url": "https://luca-zammataro.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "100 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ac72d0e7a58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&user=Luca+Zammataro&userId=5ac72d0e7a58&source=post_page-5ac72d0e7a58--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6cc40fdafc55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-for-malignancy-prediction-in-cancer-27b1a1960184&newsletterV3=5ac72d0e7a58&newsletterV3Id=6cc40fdafc55&user=Luca+Zammataro&userId=5ac72d0e7a58&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}