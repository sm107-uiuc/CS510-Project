{"url": "https://towardsdatascience.com/is-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98", "time": 1683006457.3092952, "path": "towardsdatascience.com/is-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98/", "webpage": {"metadata": {"title": "Is Upside-Down Reinforcement Learning = Imitation Learning? | by Bharat Prabhakar | Towards Data Science", "h1": "Is Upside-Down Reinforcement Learning = Imitation Learning?", "description": "Traditional Reinforcement Learning (RL) algorithms either predict rewards with value functions or maximize them using policy search. We study an alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that solves RL problems primarily using supervised learning techniques."}, "outgoing_paragraph_urls": [{"url": "https://bprabhakar.github.io/2020/02/05/neurips.html", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1912.02877", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=RrvC8YW0pT0", "anchor_text": "video", "paragraph_index": 2}, {"url": "https://github.com/bprabhakar/upside-down-reinforcement-learning", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1806.05635", "anchor_text": "Self-Imitation Learning", "paragraph_index": 12}, {"url": "https://github.com/bprabhakar/upside-down-reinforcement-learning", "anchor_text": "Github Repo", "paragraph_index": 15}, {"url": "https://bprabhakar.github.io/2020/04/18/udrl.html", "anchor_text": "bprabhakar.github.io", "paragraph_index": 16}, {"url": "http://mustard.dev", "anchor_text": "mustard.dev", "paragraph_index": 18}], "all_paragraphs": ["I found out about this work while attending an RL workshop at NeurIPS 2019. Honestly, it was one of the coolest ideas I stumbled upon at the conference. You can take a look at my other favourite ideas from the conference here. Anyway, this post is about inspecting upside-down reinforcement learning more. You can read the complete paper here, but here\u2019s what the abstract says:", "Traditional Reinforcement Learning (RL) algorithms either predict rewards with value functions or maximize them using policy search. We study an alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that solves RL problems primarily using supervised learning techniques. Here we present the first concrete implementation of UDRL and demonstrate its feasibility on certain episodic learning problems. Experimental results show that its performance can be surprisingly competitive with, and even exceed that of traditional baseline algorithms developed over decades of research.", "If you want to deep dive into the paper and understand it more, you can watch this excellent video. But tl;dr \u2014 they\u2019ve devised a new supervised learning algorithm to solve reinforcement learning tasks. No policy gradients, no value function estimations, just plain old supervised learning. Here\u2019s a figure from the paper to illustrate this better:", "What the behavior function is trying to predict is \u2014 given this observation, what is the best action to take to achieve this desired return (total reward) in this desired horizon (total timesteps). Instead of learning/modeling rewards they\u2019re used as inputs (commands) to directly predict actions.", "To understand the overall algorithm better, I ran a few quick experiments. This post documents my findings.", "In particular, I wanted to answer the following two questions:", "To answer the above questions, I implemented the algorithm to solve the Sparse Lunar Lander task (one of the tasks mentioned in the paper). The task is to learn an agent that is able to successfully land a lunar lover as shown below:", "I manually converted the \u201cLunarLander-v2\u201d environment from OpenAI Gym to a sparse one by modifying the reward function as follows:", "Code for this experiment can be accessed here. Now, over to the experiment findings.", "Answer \u2014 surprisingly quick. I was able to get the following reward curve in just my third(!) run attempt; without needing to tinker around a lot with the hyper-parameters. I don\u2019t remember the last time that happened for any of the policy gradient based algorithms.", "Of course, this needs to be taken with a grain of salt because \u201cLunar Lander\u201d is a relatively simple task. But if you look at the implemented algorithm in code, you\u2019ll agree that the algorithm is ridiculously simple!", "My first impression on reading the paper was that the idea sounds very similar to Imitation Learning. Consider the following excerpt from the paper describing the replay buffer strategy:", "So as training progresses, by design the trajectories stored in the buffer start looking more and more like expert trajectories (episodes with high returns). And learning just a mapping from states to actions on these expert trajectories (re: Imitation Learning) should suffice. This idea is actually better expressed in this prior work called \u2014 Self-Imitation Learning.", "To test this hypothesis, I ran the following experiment \u2014 trying to learn the optimal actions to take by masking out the command inputs of the behavior function (keeping everything else identical). So effectively, just learning to predict actions from just the observations alone. And here\u2019s what happened:", "Clearly, having commands as inputs to the behavior function make a difference. My guess is that having these commands help the agent further distinguish different types of high return trajectories, in turn helping it learn faster. This obviously needs to be further tested on more complex environments, but the results from my micro-experiment are definitely encouraging.", "All the code for the experiments can be accessed at this Github Repo", "This post was originally posted at bprabhakar.github.io", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building mustard.dev \u2014 Google like search for e-commerce"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4a9d346f9f98&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aglooka?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aglooka?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "Bharat Prabhakar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbda0614d3a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&user=Bharat+Prabhakar&userId=fbda0614d3a2&source=post_page-fbda0614d3a2----4a9d346f9f98---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a9d346f9f98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a9d346f9f98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://bprabhakar.github.io/2020/02/05/neurips.html", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1912.02877", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=RrvC8YW0pT0", "anchor_text": "video"}, {"url": "http://comet.ml/", "anchor_text": "comet.ml"}, {"url": "https://github.com/bprabhakar/upside-down-reinforcement-learning", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1806.05635", "anchor_text": "Self-Imitation Learning"}, {"url": "https://github.com/bprabhakar/upside-down-reinforcement-learning", "anchor_text": "Github Repo"}, {"url": "https://bprabhakar.github.io/2020/04/18/udrl.html", "anchor_text": "bprabhakar.github.io"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4a9d346f9f98---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4a9d346f9f98---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4a9d346f9f98---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4a9d346f9f98---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----4a9d346f9f98---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a9d346f9f98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&user=Bharat+Prabhakar&userId=fbda0614d3a2&source=-----4a9d346f9f98---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a9d346f9f98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&user=Bharat+Prabhakar&userId=fbda0614d3a2&source=-----4a9d346f9f98---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a9d346f9f98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4a9d346f9f98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4a9d346f9f98---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4a9d346f9f98--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aglooka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aglooka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bharat Prabhakar"}, {"url": "https://medium.com/@aglooka/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "http://mustard.dev", "anchor_text": "mustard.dev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbda0614d3a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&user=Bharat+Prabhakar&userId=fbda0614d3a2&source=post_page-fbda0614d3a2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffbda0614d3a2%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-upside-down-reinforcement-learning-imitation-learning-4a9d346f9f98&user=Bharat+Prabhakar&userId=fbda0614d3a2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}