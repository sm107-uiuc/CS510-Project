{"url": "https://towardsdatascience.com/sequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe", "time": 1683013883.7592099, "path": "towardsdatascience.com/sequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe/", "webpage": {"metadata": {"title": "Sequence-to-Sequence Models: Attention Network using Tensorflow 2 | by Nahid Alam | Towards Data Science", "h1": "Sequence-to-Sequence Models: Attention Network using Tensorflow 2", "description": "In part 1 of this series of tutorials, we discussed sequence-to-sequence models with a simple encoder-decoder network. The simple network was easier to understand but it comes with its limitation. If\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639", "anchor_text": "part 1", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639", "anchor_text": "part-1", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639", "anchor_text": "part-1", "paragraph_index": 14}, {"url": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://twitter.com/nahidalam", "anchor_text": "https://twitter.com/nahidalam", "paragraph_index": 16}], "all_paragraphs": ["In part 1 of this series of tutorials, we discussed sequence-to-sequence models with a simple encoder-decoder network. The simple network was easier to understand but it comes with its limitation.", "If you remember from part \u2014 1, the decoder decodes only based on the last hidden output of the encoder. This means, for the vanilla encoder-decoder network to work well, the encoder needs to ensure it encodes all the necessary information in that last hidden state output. This works well for a short sequence but performs poorly for long sequences.", "That is why attention is a key concept in sequence-to-sequence models [1].", "The goal of the attention mechanism is to provide contextual information to the decoder so that it can decode with higher accuracy. Rather than relying on a single context vector out of the encoder\u2019s last hidden state, the attention network represents a relation between the context vector and the entire input sequence.", "There are primarily four different approaches in calculating attention score\u2014 additive (Bahdanau\u2019s attention) and multiplicative (Luong\u2019s attention), self-attention, and key-value attention. Here we will focus on Bahdanua\u2019s attention.", "Bahdanau et al. [1] proposed the original attention mechanism that computes the attention alignment score using a one-hidden layer feed-forward network [2]", "Here, v and W are learned-parameters of the attention network. W\u2081 and W\u2082 are separate matrices that learn the transformation of the current hidden state h and the encoder output s respectively.", "Do not worry if you are a bit confused. We will write up a method for Bahdanau's attention that will make things clearer.", "In the BahdanauAttention initializer, you will observe that we are initializing three Dense layers \u2014 W1, W2, and V. As shown in Figure -3, these Dense layers will be used to calculate the score in the forward propagation \u2014 a.k.a. inside the call method.", "What do we pass to the W1 and W2 layers? W1 and W2 takes the current hidden state and encoder output as an input respectively. Inside the call method \u2014 hidden state and encoder outputs are represented by query and value respectively. You will have more clarity on score calculation once we use BahdanauAttention from a Decoder network.", "Next, we compute attention_weights as we pass the score through a softmax layer. As softmax returns the probability distribution over target classes in a multiclass classification problem [5], the attention_weights essentially represents which word the decoder paid attention to in the decoding process.", "Finally, we compute context_vector which will be used by the decoder to predict the most likely output.", "The Encoder class is the same as described in part-1. The Decoder class is also pretty similar except the addition of the attention mechanism as shown below.", "You will notice that at line #19 we are initializing the attention layer. During forward propagation in thecall method, we instantiate the attention layer with current hidden state and encoder output which translates to the query and value inside the BahdanauAttention class. The attention layer outputs context_vector and attention_weights (line # 20). The context_vector is concatenated with the decoder input x (line # 26). This concatenated result is then passed through the gru unit and a fully connected layer (line # 35). The Decoder class here also outputs the attention_weights which you can later use to visualize where the decoder paid attention to [3]", "The end-to-end workflow is the same as we described in part-1 \u2014 data cleaning, defining the Encoder and Decoder class, training the model, inference, and evaluation. For a working code, please refer to the TensorFlow example code here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Working on Computer Vision at Meraki Camera Intelligence https://twitter.com/nahidalam"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd900cc127bbe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nahidalam?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nahidalam?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "Nahid Alam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9c3cc6db069e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&user=Nahid+Alam&userId=9c3cc6db069e&source=post_page-9c3cc6db069e----d900cc127bbe---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd900cc127bbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd900cc127bbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639", "anchor_text": "part 1"}, {"url": "https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639", "anchor_text": "part-1"}, {"url": "https://towardsdatascience.com/sequence-to-sequence-models-from-rnn-to-transformers-e24097069639", "anchor_text": "part-1"}, {"url": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "https://arxiv.org/abs/1409.0473"}, {"url": "https://ruder.io/deep-learning-nlp-best-practices/index.html#attention", "anchor_text": "https://ruder.io/deep-learning-nlp-best-practices/index.html#attention"}, {"url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention", "anchor_text": "https://www.tensorflow.org/tutorials/text/nmt_with_attention"}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"}, {"url": "https://www.machinecurve.com/index.php/2020/01/08/how-does-the-softmax-activation-function-work/", "anchor_text": "https://www.machinecurve.com/index.php/2020/01/08/how-does-the-softmax-activation-function-work/"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d900cc127bbe---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----d900cc127bbe---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----d900cc127bbe---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d900cc127bbe---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d900cc127bbe---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd900cc127bbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&user=Nahid+Alam&userId=9c3cc6db069e&source=-----d900cc127bbe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd900cc127bbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&user=Nahid+Alam&userId=9c3cc6db069e&source=-----d900cc127bbe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd900cc127bbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd900cc127bbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d900cc127bbe---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d900cc127bbe--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d900cc127bbe--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d900cc127bbe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nahidalam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nahidalam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nahid Alam"}, {"url": "https://medium.com/@nahidalam/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "503 Followers"}, {"url": "https://twitter.com/nahidalam", "anchor_text": "https://twitter.com/nahidalam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9c3cc6db069e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&user=Nahid+Alam&userId=9c3cc6db069e&source=post_page-9c3cc6db069e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8381e62a44b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-to-sequence-models-attention-network-using-tensorflow-2-d900cc127bbe&newsletterV3=9c3cc6db069e&newsletterV3Id=8381e62a44b0&user=Nahid+Alam&userId=9c3cc6db069e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}