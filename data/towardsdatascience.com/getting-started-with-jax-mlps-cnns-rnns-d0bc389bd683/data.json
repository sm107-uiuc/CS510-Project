{"url": "https://towardsdatascience.com/getting-started-with-jax-mlps-cnns-rnns-d0bc389bd683", "time": 1683018274.723752, "path": "towardsdatascience.com/getting-started-with-jax-mlps-cnns-rnns-d0bc389bd683/", "webpage": {"metadata": {"title": "Getting started with JAX. A Guided Tour Through\u2026 | by Robert Lange | Towards Data Science", "h1": "Getting started with JAX", "description": "JAX, Jax, JaX. Twitter seems to know nothing else nowadays (next to COVID-19). If you are like me and want to know what the newest hypetrain is about \u2014 welcome to todays blog post \ud83e\udd17! I will walk you\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/RobertTLange/code-and-blog/blob/master/04_jax_intro/jax_workspace.ipynb", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://www.tensorflow.org/xla", "anchor_text": "XLA", "paragraph_index": 1}, {"url": "https://github.com/hips/autograd", "anchor_text": "Autograd", "paragraph_index": 1}, {"url": "https://roberttlange.github.io/posts/2019/08/blog-post-6/", "anchor_text": "Forward and backward mode", "paragraph_index": 3}, {"url": "https://github.com/deepmind/rlax", "anchor_text": "rlax", "paragraph_index": 4}, {"url": "https://github.com/deepmind/dm-haiku", "anchor_text": "haiku", "paragraph_index": 4}, {"url": "https://github.com/ethanluoyc/dhmc-jax", "anchor_text": "Hamiltonian Monte-Carlo", "paragraph_index": 4}, {"url": "https://jax.readthedocs.io/en/latest/jax.numpy.html", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://github.com/google/jax/pull/2298", "anchor_text": "open pull request", "paragraph_index": 26}, {"url": "https://de.wikipedia.org/wiki/Ornstein-Uhlenbeck-Prozess", "anchor_text": "Ornstein-Uhlenbeck (OU) process", "paragraph_index": 26}, {"url": "https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html", "anchor_text": "ReadTheDocs", "paragraph_index": 31}, {"url": "https://medium.com/@RobertTLange/membership", "anchor_text": "https://medium.com/@RobertTLange/membership", "paragraph_index": 40}], "all_paragraphs": ["JAX, Jax, JaX. Twitter seems to know nothing else nowadays (next to COVID-19). If you are like me and want to know what the newest hypetrain is about \u2014 welcome to todays blog post \ud83e\udd17! I will walk you through some exciting CS concepts which were new to me (I am not a computer engineer, so this will be an educational experience for you and me). Along the process, we will go through the individual building blocks of JAX and use them to build some standard Deep Learning architectures (Multilayer Perceptron, Convolutional Neural Nets and a Gated Recurrent Unit RNN) from scratch. In the process we will encounter the basic operators of JAX (jit, vmap, grad), dive deeper into stax - the sequential layer API of JAX - and use lax.scan to quickly compile the for-loop of an RNN. This should equip you with the basics to start speeding up & optimizing your favorite projects in JAX. Without further ado - here we go! [You can also download the blog notebook here.]", "JAX is an automatic differentiation (AD) toolbox developed by a group of people at Google Brain and the open source community. It aims to bring differentiable programming in NumPy-style onto TPUs. On the highest level JAX combines the previous projects XLA & Autograd to accelerate your favourite linear algebra-based projects.", "Python as an interpreted programming language is slow by nature. It translates one program statement to machine code at a time and computations may get stuck in the global interpreter lock (GIL). So in order to train networks at scale we need fast compilation and parallel computing! Complied CUDA kernels for example provide a set of primitive instructions which can be executed massively parallel on a NVIDIA GPU. The computation graph generated by PyTorch or TensorFlow can then be compiled into a sequence of executions (basic operations, e.g. add/multiply) with precompiled kernels. Ideally, we want to launch as few kernels as possible because this reduces communication times and memory load. And this is where XLA comes in. It optimizes memory bandwidth by \u201cfusing\u201d operations and reduces the amount of returned intermediate computations. In practice this can help to significantly speed up things.", "Autograd, on the other hand, provides automatic differentiation support for large parts of standard Python features. AD resembles the backbone of optimization in Deep Learning. It simplifies the derivative expression of a compositional function at every possible point in time. For a vast set of basic math operations we already know the functional form of their derivative. By the beauty of the chain rule, we can combine these elementary derivative and reduce the complexity of the expression at the cost of memory storage. This allows us to compute gradients which we can then use to optimize the parameters of our models using our favourite gradient-based optimization algorithm. Broadly speaking there are two types of automatic differentiation: Forward and backward mode (aka backpropagation). JAX supports AD for standard NumPy functions as well as loops which transform numerical variables.", "In principle these ingredients make JAX\u2019s applicability a lot broader then Deep Learning and provide another step into the era of \u201cCode 2.0\u201d and differentiable programming. Many recent projects focus on DL applications (such as rlax, or haiku \u2014 two of DeepMind\u2019s recent open source releases) but there are also other examples which benefit from both Numba-like speed-ups with some gradient-sauce on top (e.g. Hamiltonian Monte-Carlo). I personally, often times find myself working on projects in which I use Numba to generate synthetic data and PyTorch to train networks. JAX allows me to write all within a single framework. Furthermore, getting started in JAX comes very natural because many people deal with NumPy syntax/conventions on a daily basis. So let\u2019s get started by importing the basic JAX ingredients we will need in this Tutorial.", "We simply import the JAX version of NumPy as well as the good old vanilla version. Most of the standard NumPy functions are supported (see here for an overview) by JAX and can be called in the standard fashion. JAX automatically detects whether you have access to a GPU or TPU.", "And here is also the first difference to classic NumPy. We generate random numbers using JAX\u2019s random library and a previously generated random key. Unlike NumPy JAX uses an explicit pseudorandom number generator (PRNG). What does this mean? In order to parallelize random computations across resources, one needs to be able to fork a random number generators state. This can only be done by explicitly passing and iterating of the generators state. We will later when we generate parameters for our neural nets see how this comes in handy. So let\u2019s generate a random matrix and perform a simple matrix-matrix multiplication:", "Tada \ud83e\udd73 \u2014 even simple matrix multiplication can be speed up quite a bit.", "On a different note: Often times people post their crazy speed-ups when using JAX. Lazy Execution refers to an evaluation strategy that performs computation only when truly needed (e.g. when printing). Hence, it can make some wall time numbers deceiving. This is due to fact that is does not return the actual result of the computation. Instead the measured time only reflects the dispatching! Blocking the computation (via .block_until_ready()) enforces the actual computation and prevents JAX to simply run forward without evaluation. Only this way the time estimates provide a realistic measure to compare performance.", "Before diving into the nitty-gritty details of training some neural nets in JAX, let\u2019s have a look at the basic ingredients that make things work. jit (just-in-time compilation) lies at the core of speeding up your code. In practice we simply wrap (jit()) or decorate (@jit) the function of interest.", "The first time we call the jitted function it will be compiled and it may take a little longer. But afterwards, the machine code is ready to shine.", "The next tool in our kit is grad. It is the autodiff backbone of JAX and is inherited from the Autograd package. By wrapping your function with grad and evaluating it, you get the gradient evaluation returned. Let\u2019s have a look at how this would work with our ReLU activation function:", "Now that we know how to speed up functions and how to compute gradients, we come to the next gem: vmap \u2014 which makes batching as easy as never before. While in PyTorch one always has to be careful over which dimension you want to perform computations, vmap lets you simply write your computations for a single sample case and afterwards wrap it to make it batch compatible. It is as easy as that. Let\u2019s say you have a 100 dimensional feature vector and want to process it by a linear layer with 512 hidden units & your ReLU activation. And let\u2019s say you want to compute the layer activations for a batch with size 32.", "In jax it is convenient to keep all model parameters in a dictionary. This makes your life easier when you have to decide which dimension you want to batch/vmap over. vmap wraps the relu_layer function and takes as an input the axis over which to batch the inputs. In our case the first input to relu_layer are the parameters which are the same for the entire batch. The second input is the feature vector. We have stacked the vectors into a matrix such that our input has dimensions (batch_dim, feature_dim). We therefore need to provide vmap with batch dimension (0) in order to properly parallelize the computations. out_axes than specifies how to stack the individual samples outputs. In order to keep things consistent, we choose the first dimension to remain the batch dimension.", "At this point we got all the basic ingredients to start training our first JAX-powered deep learning model. We will start by defining a simple PyTorch MNIST dataloader and afterwards set everything up to train. JAX is a purely functional programming framework. Hence, we cant wrap things in class instances or modules used for example from the PyTorch nn.Module semantics. We, therefore, will need the following functions to train a Multilayer Perceptron:", "All of these will then be tied together in a training loop. We start by importing some additional helpers (including the optimizers from JAX) and the dataset from PyTorch. Any other dataloader will do the job similarly as long as transform the inputs to JAX-NumPy arrays.", "Next we will need a function that initializes the weights in our MLP. We will pass a list of hidden layer sizes and the previously generated PRNG key. We need to split the key iteratively to generate the individual weights of our network. Let\u2019s see how this is done for a MLP that takes the flat MNIST image as an input (28 x 28 = 784) and has two hidden layers with 512 units (e.g. 784\u2013512\u2013512\u201310).", "Now we define the forward pass through the network by iteratively looping over the layers and returning the log of the softmax output/predictions. Afterwards, we vmap the single case to create a batched version.", "Now we need some gradient-sauce: In order to get there we need to define which function we would like to optimize. This is going to be the multi-class cross-entropy loss between the one-hot encoded class labels and the softmax output of the network. Finally, for logging purposes we compute the accuracy as well.", "We can now put things together into a single update function that computes the gradient of the loss with respect to the parameters for a batch. We use the predefined optimizers and choose Adam to be our optimizer for the initialized parameters and we are ready!", "Having setup everything \u2014 it is time to run the learning loop for the 2-layer MLP!", "Until now we have defined our forward pass manually by looping over the individual layers and performing the ReLU layer transformations. This can be a little awkward but allows for maximal control over the computations. Alternatively, Jax similarly to PyTorch and Keras provides a higher-level layer of abstraction. Let me introduce you to stax and a simple CNN example.", "The stax API has a set of predefined input transformations predefined and ready to use. These include almost all standard feedforward operations. We can chain them using the stax.serial wrapper. Here is an example of a ConvNet that applies batch normalization and a Relu activation after each convolutional layer:", "The output returns a function to initialize the parameters of the network as well as a function to apply the forward pass through the network with. When initializing we have to specify the shape of the desired input as well as the batch dimension. Similarly as before we can then proceed to define the loss and accuracy. The only difference compared to the MLP case is that we no longer flatten the image.", "We can then run the learning loop again!", "Again training is smooth and the time per epoch is similar to the MLP. This is interesting since we are currently training on CPU which means that the 2D convolution can\u2019t be as easily parallelized as on the GPU. Still the compilation seems to work overtime!", "To be entirely honest, RNNs in Jax are a bit awkward. The stax API until now only supports feedforward transformations and we have to work around things a little (I have an open pull request but there is still some functionality missing). Everything is possible... but requires a little more brain stretching. Furthermore, the sequentiality of the for-loop is somewhat nasty in terms of compilation. But we will figure out how to get around that. In what follows we will try to learn to predict the next denoised values of a noisy version of the Ornstein-Uhlenbeck (OU) process with a Gated Recurrent Unit (GRU) network.", "Data Generation: JAX unfortunately has some weird characteristics when it comes to inplace replacements. Therefore, we go back to traditional NumPy to generate our Ornstein-Uhlenbeck process. We first sequentially generate the OU time series and afterwards add Gaussian noise on top. The RNN later on will try to denoise the noisy OU and to recover the original time series.", "Next we need to define our GRU layer. We do so by coding up the following hidden state update equations:", "Ultimately, we want to learn the weight matrices and biases. Here is the functional JAX/stax version:", "All of the stax function are structured in a similar way. You have a sub-function that initializes the parameters of the layer & another apply_fun that performs the transformation to the input given the parameters of the layer. Something that is special about the computations in an RNN is that we have to keep track of the hidden state. This is here done by defining the hidden state h as a parameter in the params dictionary. One benefit (or problem) of this is that we learn the initial hidden state as well. Furthermore, we would get into trouble if we simply wrote down a for-loop for executing the RNN over multiple timesteps. This is because JAX would try to compile the relevant functions for every timestep. And this may take a while. In order to overcome this we have to use lax.scan:", "scan is a JAX primitive and is lowered to a single XLA While HLO. That makes it useful for reducing compilation times for jit-compiled functions, since native Python loop constructs in an @jit function are unrolled, leading to large XLA computations. \u2014 ReadTheDocs", "In order to make use of such a powerful tool we need to define a single time step update & this is what is done in apply_fun_scan. We use partial to \"clone\" all the parameters to use at all timesteps. As before we can now \"instantiate\" our RNN and all required ingredients using the stax syntax. We choose the hidden state to have dimension 12 and the mean-squared erro loss function.", "We will train our GRU-RNN to learn to predict the data-generating process of an Ornstein-Uhlenbeck process. Initially it may be hopeless to generate an entire sequence of the process, end-to-end without additional help. Therefore, we will make use of a technique called teacher forcing.", "In teacher forcing we do not only use the next time step to compute the loss of the prediction (e.g. MSE) but also as an input to the next time step. In our case we want the network to also learn the denoising and therefore, do not use the denoised OU version but the noisy one as the input. Thereby, the network is aided in its learning process. Later on, after the network has learned parts of the generating dynamics we can disable the teacher assistance and let the network generate the entire sequence. We can then start to let the gradients flow again:", "Let\u2019s now have a closer look at the loss and the time series predictions!", "As we can observe the training loss goes down and the RNN is able to fit a sampled test sequence from the OU process fairly well. It appears that the denoising overshoots a little. The smooting seems to regulate the activity to the mean 0.5 which is the desired behavior.", "We have seen the power of combining autograd and XLA compilation to train networks fast and efficiently on your accelarator. What am I personally missing? Here is a short list:", "These are mainly tools that are specific to Deep Learning applications and I am fairly sure that they will become solved/added as we go along and more people start to pick up JAX. I am still a little undecided whether right now is the right time to fully jump onto JAX. There are still many question marks (e.g. how do I dettach variables from the computational graph). Switching would require me to rewrite quite a bit of my PyTorch codebase\u2026 What do you think?", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep RL PhD Student@TU Berlin. Intelligence. \u270d\ufe0f Support my writing: https://medium.com/@RobertTLange/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd0bc389bd683&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@RobertTLange?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@RobertTLange?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "Robert Lange"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F638b9cae9933&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&user=Robert+Lange&userId=638b9cae9933&source=post_page-638b9cae9933----d0bc389bd683---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc389bd683&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc389bd683&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "GETTING STARTED"}, {"url": "https://github.com/RobertTLange/code-and-blog/blob/master/04_jax_intro/jax_workspace.ipynb", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/xla", "anchor_text": "XLA"}, {"url": "https://github.com/hips/autograd", "anchor_text": "Autograd"}, {"url": "https://roberttlange.github.io/posts/2019/08/blog-post-6/", "anchor_text": "Forward and backward mode"}, {"url": "https://github.com/deepmind/rlax", "anchor_text": "rlax"}, {"url": "https://github.com/deepmind/dm-haiku", "anchor_text": "haiku"}, {"url": "https://github.com/ethanluoyc/dhmc-jax", "anchor_text": "Hamiltonian Monte-Carlo"}, {"url": "https://jax.readthedocs.io/en/latest/jax.numpy.html", "anchor_text": "here"}, {"url": "https://github.com/google/jax/pull/2298", "anchor_text": "open pull request"}, {"url": "https://de.wikipedia.org/wiki/Ornstein-Uhlenbeck-Prozess", "anchor_text": "Ornstein-Uhlenbeck (OU) process"}, {"url": "https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html", "anchor_text": "ReadTheDocs"}, {"url": "https://colinraffel.com/blog/you-don-t-know-jax.html", "anchor_text": "blog post"}, {"url": "https://github.com/google/jax/blob/master/docs/notebooks/Neural_Network_and_Data_Loading.ipynb", "anchor_text": "Jax documentation"}, {"url": "https://github.com/google-research/computation-thru-dynamics/tree/master/integrator_rnn_tutorial", "anchor_text": "\u2018Computation Through Dynamics\u2019"}, {"url": "https://medium.com/dunder-data/jupyter-to-medium-initial-post-ecd140d339f0", "anchor_text": "jupyter_to_medium"}, {"url": "https://medium.com/tag/mach\u0131ne?source=post_page-----d0bc389bd683---------------mach%C4%B1ne-----------------", "anchor_text": "Mach\u0131ne"}, {"url": "https://medium.com/tag/automatic-differentiation?source=post_page-----d0bc389bd683---------------automatic_differentiation-----------------", "anchor_text": "Automatic Differentiation"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d0bc389bd683---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----d0bc389bd683---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----d0bc389bd683---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0bc389bd683&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&user=Robert+Lange&userId=638b9cae9933&source=-----d0bc389bd683---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0bc389bd683&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&user=Robert+Lange&userId=638b9cae9933&source=-----d0bc389bd683---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc389bd683&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd0bc389bd683&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d0bc389bd683---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d0bc389bd683--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d0bc389bd683--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d0bc389bd683--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@RobertTLange?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@RobertTLange?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Robert Lange"}, {"url": "https://medium.com/@RobertTLange/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.2K Followers"}, {"url": "https://medium.com/@RobertTLange/membership", "anchor_text": "https://medium.com/@RobertTLange/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F638b9cae9933&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&user=Robert+Lange&userId=638b9cae9933&source=post_page-638b9cae9933--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa35dbfb4005a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-jax-mlps-cnns-rnns-d0bc389bd683&newsletterV3=638b9cae9933&newsletterV3Id=a35dbfb4005a&user=Robert+Lange&userId=638b9cae9933&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}