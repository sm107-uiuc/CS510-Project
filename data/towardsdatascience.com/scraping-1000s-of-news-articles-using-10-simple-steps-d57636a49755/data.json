{"url": "https://towardsdatascience.com/scraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755", "time": 1683009719.243327, "path": "towardsdatascience.com/scraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755/", "webpage": {"metadata": {"title": "Scraping 1000\u2019s of News Articles using 10 simple steps | by Kajal Yadav | Towards Data Science", "h1": "Scraping 1000\u2019s of News Articles using 10 simple steps", "description": "Aim of this article is to scrape news articles from different websites using Python. Generally, web scraping involves accessing numerous websites and collecting data from them. However, we can limit\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@TechyKajal/scraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755", "anchor_text": "Scraping web pages without using Software: Python", "paragraph_index": 1}, {"url": "https://medium.com/@TechyKajal/dataset-creation-for-beginners-using-software-4795ee119f6d", "anchor_text": "Scraping web Pages using Software: Octoparse", "paragraph_index": 2}, {"url": "https://medium.com/@TechyKajal/dataset-creation-for-beginners-using-software-4795ee119f6d", "anchor_text": "here.", "paragraph_index": 88}, {"url": "https://techykajal.medium.com/membership", "anchor_text": "https://techykajal.medium.com/membership", "paragraph_index": 92}], "all_paragraphs": ["Web Scraping Series: Using Python and Software", "Part-1: Scraping web pages without using Software: Python", "Part-2: Scraping web Pages using Software: Octoparse", "1.2 Who should read this article?", "2.1 A brief introduction to webpage design and HTML", "2.2 Web-scraping using BeautifulSoup in PYTHON", "Aim of this article is to scrape news articles from different websites using Python. Generally, web scraping involves accessing numerous websites and collecting data from them. However, we can limit ourselves to collect large amounts of information from a single source and use it as a dataset.", "Web Scraping is a technique employed to extract large amounts of data from websites whereby the data is extracted and saved to a local file in your computer or to a database in table (spreadsheet) format.", "So, I get motivated to do web scraping while working on my Machine-Learning project on Fake News Detection System. Whenever we begin a machine learning project, the first thing that we need is a dataset. While there are many datasets that you can find online with varied information, sometimes you wish to extract data on your own and begin your own investigation. I was needed with a dataset that I couldn\u2019t able to find anywhere according to my need.", "So this motivated me to make my own Dataset for my project accordingly. And that\u2019s how I did my project from the scratch. My Project was basically based on classifying different news articles into two main categories FAKE & REAL.", "For this project, The first task was to get a dataset which is already labeled with \u201cFAKE\u201d, so this can be achieved by scraping data from some verified & certified news websites, on which we can rely on for fact of news articles and it is really a very difficult task to get genuine \u201cFAKE NEWS\u201d.I go through these news websites to get my FAKE-NEWS Dataset", "But honestly speaking, I end up scraping data from one website i.e., Politifact.And there is a strong reason to do so, As you go through the listed links up there, you will conclude that we needed a dataset with already labeled category i.e., \u201cFAKE\u201d but also we don\u2019t want our news articles to be in a modified form as such. We want to extract a raw news article without any keywords specifying whether the given news article in a dataset is \u201cFAKE\u201d or not.So for example, If you go through the link \u201cBoomLive.in\u201d, you will find that the news articles specifying \u201cFAKE\u201d are not in its actual form and altered on basis of some analysis of the fact-checking team. So this altered text on model training in ML will give us a biased result every time and the model that we made using this kind of dataset will result into a dumb one which can only predict news articles having keywords like \u201cFAKE\u201d, \u201cDID?\u201d, \u201cIS?\u201d in it and will not be going to perform well on a new testing set of data.That\u2019s why we use Politifact to scrape our \u201cFAKE-NEWS DATASET\u201d.", "The second task was to create a \u201cREAL-NEWS\u201d dataset, So that was easy if you are scraping news-articles from trusted or verified news websites like \u201cTOI\u201d, \u201cIndiaToday\u201d, \u201cTheHindu\u201d & so many\u2026So we can trust these websites that they are listing the factual/actual data and even if not, then we are assuming the same to be true and will train our model accordingly.But for my project, I scrape data for real and fake from one website only (i.e., Politifact.com), since I am getting what I needed from it, and also it is advisable when we are scraping data using python to use only one website at a time. Although you can scrape multiple pages of that particular website altogether in one module by just running an outer for loop.", "Whoever is working on some projects where you need to scrape data in thousands, this article is definitely for you \ud83d\ude03. It doesn\u2019t matter if you are from a programming background or not, because there are many times when people other than programmers from different backgrounds needed data as per their project, survey, or whatsoever purpose.", "But non-programmers find it difficult to understand any programming language, So I will make scrapping easy for them too by introducing some software from which they can scrape any kind of data in a huge amount easily.", "Although Scraping using python is not that difficult if you follow along with me while reading this blog \ud83d\ude0e, the only thing that you need to focus on is the HTML source code of a webpage. Once, you able to understand how webpages are written in HTML and able to identify attributes and elements of your interest, you can scrape any website.", "For non-programmers, if you want to do web-scraping using python, just focus on HTML code mainly, python syntax is not that difficult to understand, It\u2019s just some libraries, some functions, and keywords that you needed to remember and understand. So I tried to explain every step with transparency, I hope at the end of this series, you will be able to scrape different types of the layout of webpages.", "This post covers the first part: News articles web scraping using PYTHON. We\u2019ll create a script that scrapes the latest news articles from different newspapers and stores the text, which will be fed into the model afterward to get a prediction of its category.", "If we want to be able to extract news articles (or, in fact, any other kind of text) from a website, the first step is to know how a website works.", "We will follow an example to understand this:", "When we insert an URL into the web browser (i.e. Google Chrome, Firefox, etc\u2026) and access to it, what we see is the combination of three technologies:", "HTML (HyperText Markup Language): it is the standard language for adding content to a website. It allows us to insert text, images, and other things to our site. In one word, HTML defines the content of every webpage on the internet.", "CSS (Cascading Style Sheets): this language allows us to set the visual design of a website. This means it determines the style/presentation of a webpage including colors, layouts, and fonts.", "JavaScript: JavaScript is a dynamic computer programming language. It allows us to make the content and the style interactive & provides a dynamic interface between client-side script and user.", "Note that these three are programming languages. They will allow us to create and manipulate every aspect of the design of a webpage.", "Let\u2019s illustrate these concepts with an example. When we visit the Politifact page, we see the following:", "If we disabled JavaScript, we would not be able to use this pop-up anymore, as you can see, we are not able to see a video pop up window now:", "If we delete CSS element from our web-page after finding it using ctrl+F on inspect window, we will see something like this:", "So, At this point, I will be going to ask you a question.", "\u201cIf you want to extract the content of a webpage via web-scraping, where do you need to look up?\u201d", "So, At this point, I hope you guys are clear about what kind of source code do we need to scrape. Yeah, you are absolutely right, If you are thinking about HTML \ud83d\ude0e", "So, the last step before performing web scraping methods is to understand the bit of the HTML language.", "HTML language is a \u201chypertext markup language\u201d that defines the content of a webpage and constitute of elements and attributes, for scraping data, you should be familiar with inspecting those elements.", "Enough talk, show me the code.", "We will first begin with installing necessary packages:", "1. beautifulsoup4To install it, Please type the following code into your python distribution.", "BeautifulSoup under bs4 package is a library used to parse HTML & XML docs into python in a very easy & convenient way and access its elements by identifying them with their tags and attributes.", "It is very easy to use, yet very powerful package to extract any kind of data from the internet in just 5\u20136 lines.", "To install it, use the following command in your IDE or use this command without an exclamation mark in a command shell.", "So as to provide BeautifulSoup with the HTML code of any page, we will need with the requests module.", "3. urllibTo install it, use the following command:", "urllib module is the URL handling module for python. It is used to fetch URLs(Uniform Resource Locator)", "Although, here we are using this module for a different purpose, to call libraries like:", "Now we will import all the required libraries:1. BeautifulSoupTo import it, use the following command onto your IDE", "This library helps us with getting HTML structure of any page that we want to work with and provides functions to access specific elements and extract relevant info.", "2. urllibTo import it, type following command", "3. requestsTo import it, just type import before this library keyword.", "This module allows us to send the HTTP requests to web-server using python. (HTTP messages consist of requests from client to server and responses from server to client.)", "It is a high-level data-manipulation tool that we needed to visualize our structured scraped data.", "will use this library to make DataFrame(Key data structure of this library). DataFrames allow us to store and manipulate tabular data in rows of observations and columns of variables.", "with the request module, we can get the HTML content and store into the page variable.Make a simple get request(just fetching a page)", "Since, requests.get(url) is a suspicious command and might throw an exception, we will call it in a try-except block", "We will also use an outer for loop for pagination purposes.", "I. See what response code the server sent back (useful fordetecting 4XX or 5XX errors.", "The HTTP 200 OK success status response code indicates that the request has succeeded.", "II. Access the full response as text(get the HTML of the page in a big string)", "It will return the HTML content of a response object in Unicode.Alternative:", "whereas, It will return the content of response in bytes.", "III. Look for a specific substring of text within the response.", "IV. Check the response\u2019s Content-Type (see if you got back HTML,JSON, XML, etc)", "Next with the time module, we can call sleep(2) function with a value of 2 seconds. Here it delayed sending requests to a web-server by 2 seconds.", "The sleep() function suspends execution of the current thread for a given number of seconds.", "Now that you\u2019ve made your HTTP request and gotten some HTML content, it\u2019s time to parse it so that you can extract the values you\u2019re looking for.", "A)Using Regular ExpressionsUsing Regular Expressions for looking up HTML content is strongly not recommended at all.", "However, regular expressions are still useful for finding specific string patterns like prices, email addresses, or phone numbers.", "Run a regular expression on the response text to look for specific string patterns:", "B)Using BeautifulSoup's object SoupBeautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work", "The below-listed command will Look for all the tags e.g.,<li> with specific attribute 'o-listicle__item'", "INSPECTING WEBPAGEFor being able to understand above code, you need to inspect the webpage & please do follow along:1)Go to listed URL above2)press ctrl+shift+I to inspect it.3)This is how your 'Inspect window' will look like:", "4)For getting above specific element & attribute in inspect window", "Did you able to locate the same tag on your machine?", "If yes, You are all set to understand every bit of HTML tags whatsoever I have used in my code.", "This command will help you to inspect how many news articles are there on a given page.Help you understand accordingly, up to what level you need to paginate your loop for extracting huge data.", "Bravo! \ud83c\udf1f We have scraped the first attribute i.e., Statement of our dataset", "In the below lines of code, I have put all concepts together & tried to fetch details for five different attributes of my Dataset.", "Append each attribute value to an empty list 'frame' for each article", "Then, extend this list to an empty list 'upperframe' for each page.", "If you wanted to visualize your data on Jupiter, you can use pandas DataFrame to do so.", "A) Opening & writing to fileThe below command will help you to write CSV file and save it to your machine in the same directory as where your python file has been saved in", "This line will write each attribute to a file with replacing any ',' with '^'.", "So, when you run this file on command shell, It will make a CSV file in your .py file directory.On opening it, you might see weird data if you don't use strip() while scraping. So do check it without applying strip() and if you don't replace '^' with ',', It will also look weird.So replace it using these simple steps:", "and running the same code again and again might throw an error if it has already created a dataset using the file writing method.", "B) converting dataframe into csv file using to_csv()So, instead of this lengthy method, you can opt for another method: to_csv() is also used to convert the data frame into a CSV file and also provide an attribute to specify the path.", "To avoid the ambiguity and allow portability of your code you can use this:", "this will append your CSV name to your destination path correctly.", "Although I will suggest using the first method using open file and writing to it and then close it, I know it is a bit lengthy & tacky to implement but at least it will not provide you with ambiguous data as to_csv method mostly does.", "See in the above image, how it extracts ambiguous data for the Statement attribute.So, instead of spending hours cleaning your data manually, I would suggest writing a few extra lines of code specified in the first method.Now, you are done with it.\u270c\ufe0f", "IMPORTANT NOTE: If you tried to copy-paste my source code for scraping different websites & run it, It might possible that it will throw an error. In fact, It will definitely throw an error because each webpage's layout is different & for that, you need to make changes accordingly.", "This article is the first part of the series of web-scraping and for those who come from non-technical backgrounds, read the second part of this series here.", "So, if any of my blog posts have been of help to you, and you feel generous at the moment, don\u2019t hesitate to buy me a coffee. \u2615\ud83d\ude0d", "I hope you will find it useful and liked my article.\ud83d\ude07 Please feel free to share your thoughts and hit me up with any queries you might have. You can reach me via the following :", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science | Kaggle | Machine Learning | Deep Learning | Python Become a member :- https://techykajal.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd57636a49755&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d57636a49755--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d57636a49755--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://techykajal.medium.com/?source=post_page-----d57636a49755--------------------------------", "anchor_text": ""}, {"url": "https://techykajal.medium.com/?source=post_page-----d57636a49755--------------------------------", "anchor_text": "Kajal Yadav"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3d250e28b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&user=Kajal+Yadav&userId=e3d250e28b9b&source=post_page-e3d250e28b9b----d57636a49755---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd57636a49755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd57636a49755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jammypodger7470", "anchor_text": "michael podger"}, {"url": "https://unsplash.com/", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@TechyKajal/scraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755", "anchor_text": "Scraping web pages without using Software: Python"}, {"url": "https://medium.com/@TechyKajal/dataset-creation-for-beginners-using-software-4795ee119f6d", "anchor_text": "Scraping web Pages using Software: Octoparse"}, {"url": "https://medium.com/@TechyKajal/dataset-creation-for-beginners-using-software-4795ee119f6d", "anchor_text": "here."}, {"url": "https://www.buymeacoffee.com/techykajal", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCdwAaZMWiRmvIBIT96ApVjw", "anchor_text": "YouTube channel"}, {"url": "https://www.youtube.com/channel/UCdwAaZMWiRmvIBIT96ApVjw", "anchor_text": "here"}, {"url": "https://medium.com/@TechyKajal", "anchor_text": "Medium"}, {"url": "http://www.linkedin.com/in/techykajal", "anchor_text": "LinkedIn"}, {"url": "https://kajalyadav.com/", "anchor_text": "https://kajalyadav.com/"}, {"url": "https://techykajal.medium.com/membership", "anchor_text": "https://techykajal.medium.com/membership"}, {"url": "https://towardsdatascience.com/8-ml-ai-projects-to-make-your-portfolio-stand-out-bfc5be94e063", "anchor_text": "8 ML/AI Projects To Make Your Portfolio Stand OutInteresting project ideas with source code and reference articles, also attaching some research papers too.towardsdatascience.com"}, {"url": "https://medium.com/datadriveninvestor/predicting-us-presidential-election-using-twitter-sentiment-analysis-with-python-8affe9e9b8f", "anchor_text": "Predicting US Presidential Election Using Twitter Sentiment Analysis with PythonFun project to revise data science fundamentals from dataset creation to data analysis to data visualizationmedium.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d57636a49755---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----d57636a49755---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----d57636a49755---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----d57636a49755---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/programming?source=post_page-----d57636a49755---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd57636a49755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&user=Kajal+Yadav&userId=e3d250e28b9b&source=-----d57636a49755---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd57636a49755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&user=Kajal+Yadav&userId=e3d250e28b9b&source=-----d57636a49755---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd57636a49755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d57636a49755--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd57636a49755&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d57636a49755---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d57636a49755--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d57636a49755--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d57636a49755--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d57636a49755--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d57636a49755--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d57636a49755--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d57636a49755--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d57636a49755--------------------------------", "anchor_text": ""}, {"url": "https://techykajal.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://techykajal.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kajal Yadav"}, {"url": "https://techykajal.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.96K Followers"}, {"url": "https://techykajal.medium.com/membership", "anchor_text": "https://techykajal.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3d250e28b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&user=Kajal+Yadav&userId=e3d250e28b9b&source=post_page-e3d250e28b9b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F40a8fbc93132&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-1000s-of-news-articles-using-10-simple-steps-d57636a49755&newsletterV3=e3d250e28b9b&newsletterV3Id=40a8fbc93132&user=Kajal+Yadav&userId=e3d250e28b9b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}