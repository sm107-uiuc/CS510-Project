{"url": "https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a", "time": 1683009399.9910579, "path": "towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a/", "webpage": {"metadata": {"title": "Entropy, Cross-Entropy, and KL-Divergence Explained! | by Aakarsh Yelisetty | Towards Data Science", "h1": "Entropy, Cross-Entropy, and KL-Divergence Explained!", "description": "Let us try to understand the most widely used loss function - Cross-Entropy."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Cross-Entropy (also known as log-loss) is one of the most commonly used loss function for classification problems. But most of us often get into solving problems without actually knowing the core concept of entropy due to the presence of today\u2019s vast libraries and frameworks and ease of using them. So, in this article, let\u2019s look into the fundamental intuition behind Entropy, relate it to Cross-Entropy and KL-Divergence. We\u2019ll also check out on an example for a classification problem using the loss function as Cross-Entropy.", "To get started with what does entropy exactly refers to, let\u2019s dive into some basics of Information Theory. In this digital age, messages are composed of bits(0s & 1s). While communicating, some bits are useful, some are redundant, some of them are errors, and so on. When we communicate a message, we want to transmit as much useful information as possible to the recipient.", "In Claude Shannon\u2019s paper, \u201cThe Mathematical Theory of Communication(1948)\u201d, he stated that to transmit 1 bit of information means to reduce the recipient\u2019s uncertainty by a factor of 2.", "Let\u2019s look at what does he mean. For example, consider a place where the weather is random with a 50\u201350 chance of either being Sunny or Rainy every day.", "Now, If a weather station tells you that it\u2019s going to be Rainy tomorrow, then they have reduced your uncertainty by a factor of 2. At first, there were two equally likely possibilities but after receiving an update from the weather station we have only one. Here, the weather station sent us a single bit of useful information and this is true no matter how they encoded this information.", "Even if the message sent is \u2018Rainy\u2019 and each character takes one byte, the total size of the message corresponds to 40 bits, but they still communicated only 1 bit of useful information.", "Suppose the weather has 8 possible states, all equally likely.", "Now, when the weather station gives you the next day\u2019s weather, then they are reducing your uncertainty by a factor of 8. Since each event has 1/8th chance to occur, the reduction factor would be 8.", "But what if the possibilities are not equally likely?", "Say, 75% chance of being sunny and 25% chance of being rainy.", "Now, if the weather station says that it\u2019s going to be rainy the next day, then your uncertainty has been dropped by a factor of 4, which is 2 bits of information. The uncertainty reduction is just the inverse of the event\u2019s probability. In this case, the inverse of 25% is 4, and log(4) to the base 2 gives 2. So, we get 2 bits of useful information.", "If the weather station says that it\u2019s going to be sunny the next day, then we get 0.41 bits of useful information. so, on average how much information are we going to get from the weather station?", "Well, there\u2019s a 75% chance that it will be sunny tomorrow and that gives you 0.41 bits of information and 25% chance that it will be rainy tomorrow which gives you 2 bits of information, which corresponds to,", "On average we get 0.81 bits of information from the weather station every day. So, what we just computed is called Entropy. It\u2019s a nice measure of how uncertain the events are. It is given by,", "Hopefully, the entropy\u2019s equation should now make complete sense. It measures the average amount of information that you get when you learn the weather each day. In general, it gives the average amount of information that we get from one sample drawn from a given probability distribution p. It tells us how unpredictable the probability distribution is.", "If we live in the middle of a desert where it would be sunny every day, on average we won\u2019t get much information every day from the weather station. The entropy would be close to zero. On the other hand, if the weather varies a lot, the entropy would be much larger.", "Now, let\u2019s talk about Cross Entropy. It is just the average message length. Considering the same example of 8 possible weather conditions, all equally likely, each one can be encoded using 3 bits.", "So, the average message length here is 3 and that\u2019s the Cross-Entropy.", "But now, suppose if you live in a sunny region, where the weather\u2019s probability distribution looks like this:", "Each day there\u2019s a 35% chance of being sunny and only 1% chance of a thunderstorm. So, we can compute the entropy of this probability distribution and we get,", "Note that the log used here is a binary log.", "So, on average, the weather station sends 3 bits but the recipient gets only 2.23 useful bits. We can do better.", "For example, let\u2019s change the code like this:", "We\u2019re now just using 2 bits for sunny or partially sunny weather, 3 bits for cloudy and mostly cloudy, 4 bits for light and medium rain, and 5 bits for heavy rain and thunderstorm. The weather is coded in such a way that it is unambiguous and if you chain multiple messages, there\u2019s only one way to interpret the sequence of bits. For example, 01100 can only mean partially sunny followed by light rain. So, if we compute the average number of bits that the station sends every day, then we get,", "This is our new and improved cross-entropy which is better than our previous 3 bits. Now, suppose if we used the same code in a different location, where the weather is reversed and is mostly rainy.", "Now for this, if we calculate the cross-entropy,", "We get 4.58 bits. It\u2019s roughly twice the entropy. On average, the station sends 4.58 bits but only 2.23 bits will be useful to the recipient. It is sending twice as much information per message as is necessary. It is because the code we are using is making some implicit assumptions about the weather distribution. For example, when we use a 2-bit message for sunny weather, we are implicitly predicting a probability of 25% for sunny weather. This is because of the negative binary log(0.25) gives 2.", "In the same way, we calculate for all the weather conditions.", "The powers of 2 in the denominator corresponds to the number of bits used for transferring the message. Now, it is pretty obvious that the predicted distribution q is quite different from the true distribution p.", "So, now we can express Cross-Entropy as a function of both True Probability Distribution p and Predicted Probability Distribution q which is expressed as:", "Note that the log used in our example is to the base 2.", "As you can see, it looks pretty similar to the equation of entropy except for the fact that we use the log of predicted probability here. If our predictions are perfect, that is the predicted distribution is equal to the true distribution, then the cross-entropy is simply equal to entropy. But, if the distributions differ, then the cross-entropy will be greater than the entropy by some number of bits. This amount by which the cross-entropy exceeds the entropy is called the Relative Entropy or more commonly known as the Kullback-Leibler Divergence (KL Divergence). In short,", "From the above example, we get", "Now, let\u2019s use Cross-Entropy in an application. Consider we are training an image classifier to classify different animals that look mostly alike such as a raccoon, a red panda, a fox, and so on.", "So, for each of the possible 7 classes, the classifier estimates a probability and this is called the predicted distribution. As this is a supervised learning problem, we know the true distribution.", "In the above example, I took an image of a raccoon, so the probability is 100% for it in the true distribution and 0 for others. We can use Cross-Entropy between these two distributions as a cost function which is called the Cross-Entropy loss.", "It\u2019s just the equation we saw earlier except it generally uses the natural log rather than the binary log. This doesn\u2019t matter much for training because the binary log(x) is equal to natural log(x)/log(2) where the denominator is a constant.", "So, when the class probabilities are mentioned as one-hot vector (it means one class has 100% and the rest of them are 0's), then the cross-entropy is just the negative log of the estimated probability for the true class.", "Now, you can see that the cost will grow very large when the predicted probability for the true class is close to 0. But when the predicted probability comes close to 1, the cost function approaches 0.", "As the loss obtained is more (since the predicted distribution is too low), we need to train the classifier with more number of examples for each class to decrease the amount of loss.", "We took an example of weather station updating about the following day\u2019s weather to understand the concept of Shannon\u2019s information theory. Then we related it to entropy and cross-entropy. At last, we took an example to look at the practical usage of the cross-entropy loss function. I hope this article clears the basic intuition behind entropy, cross-entropy, and KL Divergence and the relation between them.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An ardent learner in the field of AI, blockchain and loves to explore new tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb09cdae917a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b09cdae917a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b09cdae917a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aakarsh0705?source=post_page-----b09cdae917a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakarsh0705?source=post_page-----b09cdae917a--------------------------------", "anchor_text": "Aakarsh Yelisetty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ed920938e13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=post_page-4ed920938e13----b09cdae917a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb09cdae917a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb09cdae917a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=ErfnhcEV1O8", "anchor_text": "https://www.youtube.com/watch?v=ErfnhcEV1O8"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b09cdae917a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b09cdae917a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b09cdae917a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b09cdae917a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----b09cdae917a---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb09cdae917a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=-----b09cdae917a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb09cdae917a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=-----b09cdae917a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb09cdae917a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b09cdae917a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb09cdae917a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b09cdae917a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b09cdae917a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b09cdae917a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b09cdae917a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b09cdae917a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b09cdae917a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b09cdae917a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b09cdae917a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b09cdae917a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakarsh0705?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakarsh0705?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aakarsh Yelisetty"}, {"url": "https://medium.com/@aakarsh0705/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "141 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ed920938e13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=post_page-4ed920938e13--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe011017c9cea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-cross-entropy-and-kl-divergence-explained-b09cdae917a&newsletterV3=4ed920938e13&newsletterV3Id=e011017c9cea&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}