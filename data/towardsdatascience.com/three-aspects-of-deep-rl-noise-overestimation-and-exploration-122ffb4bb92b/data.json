{"url": "https://towardsdatascience.com/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b", "time": 1683006622.326752, "path": "towardsdatascience.com/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b/", "webpage": {"metadata": {"title": "Three aspects of Deep RL: noise, overestimation and exploration | by Rafael Stekolshchik | Towards Data Science", "h1": "Three aspects of Deep RL: noise, overestimation and exploration", "description": "Noise can be harmful, it can lead to a systematic overestimation. However, noise can be useful, such as noise for exploration. We touch on various sides of noise in Deep RL"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "DDPG", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "TD3", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "Rt", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "is the reward at time", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "t", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "Gt", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "is the cumulative reward", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "Q(s, a) is the Q-value table", "paragraph_index": 6}, {"url": "https://www.ri.cmu.edu/publications/issues-in-using-function-approximation-for-reinforcement-learning/", "anchor_text": "Issues in Using Function Approximation for Reinforcement Learning", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1509.06461", "anchor_text": "Deep Reinforcement Learning with Double Q-learning", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "neural networks are still too similar", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "one of the first algorithms", "paragraph_index": 11}, {"url": "https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id1", "anchor_text": "Deep Deterministic Policy Gradient", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "Continuous control with deep reinforcement learning", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "annealing", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "\u03b5", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "-greedy mechanism", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1802.05054", "anchor_text": "undirected exploration", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "DDPG", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1707.06347", "anchor_text": "PPO", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process", "anchor_text": "Ornstein-Uhlenbeck", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "TD3 paper", "paragraph_index": 20}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Fujimoto%2C+S", "anchor_text": "Fujimoto", "paragraph_index": 20}, {"url": "https://pypi.org/project/pybullet/", "anchor_text": "PyBullet", "paragraph_index": 25}, {"url": "https://github.com/bulletphysics/bullet3", "anchor_text": "Bullet Physics SDK", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole environment", "paragraph_index": 28}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v1_Hill_Climbing.ipynb", "anchor_text": "Cartpole-v1", "paragraph_index": 34}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v0_Hill_Climbing.ipynb", "anchor_text": "Cartpole-v0", "paragraph_index": 34}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/CartPole-Policy-Based-Hill-Climbing", "anchor_text": "see the project on Github", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1706.01905", "anchor_text": "Parameter Space Noise for Exploration", "paragraph_index": 37}], "all_paragraphs": ["We touch on various sides of noise in Deep Reinforcement Learning models. Part 1 discusses overestimation, that is the harmful property resulting from noise. Parts 2 deals with noise used for exploration, this is the useful noise. In the appendix, we will look at one more example of noise: adaptive noise.", "Part 1. We will see how researchers tried to overcome overestimation in models. First step is decoupling of the action selection from action evaluation. It was realized in Double DQN. The second step relates to the Actor-Critic architecture: here we decouple the value neural network (critic) from the policy neural network (actor). DDPG and TD3 use this architecture.", "Part 2. Exploration as a major challenge of learning. The main issue is exploration noise. We relate to models DQN, Double DQN, DDPG and TD3. Neural network models using some noise parameters have more capabilities for exploration and are more successful in Deep RL algorithms.", "Appendix. We consider the Hill-Climbing, the simple gradient-free algorithm. This algorithm adds adaptive noise directly to input variables, namely to the weight matrix determining the neural network.", "DQN and Double DQN algorithms turned out to be very successful in the case of discrete action spaces. However, it is known that these algorithms suffer from overestimation. This harmful property is much worse than underestimation, because underestimation does not accumulate. Let us see how researchers tried to overcome overestimation.", "The problem is in maximization operator using for the calculation of the target value Gt. Suppose, the evaluation value for Q(S_{t+1}, a) is already overestimated. Then from DQN key equations (see below) the agent observes that error also accumulates for Q(S_t, a) .", "Here, Rt is the reward at time t; Gt is the cumulative reward also know as TD-target; Q(s, a) is the Q-value table of the shape [space x action].", "Thrun and Schwartz in \u201cIssues in Using Function Approximation for Reinforcement Learning\u201d (1993) observed that using function approximators (i.e, neural networks) instead of just lookup tables (this is the basic technique of Q-learning) causes some noise on the output predictions. They gave an example in which the overestimation asymptotically lead to suboptimal policies.", "In 2015, Haselt et. al. in \u201cDeep Reinforcement Learning with Double Q-learning\u201d shown that estimation errors can drive the estimates up and away from the true optimal values. They supposed the solution that reduces the overestimation: Double DQN.", "The important thing that has been done in Double DQN is decoupling of the action selection from action evaluation. Let us make this clear.", "However, due to the slowly changing policy, estimates of the value of the current and target neural networks are still too similar, and this still causes a consistent overestimation.", "DDPG is one of the first algorithms that tried to use the Q-learning technique of DQN models for continuous action spaces. DDPG stands for Deep Deterministic Policy Gradient. In this case, we cannot use the maximization operator of Q-values over all actions, however, we can use the function approximator, a neural network representing Q-values. We presume that there exists a certain function Q(s, a) which is differentiable with respect to the action argument a.However, finding argmax(Q(S_t, a)) on all actions a for the given state S_t means that we must solve the optimization task at every time step. This is a very expensive task. To overcome this obstacle, a group of researchers from DeepMind in the work \u201cContinuous control with deep reinforcement learning\u201d used the Actor-Critic architecture. They used two neural networks: one, as before, in DQN: Q-network representing Q-values; another one is the actor function \ud835\udf0b(s) which provides a*, the maximum for the value function Q(s, a) as follows:", "In addition to overestimation, there is another problem in Deep RL, no less difficult. This is exploration. We cannot unconditionally believe in maximum values of the Q-table or in the value of a* = \ud835\udf0b(s). Why not? Firstly, at the beginning of training, the corresponding neural network is still \u201cyoung and stupid\u201d, and its maximum values are far from reality. Secondly, perhaps not the maximum values will lead us to the optimal strategy after hard training.", "In life, we often have to solve the following problem: to follow the beaten path \u2014 there is little risk and little reward; or to take a new unknown path with great risk \u2014 but, with some probability, a big win is possible there. Maybe it will be just super, you don\u2019t know.", "Exploitation means, that the agent uses the accumulated knowledge to select the following action. In our case, this means that for the given state, the agent finds the following action that maximizes the Q-value. The exploration means that the following action will be selected randomly.", "There is no rule that determines which strategy is better: exploration or exploitation. The real goal is to find a true balance between these two strategies. As we can see, the balance strategy changes in the learning process.", "Exploration in DQN and Double DQN", "One way to ensure adequate exploration in DQN and Double DQN is to use the annealing\u03b5-greedy mechanism. For the first episodes, exploitation is selected with a small probability, for example, 0.02 (i.e., the action will be chosen very randomly) and the exploration is selected with a probability 0.98. Starting from a certain number of episode M\u03b5, the exploration will be performed with a minimal probability \u03b5_m, for example, \u03b5_m= 0.01, and the exploitation is chosen with probability 0.99. The probability formula of exploration \u03b5 can be realized as follows:", "where i is the episode number. Let M\u03b5 = 100, \u03b5_m = 0.01. Then the probability \u03b5 of exploration looks as follows:", "In RL models with continuous action spaces, instead of \u03b5-greedy mechanism undirected exploration is applied. This method is used in DDPG , PPO and other continuous control algorithms. Authors of DDPG (Lillicrap et al., 2015) constructed undirected exploration policy \ud835\udf0b\u2019 by adding noise sampled from a noise process N to the actor policy \ud835\udf0b(s):", "where N is the noise given by Ornstein-Uhlenbeck, correlated noise process. In the TD3 paper authors (Fujimoto et. al., 2018) proposed to use the classic Gaussian noise, this is the quote:", "\u2026we use an off-policy exploration strategy, adding Gaussian noise N(0; 0:1) to each action. Unlike the original implementation of DDPG, we used uncorrelated noise for exploration as we found noise drawn from the Ornstein-Uhlenbeck (Uhlenbeck & Ornstein, 1930) process offered no performance benefits.", "A common failure mode for DDPG is that the learned Q-function begins to overestimate Q-values, then the policy (actor function) leads to significant errors.", "The name TD3 stands for Twin Delayed Deep Deterministic. TD3 retains the Actor-Critic architecture used in DDPG, and adds 3 new properties that greatly help to overcome overestimation:", "Exploration noise in trials with PyBullet Hopper", "PyBullet is a Python module for robotics and Deep RL based on the Bullet Physics SDK. Let us look at HopperBulletEnv, one of PyBullet environments associated with articulated bodies:", "The HopperBulletEnv environment is considered solved if the achieved score exceeds 2500. In TD3 trials with the HopperBulletEnv environment, I got, among others, the following results for std = 0.1 and std = 0.3:", "Here, std is the standard deviation of exploration noise in TD3. In both trials, threshold 2500 was not reached. However, I noticed the following oddities.", "We illustrate the properties of the Hill-Climbing algorithm applied to the Cartpole environment. The neural network model here is so simple that does not use tensors (no PyTorch, no Tensorflow), the neural network uses only the simplest matrix of shape [4 x 2], that is the forerunner of tensors.", "The Hill-Climbing algorithm seeks to maximize a target function Go, which in our particular case is the cumulative discounted reward:", "where \u03b3 is the discount factor, 0 < \u03b3 < 1, and Rk is the reward obtained at the time step k of the episode. The target function Go looks in Python as follows:", "As always in Deep RL, we try to exceed a certain threshold. For Cartpole-v0, this threshold score is 195, and for Cartpole-v1 it is 475. Hill-Climbing is a simple gradient-free algorithm (i.e., we do not use the gradient ascent/gradient descent methods). We try to climb to the top of the curve by only changing the arguments of the target function Go using a certain adaptive noise. However, what is the argument of our target function?", "The argument of Go is the weight matrix determining the neural network that underlies in our model. The weight matrix example for episodes 0\u20135 are presented here:", "The adaptive noise scaling for our model is realized as follows. If the current value of the target function is better than the best value obtained for the target function, we divide the noise scale by 2, and this noise is added to the weight matrix. If the current value of the target function is worse than the best obtained value, we multiply the noise scale by 2, and this noise is added to the best obtained value of the weight matrix. In both cases, a noise scale is added with some random factor different for any element of the matrix.", "For Cartpole-v1, if the weight matrix is initialized to non-zero small values (see above the left top matrix), the number of episodes = 112. Note that if the weight matrix is initialized to zeros then the number of episodes is increased from 112 to 168. The same for Cartpole-v0.", "For more information on Cartpole-v0/Cartpole-v1 with adaptive noise scaling, see the project on Github.", "A more generic formula for the noise scale", "As we saw above, the noise scale adaptively increases or decreases depending on whether the target function is lower or higher than the best obtained value. The noise scale in this algorithm is 2. In the paper \u201cParameter Space Noise for Exploration\u201d authors considers more generic formula:", "where \u03b1 is a noise scale, d is a certain distance measure between perturbed and non-perturbed policy, and \u03b4 is a threshold value. In Appendix C, authors consider the possible forms of the distance function d for algorithms DQN, DDPG and TPRO.", "[1] S.Thrun and A.Schwartz, Issues in Using Function Approximation for Reinforcement Learning, (1993), Carnegie Mellon University, The Robotics Institute [2] H.van Hasselt et. al., Deep Reinforcement Learning with Double Q-learning (2015), arXiv:1509.06461", "[14] M.Watts, Introduction to Reinforcement Learning (DDPG and TD3) for News Recommendation (2019), TowardsDataScience", "[18] R.Stekolshchik, How does the Bellman equation work in Deep RL? (2020), TowardsDataScience", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D. in Math, Algorithm and SW developer, Researcher. Fan of Deep Learning and Neural Networks. @r.stekol"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F122ffb4bb92b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://r-stekol.medium.com/?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "Rafael Stekolshchik"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57ce87a178e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=post_page-57ce87a178e5----122ffb4bb92b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F122ffb4bb92b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F122ffb4bb92b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "DDPG"}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "TD3"}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "Rt"}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "is the reward at time"}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "t"}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "Gt"}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "is the cumulative reward"}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "Q(s, a) is the Q-value table"}, {"url": "https://www.ri.cmu.edu/publications/issues-in-using-function-approximation-for-reinforcement-learning/", "anchor_text": "Issues in Using Function Approximation for Reinforcement Learning"}, {"url": "https://arxiv.org/abs/1509.06461", "anchor_text": "Deep Reinforcement Learning with Double Q-learning"}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "neural networks are still too similar"}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "one of the first algorithms"}, {"url": "https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id1", "anchor_text": "Deep Deterministic Policy Gradient"}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "Continuous control with deep reinforcement learning"}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "annealing"}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "\u03b5"}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "-greedy mechanism"}, {"url": "https://arxiv.org/abs/1802.05054", "anchor_text": "undirected exploration"}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "DDPG"}, {"url": "https://arxiv.org/abs/1707.06347", "anchor_text": "PPO"}, {"url": "https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process", "anchor_text": "Ornstein-Uhlenbeck"}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "TD3 paper"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Fujimoto%2C+S", "anchor_text": "Fujimoto"}, {"url": "https://pypi.org/project/pybullet/", "anchor_text": "PyBullet"}, {"url": "https://github.com/bulletphysics/bullet3", "anchor_text": "Bullet Physics SDK"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/HopperBulletEnv_v0-TD3", "anchor_text": "Trained agent for HopperBulletEnv"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/HopperBulletEnv_v0-TD3", "anchor_text": "HopperBulletEnv with TD3, std of noise = 0.02"}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole environment"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v1_Hill_Climbing.ipynb", "anchor_text": "Cartpole-v1"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v0_Hill_Climbing.ipynb", "anchor_text": "Cartpole-v0"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/CartPole-Policy-Based-Hill-Climbing", "anchor_text": "see the project on Github"}, {"url": "https://arxiv.org/abs/1706.01905", "anchor_text": "Parameter Space Noise for Exploration"}, {"url": "https://openai.com/blog/better-exploration-with-parameter-noise/", "anchor_text": "https://openai.com/blog/better-exploration-with-parameter-noise/"}, {"url": "https://spinningup.openai.com/en/latest/algorithms/ddpg.html", "anchor_text": "https://spinningup.openai.com/en/latest/algorithms/ddpg.html"}, {"url": "https://machinelearningmastery.com/stochastic-in-machine-learning/", "anchor_text": "https://machinelearningmastery.com/stochastic-in-machine-learning/"}, {"url": "https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process", "anchor_text": "https://en.wikipedia.org/wiki/"}, {"url": "https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process", "anchor_text": "_process"}, {"url": "https://tomstafford.staff.shef.ac.uk/?p=48", "anchor_text": "https://tomstafford.staff.shef.ac.uk/?p=48"}, {"url": "https://pybullet.org/wordpress/", "anchor_text": "https://pybullet.org/wordpress/"}, {"url": "https://medium.com/tag/overstimulation?source=post_page-----122ffb4bb92b---------------overstimulation-----------------", "anchor_text": "Overstimulation"}, {"url": "https://medium.com/tag/noise?source=post_page-----122ffb4bb92b---------------noise-----------------", "anchor_text": "Noise"}, {"url": "https://medium.com/tag/exploration?source=post_page-----122ffb4bb92b---------------exploration-----------------", "anchor_text": "Exploration"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----122ffb4bb92b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/actor-critic?source=post_page-----122ffb4bb92b---------------actor_critic-----------------", "anchor_text": "Actor Critic"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F122ffb4bb92b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=-----122ffb4bb92b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F122ffb4bb92b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=-----122ffb4bb92b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F122ffb4bb92b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F122ffb4bb92b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----122ffb4bb92b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----122ffb4bb92b--------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rafael Stekolshchik"}, {"url": "https://r-stekol.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "306 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57ce87a178e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=post_page-57ce87a178e5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbc96182db31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b&newsletterV3=57ce87a178e5&newsletterV3Id=bc96182db31e&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}