{"url": "https://towardsdatascience.com/initialization-techniques-for-neural-networks-f4ce8e64effc", "time": 1682995461.303474, "path": "towardsdatascience.com/initialization-techniques-for-neural-networks-f4ce8e64effc/", "webpage": {"metadata": {"title": "Initialization Techniques for Neural Networks | by Mayank Tripathi | Towards Data Science", "h1": "Initialization Techniques for Neural Networks", "description": "In this blog, we will see some initialization techniques used in Deep Learning. Anyone that even has little background in Machine Learning must know that we need to learn weights or hyperparameters\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1502.01852v1.pdf", "anchor_text": "https://arxiv.org/pdf/1502.01852v1.pdf", "paragraph_index": 31}, {"url": "https://twitter.com/mayank_408", "anchor_text": "Twitter", "paragraph_index": 37}, {"url": "https://www.linkedin.com/in/mayank-tripathi-a49563126/", "anchor_text": "Linkedin", "paragraph_index": 37}, {"url": "https://github.com/mayank408", "anchor_text": "Github", "paragraph_index": 37}], "all_paragraphs": ["In this blog, we will see some initialization techniques used in Deep Learning. Anyone that even has little background in Machine Learning must know that we need to learn weights or hyperparameters to make the model. These parameters govern how well our algorithm will perform on unseen data. To learn the model we need to initialize the parameters, apply the loss function and then optimize it. In this blog, we will focus on the initialization part of the network.", "If you ever have built any machine learning algorithm you must have heard that we need to \u201crandomly\u201d initialize our weights as the starting point and then start the learning process. The word random is quite vague in itself. We will see what actually goes behind this word random and what are different initialization techniques.", "It is one of the easiest ways of initialization the weights by putting all of them as equal to zero. Now let us visualize the implication of this technique using a simple two-layered network.", "As we have set all the weights ie w= 0, we can easily see that during the forward pass:", "Now let\u2019s see what happens in backpropagation", "From the above argument, we can see that at each step the change in weight is the same. Hence all the nodes in the hidden layer are learning the same parameters wrt to the input which is causing redundancy and making our network less flexible and hence less accurate. This is also called the Symmetry problem. Thus zero initialization is not a good technique.", "2. Initialization with the same Random value", "In this technique, we initialize all the weights with the same random value. I hope you already got the problem with this technique as it is quite similar to zero initialization instead we just used a random value but again the same problem persists as updation of weights will again be in same order. Hence this technique is also not used.", "3. Initialization with small Random values", "In this technique, we initialize all the weights randomly from a univariate \u201cGaussian\u201d (Normal) distribution having mean 0 and variance 1 and multiply them by a negative power of 10 to make them small. We can do this in Python using numpy as follows", "By plotting the values of the gradient, we can expect a Normal curve similar to.", "Now, we will run the learning algorithm and see how the distribution changes with different eopchs", "From the above plots, one can easily see that variance is decreasing and gradient are saturating to 0. This is known as Gradient Vanishing problem. One can also visualize this as each gradient is obtained as a result of a chain of multiplication of derivative and with each value much smaller than 1 thus gradient vanishes to zero.", "When these gradients are forward propagated with neurons having sigmoid activation, the output of a neuron is close to 0.5 as sigmoid(0) = 0.5, while in case of tanh it will be 0 centered same as the gradient graph.", "Thus we can conclude that if we take small random values the gradient vanishes on repeated chain multiplication and neurons get saturated to a value of 0.5 in case of sigmoid and 0 in case of tanh. Hence we cannot use small random values as initialization.", "4. Initialization with large Random values", "We just saw that in case of small random values, gradient vanishes. Now let us see what happens when we initialize weights with large random values. We can do this in Python using numpy as follows", "When we initialize weights to be large values the absolute sum WiXi will be very large and neurons get saturated to extremes during the forward pass as shown below.", "Below figure shows that at saturation the derivative of sigmoid vanishes to 0. A similar argument can be made for tanh as well.", "Now when we backpropagate through the network the derivates will tend to zero and hence the gradient will vanish in this case as well. So if you were thinking that we have initialized large weights so the gradient will explode instead of vanishing, it is not the case with sigmoid and tanh as activation due to saturation at large values.", "The above two arguments showed us that in both the cases either initializing weights as small values or large they tend to vanish. In small values, the gradient vanishes because of repeated chain multiplication while in large it vanishes because the derivative itself becomes zero. Hence they both can\u2019t be used.", "Before trying any new approach we will try to build some intuition why it is happening mathematically.", "Let\u2019s talk about the inputs to your neural network, you must know that we normalize our input before feeding into the network. For the sake of argument, let\u2019s consider that our input is coming from mean 0 and variance 1 normal distribution. We generalize the equation of a1 from the above network for n input as", "Now we will calculate the variance of a1", "Consider both input and weights as zero-mean first two terms will cancel out.", "Since all WiXi are identically distributed we can write", "We found that Var(a1) = (nVar(Wi))Var(Xi) or we can say that our input Xi is scaled to (nVar(Wi)) times variance. Some more maths and we would able to prove that kth hidden layer, the variance of Var(ak) = ([(nVar(Wi))]^k)Var(Xi). The physical significance of this statement is that any neuron in hidden layer can now vary n times the variation of input(which is again n times variance of the input of the previous layer) or if we plot the distribution we will find that Var(ak) is much more spread than that of Var(Xi)", "Now let\u2019s see what happens to (nVar(Wi))^k at different values of (nVar(Wi))", "Thus our job is to restrict (nVar(Wi)) = 1 which avoid the problem of exploding or vanishing gradients and spread of variance will remain constant throughout the network.", "So if we scale the weights obtained from Gaussian distribution with mean 0 and variance 1 to 1/\u221an, then we have", "So finally our task is to initialize weight from Normal Distribution with variance 1 and scale it to 1/\u221an, where n is the number of nodes in the previous layer. In Python, we could do this using", "In the case of ReLU activation function, we multiply by \u221a2/\u221an to account for the negative half (x<0) which do not contribute to any variance. This is also known as He initialization. This was proposed in https://arxiv.org/pdf/1502.01852v1.pdf", "Some of the other variants of Xavier Initialisation includes dividing by the sum of the number of input layer neurons and current hidden layer neurons. ie", "High-level API such as Keras also use Glorot Initialisation though the underlying distribution can be either Gaussian or Uniform. Below is the GitHub link of initializer function in Keras.", "If you were able to follow along with some mind-boggling maths. Awesome. So we first saw that we cant use zero or same initialization as all the weights tend to update by the same magnitude, hence hindering the learning process. Also, we saw that if we initialize weights to either too small or large values, then they tend to saturate and gradient falls to 0. Thus there is the need to initialize weights such that the variation across neurons in hidden layer remains constant and Xavier initialization allows us to do so and thus it is the most obvious choice for initialization for any network.", "There are techniques such as Batch Normalization, which tends to normalize neurons at each hidden layer before propagating it to the next layer, the same as we do with our inputs before feeding them into the network. This reduces the strong dependence on weight initialization and allows us to be a bit careless about initialization.", "For more such blogs, you can follow me, so that you get notified every time I come up with a new post.", "Also, Let\u2019s get connected on Twitter, Linkedin, Github.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Stuck at Local Minima, now trying to Converge"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff4ce8e64effc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mayank408?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mayank408?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "Mayank Tripathi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6ebc984c39e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&user=Mayank+Tripathi&userId=f6ebc984c39e&source=post_page-f6ebc984c39e----f4ce8e64effc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4ce8e64effc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4ce8e64effc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "anchor_text": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"}, {"url": "https://arxiv.org/pdf/1502.01852v1.pdf", "anchor_text": "https://arxiv.org/pdf/1502.01852v1.pdf"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer", "anchor_text": "https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer"}, {"url": "https://github.com/keras-team/keras/blob/62d097c4ff6fa694a4dbc670e9c7eb9e2bc27c74/keras/layers/core.py#L798", "anchor_text": "keras-team/kerasDeep Learning for humans. Contribute to keras-team/keras development by creating an account on GitHub.github.com"}, {"url": "https://twitter.com/mayank_408", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/mayank-tripathi-a49563126/", "anchor_text": "Linkedin"}, {"url": "https://github.com/mayank408", "anchor_text": "Github"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f4ce8e64effc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f4ce8e64effc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f4ce8e64effc---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----f4ce8e64effc---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f4ce8e64effc---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4ce8e64effc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&user=Mayank+Tripathi&userId=f6ebc984c39e&source=-----f4ce8e64effc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4ce8e64effc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&user=Mayank+Tripathi&userId=f6ebc984c39e&source=-----f4ce8e64effc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4ce8e64effc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff4ce8e64effc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f4ce8e64effc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f4ce8e64effc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mayank408?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mayank408?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mayank Tripathi"}, {"url": "https://medium.com/@mayank408/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "479 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6ebc984c39e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&user=Mayank+Tripathi&userId=f6ebc984c39e&source=post_page-f6ebc984c39e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3f16d2e60c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finitialization-techniques-for-neural-networks-f4ce8e64effc&newsletterV3=f6ebc984c39e&newsletterV3Id=f3f16d2e60c8&user=Mayank+Tripathi&userId=f6ebc984c39e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}