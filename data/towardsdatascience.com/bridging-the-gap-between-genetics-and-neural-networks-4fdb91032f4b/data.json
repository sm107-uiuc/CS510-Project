{"url": "https://towardsdatascience.com/bridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b", "time": 1683006583.703795, "path": "towardsdatascience.com/bridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b/", "webpage": {"metadata": {"title": "Bridging the Gap Between Genetics and Neural Networks | by Miri Trope | Towards Data Science", "h1": "Bridging the Gap Between Genetics and Neural Networks", "description": "I recently conducted research-work on genetic sequences. The main question that occupied my mind about this was: \u201cwhich is the simplest suggested neural network available for this purpose that is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/98adb4b86b01", "anchor_text": "Yoshua Bengio\u2019s", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1611.09340", "anchor_text": "Diet network: Thin Parameters for Fat Genomics", "paragraph_index": 0}, {"url": "http://www.image-net.org/", "anchor_text": "mageNet,", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "GG,", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "esNet", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1301.3781.pdf%C3%AC%E2%80%94%20%C3%AC%E2%80%9E%C5%93", "anchor_text": "ord2Vec,", "paragraph_index": 7}, {"url": "https://www.aclweb.org/anthology/D14-1162.pdf", "anchor_text": "love,", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "ERT", "paragraph_index": 7}, {"url": "https://www.nature.com/articles/s41586-020-2012-7", "anchor_text": "\u201cA Pneumonia Outbreak Associated With a New Coronavirus of Probable Bat Origin\u201d", "paragraph_index": 10}, {"url": "https://www.nature.com/articles/nature15393", "anchor_text": "1000 genome project", "paragraph_index": 13}, {"url": "https://www.internationalgenome.org/category/population/", "anchor_text": "frequency of individuals per population", "paragraph_index": 13}, {"url": "https://github.com/miritrope/genome", "anchor_text": "human genome", "paragraph_index": 19}, {"url": "https://medium.com/u/c79695e37339?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Daniel Godoy", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e", "anchor_text": "blog", "paragraph_index": 28}, {"url": "https://github.com/Bjarten/early-stopping-pytorch/blob/master/MNIST_Early_Stopping_example.ipynb", "anchor_text": "early stopping", "paragraph_index": 31}, {"url": "https://pytorch.org/", "anchor_text": "Pytorch", "paragraph_index": 33}, {"url": "https://arxiv.org/pdf/1810.12210.pdf", "anchor_text": "\u201cA Comparative Measurement Study of Deep Learning as a Service Framework\u201d", "paragraph_index": 33}, {"url": "https://medium.com/u/3efc011ea322?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Ashwin De Silva", "paragraph_index": 34}, {"url": "https://towardsdatascience.com/@ashwindesilva/how-to-use-google-colaboratory-to-clone-a-github-repository-e07cf8d3d22b", "anchor_text": "blog", "paragraph_index": 34}, {"url": "https://medium.com/u/e3afa9dd37eb", "anchor_text": "Mazid Osseni", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/different-types-of-regularization-on-neuronal-network-with-pytorch-a9d6faf4793e", "anchor_text": "blog", "paragraph_index": 36}, {"url": "https://www.baeldung.com/cs/learning-curve-ml", "anchor_text": "overfitting", "paragraph_index": 36}, {"url": "https://medium.com/u/c1ed18ad484c", "anchor_text": "Kevin Shen", "paragraph_index": 40}, {"url": "https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e", "anchor_text": "blog", "paragraph_index": 40}, {"url": "https://arxiv.org/pdf/1705.08741.pdf", "anchor_text": "\u201cTrain longer, generalize better: closing the generalization gap in large batch training of neural networks\u201d", "paragraph_index": 41}, {"url": "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf", "anchor_text": "Gradient Boosting Decision Tree", "paragraph_index": 50}, {"url": "https://github.com/microsoft/LightGBM", "anchor_text": "GitHub repository", "paragraph_index": 50}, {"url": "https://www.linkedin.com/in/miri-trope-916a194a/", "anchor_text": "Linkedin", "paragraph_index": 52}], "all_paragraphs": ["I recently conducted research-work on genetic sequences. The main question that occupied my mind about this was: \u201cwhich is the simplest suggested neural network available for this purpose that is most compatible with genetic data?\u201d After much literature-review, I discovered that the most \u201cdown to earth\u201d yet fascinating work related to this topic took place in Prof. Yoshua Bengio\u2019s lab. The paper named \u201cDiet network: Thin Parameters for Fat Genomics,\u201d and its main goal was to classify genetic sequences of 3,450 individuals into 26 ethnicities. That paper inspired me, and here I would like to explain the basics of building neural networks for solving that sort of a problem. For understanding this blog, no prior background in biology is needed; I will try to cover most of the necessary parts to jump straight into the computational sections.", "We are facing challenging times: the SARS-CoV-2 virus has left us helpless towards the powerful force of nature. By learning new tools: gaining intuition with regards to genomic data, and exploring which machine learning methods can best generalize that data; I hope that we can join forces together and make a change for better days, or at least use the incredible intelligence of neural networks to do something besides developing entertainment applications, but saving our lives and even our planet.", "Why Do I Find Genetics so Appealing?", "Your genetics reveal not just your biological information but the genetics history of your ancestors by representing which dominant parts survived through the years (check out \u201cancestral sequence reconstruction\u201d).", "In other words, it is the biological evolution coding of your family, and even more, according to Darwin\u2019s Theory of Evolution, the entirety of the collection of organic creatures (plants, animals, etc.) all share the same genetic principles.", "Let me walk you through other types of data, such as images and sentences, to understand the uniqueness of genetic data. On the one hand, images are two-dimensional data (or three-dimensional for volumes) with neighbor-relationships. Sentences are one-dimensional vectors of up to about a thousand values with the hierarchical nature of sentences trained by an unsupervised manner.", "On the other hand, a genetic sequence is a one-dimensional vector (a sequence) of at least hundreds of thousands of values with no well-defined relations between neighbors and far away from having a pre-trained set of models.", "Thus, a Gaussian smoothing filter that is very popular in image-processing is not relevant here, as well as all the bunch of pre-trained models in vision (ImageNet, VGG, ResNet \u2026) and natural language processing (Word2Vec, Glove, BERT \u2026) are benched out of the game.", "Think of a database consisting of thousands of genetic samples. You need to find a method that generalizes well (accuracy over 90%) with input data of tens of millions of combinations. A neural network can be a good fit because it utilizes the power of fully connected units in a way that is missing in other \u201cclassical\u201d algorithms like PCA, SVM, and decision trees that do not manage the data separately. Nevertheless, building the simplest network architecture requires more than tens of millions of free-parameters in the weights of the first layer. Dimensionality reduction (to avoid a surfeit of free parameters) is one way to face that problem; we will discuss it later in this blog.", "To clear things up and not pose difficulties over the main purpose of this forum, I present here only a high-level view of the biological parts needed in this blog. Needless to say, you are more than welcome to explore any of these biological topics further.", "A DNA molecule is a sequence of four types of bases represented by the letters of A, C, G, T. Specific parts of the sequence (even if remotely located) are correlated with a phenotype. For instance, a recent study: \u201cA Pneumonia Outbreak Associated With a New Coronavirus of Probable Bat Origin\u201d indicates that the ACE2 gene could be the host receptor (phenotype) of the SARS-CoV-2 virus. That example and many others remarkably show valuable information (criminals detection, matching cannabis strains, nutrition, and personalized medications) that can be achieved based solely on your DNA.", "Nowadays, we are closer than ever to achieving nearly full human genetic sequences. However, we are still far from covering the entirety of it. Single Nucleotide Polymorphisms SNPs are specific genotypes locations in the genomic sequence, generally represented as RS[number]. Different populations have different sequence invariants, but likely to be about the same within families (hence Asians look differently from Europeans). The analysis of SNP sequences will be a key point throughout the rest of this blog.", "In this section, I describe the data and the two main network architectures (and another network with improved parameters to overcome some of the major problems in machine learning) as well as some technical tips \u2026", "Relatively to other data types, medical datasets are difficult to find, mainly due to privacy restrictions. In light of this, the 1000 genome project achieved a remarkable breakthrough by publishing a publicly available dataset of 3,450 human DNA samples, 315K SNPs each of 26 worldwide populations. The next figure shows a histogram derived from the 1000 genomes data, depicting the frequency of individuals per population (ethnicity); The average number of samples of each population is about 133 genetic samples.", "As mentioned above, reducing the number of free parameters in a model is preferred (in our case, we are dealing with about 30 million parameters). The proposed method for achieving this uses another auxiliary network on top of the discriminative network that inputs a histogram per class (an embedding matrix calculated in an unsupervised manner). The output of this network initializes the weights of the first layer of the discriminative network. The embedding matrix is the normalized genotypes histogram per population, and its size is SNPs X [4x26], where four stands for {00, 01, 11, NA} (bi-allelic) and 26 for the number of classes (populations).", "The implementation of such an embedding matrix is described below.", "Anyhow, that is their solution; my solution is by reducing the number of the hidden units layer (see the architecture section). I called this new architecture the improved model, and one of its benefits is to overcome overfitting, as discussed later in the results section.", "Two main networks are compared in this blog. Both networks consist of two fully connected hidden layers followed by a softmax layer, but the second (see next figure) includes the auxiliary network that predicts the discriminative network\u2019s free parameters. The auxiliary network takes as input the embedding matrix and returns the weights of the discriminative network\u2019s first later (Fig. 1).", "A detailed description of the architecture can be seen in Fig.2.: batch norm followed by a dropout layer are required before each fully connected layer.", "I wrote the whole code in this work from scratch in Pytorch, it can be found at the public GitHub repository, named \u201chuman genome\u201d. Below are some general points that I find most relevant for this forum.", "To achieve objective results, we generate five-folds, one for each experiment, calculating the statistics variants at the end.", "We split the 3.5K samples into train (60%), validation (20%) and test (20%). As usual, we randomly shuffle the data and normalise the values:", "Generating the embedding matrix is conducted in two steps: the first generates a histogram of genotypes per class by bincount() and the second normalises that histogram. The outcome is a dimensionality reduction by a factor of about ten orders of magnitude.", "Here is an example of a histogram per class of a specific SNP:", "Both networks (auxiliary and discriminative) have separated computational graphs that are not linked in any way. The computational graph of the discriminative net, which contains the loss, does not have the information about the dependency between the loss and the embedding tensor. A solution can be to set the gradient value of the embedding tensor with the gradient value of the discriminative net manually and call torch.autograd.backward() on the embedding net because in the computational graph of the embedding net, the dependency between tensors is known.", "In the discriminative model\u2019s first hidden layer, we initialise its 30 million weights with the output of the auxiliary network (which is the embedding layer)", "The optimizer should update the parameters of both networks:", "Because we set the computational graph of the embedding net, the dependency between embedding and discriminative nets are known.", "Daniel Godoy explains in his blog the training process, I would like to expand the scope of his work according to our work with mini batches.", "The function: loss_fn(y, yhat) returns the mean squared error (squared L2 norm) between each element in the input y and target yhat. Because we want to calculate the loss, we need to multiply that value by the batch size and then summarise all the returned values of each minibatch.", "In the end, each epoch contains the accumulated value from the last section. Thus, to get the loss, we need to divide by the number of mini-batches in that loop.", "I highly recommend you to use early stopping, the rationale for this is automatically deciding according to the validation loss when is the right time to stop the training and save the best last model result.", "Below are some cool tools that I find useful (and free):", "I must mention the benefit of using Pytorch as the best neural network library, from my experience, in comparison with many others, it\u2019s the best in many ways. The paper: \u201cA Comparative Measurement Study of Deep Learning as a Service Framework\u201d presents an empirical analysis of the frameworks: TensorFlow, Caffe, Torch, and Theano.", "You would benefit from training your model on the cloud and saving time. Ashwin De Silva describes in his blog how you can work locally, commit and push to your GitHub repository, pull your commits on the cloud, and run your training there. In my opinion, it\u2019s worth the time and effort to script some tests with different parameters, such as the number of hidden units, dropout values, and so forth.", "In a retro- perspective view, I walked through some known difficulties among data scientists during the analysis of the results and found it important to share with you, to give you honest evidence of the dynamic behavior of developing such networks. I find the following remarks as the main characters while investigating the performance of your network:", "Let\u2019s start with the loss function: this is the \u201cbread and butter\u201d of the network performance, decreasing exponentially over the epochs. Moreover, a model that generalizes well keeps the validation loss similar to the training loss. The reason for this is simple: the model returns a higher loss value while dealing with unseen data. If you encounter a different case, your model is probably overfitting. Solutions to overfitting can be one or a combination of the following: first is lowering the units of the hidden layer or removing layers to reduce the number of free parameters. As we discussed above, our improved network as well as the auxiliary network, come to the rescue for the sake of this problem. Other possible solutions are increasing the dropout value or regularisation. Mazid Osseni, in his blog, explains different types of regularization methods and implementations. Fig. 3 shows the loss function of the simpler version of my network before (to the left) and after (to the right) dealing with the so-called overfitting problem.", "My solution is the combination of lowering the size of the hidden units (from 100 to 50 units) and increasing the dropout value of the first layer (from 0.5 to 0.8).", "The test accuracy was calculated in each architecture. Surprisingly, it seems that overcoming the overfitting or reducing the number of free parameters does not promise higher accuracy. Fig. 4 shows the test accuracy of the three architectures: notice that the accuracy is higher although the overfitting problem.", "The number of free parameters of the first layer of such model would be about the number of features (SNPs) x the number of the first layer (~300kx100). Now, we use an auxiliary network that predicts those 300kx100 free parameters. This auxiliary network takes as input a feature embedding, that is some arbitrary transformation of the vector of values each feature \u2014 SNP \u2014 takes across patients. The question is then how does this embedding look like. If we follow the embeddings considered in the paper, we would have a 4x26 dimensional embedding for the per-class histogram x 100 the number units of the first layer.", "Testing the performance with different batch sizes is an amusing task. Kevin Shen, in his blog, investigates the effect of batch size on training dynamics. According to the total training times, probably because of data diversity, the batch size is inversely proportional to the training time (Fig. 6). For the same reason, the loss is directly proportional to the batch size (Fig. 5).", "Fig. 6 clearly shows the behavior of using different batch sizes in terms of training times, both architectures have the same effect: higher batch size is more statistically efficient but does not ensure generalization. Read the paper: \u201cTrain longer, generalize better: closing the generalization gap in large batch training of neural networks\u201d to understand more about the generalization phenomenon and methods to improve the generalization performance while keeping the training time intact using large batch size.", "Notice the effect of changing the architecture in terms of the training time (Fig. 7). The training time is significantly lower either with 15 million free parameters, than the auxiliary network.", "I also compared the performance of the improved model to the decision trees approach, specifically the Light Gradient Boosting Machine that is commonly used in the data science domain. However, the performance went beyond our limits in terms of misclassification error (see Appendix for more details).", "Which Is the Best Model? (The Thousand Dollar Question)", "In this blog, I described two main networks: with and without the auxiliary network and an additional network with improved parameters. The benefit of the parameter prediction networks is that it considerably reduces the number of free parameters in the first layer of a model when the input is very high dimensional, as in genetic sequences.", "I showed how changing the parameters of the basic network yielded better generalization in terms of overfitting. I validated those networks\u2019 approaches on the publicly available 1000 genomes dataset, addressing the task of ancestry prediction based on SNP data. This work demonstrated the potential of neural network models to tackle tasks where there is a mismatch between the number of samples and their high dimensionality, like in DNA sequencing.", "Given the high accuracy achieved in the ancestry prediction task, I believe that neural network techniques can improve standard practices in the analysis of genetic data. I expect that these techniques will allow us to tackle more challenging genetic association studies.", "Thanks to Camille Rochefort-Boulanger (University of Montreal) for giving me some good advice on the implementation process.", "For those wondering which computer do I have: I would like to say some good words about my MacBook Pro, 16 GB of memory, Intel Core i7 for allowing me to work on such an amazing task, leaving me with those satisfying training times (see the Results section), and the whole \u201ccomputer laboratory\u201d experience (while working from home during the Coronavirus closure period).", "In addition to the neural network method that I highlighted in this blog, I would like to mention my experience results with Gradient Boosting Decision Trees. The implementation can be found in this GitHub repository. The parameters of the algorithm are:", "Results show that the classification error is higher with decision trees, explaining the reasons are beyond the scope of the blog.", "Dear readers, thanks for reading this blog; any of your thoughts would be much appreciated; please feel free to email me (miritrope@gmail.com) or Linkedin.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4fdb91032f4b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@miritrope?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@miritrope?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Miri Trope"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd3969155b54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&user=Miri+Trope&userId=cd3969155b54&source=post_page-cd3969155b54----4fdb91032f4b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fdb91032f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fdb91032f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/u/98adb4b86b01", "anchor_text": "Yoshua Bengio\u2019s"}, {"url": "https://arxiv.org/abs/1611.09340", "anchor_text": "Diet network: Thin Parameters for Fat Genomics"}, {"url": "http://www.image-net.org/", "anchor_text": "mageNet,"}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "GG,"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "esNet"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf%C3%AC%E2%80%94%20%C3%AC%E2%80%9E%C5%93", "anchor_text": "ord2Vec,"}, {"url": "https://www.aclweb.org/anthology/D14-1162.pdf", "anchor_text": "love,"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "ERT"}, {"url": "https://www.nature.com/articles/s41586-020-2012-7", "anchor_text": "\u201cA Pneumonia Outbreak Associated With a New Coronavirus of Probable Bat Origin\u201d"}, {"url": "https://www.nature.com/articles/nature15393", "anchor_text": "1000 genome project"}, {"url": "https://www.internationalgenome.org/category/population/", "anchor_text": "frequency of individuals per population"}, {"url": "https://github.com/miritrope/genome", "anchor_text": "human genome"}, {"url": "https://medium.com/u/c79695e37339?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Daniel Godoy"}, {"url": "https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e", "anchor_text": "blog"}, {"url": "https://github.com/Bjarten/early-stopping-pytorch/blob/master/MNIST_Early_Stopping_example.ipynb", "anchor_text": "early stopping"}, {"url": "https://pytorch.org/", "anchor_text": "Pytorch"}, {"url": "https://arxiv.org/pdf/1810.12210.pdf", "anchor_text": "\u201cA Comparative Measurement Study of Deep Learning as a Service Framework\u201d"}, {"url": "https://medium.com/u/3efc011ea322?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Ashwin De Silva"}, {"url": "https://towardsdatascience.com/@ashwindesilva/how-to-use-google-colaboratory-to-clone-a-github-repository-e07cf8d3d22b", "anchor_text": "blog"}, {"url": "https://medium.com/u/e3afa9dd37eb", "anchor_text": "Mazid Osseni"}, {"url": "https://towardsdatascience.com/different-types-of-regularization-on-neuronal-network-with-pytorch-a9d6faf4793e", "anchor_text": "blog"}, {"url": "https://www.baeldung.com/cs/learning-curve-ml", "anchor_text": "overfitting"}, {"url": "https://medium.com/u/c1ed18ad484c", "anchor_text": "Kevin Shen"}, {"url": "https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e", "anchor_text": "blog"}, {"url": "https://arxiv.org/pdf/1705.08741.pdf", "anchor_text": "\u201cTrain longer, generalize better: closing the generalization gap in large batch training of neural networks\u201d"}, {"url": "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf", "anchor_text": "Gradient Boosting Decision Tree"}, {"url": "https://github.com/microsoft/LightGBM", "anchor_text": "GitHub repository"}, {"url": "https://www.linkedin.com/in/miri-trope-916a194a/", "anchor_text": "Linkedin"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4fdb91032f4b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4fdb91032f4b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/science?source=post_page-----4fdb91032f4b---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4fdb91032f4b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4fdb91032f4b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fdb91032f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&user=Miri+Trope&userId=cd3969155b54&source=-----4fdb91032f4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fdb91032f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&user=Miri+Trope&userId=cd3969155b54&source=-----4fdb91032f4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fdb91032f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4fdb91032f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4fdb91032f4b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4fdb91032f4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@miritrope?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@miritrope?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Miri Trope"}, {"url": "https://medium.com/@miritrope/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "53 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd3969155b54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&user=Miri+Trope&userId=cd3969155b54&source=post_page-cd3969155b54--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15d2b34a4f18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbridging-the-gap-between-genetics-and-neural-networks-4fdb91032f4b&newsletterV3=cd3969155b54&newsletterV3Id=15d2b34a4f18&user=Miri+Trope&userId=cd3969155b54&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}