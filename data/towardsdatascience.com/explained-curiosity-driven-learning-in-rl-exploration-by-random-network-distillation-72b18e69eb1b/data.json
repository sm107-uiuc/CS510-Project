{"url": "https://towardsdatascience.com/explained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b", "time": 1682993969.365271, "path": "towardsdatascience.com/explained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b/", "webpage": {"metadata": {"title": "Explained: Curiosity-Driven Learning in RL | by Rani Horev | Towards Data Science", "h1": "Explained: Curiosity-Driven Learning in RL", "description": "In recent years, Reinforcement Learning has proven itself to be a powerful technique for solving closed tasks with constant rewards, most commonly games. A major challenge in the field remains\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1810.12894", "anchor_text": "paper", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1707.06347", "anchor_text": "PPO", "paragraph_index": 15}, {"url": "https://www.lyrn.ai", "anchor_text": "LyrnAI", "paragraph_index": 22}], "all_paragraphs": ["In recent years, Reinforcement Learning has proven itself to be a powerful technique for solving closed tasks with constant rewards, most commonly games. A major challenge in the field remains training a model when external feedback (reward) to actions is sparse or nonexistent. Recent models have tried to overcome this challenge by creating an intrinsic reward mechanism, mainly known as curiosity, that rewards the model for discovering new territories and states.", "A new paper from OpenAI, Exploration by Random Network Distillation (RND), presents a novel approach for the intrinsic reward. The model tries to predict if a given state has been seen before and gives bigger rewards for unfamiliar states.", "The model shows state-of-the-art results in several Atari games, including Montezuma\u2019s Revenge, which is known to be hard for RL algorithms. It is also relatively simple and has proven to be effective in environments which include distracting background noise.", "Reinforcement learning (RL) is a group of algorithms that are reward-oriented, meaning they learn how to act in different states by maximizing the rewards they receive from the environment. A challenging testbed for them are the Atari games that were developed more than 30 years ago, as they provide a rich visual input (210X160X3 per frame) and a diverse set of tasks that were designed to be difficult for humans.", "The games differ in their complexity and the frequency of the external rewards. While in Breakout a reward is given every time you hit a brick, in Montezuma\u2019s Revenge and others there are only a few rewards during a single level. Montezuma\u2019s Revenge, for example, is known to be challenging, as it requires long (hundreds of steps) and complicated combinations of actions to pass deadly obstacles and find rewards. The animations below illustrate the difference between the games.", "To succeed in games without frequent extrinsic rewards, the agent has to explore the environment in hope of discovering sparse rewards. These scenarios are common in real life, from finding lost keys in the house to discovering new cancer drugs. In such cases, the agent is required to use intrinsic rewards while acting mostly independently of extrinsic rewards. There exist two common approaches to RL with intrinsic rewards:", "These approaches perform better than models that are based only on extrinsic rewards (such as the well-known models DQN and A3C), but still worse than an average human.", "In general, when using intrinsic rewards, the assessment of future states suffers from three possible sources of error:", "RL systems with intrinsic rewards use the unfamiliar states error (Error #1) for exploration and aim to eliminate the effects of stochastic noise (Error #2) and model constraints (Error #3). To do so, the model requires 3 neural networks: A fixed target network that generates a constant output for a given state, aprediction network that tries to predict the target network\u2019s output, and a policy network that decides on the agent\u2019s next action.", "The target and prediction networks are used to generate a bigger intrinsic reward for unfamiliar states, by calculating the difference between the outputs of the two networks. The networks are of the same size and architecture \u2014 convolutional encoders (CNN) followed by fully-connected layers to embed the state into a features vector f. However, there is an important difference between them:", "As more states are fed into the system, the prediction network becomes better at predicting the target network output when it receives known states. When reaching previously-visited states, the agent receives a small reward (because the target output is predictable) and the agent is disincentivized to reach them again. In other words, unlike common models, the agent isn\u2019t trying to predict the next state based on the current state and action, but instead tries to predict the novelty of a future state.", "The target-prediction architecture has several benefits:", "One of the challenges in this model is that the intrinsic reward decreases as more states become familiar and might vary between different environments, making it difficult to choose hyperparameters. To overcome this, the intrinsic rewards are normalized in each update cycle.", "The policy network\u2019s role is to decide on the next action based on the current state and its internal model, which was trained on previous states. To make that decision, it uses an input embedder and a policy optimizer:", "The input embedder encodes the environment state to features. The paper compares 2 architectures \u2014 CNN or a mixture of CNN and recurrent layers (GRU cells). The recurrent layers are supposed to help with predicting the next action by capturing longer contexts of the game, e.g. events that happened before the current state, and were indeed found to perform better than CNN-only layers in most cases.", "A major problem in training a policy model is convergence, since policies tend to change drastically as a result of a single update in rewards. For example, in some architectures, a single bad episode (game) can completely change your strategy. Therefore, on top of the embedding layers, the network has a Proximal Policy Optimizer (PPO) that predicts the next action based on the embedded state. The main contribution of PPO is optimizing the policy safely without radical updates, by bounding the difference between consecutive policies updates.", "To update the policy, PPO first needs to estimate the future intrinsic and extrinsic rewards for a given state (\u201cValue Head\u201d). Handling each type of reward separately allows more flexibility in determining the effect each type has on the policy and in the way each type is calculated:", "The charts below shows the the policy network and the entire architecture:", "The paper compares, as a baseline, the RND model to state-of-the-art (SOTA) algorithms and two similar models as an ablation test:", "The RND agent achieved state-of-the-art results in 3 out of 6 games, and was able to reach a better score than the \u201caverage human\u201d in Montezuma\u2019s Revenge. However, its performance in 2 other games was significantly lower compared to other SOTA algorithms. The paper does not explain the nature of games in which this technique is less useful.", "The RND model exemplifies the progress that was achieved in recent years in hard exploration games. The innovative part of the model, the fixed and target networks, is promising thanks to its simplicity (implementation and computation) and its ability to work with various policy algorithms.", "On the other hand, there is still a long way to go \u2014 there is no model to rule them all, and the performance varies among different games. In addition, while RNN might help a bit with keeping a longer context, global exploration is still a challenge. Scenarios that require long relations, e.g. using a key that was found in the first room to open the last door, are still unreachable.", "To stay updated with the latest Machine Learning research, subscribe to my newsletter on LyrnAI", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Learn something new every day. Currently Deep Learning :)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F72b18e69eb1b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ranihorev?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "Rani Horev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53f9e9fdd8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&user=Rani+Horev&userId=53f9e9fdd8d8&source=post_page-53f9e9fdd8d8----72b18e69eb1b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72b18e69eb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72b18e69eb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.lyrn.ai", "anchor_text": "Explained"}, {"url": "https://arxiv.org/pdf/1810.12894", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1707.06347", "anchor_text": "PPO"}, {"url": "https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/", "anchor_text": "blog"}, {"url": "https://www.lyrn.ai", "anchor_text": "LyrnAI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----72b18e69eb1b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----72b18e69eb1b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/openai?source=post_page-----72b18e69eb1b---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----72b18e69eb1b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72b18e69eb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&user=Rani+Horev&userId=53f9e9fdd8d8&source=-----72b18e69eb1b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72b18e69eb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&user=Rani+Horev&userId=53f9e9fdd8d8&source=-----72b18e69eb1b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72b18e69eb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F72b18e69eb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----72b18e69eb1b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----72b18e69eb1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rani Horev"}, {"url": "https://medium.com/@ranihorev/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53f9e9fdd8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&user=Rani+Horev&userId=53f9e9fdd8d8&source=post_page-53f9e9fdd8d8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9bc3579798b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b&newsletterV3=53f9e9fdd8d8&newsletterV3Id=9bc3579798b7&user=Rani+Horev&userId=53f9e9fdd8d8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}