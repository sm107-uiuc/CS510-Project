{"url": "https://towardsdatascience.com/what-is-the-next-paradigm-in-nlp-9df7fcd2919a", "time": 1682996599.757123, "path": "towardsdatascience.com/what-is-the-next-paradigm-in-nlp-9df7fcd2919a/", "webpage": {"metadata": {"title": "What is the Next Paradigm in NLP? | by Jamal Shareef | Towards Data Science", "h1": "What is the Next Paradigm in NLP?", "description": "If you\u2019ve ever interacted with a User Assistant such as Alexa, Cortana, Google or Siri, you may have been impressed with the dialogue\u2026to a point. There are many amazing things that these assistants\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "bag of words model", "paragraph_index": 12}, {"url": "https://research.fb.com/downloads/babi/", "anchor_text": "bAbI dataset from Facebook Research", "paragraph_index": 16}, {"url": "https://openai.com/blog/better-language-models/#sample1", "anchor_text": "OpenAI blog post", "paragraph_index": 22}, {"url": "https://medium.com/@joealato/attention-in-nlp-734c6fa9d983", "anchor_text": "NLP Attention Theory", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1510.03055.pdf", "anchor_text": "Maximum Mutual Information (MMI)", "paragraph_index": 24}], "all_paragraphs": ["If you\u2019ve ever interacted with a User Assistant such as Alexa, Cortana, Google or Siri, you may have been impressed with the dialogue\u2026to a point. There are many amazing things that these assistants can do and infer about your speech input. Each assistant has its strengths and weaknesses but all assistants, as well as other types of text-based NLP artificial intelligence like chatbots, and customer service bots tend to fall apart as the dialogue continues.", "Current NLP machine learning is great at a few things, good at others, and poor in the case of extended contextual exchanges. Machine logic is very good at detecting key words and parts of speech and therefore identifying spam and general sentiment. Machines are poor at lengthy exchanges because inferring meaning and relationships between words and thus ideas becomes an increasingly difficult problem for current machine learning models to solve. But why is this problem so hard for machine logic?", "Firstly, given the intended application of content creation either in long complex dialogues or creative writing what are the current machine learning tools? I would like to specifically focus on the latter application, that of machine learning creative writing in the form of poetry and prose.", "A machine learning model can be trained on a dataset of sample poetry to create its own new poetry. Depending on the model and the amount of poem data it is exposed to the results aren\u2019t always very meaningful or thought provoking. The medium of poetry however can be very abstract and for this reason, some of the unnatural sentences that our model may produce are somewhat masked.", "A method called Seq2Seq or Sequence to Sequence is often used for conversation modeling. We can feed in words from sample poems in a sequence to train the machine to select the \u2018best\u2019, most probable next words that would follow. The chain continues on with the current outputted word becoming the input for determining the next word that will follow and so on.", "This Seq2Seq model can be adapted for poetry by including an offset from the training data and possibly limiting our poem lines to a certain length. In the table below our dataset is the input and our expected output is the Target.", "I used several quotes (400 lines) from poet Nayyirah Waheed as the dataset to train the model. The results of the trained model poems included a lot of word for word copying and incomplete sentences. This could be improved with a larger dataset, but I suspected the same phenomenon would be observed for much of those datasets as well.", "As expected using more lines (~1600 lines) of Robert Frost poems as the training set yielded similar results. There are still some snippets that are taken verbatim from the dataset as well as lines and entire poems that are syntactically correct but nonsensical.", "The answer to improving these results could be more data. More data from the poets, or additionally combining data from multiple poets. As a thought exercise we could imagine giving it all the data from every poet. In theory the prospect of adding more and more data is very appealing. It would virtually eliminate the instances where our model blatantly regurgitates entire lines because it didn\u2019t have enough experience with any other follow-up words in the Seq2Seq method. If bigger data alleviates our copycat problem, it also exacerbates the incidence of incomplete thoughts and conflicting styles.", "In order to improve our results we have to realize the limitations of our given training model. To put it simply, we can\u2019t expect to see meaningful poetry with the basic Seq2Seq method in its current form. Expecting to produce excellent prose from this model is even further out of the question. Prose has characters and locations and motives which must be retained, revisited, changed and carried through as the story unfolds. There is nothing in our basic model that can ensure that these considerations would be upheld.", "I am hesitant to suggest the obvious answer \u2014 that if we want our machines to produce things like human beings we need to teach them like we teach ourselves. Computer logic is vastly different from our own and attempting to impart meaning to words for that logic is effectively impossible. I would assert that we cannot throw more data at the problem. No matter the size of the data, any model which relies on the next best word method cannot maintain coherency over long stretches \u2014 it is inherent in the mathematics.", "We cannot give a machine model a book to read and then ask for a book report with deep analysis. We also should not expect that the act of reading for comprehension will improve future writing by the model.", "The current state of machine reading comprehension uses a form of the Memory Network model. A model can be trained to receive a story as input and then answer basic questions about the story. The story is a set of sentences with proper nouns, verbs and pronouns which refer to preceding nouns. Each sentence is tokenized and later vectorized into a bag of words model \u2014 which is a vector representing each word as either a \u20181\u2019 if it is in the sentence or a \u20180\u2019 if it does not appear.", "Once the data has been cleaned and vectorized the model can be trained to answer questions similar to that below in the Question Module. The trained model can take into account the order of operations and make correct assumptions about pronouns and other parts of speech.", "The best models also do not require that the story explicitly states facts in the form that they are answered. For example, \u201cThe cat was happy. It weighs 20 pounds\u201d, \u201cQuestion: How much does the cat weigh\u201d can be correctly answered but so can passages such as \u201cIt was half past 11 and Stacie was not happy to see the number 20 on the scale while weighing her cat\u201d.", "It is an impressive feat as you can imagine the input may be large, and the answers can be arrived at very fast, much faster than by a human. It is still a game of memory however and the responses do not rise to the level of an essay.", "There are further applications that are being researched, one of note is the bAbI dataset from Facebook Research. bAbI shows several applications of the Memory Network in action such as inferring who the person speaking is in a Children\u2019s Book, or answering general trivia. There is a lot of great work going on and these datasets are meant to spur the development community to greater heights. The research is ongoing and I am hopeful that whether it is in the public or private sector that we continue to see this information published in papers and released as openly as possible to the public.", "It would seem however that we would have great difficulty taking the next leap and expecting the model to make any deeper connections and associations when given a story. As with the general trivia dataset from bAbI and the popular personal voice assistants in our smart devices when given a large enough database, many types of questions may be successfully answered. What we are still missing is inference from the data and the ability to take some type of \u2018creative\u2019 license with said data.", "A model that can create large amounts of content that make sense and is compelling would require extremely bespoke logic. Perhaps so much so that its creators would be better off just writing that novel themselves. If this work is ever accomplished however, the payoff would be extremely worthwhile. This \u2018cutting-edge\u2019 model would in turn be able to create so much more in quantity and with greater ease. Machine Learning poems and novels won\u2019t be a match for quality compared to our own works any time soon, but there are plenty of beneficial use cases for such content creation.", "It could improve automated customer service interactions, personal assistant AI\u2019s would be so much more powerful, language translation would benefit greatly. The actual content created by such a model could be good enough for consumption at least where the human writer has no time or desire to handcraft the content.", "At the same time, the potential for bad actors to use such a powerful content creation tool is a very real danger. The ability to create doctored and edited audio and video combined with believable, automatically generated content could have catastrophic consequences. It has given pause to the community with calls for governments and organizations to establish guidelines for uses of these content creation tools.", "OpenAI has a model, GPT-2 which was created with unsupervised training on a dataset of 8 million web pages using 1.5 billion parameters. At the time of this writing however only the 345 million parameter model has been made available to the public due to the fear of potential for misuse of the full model.", "The goal of one of GPT-2's functions is to use a text story prompt upon which it will add a few paragraphs in the same style and with through lines to the original established ideas. The results can be impressive and I would urge anyone to reference the OpenAI blog post for a full look at all the examples.", "The answer to our question then is not more data, or to teach the machine better. Unless there is a massive shift in the way computers operate, we will continue to have to translate words and symbols into math so that they can be modeled by machine computation. Meaning as we know it is lost in translation. One of the goals of current research is to bring that meaning back or to simulate its effect mathematically in some way to create more diverse responses with context and larger and more interesting stories.", "There are models which build upon the base Seq2Seq model and add additional layers, such as an Attention Layer (see K.Loginova, NLP Attention Theory, 2018). Other models forego Seq2Seq and use a different paradigm such as the Maximum Mutual Information (MMI) model.", "Our best models can be very convincing in a few lines of poetry, or in a handful of paragraphs but things do fall apart in larger blocks of content. The difficult work continues as we try to impart one our greatest gifts to one of our most powerful tools.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9df7fcd2919a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jamal.k.shareef?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jamal.k.shareef?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "Jamal Shareef"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cf4dd8d7c94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&user=Jamal+Shareef&userId=1cf4dd8d7c94&source=post_page-1cf4dd8d7c94----9df7fcd2919a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9df7fcd2919a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9df7fcd2919a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/lazyprogrammer", "anchor_text": "LazyProgrammer at Github"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "bag of words model"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "OpenAI blog"}, {"url": "https://research.fb.com/downloads/babi/", "anchor_text": "bAbI dataset from Facebook Research"}, {"url": "https://openai.com/blog/better-language-models/#sample1", "anchor_text": "OpenAI Blog Post"}, {"url": "https://openai.com/blog/better-language-models/#sample1", "anchor_text": "OpenAI blog post"}, {"url": "https://medium.com/@joealato/attention-in-nlp-734c6fa9d983", "anchor_text": "NLP Attention Theory"}, {"url": "https://arxiv.org/pdf/1510.03055.pdf", "anchor_text": "Maximum Mutual Information (MMI)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9df7fcd2919a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----9df7fcd2919a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/seq2seq?source=post_page-----9df7fcd2919a---------------seq2seq-----------------", "anchor_text": "Seq2seq"}, {"url": "https://medium.com/tag/poetry?source=post_page-----9df7fcd2919a---------------poetry-----------------", "anchor_text": "Poetry"}, {"url": "https://medium.com/tag/prose?source=post_page-----9df7fcd2919a---------------prose-----------------", "anchor_text": "Prose"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9df7fcd2919a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&user=Jamal+Shareef&userId=1cf4dd8d7c94&source=-----9df7fcd2919a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9df7fcd2919a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&user=Jamal+Shareef&userId=1cf4dd8d7c94&source=-----9df7fcd2919a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9df7fcd2919a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9df7fcd2919a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9df7fcd2919a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9df7fcd2919a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jamal.k.shareef?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jamal.k.shareef?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jamal Shareef"}, {"url": "https://medium.com/@jamal.k.shareef/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "8 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cf4dd8d7c94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&user=Jamal+Shareef&userId=1cf4dd8d7c94&source=post_page-1cf4dd8d7c94--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1cf4dd8d7c94%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-the-next-paradigm-in-nlp-9df7fcd2919a&user=Jamal+Shareef&userId=1cf4dd8d7c94&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}