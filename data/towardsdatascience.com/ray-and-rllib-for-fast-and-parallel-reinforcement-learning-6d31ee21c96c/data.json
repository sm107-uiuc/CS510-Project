{"url": "https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c", "time": 1683005534.6266408, "path": "towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c/", "webpage": {"metadata": {"title": "Ray and RLlib for Fast and Parallel Reinforcement Learning | by Christian Hubbs | Towards Data Science", "h1": "Ray and RLlib for Fast and Parallel Reinforcement Learning", "description": "Ray is more than just a library for multi-processing; Ray\u2019s real power comes from the RLlib and Tune libraries that leverage this capability for reinforcement learning. It enables you to scale\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.datahubbs.com/policy-gradients-and-advantage-actor-critic/", "anchor_text": "Advantage Actor Critic Algorithm (A2C)", "paragraph_index": 3}, {"url": "https://github.com/ray-project/ray/tree/master/rllib/agents", "anchor_text": "long list", "paragraph_index": 5}, {"url": "https://www.datahubbs.com/reinforcement-learning-is-wrong-for-your-business-application/", "anchor_text": "use it in your company", "paragraph_index": 15}, {"url": "https://ray.readthedocs.io/en/latest/rllib-env.html", "anchor_text": "explicitly states", "paragraph_index": 16}, {"url": "https://www.datahubbs.com/building-custom-gym-environments-for-rl/", "anchor_text": "step-by-step process on setting up environments here", "paragraph_index": 17}, {"url": "https://stackoverflow.com/questions/58551029/rllib-use-custom-registered-environments/60792871#60792871", "anchor_text": "The best way I\u2019ve found to do this", "paragraph_index": 18}, {"url": "http://gym.openai.com/docs/", "anchor_text": "OpenAI Gym API", "paragraph_index": 22}, {"url": "https://github.com/ray-project/ray/issues/6372", "anchor_text": "currently", "paragraph_index": 23}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P", "paragraph_index": 27}], "all_paragraphs": ["Ray is more than just a library for multi-processing; Ray\u2019s real power comes from the RLlib and Tune libraries that leverage this capability for reinforcement learning. It enables you to scale training to large-scaled distributed servers, or just take advantage of the parallelization properties to more efficiently train using your own laptop. The choice is yours.", "We show how to train a custom reinforcement learning environment that has been built on top of OpenAI Gym using Ray and RLlib.", "Once you\u2019ve installed Ray and RLlib with pip install ray[rllib], you can train your first RL agent with a single command in the command line:", "This will tell your computer to train using the Advantage Actor Critic Algorithm (A2C) using the CartPole environment. A2C and a host of other algorithms are already built into the library meaning you don\u2019t have to worry about the details of implementing those yourself.", "This is really great, particularly if you\u2019re looking to train using a standard environment and algorithm. If you want to do more, however, you\u2019re going to have to dig a bit deeper.", "The various algorithms you can access are available through ray.rllib.agents. Here, you can find a long list of different implementations in both PyTorch and Tensorflow to begin playing with.", "These are all accessed using the algorithm\u2019s trainer method. For example, if you want to use A2C as shown above, you can run:", "If you want to try a DQN instead, you can call:", "All the algorithms follow the same basic construction alternating from lower case algo abbreviation to uppercase algo abbreviation followed by \u201cTrainer.\u201d", "Changing hyperparameters is as easy as passing a dictionary of configurations to the config argument. A quick way to see what\u2019s available to you is to call trainer.config to print out the options that are available for your chosen algorithm. A few examples include:", "There are lots of others to set and customize from the network (typically located in model dictionary) to various callbacks and multi-agent settings.", "I want to turn and show a quick example to get you started and show you how this works with a standard, OpenAI Gym environment.", "Choose your IDE or text editor of choice and try the following:", "The config dictionary changed the defaults for the values above. You can see how we can influence the number of layers and nodes in the network by nesting a dictionary called model in the config dictionary. Once we've specified our configuration, calling the train() method on our trainer object will send the environment to the workers and begin collecting data. Once enough data is collected (1,000 samples according to our settings above) the model will update and send the output to a new dictionary called results.", "If you want to run multiple updates, you can set up a training loop to continuously call the train() method for a given number of iterations or until some other threshold has been reached.", "OpenAI Gym and all of its extensions are great, but if you\u2019re looking for novel applications of RL or to use it in your company, you\u2019re going to need to work with a custom environment.", "Unfortunately, the current version of Ray (0.9) explicitly states that it is not compatible with the gym registry. Thankfully, it isn\u2019t too difficult to put together a helper function to get custom gym environments to work with Ray.", "Let\u2019s assume you have some environment called MyEnv-v0 that is properly registered so that you can invoke it with gym.make('MyEnv-v0') like you would with any other gym environment (if you haven't already, you can check out my step-by-step process on setting up environments here).", "To call that custom environment from Ray, you need to wrap it in a function that will return the environment class, not an instantiated object. The best way I\u2019ve found to do this is with a create_env() helper function:", "From here, you can set up your agent and train it on this new environment with only a slight modification to the trainer.", "Note that above, we call the environment with the env_creator, everything else remains the same.", "If you\u2019re used to building your own models from the environment to the networks and algorithms, then there are some features you need to be cognizant of when working with Ray.", "First, Ray adheres to the OpenAI Gym API meaning that your environments need to have step() and reset() methods as well as carefully specified observation_space and action_space attributes. I had always been a bit lazy with respect to these last two, because I could simply define my network input and output dimensions and not have to regard the range of input values, for example, that the gym.spaces methods require. Ray checks all the inputs to ensure that they fall within that specified range (I spent too much time debugging runs before realizing that the low value on my gym.spaces.Box was set to 0, but the environment was returning values on the order of -1e-17 and causing it to crash).", "When setting up your action and observation spaces, stick to Box, Discrete, and Tuple. The MultiDiscrete and MultiBinary don't work (currently) and will cause the run to crash. Instead, wrap Box or Discrete spaces in the Tuple function.", "Take advantage of custom pre-processing when you can. Ray makes assumptions about your state inputs, which usually work just fine, but it also enables you to customize the pre-processing steps which may help your training.", "Ray can greatly speed up training and make it far easier to get started with deep reinforcement learning. RLlib isn\u2019t the end (we just scratched the surface of its capabilities here anyway), it has a powerful cousin called Tune which enables you to adjust the hyperparameters of your model and manages all of the important data collection and back-end work for you. Make sure you check back for updates on how to bring this library into your work process.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI/ML researcher writing about technology, economics, and business. Connect with me: https://bit.ly/2scbU1P"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6d31ee21c96c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b----6d31ee21c96c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d31ee21c96c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d31ee21c96c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@the_gerbs1?utm_source=medium&utm_medium=referral", "anchor_text": "Jean Gerber"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.datahubbs.com/policy-gradients-and-advantage-actor-critic/", "anchor_text": "Advantage Actor Critic Algorithm (A2C)"}, {"url": "https://github.com/ray-project/ray/tree/master/rllib/agents", "anchor_text": "long list"}, {"url": "https://www.datahubbs.com/two-headed-a2c-network-in-pytorch/", "anchor_text": "one neural network with multiple output heads"}, {"url": "https://www.datahubbs.com/reinforcement-learning-is-wrong-for-your-business-application/", "anchor_text": "use it in your company"}, {"url": "https://ray.readthedocs.io/en/latest/rllib-env.html", "anchor_text": "explicitly states"}, {"url": "https://www.datahubbs.com/building-custom-gym-environments-for-rl/", "anchor_text": "step-by-step process on setting up environments here"}, {"url": "https://stackoverflow.com/questions/58551029/rllib-use-custom-registered-environments/60792871#60792871", "anchor_text": "The best way I\u2019ve found to do this"}, {"url": "http://gym.openai.com/docs/", "anchor_text": "OpenAI Gym API"}, {"url": "https://github.com/ray-project/ray/issues/6372", "anchor_text": "currently"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6d31ee21c96c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6d31ee21c96c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----6d31ee21c96c---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/parallel-computing?source=post_page-----6d31ee21c96c---------------parallel_computing-----------------", "anchor_text": "Parallel Computing"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----6d31ee21c96c---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d31ee21c96c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----6d31ee21c96c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d31ee21c96c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----6d31ee21c96c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d31ee21c96c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6d31ee21c96c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6d31ee21c96c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6d31ee21c96c--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://christiandhubbs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "657 Followers"}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd00e5d4c3897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c&newsletterV3=8c5ed989fb1b&newsletterV3Id=d00e5d4c3897&user=Christian+Hubbs&userId=8c5ed989fb1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}