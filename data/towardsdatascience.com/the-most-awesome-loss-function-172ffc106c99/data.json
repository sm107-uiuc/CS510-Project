{"url": "https://towardsdatascience.com/the-most-awesome-loss-function-172ffc106c99", "time": 1683013729.653468, "path": "towardsdatascience.com/the-most-awesome-loss-function-172ffc106c99/", "webpage": {"metadata": {"title": "The Most Awesome Loss Function. Paper Review: General and Adaptive\u2026 | by Saptashwa Bhattacharyya | Towards Data Science", "h1": "The Most Awesome Loss Function", "description": "Recently, I came across the amazing paper presented in CVPR 2019 by Jon Barron about developing a robust and adaptive loss function for Machine Learning problems. This post is a review of that paper\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1701.03077", "anchor_text": "amazing paper", "paragraph_index": 0}, {"url": "https://jonbarron.info/", "anchor_text": "Jon Barron", "paragraph_index": 0}, {"url": "https://github.com/jonbarron/robust_loss_pytorch", "anchor_text": "Jon Barron\u2019s GitHub", "paragraph_index": 10}, {"url": "https://pytorch.org/", "anchor_text": "pytorch", "paragraph_index": 12}, {"url": "https://pypi.org/project/celluloid/", "anchor_text": "Celluloid", "paragraph_index": 17}, {"url": "https://github.com/jonbarron/robust_loss_pytorch", "anchor_text": "Jon\u2019s GitHub", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1701.03077.pdf", "anchor_text": "\u201cA General and Adaptive Robust Loss Function\u201d", "paragraph_index": 19}, {"url": "https://github.com/jonbarron/robust_loss_pytorch", "anchor_text": "Jon Barron\u2019s GitHub", "paragraph_index": 20}, {"url": "https://github.com/suvoooo/Machine_Learning/blob/master/AdaptiveLoss.ipynb", "anchor_text": "GitHub Link", "paragraph_index": 21}], "all_paragraphs": ["Recently, I came across the amazing paper presented in CVPR 2019 by Jon Barron about developing a robust and adaptive loss function for Machine Learning problems. This post is a review of that paper along with some necessary concepts and, it will also contain an implementation of the loss function on a simple regression problem.", "Consider one of the most used errors in machine learning problems- Mean Squared Error (MSE). As you know it is of the form (y-x)\u00b2. One of the key characteristics of the MSE is that its high sensitivity towards large errors compared to small ones. A model trained with MSE will be biased towards reducing the largest errors. For example, a single error of 3 units will be given same importance to 9 errors of 1 unit.", "I created an example using Scikit-Learn to demonstrate how the fit varies in a simple data-set with and without taking the effect of outliers.", "As you can see the fit line including the outliers get heavily influenced by the outliers but, the optimization problem should require the model to get more influenced by the inliers. At this point you can already think about Mean Absolute Error (MAE) as better choice than MSE, due to less sensitivity to large errors. There are various types of robust losses (like MAE) and for a particular problem we may need to test various losses. Wouldn\u2019t it be amazing to test various loss functions on the fly while training a network? The main idea of the paper is to introduce a generalized loss function where the robustness of the loss function can be varied and, this hyperparameter can be trained while training the network, to improve performance. It is way less time consuming than finding the best loss say, by performing grid-search cross-validation. Let\u2019s get started with the definition below \u2014", "The general form of the robust and adaptive loss is as below \u2014", "\u03b1 controls the robustness of the loss function. c can be considered as a scale parameter which controls the size of the bowl near x=0. Since \u03b1 acts as hyperparameter, we can see that for different values of \u03b1 the loss function takes familiar forms. Let\u2019s see below \u2014", "The loss function is undefined at \u03b1 = 0 and 2, but taking the limit we can make approximations. From \u03b1 =2 to \u03b1 =1 the loss smoothly makes a transition from L2 loss to L1 loss. For different values of \u03b1 we can plot the loss function to see how it behaves (fig. 2).", "We can also spend some time with the first derivative of this loss function because the derivative is needed for gradient-based optimization. For various values of \u03b1 the derivatives w.r.t x are shown below. In figure 2, I have also plotted the derivatives along with the loss function for different \u03b1.", "The figure below is very important to understand the behaviour of this loss function and its derivative. For the plots below, I have fixed the scale parameter c to 1.1. When x = 6.6, we can consider this as like x = 6\u00d7 c. We can draw the following inferences about the loss and its derivative \u2014", "I have also plotted below the surface plots of robust loss and its derivative for different values of \u03b1.", "Since we have gone through the basics and properties of the robust and adaptive loss function, let us put this into action. Codes used below are just slightly modified from what can be found in Jon Barron\u2019s GitHub repository. I have also created an animation to depict how the adaptive loss finds the best-fit line as the number of iteration increases.", "Rather than cloning the repository and working with it, we can install it locally using pip in Colab.", "We create a simple linear dataset including normally distributed noise and also outliers. Since the library uses pytorch, we convert the numpy arrays of x, y to tensors using torch.", "Next we define a Linear regression class using pytorch modules as below-", "Next, we fit a linear regression model to our data but, first the general form of the loss function is used. Here we use a fixed value of \u03b1 (\u03b1 = 2.0) and it remains constant throughout the optimization procedure. As we have seen for \u03b1 = 2.0 the loss function replicates L2 loss and this as we know is not optimal for problems including outliers. For optimization we use the Adam optimizer with a learning rate of 0.01.", "Using the general form of the robust loss function and a fixed value of \u03b1, we can obtain the fit line. The original data, true line (line with the same slope and bias used to generate data-points excluding the outliers) and fit line are plotted below in fig. 4.", "The general form of the loss function doesn\u2019t allow \u03b1 to change and thus we have to fine tune the \u03b1 parameter by hand or by performing a grid-search. Also, as the figure above suggests that the fit is affected by the outliers because we used L2 loss. This is the general scenario but, what happens if we use the adaptive version of the loss function ? We call the adaptive loss module and just initialize \u03b1 and let it adapt itself at each iteration step.", "Using this, and also some extra bit of code using Celluloid module, I created the animation below (figure 5). Here, you clearly see, how with increasing iterations adaptive loss finds the best fit line. This is close to the true line and it is negligibly affected by the outliers.", "We have seen how the robust loss including an hyperparameter \u03b1 can be used to find the best loss-function on the fly. The paper also demonstrates how the robustness of the loss-function with \u03b1 as continuous hyperparameter can be introduced to classic computer vision algorithms. Examples of implementing adaptive loss for Variational Autoencoder and Monocular depth estimations are shown in the paper and these codes are also available in Jon\u2019s GitHub. However, the most fascinating part for me was the motivation and step by step derivation of the loss function as described in the paper. It\u2019s easy to read so, I suggest to take a look at the paper!", "[1] \u201cA General and Adaptive Robust Loss Function\u201d; J. Barron, Google Research.", "[2] Robust-Loss: Linear regression example; Jon Barron\u2019s GitHub.", "[3] Surface plot of robust loss and animation: GitHub Link.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F172ffc106c99&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----172ffc106c99--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----172ffc106c99--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://saptashwa.medium.com/?source=post_page-----172ffc106c99--------------------------------", "anchor_text": ""}, {"url": "https://saptashwa.medium.com/?source=post_page-----172ffc106c99--------------------------------", "anchor_text": "Saptashwa Bhattacharyya"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a3c3c477239&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=post_page-9a3c3c477239----172ffc106c99---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F172ffc106c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F172ffc106c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.flickr.com/photos/suvob/50179723953/", "anchor_text": "Toy Train"}, {"url": "https://arxiv.org/abs/1701.03077", "anchor_text": "amazing paper"}, {"url": "https://jonbarron.info/", "anchor_text": "Jon Barron"}, {"url": "https://github.com/jonbarron/robust_loss_pytorch", "anchor_text": "Jon Barron\u2019s GitHub"}, {"url": "https://pytorch.org/", "anchor_text": "pytorch"}, {"url": "https://pypi.org/project/celluloid/", "anchor_text": "Celluloid"}, {"url": "https://github.com/jonbarron/robust_loss_pytorch", "anchor_text": "Jon\u2019s GitHub"}, {"url": "https://arxiv.org/pdf/1701.03077.pdf", "anchor_text": "\u201cA General and Adaptive Robust Loss Function\u201d"}, {"url": "https://github.com/jonbarron/robust_loss_pytorch", "anchor_text": "Jon Barron\u2019s GitHub"}, {"url": "https://github.com/suvoooo/Machine_Learning/blob/master/AdaptiveLoss.ipynb", "anchor_text": "GitHub Link"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----172ffc106c99---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----172ffc106c99---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/loss?source=post_page-----172ffc106c99---------------loss-----------------", "anchor_text": "Loss"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----172ffc106c99---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F172ffc106c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=-----172ffc106c99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F172ffc106c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=-----172ffc106c99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F172ffc106c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----172ffc106c99--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F172ffc106c99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----172ffc106c99---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----172ffc106c99--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----172ffc106c99--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----172ffc106c99--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----172ffc106c99--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----172ffc106c99--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----172ffc106c99--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----172ffc106c99--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----172ffc106c99--------------------------------", "anchor_text": ""}, {"url": "https://saptashwa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://saptashwa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Saptashwa Bhattacharyya"}, {"url": "https://saptashwa.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://www.linkedin.com/in/saptashwa", "anchor_text": "https://www.linkedin.com/in/saptashwa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a3c3c477239&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=post_page-9a3c3c477239--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F423a8008308d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-most-awesome-loss-function-172ffc106c99&newsletterV3=9a3c3c477239&newsletterV3Id=423a8008308d&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}