{"url": "https://towardsdatascience.com/qrnn-a-potential-competitor-to-the-transformer-86b5aef6c137", "time": 1683014810.695393, "path": "towardsdatascience.com/qrnn-a-potential-competitor-to-the-transformer-86b5aef6c137/", "webpage": {"metadata": {"title": "QRNN: A Potential Competitor to the Transformer | by Rohan Jagtap | Towards Data Science", "h1": "QRNN: A Potential Competitor to the Transformer", "description": "Recurrent Neural Networks (RNNs) have been in the sequence modeling business for a long time. But RNNs are slow; they process one token at a time. Moreover, the recurrent architecture adds a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "Quasi-Recurrent Neural Networks", "paragraph_index": 1}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Colah\u2019s Blog", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1603.07285v1", "anchor_text": "A Guide to Convolution Arithmetic for Deep Learning", "paragraph_index": 5}, {"url": "https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm", "anchor_text": "broadcasted", "paragraph_index": 24}], "all_paragraphs": ["Recurrent Neural Networks (RNNs) have been in the sequence modeling business for a long time. But RNNs are slow; they process one token at a time. Moreover, the recurrent architecture adds a limitation of fixed-length encoding vectors for the complete sequence. To overcome these issues, architectures like CNN-LSTM, Transformer, QRNNs burgeoned.", "In this article, we\u2019ll be discussing the QRNN model proposed in the paper, \u201cQuasi-Recurrent Neural Networks.\u201d It is essentially an approach for adding convolution to recurrence and recurrence to convolution. You will get this as you proceed through the article.", "LSTM is the most well-known variant of RNNs. The red blocks are linear functions or matrix multiplications, and the blue ones are parameter-less element-wise blocks. An LSTM-cell applies gated functions (input, forget, output) to obtain the output and a memory element called the hidden state. This hidden state contains contextual information of the entire sequence. Since a single vector encodes the complete sequence, LSTMs cannot remember long-term dependencies. Moreover, the computation at each timestep is dependent on the hidden state of the previous timestep, i.e., LSTM computes one timestep at a time. Hence, the computations cannot be done in parallel.", "Colah\u2019s Blog is, by far, one of the best explanations for RNNs (in my opinion). Consider giving it a read if you\u2019re interested in knowing the math behind LSTM.", "CNN, on the other hand, captures spatial features (mostly used in images). The red blocks are convolution operations, and blue blocks are parameter-less pooling operations. CNNs use kernels (or filters) to capture correspondence between features using a sliding window. This overcomes the fixed-length hidden representation (and thus, the long term dependency issue) as well as the lack of parallelism limitation of the RNNs. But, CNNs show no regard to the temporal nature of a sequence, i.e., time invariance. The pooling layers simply reduce the dimensionality of the channels without considering the sequence order information.", "A Guide to Convolution Arithmetic for Deep Learning is one of the best papers on convolution operations involved in DL. Worth a read!", "QRNN addresses the drawbacks of both the standard architectures. It allows parallel processing and captures long term dependencies like CNN, and also allows the output to depend on the order of tokens in the sequence like RNN.", "So, to start with, the QRNN architecture has 2 components corresponding to the Convolutional (red) and Pooling (blue) components in CNN.", "The convolutional component operates with the following:", "The convolution operation is applied in parallel over the sequence as well as the mini-batch.", "To preserve the causality of the model (i.e., only the past tokens should predict the future), a concept called masked-convolutions is used. That is, the input sequence is padded to the left by \u2018kernel_size - 1\u2019 zeros. So, only \u2018sequence_length - kernel_size + 1\u2019 past tokens may predict a given token. For a better intuition, refer the figure below:", "Next, we use extra kernel banks based on our pooling function (to be discussed in the next section), to get gated vectors like in LSTM:", "Here, * is the convolution operation; Z is the output discussed above (call it the \u2018input gate\u2019 output); F is the \u2018forget gate\u2019 output obtained using the extra kernel bank W_f; O is the \u2018output gate\u2019 output obtained using the extra kernel bank W_o.", "Fun fact: As discussed above, these convolutions are applied over the past \u2018sequence_length - kernel_size + 1\u2019 tokens only. So, if we take kernel_size = 2, we get LSTM-like equations:", "Pooling, in general, is a parameter-less function that captures important features among the convoluted features. In case of images, usually, Max-Pooling and Average Pooling are used. However, we cannot simply take the average or the max between features in case of sequences. It needs to have some recurrence. Hence, the QRNN paper has proposed pooling functions inspired by the element-wise gated architecture in the traditional LSTM-cell. It is essentially a parameter-less function that will mix the hidden states across the timesteps.", "The simplest option is \u201cdynamic average pooling,\u201d which uses just the forget gate (hence termed f-pooling):", "where \u2299 is element-wise matrix multiplication.", "As you can see, it is more or less a \u2018Moving Average\u2019 of the output with the forget gate as the parameter.", "Another option is to use the forget gate as well as the output gate (hence, fo-pooling):", "Or the pooling may additionally have a dedicated input gate (ifo-pooling):", "After examining various recurrent dropout schemes, QRNN uses an extension to a scheme called \u2018zone out.\u2019 It essentially selects a random subset of channels to dropout at each timestep, and for those channels, it simply copies the current channel value to the next time step without any modifications.", "Conveniently, this is equivalent to stochastically setting a subset of the QRNN\u2019s forget gate channels to 1, or applying dropout on 1\u2212F.", "The DenseNet architecture suggests having skip-connections between each layer and every layer ahead of it, contrary to the convention of having skip-connections over subsequent layers. Thus, there would be L(L - 1) skip connections for a network with L layers. This helps gradient flow and convergence, but accounts for quadratic space.", "In a regular RNN-based seq2seq model, we simply initialize the decoder with the encoder\u2019s last hidden state and then modify it further for the decoder sequence. Well, we cannot do this for the recurrent pooling layers as here, the encoder state wouldn\u2019t be able to contribute much to the decoder\u2019s hidden state. Hence, the authors have proposed a modified decoder architecture.", "The last hidden state (hidden state of the last token) from the encoder is projected linearly (linear layer), and added (broadcasted as the encoder vector is smaller) to the convolution output of each timestep of the decoder layer before applying any activations:", "~ means belonging to the encoder; V is the linear weight applied to the last encoder hidden state.", "Attention is applied only to the last hidden state of the decoder.", "where s is the encoder\u2019s sequence length, t is the decoder\u2019s sequence length, L means the last layer.", "First, the dot product of the un-gated last layer hidden states of the decoder is taken with the last layer encoder hidden states. This will result in a matrix of shape (t, s). Softmax is taken over s, and this score is used to obtain the attentional sum, k_t of shape (t, hidden_dim). k_t is then used alongside c_t to obtain the gated last layer hidden state for the decoder.", "QRNN achieves comparable and, in some cases, slightly better results than the LSTM architectures with up to 17x faster computations.", "Recently a model, pQRNN, which is based on QRNN, has achieved comparable results to BERT on sequence classification with just 1.3M parameters (opposed to BERT, which is 440M parameters):", "We discussed the novel QRNN architecture in depth. We saw how it adds recurrence to a convolution-based model and hence, speeds up sequence modeling. The speed and performance of QRNN definitely makes us reconsider transformers for some NLP tasks.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F86b5aef6c137&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://rojagtap.medium.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----86b5aef6c137---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86b5aef6c137&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&user=Rohan+Jagtap&userId=39646f947a4c&source=-----86b5aef6c137---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86b5aef6c137&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&source=-----86b5aef6c137---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@bradencollum?utm_source=medium&utm_medium=referral", "anchor_text": "Braden Collum"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "Quasi-Recurrent Neural Networks"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Colah\u2019s Blog"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1603.07285v1", "anchor_text": "A Guide to Convolution Arithmetic for Deep Learning"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1608.06993", "anchor_text": "DenseNet Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm", "anchor_text": "broadcasted"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN Paper"}, {"url": "https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html", "anchor_text": "Google AI Blog"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "Quasi-Recurrent Neural NetworksRecurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's\u2026arxiv.org"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----86b5aef6c137---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----86b5aef6c137---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----86b5aef6c137---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----86b5aef6c137---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----86b5aef6c137---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86b5aef6c137&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&user=Rohan+Jagtap&userId=39646f947a4c&source=-----86b5aef6c137---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86b5aef6c137&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&user=Rohan+Jagtap&userId=39646f947a4c&source=-----86b5aef6c137---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86b5aef6c137&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----86b5aef6c137---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=-----86b5aef6c137---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Written by Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "465 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----86b5aef6c137---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrnn-a-potential-competitor-to-the-transformer-86b5aef6c137&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=-----86b5aef6c137---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gpt-3-explained-19e5f2bd3288?source=author_recirc-----86b5aef6c137----0---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=author_recirc-----86b5aef6c137----0---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=author_recirc-----86b5aef6c137----0---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Rohan Jagtap"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----86b5aef6c137----0---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/gpt-3-explained-19e5f2bd3288?source=author_recirc-----86b5aef6c137----0---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "GPT-3 ExplainedUnderstanding Transformer-Based Self-Supervised Architectures"}, {"url": "https://towardsdatascience.com/gpt-3-explained-19e5f2bd3288?source=author_recirc-----86b5aef6c137----0---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "\u00b711 min read\u00b7Jan 12, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F19e5f2bd3288&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-explained-19e5f2bd3288&user=Rohan+Jagtap&userId=39646f947a4c&source=-----19e5f2bd3288----0-----------------clap_footer----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gpt-3-explained-19e5f2bd3288?source=author_recirc-----86b5aef6c137----0---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19e5f2bd3288&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-explained-19e5f2bd3288&source=-----86b5aef6c137----0-----------------bookmark_preview----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----86b5aef6c137----1---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----86b5aef6c137----1---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----86b5aef6c137----1---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----86b5aef6c137----1---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----86b5aef6c137----1---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----86b5aef6c137----1---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----86b5aef6c137----1---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----86b5aef6c137----1-----------------bookmark_preview----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----86b5aef6c137----2---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----86b5aef6c137----2---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----86b5aef6c137----2---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----86b5aef6c137----2---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----86b5aef6c137----2---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----86b5aef6c137----2---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----86b5aef6c137----2---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----86b5aef6c137----2-----------------bookmark_preview----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4?source=author_recirc-----86b5aef6c137----3---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=author_recirc-----86b5aef6c137----3---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=author_recirc-----86b5aef6c137----3---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/dataseries?source=author_recirc-----86b5aef6c137----3---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "DataSeries"}, {"url": "https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4?source=author_recirc-----86b5aef6c137----3---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "OpenAI GPT: Generative Pre-Training for Language UnderstandingUnderstanding Transformer-Based Self-Supervised Architectures"}, {"url": "https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4?source=author_recirc-----86b5aef6c137----3---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": "\u00b75 min read\u00b7Jul 4, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdataseries%2Fbbbdb42b7ff4&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdataseries%2Fopenai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4&user=Rohan+Jagtap&userId=39646f947a4c&source=-----bbbdb42b7ff4----3-----------------clap_footer----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4?source=author_recirc-----86b5aef6c137----3---------------------c1119112_f6ea_4abb_811f_82c67ae19e91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbbbdb42b7ff4&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdataseries%2Fopenai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4&source=-----86b5aef6c137----3-----------------bookmark_preview----c1119112_f6ea_4abb_811f_82c67ae19e91-------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "See all from Rohan Jagtap"}, {"url": "https://towardsdatascience.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----86b5aef6c137----0-----------------bookmark_preview----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----1-----------------clap_footer----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----86b5aef6c137----1-----------------bookmark_preview----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Babar M Bhatti"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Essential Guide to Foundation Models and Large Language ModelsThe term Foundation Model (FM) was coined by Stanford researchers to introduce a new category of ML models. They defined FMs as models\u2026"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "\u00b714 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&user=Babar+M+Bhatti&userId=10dee34829b&source=-----27dab58f7404----0-----------------clap_footer----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----86b5aef6c137----0---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&source=-----86b5aef6c137----0-----------------bookmark_preview----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----86b5aef6c137----1---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----86b5aef6c137----1-----------------bookmark_preview----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----86b5aef6c137----2---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----86b5aef6c137----2---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----86b5aef6c137----2---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----86b5aef6c137----2---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----86b5aef6c137----2---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----86b5aef6c137----2---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----2-----------------clap_footer----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----86b5aef6c137----2---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----86b5aef6c137----2-----------------bookmark_preview----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----86b5aef6c137----3---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----86b5aef6c137----3---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----86b5aef6c137----3---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----86b5aef6c137----3---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----86b5aef6c137----3---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----86b5aef6c137----3---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----3-----------------clap_footer----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----86b5aef6c137----3---------------------dff10da8_85f6_4038_a64f_bc08a5e32664-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----86b5aef6c137----3-----------------bookmark_preview----dff10da8_85f6_4038_a64f_bc08a5e32664-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----86b5aef6c137--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}