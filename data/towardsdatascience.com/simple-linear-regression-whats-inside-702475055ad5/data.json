{"url": "https://towardsdatascience.com/simple-linear-regression-whats-inside-702475055ad5", "time": 1683010292.267765, "path": "towardsdatascience.com/simple-linear-regression-whats-inside-702475055ad5/", "webpage": {"metadata": {"title": "Simple Linear Regression: What\u2019s inside? | by Sampath Routu | Towards Data Science", "h1": "Simple Linear Regression: What\u2019s inside?", "description": "Regression is a statistical approach that suggests predicting a dependent variable (goal feature) with the help of other independent variables (data). Regression is one of the most known and\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Regression is a statistical approach that suggests predicting a dependent variable (goal feature) with the help of other independent variables (data). Regression is one of the most known and understood statistical methods.", "Linear regression is a model that assumes a linear relationship between its dependent and independent variables. Linear regression further branches out to Simple Linear Regression (SLR) and Multiple Linear Regression (MLR). We will explore Single Linear Regression, regression with one dependent and one independent variable, because of its simplicity. SLR\u2019s math is the base of many other machine learning models.", "Here I will elaborate on Simple Linear Regression to gain intuition on how it works. I will use an NBA Game Score dataset (link below) to demonstrate SLR and finally compare it to Scikit-learn\u2019s Linear Regression model.", "To understand SLR, let\u2019s break down the concepts we must go through", "The slope intercept form of a line is Y= MX+B.", "Y is the dependent variable (Goal), X is the independent variable (Data), M and B are the characteristics of the line. Slope (M) gives us how related variables X and Y are and Intercept (B) gives us information on the value of the dependent variable when the rate of change is eliminated.", "In SLR the equation is written as y = b0 + x b1. b0 and b1 are the intercept and slope respectively. They are determined by the given formulas below to find the line of best fit.", "But, this is not always the case in regression. Let\u2019s understand why in the gradient descent section. If you are curious about how we stumbled upon these, do check out the optional section.", "The loss function is a metric that suggests how much the predicted value deviates from the actual value. There are plenty of loss functions available, we will look at Mean Squared Error (MSE).", "MSE, as the name suggests, squares the difference between the actual and predicted value for each record, sums it up and divides it by the number of records. Our goal is to find a model that yields the smallest loss.", "Gradient descent is an optimizing algorithm that updates the parameters iteratively to find the model with the slightest loss. The loss function for a model with one or two parameters can be partially differentiated to find the minimum. But as our dimensions increase it\u2019s hard to visualize the parameters, let alone the eigenvalues for each solution. Due to multiple occurrences of local minima, we will have to traverse through all the combinations of eigenvalues to make sure we found the global minima. Although the global minima problem is not fully rectified, Gradient Descent helps to find the minima for models with higher orders.", "But we haven\u2019t explored the base of the problem: the loss function. MSE (being a quadratic function) guarantees us there will always be a point on the curve whose gradient is zero, but there are loss functions that do not guarantee a point with zero gradient or the point with a zero gradient might not always be the global minima. To overcome this problem gradient descent is employed.", "Specific to our situation, we can choose to find the coefficients by the formulae presented above or we can start with random non zero values and let it work its way to the best fit. The mathematical significance of the gradient descent algorithm deserves an article for itself. For now, I will go through the intuition required to implement the algorithm. The mathematical approach is similar to that of the coefficients, I figured it would be redundant to include it (I will link it in the end if you are curious).", "Now, let\u2019s see what gradient descent is all about. Imagine a person is hiking downhill with no optical senses; the person\u2019s goal is to reach the bottom of the valley. Intuitively he takes a step forward, if the slope is downward he would continue to move until he encounters a change in slope. Once the person feels no elevation while moving, he/she stops.", "But as expressed above, it wouldn\u2019t make sense to take steps of a defined length and then evaluate for course correction, as the person could have passed the minimum only to realize he/she moved in the wrong direction. This is where the learning rate comes into the picture. It penalizes large steps to make sure the person does not take a step over the minimum.", "How does the person choose the learning rate?", "Unfortunately, there\u2019s no \u201cone fits all \u201d form for the learning rate. One way we could get a rough estimate on the value is by trial and error. Issues we must be on the lookout for are high and low learning rates. Both can be computationally expensive, so always run the model for a few iterations in the beginning to examine the movement of the loss. the wasted computational power and time.", "One analogy that fits with a high learning rate is a metallic ball bearing and bowl. When a ball is released from the brim of the bowl, its velocity keeps increasing as the direction of the force is towards the minimum. When it passes the minimum, the direction of the ball will be away from the minimum, but the direction of the force acting on the ball will be the opposite. This way, through some lossy oscillations, it finally attains stable equilibrium at the bottom of the bowl. The lossy oscillations are the wasted computational power and time.", "Now, let\u2019s merge this with SLR. The person we were referring to are b0 and b1 coefficients. The valley we saw is the curve of MSE plotted against b0 and b1 parameters. Learning Rate(alpha) gives us the step size on how much to modify the parameters without having to skip the minimum on each pass.", "Although not crucial, it\u2019s fun to explore the mechanics of the model. Let\u2019s first establish the basic equations and lingo for deriving the equations. Before we start, make sure you are familiar with the basics of calculus.", "Now that we\u2019re all set with the equations, let\u2019s dive into b0 and b1 are obtained. Now if we plot any of the coefficients (keeping the other constant) against E, it will look something like this", "The minimum of the function can be found by partially differentiating the function w.r.t the coefficient and equating the gradient to zero. Now our goals are", "Now to find the minimum equate the gradient to zero.", "Although we did find the value of b0, it is dependent on b1.", "Substitute the value of b0 and partially differentiate", "Similar to b0, equate the gradient to zero", "This checks both our goals; we found an equation for b1 and it is only dependent on data that is available to us. We were able to find the values using the basic properties of curves.", "First, let\u2019s process the data and then implement the models using", "Using standard python libraries, let\u2019s import our data and visualize the spread.", "Implementing the model using gradient descent", "Now plot the line from the obtained parameters.", "I stored every value for a few hundred iterations with a small learning rate to visualize how the model matures. Using pyplot I saved all the plots to make a gif.", "Let\u2019s now plot the loss history of the model.", "In the comparison plot above, we can see the LinearRegression line and the Coefficient line overlap. But on the other hand, the gradient descent line is slightly skewed to the LinearRegression line. What does this mean? Let\u2019s investigate.", "Although there is a contrast in the loss of the lines, the loss between the predictions and the actual values is quite similar for all the models. We can safely infer the gradient descent line- although it does not have the same parameters- has positioned itself in a way that is approximately equal to the ideal condition.", "Finally, this is what\u2019s inside simple linear regression. Thank you.", "All pictures other than the cited ones are produced by the author", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F702475055ad5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----702475055ad5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----702475055ad5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sampajo?source=post_page-----702475055ad5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sampajo?source=post_page-----702475055ad5--------------------------------", "anchor_text": "Sampath Routu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1390c63937b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&user=Sampath+Routu&userId=1390c63937b0&source=post_page-1390c63937b0----702475055ad5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F702475055ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F702475055ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.onlinemath4all.com/images/slopeinterceptform1.png", "anchor_text": "onlinemath4all"}, {"url": "https://www.kdnuggets.com/2018/06/intuitive-introduction-gradient-descent.html", "anchor_text": "kdnuggets"}, {"url": "https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/images/lr1.png", "anchor_text": "deeplearningwizard"}, {"url": "https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/", "anchor_text": "How To Implement Simple Linear Regression From Scratch With Python - Machine Learning MasteryLinear regression is a prediction method that is more than 200 years old. Simple linear regression is a great first\u2026machinelearningmastery.com"}, {"url": "https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931", "anchor_text": "Linear Regression using Gradient DescentIn this tutorial you can learn how the gradient descent algorithm works and implement it from scratch in python. First\u2026towardsdatascience.com"}, {"url": "https://www.kaggle.com/ionaskel/nba-games-stats-from-2014-to-2018", "anchor_text": "NBA Team Game Stats from 2014 to 2018Stats from every game during 2014 - 2018 nba periodwww.kaggle.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----702475055ad5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----702475055ad5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----702475055ad5---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F702475055ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&user=Sampath+Routu&userId=1390c63937b0&source=-----702475055ad5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F702475055ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&user=Sampath+Routu&userId=1390c63937b0&source=-----702475055ad5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F702475055ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----702475055ad5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F702475055ad5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----702475055ad5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----702475055ad5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----702475055ad5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----702475055ad5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----702475055ad5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----702475055ad5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----702475055ad5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----702475055ad5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----702475055ad5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sampajo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sampajo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sampath Routu"}, {"url": "https://medium.com/@sampajo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "109 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1390c63937b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&user=Sampath+Routu&userId=1390c63937b0&source=post_page-1390c63937b0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff7a908ad7aa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-linear-regression-whats-inside-702475055ad5&newsletterV3=1390c63937b0&newsletterV3Id=f7a908ad7aa2&user=Sampath+Routu&userId=1390c63937b0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}