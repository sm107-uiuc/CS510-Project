{"url": "https://towardsdatascience.com/how-to-turn-text-into-features-478b57632e99", "time": 1683016181.0755289, "path": "towardsdatascience.com/how-to-turn-text-into-features-478b57632e99/", "webpage": {"metadata": {"title": "How to turn Text into Features. A comprehensive guide into using NLP\u2026 | by Tiago Duque | Towards Data Science", "h1": "How to turn Text into Features", "description": "Imagine you\u2019ve been tasked with the activity of building a Sentiment Analysis tool for your company product reviews. As a seasoned Data Scientist, you built many insights about future sale\u2026"}, "outgoing_paragraph_urls": [{"url": "https://web.archive.org/web/20120104170705/http://oxforddictionaries.com/words/the-oec-facts-about-the-language", "anchor_text": "Oxford English Corpus", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "Wikipedia page", "paragraph_index": 46}, {"url": "https://gist.github.com/Sirsirious/b28c676ecb95883e41b7e46ecd9cf68d", "anchor_text": "ere", "paragraph_index": 56}, {"url": "https://www.coursera.org/learn/probabilistic-models-in-nlp", "anchor_text": "https://www.coursera.org/learn/probabilistic-models-in-nlp", "paragraph_index": 69}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "Principal Component Analysis \u2014 PCA", "paragraph_index": 71}, {"url": "https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/", "anchor_text": "IBM Research Blog", "paragraph_index": 71}, {"url": "https://www.google.com/imgres?imgurl=https%3A%2F%2Fjournals.plos.org%2Fplosone%2Farticle%2Ffigure%2Fimage%3Fdownload%26size%3Dlarge%26id%3Dinfo%3Adoi%2F10.1371%2Fjournal.pone.0231189.g008&imgrefurl=https%3A%2F%2Fjournals.plos.org%2Fplosone%2Farticle%3Fid%3D10.1371%2Fjournal.pone.0231189&tbnid=Rzp3wYmO7VhVLM&vet=12ahUKEwiIutbFrezsAhV3M7kGHRh2BMEQMyhGegQIARBS..i&docid=imJ6d9GoTZJtLM&w=1479&h=1020&q=word%20embeddings%20example&ved=2ahUKEwiIutbFrezsAhV3M7kGHRh2BMEQMyhGegQIARBS", "anchor_text": "Journal Plos One", "paragraph_index": 73}, {"url": "https://medium.com/analytics-vidhya/nlp-preprocessing-pipeline-what-when-why-2fc808899d1f", "anchor_text": "preprocessing", "paragraph_index": 76}], "all_paragraphs": ["Imagine you\u2019ve been tasked with the activity of building a Sentiment Analysis tool for your company product reviews. As a seasoned Data Scientist, you built many insights about future sale predictions and was even able to classify customers based on their purchase behavior.", "But now, you\u2019re intrigued: you have this bunch of text entries and have to turn them into features for a Machine Learning model. How can that be done? That\u2019s a common question when Data Scientists meet text for the first time.", "As simple as it may look for experienced NLP Data Scientists, turning text into features is not that trivial for newcomers in the area. The purpose of this article is to provide a guide into turning Text to Features, as a continuation to the NLP Series that I\u2019ve been building for the last months (and its been a while since the last article, I know).", "Previously, I\u2019ve talked about several steps in the NLP Preprocessing Pipeline. Now, this is the last step of the Preprocessing Pipeline, when your dearly curated text is finally turned into usable features for Machine Learning Models (if that is your purpose \u2014 there can be NLP without Machine Learning, a topic for another time).", "As usual, more than just explaining how to do it, I\u2019ll present three of the most commonly used techniques of modeling: the Bag of Words Model, the Model based on the TF-IDF Algorithm and the Word2Vec Models. Let\u2019s get started with the discussion.", "For those of you that are not used to this word, let me digress a little over it. Feature is the name given to selected or treated data that is prepared to be used as input to algorithms (usually Machine Learning Algorithms). Features can be things like the price of a house, the RGB value of a pixel or, in our case, the representation of a word.", "There\u2019s even a cool skill called \u201cfeature engineering\u201d where Data Scientists work over data to get features from data. These features may even not be explicit in data, but can be obtained by modifying existing data or adding new data to make it more complete, hence helping achieve more robust decisions.", "So, ultimately, our objective is to get the raw data (text) and turn into features (something computer algorithms can work with).", "The techniques used to turn Text into features can be referred to as \u201cText Vectorization\u201d techniques, since they all aim at one purpose: turning text into vectors (or arrays, if you want it simpler; or tensors, if you want it more complex), that can be then fed to machine learning models in a classical way.", "Thinking about our expected outcome as a vector is a good starting point to visualize how we can turn text into features. Let us reason a little more about this. Consider the following phrase:", "I want to turn my text into data.", "In simple computational terms, vectors are lists with n positions. One natural way to think about how to turn text into vectors would be to create an ordered list of all the words, as such:", "However, what happens if you have shorter or larger entries? And also, how could your machine learning algorithm (which is basically a series of matrix and vector computations) compare two words \u2014 these symbols invented by mankind that have special meanings?", "Since you\u2019re smart, you already thought: let me make a dictionary or some similar structure (in general, a vocabulary map) and use word indexes instead of words!", "You\u2019re on the right path, but lets consider some of the problems here: Is the word \u201cmy\u201d more important than \u201cwant\u201d? That\u2019s what your data \u2018tells\u2019 for a Machine Learning Algorithm. Machine Learning Algorithms don\u2019t care if the number is an index, only that it is a value to compute over (of course, you could have a categorical feature, but we\u2019ll see that further down).", "\u201cNormalize, normalize, normalize!!!\u201d, one can think. However, remember: these are not values! They are indices.", "If you\u2019ve been playing long enough with Data Science and Machine Learning, you\u2019re probably thinking about the one solution: Use \u201cOne Hot Encoding\u201d.", "One Hot Encoding is a process for encoding categorical features where each possible value for that feature is mapped to a new \u201ccolumn\u201d and, if present, this column is set to 1, 0 otherwise.", "Let us consider this using the previously mentioned vocabulary \u201cmap\u201d and the proposed phrase (in this case, the vocabulary is the same as the words of the phrase). We get this:", "Now, in case we wanted to encode: \u201cI want my data\u201d, we\u2019d get:", "Great, we\u2019ve found a way to encode our data into a Machine Learning way! But there are plenty of problems to solve there: let us consider the first one \u2014 word frequencies \u2014 that\u2019s where Bag of Words models come in!", "One hot encoding only treats values as \u201cpresent\u201d and \u201cnot present\u201d. That\u2019s not appropriate for text. In many text applications, the frequency of words play an important role. Consider the two following paragraphs:", "The dog is a domesticated carnivore of the family Canidae. It is part of the wolf-like canids, and is the most widely abundant terrestrial carnivore. The dog and the exant gray wolf are sister taxa as modern wolves are not closely related to the wolves that were first domesticated, which implies that the direct ancestor of the dog is extinct. The dog was the first species to be domesticated, and has been selectively bred over millennia for various behaviors, sensory capabilities, and physical attributes.", "Today I went out with my dog and I found 100 dollars in the park. I\u2019m sad to think that this money could be the lunch money for a week for some poor elderly lady.", "The first is the first paragraph of the Wikipedia article about dogs and the second is a pseudo-blog-entry that I made with the purpose of demonstrating this problem. The question: which web page would you recommend to a user that\u2019s searching a hypothetical engine with the word \u201cdog\u201d?", "One hot encoding would give the same value for \u201cdog\u201d in both entries, so that\u2019s not good.", "Enters the Bag of Words model!", "This model proposes that, instead of a vector of boolean values, you use a vector of word frequencies. In the above example, the column for word \u2018dog\u2019 would receive the value of \u20184\u2019 in the first text and only \u20181\u2019 in the second text. Now, it is okay to normalize the values, but not necessary (only for faster processing).", "This model is called a \u201cBag\u201d because it does not keep word \u201corder\u201d (just like our mommy\u2019s bags in the 90\u2019s were always messy).", "But before talking about the Bag of Words model shortcomings, let\u2019s see how to implement it using Python and Numpy.", "It is very straightforward, but let\u2019s see it step by step. I decided to make it a class, so I can instantiate multiple BoW\u2019s in a model. This is the class constructor:", "Basically we have a set of all words and a couple of dicts for storing the word indexes.", "Next, we prepare our Bag of Words by fitting it (adding all the documents at once, so it can \u2018learn\u2019 which words we can use).", "Finally, we can transform new inputs, returning arrays of the size of the entire vocabulary with the count the number of times that the word appears. For that I\u2019m using numpy, which is a math and algebra library that is specialized in vector/matrix algebra. It\u2019s python\u2019s currently default library for this kind of task (and used as input format to most machine learning libraries).", "This is an example usage and output:", "Now that we\u2019ve seen how it is done, we can talk about the problems in the model:", "First: it ignores word order completely. You can understand this by looking at the following image:", "Second: It tends to be very high dimensional. If your corpus accounts only the 90% most common English words at least once, according to the Oxford English Corpus, that\u2019ll result in a vector of at least 7000 dimensions (most of which will be of zeroes, but is a lot of dimensions, nonetheless)!", "But the BoW is simple and can bring quick results for simple problems. If you\u2019re not sure where to start when building a NLP solution, try the basic: go for a BoW model. It works as a good baseline estimator.", "Just to point out, there is one variation that is useful in Big Data cases, in special for situations where you have to compare texts in a character base (not accounting for semantics). This method uses text shingles (shingling). In this case, instead of breaking sentences into words, break at every k characters, or at every stop-word. Read more here:", "This is not actually a model, but rather an improvement into computing the \u201cpertinence\u201d of a word over a document. For simplicity, I\u2019ll call it a model.", "In TF-IDF Model, instead of storing the frequency of words, we store the result of the tf-idf algorithm over the input data. TF stands for Term Frequency and IDF stands for Inverse Document Frequency.", "In short, TF-IDF computes the weight of a word in a specific document, taking into account the overall distribution of words. The way the results are presented is the same as Bag of Words: a sparse vector (0\u2019s for words that does not appear, some float otherwise).", "TF-IDF models can return better results when used, for example, in Sentiment Analysis tasks, when compared to Bags of Words.", "Let us break this algorithm in two parts for better understanding: computing the global Inverse Document Frequency, and then computing the individual TF-IDF Score.", "The first part is to compute the global IDF value for every word. This value represents the weight of each word over all documents (in the entire corpus). This means that very common words will have less weight overall, while rare words will weight more (this even removes the need to do stop-word removal, since because they are very common, they will weight less).", "There are many distinct ways to do the computation of both TF and IDF (look at the algorithm Wikipedia page, for example). I\u2019ve chosen to use the logscaled IDF, which is computed with the following formula:", "To start, we define a helper method:", "1) A simple method to turn a sentence into a dict of words and frequencies (one can use Python collections \u201cCounter\u201d for best performance, but I\u2019ll use the old dict way for simplicity):", "Then we initialize the IDF Class similar to the BoW Class:", "For the fitting, we compute the global term frequency, followed by the idf for each word according to the formula we mentioned above.", "Great, we have the global IDF for our document. Now, the TF-IDF is computed for each sentence, by finding the Term Frequency Score for each term in each sentence and multiplying by the global term IDF.", "In short, we get the TF-IDF score for a sentence by:", "Here\u2019s the code (notice that I added a few conditionals and private methods to account for batch transformation):", "And with that our TF-IDF featurizer is done!", "Now, for each sentence, we get an array the size of the entire vocabulary with the pertinence of every word to that sentence (0 if its missing).", "You can find the code used to generate the above \u2018explanation\u2019 here.", "Here\u2019s the commit with the implementations made up to this point, be aware that there\u2019ll be a few differences in code since I\u2019ll be also using some structures that are built into my Toolset.:", "As we have seen, BoW and TF-IDF methods produce a vector the size of the entire vocabulary for a single sentence. This disregards for word order or position, making these techniques bad for continuity sensitive applications (most NLP applications are).", "One possible workaround would be to use a BoW/TFIDF array for each word, stack them and pass them as features, like in the following image (exemplified as the input layer of an Artificial Neural Network):", "For the above image, you\u2019d have a 10000*4 sparse matrix for a single sentence of 4 words (even using a bit for every boolean, you\u2019d get around 1kb per word per sentence! Imagine a large corpus?). The compute time and storage used to train a simple sentiment analysis model would make it too expensive or unfeasible (in fact, a few years ago, text was an almost untouched topic when the matter was Machine Learning due to these constraints allied to the lack of enough memory and processing power).", "However, this approaches allow us to keep word sequence. What if we could reduce the dimension of this vector?", "I won\u2019t go deep into the explanation of Word Embeddings, since there are several methods of computing them and most involve deep neural networks, which themselves take a time to explain (and this is not the focus of this article).", "But I won\u2019t refrain myself to give you the essential and most important information about it.", "Let us state it this way:", "Word embeddings are vector representation of words learnt from context training. Its not a score for each word, instead, it\u2019s more like the word\u2019s \u201ccoordinates\u201d.", "So when training a model instead of one-hot encodings the size of the vocabulary, you feed an array of word Embeddings representing the input. This array has a predefined d dimension depth, which is usually much less than the vocabulary size.", "One of the most well known techniques to generate Word Embeddings is Word2Vec, which originated the method. Word2Vec itself can be computed using two distinct techniques, but the details are not so important here.", "Instead, it is better for you to know that there are variations to the way to train/use embeddings. Here\u2019s a summary of techniques used to create embeddings:", "If you want to learn more, I suggest this amazing course from DeepLearning.AI presented at Coursera: https://www.coursera.org/learn/probabilistic-models-in-nlp", "The way word embeddings are generated, usually through a training process, allows the words to be encoded according to their contexts. This causes the interesting effect of embeddings also being able to represent word semantics (to an extent).", "Since words are represented by coordinates, they be compared against (to compare similarity). And if the dimension is properly reduced with techniques such as Principal Component Analysis \u2014 PCA, words can be plotted and the plotting usually shows words with similar meanings closer together, as like in the following image, taken from IBM Research Blog:", "The coordinates are usually given in a big number of dimensions, usually between 8 and 1024. This way, instead of a stack of 10000 dimension arrays filled with 0s, we have a stack of 8 to 1024 dimension arrays that are not sparse (they are densely filled with floats). This is better for computers to work with.", "Here\u2019s another cool example taken from an article by David Rozado in Journal Plos One:", "When considering how people use Embeddings, it is important to point out that word Embeddings can be generated in two distinct ways:", "I won\u2019t discuss how one can pretrain an embedding here, but I\u2019m preparing a post exclusively tailored to explain this practice. I suggest you subscribe to my account to get a notification for when it is posted, or keep an eye to this paragraph, for I\u2019ll post a link to the story right underneath it when it\u2019s ready.", "Just to add a little more about the topic, one can also do Feature Engineering when turning text into features. In other words, a Data Scientist can apply its own rules (usually through preprocessing) to define what should be extracted before turning text into numerical arrays.", "As we\u2019ve seen, turning text into features may seem a simple and trivial thing, but there are many things to consider.", "Currently, most state of the art techniques in NLP only use Word Embeddings, since they are more robust and able to be used in a sequential manner.", "Of course, each algorithm and problem requires a specific manipulation of input text. As an example, Seq2Seq models usually have a fixed sequence", "length. To enforce this length, padding or compression is used. But in the end, the padding and the words are all converted into Embeddings.", "Now that we\u2019ve done with the most basic topics in NLP Preprocessing, we can start talking about applications and techniques. Don\u2019t forget to subscribe to stay tuned and receive a notification about the next topic!", "Also, always check what\u2019s new on code in my repository. Feel free to reuse and contribute to the code according to the current Licensing. And you can get in touch with me through Linkedin if you want some help, just check my profile! See you in the next article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A Data Scientist passionate about data and text. Trying to understand and clearly explain all important nuances of Natural Language Processing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F478b57632e99&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----478b57632e99--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----478b57632e99--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tfduque.medium.com/?source=post_page-----478b57632e99--------------------------------", "anchor_text": ""}, {"url": "https://tfduque.medium.com/?source=post_page-----478b57632e99--------------------------------", "anchor_text": "Tiago Duque"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6698b3e89e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&user=Tiago+Duque&userId=f6698b3e89e3&source=post_page-f6698b3e89e3----478b57632e99---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F478b57632e99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F478b57632e99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://developers.google.com/machine-learning/crash-course/representation/feature-engineering", "anchor_text": "Google Developers Machine Learning Crash Course"}, {"url": "https://br.pinterest.com/pin/348747564875387277/", "anchor_text": "Cocoon Innovations"}, {"url": "https://web.archive.org/web/20120104170705/http://oxforddictionaries.com/words/the-oec-facts-about-the-language", "anchor_text": "Oxford English Corpus"}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/near-duplicates-and-shingling-1.html", "anchor_text": "Near-duplicates and shinglingNext: References and further reading Up: Web search basics Previous: Index size and estimation Contents Index One\u2026nlp.stanford.edu"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "Wikipedia page"}, {"url": "https://gist.github.com/Sirsirious/b28c676ecb95883e41b7e46ecd9cf68d", "anchor_text": "ere"}, {"url": "https://github.com/Sirsirious/NLPTools/tree/b48910b1db6e0de463b1713fcb65521df98e58e6", "anchor_text": "Sirsirious/NLPToolsYou can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or\u2026github.com"}, {"url": "https://www.coursera.org/learn/probabilistic-models-in-nlp", "anchor_text": "https://www.coursera.org/learn/probabilistic-models-in-nlp"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "Principal Component Analysis \u2014 PCA"}, {"url": "https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/", "anchor_text": "IBM Research Blog"}, {"url": "https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/", "anchor_text": "IBM Research Editorial Staff."}, {"url": "https://www.google.com/imgres?imgurl=https%3A%2F%2Fjournals.plos.org%2Fplosone%2Farticle%2Ffigure%2Fimage%3Fdownload%26size%3Dlarge%26id%3Dinfo%3Adoi%2F10.1371%2Fjournal.pone.0231189.g008&imgrefurl=https%3A%2F%2Fjournals.plos.org%2Fplosone%2Farticle%3Fid%3D10.1371%2Fjournal.pone.0231189&tbnid=Rzp3wYmO7VhVLM&vet=12ahUKEwiIutbFrezsAhV3M7kGHRh2BMEQMyhGegQIARBS..i&docid=imJ6d9GoTZJtLM&w=1479&h=1020&q=word%20embeddings%20example&ved=2ahUKEwiIutbFrezsAhV3M7kGHRh2BMEQMyhGegQIARBS", "anchor_text": "Journal Plos One"}, {"url": "https://keras.io/api/layers/core_layers/embedding/", "anchor_text": "Keras"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html", "anchor_text": "Pytorch"}, {"url": "https://spacy.io/usage/vectors-similarity", "anchor_text": "spaCy"}, {"url": "https://medium.com/analytics-vidhya/nlp-preprocessing-pipeline-what-when-why-2fc808899d1f", "anchor_text": "preprocessing"}, {"url": "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html", "anchor_text": "pytorch tutorial on seq2seq models"}, {"url": "https://github.com/Sirsirious/NLPTools", "anchor_text": "Sirsirious/NLPToolsYou can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or\u2026github.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----478b57632e99---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----478b57632e99---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----478b57632e99---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----478b57632e99---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----478b57632e99---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F478b57632e99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&user=Tiago+Duque&userId=f6698b3e89e3&source=-----478b57632e99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F478b57632e99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&user=Tiago+Duque&userId=f6698b3e89e3&source=-----478b57632e99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F478b57632e99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----478b57632e99--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F478b57632e99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----478b57632e99---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----478b57632e99--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----478b57632e99--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----478b57632e99--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----478b57632e99--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----478b57632e99--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----478b57632e99--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----478b57632e99--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----478b57632e99--------------------------------", "anchor_text": ""}, {"url": "https://tfduque.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tfduque.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tiago Duque"}, {"url": "https://tfduque.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "225 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6698b3e89e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&user=Tiago+Duque&userId=f6698b3e89e3&source=post_page-f6698b3e89e3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbb155ace8381&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-turn-text-into-features-478b57632e99&newsletterV3=f6698b3e89e3&newsletterV3Id=bb155ace8381&user=Tiago+Duque&userId=f6698b3e89e3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}