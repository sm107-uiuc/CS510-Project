{"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a", "time": 1682995359.083573, "path": "towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a/", "webpage": {"metadata": {"title": "Rosenblatt\u2019s perceptron, the first modern neural network | by Jean-Christophe B. Loiseau | Towards Data Science", "h1": "Rosenblatt\u2019s perceptron, the first modern neural network", "description": "Over the past decade, machine learning has been having a transformative impact in numerous fields such as cognitive neurosciences, image classification, recommendation systems, or engineering\u2026"}, "outgoing_paragraph_urls": [{"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "AlphaStar", "paragraph_index": 0}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 0}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 0}, {"url": "https://keras.io/", "anchor_text": "Keras", "paragraph_index": 0}, {"url": "https://github.com/FluxML/Flux.jl", "anchor_text": "Flux.jl", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?time_continue=23&v=jSt6Opt5nm4", "anchor_text": "How I came to hate math", "paragraph_index": 1}, {"url": "https://medium.com/u/a2db0221ff40?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Susannah Shattuck", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/people-dont-trust-ai-we-need-to-change-that-d1de5a4a0021", "anchor_text": "people don\u2019t trust AI", "paragraph_index": 1}, {"url": "https://www.ibm.com/downloads/cas/QQ5KZLEL", "anchor_text": "2018 study from IBM", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1806.07366", "anchor_text": "neural ordinary differential equations", "paragraph_index": 3}, {"url": "https://medium.com/u/e8ec6fa4d7d4?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Favio V\u00e1zquez", "paragraph_index": 3}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt", "paragraph_index": 15}, {"url": "https://medium.com/u/202534492f47?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Akshay Chandra Lagandula", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Hebbian_theory", "anchor_text": "Hebbian theory", "paragraph_index": 17}, {"url": "https://medium.com/u/202534492f47?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Akshay Chandra Lagandula", "paragraph_index": 31}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Seymour_Papert", "anchor_text": "Seymour Papert", "paragraph_index": 36}, {"url": "https://artsetmetiers.fr/en", "anchor_text": "Ecole Nationale Sup\u00e9rieure d\u2019Arts et M\u00e9tiers", "paragraph_index": 37}, {"url": "https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea", "anchor_text": "low-rank structure and data-driven modeling", "paragraph_index": 41}, {"url": "https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5", "anchor_text": "Machine learning basics", "paragraph_index": 41}], "all_paragraphs": ["Over the past decade, machine learning has been having a transformative impact in numerous fields such as cognitive neurosciences, image classification, recommendation systems, or engineering. Recently, neural networks and deep learning have attracted even more attention with their successes being regularly reported by both the scientific and mainstream media, see for instance Deep Mind\u2019s AlphaGo and AlphaGo Zero or the more recent AlphaStar. This renewed interest is partially due to the access to open-source libraries such as TensorFlow, PyTorch, Keras, or Flux.jl to name just a few.", "Although this increased access to efficient and versatile libraries has opened the door to innovative applications by reducing the knowledge required in computer science to implement deep learning algorithms, a good understanding of the underlying mathematical theories is still needed to come up with efficient neural network architecture for the task considered. Unfortunately, the image society has of mathematics may scare students away (see the documentary How I came to hate math for an illustration). This lack of mathematical literacy may also be one of the reasons why politics and non-tech industries are often either skeptical or way too optimistic about deep learning performances and capabilities. Additionally, Susannah Shattuck recently published a post discussing why people don\u2019t trust AI and why the industry may be reluctant to adopt it. One of the key reasons she cites, although not the only one, is the following :", "In a 2018 study from IBM, 63% of respondents cited a lack of technical skills as a barrier to AI implementation.", "Even though deep learning made it only recently to the mainstream media, its history dates back to the early 1940s with the first mathematical model of an artificial neuron by McCulloch & Pitts. Since then, numerous architectures have been proposed in the scientific literature, from the single-layer perceptron of Frank Rosenblatt (1958) to the recent neural ordinary differential equations (2018), to tackle various tasks (e.g. playing Go, time-series prediction, image classification, pattern extraction, etc). The timeline below (courtesy of Favio V\u00e1zquez) provides a fairly accurate picture of deep learning\u2019s history.", "As you can see, this history is pretty dense. Covering all of these different architectures over the course of a limited number of blog posts would thus be unrealistic. Moreover, some of these neural network architectures may draw from advanced mathematical fields or even from statistical physics. Rather than discussing at length every single one of these architectures, these series aim to gradually introduce beginners to the mathematical theories that underlie deep learning, the basic algorithms it uses as well as providing some historical perspectives about its development. For that purpose, we will start with simple linear classifiers such as Rosenblatt\u2019s single-layer perception or the logistic regression before moving on to fully connected neural networks and other widespread architectures such as convolutional neural networks or LSTM networks. Various other subjects, e.g. convex and non-convex optimization, the universal approximation theorem, or technical and ethical good practices will also be addressed along the way. Because we aim to help beginners understand the inner workings of deep learning algorithms, all of the implementations that will be presented rely essentially on SciPy and NumPy rather than on highly optimized libraries like TensorFlow, at least whenever possible. Moreover, for the sake of pedagogy and science outreach, all the codes used in this series are freely available on GitHub [here]. Without further ado, let us get started!", "The very first mathematical model of an artificial neuron was the Threshold Logic Unit proposed by Warren S. McCulloch (1898\u20131969, American neurophysiologist) and Walter H. Pitts Jr (1923\u20131969, American logician) in 1943. Before diving into their model, let us however quickly review first how a biological neuron actually works.", "Neurons are the building blocks of the brain. In a nutshell, neurons are electrically excitable cells that communicate with other cells via specialized connections. Different biological models exist to describe their properties and behaviors, see for instance", "Although some of these models start to be adopted as the building blocks of elaborate neural networks (see spiking neural nets for instance), we will hereafter restrict ourselves to a very high-level description of neurons. A schematic representation is shown in the figure below. For our purposes, only the following elements are of interest to us :", "The operating principle of a biological neuron can be summarized as follows. First, it takes inputs from its dendrites (i.e. from other neurons). In a second step, a weighted sum of these inputs is performed within the soma. The result is then passed on to the axon hillock. If this weighted sum is larger than the threshold limit, the neuron will fire. Otherwise, it stays at rest. The state of our neuron (on or off) then propagates through its axon and is passed on to the other connected neurons via its synapses. Albeit very simple, this high-level description of the operating principle of a biological neuron is sufficient to understand the mathematical model of an artificial neuron proposed by McCulloch & Pitts in 1943.", "Based on this basic understanding of the neuron\u2019s operating principle, McCulloch & Pitts proposed the very first mathematical model of an artificial neuron in their seminal paper A logical calculus of the ideas immanent in nervous activity back in 1943. Although very simple, their model has proven extremely versatile and easy to modify. Today, variations of their original model have now become the elementary building blocks of most neural networks, from the simple single-layer perceptron all the way to the 152 layers-deep neural networks used by Microsoft to win the 2016 ImageNet contest.", "McCulloch & Pitts\u2019 neuron model, hereafter denoted simply as MCP neuron, can be defined by the following rules :", "Given the input x = [ x\u2081, x\u2082, x\u2083, \u2026, x\u2099 ]\u1d40, the inhibitory input i and the threshold \u0398, the output y is computed as follows", "For anyone with basic knowledge of neural networks, such a model looks suspiciously like a modern artificial neuron, and that is precisely because it is!", "A lot of different papers and blog posts have shown how one could use MCP neurons to implement different boolean functions such as OR, AND, or NOT. These are illustrated below using Marvin Minsky\u2019s notation.", "It must be emphasized that, by stacking multiple MCP neurons, more complex functions (e.g. a flip-flop, division by two, etc) can also be represented. Despite this flexibility, MCP neurons suffer from major limitations, namely", "Nonetheless, the MCP neuron caused great excitation in the research community back then and, more than half a century later, gave rise to modern deep learning. Along the way, one of the most important improvements, tackling some of the MCP neuron\u2019s limitations, came from Frank Rosenblatt and his perceptron.", "Note: Akshay Chandra Lagandula published last summer a nice introduction to McCulloch & Pitts\u2019 neuron. Most notably, he illustrates how Boolean functions (e.g. AND, OR, etc) can be implemented using this model. For more in-depth details (and nice figures), interested readers are strongly encouraged to check it out.", "Almost fifteen years after McCulloch & Pitts, the American psychologist Frank Rosenblatt (1928\u20131971), inspired by the Hebbian theory of synaptic plasticity (i.e. the adaptation of brain neurons during the learning process), came up with the perceptron, a major improvement over the MCP neuron model. This invention granted him international recognition and, to this date, the Institute of Electrical and Electronics Engineers (IEEE), \u201cthe world\u2019s largest professional association dedicated to advancing technological innovation and excellence for the benefit of humanity\u201d, named its annual award in his honor.", "Rosenblatt\u2019s major achievement has been to show that, by relaxing some of the MCP\u2019s rules (namely the absolute inhibition, the equal contribution of all inputs as well as their integer nature), artificial neurons could actually learn from data. More importantly, he came up with a supervised learning algorithm for this modified MCP neuron model that enabled the artificial neuron to figure out the correct weights directly from training data by itself. Before diving into the machine learning fun stuff, let us quickly discuss the type of problems that can be addressed by the perceptron.", "Binary (or binomial) classification is the task of classifying the elements of a given set into two groups (e.g. classifying whether an image depicts a cat or a dog) based on a prescribed rule. The figure below depicts two instances of such a problem. On the left, the task is to identify the separatrix between two linearly separable classes (i.e. the separatrix is a simple straight line) while, on the right, the two classes are nonlinearly separable (i.e. the separatrix is not a simple straight line).", "As we will see, Rosenblatt\u2019s perceptron can handle only classification tasks for linearly separable classes. It must be noted however that, the example on the right figure could also be potentially treated by the perceptron, although it requires a preprocessing of the inputs known as feature engineering to recast it into a linearly separable problem. This will be addressed (hopefully) in a later post.", "To get a better understanding of the perceptron\u2019s ability to tackle binary classification problems, let us consider the artificial neuron model it relies on.", "As you can see, this neuron is quite similar to the one proposed in 1943 by McCulloch & Pitts. It however has some major differences, namely", "In mathematical terms, the non-linearity of the artificial neuron on which the perceptron relies is", "This function corresponds to the Heaviside function (i.e. H(z) = 0 if z < 0 and H(z) = 1 otherwise). Note that equivalent formulations of the perceptron, wherein the binary output is defined as y \u2208 {-1, 1}, consider the signum function rather than the Heaviside one, i.e.", "No matter the formulation, the decision boundary for the perceptron (and many other linear classifiers) is thus", "alternatively, using our compact mathematical notation", "This decision function depends linearly on the inputs x\u2096, hence the name Linear Classifier. Moreover, this equation is that of a hyperplane (a simple point in 1D, a straight line in 2D, a regular plane in 3D, etc). The vector w of synaptic weights is normal to this plane while the bias b is the offset from the origin. Now that we have a better understanding of why Rosenblatt\u2019s perceptron can be used for linear classification, the question that remains to be answered is", "Given a set of M examples (x\u2098, y\u2098), how can the perceptron learn the correct synaptic weights w and bias b to correctly separate the two classes?", "As discussed earlier, the major achievement of Rosenblatt was not only to show that his modification of the MCP neuron could actually be used to perform binary classification but also to come up with a fairly simple and yet relatively efficient algorithm enabling the perceptron to learn the correct synaptic weights w from examples. This algorithm is given below", "Before moving on to the Python implementation, let us consider four simple thought experiments to illustrate how it works.", "As you can see, this algorithm is extremely simple. It may not be clear however why, at first sight, such a simple algorithm could actually converge to a useful set of synaptic weights. Although relatively simple, the proof of convergence will not be presented herein and will actually be the subject of an upcoming post. For the rest of this post, just make a leap of faith and trust me, it does converge. In the meantime, if you are a skeptic or simply not convinced, you can check out the post by Akshay Chandra Lagandula to get some geometric intuition of why it works.", "Let us now move on to the fun stuff and implement this simple learning algorithm in Python. Assuming you are already familiar with Python, the following code should be quite self-explanatory.", "Note that, for the sake of clarity and usability, we will try throughout this course to stick to the scikit-learn API. An extended version of this code (with various sanity checks and other stuff) is freely available on my TowardsDataScience Github repo (here). The resulting decision boundary learned by our model is shown below", "For this particular example, it took our perceptron three passes over the whole dataset to correctly learn this decision boundary. Although it correctly classifies all of the examples from our training dataset, we\u2019ll see in later posts that the generalization capabilities of the perceptron are rather limited, notably due to the small margins it has and to its high sensitivity to noisy data that may even prevent the learning algorithm from converging. Nonetheless, do not hesitate to download the corresponding script from Github and play with this simple implementation to build your intuition about why it works, how it works and what are its limitations. After all,", "Smithing makes the smith, sailing makes the sailor and practice makes perfect.", "The simplicity and efficiency of this learning algorithm for linearly separable problems are some of the key reasons why it got so popular in the late 1950s and early 1960s. This popularity however caused Rosenblatt to oversell his perceptron ability to learn, giving rise to unrealistic expectations in the scientific community and/or also reported by the media. As we will see in coming posts, this perceptron indeed suffers from major limitations greatly restricting its applicability to real-life problems. The coup de gr\u00e2ce came from Marvin Minsky (1927\u20132016, American cognitive scientist) and Seymour Papert (1928\u20132016, South African-born American mathematician) who published in 1969 the notoriously famous book Perceptrons: an introduction to computational geometry. In this book, the authors have shown how limited Rosenblatt\u2019s perceptron (and any other single-layer perceptron) actually is and, notably, that it can't learn the simple logical XOR function. Some argue that the publication of this book and the demonstration of the perceptron\u2019s limits has triggered the so-called AI winter of the 1980s\u2026", "This post is the first from a series adapted from the introductory course to deep learning I teach at Ecole Nationale Sup\u00e9rieure d\u2019Arts et M\u00e9tiers (Paris, France). Since we must learn to walk before we can run, our attention has been focused herein on the very preliminaries of deep learning, both from a historical and mathematical point of view, namely the artificial neuron model of McCulloch & Pitts and the single-layer perceptron of Rosenblatt. Because these are the very elementary building blocks of modern neural networks, do not hesitate to read as much as you can about them and play with Jupyter Notebooks to make sure you fully grasp their properties and limitations before moving on to modern deep learning. I know tagging a post on the single-layer perceptron as being deep learning may be far-fetched. However, even though plenty of tutorials can be found online (some really good and some a bit more dubious) to run deep learning libraries as TensorFlow without requiring a deep (no pun intended) understanding of the underlying mathematics, having such insights will prove extremely valuable and prevent you from succumbing to the common pitfalls of deep learning later on. So let\u2019s take one step at a time, will you?", "In the next few posts, the following subjects will be discussed :", "Finally, you will find below a list of additional online resources on the history and the mathematics of the McCulloch & Pitts neuron and Rosenblatt\u2019s perceptron. Do not hesitate to check these out as they might treat some aspects we only glassed over!", "PS: If you know any other relevant link, do not hesitate to message me and I\u2019ll edit the post to add it :)", "Want to read more of this content ? Check out my other articles on low-rank structure and data-driven modeling or simply my Machine learning basics!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Assistant Professor in Fluid Mechanics and Applied Mathematics. Passionate about machine learning, physics and science outreach."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F37a3ec09038a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Jean-Christophe B. Loiseau"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F147ab927857&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=post_page-147ab927857----37a3ec09038a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "AlphaStar"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "https://github.com/FluxML/Flux.jl", "anchor_text": "Flux.jl"}, {"url": "https://www.youtube.com/watch?time_continue=23&v=jSt6Opt5nm4", "anchor_text": "How I came to hate math"}, {"url": "https://medium.com/u/a2db0221ff40?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Susannah Shattuck"}, {"url": "https://towardsdatascience.com/people-dont-trust-ai-we-need-to-change-that-d1de5a4a0021", "anchor_text": "people don\u2019t trust AI"}, {"url": "https://www.ibm.com/downloads/cas/QQ5KZLEL", "anchor_text": "2018 study from IBM"}, {"url": "https://towardsdatascience.com/people-dont-trust-ai-we-need-to-change-that-d1de5a4a0021", "anchor_text": "People don\u2019t trust AI. We need to change that.94% of executives think AI is key to business, but only 18% have adopted AI at scale. The issue is mistrust of AI \u2014 we\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/abs/1806.07366", "anchor_text": "neural ordinary differential equations"}, {"url": "https://medium.com/u/e8ec6fa4d7d4?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Favio V\u00e1zquez"}, {"url": "https://medium.com/u/e8ec6fa4d7d4?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Favio V\u00e1zquez"}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Louis_Lapicque", "anchor_text": "Louis Lapicque"}, {"url": "https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model", "anchor_text": "Hodgkin-Huxley model"}, {"url": "https://en.wikipedia.org/wiki/Alan_Lloyd_Hodgkin", "anchor_text": "Alan A. Hodgkin"}, {"url": "https://en.wikipedia.org/wiki/Andrew_Huxley", "anchor_text": "Andrew F. Huxley"}, {"url": "https://en.wikipedia.org/wiki/FitzHugh%E2%80%93Nagumo_model", "anchor_text": "FitzHugh-Nagumo model"}, {"url": "https://www.izhikevich.org/publications/spikes.htm", "anchor_text": "model of spiking neurons"}, {"url": "https://en.wikipedia.org/wiki/Neuron", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky"}, {"url": "https://medium.com/u/202534492f47?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Akshay Chandra Lagandula"}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt"}, {"url": "https://medium.com/u/202534492f47?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Akshay Chandra Lagandula"}, {"url": "https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1", "anchor_text": "McCulloch-Pitts Neuron \u2014 Mankind\u2019s First Mathematical Model Of A Biological NeuronIt is very well known that the most fundamental unit of deep neural networks is called an artificial neuron/perceptron\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d", "anchor_text": "Perceptron: The Artificial Neuron (An Essential Upgrade To The McCulloch-Pitts Neuron)The most fundamental unit of a deep neural network is called an artificial neuron, which takes an input, processes it\u2026towardsdatascience.com"}, {"url": "https://en.wikipedia.org/wiki/Hebbian_theory", "anchor_text": "Hebbian theory"}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "Wikipedia"}, {"url": "https://commons.wikimedia.org/wiki/File:Perceptron_moj.png#/media/File:Perceptron_moj.png", "anchor_text": "Wikipedia"}, {"url": "https://medium.com/u/202534492f47?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Akshay Chandra Lagandula"}, {"url": "https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975", "anchor_text": "Perceptron Learning Algorithm: A Graphical Explanation Of Why It WorksThis post will discuss the famous Perceptron Learning Algorithm proposed by Minsky and Papert in 1969. This is a\u2026towardsdatascience.com"}, {"url": "https://github.com/loiseaujc/TowardsDataScience/tree/master", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky"}, {"url": "https://en.wikipedia.org/wiki/Seymour_Papert", "anchor_text": "Seymour Papert"}, {"url": "https://artsetmetiers.fr/en", "anchor_text": "Ecole Nationale Sup\u00e9rieure d\u2019Arts et M\u00e9tiers"}, {"url": "https://towardsdatascience.com/improving-upon-rosenblatts-perceptron-d0517d3c5939", "anchor_text": "Improving upon Rosenblatt\u2019s perceptronAdaptive Linear Neurons and the Delta Ruletowardsdatascience.com"}, {"url": "https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#adaptive-linear-neurons-and-the-delta-rule", "anchor_text": "Single-Layer Neural Networks and Gradient Descent"}, {"url": "https://jontysinai.github.io/jekyll/update/2017/11/11/the-perceptron.html", "anchor_text": "The Perceptron"}, {"url": "https://medium.com/@NautilusMag/the-man-who-tried-to-redeem-the-world-with-logic-acb943e5d321", "anchor_text": "post"}, {"url": "https://medium.com/u/c731395f912d?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Nautilus"}, {"url": "https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea", "anchor_text": "low-rank structure and data-driven modeling"}, {"url": "https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5", "anchor_text": "Machine learning basics"}, {"url": "https://towardsdatascience.com/binary-cross-entropy-and-logistic-regression-bf7098e75559", "anchor_text": "Binary cross-entropy and logistic regressionEver wondered why we use it, where it comes from and how to optimize it efficiently? Here is one explanation (code\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----37a3ec09038a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----37a3ec09038a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/applied-mathematics?source=post_page-----37a3ec09038a---------------applied_mathematics-----------------", "anchor_text": "Applied Mathematics"}, {"url": "https://medium.com/tag/lecture-notes?source=post_page-----37a3ec09038a---------------lecture_notes-----------------", "anchor_text": "Lecture Notes"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----37a3ec09038a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----37a3ec09038a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----37a3ec09038a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----37a3ec09038a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----37a3ec09038a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----37a3ec09038a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----37a3ec09038a--------------------------------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jean-Christophe B. Loiseau"}, {"url": "https://loiseau-jc.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "280 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F147ab927857&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=post_page-147ab927857--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F961520fda2b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&newsletterV3=147ab927857&newsletterV3Id=961520fda2b6&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}