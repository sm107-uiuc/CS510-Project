{"url": "https://towardsdatascience.com/how-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f", "time": 1682994567.0820942, "path": "towardsdatascience.com/how-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f/", "webpage": {"metadata": {"title": "How Biases in Language get Perpetuated by Technology | by Sathvik Nair | Towards Data Science", "h1": "How Biases in Language get Perpetuated by Technology", "description": "Especially over the course of this year, the tech industry has been widely criticized over its relentless pursuit of artificial intelligence (AI) research and development. This is because the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.media.mit.edu/articles/facial-recognition-software-is-biased-towards-white-men-researcher-finds/", "anchor_text": "minority populations,", "paragraph_index": 1}, {"url": "https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html?_r=0", "anchor_text": "spouting racist remarks", "paragraph_index": 1}, {"url": "https://github.com/sathvikn/word_embedding_bias/blob/master/biased_embeddings_1.ipynb", "anchor_text": "Python notebooks", "paragraph_index": 13}, {"url": "https://github.com/sathvikn/word_embedding_bias/tree/master", "anchor_text": "reading list", "paragraph_index": 13}], "all_paragraphs": ["Especially over the course of this year, the tech industry has been widely criticized over its relentless pursuit of artificial intelligence (AI) research and development. This is because the dominant paradigm in the field is called machine learning, in which a computer relies on recognizing patterns in existing data to perform a particular task. In machine learning, many algorithms work by taking in a set of data, creating a model, and generating a prediction.", "However, what happens when the data these methods are based on are biased? Every so often, we see stories in the news about facial recognition technologies failing on minority populations, or Twitter bots spouting racist remarks. But the truth of the matter is that we keep on hearing about bias and AI without extensively learning about how exactly these biases get encoded in the technologies we use.", "As a result, I\u2019ll be explaining some of the shortcomings of a tool known as word embeddings, because they\u2019re used for a wide variety of tasks involving computers and human language, or natural language processing (NLP), and because it\u2019s relatively easy to explore and explain how these tools can be problematic without using lots of complicated technical jargon. First, let\u2019s learn more about NLP and how word embeddings fit in, and then we\u2019ll learn about how embeddings themselves contribute to the creation of biased results.", "The field of NLP relies on one key paradigm: treating text as data. This text can come from any source\u2013 movie reviews, ancient poems, even spoken words \u2014 and can be used for any task, whether it\u2019s detecting if an essay has a positive or negative tone, translating a phrase to another language, or even conducting a search online. However, all tasks an NLP system is used for involve creating mathematical models of the text. In order to do this, it\u2019s critical to have a numerical representation of each word in the input text so that a model can generate an output, such as a list of relevant websites or an accurate translation, based on the text provided.", "Word embeddings essentially increase the information about each word that gets captured in these numerical representations. The name of one of the most famous algorithms, \u201cWord2Vec\u201d presents this idea quite well, in which a word gets represented as a vector, or collection of numbers, itself generated by machine learning tools. Although NLP methods have been around for years before the introduction of word embeddings in the early 2010s, these techniques truly revolutionized the field, allowing for some critical discoveries later in this decade.", "The reason why word embeddings are so effective is that they are able to encode the relationship between each word with every other word in the text; this was unavailable in previous representations of words. Specifically, this is done through the idea that a word is defined by the words around it. If two words get mentioned in a similar context (\u201cgood\u201d and \u201cgreat\u201d for instance) in the training corpus (the body of text from which the embeddings get \u201clearned\u201d), then their corresponding vectors will also be similar.", "In order to find out why word embeddings can become problematic, we need to look at the way the models they\u2019re based on are evaluated. Having a robust way to assess a machine learning model is as critical as the model itself. The most common way to see if word embeddings are accurate is by using them to evaluate analogies. This is because the task is quite simple; mathematically, it\u2019s just adding and subtracting the vectors.", "Let\u2019s take the example man is to woman as king is to ______. Given a set of inputs like this, a quick transformation of the data would lead to the embedding for queen. I plotted the vectors for the four words below.", "In addition to these analogies, the simple steps of adding and subtracting vectors can also capture grammatical relationships like if words are singular or plural, and even facts about the world like countries and capitals.", "However, if we give our program the query \u201cMan is to woman as doctor is to ______?\u201d it ends up outputting \u201cnurse.\u201d The bias isn\u2019t just limited to gender, as the system recognizes \u201cPolice is to white as criminal is to Black,\u201d and \u201cLawful is to Christianity as terrorist is to Islamic.\u201d", "Because word embeddings get fed into other algorithms, their inherent bias could result in particularly problematic situations\u2013 an HR professional searches for \u201cengineers,\u201d on a site like LinkedIn, and sees that male engineers might be ranked higher than their equally talented female peers, or more dangerously, if a police department is tasked to heavily patrol a primarily Black neighborhood based on written crime reports.", "We could expect the text the embeddings were \u201clearned\u201d from, Wikipedia and news articles, to be relatively unbiased, but the words themselves might be mentioned in similar contexts. For instance, female gender pronouns like \u201cher\u201d might be used more frequently around the word \u201cnurse,\u201d just because our texts might talk about female nurses more than male nurses. However, allowing these associations to govern large-scale software systems is quite risky.", "At this point, we\u2019ve come to a grave conclusion. If we cannot effectively evaluate the building blocks of modern NLP, how can we trust the algorithms that use them?", "Let\u2019s now look at the dataset in more detail. I\u2019ve done these analyses in a couple Python notebooks written as a companion to this post, so feel free to follow along there if you want! I\u2019ve written a more technical treatment of the previous discussion as well. If you\u2019re familiar with the concept already, check out the reading list in the Github repo.", "First, we compare the vectors for one word with a pair of words. When we compare \u201cengineer\u201d to both \u201cman\u201d and \u201cwoman,\u201d we find that the vector for \u201cengineer\u201d is more similar to the vector for \u201cman\u201d than the vector for \u201cwoman.\u201d The difference isn\u2019t too big, but quite noticeable. However, when we compare \u201cengineer\u201d to the pair \u201cAsian\u201d and \u201cAfrican-American,\u201d we see that \u201cAsian\u201d is much more similar to \u201cengineer\u201d than \u201cAfrican-American.\u201d", "Next, we take a pair of vectors, such as \u201cman\u201d and \u201cwoman,\u201d and look at the vectors that represent people that are closest to them. The vector that was most similar to \u201cwoman\u201d that also represented a person corresponded to the word \u201cvictim,\u201d and other vectors that were close to \u201cwoman\u201d represented occupations like \u201cteacher\u201d and \u201cprostitute,\u201d while the most similar vectors to \u201cman\u201d referred to words like \u201csoldier\u201d and \u201chero.\u201d When we look at the vectors for \u201ccitizen\u201d and \u201cimmigrant,\u201d the most similar vectors to \u201ccitizen\u201d are usually professional occupations like \u201clawyer\u201d and \u201cbusinessman,\u201d but \u201cpeasant\u201d and \u201claborer\u201d are closer to \u201cimmigrant.\u201d Lastly, when comparing the vectors for \u201cChristianity\u201d and \u201cIslam,\u201d we find that both vectors are close to many religious terms, but the vector for \u201cIslam\u201d is far closer to the vectors for \u201cradical,\u201d \u201cfundamentalist,\u201d and \u201cextremist.\u201d", "The final piece of analysis we perform is probably the most interesting. Vectors aren\u2019t just a collection of numbers; they\u2019re also a way to represent these numbers in a space. Thus, this relies on human biases transforming the vector space that our dataset lives in. We look at the vector that quantifies a particular type of bias. For instance, the vector corresponding to the difference between \u201che\u201d and \u201cshe\u201d can represent a \u201cgendered component.\u201d This is because the numerical value of the vector captures how \u201cmale\u201d and \u201cfemale\u201d vectors differ numerically. If we transform the dataset such that we assign a score to each vector based on the difference between \u201che\u201d and \u201cshe,\u201d we find that words that have to do with sports and the military have higher scores, and words that describe the performing arts as well as female family members have lower scores. I plotted all the vectors after doing the transformation to get the result below.", "Let\u2019s look at a couple of the points on this graph.", "In fact, trying to \u201cde-bias\u201d word embeddings so they don\u2019t contain problematic relationships is a major area of research in NLP, and translating bias to operations on a vector space is a key paradigm in work like this.", "To recap, we\u2019ve gone over word embeddings and how they\u2019ve enabled many discoveries in the field of natural language processing because they encode important information about a word in a numerical form. We\u2019ve also talked about how the way they\u2019re evaluated is fundamentally flawed because they infer relationships between words that perpetuate the biases in the language we use. Finally, we\u2019ve analyzed the dataset and have a basic understanding of what causes these inferences.", "When we look at systems that use NLP technologies, from Web searching to virtual assistants, it\u2019s crucial for us to understand that these systems have been trained on data generated by humans. This is because technology is often painted as an objective ideal that addresses the world\u2019s problems by writing calculations on a blank slate. However, as soon as our technical systems use data that\u2019s generated by humans, in products that should be used by humans, we need to be aware of the biases that get reinforced by these systems and also how they can be fixed.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "UC Berkeley CompSci/CogSci, views expressed here are my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb4edc5532f3f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sathvik?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sathvik?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "Sathvik Nair"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7b9196557099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&user=Sathvik+Nair&userId=7b9196557099&source=post_page-7b9196557099----b4edc5532f3f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4edc5532f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4edc5532f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.media.mit.edu/articles/facial-recognition-software-is-biased-towards-white-men-researcher-finds/", "anchor_text": "minority populations,"}, {"url": "https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html?_r=0", "anchor_text": "spouting racist remarks"}, {"url": "https://github.com/sathvikn/word_embedding_bias/blob/master/biased_embeddings_1.ipynb", "anchor_text": "Python notebooks"}, {"url": "https://github.com/sathvikn/word_embedding_bias/tree/master", "anchor_text": "reading list"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b4edc5532f3f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----b4edc5532f3f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/ethics-and-technology?source=post_page-----b4edc5532f3f---------------ethics_and_technology-----------------", "anchor_text": "Ethics And Technology"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b4edc5532f3f---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/ai-ethics?source=post_page-----b4edc5532f3f---------------ai_ethics-----------------", "anchor_text": "Ai Ethics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb4edc5532f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&user=Sathvik+Nair&userId=7b9196557099&source=-----b4edc5532f3f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb4edc5532f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&user=Sathvik+Nair&userId=7b9196557099&source=-----b4edc5532f3f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4edc5532f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb4edc5532f3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b4edc5532f3f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b4edc5532f3f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sathvik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sathvik?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sathvik Nair"}, {"url": "https://medium.com/@sathvik/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "94 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7b9196557099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&user=Sathvik+Nair&userId=7b9196557099&source=post_page-7b9196557099--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7062751ffe2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-biases-in-language-get-perpetuated-by-technology-b4edc5532f3f&newsletterV3=7b9196557099&newsletterV3Id=7062751ffe2d&user=Sathvik+Nair&userId=7b9196557099&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}