{"url": "https://towardsdatascience.com/nns-aynk-c34efe37f15a", "time": 1682993308.337754, "path": "towardsdatascience.com/nns-aynk-c34efe37f15a/", "webpage": {"metadata": {"title": "Neural Networks: All You Need to Know | by Suryansh S. | Towards Data Science", "h1": "Neural Networks: All You Need to Know", "description": "The backbone of any large scale ML project starts with a Network\u2026 A Neural Network and Here\u2019s all you need to know about them."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Neural Nets", "paragraph_index": 0}, {"url": "https://twitter.com/SuryanshTweets", "anchor_text": "on twitter", "paragraph_index": 2}, {"url": "https://medium.com/@SuryanshWrites", "anchor_text": "on here", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network#History", "anchor_text": "Wiki article", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch", "anchor_text": "Warren McCulloch", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Walter_Pitts", "anchor_text": "Walter Pitts", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "The Perceptron", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Alexey_Ivakhnenko", "anchor_text": "Alexey Ivakhnenko", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Seymour_Papert", "anchor_text": "Seymour Papert", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Paul_Werbos", "anchor_text": "Paul Werbos", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Backpropagation", "anchor_text": "Back-propagation", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Exclusive_or", "anchor_text": "XOR", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer", "anchor_text": "Max-pooling", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "Recurrent NNs", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber", "anchor_text": "J\u00fcrgen Schmidhuber\u2019s", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Pattern_recognition", "anchor_text": "pattern recognition", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "Convolutional Neural Networks", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Function_composition", "anchor_text": "composite function", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/gas-and-nns-6a41f1e8146d", "anchor_text": "here(Genetic Algorithms + Neural Networks = Best of Both Worlds)", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Deep_learning", "anchor_text": "Deep Learning", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "Gradient Descent", "paragraph_index": 32}, {"url": "https://hackernoon.com/gradient-descent-aynk-7cbe95a778da", "anchor_text": "Gradient Descent: All YOU Need to Know", "paragraph_index": 32}, {"url": "https://en.wikipedia.org/wiki/Backpropagation", "anchor_text": "Back-Propagation", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Chain_rule", "anchor_text": "chain rule", "paragraph_index": 33}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "Click here", "paragraph_index": 33}, {"url": "https://jupyter.org/", "anchor_text": "JupyterNotebook", "paragraph_index": 35}, {"url": "https://en.wikipedia.org/wiki/Exclusive_or", "anchor_text": "XOR logic gate", "paragraph_index": 35}, {"url": "https://github.com/Frixoe/xor-neural-network/blob/master/XOR-Net-Notebook.ipynb", "anchor_text": "Click here", "paragraph_index": 35}], "all_paragraphs": ["As stated in the sub-title, Neural Nets(NNs) are being used almost everywhere, where there is need of a heuristic to solve a problem. This article will teach you all you need to know about a NN. After reading this article, you should have a general knowledge of NNs, how they work, and how to make one yourself.", "Here\u2019s what I will be going over:", "I occasionally write these somewhat short but informative articles to help you learn more about AI and Machine Learning. You can follow me on twitter or on here to stay up to date with my latest articles. You can also ask me questions on twitter or leave a comment here.", "Since, I do not want to bore you with a lot of history about NNs, I will only be going over their history, very briefly. Here\u2019s a Wiki article on the topic if you want more in-depth knowledge on their history. This section is majorly based off of the wiki article.", "It all started when Warren McCulloch and Walter Pitts created the first model of an NN in 1943. Their model was purely based on mathematics and algorithms and couldn\u2019t be tested due to the lack of computational resources.", "Later on, in 1958, Frank Rosenblatt created the first ever model that could do pattern recognition. This would change it all. The Perceptron. However, he only gave the notation and the model. The actual model still could not be tested. There were relatively minor researches done before this.", "The first NNs that could be tested and had many layers were published by Alexey Ivakhnenko and Lapa in 1965.", "After these, the research on NNs stagnated due to high feasibility of Machine Learning models. This was done by Marvin Minsky and Seymour Papert in 1969.", "This stagnation however, was relatively short-termed as 6 years later in 1975 Paul Werbos came up with Back-propagation, which solved the XOR problem and in general made NN learning more efficient.", "Max-pooling was later introduced in 1992 which helped with 3D object recognition as it helped with least shift invariance and tolerance to deformation.", "Between 2009 and 2012, Recurrent NNs and Deep Feed Forward NNs created by J\u00fcrgen Schmidhuber\u2019s research group went on to win 8 international competitions in pattern recognition and machine learning.", "In 2011, Deep NNs started incorporating convolutional layers with max-pooling layers whose output was then passed to several fully connected layers which were followed by an output layer. These are called Convolutional Neural Networks.", "There have been some more researches done after these but these are the main topics one should know about.", "A good way to think of an NN is as a composite function. You give it some input and it gives you some output.", "There are 3 parts that make up the architecture of a basic NN. These are:", "All of the things mentioned above are what you need to construct the bare bones architecture of an NN.", "You can think of these as the building blocks/bricks of a building. Depending on how you want the building to function, you will arrange the bricks and vice versa. The cement can be thought of as the weights. No matter how strong your weights are, if you don\u2019t have a good amount of bricks for the problem at hand, the building will crumble to the ground. However, you can just get the building to function with minimal accuracy(using the least amount of bricks) and then, progressively build upon that architecture to solve a problem.", "I will talk more about the weights, biases, and units in later section. Those sections might be short but the sections are there to emphasize their importance.", "Being the least important out of the three parts of an NNs architectures, these are functions which contain weights and biases in them and wait for the data to come them. After the data arrives, they, perform some computations and then use an activation function to restrict the data to a range(mostly).", "Think of these units as a box containing the weights and the biases. The box is open from 2 ends. One end receives data, the other end outputs the modified data. The data then starts to come into the box, the box then multiplies the weights with the data and then adds a bias to the multiplied data. This is a single unit which can also be thought of as a function. This function is similar to this, which is the function template for a straight line:", "Imagine having multiple of these. Since now, you will be computing multiple outputs for the same data-point(input). These outputs then get sent to another unit as well which then computes the final output of the NN.", "If all of this flew past you then, keep reading and you should be able to understand more.", "Being the most important part of an NN, these(and the biases) are the numbers the NN has to learn in order to generalize to a problem. That is all you need to know at this point.", "These numbers represent what the NN \u201cthinks\u201d it should add after multiplying the weights with the data. Of course, these are always wrong but the NN then learns the optimal biases as well.", "These are the values which you have to manually set. If you think of an NN as a machine, the nobs that change the behavior of the machine would be the hyper-parameters of the NN.", "You can read another one of my articles here(Genetic Algorithms + Neural Networks = Best of Both Worlds) to find out how to make your computer learn the \u201coptimal\u201d hyper-parameters for an NN.", "These are also known as mapping functions. They take some input on the x-axis and output a value in a restricted range(mostly). They are used to convert large outputs from the units into a smaller value \u2014 most of the times \u2014 and promote non-linearity in your NN. Your choice of an activation function can drastically improve or hinder the performance of your NN. You can choose different activation functions for different units if you like.", "Here are some common activation functions:", "These are what help an NN gain complexity in any problem. Increasing layers(with units) can increase the non-linearity of the output of an NN.", "Each layer contains some amount of Units. The amount in most cases is entirely up to the creator. However, having too many layers for a simple task can unnecessarily increase its complexity and in most cases decrease its accuracy. The opposite also holds true.", "There are 2 layers which every NN has. Those are the input and output layers. Any layer in between those is called a hidden layer. The NN in the picture shown below contains an input layer(with 8 units), an output layer(with 4 units) and 3 hidden layers with each containing 9 units.", "An NN with 2 or more hidden layers with each layer containing a large amount of units is called a Deep Neural Network which has spawned a new field of learning called Deep Learning. The NN shown in the picture is one such example.", "The most common way to teach an NN to generalize to a problem is to use Gradient Descent. Since I have already written an elaborate article on this topic which you can read to fully understand GD(Gradient Descent), I will not be explaining GD in this article. Here\u2019s the GD article: Gradient Descent: All YOU Need to Know.", "Coupled with GD another common way to teach an NN is to use Back-Propagation. Using this, the error at the output layer of the NN is propagated backwards using the chain rule from calculus. This for a beginner can be very challenging to understand without a good grasp on calculus so don\u2019t get overwhelmed by it. Click here to view an article that really helped me when I was struggling with Back-Propagation. It took me over a day and a half to figure out what was going on when the errors were being propagated backwards.", "There are many different caveats in training an NN. However, going over them in an article meant for beginners would be highly tedious and unnecessarily overwhelming for the beginners.", "To explain how everything is managed in a project, I have created a JupyterNotebook containing a small NN which learns the XOR logic gate. Click here to view the notebook.", "After viewing and understand what is happening in the notebook, you should have a general idea of how a basic NN is constructed.", "The training data in the NN created in the notebook is arranged in a matrix. This is how data is generally arranged in. The dimensions of the matrices shown in different projects might vary.", "Usually with large amounts of data, the data gets split into 2 categories: the training data(60%) and the test data(40%). The NN then trains on the training data and then tests its accuracy on the test data.", "If you still can\u2019t understand what\u2019s going on, I recommend looking at the links to resources provided below.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc34efe37f15a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@SuryanshWrites?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@SuryanshWrites?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "Suryansh S."}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf42a9b53d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&user=Suryansh+S.&userId=bf42a9b53d2b&source=post_page-bf42a9b53d2b----c34efe37f15a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc34efe37f15a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc34efe37f15a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Neural Nets"}, {"url": "https://twitter.com/SuryanshTweets", "anchor_text": "on twitter"}, {"url": "https://medium.com/@SuryanshWrites", "anchor_text": "on here"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network#History", "anchor_text": "Wiki article"}, {"url": "https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch", "anchor_text": "Warren McCulloch"}, {"url": "https://en.wikipedia.org/wiki/Walter_Pitts", "anchor_text": "Walter Pitts"}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt"}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "The Perceptron"}, {"url": "https://en.wikipedia.org/wiki/Alexey_Ivakhnenko", "anchor_text": "Alexey Ivakhnenko"}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky"}, {"url": "https://en.wikipedia.org/wiki/Seymour_Papert", "anchor_text": "Seymour Papert"}, {"url": "https://en.wikipedia.org/wiki/Paul_Werbos", "anchor_text": "Paul Werbos"}, {"url": "https://en.wikipedia.org/wiki/Backpropagation", "anchor_text": "Back-propagation"}, {"url": "https://en.wikipedia.org/wiki/Exclusive_or", "anchor_text": "XOR"}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer", "anchor_text": "Max-pooling"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "Recurrent NNs"}, {"url": "https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber", "anchor_text": "J\u00fcrgen Schmidhuber\u2019s"}, {"url": "https://en.wikipedia.org/wiki/Pattern_recognition", "anchor_text": "pattern recognition"}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning"}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "Convolutional Neural Networks"}, {"url": "https://en.wikipedia.org/wiki/Function_composition", "anchor_text": "composite function"}, {"url": "https://towardsdatascience.com/gas-and-nns-6a41f1e8146d", "anchor_text": "here(Genetic Algorithms + Neural Networks = Best of Both Worlds)"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "Sigmoid"}, {"url": "https://reference.wolfram.com/language/ref/Tanh.html", "anchor_text": "Tanh"}, {"url": "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)", "anchor_text": "ReLU"}, {"url": "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)", "anchor_text": "Leaky ReLU"}, {"url": "https://en.wikipedia.org/wiki/Deep_learning", "anchor_text": "Deep Learning"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "Gradient Descent"}, {"url": "https://hackernoon.com/gradient-descent-aynk-7cbe95a778da", "anchor_text": "Gradient Descent: All YOU Need to Know"}, {"url": "https://en.wikipedia.org/wiki/Backpropagation", "anchor_text": "Back-Propagation"}, {"url": "https://en.wikipedia.org/wiki/Chain_rule", "anchor_text": "chain rule"}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "Click here"}, {"url": "https://jupyter.org/", "anchor_text": "JupyterNotebook"}, {"url": "https://en.wikipedia.org/wiki/Exclusive_or", "anchor_text": "XOR logic gate"}, {"url": "https://github.com/Frixoe/xor-neural-network/blob/master/XOR-Net-Notebook.ipynb", "anchor_text": "Click here"}, {"url": "https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A", "anchor_text": "Siraj Raval"}, {"url": "https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw", "anchor_text": "3Blue1Brown"}, {"url": "https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aCibgK1PTWWu9by6XFdCfh", "anchor_text": "The Coding Train"}, {"url": "https://www.youtube.com/channel/UCsBKTrp45lTfHa_p49I2AEQ", "anchor_text": "Brandon Rohrer"}, {"url": "https://www.youtube.com/channel/UCrBzGHKmGDcwLFnQGHJ3XYg", "anchor_text": "giant_neural_network"}, {"url": "https://www.youtube.com/channel/UCiDouKcxRmAdc5OeZdiRwAg", "anchor_text": "Hugo Larochelle"}, {"url": "https://www.youtube.com/channel/UCQALLeQPoZdZC4JNUboVEUg", "anchor_text": "Jabrils"}, {"url": "https://www.youtube.com/channel/UCgBncpylJ1kiVaPyP-PZauQ", "anchor_text": "Luis Serrano"}, {"url": "https://www.coursera.org/learn/neural-networks", "anchor_text": "Neural Networks for Machine Learning"}, {"url": "https://www.coursera.org/specializations/deep-learning", "anchor_text": "Deep Learning Specialization"}, {"url": "https://www.coursera.org/learn/intro-to-deep-learning", "anchor_text": "Introduction to Deep Learning"}, {"url": "https://twitter.com/SuryanshTweets", "anchor_text": "Follow me on Twitter"}, {"url": "https://medium.com/@SuryanshWrites", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c34efe37f15a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----c34efe37f15a---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c34efe37f15a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c34efe37f15a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/learning?source=post_page-----c34efe37f15a---------------learning-----------------", "anchor_text": "Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc34efe37f15a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&user=Suryansh+S.&userId=bf42a9b53d2b&source=-----c34efe37f15a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc34efe37f15a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&user=Suryansh+S.&userId=bf42a9b53d2b&source=-----c34efe37f15a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc34efe37f15a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc34efe37f15a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c34efe37f15a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c34efe37f15a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c34efe37f15a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c34efe37f15a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@SuryanshWrites?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@SuryanshWrites?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Suryansh S."}, {"url": "https://medium.com/@SuryanshWrites/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "636 Followers"}, {"url": "https://suryansh.xyz", "anchor_text": "https://suryansh.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf42a9b53d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&user=Suryansh+S.&userId=bf42a9b53d2b&source=post_page-bf42a9b53d2b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fbf42a9b53d2b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnns-aynk-c34efe37f15a&user=Suryansh+S.&userId=bf42a9b53d2b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}