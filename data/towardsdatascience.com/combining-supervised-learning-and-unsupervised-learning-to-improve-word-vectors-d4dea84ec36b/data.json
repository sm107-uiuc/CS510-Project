{"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "time": 1682994730.07358, "path": "towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b/", "webpage": {"metadata": {"title": "Combining supervised learning and unsupervised learning to improve word vectors | by Edward Ma | Towards Data Science", "h1": "Combining supervised learning and unsupervised learning to improve word vectors", "description": "To achieve state-of-the-art result in NLP tasks, researchers try tremendous way to let machine understand language and solving downstream tasks such as textual entailment, semantic classification\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "word2vec", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c", "anchor_text": "skip-thought", "paragraph_index": 3}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "paper", "paragraph_index": 8}, {"url": "http://medium.com/@makcedward/", "anchor_text": "Medium Blog", "paragraph_index": 11}, {"url": "https://www.linkedin.com/in/edwardma1026", "anchor_text": "LinkedIn", "paragraph_index": 11}, {"url": "https://github.com/makcedward", "anchor_text": "Github", "paragraph_index": 11}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Improving Language Understanding by Generative Pre-Training", "paragraph_index": 12}, {"url": "https://github.com/openai/finetune-transformer-lm", "anchor_text": "Finetuned Transformer LM in tensorflow", "paragraph_index": 13}, {"url": "https://makcedward.github.io/", "anchor_text": "https://makcedward.github.io/", "paragraph_index": 15}], "all_paragraphs": ["To achieve state-of-the-art result in NLP tasks, researchers try tremendous way to let machine understand language and solving downstream tasks such as textual entailment, semantic classification. OpenAI released a new model which named as Generative Pre-Training (GPT).", "After reading this article, you will understand:", "This approach includes 2 steps. First of all, model is trained via unsupervised learning based-on a vast amount of data. Second part is using a target data set (domain data) to fine-tune the model from previous step via supervised learning.", "There is no denying that there are unlimited unlabeled data for NLP. Radford et al. believe that leveraging unlimited corpus help to train a model for general purpose just like word2vec (word embeddings) and skip-thought (sentence embeddings). We do not need consider about the volume of training data because we can easily get a lot of corpus.", "However, there is still have a limitation. Although we can use as much as corpus we can, it is disconnected with our domain data in most of time. From my previous work, I noticed most of word in my domain data does not exist in lots of off-the-shelf word embeddings model.", "In stead of using RNN architecture, Radford et al. applies transformer architecture to train the first model. Because they believe that transformer architecture is able capture a longer range signal (language characteristics). The limitation is high computation time. Since Radford et al. use 12 self-attention block and high dimensional inner states, it takes several weeks to train the initial model even using GPU.", "After that, target data set (it should be a small data set by comparing to the previous dataset in most of time) will be leveraged to fine-tune the model via supervised learning.", "Sequence-to-sequence (aka RNN) model has a limitation which we need to define the fixed-length context vector and it hurt the capability of memorize a very long sentences. Meanwhile, attention mechanism was born to overcome this issue. The architecture is called as \u201ctransformer\u201d which is multi-headed self-attention. In the family of attention mechanism, we have lots of variant of attention and Radford et al. decide to use self-attention.", "For detail of transformer, you may check out this paper. Go back to the architecture, input features are text and position of text to compute a word vectors. Position of text refers to word position of input. The flow is:", "Finally, combing multi-head (total 12 self-attention block) to together for computing vectors.", "After training a model from previous step, this supervised fine-tuning process help to obtain vectors for target tasks. Assuming input is a sequence of input tokens with label, we can get a token\u2019s vectors from pre-trained model.", "I am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence , especially in NLP and platform related. You can reach me from Medium Blog, LinkedIn or Github.", "Radford A., Narasimhan K., Salimans Tim., Sutskever I.. 2018. Improving Language Understanding by Generative Pre-Training.", "Finetuned Transformer LM in tensorflow (Original)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Focus in Natural Language Processing, Data Science Platform Architecture. https://makcedward.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd4dea84ec36b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@makcedward?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "Edward Ma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba547bff904f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&user=Edward+Ma&userId=ba547bff904f&source=post_page-ba547bff904f----d4dea84ec36b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4dea84ec36b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4dea84ec36b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@makcedward?utm_source=medium&utm_medium=referral", "anchor_text": "Edward Ma"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "word2vec"}, {"url": "https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c", "anchor_text": "skip-thought"}, {"url": "https://unsplash.com/@joshstyle?utm_source=medium&utm_medium=referral", "anchor_text": "JOSHUA COLEMAN"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@ari_spada?utm_source=medium&utm_medium=referral", "anchor_text": "Ari Spada"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb", "anchor_text": "BERT"}, {"url": "http://medium.com/@makcedward/", "anchor_text": "Medium Blog"}, {"url": "https://www.linkedin.com/in/edwardma1026", "anchor_text": "LinkedIn"}, {"url": "https://github.com/makcedward", "anchor_text": "Github"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Improving Language Understanding by Generative Pre-Training"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Attention is all you need"}, {"url": "https://github.com/openai/finetune-transformer-lm", "anchor_text": "Finetuned Transformer LM in tensorflow"}, {"url": "https://github.com/huggingface/pytorch-openai-transformer-lm", "anchor_text": "Finetuned Transformer LM in pytorch"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d4dea84ec36b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d4dea84ec36b---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----d4dea84ec36b---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d4dea84ec36b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/supervised-learning?source=post_page-----d4dea84ec36b---------------supervised_learning-----------------", "anchor_text": "Supervised Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4dea84ec36b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&user=Edward+Ma&userId=ba547bff904f&source=-----d4dea84ec36b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4dea84ec36b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&user=Edward+Ma&userId=ba547bff904f&source=-----d4dea84ec36b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4dea84ec36b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd4dea84ec36b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d4dea84ec36b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d4dea84ec36b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Edward Ma"}, {"url": "https://medium.com/@makcedward/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://makcedward.github.io/", "anchor_text": "https://makcedward.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba547bff904f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&user=Edward+Ma&userId=ba547bff904f&source=post_page-ba547bff904f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fde3db5912a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b&newsletterV3=ba547bff904f&newsletterV3Id=de3db5912a7c&user=Edward+Ma&userId=ba547bff904f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}