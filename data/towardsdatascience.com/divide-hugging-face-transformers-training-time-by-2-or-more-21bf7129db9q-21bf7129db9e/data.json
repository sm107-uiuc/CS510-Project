{"url": "https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e", "time": 1683007736.437136, "path": "towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e/", "webpage": {"metadata": {"title": "Divide HuggingFace training time by 2 | Towards Data Science", "h1": "Divide Hugging Face Transformers training time by 2 or more with dynamic padding and uniform length batching", "description": "Learn about dynamic padding and uniform length batching to reduce training time, iterate more and thus achieve better results."}, "outgoing_paragraph_urls": [{"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Divide-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report", "paragraph_index": 0}, {"url": "https://www.lefebvre-sarrut.eu/", "anchor_text": "Lefebvre Sarrut", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/why-we-switched-from-spacy-to-flair-to-anonymize-french-legal-cases-e7588566825f", "anchor_text": "precedent article", "paragraph_index": 2}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 2}, {"url": "https://camembert-model.fr/", "anchor_text": "CamemBERT", "paragraph_index": 2}, {"url": "https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8", "anchor_text": "there", "paragraph_index": 3}, {"url": "https://twitter.com/pommedeterre33", "anchor_text": "https://twitter.com/pommedeterre33", "paragraph_index": 5}, {"url": "https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Weights & Biases", "paragraph_index": 6}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://medium.com/u/b1574f0c6c5e?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Hugging Face", "paragraph_index": 11}, {"url": "https://github.com/huggingface/transformers/releases/tag/v2.9.0", "anchor_text": "version 2.9", "paragraph_index": 11}, {"url": "https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py", "anchor_text": "Trainer", "paragraph_index": 11}, {"url": "https://medium.com/u/8536ebfbc90b?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "William Falcon", "paragraph_index": 11}, {"url": "https://medium.com/u/3f4e5d4653f0?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Tianlei Wu", "paragraph_index": 12}, {"url": "https://medium.com/u/85d826944fa5?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Azure DevOps", "paragraph_index": 12}, {"url": "https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333", "anchor_text": "reduce inference times", "paragraph_index": 12}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "there", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e", "anchor_text": "article", "paragraph_index": 14}, {"url": "https://medium.com/u/8536ebfbc90b?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "William Falcon", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1809.05053.pdf", "anchor_text": "X-NLI", "paragraph_index": 17}, {"url": "https://arxiv.org/pdf/1911.03894.pdf", "anchor_text": "CamemBERT", "paragraph_index": 19}, {"url": "https://huggingface.co/camembert-base", "anchor_text": "camembert-base", "paragraph_index": 19}, {"url": "https://huggingface.co/camembert-base", "anchor_text": "camembert-large", "paragraph_index": 19}, {"url": "https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md", "anchor_text": "few other things", "paragraph_index": 20}, {"url": "http://www.msmarco.org/", "anchor_text": "Ms Marco", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "anchor_text": "Mean Reciprocal Rank", "paragraph_index": 22}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Divide-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report", "paragraph_index": 26}, {"url": "https://github.com/google-research/bert/blob/master/README.md", "anchor_text": "original BERT repo README", "paragraph_index": 29}, {"url": "https://pytorch.org/docs/stable/data.html#working-with-collate-fn", "anchor_text": "Pytorch documentation", "paragraph_index": 31}, {"url": "https://pytorch.org/docs/stable/data.html", "anchor_text": "Pytorch Dataloader", "paragraph_index": 32}, {"url": "https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Weights & Biases", "paragraph_index": 49}, {"url": "https://github.com/NVIDIA/apex", "anchor_text": "Nvidia apex library", "paragraph_index": 50}, {"url": "https://nvidia.github.io/apex/amp.html", "anchor_text": "apex documentation", "paragraph_index": 50}, {"url": "https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Weights & Biases", "paragraph_index": 57}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report", "paragraph_index": 57}, {"url": "https://twitter.com/pommedeterre33", "anchor_text": "https://twitter.com/pommedeterre33", "paragraph_index": 65}], "all_paragraphs": ["update 06/04/2020: following a suggestion, experiments performed on large flavor of CamemBERT have been added to the report. TL;DR: training times have been reduced from 4h to 1h30.", "I work at Lefebvre Sarrut, a publishing company which is a major player in the European legal industry. As explained in precedent articles, our needs require us to apply deep learning on very large text datasets (legal case anonymization, document classification, etc.), that is why we are so interested in making all our (machine learning) tools much faster.", "In the spirit of a precedent article, the purpose of this one is to explore 2 very simple optimizations which may significantly decrease training time on Transformers library without negative effect on accuracy. We ran 21 experiments + 12 reproducibility experiments on a large well-known NLP dataset (French part of X-NLI), and we show that by simply using an out-of-the-box French BERT model (CamemBERT), default parameters, a single consumer grade GPU, and these optimizations, for base flavor of the model, we can reach, for 128 max token length, in a 16 min training an accuracy of 81.5%, beating by 0.5 points the score obtained with a 56 min training without any optimization, and beating by 0.3 points the score reported for this task by the CamemBERT model authors. Gains are even more impressive on the same model, for 493 max token length, where training time decreases from 4h38 without any optimization to 1h01 with all optimizations, still reaching the same score. Similar training time reduction have been reached with large model (from 4h to 1h30 for 128 tokens length).", "Source code to reproduce the experiments is available there.", "These optimizations are not task / model / language specific but provided code below is written for Pytorch.", "If you are interested in this topic, follow me on Twitter: https://twitter.com/pommedeterre33", "Moreover, we logged all our experiments on Weights & Biases, a kind of online Tensorboard with a larger scope, so you can analyze by yourself our experiment results, the report here.", "(note that \u201cuniform length batching\u201d is named \u201csmart batching\u201d on the report)", "\u201cDynamic/variable length padding\u201d was a thing in NLP in 2017\u20132018, but now that a single neural network architecture makes any GPU owner able to reach SOTA scores on almost any task, it seems the NLP community has switched its focus from finding the optimal batch size to building stuff over the magic architecture. \u201cUniform length batching\u201d is a naive idea to push dynamic padding gains further that we wanted to check.", "At the time of this writing, both are not available out of the box on Transformers library for most common tasks.", "You can find dynamic padding only for text generation/language modeling tasks. Hopefully, we will see that both techniques are easy to implement on the user side for common tasks (classification, etc.) and we think that most NLP practitioners should test/use them.", "Over the last year, Transformers library from Hugging Face became the standard way to use large pre-trained language NLP models. It comes with plenty of features covering most NLP use cases, and has a polished API up to a point where you start to expect it to be perfect. This feeling is even stronger since version 2.9 bringing us the Trainer class, an adaptation of the carefully designed William Falcon\u2019s Pytorch Lightning training API to specific Transformers requirements, which free users from most engineering aspects of training (Tensorboard logging, mixed precision, gradient accumulation, multi-GPU setup, etc.) and therefore is the new default way to fine-tune models.", "In a situation where everything is so polished, you tend to believe that everything has been optimized to its maximum. One should not forget that the library is still young, and the team is working on several fronts at the same time (recent example being working with Tianlei Wu from Azure DevOps to leverage ONNX and reduce inference times in some setup). When you go deep inside the library, you may still find some interesting low-hanging fruits to seize.", "We ran experiments and summarized what we found below and there.", "If you are not yet a \u201cpadding / attention mask\u201d expert, you may want to read this excellent article written for old style RNN (from William Falcon\u2026 again!). To make it short, training neural networks on a batch of sequences requires them to have the exact same length to build the batch matrix representation. Because real life NLP datasets are always made of texts of variable lengths, we often need to make some sequences shorter by truncating them, and some others longer by adding at the end a repeated fake token called \u201cpad\u201d token. Because the pad token doesn\u2019t represent a real word/subword/signal, when most computations are done, before computing the loss, we erase the pad token signal by multiplying it by 0 through the attention mask matrix.", "The main idea behind the 2 optimizations is to avoid as much unused computation as possible:", "Moreover, we check the benefit of a third option, mixed precision, what it\u2019s worth alone or combined with the 2 techniques described above.", "To check if these optimizations work well with a transformer based language model, we ran 14 experiments with different settings on the French part of X-NLI.", "If you don\u2019t know X-NLI, it is basically a sentence pair classification task where the model needs to guess if the second sentence entails/contradicts/is neutral compared to the first one (3 classes). X-NLI train set is a machine translation of a large English dataset, and test set is made of 5K pairs manually translated in 15 languages (including French) from the same English dataset.", "The model used for the experiments is CamemBERT, a Roberta architecture trained with French texts by Facebook FAIR and Inria labs. Several flavors exist, figures below are related to the first one which have been released and known as camembert-base in Transformers (110M parameters trained on 130 Gb data). camembert-large experiments are available in the report.", "Camembert paper authors reached an accuracy of 81.2% in 10 epochs with early stopping,1e-5 learning rate, sequence length of 512 tokens and few other things.", "For each experiment, we limit training to 1 epoch because we only have access to a single GPU to run all experiments. All runs are using the same seed (exception being \u201creproducibility\u201d experiments with different seeds, as explained below). If not said otherwise, hyper parameters are kept to default values because we do not have the resources to perform a grid search for each experiment. As we will see, in most experiments we beat the author reported score on X-NLI, so default settings are probably good enough.", "X-NLI is mainly made of pairs of short sentences: when both parts of the pair are concatenated in a single sequence, over 97% of pairs have a length shorter than 128 tokens. This length distribution has a large impact on training time reduction opportunities. To make it short, on this dataset, it is OK to truncate sequences at 128 tokens to build \u201clarge\u201d batches even on a consumer-grade GPU. On a more balanced dataset like Ms Marco, you need a 512-token limit to reach close to SOTA Mean Reciprocal Rank (a common information retrieval measure). In both cases, the 2 techniques presented here bring large training time reduction (by a factor of 2 or more), but for different reasons that we will analyze below. In our use of these optimizations on our private datasets, we always got a significant effect on training times, whatever the characteristic of the dataset.", "Time and accuracy are measured on a single Nvidia 2080 TI GPU. Each combination of the 3 optimization options has been run 2 times, one with a batch of 64 sequences truncated to 128 tokens and a second time with a batch of 16 sequences (2 steps of 8 sequences and a single gradient accumulation) truncated to 493 tokens. 493 is the number of Camembert subword tokens in the longest sequence in number of characters, and probably one of the longest token sequences in number of tokens in the train set. The step sizes have been set to the maximum this GPU can take in memory.", "Timings include intermediary evaluations (in our case every 500 mini batches) as in real life that\u2019s how most NLP practitioners run training.", "By using both optimizations and mixed precision, we beat in a 16mn training the score of a 4h38 training!", "Findings for base model are the same for large model, measures from the additional 12 experiments are in the report.", "As explained above, pad token signal is canceled by the application of the attention mask. More pad tokens you put at the end of a sequence, more unused computations you will perform.", "In the Trainer class, you define a (fixed) sequence length, and all sequences of the train set are padded / truncated to reach this length, without any exception. On X-NLI, shortest sequences are 10 tokens long, if you provide a 128 tokens length, you will add 118 pad tokens to those 10 tokens sequences, and then perform computations over those 118 noisy tokens.", "Worst, as written in the original BERT repo README, \u201c\u2026attention is quadratic to the sequence length. In other words, a batch of 64 sequences of length 512 is much more expensive than a batch of 256 sequences of length 128.\u201d.", "A mini batch is made of a small selection of sequences sampled from the dataset. Even when selected randomly in X-NLI, chances are that the longest sequence in a mini batch is shorter than the maximum sequence length set for the whole train set. Because the learning / gradient descent is performed at the mini batch level, we have the opportunity to limit the padding effect, more precisely we can first search for the longest sequence length in the mini batch, and then pad the other sequences accordingly.", "Those operations can be performed in the collate_fn function. The purpose of this function is described in the Pytorch documentation, basically it takes the individual examples returned by the Dataset and merges them to build the tensor matrix to send to the model.", "As explained above, the idea is to adjust the sequence length at the mini batch level instead of dataset level. That way we can limit unused computation. The work is performed inside the Pytorch Dataloader. Let\u2019s remind how it works:", "collate_fn is the perfect place to perform the dynamic padding. Fortunately, Pytorch Dataloader has a parameter to provide our own implementation in its constructor, no need to override anything. Trainer class from Transformers library has a similar parameter in its constructor, we will use it. Instead of a function, it waits for an instance of a \u201cCollator\u201d (a Transformers specific class) which has a single purpose, wrap the collate method.", "Find below a possible implementation of Collator class.", "We run 4 experiments that we group per batch size, for each group we compare cases where dynamic padding is used and not. When it is enabled for:", "Timing decrease is in both cases significant, and is 4X stronger for long sequences. It makes sense, in the train set, 97% of examples are shorter than 128 tokens, so for most of them, we pay a tax for having a 493 max sequence size. By using the optimization, we pay only for the useful computation.", "For 128 tokens truncation, there is still a gain as most sequences are still much smaller than 128 tokens, and BERT complexity being quadratic regarding its input length, the avoided computation cost is much lower and training time decreases of \u201conly\u201d 15%.", "We run 4 experiments that we group per batch size, for each group we compare cases where dynamic padding is used and not. When it is enabled for:", "It appears that accuracy improves with dynamic padding in both cases.", "Uniform size batching consists of simply building batches made of similar length sequences. The purpose is to make padding as minimal as possible when combined with dynamic padding.", "There are many ways to implement it, the one we followed was to:", "That way each batch is made of similar length sequences, but following batches are of different lengths.", "Naive (simple to understand / not clean) implementation may look something like this:", "For time reduction, we previously show that dynamic padding brings large training time reduction, let\u2019s compare training time with dynamic padding and no uniform size batching, and with both optimizations enabled. For:", "So in both situations, our naive idea seems to bring another significant training time decrease.", "Usually neural networks are trained on randomly ordered data points. Uniform size batching limits this randomness, hence introduces a kind of bias which may, in theory, impact accuracy.", "We will compare the setups with and without the uniform size batching only:", "In both cases, there is an improvement, and we may conclude that there is no negative impact on accuracy.", "However, we run many experiments combining several options, and according to the Weights & Biases dashboard, the use of uniform size batching is negatively correlated with accuracy. After a manual checking of experiments pairs (with/without the option), this effect is not obvious.", "Mixed precision is possible on Pytorch through the Nvidia apex library. To make it short, in its most common mode, mixed precision consists of performing most operations with half precision and accumulating results in full precision (more info in apex documentation).", "Apex is known for bringing improvement in some scenarios, sometimes it also brings some instability (e.g., the loss amplitude during training is bigger than without mixed precision), and quite rarely it avoids the model to converge. Said otherwise, it\u2019s not a silver bullet, but an interesting tool to test on your case.", "The good news is that Trainer class implements it out of the box, to leverage it, you just need to add the right flag to your command line (\u201c \u2014 fp16\u201d).", "Regarding training time for mini batches of 16 long sequences, the situation is unusual. For:", "The reason is probably that in the second case, it adds overhead and doesn\u2019t help that much as most batches are only made of short sequences. Mixed precision helps the most with big matrix operations.", "When applied to mini batches of 64 short sequences, things are as expected:", "This time, even when the step is made of short sequence, each contains 64 sequences, making the matrix big enough to benefit from mixed precision.", "Regarding accuracy, there is no clear pattern. You can make your own idea by yourself by checking the Weights & Biases report.", "All experiments have been run using the same seed. It may happen that we were lucky and our approach was hitting accuracy but not with this seed and on this dataset.", "We reran the 16 min training with all optimizations enabled setting 5 times with different seeds and accuracy / timing are reproduced.", "Same kind of reproduction experiments have been also performed for the large model. Results are the same.", "We have shown that both techniques constantly provide significant time reduction without reducing accuracy. Moreover, we learned that on a dataset with small batches, one should be careful with mixed precision, because it can lead to unexpected slower training if there is not enough computation to perform.", "We are convinced that both techniques are low-hanging fruits that should be widely used by Transformers users.", "To finish on a more general thought, we are pleasantly surprised by the results obtained by such simple ideas. Just for the story, in another unrelated experiment, we noticed that the French train set of X-NLI (which is a machine translation of an english dataset) was of low quality (many examples are absolute nonsense in French), and we were wondering if translating it with a better quality would improve the accuracy on the test set (which is a manual translation). It represented an important opportunity to us, because if it worked it would mean having plenty of dataset in French to play with. We spent a few bucks on DeepL, the translation was much better\u2026 and the accuracy didn\u2019t change (we even thought there was a bug in our measures). Not all simple ideas are created equal!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of R&D, applied ML engineer (Lefebvre Sarrut, Paris). Former: tax lawyer (Deloitte, Paris), CPA (Constantin, NYC) https://twitter.com/pommedeterre33"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F21bf7129db9e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pommedeterre33?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pommedeterre33?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Micha\u00ebl Benesty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9515e0e75a23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&user=Micha%C3%ABl+Benesty&userId=9515e0e75a23&source=post_page-9515e0e75a23----21bf7129db9e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21bf7129db9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21bf7129db9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pngwing.com/en/free-png-ncpgf", "anchor_text": "this"}, {"url": "https://www.pexels.com/fr-fr/photo/851989/", "anchor_text": "that"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Divide-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report"}, {"url": "https://www.lefebvre-sarrut.eu/", "anchor_text": "Lefebvre Sarrut"}, {"url": "https://towardsdatascience.com/why-we-switched-from-spacy-to-flair-to-anonymize-french-legal-cases-e7588566825f", "anchor_text": "precedent article"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://camembert-model.fr/", "anchor_text": "CamemBERT"}, {"url": "https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8", "anchor_text": "there"}, {"url": "https://twitter.com/pommedeterre33", "anchor_text": "https://twitter.com/pommedeterre33"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report"}, {"url": "https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Weights & Biases"}, {"url": "https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Weights & Biases"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "here"}, {"url": "https://medium.com/u/b1574f0c6c5e?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Hugging Face"}, {"url": "https://github.com/huggingface/transformers/releases/tag/v2.9.0", "anchor_text": "version 2.9"}, {"url": "https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py", "anchor_text": "Trainer"}, {"url": "https://medium.com/u/8536ebfbc90b?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "William Falcon"}, {"url": "https://medium.com/u/3f4e5d4653f0?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Tianlei Wu"}, {"url": "https://medium.com/u/85d826944fa5?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Azure DevOps"}, {"url": "https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333", "anchor_text": "reduce inference times"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "there"}, {"url": "https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e", "anchor_text": "article"}, {"url": "https://medium.com/u/8536ebfbc90b?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "William Falcon"}, {"url": "https://arxiv.org/pdf/1809.05053.pdf", "anchor_text": "X-NLI"}, {"url": "https://arxiv.org/pdf/1911.03894.pdf", "anchor_text": "CamemBERT"}, {"url": "https://huggingface.co/camembert-base", "anchor_text": "camembert-base"}, {"url": "https://huggingface.co/camembert-base", "anchor_text": "camembert-large"}, {"url": "https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md", "anchor_text": "few other things"}, {"url": "http://www.msmarco.org/", "anchor_text": "Ms Marco"}, {"url": "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "anchor_text": "Mean Reciprocal Rank"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "here"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Divide-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report"}, {"url": "https://github.com/google-research/bert/blob/master/README.md", "anchor_text": "original BERT repo README"}, {"url": "https://pytorch.org/docs/stable/data.html#working-with-collate-fn", "anchor_text": "Pytorch documentation"}, {"url": "https://pytorch.org/docs/stable/data.html", "anchor_text": "Pytorch Dataloader"}, {"url": "https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html", "anchor_text": "https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html"}, {"url": "http://twitter.com/dataclass", "anchor_text": "@dataclass"}, {"url": "https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Weights & Biases"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report"}, {"url": "https://github.com/NVIDIA/apex", "anchor_text": "Nvidia apex library"}, {"url": "https://nvidia.github.io/apex/amp.html", "anchor_text": "apex documentation"}, {"url": "https://medium.com/u/c4669728a9af?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Weights & Biases"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report"}, {"url": "https://app.wandb.ai/pommedeterresautee/speed_training/reports/Decrease-HuggingFace-Transformers-training-times-by-2-or-more-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI", "anchor_text": "report"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----21bf7129db9e---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----21bf7129db9e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----21bf7129db9e---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----21bf7129db9e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----21bf7129db9e---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F21bf7129db9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&user=Micha%C3%ABl+Benesty&userId=9515e0e75a23&source=-----21bf7129db9e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F21bf7129db9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&user=Micha%C3%ABl+Benesty&userId=9515e0e75a23&source=-----21bf7129db9e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21bf7129db9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F21bf7129db9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----21bf7129db9e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----21bf7129db9e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----21bf7129db9e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----21bf7129db9e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pommedeterre33?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pommedeterre33?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Micha\u00ebl Benesty"}, {"url": "https://medium.com/@pommedeterre33/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "487 Followers"}, {"url": "https://twitter.com/pommedeterre33", "anchor_text": "https://twitter.com/pommedeterre33"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9515e0e75a23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&user=Micha%C3%ABl+Benesty&userId=9515e0e75a23&source=post_page-9515e0e75a23--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa1c87c1ea8bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdivide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e&newsletterV3=9515e0e75a23&newsletterV3Id=a1c87c1ea8bb&user=Micha%C3%ABl+Benesty&userId=9515e0e75a23&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}