{"url": "https://towardsdatascience.com/overview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98", "time": 1682993592.167648, "path": "towardsdatascience.com/overview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98/", "webpage": {"metadata": {"title": "Overview of Transfer Learning in NLP \u2014 Contextual Word Vectors | by Pooja Rao | Towards Data Science", "h1": "Overview of Transfer Learning in NLP \u2014 Contextual Word Vectors", "description": "The first post of the series discussed transfer learning in NLP and the publication \u2014 Semi-supervised Sequence Learning. We saw how a simple pre-training step using a sequence autoencoder improved\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first post", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf", "anchor_text": "Semi-supervised Sequence Learning", "paragraph_index": 0}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77/#cd96", "anchor_text": "list", "paragraph_index": 1}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first review", "paragraph_index": 8}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first review", "paragraph_index": 10}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first review", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1611.01603", "anchor_text": "Bidirectional Attention Flow for Machine Comprehension", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1603.01417", "anchor_text": "Dynamic Memory Networks for Visual and Textual Question Answering", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1602.02410", "anchor_text": "Exploring the limits of language modeling", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1312.3005", "anchor_text": "One billion word benchmark for measuring progress in statistical language modeling", "paragraph_index": 30}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://twitter.com/poojaraosb", "anchor_text": "Twitter", "paragraph_index": 31}, {"url": "https://www.linkedin.com/in/pooja-rao/", "anchor_text": "LinkedIn", "paragraph_index": 31}, {"url": "http://poojarao.in", "anchor_text": "poojarao.in", "paragraph_index": 33}], "all_paragraphs": ["The first post of the series discussed transfer learning in NLP and the publication \u2014 Semi-supervised Sequence Learning. We saw how a simple pre-training step using a sequence autoencoder improved the results on all four classification tasks.", "In this post, I would like to give a brief synopsis of the next two publications in the list. Contextualized word representation is the focus of both the publications. The researchers try to produce word representations commonly known as embeddings, which are aware of the contextual meaning in the sentence. The process of generating the word embeddings is where the difference lies between the two. CoVe uses a supervised task like machine translation while ELMo uses an unsupervised task like language modelling.", "Let\u2019s dive deep into each of the research publication and chalk out the similarities and the differences.", "Machine translation data can be the potential equivalent to ImageNet in NLP. (owing to the immense success of transfer learning in computer vision using ImageNet). The authors try to prove this hypothesis by adopting the attentional sequence-to-sequence model trained for machine translation. Machine translation sequence-to-sequence models encode words in context and decode them into another language. These models usually contain an LSTM-based encoder. They make use of this encoder to obtain, what they call, the contextual vectors.", "This process involves two steps. a) Train the attentional sequence-to-sequence model for machine translation. b) Transfer the learnt information to downstream tasks.", "English-to-German translation data is used to train the machine translation model. They use a standard, two-layer, bidirectional LSTM network as the encoder (MT-LSTM), initialized with GloVe vectors and a two-layer, unidirectional LSTM with attention as the decoder. The outputs of the pre-trained encoder, MT-LSTM, are treated as the context vectors.", "w is the sequence of input words and GloVe(w) is the sequence of word vectors produced by the GloVe model. CoVe(w) is the sequence of context vectors produced by MT-LSTM.", "The authors check the benefits of the CoVe vectors on a range of datasets for classification tasks namely \u2014 sentiment analysis, question classification and entailment; and question answering. The classification models use a biattentive classification network [1] and the question answering model uses a dynamic coattention network [2]. The architectures of these networks are beyond the scope of this article.", "Unlike the first review where the target task was predefined, there is no targeted downstream task here. The aim is to adapt the vectors to any downstream task.", "Each vector in GloVe(w) is concatenated with its corresponding vector in CoVe(w)", "The authors let these concatenated vectors be the inputs to the classification and the question answering networks. These vectors remain fixed throughout the experiments (unlike the first review, where the pre-trained weights are fine-tuned on the target task).", "The models using CoVe along with GloVe consistently perform better than the models using only GloVe. Appending character n-gram embeddings boost performance even further for some tasks. They achieve new state-of-the-art performance (at the time) on the SST-5 and SNLI datasets. One important insight from the experiments is as follows. Larger the machine translation dataset, better the improvement in the downstream target task using CoVe. This is one of the important insights from the experiments. Thus, this correlation can be an evidence of their hypothesis of using machine translation data as an equivalent to ImageNet in NLP.", "This research from the Allen Institute for AI introduces another type of deep contextual word representations. The aim is to learn representations that model the syntax, semantics and polysemy (existence of many possible meanings for a word). The researchers train a bidirectional LSTM network on the language modelling task to derive vectors. These are called ELMo (Embeddings from Language Model). ELMo vectors are a function of all the internal layers of the language model and hence deeper than the CoVe.", "A bidirectional language model (biLM) is a combination of forward language model and backward language model. Forward LM computes the probability of the sequence by modelling the probability of token k provided the history of k-1 tokens. Backward LM is similar to forward LM, running the sequence in reverse order predicting the previous token provided the future context. The architecture of biLM is comparable to [3]. It is trained on the 1B Word Benchmark corpus [4].", "The authors fix the pre-trained biLM and use it to compute representations for any downstream task. In some cases, fine-tuning the biLM on the downstream task data leads to performance improvement. Hence, they fine-tune the biLM for one epoch on the task-specific data removing the supervised labels and fix the weights throughout the task experiments thereafter (unlike the first review where they fine-tune the weights during the task experiments). They use the fine-tuned biLM in most of the cases.", "ELMo is a task-specific combination of the intermediate layer representations in the biLM. A L-layer biLM computes a set of 2L+1 representations as shown below.", "ELMo computes a task-specific weighted combination of all the layers in biLM as shown below.", "s^task are the softmax-normalized weights and the scalar parameter \u03b3 is decided experimentally. In the simplest case, it can choose only the last layer like CoVe.", "The pre-trained word embeddings are concatenated with ELMo vectors from the biLM to be passed into the downstream tasks as inputs.", "They validate the performance of ELMo vectors across a variety of NLP tasks and achieve a new state-of-the-art (at the time) on each. They use different baseline architectures for each of the tasks and add ELMo vectors to the input. They achieve relative error reductions ranging from 6\u201320% over strong base models. The results are as shown below.", "They show that a weighted combination of representation from all layers improves overall performance than just using the last layer. ELMo beats the performance of CoVe in tasks where comparison is possible. Sometimes, including ELMo at the output layer of the downstream task model improves the performance. It also successfully disambiguates a word in context achieving polysemy. See the example below.", "The experiments reveal that different layers of the biLM depict a different sort of information of a word. The second layer performs better at word sense disambiguation while the first layer achieves higher accuracy at POS tagging. This way, inclusion of all biLM layers is crucial.", "Adding a large unsupervised dataset via ELMo representations is advantageous. The model converges to a higher accuracy faster and requires a smaller training set. It enhances the usage of the training data by the model. It significantly reduces the amount of training data needed to reach a level of performance.", "Thus, the ELMo captures the syntax and semantics of a word in context using the deep embeddings from the language model.", "CoVe and ELMo are similar in the way that they compute contextual word representations. Some of the similarities \u2014", "Some of the major differences \u2014", "One significant difference between the first review and this review of the contextual word vectors is that the researchers choose a generic dataset for pre-training independent of the task. This may be regarded as the next step in moving towards an ImageNet like dataset.", "[1] Bidirectional Attention Flow for Machine Comprehension", "[2] Dynamic Memory Networks for Visual and Textual Question Answering", "[3] Exploring the limits of language modeling", "[4] One billion word benchmark for measuring progress in statistical language modeling", "This is the second post in the series in where I give a brief about recent publications about transfer learning in NLP. You can find the first post here. Any feedback/suggestions are highly appreciated. Feel free to comment below or reach out to me on Twitter or LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student @ University of Lausanne. I write about NLP, ML, HCI and research in general | poojarao.in"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffd2e8c72eb98&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@poojarao126?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@poojarao126?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "Pooja Rao"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46ba9746950e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&user=Pooja+Rao&userId=46ba9746950e&source=post_page-46ba9746950e----fd2e8c72eb98---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd2e8c72eb98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd2e8c72eb98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first post"}, {"url": "https://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf", "anchor_text": "Semi-supervised Sequence Learning"}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77/#cd96", "anchor_text": "list"}, {"url": "https://arxiv.org/abs/1708.00107", "anchor_text": "CoVe"}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "ELMo"}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first review"}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first review"}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "first review"}, {"url": "https://arxiv.org/abs/1611.01603", "anchor_text": "Bidirectional Attention Flow for Machine Comprehension"}, {"url": "https://arxiv.org/abs/1603.01417", "anchor_text": "Dynamic Memory Networks for Visual and Textual Question Answering"}, {"url": "https://arxiv.org/abs/1602.02410", "anchor_text": "Exploring the limits of language modeling"}, {"url": "https://arxiv.org/abs/1312.3005", "anchor_text": "One billion word benchmark for measuring progress in statistical language modeling"}, {"url": "https://arxiv.org/abs/1708.00107", "anchor_text": "research paper"}, {"url": "https://einstein.ai/research/learned-in-translation-contextualized-word-vectors", "anchor_text": "official blog post"}, {"url": "https://github.com/salesforce/cove", "anchor_text": "code"}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "research paper"}, {"url": "https://allennlp.org/elmo", "anchor_text": "official blog post"}, {"url": "https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md", "anchor_text": "code"}, {"url": "https://medium.com/@poojarao126/overview-of-transfer-learning-in-nlp-d74749d00d77", "anchor_text": "here"}, {"url": "https://twitter.com/poojaraosb", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/pooja-rao/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----fd2e8c72eb98---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fd2e8c72eb98---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----fd2e8c72eb98---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fd2e8c72eb98---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/100daysofmlcode?source=post_page-----fd2e8c72eb98---------------100daysofmlcode-----------------", "anchor_text": "100daysofmlcode"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd2e8c72eb98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&user=Pooja+Rao&userId=46ba9746950e&source=-----fd2e8c72eb98---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd2e8c72eb98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&user=Pooja+Rao&userId=46ba9746950e&source=-----fd2e8c72eb98---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd2e8c72eb98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffd2e8c72eb98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fd2e8c72eb98---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fd2e8c72eb98--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@poojarao126?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@poojarao126?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pooja Rao"}, {"url": "https://medium.com/@poojarao126/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "33 Followers"}, {"url": "http://poojarao.in", "anchor_text": "poojarao.in"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46ba9746950e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&user=Pooja+Rao&userId=46ba9746950e&source=post_page-46ba9746950e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F46ba9746950e%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverview-of-transfer-learning-in-nlp-part-ii-fd2e8c72eb98&user=Pooja+Rao&userId=46ba9746950e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}