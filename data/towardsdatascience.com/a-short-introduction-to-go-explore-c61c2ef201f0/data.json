{"url": "https://towardsdatascience.com/a-short-introduction-to-go-explore-c61c2ef201f0", "time": 1683009840.428441, "path": "towardsdatascience.com/a-short-introduction-to-go-explore-c61c2ef201f0/", "webpage": {"metadata": {"title": "A short introduction to Go-Explore | by Fabian Stern | Towards Data Science", "h1": "A short introduction to Go-Explore", "description": "What we are seeing these days in the machine learning community is an actual arms race: every month, every week researchers publish new papers that claim to produce state-of-the-art (SOTA) results on\u2026"}, "outgoing_paragraph_urls": [{"url": "https://paperswithcode.com", "anchor_text": "paperswithcode", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1901.10995v2.pdf", "anchor_text": "new method", "paragraph_index": 0}], "all_paragraphs": ["What we are seeing these days in the machine learning community is an actual arms race: every month, every week researchers publish new papers that claim to produce state-of-the-art (SOTA) results on a dataset. I am overall interested in a lot of areas in machine learning so I really struggle to keep up with the flood of new papers. One website that helped me a lot in this regard is paperswithcode that features comprehensive overviews of SOTA models over a myriad of different datasets. One of the extremely competitive areas are the classical Atari games. Just this year researchers from Google proposed a new method to achieve superhuman performance on the two notoriously difficult games Pitfall and Montezuma\u2019s Revenge. Here\u2019s how they did it.", "First of all, we need two to understand why especially these two games are so hard. The whole idea of Reinforcement learning is that the agent learns through the collection of rewards from the environment. The agent will try a number of series of actions and prefer a combination of actions that yields the highest combined reward. The optimal environment has very dense rewards so that the agent immediately gets feedback on how good its action was. An example of this is Pacman, where the maze is full of coins to collect. A problem occurs if the rewards are too sparse. Then the agent lacks feedback to improve his actions. According to the Google paper, the two main problems that arise from those sparse rewards are detachment and derailment", "To counter the problem of sparse rewards people came up with the concept of intrinsic rewards (IM). Here rewards are artificially spread out evenly throughout the whole environment in order to encourage the agent to explore.", "The easiest way to illustrate detachment is through a simple picture. Take a look at image 1. Here the IMs are in green and spread in both mazes.", "The agent starts in the middle and at first explores the maze on the left side. Most reinforcement learning algorithms act stochastically, so at some point in time the agent might decide to start exploring the right maze. Now the problem is there is still some part in the maze on the left that was not explored and therefore still has rewards but it is unlikely that the agent will collect them. The reason for this is another problem in reinforcement learning called catastrophic forgetting. When an agent explores a new area it overrides its past experiences and will therefore worse the longer ago it visited a certain area.", "To understand this problem we need to look a little bit under the hood of reinforcement learning algorithms. In a lot of environments, there is a conflict between exploration and exploitation, i.e. the agent can take an already well-known path where a good reward is guaranteed (exploitation) or perform new actions hoping that this will lead to an even higher reward with the risk of receiving a lower one (exploration). To counter this conflict a lot of algorithms employ some kind of \u025b-greedy policy. This means that in most cases the agent will perform the action that is optimal to the best of its knowledge (exploitation) but with some probability, it will perform a random action (exploration). This can be problematic. Imagine an agent wants to reach a certain state that can only be reached through a combination of a lot of actions. It is unlikely that the agent will even reach this state because along the way it will perform a number of different actions, it derails from the actual desired path.", "The Google researchers have developed a method to solve both detachment and derailment. It consists of two distinct phases, namely exploring and robustifying.", "This stage does not use any neural nets or other machine learning algorithms but is solely based on random (maybe semi-guided) exploration. Here the authors introduced the idea of cells. Cells are downscaled and grayscale images of the actual game frames (image 2).", "The main goal here is to find interesting cells. What are those? Interesting cells that have not been found before and where the agent collected a high reward to get there. Whenever a new cell is discovered four values are added to an archive of cells:", "If a cell is encountered that is already in the archive the corresponding entry is updated in case it is \u2018better\u2019, i.e. higher total reward or shorter trajectory.", "In every iteration the algorithm does the following:", "These two steps help to prevent derailment and detachment. We build an archive of states and how to get there, so good ones will not get lost (prevents detachment). In the first step, we directly go to the state, there is no policy and therefore no random action involved. Derailment is therefore also avoided.", "The first phase made a huge implicit assumption: that our environment is deterministic. In order to \u2018go\u2019 to a cell from our archive, we need to be able to reset the environment and be sure that the series of actions that we stored really lead to our desired \u2018good\u2019 cell. This is easy to achieve in a computer game, especially in a simple Atari game, because here the same actions will always lead to the same results. But of course people also want to use reinforcement methods in the real world which is often hardly deterministic. That\u2019s what we need this second phase for: make the trajectory to good outcome cells that we found through random exploration more robust to noise and non-determinism.", "To do this, the paper makes use of the so-called backward algorithm. Image, we have a series of cells c(1), c(2), \u2026, c(n-1),c(n) that we found as very good in the exploring stage, and now want to make more robust. In the first step, we set our environment to c(n-1) and train a reinforcement algorithm to perform the actions that are necessary to reach c(n). If this algorithm finds a trajectory that gets the same or higher reward compared to our randomly found we go one step back to c(n-2) and try to reach c(n) from there. This is an iterative process that continues until we find a good policy that finds a series of actions to go from c(1) (start state) to c(n) (end state).", "As mentioned above, Atari games are deterministic, but we want to make our policy more robust to non-determinism. So, what to do? There are two relatively simple actions to introduce noise in our environment:", "The results achieved with the Go-Explore method are outstanding: not only perform they much better than any other reinforcement algorithm, but they also operate on a superhuman level (image 3).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Student and machine learning enthusiast from Germany"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc61c2ef201f0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@fabianstern97?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabianstern97?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "Fabian Stern"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdaec84828e49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&user=Fabian+Stern&userId=daec84828e49&source=post_page-daec84828e49----c61c2ef201f0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc61c2ef201f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc61c2ef201f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jamie452?utm_source=medium&utm_medium=referral", "anchor_text": "Jamie Street"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://paperswithcode.com", "anchor_text": "paperswithcode"}, {"url": "https://arxiv.org/pdf/1901.10995v2.pdf", "anchor_text": "new method"}, {"url": "https://arxiv.org/pdf/1901.10995v2.pdf", "anchor_text": "[1]"}, {"url": "https://arxiv.org/pdf/1901.10995v2.pdf", "anchor_text": "[1]"}, {"url": "https://arxiv.org/pdf/1901.10995v2.pdf", "anchor_text": "[1]"}, {"url": "https://arxiv.org/pdf/1901.10995v2.pdf", "anchor_text": "Go-Explore: a New Approach for Hard-Exploration Problems"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c61c2ef201f0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/atari?source=post_page-----c61c2ef201f0---------------atari-----------------", "anchor_text": "Atari"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----c61c2ef201f0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc61c2ef201f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&user=Fabian+Stern&userId=daec84828e49&source=-----c61c2ef201f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc61c2ef201f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&user=Fabian+Stern&userId=daec84828e49&source=-----c61c2ef201f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc61c2ef201f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc61c2ef201f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c61c2ef201f0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c61c2ef201f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabianstern97?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabianstern97?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fabian Stern"}, {"url": "https://medium.com/@fabianstern97/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "30 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdaec84828e49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&user=Fabian+Stern&userId=daec84828e49&source=post_page-daec84828e49--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fdaec84828e49%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-short-introduction-to-go-explore-c61c2ef201f0&user=Fabian+Stern&userId=daec84828e49&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}