{"url": "https://towardsdatascience.com/a-neural-network-from-scratch-c09fd2dea45d", "time": 1683006735.127985, "path": "towardsdatascience.com/a-neural-network-from-scratch-c09fd2dea45d/", "webpage": {"metadata": {"title": "A neural network from scratch. Predicting Darth Vader using neurons\u2026 | by Dennis Bakhuis | Towards Data Science", "h1": "A neural network from scratch", "description": "In this post, I would like to show you how to create a neural network in Python from scratch. The only external library we will be using is Numpy for some linear algebra. Because it is May the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/dennisbakhuis/Tutorials/blob/master/2_Neural_Network/Artificial_Neural_Network.ipynb", "anchor_text": "a Jupyter Notebook", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/environments-conda-pip-aaaaah-d2503877884c", "anchor_text": "here is a short explanation", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/master-python-in-10-minutes-a-day-ac32996b5ded", "anchor_text": "Pip, conda, aaah is part of my Python-10-minutes-a-day course", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88", "anchor_text": "my previous tutorial", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/environments-conda-pip-aaaaah-d2503877884c", "anchor_text": "my prior Jupyter Notebook", "paragraph_index": 11}, {"url": "https://www.wolframalpha.com/", "anchor_text": "Wolfram Alpha", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Chain_rule", "anchor_text": "chain-rules", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/python-pitfall-mutable-default-arguments-9385e8265422", "anchor_text": "blog-post", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Differentiation_rules#The_chain_rule", "anchor_text": "chain rule", "paragraph_index": 36}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network", "anchor_text": "my Github", "paragraph_index": 50}, {"url": "https://www.kaggle.com/azeembootwala/titanic", "anchor_text": "Kaggle Titanic competition /preprocessed by Azeem Bootwala", "paragraph_index": 56}, {"url": "https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning", "anchor_text": "Deep learning course", "paragraph_index": 61}, {"url": "https://en.wikipedia.org/wiki/Rose_(mathematics)", "anchor_text": "Rose-functions", "paragraph_index": 61}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network", "anchor_text": "Github", "paragraph_index": 62}, {"url": "https://www.youtube.com/watch?v=Vxq9yj2pVWk", "anchor_text": "\u2018enhance the resolution\u2019", "paragraph_index": 72}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network", "anchor_text": "Jupyter Notebook", "paragraph_index": 74}, {"url": "http://niceclipart.com/6378/star-wars-darth-vader.html", "anchor_text": "niceclipart.com", "paragraph_index": 74}, {"url": "https://linkedin.com/in/dennisbakhuis", "anchor_text": "LinkedIn", "paragraph_index": 78}], "all_paragraphs": ["In this post, I would like to show you how to create a neural network in Python from scratch. The only external library we will be using is Numpy for some linear algebra. Because it is May the fourth, as a bonus, we are going to use this freshly created neural network to fit a complex message, intercepted from Mustafar. Coding a neural network step by step will help you understand the inner workings, which are very similar to popular frameworks such as Torch and Tensorflow. Personally, I think that build things from scratch is the best way to master a topic and of course, a lot of fun!", "\u2018This is the way\u2019 \u2014 Din Djarin", "This post is also available as a Jupyter Notebook on my Github, so you can code along while reading.", "If you are new to Python and Jupyter, here is a short explanation on how I manage my Python environment and packages.", "\ud83d\udc49 Pip, conda, aaah is part of my Python-10-minutes-a-day course!", "A short overview of the topics we will be discussing:", "When we would ask a random person about Machine Learning, there is a big chance that neural networks are mentioned. Not only does the terminology play with our imagination, but these mathematical structures have also proven themselves to solve complex tasks. Examples of such tasks are object detection, audio transcribing, and text translation. Neural networks can be relatively small, like the one shown in figure 1, yet still be powerful and predict complex systems.", "In the diagram, we show an artificial neural network (ANN), or just simply neural network (NN), which has three layers. By convention, we do not count the input layer, and later we see that this layer in the diagram represents the input data you feed into the NN. This particular NN has two hidden layers. While I am not sure why it is exactly called a hidden layer, I can imagine that one reason could be that these layers are \u2018hidden\u2019 from the user. A user inputs data through the input layer and gets results from the output layer, therefore, not interacting with the hidden layers. For the user, there is no difference between a system with one or twenty hidden layers. In contrast to the input layer, the output is an actual layer, in our case a single neuron, which \u2018collects\u2019 the results from the former hidden layer.", "In my previous tutorial, I tried to explain how a logistic regression (and a bit of linear regression) works. A logistic regression can be seen as the tiniest possible NN, with just a single layer, consisting of a single neuron. It is probably best to first start with a short recap of the logistic model shown in figure 2.", "For this example, the first step is to massage the input data in such a way that the individual features (\ud835\udc651, \ud835\udc652, \ud835\udc653) are in the rows of our input vector \ud835\udc4b, and the columns are the examples (training samples). This input vector \ud835\udc4b is used in the forward pas of our single neuron. This neuron is divided in two operations. First, a linear operation (linear regression), Z =\ud835\udc4b\ud835\udc4a+\ud835\udc4f, followed by an activation function \ud835\udc34=\ud835\udc54(\ud835\udc67). In the logistic regression tutorial, we performed binary logistic regression. This specifically uses the Sigmoid activation function \ud835\udf0e(\ud835\udc67)=1/(1+exp(\u2212\ud835\udc67)) and therefore we have denoted it there as \ud835\udf0e(\ud835\udc67). Here we generalize this by using \ud835\udc54(\ud835\udc67) to indicate an activation function. We have to define which activation function we will be using. As we will learn today, there are many activation functions to choose from.", "The activation function uses as input the result from the linear part of the neuron. This is the inner product (also called dot product) of the weights vector \ud835\udc4a and the input vector \ud835\udc4b, with the bias term \ud835\udc4f added. The bias term \ud835\udc4f and the weights vector \ud835\udc4a (consisting of \ud835\udc641, \ud835\udc642, \ud835\udc643) are the trainable parameters of this system. Each trainable weight (\ud835\udc641, \ud835\udc642, \ud835\udc643) corresponds to an input feature (\ud835\udc651, \ud835\udc652, \ud835\udc653) and represents the \u2018weight\u2019 this feature is adding to the problem. In the diagram these weights are shown inside the dashed frame of the output layer, meaning that these weights are linked to that layer.", "I hope this short recap was clear, otherwise, I could recommend my prior Jupyter Notebook for a more thorough explanation and a step by step example in Numpy.", "While logistic regression is a great tool, it can only divide the parameter space in a line, at least in the form we have presented here. For example, if you have two features \ud835\udc651 and \ud835\udc652, which will be used to predict \ud835\udc66, the logistic classifier is only able to have a linear boundary between the two parameters. If this does not make sense yet don\u2019t worry about it as we will work on an example to illustrate this problem.", "To have the system predict more complex relations, we can add more neurons to a layer or add more layers to our network. Each of the neurons is some sort of logistic regression unit and many of these combined can predict highly non-linear relations (see figure 3). I say kind of, because in regular logistic regression we typically use the Sigmoid activation function, while in a NN, many other activation functions perform much better.", "Before we will start to generalize a NN and the used layers, observe how the different neurons are connected in figure 3. You will notice that each node is connected to all nodes of the next layer. This is called densely connected (sometimes fully connected) and such a layer is often called a \u2018Dense layer\u2019. In the next section, we will try to generalize the network and identify the required structures.", "Before we can start coding our system, let us first try to generalize the steps required. The goal of this tutorial is to create a general NN class in which we can add an arbitrary number of layers, containing an arbitrary number of neurons. Later we will test this structure on various problems with varying complexity.", "When we think of the input layer, we have already discovered that it is not an actual layer of the neural network, but the input data, reshaped in the right form. Therefore, the input \u2018layer\u2019 is not part of our architecture. Of course, the input data has a number of features and therefore, defines at least one dimension of our first layer.", "The neural network will consist of an arbitrary number of layers. These will be structures that behave in a similar fashion and are sequential to each other. This means that we will have a layer structure, and some sort of parent structure to hold all the layers. When doing the forward pass, we will loop through all the layers and use the output of the previous layer as input for the next layer. The output of the final layer, also called the final activation \ud835\udc34 is the output of the neural network. This means that, if the neural network is used to predict binary values, the output has to be converted (or rounded) to the actual predictions.", "To test our prediction, we need to have a cost function defined. A cost function is a measure on how well you model predicts and therefore needs the predicted values and the true values. In the most formal sense, we need to define a loss function, which calculates the error on the prediction of a single example, and combine all these losses in a cost function to get one measure for the complete batch we input into our model. The loss and cost functions are only required once and not in every layer and therefore, it should be implemented in the parent structure.", "To optimize the weights and hopefully improve the model, we need to calculate the gradients using the backward pass. First we need to calculate the gradient of the loss function in the parent class. This will be the input for the last layer, and from that point we loop backwards to calculate the gradients \ud835\udc51\ud835\udc4a and \ud835\udc51\ud835\udc4f and with each iteration, the input of the previous layer will be the input of the next. To calculate the gradients we need the inputs from the forward pass and for this we can do a clever trick and cache the values of \ud835\udc67 and \ud835\udc34 during the forward pass. It may seem intimidating, but these steps are mostly bookkeeping. All of these steps are visualized in the diagram of figure 4.", "After we have calculated the gradients for each layer, we can do an update step and perform a step of gradient descent. This step is practically identical to the one we discussed in the logistic regression tutorial, in which we use the learning rate to \u2018move\u2019 the weights a step closer to the true results. This time however, this has to be done for each layer inside our neural network.", "We should now realize that all layers in the network are fundamentally the same. They can differ in the amount of nodes or the activation function, but there is no structural difference between the first layer \ud835\udc3f1 and the output layer \ud835\udc3f\ud835\udc5c. This means that we can create one layer structure as one building block for our neural network.", "Our neural network will only consist of Dense layers, i.e. layers in which all neurons are fully connected to the neurons in the next layer, so we will only make a single layer structure. In this structure, we have to do the math for the forward pass, the backward pass, and the update step. The input of each layer is the output of the previous layer. Obviously, for the first layer, this is the input vector. For the backward pass, we sequentially go through the network in the reverse direction. However, here we need to do an additional step as we need to calculate the gradients with respect to the defined loss function. For this we need to input the true labels \ud835\udc4c and the predicted output \ud835\udc34 during the backward pass. After this, we have a value that a layer would expect, which we will call \ud835\udc51\ud835\udc34. Each layer handles the backwards pass in the exact same manner.", "Both, the forward and backward passes of each layer are illustrated in figure 5. Let us go through each pass, step by step. In the forward pass, it expects as input, the output of the previous layer \ud835\udc34\ud835\udc5d (or input vector \ud835\udc4b). Before the linear equation, the input \ud835\udc34\ud835\udc5d is cached for the backward pass. Next, we calculate the linear part in a single vectorized inner product. This is done for all neurons in the layer as one single step. After the inner product, the bias term is added. Next, we also cache the value of \ud835\udc67\ud835\udc5d for the backward pass. Finally, we will calculate the activation function and pass the result to the next layer (or if this is the final layer, this is the output).", "The backward pass is just the forward pass in reverse, but expects the previous gradient as an input. First we need to calculate the differential of the activation function. These are relatively easy to calculate (or found on the internet). We calculate the gradients, using the cached values of \ud835\udc34\ud835\udc5d and \ud835\udc67\ud835\udc5d. As a last step we calculate \ud835\udc51\ud835\udc34\ud835\udc5d which will be the input to the next layer. While the math is similar to the previous tutorial, it is now a bit more streamlined. I do not want to focus to much on the actual differentials, but will explain them a bit more in the code. If you really want to know how these differentials are calculated, I would suggest to get a pen and paper and try to calculate them. They are not hard and Wolfram Alpha can help you :-).", "As a final note, you should realize that in the backward pass we are doing the mother of all chain-rules. To calculate the gradient of the first layer, we have to chain all other differentials together. If it is not completely clear what each step is doing, do not worry to much about it. I will try to explain each step during the coding part. Experience and understanding come from experimentation.", "Before we start coding, let us first make a short summary on what we are planning to make. The idea is to create two structures, classes in this case. One class defines the layer, while the other class acts as a parent and holds all the layers of the complete neural network.", "We will be using linear algebra routines from Numpy and therefore, need to import it. Also, it is good practice to define meaningful errors, so we will be defining a couple of exceptions. Another way is using Python\u2019s logging module, which is another great tool.", "Next, let us create our new class called DenseLayer. This class takes a constructor with two required and two optional parameters:", "As there can me different activation functions and we do not want to check which activation we have used an if-statement, we make a reference to the used activation functions in the init-statement.", "In the initialize method, the weights are initialized. Notice that the number of neurons nh (units) are in the rows, and the number of input features nx are the columns. This is required to make our dot product work later.", "Another thing I learned recently is the use of \u2018self\u2019 in Python. While I thought I understood the concept, I did not fully understand the consequences. The definition of the class is separated from the values of each instance. These values are stored in the self-object, i.e. the object of the instance itself. In other programming languages it is common to define types it the class itself. However, in Python, you have to define these in the init-method. If you would not do this, the variable is shared over all your instances and you can get weird results. I just found this blog-post where Don Cross has a very clear explanation. Recommended read if you are interested!", "Next, we define all used activation functions. The Sigmoid we already know from the previous Tutorial, however, we also include Tanh and Relu, which are also very commonly used activation functions.", "Tanh or hyperbolic tangent in full is a function which is similar to the Sigmoid, however, it maps all real values to values between -1 and +1. It was very popular before the Relu function made its way to the stage.", "Relu, which stands for the Rectified linear unit, is probably the most popular activation function at the moment. It is fast to calculate and has often better results than the Tanh function. Therefore, if you are not sure, the Relu function is a great start. The Relu function maps all values smaller than 0 to zero and all values larger than 0 as the value itself.", "The last activation function we will introduce is called the linear function. This is the same as not having an activation function and is just a placeholder. What comes in, comes out and we use it to test our previous exercise on linear regression.", "The backward pass needs the differentials for all these functions, which are provided with the Grad suffix. Feel free to check if these differentials are correct. Note that each function has one additional step which is not part of the function differential itself, but required to apply the chain rule. We multiply the incoming differential with the calculated differential. This step is run continuously through the whole backward propagation.", "Next, we define the forward propagation step, which should look very familiar to the logistic regression version:", "We first calculate the linear part. Store the values for Z and A for later use in the backpropagation and next apply the activation function.", "The backward propagation should look familiar as well, however, we have split the differential part of the activation function to the functions itself. Also, this function expects dA as the input, which is the more generalized form for multiple layers. In our previous single layer example, we combined the differential of the loss function in this step. This step is now included in our parent class and not in each layer, as it is only required just before the last layer.", "The gradients are stored in each layer and can be used later by the update function, which performs the gradient descent step. The backward passes chain together for each layer as we will see later when we work on the parent class. The only required function remaining is the update function, which performs the gradient descent step. Nothing spectacular but it expects a learning rate.", "While not required, these next function help in printing the model and return the number of output nodes, which are used as an input dimension for the next layer.", "Alright, one down and one to go. Next, we will create a class that will combine the layers. It will also hold the loss function and has to calculate the gradient of the loss. For convenience, we will also add a wrapper to add layers and add a way to pretty print our model.", "First, we start again with the constructor, which has two options, the loss function to be used, and the randomMultiplier used for new layers. The loss function is again created in a function reference and are called using wrapper functions. The model initializes with no layers (empty).", "The next method is a helper function to add layers to the model. You need to give the inputDimension, i.e. the number of input features of the first layer. For the second and further layers, it will look for the previous layer and use that as the input dimension.", "You have to specify the number of neurons in the layer (units) and which activation function to use. If you do not specify an activation function, it will not use an activation function and you will end up having a linear system.", "Next, we define the loss functions and their differentials. Feel free to check if the differentials are correct. the cost method is a wrapper to call the appropriate cost function in the training loop, we will be defining later.", "The forward, backward, and update method are quite similar as they loop over all the layers. Only the backward pass has to first calculate the gradient of the loss function, which is then used as the input for the first (from the right) layer.", "The last two methods are straight forward and are used for pretty printing and one for printing trainable parameter. Nothing special.", "Alright, the classes are done. Now we need to put these classes to a test. While it looks quite ordered, a small error in one of the differentials can make our whole system useless. Therefore, we will test in small steps in the next section. Looking forward to the spin!", "If you want to copy all the classes in one go, you can download them from my Github.", "Nothing is more annoying than waiting for nothing. Therefore, it is important to first test the easy things instead of training for hours to see that you made a mistake in the loss function. Let us repeat the experiments we have done in the last experiments.", "We created an array with a couple of input values \ud835\udc4b. Next we supplied \ud835\udc4c using the super-duper complicated formula \ud835\udc66=2\ud835\udc65+1. In Numpy this looks like this:", "Now we will build our model which will try to find our difficult formula and match the true value of 25. For this, we will initiate our fresh class and set the loss to mean-square-error. Next, we will add a single layer, with a single neuron and no activation function. Let's also try our pretty print :-)", "Now we will need our training loop again. It will look very familiar to our logistic regression model from last time:", "The cost decreases as expected and due to the long training, the value is close to machine precision. Would we again find 25 when we input 12 in the forward method?", "Bad news, the Titanic sank yet again and we are in need of a binary logistic classifier. Without too many comments, we import the data (source: Kaggle Titanic competition /preprocessed by Azeem Bootwala):", "We need to create a model which has 14 input features and of course, the sigmoid activation function. The loss function will be cross-entropy, which is the default, so we do not need to specify it.", "Before we create the training loop, we define, just like last time, an accuracy function to quickly calculate the accuracy. Last time we had an accuracy of approximately 80%.", "Now, let's run the training loop and see if we can match the previous result.", "As you can see, not bad at all.", "The cool stuff starts with more complex functions. The Deep learning course from Andrew Ng showed a way to predict Rose-functions using a model with multiple nodes. Let's try that as well!", "First, we need to import the data. The code to generate the data is in the Jupyter Notebook on Github.", "The mathematical equation creates beautiful flower-like structure as seen in figure 7. We have colored three of the 7 petals to a different value and are will now use our neural network to predict on the x,y coordinates if the value should be 0 or 1 (the color).", "But before we do this let us first show what will happen when we try to predict this using a model that is too simple, like a logistic regression, which can only have a linear boundary. In the next code snippet I include the model creation, the training loop, and a helper function to visualize the result.", "The result is shown on the left in figure 8. Definitely not a great fit and clearly a line. Now let's add another layer with four units to the model, with the last layer being the same Sigmoid layer. The activation function for this model we keep similar to the one from Andrew, which has a Tanh activation function.", "This is something that really amazes me. Just a tiny layer more and we have the power to learn this much more complex function. Really great stuff!", "As it is the May the 4th (be with you), it cannot be a coincidence that we have intercepted a secret transmission from Mustafar. It is of the utmost importance that we train our model with this data, such that our ai (read: model) can fully comprehend the nature of this transmission.", "The first step is again, import the data, that is the 2d transmission data. I also created some helper functions to create models, a training loop, and one the visualize and analyse the transmission.", "What you will learn when creating larger models and especially more complex models, is that the amount of hyper parameters vastly increases. There are guidelines on how to choose them, but there is no golden rule and therefore, you often have to try many of them.", "I did a parameter sweep in which I automated the search to find the optimal number of layers, units, and iterations. I have done this quite crude and it took 15 hours to run almost 100 models. The parameter search is included in the Jupyter Notebook, in the appendix section. The parameters below gave great results with a loss lower than 0.09. The loss value itself does not have a meaning, it is just a measure of how well the predicted value matches the true value and should be minimized. Feel free to experiment with these settings yourself. If you find better parameters, please share them with me :-).", "Our model has four sequential layers. The first three with many units and the Relu activation function. The last layer is a simple logistic layer. Our implementation does not have all the bells and whistles, which are available in pyTorch or Tensorflow (feel free to add them). One thing which would be great is to adjust the learning rate while training. First start with a large learning rate and decrease it when you are closer to a converge. The approach below is to simulate this.", "After 28000 iterations, we reach a state of convergence which must be sufficient to analyze the data. For this we are going to use our testModel function. Using the h parameter we can \u2018enhance the resolution\u2019 of our messages. The results of various \u2018enhancements\u2019 are shown in figure 9.", "Oh boy, it is the dark lord himself! Aaah! Of course, the is no such thing as enhance the resolution and we just fed the neural network with a larger mesh. The result is still pretty awesome, I must say myself :-).", "A cool next tryout would be to use and RGB images as input. For this, you need to reshape the input image such that the RGB channels are stacked vertically. For our current input image, this would mean, three times longer input vector (three channels). Procedure for creating and manipulating the input data are found in the Jupyter Notebook. The clip-art source of the input image was downloaded from niceclipart.com.", "I hope you guys had fun, writing your own neural network. In my opinion, writing these things from the ground up is the best way to learn how it actually works. I hope that you see that these systems are not magical, but simple matrix multiplications, unfortunately just a very lot of them. The most difficult part is, of course, the backpropagation, where we need to calculate the gradients. Our simple NNs are quite doable, but adding more layers and different types of layers, can make it a bit more cumbersome. Still, the essence is very similar to what we have done today.", "My suggestion is to play around with these structures, rewrite parts of them, or even better, write your own from scratch!", "\u2018this is the way\u2019 \u2014 Din Djarin", "Please let me know if you have any comments! Feel free to connect on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist with a passion for natural language processing and deep learning. Python and open source enthusiast. Background in fluid dynamics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc09fd2dea45d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dennisbakhuis.medium.com/?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "Dennis Bakhuis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5b8617eb89bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=post_page-5b8617eb89bb----c09fd2dea45d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc09fd2dea45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc09fd2dea45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/dennisbakhuis/Tutorials/blob/master/2_Neural_Network/Artificial_Neural_Network.ipynb", "anchor_text": "a Jupyter Notebook"}, {"url": "https://towardsdatascience.com/environments-conda-pip-aaaaah-d2503877884c", "anchor_text": "here is a short explanation"}, {"url": "https://towardsdatascience.com/master-python-in-10-minutes-a-day-ac32996b5ded", "anchor_text": "Pip, conda, aaah is part of my Python-10-minutes-a-day course"}, {"url": "https://towardsdatascience.com/a-logistic-regression-from-scratch-3824468b1f88", "anchor_text": "my previous tutorial"}, {"url": "https://towardsdatascience.com/environments-conda-pip-aaaaah-d2503877884c", "anchor_text": "my prior Jupyter Notebook"}, {"url": "https://www.wolframalpha.com/", "anchor_text": "Wolfram Alpha"}, {"url": "https://en.wikipedia.org/wiki/Chain_rule", "anchor_text": "chain-rules"}, {"url": "https://towardsdatascience.com/python-pitfall-mutable-default-arguments-9385e8265422", "anchor_text": "blog-post"}, {"url": "https://en.wikipedia.org/wiki/Differentiation_rules#The_chain_rule", "anchor_text": "chain rule"}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network", "anchor_text": "my Github"}, {"url": "https://www.kaggle.com/azeembootwala/titanic", "anchor_text": "Kaggle Titanic competition /preprocessed by Azeem Bootwala"}, {"url": "https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning", "anchor_text": "Deep learning course"}, {"url": "https://en.wikipedia.org/wiki/Rose_(mathematics)", "anchor_text": "Rose-functions"}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network", "anchor_text": "Github"}, {"url": "https://www.youtube.com/watch?v=Vxq9yj2pVWk", "anchor_text": "\u2018enhance the resolution\u2019"}, {"url": "https://github.com/dennisbakhuis/Tutorials/tree/master/2_Neural_Network", "anchor_text": "Jupyter Notebook"}, {"url": "http://niceclipart.com/6378/star-wars-darth-vader.html", "anchor_text": "niceclipart.com"}, {"url": "https://linkedin.com/in/dennisbakhuis", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/python?source=post_page-----c09fd2dea45d---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c09fd2dea45d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c09fd2dea45d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c09fd2dea45d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c09fd2dea45d---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc09fd2dea45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=-----c09fd2dea45d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc09fd2dea45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=-----c09fd2dea45d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc09fd2dea45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc09fd2dea45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c09fd2dea45d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c09fd2dea45d--------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dennisbakhuis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dennis Bakhuis"}, {"url": "https://dennisbakhuis.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5b8617eb89bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=post_page-5b8617eb89bb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc167ef22c4d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-neural-network-from-scratch-c09fd2dea45d&newsletterV3=5b8617eb89bb&newsletterV3Id=c167ef22c4d5&user=Dennis+Bakhuis&userId=5b8617eb89bb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}