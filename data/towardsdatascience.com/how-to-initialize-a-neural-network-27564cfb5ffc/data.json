{"url": "https://towardsdatascience.com/how-to-initialize-a-neural-network-27564cfb5ffc", "time": 1682996718.094809, "path": "towardsdatascience.com/how-to-initialize-a-neural-network-27564cfb5ffc/", "webpage": {"metadata": {"title": "How to initialize a Neural Network | by Thomas Chambon | Towards Data Science", "h1": "How to initialize a Neural Network", "description": "Training a neural net is far from being a straightforward task, as the slightest mistake leads to non-optimal results without any warning. Training depends on many factors and parameters and thus\u2026"}, "outgoing_paragraph_urls": [{"url": "http://karpathy.github.io/2019/04/25/recipe/", "anchor_text": "thoughtful approach", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1901.09321.pdf", "anchor_text": "is very important", "paragraph_index": 1}, {"url": "https://github.com/fastai", "anchor_text": "fastai library", "paragraph_index": 2}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net", "anchor_text": "in this github repository.", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1511.06422", "anchor_text": "All you need is a good init", "paragraph_index": 33}, {"url": "https://course.fast.ai/", "anchor_text": "fastai MOOC", "paragraph_index": 36}, {"url": "https://github.com/fastai/imagenette", "anchor_text": "imagenette dataset", "paragraph_index": 38}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Simple%20model.ipynb", "anchor_text": "in this notebook", "paragraph_index": 39}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py", "anchor_text": "as implemented in the fastai library", "paragraph_index": 48}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20no%20batchnorm.ipynb", "anchor_text": "in this notebook.", "paragraph_index": 50}, {"url": "https://arxiv.org/abs/1812.01187", "anchor_text": "Bag of Tricks for Image Classification with Convolutional Neural Networks", "paragraph_index": 55}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20with%20batchnorm.ipynb", "anchor_text": "in this notebook.", "paragraph_index": 57}], "all_paragraphs": ["Training a neural net is far from being a straightforward task, as the slightest mistake leads to non-optimal results without any warning. Training depends on many factors and parameters and thus require a thoughtful approach.", "It is known that the beginning of training (i.e., the first few iterations) is very important. When done improperly, you get bad results \u2014 sometimes, the network won\u2019t even learn anything at all! For this reason, the way you initialize the weights of the neural network is one of the key factors to good training.", "The goal of this article is to explain why initialization is impacting and present a different number of ways to implement it efficiently. We will test our approaches against practical examples.The code uses the fastai library (based on pytorch). All experiment notebooks are available in this github repository.", "Neural-net training essentially consists in repeating the two following steps:", "During the forward step, the activations (and then the gradients) can quickly get really big or really small \u2014 this is due to the fact that we repeat a lot of matrix multiplications. More specifically, we might get either:", "Either of these effects is fatal for training. Below is an example of explosion with randomly initialized weights, on the first forward pass.", "In this particular example, the mean and standard deviation is already huge at the 10th layer!", "What makes things even trickier is that, in practice, you can still get non-optimal results after long periods of training even while avoiding explosion or vanishing effects. This is illustrated below on a simple convnet (experiments will be detailed in the second part of the article):", "Notice that the default pytorch approach is not the best one, and that random init does not learn a lot (also: this is only a 5-layers network, meaning that a deeper network would not learn anything).", "Recall that the goal of a good initialization is to:", "What is a good range in practice? Quantitatively speaking, it implies having the output of the Matrix multiplications with the input vector produce an output vector (i.e. activations) with mean near 0 and standard deviation near 1. Then each layer will propagate these statistics across all the layers. And even on a deep network, you will have stable statistics on the first iterations.", "We now discuss two approaches to do so.", "So let\u2019s picture the issue. If the initialized weights are too big at the beginning of training, then each matrix multiplication will exponentially increase the activations, leading to what we call gradient explosion. Conversely, if the weights are too small, then each matrix multiplication will decrease the activations until they vanish completely.", "So the key here is to scale the weights matrix to get outputs of matrix multiplication with a mean around 0 and a standard deviation of 1.", "But then how to define the scale of the weights? Well, since each weight (as well as the input) is independent and distributed according to a normal distribution, we can get help by working out some math.", "Two famous papers present a good initialization scheme based on this idea:", "In practice, the two schemes are quite similar: the \u201cmain\u201d difference is that Kaiming initialization takes into account the ReLU activation function following each matrix multiplication.", "Nowadays, most neural nets use ReLU (or a similar function like leaky ReLU). Here, we only focus on the Kaiming initialization.", "The simplified formula (for standard ReLU) is to scale the random weights (drawn from a standard distribution) by:", "For instance, if we have an input of size 512:", "In addition, all bias parameters should be initialized to zeros.", "Note that for Leaky ReLU the formula has an additional component, which we do not consider here (we refer the reader to the original paper).", "Let\u2019s check how this approach works on our previous example:", "Notice that now we get an activation with mean 0.64 and standard deviation 0.87 after initialization. Obviously, this is not perfect (how could it be with random numbers?), but much better than normally-distributed random weights.", "After 50 layers, we get a mean of 0.27 and a standard deviation of 0.464, so no more explosion or vanishing effects.", "The math derivations that lead to the magic scaling number of math.sqrt(2 / size of input vector) are provided in the Kaiming paper. In addition, we provide below some useful code, which the reader can skip entirely to proceed to the next section. Note that the code requires an understanding of how to do matrix multiplications and what variance / standard deviation is.", "To understand the formula, we can think about what is the variance of the result of a matrix multiplication. In this example, we have a 512 vector multiplied by a 512x512 matrix, with an output of a 512 vector.", "So in our case, the variance of the output of a matrix multiplication is around the size of the input vector. And, by definition, the standard deviation is the square root of that.", "This is why dividing the weight matrix by the square root of the input vector size (512 in this example) gives us results with a standard deviation of 1.", "But where does the numerator of \u201c2\u201d come from? This is only to take into account the ReLU layer.", "As you know, ReLU sets the negative numbers to 0 (it\u2019s only max(0, input)). So, because we have numbers centered around a mean of 0, it basically removes half the variance. This is why we add a numerator of 2.", "The Kaiming init works great in practice, so why consider another approach? It turns out that there are some downsides of Kaming init:", "So what can we do to get a good initialization scheme, without manually customizing the Kaiming init for more complex architectures?", "The paper All you need is a good init, from 2015, shows an interesting approach. It is called LSUV (Layer-sequential unit-variance).", "The solution consists in using a simple algorithm: first, initialize all the layers with orthogonal initialization. Then, take a mini batch input and, for each layer, compute the standard deviation of its output. Dividing each layer by the resulting deviation then resets it to 1. Below is the algorithm as explained in the paper:", "After some testing, I have found that orthogonal initialization gives similar (and sometimes worse) results than doing a Kaiming init before ReLU.", "Jeremy Howard, in the fastai MOOC, shows another implementation, which adds an update to the weights to keep a mean around 0. In my experiments, I also find that keeping the mean around 0 gives better results.", "Now let\u2019s compare the results of these two approaches.", "We will check the performance of the different initialization schemes on two architectures: a \u201csimple\u201d convnet with 5 layers, and a more complex resnet-like architecture.The task is to do image classification on the imagenette dataset (a subset of 10 classes from the Imagenet dataset).", "This experiment can be found in this notebook. Note that because of randomness, the results could be slightly different each time (but it does not change the order and the big picture).", "It uses a simple model, defined as:", "Below is a comparison of 3 initialization schemes: Pytorch default\u2019s init (it\u2019s a kaiming init but with some specific parameters), Kaiming init and LSUV init.", "Note that the random init performance is so bad we removed it from results that follow.", "Activations stats after initThe first question is what are the activations stats after a forward pass for the first iteration? The closer we are to a mean of 0 and a standard deviation of 1, the better it will be.", "This figure shows the stats of the activations at each layer, after initialization (before training).", "For the standard deviation (right figure), both the LSUV and Kaiming init are close to one (and LSUV is closer). But for the pytorch default, the standard deviation is way lower.", "For the mean value though, the Kaiming init has worse results. It is understandable because Kaiming init doesn\u2019t take into account the ReLU effect on the mean. So the mean is around 0.5 and not 0.", "Now let\u2019s check if we get similar results on a more complex architecture.", "The architecture is xresnet-50, as implemented in the fastai library. It has 10x more layers than our previous simple model.", "We will check it in 2 steps:", "Step 1: Without batchnormThis experiment can be found in this notebook.", "Without batchnorm, the results for 10 epochs are:", "The plot shows that the accuracy (y-axis) is of 67% for LSUV, 57% for Kaiming init and 48% for the pytorch default. The difference is huge!", "Let\u2019s check the activations stats before training:", "Let\u2019s zoom to get a better scale:", "We see that some layers have stats of 0: it is by design of the xresnet50, and independent of the init scheme. It is a trick from the paper Bag of Tricks for Image Classification with Convolutional Neural Networks (implemented in the fastai library).", "We see that the best init scheme for this example gives much better results for the full training, even after 10 full epochs. This shows the importance of keeping good stats across the layers during the first iteration.", "This experiment can be found in this notebook.", "Because batchnorm is normalizing the output of a layer, we should expect the init schemes to have less impact.", "The results show close accuracy for all init schemes, near 88%. Note that at each run the best init scheme may change depending on the random generator.", "It shows that batchnom layers make the network less sensitive to the initialization scheme.", "The activations stats before training are the following:", "Like before, the best seems to be the LSUV init (only one to keep a mean around 0 as well as a standard deviation close to 1).", "But the results show this has no impact on the accuracy, at least for this architecture and this dataset. It confirms one thing though: batchnorm makes the network much less sensitive to the quality of the initialization.", "What to remember from this article?", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F27564cfb5ffc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thomas.chambon?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thomas.chambon?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "Thomas Chambon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff54b0d2974f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&user=Thomas+Chambon&userId=f54b0d2974f0&source=post_page-f54b0d2974f0----27564cfb5ffc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27564cfb5ffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27564cfb5ffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/photos/sport-tracks-running-run-sprint-1201014/", "anchor_text": "https://pixabay.com/photos/sport-tracks-running-run-sprint-1201014/"}, {"url": "http://karpathy.github.io/2019/04/25/recipe/", "anchor_text": "thoughtful approach"}, {"url": "https://arxiv.org/pdf/1901.09321.pdf", "anchor_text": "is very important"}, {"url": "https://github.com/fastai", "anchor_text": "fastai library"}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net", "anchor_text": "in this github repository."}, {"url": "http://proceedings.mlr.press/v9/glorot10a.html", "anchor_text": "Understanding the difficulty of training deep feedforward neural networks"}, {"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"url": "https://arxiv.org/abs/1511.06422", "anchor_text": "All you need is a good init"}, {"url": "https://course.fast.ai/", "anchor_text": "fastai MOOC"}, {"url": "https://github.com/fastai/imagenette", "anchor_text": "imagenette dataset"}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Simple%20model.ipynb", "anchor_text": "in this notebook"}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py", "anchor_text": "as implemented in the fastai library"}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20no%20batchnorm.ipynb", "anchor_text": "in this notebook."}, {"url": "https://arxiv.org/abs/1812.01187", "anchor_text": "Bag of Tricks for Image Classification with Convolutional Neural Networks"}, {"url": "https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20with%20batchnorm.ipynb", "anchor_text": "in this notebook."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----27564cfb5ffc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----27564cfb5ffc---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----27564cfb5ffc---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----27564cfb5ffc---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27564cfb5ffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&user=Thomas+Chambon&userId=f54b0d2974f0&source=-----27564cfb5ffc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27564cfb5ffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&user=Thomas+Chambon&userId=f54b0d2974f0&source=-----27564cfb5ffc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27564cfb5ffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F27564cfb5ffc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----27564cfb5ffc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----27564cfb5ffc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thomas.chambon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thomas.chambon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thomas Chambon"}, {"url": "https://medium.com/@thomas.chambon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "45 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff54b0d2974f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&user=Thomas+Chambon&userId=f54b0d2974f0&source=post_page-f54b0d2974f0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5fbbef91928a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-initialize-a-neural-network-27564cfb5ffc&newsletterV3=f54b0d2974f0&newsletterV3Id=5fbbef91928a&user=Thomas+Chambon&userId=f54b0d2974f0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}