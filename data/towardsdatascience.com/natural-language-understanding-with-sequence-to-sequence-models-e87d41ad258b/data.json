{"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "time": 1683000228.251662, "path": "towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b/", "webpage": {"metadata": {"title": "Natural Language Understanding with Sequence to Sequence Models | by Michel Kana, Ph.D | Towards Data Science", "h1": "Natural Language Understanding with Sequence to Sequence Models", "description": "How to predict the intent behind a customer query. Seq2Seq models explained. Slot filling demonstrated on ATIS dataset with Keras."}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Sutskever et al., 2014", "paragraph_index": 10}, {"url": "http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf", "anchor_text": "Cho et al., 2014", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU algorithm", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "anchor_text": "pre-trained embeddings", "paragraph_index": 12}, {"url": "https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html", "anchor_text": "ATIS dataset", "paragraph_index": 18}, {"url": "https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk", "anchor_text": "this repository", "paragraph_index": 18}, {"url": "https://github.com/neonbjb/ml-notebooks/blob/master/keras-seq2seq-with-attention/keras_translate_notebook.ipynb", "anchor_text": "(source", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU algorithm", "paragraph_index": 29}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib5", "anchor_text": "2014", "paragraph_index": 32}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib6", "anchor_text": "2016", "paragraph_index": 32}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib10", "anchor_text": "2016", "paragraph_index": 32}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib4", "anchor_text": "2018", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "anchor_text": "next article", "paragraph_index": 33}], "all_paragraphs": ["Natural Language Understanding (NLU), the technology behind conversational AI (chatbots, virtual assistant, augmented analytics) typically includes the intent classification and slot filling tasks, aiming to provide a semantic tool for user utterances. Intent classification focuses on predicting the intent of the query, while slot filling extracts semantic concepts in the query. For example the user query could be \u201cFind me an action movie by Steven Spielberg\u201d. The intent here is \u201cfind_movie\u201d while the slots are \u201cgenre\u201d with value \u201caction\u201d and \u201cdirected_by\u201d with value \u201cSteven Spielberg\u201d.", "Intent classification is a classification problem that predicts the intent label and slot filling is a sequence labeling task that tags the input word sequence. In the research it is common to find state-of-the-art performance for intent classification and slot filling using Recurrent neural network (RNN) based approaches, particularly gated recurrent unit (GRU) and long short-term memory (LSTM) models.", "This article deals with Slot Filling task. We first introduce the machine translation tasks to motivate sequence-to-sequence models, which have ruled the world of neural machine translation for years. We solve the task of Slot Filling with a sequence-to-sequence model. The approach is presented theoretically and implemented practically using the ATIS dataset, a standard benchmark dataset widely used as an intent classification and slot filling task.", "Language translation is hard, not only for humans. In the old days we used Rule-based Machine Translation, having linguists creating and maintaining rules for converting text in the source language to the target language, at the lexical, syntactic, or semantic level. Later, Statistical Machine Translation rather used statistical models that learn to translate given a large corpus of examples. Given a text in the source language, e.g. French (f), what is the most probable translation in the target language, e.g. English (e)? The translation model p(f|e) is trained on parallel corpus and the language model p(e) is calculated on a target corpus only (English).", "Although effective, and applied and commercialized for years by big players such as IBM and Google, statistical machine translation methods suffered from a narrow focus on the phrases being translated, losing the broader nature of the target text. Moreover the statistical approaches require careful tuning of each module in the translation pipeline.", "Back in the old time, traditional phrase-based translation systems performed their work by breaking down sentences into multiple chunks and translating them phrase-by-phrase. This approach created disfluent translation.", "Neural Machine Translation came to life in 2014, introducing the use of neural network models to learn a statistical model for machine translation. Since then, we do not have to calculate the rules for conditional probabilities. The networks learn their weights and discover all the rules and probabilities, which linguists and statisticians would spend a tremendous amount of energy to code.", "In our previous article, we show how recurrent neural networks, especially LSTMs have been used for 20 years to predict a sequence element at time step t based on the previous element at time step t-1. Could recurrent neural networks help for language translation as well?", "Directly using a LSTM to map a sequence of words from one language to another runs into problems quickly. For a single LSTM to work, you would need input and output sequences to have the same sequence lengths, and for translation they rarely do. For example, the two-words English phrase \u201cis playing\u201d is translated to the one-word German \u201cspielt.\u201d", "The winner solution to the problem introduced in the previous section is the encoder-decoder architecture, the so-called sequence-to-sequence learning with neural network and its ability to encode the source text into an internal fixed-length representation called the context vector. Once encoded, different decoding systems could be used to translate the context into different languages.", "Sutskever et al., 2014, Cho et al., 2014 invented this approach. Sutskever\u2019s model achieved a BLEU score of 34.81 in 2014, which was a quantum jump compared to the baseline score developed with a statistical machine translation system of 33.30 for English to French translation. This was the first example of a neural machine translation system that outperformed a phrase-based statistical machine translation baseline on a large scale problem. Sutskever\u2019s model was used to re-score the list of best translations and improved the score to 36.5 what brought it close to the best result at the time of 37.0. The BLEU algorithm (BiLingual Evaluation Understudy) evaluates the quality of a translation, by comparing the number of n-grams between the candidate translation and the reference. Sutslever\u2019s model was trained on a subset of the 12 million sentences in the dataset, comprised of 348 million French words and 304 million English words.", "The Encoder and Decoder are usually layers of LSTMs or GRUs. Although going too deep is not recommended, Google Translate was using ca. 7 layers. Few blogs recommend to feed words into the networks in inverse order to improve performance. In many practical cases, a bidirectional architecture is used in the encoder, by using a layer that learns from the original sentence in normal order and another layer that learns from the original sentence in reverse order of words. The context vectors from both layers are concatenated to produce a unique context vector that is fed into the decoder.", "In an example of a conversational agent, the input sequence \u201cHow are you\u201d is first transformed into word embedding by using a pre-trained embeddings or by training our own embedding on our dataset.", "The embedding vectors is typically of size 200 or 300. Such vector is passed through LSTM cells of the encoder to create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output \u201cI am fine\u201d.", "Because encoder and decoder are both recurrent neural networks, each time step, one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen. The input to the encoder is the vector embedding of the current word from the input sentence, e.g. \u201cJe suis \u00e9tudiant\u201d. The hidden states of the encoder are the memory representation from previous words. The last hidden state is actually the context that is passed along to the decoder, which generates a sequence of its own that represents the output \u201cI am a student\u201d.", "The context is a vector of numbers. Its size is typically the number of hidden units in the encoder RNN. Below, we show an example vector of size 4, but the context vector would usually be of size 256, 512, or 1024.", "The RNNs models used in an encoder-decoder architecture can be unidirectional or bidirectional, single or multi-layered, simple vanilla RNN or LSTM or GRU. Below we show an example of deep multi-layer unidirectional RNN which uses LSTM as a recurrent unit. Here, \u201c<s>\u201d indicates the start of the decoding process while \u201c</s>\u201d tells the decoder to stop.", "In this section, we will implement a sequence-to-sequence model for natural language understanding. Typically, this involves two tasks: Intent Detection (ID) and Slot Filling (SF). The former tries to classify a user utterance into an intent. The latter tries to \ufb01nd what are the \u201carguments\u201d of such intent.", "We train our model on the well-known ATIS dataset (Airline Travel Information System). A preprocessed version of the dataset in Pickle format was obtained from this repository. The dataset contains queries submitted by travelers to the information system. The intent of the user is labelled as well as the utterance (slot fillings).", "The snippet below stores the train data from dictionaries and tensors in separated variables. It also displays few examples of queries, their word vectors, the intent, the slots and the slots vectors.", "Below we display an example query for each intent class in a nice layout.", "As an example, let\u2019s consider the user query \u201ci want to fly from boston at 838 am and arrive in denver at 1110 in the morning\u201d. The model should classify this user query as \u201cflight\u201d intent. It should also parse the query, identify and fill all slots necessary for understanding the query. Although the words \u201cI\u201d, \u201cwant\u201d, \u201cto\u201d, \u201cfly\u201d, \u201cfrom\u201d, \u201cat\u201d, \u201cand\u201d, \u201carrive\u201d, \u201cin\u201d, \u201cthe\u201d contribute to understand the context of the intent, the model should correctly label the entities needed to fulfill user\u2019s goal in its intention to take a flight. These are \u201cboston\u201d as departure city (B-fromloc.city), \u201c8:38 am\u201d as departure time (B-depart_time.time), \u201cdenver\u201d as destination city (B-toloc.city_name), \u201c11:10\u201d as arrival time (B-arrive_time.time) and \u201cmorning\u201d as arrival period of day (B-arrive_time.period_of_day). Some of the 129 slot categories are shown in the following figure.", "As you probably noticed, the text of queries is already tokenized and a vocabulary is also provided in the ATIS dataset. Queries have a start (\u201cBOS\u201d) and end (\u201cEOS\u201d) token, as shown below.", "We create tensors by padding each query vector and slot vector to a maximum length. We provide two tensors for the target slots. One is the teacher tensor, which forces the decoder to follow a correct output slot. The other is the true target tensor, which defines what the decoder should output given the teacher tensor. The only difference between the two is that the target tensor is just the teacher tensor shifted left by one slot label. This trick was used for English to Spanish translation with a sequence-to-sequence model (source). Both input and target tensors have the shape (None, 48).", "In other to compute the vocabulary size, we combine train and test vocabularies. The input vocabulary has 943 words, while the output vocabulary has 129 words.", "The seq2seq model is created below using a GPU-ready LSTM in the encoder and decoder. The input is put through an encoder model which gives us the encoder output of shape (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size). Both encoder and decoders use an Embedding layer to project the sentences to learn a meaningful representation of the user query, which is fed to a unidirectional LSTM layer with 1024 cells.", "Because our targets (slots vectors) are not one-hot encoded, we use sparse categorical crossentropy as loss function. With Adam as optimizer in 50 epochs, we use 3982 training samples, and 996 validation samples. Accuracy is up to 98% on the training data with a validation accuracy just below 96%. The validation accuracy starts to plateau around epoch 45, suggesting this model isn\u2019t really inferring anymore and is starting overfitting.", "Prediction will require two separate models from training. We need to break up the encoder and decoder mechanisms. We then run the entire input sequence through the encoder, then create the output by predicting with the decoder one step at a time.", "Below we see the slot filling predicted for one unseen query. The model shows a good understanding of natural language text by identifying the \u201ccheapest flight\u201d as a B-cost_relative slot, \u201cboston\u201d as a B-fromloc.city_name and \u201csan fancisco\u201d as a B-toloc.city_name.", "We now evaluate the trained model on the full test dataset using the BLEU algorithm (BiLingual Evaluation Understudy), which is used to measure the quality of a translation, by comparing the number of n-grams between the predicted slot fillings and the true slot fillings.", "The results below suggests that overall the model is performing very well, especially when comparing groups of 4-grams between the predicted slots and the true ones. A BLEU score of 69% is an acceptable result.", "In this article we looked at Natural Language Understanding, especially at the special task of Slot Filling. We introduced current approaches in sequence data processing and language translation. Thanks to the practical implementation of few models on the ATIS dataset about flight requests, we demonstrated how a sequence-to-sequence model achieves 69% BLEU score on the slot filling task.", "Recently, several joint learning methods for intent classification and slot filling were proposed to exploit attention mechanisms and improve the performance over independent models (Guo et al., 2014; Hakkani-T\u00fcr et al., 2016; Liu and Lane, 2016; Goo et al., 2018).", "Thanks for reading that far. In the next article we improve our sequence-to-sequence model with attention approach. Keep reading!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Husband & Dad. Mental health advocate. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe87d41ad258b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michel-kana.medium.com/?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----e87d41ad258b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe87d41ad258b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe87d41ad258b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/lstm-based-african-language-classification-e4f644c0f29e", "anchor_text": "LSTM-based African Language ClassificationTired of German-French dataset? Look at Yemba, and stand out. Mechanics of LSTM, GRU explained and applied, with\u2026towardsdatascience.com"}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Sutskever et al., 2014"}, {"url": "http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf", "anchor_text": "Cho et al., 2014"}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU algorithm"}, {"url": "https://github.com/tensorflow/nmt", "anchor_text": "source"}, {"url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "anchor_text": "pre-trained embeddings"}, {"url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "anchor_text": "Representing text in natural language processingUnderstanding the written words: gentle review of Word2vec, GloVe, TF-IDF, Bag-of-words, N-grams, 1-hot encoding\u2026towardsdatascience.com"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "Jay. Alammar"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "Jay. Alammar"}, {"url": "https://github.com/lmthang/thesis", "anchor_text": "source"}, {"url": "https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html", "anchor_text": "ATIS dataset"}, {"url": "https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk", "anchor_text": "this repository"}, {"url": "https://github.com/neonbjb/ml-notebooks/blob/master/keras-seq2seq-with-attention/keras_translate_notebook.ipynb", "anchor_text": "(source"}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU algorithm"}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib5", "anchor_text": "2014"}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib6", "anchor_text": "2016"}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib10", "anchor_text": "2016"}, {"url": "https://www.groundai.com/project/bert-for-joint-intent-classification-and-slot-filling/#bib.bib4", "anchor_text": "2018"}, {"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "anchor_text": "next article"}, {"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "anchor_text": "Practical guide to attention mechanism for NLU tasksTested hands-on strategies to tackle attention for improving sequence to sequence modelstowardsdatascience.com"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----e87d41ad258b---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/chatbots?source=post_page-----e87d41ad258b---------------chatbots-----------------", "anchor_text": "Chatbots"}, {"url": "https://medium.com/tag/conversational-ai?source=post_page-----e87d41ad258b---------------conversational_ai-----------------", "anchor_text": "Conversational AI"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e87d41ad258b---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/augmented-analytics?source=post_page-----e87d41ad258b---------------augmented_analytics-----------------", "anchor_text": "Augmented Analytics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe87d41ad258b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----e87d41ad258b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe87d41ad258b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----e87d41ad258b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe87d41ad258b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe87d41ad258b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e87d41ad258b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e87d41ad258b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e87d41ad258b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e87d41ad258b--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://michel-kana.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69e95067d2a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b&newsletterV3=cb0b01fe20d2&newsletterV3Id=69e95067d2a1&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}