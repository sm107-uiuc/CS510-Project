{"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "time": 1683008397.554084, "path": "towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96/", "webpage": {"metadata": {"title": "Deep Learning with PyTorch. First contact with PyTorch for\u2026 | by Jordi TORRES.AI | Towards Data Science", "h1": "Deep Learning with PyTorch", "description": "This post will introduce the reader to the basics features of PyTorch which enables us to implement Deep Learning models using Python language"}, "outgoing_paragraph_urls": [{"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 2}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow vs PyTorch: The battle continues", "paragraph_index": 3}, {"url": "https://colab.research.google.com/", "anchor_text": "Colaboratory", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a", "anchor_text": "PyTorch and TensorFlow", "paragraph_index": 4}, {"url": "https://research.google.com/colaboratory/faq.html", "anchor_text": "faq page", "paragraph_index": 4}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_04_Deep_Learning_with_PyTorch.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 8}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_04_Deep_Learning_with_PyTorch.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "previous post", "paragraph_index": 9}, {"url": "http://yann.lecun.com/exdb/mnist", "anchor_text": "MNIST database page", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "previous post", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/learning-process-of-a-deep-neural-network-5a9768d7a651", "anchor_text": "other activation functions", "paragraph_index": 32}, {"url": "https://pytorch.org/docs/stable/nn.html#logsoftmax", "anchor_text": "LogSoftmax function", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "previous post", "paragraph_index": 34}, {"url": "https://pytorch.org/docs/stable/nn.html#torch.nn.Module", "anchor_text": "the oficial documentation", "paragraph_index": 41}, {"url": "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/", "anchor_text": "this article", "paragraph_index": 44}, {"url": "http://pytorch.org/docs/nn.html#torch.nn.NLLLoss", "anchor_text": "NLLloss", "paragraph_index": 58}, {"url": "https://towardsdatascience.com/pytorch-performance-analysis-with-tensorboard-7c61f91071aa", "anchor_text": "the next post", "paragraph_index": 70}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 71}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 71}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 72}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 73}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 76}], "all_paragraphs": ["We are to going to be using PyTorch in many of the posts in this series, so the reader need to make sure that she/he is familiar with it. This post will introduce the reader to the basics features of PyTorch which enables us to implement Deep Learning models using Python language. The post doesn\u2019t pretend to be a complete manual of PyTorch, it only introduces the minimum knowledge of PyTorch to start coding neural networks in PyTorch and we will be introducing new features as we need them throughout the series. Enjoy it!", "The clear leaders in Deep Learning frameworks arena are now the Google-developed TensorFlow and the Facebook-developed PyTorch, and they are pulling away from the rest of the market in usage, share, and momentum.", "Three years ago appeared the first version of PyTorch and without question, it is gaining great momentum. Initially incubated by Facebook, PyTorch rapidly developed a reputation from being an ideal flexible framework for rapid experimentation and prototyping gaining thousands of fans within the Deep Learning community. For instance, PhD students in my research team prefer to use PyTorch because it allows to them to write native looking Python code and still get all the benefits of a good framework like auto-differentiation and built-in optimization. This is the reason I decided to use PyTorch in this series.", "Though PyTorch has gained momentum in the marketplace thanks to Facebook (and AWS), TensorFlow continues to be ahead in all aspects and is the most used for the industry right now. Yo can read this brief post \u201cTensorFlow vs PyTorch: The battle continues\u201d for a more detail about both environments.", "I suggest using the Colaboratory (Colab) offered by Google to execute the code described in this post. It basically consists of a Jupyter notebook environment that requires no configuration and runs completely in the Cloud allowing the use different Deep Learning libraries as PyTorch and TensorFlow. One important feature of Colab is that it provides GPU (and TPU) totally free. Detailed information about the service can be found on the faq page.", "By default, Colab notebooks run on CPU. You can switch your notebook to run with GPU (or TPU). In order to obtain access to one GPU you need to choose the tab \u201cRuntime\u201d and then select \u201cChange runtime type\u201d as shown in the following figure:", "When a pop-up window appears, select GPU. Ensure \u201cHardware accelerator\u201d is set to GPU (the default is CPU). Afterwards, ensure that you are connected to the runtime (there is a green check next to \u201cCONNECTED\u201d in the menu ribbon):", "Now you are able to run the code presented in this post. I suggest to copy & paste the code of this post in a Colab notebook in order to see the execution meanwhile you are reading this post. Ready?", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "In this post we will program a neural network model that classifies handwritten digits presented in the previous post. Remember that we created a mathematical model that, given an image, the model identify the number it represents returning a vector with 10 positions indicating the likelihood of each of the ten possible digits.", "In order to guide the explanation, we will follow a list of steps to be taken to program a neural network:", "We always need to import torch, the core Python library for PyTorch. For our example we will also import the torchvision package, as well as the usual libraries numpy and matplotlib.", "For clarity of the code we could define here some hyperparameters that we will need for training:", "Next step is to load data that will be used to train our neural network . We will use the MNIST dataset already introduced in the previous post, which can be downloaded from The MNIST database page using torchvision.dataset. PyTorch Datasets are objects that return a single datapoint on request. Then it is passed on to a Dataloader which handles batching of datapoints and parallelism. This is the code for our example:", "Because data is usually too large to fit data into CPU or GPU memory at once, it is split into batches of equal size. Every batch includes data samples and target labels, and both of them have to be tensors (which we will present below). The BATCH_SIZE argument indicates the number of data that we will use for each update of the model parameters.", "This dataset contains 60,000 images of hand-made digits to train the model and it is ideal for entering pattern recognition techniques for the first time without having to spend much time preprocessing and formatting data, both very important and expensive steps in the analysis of data and of special complexity when working with images.", "We can verify that the previous code have loaded the expected data with the library matplotlib.pyplot :", "Remember that in the previous post we explained that to facilitate the entry of data into our neural network we make a transformation of the input (image) from 2 dimensions (2D) to a vector of 1 dimension (1D). That is, the matrix of 28\u00d728 numbers can be represented by a vector (array) of 784 numbers (concatenating row by row).", "We will apply this transformation when we ingest the data to the neural network using this type of transformation (e.g. applied to the first image):", "A Tensor is a multi-dimensional array, fundamental building block of PyTorch, equivalent to NumPy, that stores a collection of numbers:", "And we can know its dimensions and size with:", "Apart from dimensions, a tensor is characterized by the type of its elements. For this we have the dtype argument that is deliberately similar to the standard NumPy argument type of the same name:", "Torch defines nine types of CPU tensor and nine types of GPU tensor:", "As you can see there are a specific types for GPU tensors. PyTorch transparently supports CUDA GPUs, which means that all operations have two versions \u2014 CPU and GPU \u2014 that are automatically selected. The decision is made based on the type of tensors that you are operating on.", "There are different ways to create a tensor in PyTorch: calling a constructor of the required type, converting a NumPy array (or a Python list) into a tensor or asking PyTorch to create a tensor with specific data. For example we can use torch.zeros() function to create a tensor filled with zero values:", "An element of a tensor can be accessed using its index (which starts at 0):", "Furthermore, just like in the usual data structures in Python, we can use the range notation in indexing to select and manipulate portions of the tensor with the help of the \u201c : \u201d character. Indexes start at 0 and we can use negative values for the indexes, where -1 is the last element and so on. Let\u2019s look at a the following code for examples:", "PyTorch tensors can be converted to NumPy matrices and vice versa very efficiently. By doing so, we can take advantage of the tremendous amount of functionality in Python ecosystem that has evolved around the NumPy array type. Let\u2019s see with a simple code how it works:", "This array x can be easily converted to a tensor as follows:", "We can see that the second print indicates that it is a tensor. Conversely, if we want to transform a tensor into a NumPy array, we can do it as follows:", "We will use reshape() function, that returns a tensor with the same data and number of elements as input , but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy (in memory):", "In the torch.nn package, you can find many predefined classes providing the basic functionality blocks required for programming neural networks. To define the model presented in the previous post, it can be done with the Sequentialclass from this package:", "The code is defining a neural network composed of a two dense layers (linear layer) of 10 neurons each, one with a Sigmoid activation function and the other with the Softmax activation function. As we advance the series we will introduce other activation functions, as ReLU, that we will use in a next post in this series.", "I would like to highlight that the previous code adds a small transformation to the neural network presented in the previous post: additionally, it is applying a logarithm operation to each of the outputs of the last layer. Specifically, the LogSoftmax function which can be seen as:", "where Softmax is calculated as defined in the previous post. There are a number of practical and theoretical advantages of LogSoftmax over Softmax that motivate its use in building neural networks that we will discuss in a later section.", "In summary, the network that we have defined can be visually represented as shown in the following figure:", "The first layer of the neural network receives a tensor of 784 characteristics that represent the pixels that are passed to each and every one of the 10 neurons in the first layer. Once these 10 neurons have processed this information, each of them passes the information to all the neurons in the next layer, that is, all 10 neurons in the first layer are connected to all 10 neurons in the second layer.", "The second layer is a layer with a softmax activation function of 10 neurons, which means it will return a tensor of 10 probability values, representing the 10 possible digits. In general, the output layer of a classification network will have as many neurons as classes, except in a binary classification, where it is only needed one neuron. Let\u2019s remember that we have used a LogSoftmax layer, instead of Softmax, so that each value that returns will be the logarithm of the probability that the image of the current digit belongs to each of the classes.", "We can use this simple example to analyze the parameters that make up a neural network. For example, in the first layer, for each of the ten neurons, 784 parameters are required for the weights, and therefore 10 \u00d7 784 parameters to store the weights of the 10 neurons. Furthermore, 10 additional parameters are required for the 10 biases corresponding to each of the neurons. Therefore, for the first layer 7,850 parameters are required.", "In the second layer, being a softmax function, it is required to connect all its 10 neurons with the 10 neurons of the previous layer and, therefore, 10 \u00d7 10 parameters are required for the weights; in addition to the 10 biases corresponding to each node. This gives us a total of 110 required parameters in the second layer.", "In summary, for our extremely simple neural network we see that 7,960 parameters are required, 7,850 parameters for the first layer and 110 for the second.", "By subclassing the nn.Module, the base class for all neural network modules, we can create our own building blocks, which can be stacked together and reused later, which is what is usually done. But given the initiation nature of this post, we can move forward with this basic way of defining our neural networks. The reader can check the oficial documentation for more details on this topic.", "As we shown in the previous post these models are trained by solving iteratively an unconstrained optimization problem. In each iteration, a random batch of the training data is fed into the model to compute the Loss function value. Then, the gradient of the Loss function with respect to the weights of the network is computed (backpropagation) and an update of the weights in the negative direction of the gradient is done. These networks are trained until they converge into a Loss function minimum.", "Loss functions, around 20 different in PyTorch, reside in the nn package and are implemented as an nn.Module subclass. Some of the common standard loss functions that we will use in this series are:", "Since we are dealing with a multi-class classification problem, we chose cross-entropy as our Loss function. In this example, we use negative log-likelihood nn.NLLLoss () that combines with the softmax nn.LogSoftmax () function that we have already introduced. As we said we don\u2019t apply Softmax to increase the numerical stability of the training process presenting an alternative way to calculate the Softmax first, which uses exponentiation, and then calculating cross-entropy loss, which uses a logarithm of probabilities. If the reader is interested in more details about this, I recommend having a look at this article.", "The downside of defining a neural network with LogSoftmax is that we need to remember to apply softmax every time we need to get probabilities from our neural network output.", "In general, the reader will see the Loss function assigned to criterion in PyTorch codes:", "Therefore the way to calculate the error made will be as:", "where we indicate in the arguments the output of our neural network and the correct label.", "Remember that the optimizer takes the gradients of model parameters and change these parameters in order to decrease the loss value. We make use of torch.optim which is a module provided by PyTorch to optimize the model, perform gradient descent and update the weights by backpropagation. This package allows us to choose between several algorithms (AdaGrad, RMSProp, Adam, etc.) that are different variants of the gradient descent algorithm, the generic optimization algorithm capable of finding optimal solutions to a wide range of problems. At the moment in this example we will use the basic Stochastic Gradient Descent (SGD):", "The arguments are the parameters that the optimizer must adjust and a learning rate that indicates how these adjustments should be. Remember that the optimizer iteratively adjusts the parameters (weights and biases) of the model in the right direction (adding a \u201clittle\u201d or subtracting a \u201clittle\u201d to its value, where this \u201clittle\u201d is defined by the learning rate) so that lead to a decrease in error. In general, the procedure is repeated until the error falls below an acceptable level.", "Derivatives are used to calculate the correct direction, and specifically the gradient of the error with respect to the parameters. The autograd package in PyTorch provides exactly this functionality by automatic differentiation to automate the computation of backward passes in neural networks.", "Once our model has been defined and the learning method configured, it is ready to be trained. Therefore, we only have to define the training loop that will iterate over all the data so that the optimizer iteratively adjusts the weights. Let\u2019s discuss the common blueprint of a training loop with these lines of code:", "Line 1: Usually, the training loop iterates over our data over and over again. Remember that one iteration over a full set of examples is called an epoch. The EPOCHS variable indicates the number of iterations over the full set of examples.", "Line 2: We already presented that data is usually too large to fit into CPU or GPU memory at once, so it is split into batches of equal size. Every batch includes data samples and target labels, and both of them have to be tensors.", "Line 3: To facilitate the entry of data into our neural network we must make a transformation of the input (image) from 2 dimensions (2D) to a vector of 1 dimension (1D).", "Line 4: We pass each batch of image tensors into the model which will return a tensor having predictions for that batch, the forward pass.", "Line 5: Having got the predictions we pass them into the cross-entropy Loss function ( criterion ) along with their actual labels and calculate the Loss. Usually, Loss functions accept two arguments: output from the network (prediction) and desired output (ground-truth data, which is also called the label of the data sample).", "Some PyTorch\u2019s Loss functions take class labels as their targets (e.g. NLLloss), so if we use them (as in our case), we don\u2019t need to convert targets into one-hot vectors as we presented in the previous post (for ease the explanation).", "Line 6: We do a backward pass using the Loss value in order to compute gradient of the Loss with respect to model parameters. After loss.backward() call is finished, we have the gradients accumulated.", "Line 7: Now it\u2019s time for the optimizer to modify the model parameters with the method step() that takes all gradients from the parameters and applies them.", "Line 8: The last, but not least, piece of the training loop is our responsibility to zero gradients of parameters. Calling zero_grad() on our network clear the gradients of all optimized variables.", "In order to check how the training process evolve we have added a couple of lines of code in this training loop. First, we get the training loss for the entire epoch by adding all the losses for each batch iteration:", "And second averaging it over by the iteration count and print it:", "Looking at this output we see how the training loop adjusts the weights of the network so that in each iteration the loss function produces a smaller loss.", "In PyTorch, there is no a \u201cprefab\u201d data model tuning function as fit() in Keras or Scikit-learn, so the training loop must be specified by the programmer.", "Now, that we have finished the training of our model, we will probably want to test how well our model was generalized by applying it on a test dataset.", "In PyTorch it is required again that the programmer specifies the evaluation loop:", "The reader can see the similarity of the instructions in this loop with those of the previous training loop. But in this case, instead of keeping the Loss calculation, the Accuracy is calculated, that is, the percentage of hits with data that the model has never seen before.", "And that\u2019s all! You have already programmed an entire neural network. Congrats!", "See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa93b09bdae96&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----a93b09bdae96---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa93b09bdae96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa93b09bdae96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 04"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/8-pytorch-b\u00e1sico-a60ce5fc8b74", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/8-pytorch-b%C3%A1sico-a60ce5fc8b74", "anchor_text": "8. PyTorch b\u00e1sicoAcceso abierto al cap\u00edtulo 8 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow vs PyTorch: The battle continues"}, {"url": "https://colab.research.google.com/", "anchor_text": "Colaboratory"}, {"url": "https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a", "anchor_text": "PyTorch and TensorFlow"}, {"url": "https://research.google.com/colaboratory/faq.html", "anchor_text": "faq page"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_04_Deep_Learning_with_PyTorch.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_04_Deep_Learning_with_PyTorch.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "previous post"}, {"url": "https://torres.ai", "anchor_text": "torres.ai"}, {"url": "http://yann.lecun.com/exdb/mnist", "anchor_text": "MNIST database page"}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "previous post"}, {"url": "https://towardsdatascience.com/learning-process-of-a-deep-neural-network-5a9768d7a651", "anchor_text": "other activation functions"}, {"url": "https://pytorch.org/docs/stable/nn.html#logsoftmax", "anchor_text": "LogSoftmax function"}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "previous post"}, {"url": "https://torres.ai", "anchor_text": "torres.ai"}, {"url": "https://pytorch.org/docs/stable/nn.html#torch.nn.Module", "anchor_text": "the oficial documentation"}, {"url": "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/", "anchor_text": "this article"}, {"url": "http://pytorch.org/docs/nn.html#torch.nn.NLLLoss", "anchor_text": "NLLloss"}, {"url": "https://towardsdatascience.com/pytorch-performance-analysis-with-tensorboard-7c61f91071aa", "anchor_text": "the next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----a93b09bdae96---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a93b09bdae96---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a93b09bdae96---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a93b09bdae96---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----a93b09bdae96---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa93b09bdae96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----a93b09bdae96---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa93b09bdae96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----a93b09bdae96---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa93b09bdae96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa93b09bdae96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a93b09bdae96---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a93b09bdae96--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a93b09bdae96--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a93b09bdae96--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-pytorch-a93b09bdae96&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}