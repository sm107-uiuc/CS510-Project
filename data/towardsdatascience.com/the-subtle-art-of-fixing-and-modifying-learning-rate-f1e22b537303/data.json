{"url": "https://towardsdatascience.com/the-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303", "time": 1682997222.038243, "path": "towardsdatascience.com/the-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303/", "webpage": {"metadata": {"title": "The Subtle Art of Fixing and Modifying Learning Rate | by Abinash Mohanty | Towards Data Science", "h1": "The Subtle Art of Fixing and Modifying Learning Rate", "description": "Learning rate is one of the most critical hyper-parameters and has the potential to decide the fate of your deep learning algorithm. If you mess it up, then the optimizer might not be able to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1506.01186", "anchor_text": "Cyclical Learning Rates for Training Neural Networks", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1608.03983", "anchor_text": "Stochastic Gradient Descent with Warm Restarts", "paragraph_index": 14}, {"url": "https://github.com/bckenstler/CLR", "anchor_text": "here", "paragraph_index": 15}], "all_paragraphs": ["Learning rate is one of the most critical hyper-parameters and has the potential to decide the fate of your deep learning algorithm. If you mess it up, then the optimizer might not be able to converge at all! It acts as a gate which controls how much the optimizer is updating the parameters w.r.t. gradient of loss. Speaking in terms of equations, gradient is given by:", "Using this gradient from the mini-batch, stochastic gradient descent follows the estimated downhill:", "where \u03f5 is the learning rate. The following figure explains the effects of learning rate on gradient descent. A very small learning rate will make gradient descent take small steps even if the gradient is big, thus slowing the process of learning. If the learning rate is high, then if becomes impossible to learn very small changes in the parameters needed to fine tune the model towards the end of the training process, so the error flattens out very early. If the learning rate is very high, then gradient descent takes big steps and jumps around. This can lead to divergence and thus increase the error.", "Now the question arises, what is the best value of the learning rate and how to decide it ? A systematic way to estimate a good learning rate is by training the model initially with a very low learning rate and increasing it (either linearly or exponentially) at each iteration (illustrated below). We keep doing it to the point where the loss stops decreasing and starts to increase. That means that the learning rate is too high for the application and so gradient descent is diverging. For practical applications our learning rate should ideally be 1 or 2 step smaller than this value.", "If we keep track of the learning rate and plot log of the learning rate and the error we will see a plot as shown below. A good learning rate somewhere to the left to the lowest point of the graph (as demonstrated in below graph). In this case, its 0.001 to 0.01.", "In general no fixed learning rate works best for the entire training process. Typically we start with a learning rate found using the method described above. During the training process we change learning rate to best facilitate learning. There are many different ways to accomplish this. In this blog, we will go through a few popular learning rate scheduler.", "Step decay schedule drops the learning rate by a factor every few epochs. Mathematical form of step decay is:", "where, \u03f5_{k} is the learning rate for k_{th} epoch, \u03f5_{0} is the initial learning rate, \u03b1 is the fraction by which learning rate is reduced, \u230a . \u230b is floor operation and N is the number of epochs after which learning rate is dropped.", "In Tensorflow, this can be done easily. To modify the learning rate we need a variable to store the learning rate and a variable to store the number of iterations.", "This technique is also known as learning rate annealing. We start with a relatively high learning rate and then gradually lower it during training. The intuition behind this approach is that we\u2019d like to traverse quickly from the initial parameters to a range of \u201cgood\u201d parameter values but then we\u2019d like a learning rate small enough that we can explore the \u201cdeeper, but narrower parts of the loss function\u201d (fine tuning the parameters to get best results).", "In practice, it is common to decay the learning rate until iteration \u03c4. In case of linear decay, the learning rate is modified in the following manner:", "In Tensorflow, this can be implemented like we implemented step decay. In this case, we make staircase=False, this uses a floating division and thus leads to gradual decrease in learning rate.", "This technique is also very popular and its intuitive also. Keep using a big learning rate to quickly approach a local minima and reduce it once we hit a plateau (i.e. this learning rate is too big for now, we need smaller value to be able to fine tune the parameters more). The term plateau refers to the point when the change in loss w.r.t. training iterations is less then a threshold \u03b8. What it essentially means is the loss vs iterations curve becomes flat. This is illustrated in the figure below.", "These sort of custom learning rate decay scheduler can be easily implemented by making the learning rate a placeholder. We then calculate the learning rate based on some set of rules and pass it to Tensorflow in the feed_dict along with other data (input, output, dropout ratio, etc).", "All the schemes we discussed so far were targeted at starting with a large learning rate and making it smaller as training progressed. some works like Cyclical Learning Rates for Training Neural Networks and Stochastic Gradient Descent with Warm Restarts suggest otherwise. The underlying assumption behind cyclic learning rate is \u201c increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect\u201d. The authors of these works have demonstrated that a cyclical learning rate schedule which varies between two bound values can deliver results better than the traditional learning rate schedules. The intuition behind why cyclic learning rates work are:", "The codes for cyclic learning rate can be found here.", "As an end note, there is no single learning rate schedule scheme that works best for all applications and architectures. I almost always start my experiments with the linear decay scheme (decaying the learning rate until some iteration and then keeping it constant). You can always start with some scheme and if the training error doesn\u2019t go down as expected, try some other scheme. Happy coding.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff1e22b537303&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1e22b537303--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1e22b537303--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@abinashm89?source=post_page-----f1e22b537303--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abinashm89?source=post_page-----f1e22b537303--------------------------------", "anchor_text": "Abinash Mohanty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7437301380b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&user=Abinash+Mohanty&userId=7437301380b4&source=post_page-7437301380b4----f1e22b537303---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1e22b537303&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1e22b537303&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://homework-writer.com/blog/lab-report", "anchor_text": "internet"}, {"url": "https://arxiv.org/abs/1506.01186", "anchor_text": "Cyclical Learning Rates for Training Neural Networks"}, {"url": "https://arxiv.org/abs/1608.03983", "anchor_text": "Stochastic Gradient Descent with Warm Restarts"}, {"url": "https://arxiv.org/abs/1703.04933", "anchor_text": "article"}, {"url": "https://github.com/bckenstler/CLR", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f1e22b537303---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/learning-rate?source=post_page-----f1e22b537303---------------learning_rate-----------------", "anchor_text": "Learning Rate"}, {"url": "https://medium.com/tag/neural-network-training?source=post_page-----f1e22b537303---------------neural_network_training-----------------", "anchor_text": "Neural Network Training"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f1e22b537303---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1e22b537303&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&user=Abinash+Mohanty&userId=7437301380b4&source=-----f1e22b537303---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1e22b537303&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&user=Abinash+Mohanty&userId=7437301380b4&source=-----f1e22b537303---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1e22b537303&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1e22b537303--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff1e22b537303&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f1e22b537303---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1e22b537303--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f1e22b537303--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f1e22b537303--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f1e22b537303--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f1e22b537303--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f1e22b537303--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f1e22b537303--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f1e22b537303--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abinashm89?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abinashm89?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abinash Mohanty"}, {"url": "https://medium.com/@abinashm89/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "15 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7437301380b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&user=Abinash+Mohanty&userId=7437301380b4&source=post_page-7437301380b4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9e470393afdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303&newsletterV3=7437301380b4&newsletterV3Id=9e470393afdc&user=Abinash+Mohanty&userId=7437301380b4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}