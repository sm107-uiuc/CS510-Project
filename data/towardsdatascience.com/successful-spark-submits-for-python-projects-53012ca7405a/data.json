{"url": "https://towardsdatascience.com/successful-spark-submits-for-python-projects-53012ca7405a", "time": 1683005861.024411, "path": "towardsdatascience.com/successful-spark-submits-for-python-projects-53012ca7405a/", "webpage": {"metadata": {"title": "Successful spark-submits for Python projects. | by Kyle Jarvis | Towards Data Science", "h1": "Successful spark-submits for Python projects.", "description": "TL; DR: The best way to learn is to do, and the best way to learn something hard is to do it in small steps. This is a guide for Python practitioners (probably data scientists) working on a package\u2026"}, "outgoing_paragraph_urls": [{"url": "https://packaging.python.org/tutorials/packaging-projects/", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://spark.apache.org/docs/latest/running-on-yarn.html", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://spark.apache.org/docs/latest/running-on-yarn.html", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/how-to-setup-the-pyspark-environment-for-development-with-good-software-engineering-practices-5fb457433a86", "anchor_text": "How to setup the Python and Spark environment for development, with good software engineering practices", "paragraph_index": 31}, {"url": "https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f", "anchor_text": "Best Practices Writing Production-Grade PySpark Jobs", "paragraph_index": 32}], "all_paragraphs": ["TL; DR: The best way to learn is to do, and the best way to learn something hard is to do it in small steps. This is a guide for Python practitioners (probably data scientists) working on a package in an isolated development environment, who want to start successfully executing Spark jobs using that package on a cluster. The big data tech stack can seem daunting at first: this article is aimed at those who are just getting to grips with working effectively with a cluster, who want to get their project off the ground and running, then feel confident to explore, build on this experience, and refine their style.", "Enough, already! Take me to the guide (scroll down).", "When writing, developing and testing our Python packages for Spark, it\u2019s quite likely that we\u2019ll be working in some kind of isolated development environment; on a desktop, or dedicated cloud-computing resource. Crucially, the Python environment we\u2019ve been at liberty to put together, the one with our favourite minor versions of all the best packages, is likely to be different from the Python environment(s) accessible to a vanilla spark-submit job executed on our friendly neighbourhood cluster. Generally, it\u2019s desirable to have some degree of flexibility, and be able to quickly incorporate additional dependencies into a prototype environment. In the heat and excitement of developing our code, the foresight to prepare a cluster-wide environment compatible with our package, ready to receive and execute our Spark job may have eluded us. Now, this unforeseen hurdle is stopping us from running our project at scale, which is sad for a data scientist.", "In what follows we\u2019ll walk through the steps necessary to get a Python package running on Spark using spark-submit, in a transparent and accessible way. The steps that are covered in the guide are:", "The existing resources relevant to this topic (see further reading) are mostly aimed at producing production-grade Spark jobs with Python. They are really great, but could pose a steep learning curve to those less familiar with the intricacies and nuances of working Spark in cluster mode; there\u2019s a lot to take in. If you\u2019re just starting out with these kinds of big data technologies, you\u2019ll want a good understanding of the basics. This guide focuses on the basics and key concepts, highlighting the things you\u2019ll definitely encounter when starting out.", "By trade, I\u2019m a data scientist. I work on a commodity cluster running Hadoop and use Spark to build big-data pipelines for machine-learning and data-science projects, processing extremely large data sets containing billions of rows. Development happens in an isolated environment, with Anaconda as the Python distribution. Typically, I\u2019ll have set up a couple of different virtual environments in order to decouple the Spark processing steps from other aspects of the project.", "In development, my project will be structured something like this.", "I\u2019ll use a Jupyter notebook to play around and tweak transformations, scrutinising the results until they match what I\u2019m trying to achieve. Eventually, once all the code is doing exactly what it should be, the next step is to package it up, and call the transformation steps from a script that we will pass to a spark-submit job. The spark-submit job will setup and configure Spark as per our instructions, execute the program we pass to it, then cleanly release the resources that were being used. A simply Python program passed to spark-submit might look like this:", "Getting this to work smoothly on a cluster, fresh out of an isolated development environment can be tricky. There are lots of moving parts. This guide exists because, I\u2019ve gone through this process myself, and I think having this to refer to would have sped things up considerably.", "Disclaimer: I won\u2019t begin to go into how to properly configure and create a Python object bound to a Spark session. That is a topic in its own right, and it\u2019s assumed that you\u2019ve had success in doing that already. Here we delegate the intricacies of this task to the hypothetical get_spark() .", "The first step is to package up all the python files, modules and scripts that belong to the package, i.e. the contents of the./src/ directory. There\u2019s nothing special about this step, you can read about the use of setuptools here. Create your setup.py file and python setup.py bdist_egg . After this step, we\u2019ll have some new files and directories, including a.egg , which for the purposes of this guide is just like a .zip . When we run our spark-submit job later, we\u2019ll let the program know about the egg file containing our project source code, and it will send this to the driver and workers. When our application is launched, the top-level of any artefacts (eggs, zips) passed to --py-files is added to PYTHON_PATH , which is how we can import our package from a program passed to spark-submit.", "2. Preparing a minimal working environment.", "When running my Spark jobs with a Python driver program, I like to use a minimal working environment. It\u2019s too easy to end up with a bloated virtual environment when prototyping and developing, so this is a useful point to revisit whether any pruning can be done. If the program we\u2019re running is ultimately expressed as pure Spark (vs. doing anything with numpy, pandas) then I find I can get very far with only the builtin modules and a yaml parser.", "Let\u2019s go ahead and use conda to create a new environment without any of the packages that are installed by default (which we probably don\u2019t need for our spark job).", "Running the above command will give us an extra directory in ./envs/ that contains our minimal working environment. We are likely to use this environment over and over again as we tweak our Spark program, so we\u2019ll upload it to the Hadoop file system, rather than shipping and distributing it every time we want to run a Spark job. We\u2019ll upload our environment to Hadoop as a .zip, that will keep everything neat, and we can tell spark-submit that we\u2019ve created an archive we\u2019d like our executors to have access to using the --archives flag. To do this, first follow these steps:", "Now move up one level and run the following command:", "Remember this hdfs path, we\u2019ll refer to it later in our spark-submit script. The commands ran above will have made the below additions to our project directory.", "Having prepared and uploaded our spark_submit_env to hdfs, we now turn to preparing the specific dependencies that our Spark job depends upon. There are multiple ways of achieving the same result here, and it\u2019s pretty straightforward. These steps will be intuitive for anyone who has ever packaged a project, however, we do want to make sure we have the right versions of the packages for the specific version of Python we have in our spark_submit_env . Scribble down the packages we need for our Spark job in a requirements.txt :", "Then, source activate spark_submit_env , so that which pip reassuringly tells us /path/to/your/project/envs/bin/pip . Now we can be confident the packages we need for our spark-submit job will play nicely with the environment we\u2019ve prepared. We can now pip install the dependencies listed in requirements to a specified directory that we\u2019ll zip up and pass to Spark.", "pip install -t ./dependencies -r requirements.txt", "Note that we could directly package the dependencies as part of the working environment setup over the course of development and distribute this, but explicitly listing and bundling the package dependencies separately keeps things nice and transparent. We can also cut out superfluous packages that are useful for eda, visualisation etc., but not directly required for the steps delegated to Spark.", "This is where we bring together all the steps that we\u2019ve been through so far. This is the script we will run to invoke Spark, and where we\u2019ll make it aware of the Python environment we prepared, our packaged project and its dependencies. The spark-submit script will look something like this:", "Here we declare the variable PYTHON_ZIP and assign to it the file url where we uploaded our Python environment in step 2. The #pythonlib suffix is a little trick that allows us to specify name aliases when running Spark on YARN. We can use it to specify a file name alias for anything passed to --py-files or --archives . This alias is the actual name the Spark app will see. You can read more here.", "Here we\u2019re setting the environment variable PYSPARK_PYTHON to point to the python executable that we package into our spark_submit_env. Search for the above here to read more.", "Anything that\u2019s passed to --archives get\u2019s unzipped into the working directory of each executor, so passing the hdfs url of the zipped environment is how these executors access the binaries and modules of the environment we prepared.", "Individual files passed to --files are also copied to the executors working directories, again we can name them using the /path/to/file#alias syntax. For example, we can pass a yaml file to be parsed by the driver program, as illustrated in spark_submit_example.py .", "After specifying our [OPTIONS] we pass the actual Python file that\u2019s executed by the driver:spark_submit_example.py, as well as any command line arguments for the program, which can be parsed in the usual way. For example, the first argument might be a yaml file appConf.yml , and there might be subsequent arguments to provide more context for the program.", "We\u2019re finally in a position to execute the spark-submit script that we\u2019ve described above:", "One final word of caution, when working with virtual environments, be cautious of modifications to PATH . I\u2019ve run into issues previously when trying to spark-submit without remembering to deactivate a particular virtual environment.", "If you\u2019ve made it this far I hope it\u2019s because you\u2019ve found the contents of this guide helpful \u2014 I\u2019d love to hear if that\u2019s the case. I\u2019d be more than willing to expand on any specifics, should anyone be sufficiently engaged to request that. I\u2019m very open to feedback. So, if you spot an inconsistency or mistake, please shout; I\u2019ll promptly remove the article and delete all traces of it ever having existed, and my account, and my LinkedIn. I will leave the inhospitable environments pictogram though, that took me a long time.", "When you\u2019re confident with the steps that have been covered in this guide, you might like to look at the following resources:", "How to setup the Python and Spark environment for development, with good software engineering practices", "Best Practices Writing Production-Grade PySpark Jobs", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Experimental Physicist turned full-stack data-science enthusiast. Working with big data and machine learning in a global investment bank."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F53012ca7405a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----53012ca7405a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----53012ca7405a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kyle.jarvis1905?source=post_page-----53012ca7405a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.jarvis1905?source=post_page-----53012ca7405a--------------------------------", "anchor_text": "Kyle Jarvis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5de09a73c99f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&user=Kyle+Jarvis&userId=5de09a73c99f&source=post_page-5de09a73c99f----53012ca7405a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53012ca7405a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53012ca7405a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://packaging.python.org/tutorials/packaging-projects/", "anchor_text": "here"}, {"url": "https://spark.apache.org/docs/latest/running-on-yarn.html", "anchor_text": "here"}, {"url": "https://spark.apache.org/docs/latest/running-on-yarn.html", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/how-to-setup-the-pyspark-environment-for-development-with-good-software-engineering-practices-5fb457433a86", "anchor_text": "How to setup the Python and Spark environment for development, with good software engineering practices"}, {"url": "https://medium.com/@lubna_22592/building-production-pyspark-jobs-5480d03fd71e", "anchor_text": "Building Production PySpark Jobs"}, {"url": "https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f", "anchor_text": "Best Practices Writing Production-Grade PySpark Jobs"}, {"url": "https://medium.com/tag/python?source=post_page-----53012ca7405a---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/spark?source=post_page-----53012ca7405a---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/data-science?source=post_page-----53012ca7405a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/hadoop?source=post_page-----53012ca7405a---------------hadoop-----------------", "anchor_text": "Hadoop"}, {"url": "https://medium.com/tag/big-data?source=post_page-----53012ca7405a---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53012ca7405a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&user=Kyle+Jarvis&userId=5de09a73c99f&source=-----53012ca7405a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53012ca7405a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&user=Kyle+Jarvis&userId=5de09a73c99f&source=-----53012ca7405a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53012ca7405a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----53012ca7405a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F53012ca7405a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----53012ca7405a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----53012ca7405a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----53012ca7405a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----53012ca7405a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----53012ca7405a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----53012ca7405a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----53012ca7405a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----53012ca7405a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----53012ca7405a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.jarvis1905?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kyle.jarvis1905?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kyle Jarvis"}, {"url": "https://medium.com/@kyle.jarvis1905/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "46 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5de09a73c99f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&user=Kyle+Jarvis&userId=5de09a73c99f&source=post_page-5de09a73c99f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F5de09a73c99f%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsuccessful-spark-submits-for-python-projects-53012ca7405a&user=Kyle+Jarvis&userId=5de09a73c99f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}