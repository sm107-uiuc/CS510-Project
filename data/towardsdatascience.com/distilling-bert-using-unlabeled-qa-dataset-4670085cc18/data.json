{"url": "https://towardsdatascience.com/distilling-bert-using-unlabeled-qa-dataset-4670085cc18", "time": 1683015499.227071, "path": "towardsdatascience.com/distilling-bert-using-unlabeled-qa-dataset-4670085cc18/", "webpage": {"metadata": {"title": "Distilling BERT Using an Unlabeled Question-Answering Dataset | Towards Data Science", "h1": "Distilling BERT Using an Unlabeled Question-Answering Dataset", "description": "How to leverage unlabeled data for a question-answering task using knowledge distillation"}, "outgoing_paragraph_urls": [{"url": "https://blog.griddynamics.com/question-answering-system-using-bert/", "anchor_text": "question answering system for a photo & video cameras online store", "paragraph_index": 3}, {"url": "https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html", "anchor_text": "USE-QA", "paragraph_index": 14}, {"url": "https://github.com/huggingface/transformers/tree/master/examples/distillation", "anchor_text": "examples", "paragraph_index": 15}, {"url": "https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255", "anchor_text": "Gradient accumulation", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/2006.10029", "anchor_text": "SimCLRv2", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1911.04252", "anchor_text": "Noisy Students", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1908.08962", "anchor_text": "Well-Read Students Learn Better", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1806.03822", "anchor_text": "Percy Liang, Know What You Don\u2019t Know: Unanswerable Questions for SQuAD", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network", "paragraph_index": 22}], "all_paragraphs": ["The data labeling process is quite complicated, especially for tasks such as Machine Reading Comprehension (Question Answering). In this post, I want to describe one of the techniques we used to adapt the question-answering model to a specific domain using a limited amount of labeled data \u2014 Knowledge Distillation. It turned out that we can use it not only to \u201ccompress\u201d the model but also to leverage in-domain unlabeled data.", "One of the simplest forms of Question Answering systems is Machine Reading Comprehension (MRC). There the task is to find a short answer to a question within the provided document. The most popular benchmark for MRC is the Stanford Question Answer Dataset (SQuAD) [1]. It contains 100,000 question-answer pairs and 53,775 unanswerable questions written for 23,215 paragraphs from popular Wikipedia articles. The answer to every question is a segment of text (a span) from the corresponding reading passage. For unanswerable questions, the system should determine when no answer is supported by the paragraph and abstain from answering.", "There are also other question-answering datasets available, such as Natural Questions and MS MARCO, but SQuAD 2.0 is one of the most used and it was the starting point for our project.", "Recently, we worked on a question answering system for a photo & video cameras online store where we trained a machine reading comprehension model. In our project, we tested various pre-trained question answering models (thanks to \ud83e\udd17 Hugging Face) on our small labeled dataset and found that ALBERT-xxlarge [2] trained on SQuAD 2.0 shows promising results on our domain:", "But the model is so slow that we cannot use it in production. One of the commonly used methods in such cases is the use of Knowledge Distillation.", "Knowledge Distillation [3] is usually used as a model compression technique when we want to train a faster or smaller model. In this process, we train a smaller model (student) using output probabilities from our primary larger model (teacher), so a student starts to \u201cimitate\u201d its teacher\u2019s behavior. The loss based on comparing output distributions is much richer than from only hard targets. The idea is that by using such soft labels, we can transfer some \u201cdark knowledge\u201d from the teacher model. Additionally, learning from soft labels prevents the model from being too sure about its prediction, which is similar to a label smoothing [4] technique.", "Knowledge distillation is a beautiful technique that works surprisingly well and is especially useful with Transformers \u2014 larger models often show better results, but it\u2019s hard to put such big models into production.", "The models distilled or trained on SQuAD don\u2019t show competitive results on our dataset. The SQuAD dataset doesn\u2019t overlap with our domain, so such distillation doesn\u2019t work well. It also seems that larger models trained on SQuAD work much better on out-of-domain data. To make the distillation process work, we need to use datasets from our domain. Besides our small labeled dataset, we also had about 30,000 unlabeled (no highlighted answers) question-document pairs, and we thought about how we can use them.", "What do we need for distillation? A teacher and a labeled dataset. ALBERT-xxlarge can be a teacher model. We don\u2019t have labels for our 30K examples, but can we just remove part of the loss which uses labels? Sure, we will inherit more teacher mistakes during knowledge distillation without ground-through labels. But we don\u2019t have models better than ALBERT-xxlarge at the moment, so even getting similar results with a smaller model would be useful for us. So, we tried to distill the knowledge from ALBERT-xxlarge to ALBERT-base using only unlabeled 30K examples.", "As you can see, we got ALBERT-base with F1/EM close to its teacher, and we didn\u2019t use any labeled data for training. Of course, it doesn\u2019t mean we don\u2019t need labeled data anymore, the score is still far from ideal, and we also inherited teacher mistakes, so adding labeled data may improve this training procedure.", "We can also think about distillation as an additional pre-training step to achieve better sample-efficiency when using labeled data. Below, you can see that distillation helps for further fine-tuning on labeled data in our case.", "The better teacher you have, the better student you can probably train. So, one of the main directions can be improving the teacher model. Besides using our labeled dataset, there is another exciting technique \u2014 self-distillation, when we use the same model architecture for both student and teacher. Self-distillation allows us to train a student model that can perform better than its teacher. That may sound strange, but because a student updates its weights learning from the data that the teacher didn\u2019t see, this can lead to a slightly better performance of the student (of comparable size) on the data from that distribution. Our experiments also reproduced this behavior when we applied self-distillation for ALBERT-xxlarge and then used it as one of our teachers for further distillation to a smaller model.", "And of course, the distillation training procedure allows us to effectively use an ensemble of teacher models and distill them to a single smaller model. Combining all those approaches (see below) and a domain-adaptive language pre-training, we were able to achive good results using a limited number of labeled examples.", "Worth to say that even though we didn\u2019t label any data for distillation, we still had questions, which is not always the case. Compared to NLP tasks like text classification and NER, the question-answering model\u2019s input consists of a question and a document. And even though the question is an input, it can be considered as a part of the data labeling process (you have to write a question, not just highlight an answer). In this way, a truly unlabeled QA dataset is when we only have documents without questions.", "But even though we had questions, we didn\u2019t manually prepare each question to a specific document. We collected these 30,000 QA pairs matching the questions with a set of independent documents using the pre-trained USE-QA model. In such a way, we can start with a pre-trained model and approach the further model improvements in production. After we start collecting real questions asked by users interacting with our system, we can find candidate documents for these questions in the same way and use this dataset for knowledge distillation without labeling many examples.", "We used the examples from \ud83e\udd17 Hugging Face Transformers for knowledge distillation, and 1x NVIDIA 2080TI for all of our experiments. Gradient accumulation allowed us to work with larger models such as ALBERT-xxlarge and RoBERTa-large, and mixed precision (fp16) to train the models faster. To do self-distillation for ALBERT-xxlarge on one 2080TI, we first pre-computed the teacher\u2019s predictions (soft labels).", "Knowledge distillation is a handy technique that allows us to \u201ccompress\u201d huge Transformer models to use them in production. We can also use it in the form of pre-training on unlabeled data, which can help improve sample-efficiency when fine-tuning on labeled examples. That can be especially useful for tasks such as machine reading comprehension, where the process of labeling data is quite complicated. Distillation using unlabeled data is not a novel idea, and it already showed good results in both computer vision (SimCLRv2, Noisy Students) and NLP (Well-Read Students Learn Better).", "If you\u2019d like to learn more about our journey of building a question answering system, check out our more detailed post, where we describe how we collected and labeled data, fine-tuned the model, and applied various techniques such as knowledge distillation and pruning:", "To learn more about knowledge distillation and other compression methods, I recommend the following resources:", "And the following resources will help you dive into the Machine Reading Comprehension task:", "[1] Pranav Rajpurkar, Robin Jia, Percy Liang, Know What You Don\u2019t Know: Unanswerable Questions for SQuAD (2018), ACL 2018", "[2] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ALBERT: A Lite BERT for Self-supervised Learning of Language Representations (2019)", "[3] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, Distilling the Knowledge in a Neural Network (2015), NIPS 2014 Deep Learning Workshop"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4670085cc18&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@sparakhin?source=post_page-----4670085cc18--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4670085cc18--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sparakhin?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Sergey Parakhin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b6af71300d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=post_page-2b6af71300d6----4670085cc18---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4670085cc18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=-----4670085cc18---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4670085cc18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&source=-----4670085cc18---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@alfonsmc10?utm_source=medium&utm_medium=referral", "anchor_text": "Alfons Morales"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://blog.griddynamics.com/question-answering-system-using-bert/", "anchor_text": "question answering system for a photo & video cameras online store"}, {"url": "https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html", "anchor_text": "USE-QA"}, {"url": "https://github.com/huggingface/transformers/tree/master/examples/distillation", "anchor_text": "examples"}, {"url": "https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255", "anchor_text": "Gradient accumulation"}, {"url": "https://arxiv.org/abs/2006.10029", "anchor_text": "SimCLRv2"}, {"url": "https://arxiv.org/abs/1911.04252", "anchor_text": "Noisy Students"}, {"url": "https://arxiv.org/abs/1908.08962", "anchor_text": "Well-Read Students Learn Better"}, {"url": "https://blog.griddynamics.com/question-answering-system-using-bert/", "anchor_text": "How we built Question Answering system for an online store using BERTIn this blog post, we describe our experience of building Question Answering systems based on Transformer models such\u2026blog.griddynamics.com"}, {"url": "https://neptune.ai/blog/knowledge-distillation", "anchor_text": "Knowledge Distillation: Principles, Algorithms, Applications"}, {"url": "https://blog.rasa.com/compressing-bert-for-faster-prediction-2/", "anchor_text": "Overview of BERT compression techniques (Rasa)"}, {"url": "https://www.pragmatic.ml/a-survey-of-methods-for-model-compression-in-nlp", "anchor_text": "A Survey of Methods for Model Compression in NLP"}, {"url": "https://medium.com/huggingface/distilbert-8cf3380435b5", "anchor_text": "Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT"}, {"url": "https://arxiv.org/abs/1908.08962", "anchor_text": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"}, {"url": "http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture10-QA.pdf", "anchor_text": "Question Answering Architectures (CS224N Stanford lecture)"}, {"url": "https://arxiv.org/abs/2006.11880", "anchor_text": "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets"}, {"url": "https://arxiv.org/abs/1901.11373", "anchor_text": "Learning and Evaluating General Linguistic Intelligence (SQuAD, sample-efficiency, generalization)"}, {"url": "https://arxiv.org/abs/1910.06431", "anchor_text": "DeepLIFTing BERT\u2019s Attention in Question Answering"}, {"url": "https://arxiv.org/abs/1909.04925", "anchor_text": "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations"}, {"url": "https://arxiv.org/abs/2004.03490", "anchor_text": "What do Models Learn from Question Answering Datasets?"}, {"url": "https://arxiv.org/abs/2008.02637", "anchor_text": "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets"}, {"url": "https://arxiv.org/abs/1809.10735", "anchor_text": "A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC"}, {"url": "https://arxiv.org/abs/1806.03822", "anchor_text": "Percy Liang, Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://arxiv.org/abs/1906.02629", "anchor_text": "When Does Label Smoothing Help?"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4670085cc18---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4670085cc18---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4670085cc18---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/knowledge-distillation?source=post_page-----4670085cc18---------------knowledge_distillation-----------------", "anchor_text": "Knowledge Distillation"}, {"url": "https://medium.com/tag/question-answering?source=post_page-----4670085cc18---------------question_answering-----------------", "anchor_text": "Question Answering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4670085cc18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=-----4670085cc18---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4670085cc18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=-----4670085cc18---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4670085cc18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@sparakhin?source=post_page-----4670085cc18--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4670085cc18--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b6af71300d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=post_page-2b6af71300d6----4670085cc18---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F2b6af71300d6%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=-----4670085cc18---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@sparakhin?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Written by Sergey Parakhin"}, {"url": "https://medium.com/@sparakhin/followers?source=post_page-----4670085cc18--------------------------------", "anchor_text": "9 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://twitter.com/sparakhin", "anchor_text": "https://twitter.com/sparakhin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b6af71300d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=post_page-2b6af71300d6----4670085cc18---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F2b6af71300d6%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistilling-bert-using-unlabeled-qa-dataset-4670085cc18&user=Sergey+Parakhin&userId=2b6af71300d6&source=-----4670085cc18---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4670085cc18----0---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----4670085cc18----0---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----4670085cc18----0---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4670085cc18----0---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4670085cc18----0---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4670085cc18----0---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----0-----------------clap_footer----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4670085cc18----0---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----4670085cc18----0-----------------bookmark_preview----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4670085cc18----1---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----4670085cc18----1---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----4670085cc18----1---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4670085cc18----1---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4670085cc18----1---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4670085cc18----1---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----1-----------------clap_footer----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4670085cc18----1---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----4670085cc18----1-----------------bookmark_preview----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4670085cc18----2---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----4670085cc18----2---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----4670085cc18----2---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4670085cc18----2---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4670085cc18----2---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4670085cc18----2---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4670085cc18----2---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----4670085cc18----2-----------------bookmark_preview----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----4670085cc18----3---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----4670085cc18----3---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----4670085cc18----3---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4670085cc18----3---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----4670085cc18----3---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "Time-Series Forecasting: Deep Learning vs Statistics \u2014 Who Wins?A comprehensive guide on the ultimate dilemma"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----4670085cc18----3---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": "\u00b714 min read\u00b7Apr 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----c568389d02df----3-----------------clap_footer----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----4670085cc18----3---------------------b3824ba7_1997_4f32_8e03_44a8e855dfab-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&source=-----4670085cc18----3-----------------bookmark_preview----b3824ba7_1997_4f32_8e03_44a8e855dfab-------", "anchor_text": ""}, {"url": "https://medium.com/@sparakhin?source=post_page-----4670085cc18--------------------------------", "anchor_text": "See all from Sergey Parakhin"}, {"url": "https://towardsdatascience.com/?source=post_page-----4670085cc18--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----4670085cc18----0-----------------bookmark_preview----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----1-----------------clap_footer----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----4670085cc18----1-----------------bookmark_preview----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----0-----------------clap_footer----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4670085cc18----0---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----4670085cc18----0-----------------bookmark_preview----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4670085cc18----1---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----4670085cc18----1-----------------bookmark_preview----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----4670085cc18----2---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----4670085cc18----2---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----4670085cc18----2---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Arslan Mirza"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----4670085cc18----2---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----4670085cc18----2---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "How To Build Your Own Custom ChatGPT BotA step-by-step guide to building and fine-tuning custom ChatGPT models"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----4670085cc18----2---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "\u00b79 min read\u00b7Mar 29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&user=Arslan+Mirza&userId=35aaa5742af7&source=-----cf4af959adcc----2-----------------clap_footer----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----4670085cc18----2---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&source=-----4670085cc18----2-----------------bookmark_preview----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/interpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074?source=read_next_recirc-----4670085cc18----3---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=read_next_recirc-----4670085cc18----3---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=read_next_recirc-----4670085cc18----3---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Ruben Winastwan"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----4670085cc18----3---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/interpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074?source=read_next_recirc-----4670085cc18----3---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "Interpreting the Prediction of BERT Model for Text ClassificationHow to Use Integrated Gradients to Interpret BERT Model\u2019s Prediction"}, {"url": "https://towardsdatascience.com/interpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074?source=read_next_recirc-----4670085cc18----3---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": "\u00b713 min read\u00b7Dec 20, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ab09f8ef074&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----5ab09f8ef074----3-----------------clap_footer----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/interpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074?source=read_next_recirc-----4670085cc18----3---------------------ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ab09f8ef074&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074&source=-----4670085cc18----3-----------------bookmark_preview----ead52870_8903_4d2f_a9d9_3d8b69c7e00c-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----4670085cc18--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4670085cc18--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----4670085cc18--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}