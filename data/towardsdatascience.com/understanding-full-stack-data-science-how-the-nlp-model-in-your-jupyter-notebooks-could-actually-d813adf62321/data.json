{"url": "https://towardsdatascience.com/understanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321", "time": 1683016513.4229782, "path": "towardsdatascience.com/understanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321/", "webpage": {"metadata": {"title": "Understanding Full-Stack Data Science: How the NLP model in your Jupyter Notebooks Could Actually Help Stop Disasters, Part I | by Bowen Chen | Towards Data Science", "h1": "Understanding Full-Stack Data Science: How the NLP model in your Jupyter Notebooks Could Actually Help Stop Disasters, Part I", "description": "Emergencies could happen anytime, anywhere. As we navigate through this devastating global pandemic, we are now witnessing the absolute brilliance of all the frontline first responders. In fact\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/landlord/multilingual-disaster-response-messages", "anchor_text": "multilingual disaster response messages", "paragraph_index": 5}, {"url": "https://github.com/PrashantSaikia/Wordcloud-in-Plotly/blob/master/plotly_wordcloud.py", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://python-poetry.org/", "anchor_text": "Poetry", "paragraph_index": 31}, {"url": "https://github.com/dephell/dephell", "anchor_text": "depshell", "paragraph_index": 33}, {"url": "https://circleci.com/docs/", "anchor_text": "here", "paragraph_index": 40}, {"url": "https://towardsdatascience.com/understanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-460f17baf30b", "anchor_text": "Part 2", "paragraph_index": 50}, {"url": "https://towardsdatascience.com/understanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-460f17baf30b", "anchor_text": "part 2", "paragraph_index": 55}, {"url": "http://Archera.ai", "anchor_text": "Archera.ai", "paragraph_index": 57}], "all_paragraphs": ["Emergencies could happen anytime, anywhere. As we navigate through this devastating global pandemic, we are now witnessing the absolute brilliance of all the frontline first responders. In fact, sending rescue units are only part of their already laborious jobs. When a disaster hit, aid request messages would come fast and furiously, from all different channels. Human labor can be quickly overrun by all the messages requesting rescue, from all different channels.", "Therefore, an automated and reliable way of swiftly classifying the request for rescue message into the correct corresponding channel. A natural choice would be building a message classifier with some machine learning models. So we would normally be pulling up our jupyter notebooks and starting to develop right? In fact, that would only be part of the work done here. With Jupyter Notebooks, we can create engaging visuals of data and easy-to-read model representations, but these models could only be looked at without making any real impact. Therefore, being able to convert the models into a production-ready code base would crucial for a team to directly create capabilities. In this blog post, we will illustrate the full-stack data science practice by building a fully-containerized, continuously integrated disaster messaging application on Heroku, the PaaS platform. Since the dataset we use (the Figure Eight disaster messages) is rather well known in the data science community, we will put the majority of the focus on the advanced tools used to productionize the model, such as Sklearn Pipeline, Pytest, Docker, and CircleCI. To avoid the post getting too long to read, we will focus on discussing the model building and packaging processes of this", "Since a variety of different files will be involved, having a clear understanding of the project structure is crucial to the success of the project. In the root directory of the project, we will have a standard .gitignore file and a ReadMe.md file. 4 subdirectories are also present,", "The .circleci and notebooks folders\u2019 purposes are quite obvious. We will have a look at what is inside the disaster_messaging_classification_model and disaster_messaging_app when we are constructing those modules.", "We will use the multicategory disaster response messages dataset from Kaggle, which contains the message in English, the original message, the channel it was sent, and all of the 37 message categories encoded in binary. One message could belong to multiple categories. The first 5 rows are shown below.", "We can manually download the data and save the files into the data folder. In general practice, however, to ensure we always have the latest data from the data source, we will use shell scripts to automate the data extraction process. For our project, the data is from Kaggle\u2019s multilingual disaster response messages. The dataset has 30,000 messages extracted from multiple channels. The general process is the following,", "All of these steps could be completed in terminal shell commands,", "With this shell script, the whole above process could be executed in the following 1 line", "after the execution, you will be able to see the disaster_response_messages.csv file exists in the data folder of the disaster_messaging_classification_model directory. Now as we already have the data, we can proceed to building, training, and set up the prediction pipeline for this machine learning model.", "Building the model should be a relatively familiar process for most of us. We obtain the data, import relevant machine learning frameworks into our jupyter notebooks, train the model, and then evaluate the model. With the help of sklearn pipelines, we can build a model pipeline that is self-contained, stable, and reproducible. As this disaster_messaging_classification_model directory contains multiple relevant files, We will start by looking at the structure of this folder.", "To extract data into a model-usable format, we will follow the following steps to extract CSVs and save the transformed data into a SQLite database.", "4. define a function that calls the above 3 steps (defined with process_data())", "Since the raw data is relatively clean, the majority of the data transformation operations will be performed when we are creating tokenized sentence features (coming up next).", "To convert raw text to machine-readable word tokens, we will need to build a tokenizer that removes the stopwords (such as this, that, I, am\u2026), lemmatized the word to its original format, remove URLs and strip extra whitespace. The module is built as a transformer in the file named message_tokenizer.py. Since this tokenizer transformer is used as a feature creator, we will put it in the features directory.", "The pipeline of the model consists of only 4 steps,", "The above 4 steps could easily be built into a pipeline function like the following.", "As a next step, we can proceed to construct the training function, which involves the steps as the following", "The load_data_from_db function will have the option to choose whether the training data or test data will be loaded (defaulted to be \u201ctrain\u201d), the specific definition is shown below", "The save_pipeline() function will save the newly trained model to a specific location (trained_models directory) with a specific naming convention (model_name_version.pkl) as defined in config.py, at the same time it will remove the older versions of model pipelines to avoid conflicts in referencing. The specific definition is shown below", "The evaluate_model() function will use the built-in sklearn evaluation APIs and generate a performance report, which will be saved in trained_models/performance_report directory. The specific definition is shown below", "After the model is trained, saved, and evaluated, we are now ready to use the model to generate some predictions when new data comes in. The prediction scripts consist of only 3 steps, which could be implemented in a single function", "2. Load the pipeline and predict using the input data", "3. Assemble the payload that contains the predictions result and the model version", "The specific definition is shown below", "We will build the word cloud visualizations that would be used on the frontend. Since our frontend only uses javascript to display the word cloud visuals, we will rely on Plotly\u2019s special layout structure and the plotly_wordcloud function to generate 3 visuals of the most frequently used words in the 3 different channels our messages are from \u2014 news, social media, and direct messaging.", "The overall steps are the following,", "The VisualsGeneration class is defined as the following.", "The plotly_wordcloud function (recipe provided by Prashant here) follows the below general steps,", "The function is defined as the following,", "With our model pipeline, training, and predicting scripts built, we can use a series of shell commands in the terminal to execute the workflow related to this model. For instance, we can call the following 2 commands in the terminal to package the saved trained model and upload the model to the PyPI server.", "Executing model packaging would require the existence of the training data, dependencies packages all installed in the current execution environment and the model is already trained and saved. In other words, there will be a series of steps we will need to complete before we finally get to upload the trained model.", "We will start by creating the appropriate environment definition of our model package, which could be accomplished by building out the pyproject.toml file used by Poetry, the python dependency management tool. The pyproject. toml file looks like the following,", "We can then call poetry lock in the terminal, which will generate a poetry.lock file that contains all the locked versions of the package dependencies needed in this project. Locked versions will allow us to have the necessary consistency in the installations of package dependencies in case part of the dependent package received an update that would cause unexpected problems to our project.", "For every python package we upload to the PyPI server, we will need to create a setup.py file. Fortunately, we can use a tool named depshell to create the file for us. We just need to call the following commands to create the setup.py file.", "The created setup.py file looks like the following", "With these many shell commands to be executed, simply creating a file that contains all the steps will be less ideal for readability and transparency purposes. Fortunately, we can create a Makefile that effectively cluster the steps into groups that only contain commands that accomplish a single task. For example, to install and build the package dependencies definition, we can write the following series of commands into the Makefile,", "Then to execute this series of commands, we will only need to callmake poetry-install in the terminal, all of the above set of commands will be executed. The following outlines the steps which will be put into the Makefile", "The full Makefile looks like the following,", "This is good, as a lot of steps could be summarized into a short make statement. To further make the entire process automated, we will use a config file that instructs circleci to perform these steps one by one.", "Configuring the CircleCI continuous integration tools", "CicleCI is one of the powerful continuous integration tools that allow our model to be automatically trained, tested, and packaged to distribution-ready. It also uses its own hosted docker image that mimics the process of someone else who is going to use your project effectively, as it follows the same setup steps when the project is installed locally. We will only need to define a config.yaml file in the designated folder named .circleci. More resources and documentation about circleci can be found here.", "So, now the question becomes what will be the steps we need to follow to completely make the project work from scratch? We already have most of the steps in the Makefile, we only need to connect them.", "The circle config file will start with 3 lines of conventional header indicating the circleci\u2019s python docker image version, which in our case will be 0.2.1.", "Then on the next line, jobs will indicate the start of the automated workflow steps to be completed to accomplish the task defined in our project (the 9 steps we defined above). The config.yaml file will look like the following.", "After configuring the CI workflow with the yaml file, we will proceed to CircileCI\u2019s website and setup its connection between github repository.", "We will start by logging into CircleCI using the github account that contains our project repository.", "We will then navigate to the second icon on the left menu bar (Projects) and find our corresponding project repository. In my case, it\u2019s called Disaster_Response_Messaging_Classifier . We will then click the blue Set UP Project button.", "On the language dropdown menu, we will select \u201cPython\u201d, then copy our config.yaml file into the editor right below the dropdown menu. Then click \u201cAdd Config\u201d button.", "We will land on the Dashboard page. Since my project has been built multiple times, the status of the workflow completion will be shown as line items on the dashboard. To setup the environment variables needed to complete the project, we will click the\u201cProject Settings\u201d button on the top left corner.", "On the next page, navigate to the left panel and click on \u201cEnvironment Variables\u201d. We will need to add the following 4 environment variables for the workflow to be fully functional.", "The other 3 environment variables are used for deployments, which will be talked about in Part 2.", "We have now finished setting up the continuous integration tools for our project, now it\u2019s time to see it work in action.", "As the title suggests. We will commit and push to the repo and watch the workflow go on its own. If everything is done correctly, we will see the following screen indicating the workflow is successfully completed.", "We can then login into our PyPI server and verify if the package is correctly pushed into the host. It is indeed in our projects list, yay!", "We have now successfully completed building the machine learning model that will help us classify messages into their correct channel. More importantly, we have fully automated the model building, training and packaging process for this model. Every time, when we want to change the model definition, or even want to retrain the model when there\u2019s new data coming in, all we have to do is as simple as a commit and push.", "Thank you for staying until the end. This is only part of the story, as we will talk about some even more exciting deployment procedures that will make this project truly end-to-end in part 2.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer@ Archera.ai, Basketball Player Training How to Dunk, Life-long Knicks Fan, Living the Dream"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd813adf62321&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d813adf62321--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d813adf62321--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://bowenchen.medium.com/?source=post_page-----d813adf62321--------------------------------", "anchor_text": ""}, {"url": "https://bowenchen.medium.com/?source=post_page-----d813adf62321--------------------------------", "anchor_text": "Bowen Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4eab6908bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&user=Bowen+Chen&userId=e4eab6908bf4&source=post_page-e4eab6908bf4----d813adf62321---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd813adf62321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd813adf62321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@zhenhu2424?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Zhen Hu"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/landlord/multilingual-disaster-response-messages", "anchor_text": "multilingual disaster response messages"}, {"url": "https://python-poetry.org/", "anchor_text": "here"}, {"url": "https://github.com/PrashantSaikia/Wordcloud-in-Plotly/blob/master/plotly_wordcloud.py", "anchor_text": "here"}, {"url": "https://python-poetry.org/", "anchor_text": "Poetry"}, {"url": "https://github.com/dephell/dephell", "anchor_text": "depshell"}, {"url": "https://circleci.com/docs/", "anchor_text": "here"}, {"url": "https://github.com/Kaggle/kaggle-api", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/understanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-460f17baf30b", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/understanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-460f17baf30b", "anchor_text": "part 2"}, {"url": "https://medium.com/tag/machine-leanring?source=post_page-----d813adf62321---------------machine_leanring-----------------", "anchor_text": "Machine Leanring"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d813adf62321---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/full-stack?source=post_page-----d813adf62321---------------full_stack-----------------", "anchor_text": "Full Stack"}, {"url": "https://medium.com/tag/model-automation?source=post_page-----d813adf62321---------------model_automation-----------------", "anchor_text": "Model Automation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd813adf62321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&user=Bowen+Chen&userId=e4eab6908bf4&source=-----d813adf62321---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd813adf62321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&user=Bowen+Chen&userId=e4eab6908bf4&source=-----d813adf62321---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd813adf62321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d813adf62321--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd813adf62321&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d813adf62321---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d813adf62321--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d813adf62321--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d813adf62321--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d813adf62321--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d813adf62321--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d813adf62321--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d813adf62321--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d813adf62321--------------------------------", "anchor_text": ""}, {"url": "https://bowenchen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://bowenchen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bowen Chen"}, {"url": "https://bowenchen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "141 Followers"}, {"url": "http://Archera.ai", "anchor_text": "Archera.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4eab6908bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&user=Bowen+Chen&userId=e4eab6908bf4&source=post_page-e4eab6908bf4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7b1d5930fa0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-full-stack-data-science-how-the-nlp-model-in-your-jupyter-notebooks-could-actually-d813adf62321&newsletterV3=e4eab6908bf4&newsletterV3Id=7b1d5930fa0e&user=Bowen+Chen&userId=e4eab6908bf4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}