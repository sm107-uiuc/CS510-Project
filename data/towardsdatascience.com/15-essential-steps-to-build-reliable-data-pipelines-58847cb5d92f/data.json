{"url": "https://towardsdatascience.com/15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f", "time": 1683017139.693045, "path": "towardsdatascience.com/15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f/", "webpage": {"metadata": {"title": "15 Essential Steps To Build Reliable Data Pipelines | by Anna Geller | Towards Data Science", "h1": "15 Essential Steps To Build Reliable Data Pipelines", "description": "If I learned anything from working as a data engineer, it is that practically any data pipeline fails at some point. Broken connection, broken dependencies, data arriving too late, or some external\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/manage-files-and-database-connections-in-python-like-a-pro-73e8fc0b7967", "anchor_text": "closing database connections", "paragraph_index": 6}, {"url": "https://docs.prefect.io/core/concepts/results.html#choose-a-result-type", "anchor_text": "several abstractions", "paragraph_index": 13}, {"url": "https://softwareengineeringdaily.com/2020/04/29/prefect-dataflow-scheduler-with-jeremiah-lowin/", "anchor_text": "Many companies", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Change_data_capture", "anchor_text": "Change Data Capture", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Slowly_changing_dimension", "anchor_text": "Slowly Changing Dimension", "paragraph_index": 36}, {"url": "https://airflow.apache.org/", "anchor_text": "Airflow", "paragraph_index": 38}, {"url": "https://towardsdatascience.com/the-rise-of-data-downtime-841650cedfd5", "anchor_text": "data downtime", "paragraph_index": 39}, {"url": "http://www.annageller.com", "anchor_text": "www.annageller.com", "paragraph_index": 42}, {"url": "https://annageller.medium.com/subscribe", "anchor_text": "https://annageller.medium.com/subscribe", "paragraph_index": 42}], "all_paragraphs": ["If I learned anything from working as a data engineer, it is that practically any data pipeline fails at some point. Broken connection, broken dependencies, data arriving too late, or some external systems or APIs unreachable\u2026 There are many reasons why the failure may occur, but regardless of the cause, we can do a lot to mitigate the impact of a data pipeline\u2019s failure.", "Anytime we integrate a new data source, we usually need to backload the entire history into our data store. Then, for the day-to-day business, we would create a pipeline ingesting only new data, and we would potentially discard the code for the initial ingestion.", "Let\u2019s imagine that, for some reason, our data has been compromised. For instance, you found out that there are gaps in some source data at various intervals (could have happened due to an outage or API unavailability). To regain trust in your data, you may want to backload the entire history again. Therefore, it\u2019s handy to have both the regular and initial ingestion pipeline, which can be used to fix unexpected data incidents.", "One approach that can mitigate the problem discussed before is to make your data pipeline flexible enough to take input parameters such as a start date from which you want to extract, transform, and load your data. This approach even allows you to have a single data pipeline used for both initial and regular ingestion.", "Imagine that you come to work on Monday and you notice that one pipeline failed already on Saturday morning\u200a\u2014\u200anow you can easily backfill your data for the entire weekend without having to write any new code.", "I don\u2019t have any current statistics at hand, but likely 60% of all IT problems can be solved by retrying:", "If you plan for potential failure, you design your data pipelines in such a way that they can be retried at any time without unintended consequences. This means that you clean up resources by removing temporary folders, closing database connections, or truncating temporary tables and making sure that you don\u2019t end up with duplicates due to inserting the same data several times.", "One of the main reasons why many data pipelines are hard to maintain is that an entire workflow is often treated as a single giant script.", "What\u2019s the problem in treating workflows as scripts?", "A better solution is to treat your data pipelines as workflows: a collection of small self-contained, and retriable tasks that must be executed in a specific order and run at a specific time to ensure that dependencies are met, and that data remains available and reliable.", "To make use of atomic components, you need to have a way of storing temporary results so that you can retry your workflow from any task that has recently failed. You can\u2019t retry the transform task if you didn\u2019t cache the result of the extract task. Single stages can only be retried if the result of an upstream task is stored somewhere.", "Often retry is not enough: your data pipeline might have failed because there were some missing values or wrong data types. Or maybe an API didn\u2019t return one field which you need for your table. In all those cases, you can fix the problem much more efficiently if you cached the intermediate result and can inspect the data to find out what went wrong.", "There are many ways of implementing result caching in your workflows, such as building a reusable logic that stores intermediate data in Redis, S3, or in some temporary staging area tables. As long as you build your data pipelines in Python, you can easily pickle any Python object and dump it to a shared drive or S3.", "If you use workflow tools such as Prefect, there are several abstractions that you can use to cache your task results in your preferred format (pickle, JSON, or other forms of serialization) to a storage system of your choice.", "It\u2019s usually better to log too much than too little. It\u2019s easier to identify where something went wrong if you give yourself and others enough information about the task at hand in your data pipeline.", "For instance, it\u2019s useful to log the shape of your dataframe (i.e., the number of rows and columns), its data types, and information about the column names, which can help identify the issue much quicker.", "I often encounter business requirements to quickly integrate some data and move on to the next task due to deadlines and task overload before doing proper QA of the data and the pipeline logic. Moving fast is not a good strategy for building data pipelines.", "Once bad data gets into the system and gets used by other downstream pipelines and other child pipelines of those pipelines down the road (let alone reports!), it becomes increasingly hard to correct the mistake.", "Even small mistakes can make your stakeholders lose trust in your entire data ecosystem.", "Moral of the story: don\u2019t let the deadlines and overload of tasks pressure you to release not thoroughly tested data pipelines. Otherwise, you may pay for it later in the form of bad data quality.", "Simplicity is our best friend in software and data engineering. If there are tools out there that can help us maintain our data ecosystem, it\u2019s better to use them rather than hacking our own solutions.", "Many companies (who are NOT software companies!) considered workflow management so simple that they decided to develop their own workflow orchestration systems: starting by just building a system to make scripts run on a schedule on the platform of their choice (ex. Kubernetes or Fargate). Then they noticed, they need a database to store the schedules and the information about dependencies. And then, the only thing that is left is adding a retry logic. And a timeout logic to avoid zombie tasks. And a scale-out logic to avoid that a single execution environment explodes from too many tasks assigned to the same worker node. And, obviously, we need to ensure that our database scales, as well. Then just adding a mechanism to pass environment variables in a secure way. Then also implementing centralized logging and monitoring. And then, to display it all in a nice UI, of course, with an Auth layer and a custom domain. And then\u2026 Then, you end up maintaining the platform for data engineering rather than maintaining your data pipelines.", "And the worst part is that when this person who built and maintained the homegrown workflow orchestration system (who likely didn\u2019t have enough time to document the whole process due to the enormous number of components he or she already had to maintain) leaves the company, you are left alone.", "Regardless of how well-engineered our system is, day-to-day operations can sometimes force us to act quickly to react to business needs. For instance, you may have an entire data pipeline that relies on a file being uploaded to SFTP by a business user. Imagine that this user had a bad day and forgot to do it, so this file that your data pipeline depends on doesn\u2019t exist. What do you do?", "With a well-designed data pipeline, we may skip such a task and notify responsible people about the incident. Of course, the whole idea of having an important data pipeline reliant on one person manually uploading some file demonstrates a business process that is very fragile in the first place. Still, sometimes you must deal with what you get \u2014 you can\u2019t change business processes overnight.", "What we can do as data engineers is to have a system that makes it easy to handle such fragile use cases without impacting other data and processes.", "A good data engineering platform allows us to skip, remove, or unschedule a task or a pipeline when needed.", "From my experience, it pays off to make data pipelines as small and dependency-free as possible. This is easier said than done when there are tens of downstream projects that rely on data from a very early stage in a pipeline.", "My favorite solution to this problem is to build pipelines in a parent-child relationship. In this setup, parent data pipelines ensure that the small independent child pipelines are executed in the correct order. This workflow design prevents from ending up in a single data pipeline that includes hundreds of completely unrelated tasks only to make sure that they are triggered in the right order. At the same time, this workflow design helps with the separation of concerns: only because marketing data usually arrives before sales data doesn\u2019t mean that they belong into the same pipeline, neither should it prevent you from being able to trigger each of them separately when needed.", "I discussed this concept in more detail in the following article:", "The data about the execution of your data pipelines is valuable, don\u2019t throw it away. Over time, it allows you to see which tasks got added later, which got removed and how the entire workflow evolved. To collect metadata about your data pipelines, you must have some way of versioning your workflows. If you don\u2019t have a workflow management platform that allows versioning, you can keep track of it via execution logs and by storing workflow code in a version control system.", "There are many ways of how you can get notified when your data pipeline fails. Many use Pagerduty; others want to get notified via Slack, Teams, or emails. Regardless of what you choose, it\u2019s crucial to build a system in which there are people who consistently check those error notifications and fix them. In my previous job, there was a designated \u201con-call\u201d person every week that was responsible for monitoring and fixing ad-hoc data issues.", "It\u2019s crucial to have a system that prevents situations where nobody feels responsible for fixing errors, or in contrast, that two people end up fixing the same problem while another issue remains untackled. Communication is so critical. The worst-case scenario for a data engineering team is a monitoring system where there are so many error messages flowing into your notification channel that people stop looking at it or mute it forever.", "A common antipattern is when somebody wants to make the code run successfully regardless of how your data looks like and simply logs the exception.", "The problem with this approach is that this code will always be successful, even when, in the end, you get no data due to an error that has been silenced with this exception. If something is wrong in your data pipeline, let it fail and notify your team about the issue.", "This point may seem obvious to some experienced data warehouse developers, but surprisingly not everybody is storing the history of data changes. There are plenty of ways of historizing your data, such as:", "With a historizarion logic, you can identify and correct potential data quality problems much faster, as you can find out what changed when. If you want to learn more about this topic, look for resources related to Change Data Capture and Slowly Changing Dimension.", "Nearly any IT manager strives for standardization. By standardizing the processes, we can enforce rules to make single components almost identical, making maintenance and usage of a system easier. Some say that premature optimization is the root of all evil. I would say that often premature standardization is the root of all evil.", "You can\u2019t force data engineers to write all data pipelines using only one specific tool or method, as long as it is not general enough to reflect all business use cases. For instance, it\u2019s hard to encapsulate all data pipelines in a drag-and-drop ETL tool because it doesn\u2019t work when complex business logic can only be captured in custom code. Similarly, it would be best if you didn\u2019t use, for example, only a BashOperator in Airflow for all tasks because this way, you are not using this tool as a workflow management platform but rather as a job scheduler, and you are back to unmaintainable giant scripts rather than reliable data pipelines as code with atomic self-contained tasks.", "In this article, we looked at essential principles for building reliable data pipelines. I tried to make this article general enough to reflect the main problems when designing workflows rather than focusing on a single platform. Current data ecosystems are oftentimes very fragmented and prone to data downtime, but with a smart approach, we can mitigate the impact of a data pipeline\u2019s failure.", "Here are some posts about related topics:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "DevRel, Data Professional, Cloud & .py fan. www.annageller.com. Get my articles via email: https://annageller.medium.com/subscribe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F58847cb5d92f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://annageller.medium.com/?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": ""}, {"url": "https://annageller.medium.com/?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "Anna Geller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bdf5a3385cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&user=Anna+Geller&userId=5bdf5a3385cd&source=post_page-5bdf5a3385cd----58847cb5d92f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F58847cb5d92f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F58847cb5d92f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@muffin?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Kenneth Carpina"}, {"url": "https://www.pexels.com/photo/close-up-photo-of-street-lamps-1595437/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://media.makeameme.org/created/Hello-IT-Have.jpg", "anchor_text": "The IT Crowd"}, {"url": "https://towardsdatascience.com/manage-files-and-database-connections-in-python-like-a-pro-73e8fc0b7967", "anchor_text": "closing database connections"}, {"url": "https://docs.prefect.io/core/concepts/results.html#choose-a-result-type", "anchor_text": "several abstractions"}, {"url": "https://www.pexels.com/@goumbik?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Lukas"}, {"url": "https://www.pexels.com/photo/chart-close-up-data-desk-590022/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://softwareengineeringdaily.com/2020/04/29/prefect-dataflow-scheduler-with-jeremiah-lowin/", "anchor_text": "Many companies"}, {"url": "https://unsplash.com/@charlfolscher?utm_source=medium&utm_medium=referral", "anchor_text": "Charl Folscher"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/managing-dependencies-between-data-pipelines-in-apache-airflow-prefect-f4eba65886df", "anchor_text": "Managing dependencies between data pipelines in Apache Airflow & PrefectMany workflow schedulers let us manage dependencies within a single data pipeline. But what if you have dependencies\u2026towardsdatascience.com"}, {"url": "https://www.google.com/url?sa=i&url=https%3A%2F%2Fgfycat.com%2Fdiscover%2Fgroup-chat-messages-gifs&psig=AOvVaw1eD8tIySosCbF0tPvq2f0f&ust=1605820235807000&source=images&cd=vfe&ved=2ahUKEwi4tIzXgI3tAhWiQRUIHRK5DKAQjRx6BAgAEAc", "anchor_text": "gfycat.com"}, {"url": "https://en.wikipedia.org/wiki/Slowly_changing_dimension", "anchor_text": "Slowly Changing Dimension"}, {"url": "https://youtu.be/4Spo2QRTz1k?t=1263", "anchor_text": "periodic snapshots"}, {"url": "https://en.wikipedia.org/wiki/Change_data_capture", "anchor_text": "Change Data Capture"}, {"url": "https://en.wikipedia.org/wiki/Slowly_changing_dimension", "anchor_text": "Slowly Changing Dimension"}, {"url": "https://airflow.apache.org/", "anchor_text": "Airflow"}, {"url": "https://towardsdatascience.com/the-rise-of-data-downtime-841650cedfd5", "anchor_text": "data downtime"}, {"url": "https://medium.com/the-prefect-blog/your-code-will-fail-but-thats-ok-f0327a208dbe", "anchor_text": "Your Code Will Fail (but that\u2019s ok)A primer on negative engineeringmedium.com"}, {"url": "https://towardsdatascience.com/10-data-engineering-practices-to-ensure-data-and-code-quality-6224d012d8fb", "anchor_text": "10 Data Engineering Practices to Ensure Data and Code QualityWhat I learned from working with data at various companiestowardsdatascience.com"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----58847cb5d92f---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----58847cb5d92f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/software-development?source=post_page-----58847cb5d92f---------------software_development-----------------", "anchor_text": "Software Development"}, {"url": "https://medium.com/tag/python?source=post_page-----58847cb5d92f---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data?source=post_page-----58847cb5d92f---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F58847cb5d92f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&user=Anna+Geller&userId=5bdf5a3385cd&source=-----58847cb5d92f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F58847cb5d92f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&user=Anna+Geller&userId=5bdf5a3385cd&source=-----58847cb5d92f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F58847cb5d92f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F58847cb5d92f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----58847cb5d92f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----58847cb5d92f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----58847cb5d92f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----58847cb5d92f--------------------------------", "anchor_text": ""}, {"url": "https://annageller.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://annageller.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anna Geller"}, {"url": "https://annageller.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.4K Followers"}, {"url": "http://www.annageller.com", "anchor_text": "www.annageller.com"}, {"url": "https://annageller.medium.com/subscribe", "anchor_text": "https://annageller.medium.com/subscribe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bdf5a3385cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&user=Anna+Geller&userId=5bdf5a3385cd&source=post_page-5bdf5a3385cd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F908ff40c4340&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F15-essential-steps-to-build-reliable-data-pipelines-58847cb5d92f&newsletterV3=5bdf5a3385cd&newsletterV3Id=908ff40c4340&user=Anna+Geller&userId=5bdf5a3385cd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}