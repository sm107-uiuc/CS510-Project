{"url": "https://towardsdatascience.com/how-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed", "time": 1682996118.258262, "path": "towardsdatascience.com/how-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed/", "webpage": {"metadata": {"title": "How to Generate Prediction Intervals with Scikit-Learn and Python | by Will Koehrsen | Towards Data Science", "h1": "How to Generate Prediction Intervals with Scikit-Learn and Python", "description": "\u201cAll models are wrong but some are useful\u201d \u2014 George Box. It\u2019s critical to keep this sage advice in mind when we present machine learning predictions. With all machine learning pipelines, there are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/All_models_are_wrong", "anchor_text": "George Box", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Latent_variable", "anchor_text": "(latent variables)", "paragraph_index": 0}, {"url": "https://github.com/WillKoehrsen/Data-Analysis/tree/master/prediction-intervals", "anchor_text": "on GitHub", "paragraph_index": 2}, {"url": "https://nbviewer.jupyter.org/github/WillKoehrsen/Data-Analysis/blob/master/prediction-intervals/prediction_intervals.ipynb", "anchor_text": "Jupyter Notebook on nbviewer.", "paragraph_index": 2}, {"url": "https://drivendata.org", "anchor_text": "DrivenData", "paragraph_index": 3}, {"url": "https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://github.com/WillKoehrsen/Data-Analysis/tree/master/prediction-intervals/data", "anchor_text": "cleaned-up version in GitHub", "paragraph_index": 3}, {"url": "http://get.cortexintel.com", "anchor_text": "Cortex Building Intel!)", "paragraph_index": 4}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor", "anchor_text": "Gradient Boosting Regressor", "paragraph_index": 5}, {"url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html", "anchor_text": "this example", "paragraph_index": 5}, {"url": "https://plot.ly/python/", "anchor_text": "of plotly", "paragraph_index": 10}, {"url": "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf", "anchor_text": "Friedman\u2019s 1999 paper \u201cGreedy Function Approximation: A Gradient Boosting Machine\u201d", "paragraph_index": 18}, {"url": "https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "anchor_text": "quantile loss", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3", "anchor_text": "this article", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Quantile_regression", "anchor_text": "Wikipedia page", "paragraph_index": 26}, {"url": "https://stats.stackexchange.com/questions/85560/shape-of-confidence-interval-for-predicted-values-in-linear-regression", "anchor_text": "confidence intervals from linear regression", "paragraph_index": 27}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "on twitter", "paragraph_index": 29}, {"url": "http://get.cortexintel.com", "anchor_text": "Cortex.", "paragraph_index": 29}], "all_paragraphs": ["\u201cAll models are wrong but some are useful\u201d \u2014 George Box. It\u2019s critical to keep this sage advice in mind when we present machine learning predictions. With all machine learning pipelines, there are limitations: features which affect the target that are not in the data (latent variables), or assumptions made by the model which don\u2019t align with reality. These are overlooked when we show a single exact number for a prediction \u2014 the house will be $450,300.01 \u2014which gives the impression we are entirely confident our model is a source of truth.", "A more honest way to show predictions from a model is as a range of estimates: there might be a most likely value, but there is also a wide interval where the real value could be. This isn\u2019t a topic typically addressed in data science courses, but it\u2019s crucial that we show uncertainty in predictions and don\u2019t oversell the capabilities of machine learning. While people crave certainty, I think it\u2019s better to show a wide prediction interval that does contain the true value than an exact estimate which is far from reality.", "In this article, we\u2019ll walk through one method of producing uncertainty intervals in Scikit-Learn. The full code is available on GitHub with an interactive version of the Jupyter Notebook on nbviewer. We\u2019ll focus primarily on implementation, with a brief section and resources for understanding the theory at the end. Generating prediction intervals is another tool in the data science toolbox, one critical for earning the trust of non-data-scientists.", "For this walk-through, we\u2019ll use real-world building energy data from a machine learning competition which was hosted on DrivenData. You can get the raw data here, but I\u2019ve provided a cleaned-up version in GitHub which has energy and eight features measured at 15-minute intervals.", "The objective is to predict the energy consumption from the features. (This is an actual task we do every day at Cortex Building Intel!). There are undoubtedly hidden features (latent variables) not captured in our data that affect energy consumption, and therefore, we want to show the uncertainty in our estimates by predicting both an upper and lower bound for energy use.", "To generate prediction intervals in Scikit-Learn, we\u2019ll use the Gradient Boosting Regressor, working from this example in the docs. The basic idea is straightforward:", "At a high level, the loss is the function optimized by the model. When we change the loss to quantile and choose alpha (the quantile), we\u2019re able to get predictions corresponding to percentiles. If we use lower and upper quantiles, we can produce an estimated range. (We won\u2019t get into the details on the quantile loss right here \u2014 see the background on Quantile Loss below.)", "After splitting the data into train and test sets, we build the model. We actually have to use 3 separate Gradient Boosting Regressors because each model is optimizing a different function and must be trained separately.", "Training and predicting uses the familiar Scikit-Learn syntax:", "Just like that, we have prediction intervals!", "With a little bit of plotly, we can generate a nice interactive plot.", "As with any machine learning model, we want to quantify the error for our predictions on the test set (where we have the actual answers). Measuring the error of a prediction interval is a little bit trickier than a point prediction. We can calculate the percentage of the time the actual value is within the range, but this can be easily optimized by making the interval very wide. Therefore, we also want a metric that takes into account how far away the predictions are from the actual value, such as absolute error.", "In the notebook, I\u2019ve provided a function that calculates the absolute error for the lower, mid, and upper predictions and then averages the upper and lower error for an \u201cInterval\u201d absolute error. We can do this for each data point and then plot a boxplot of the errors (the percent in bounds is in the title):", "Interestingly, for this model, the median absolute error for the lower prediction is actually less than for the mid prediction. This model doesn\u2019t have superb accuracy and could probably benefit from optimization (adjusting model hyperparameters). The actual value is between the lower and upper bounds just over half the time, a metric we could increase by lowering the lower quantile and raising the upper quantile at a loss in precision.", "There are probably better metrics, but I selected these because they are simple to calculate and easy to interpret. The actual metrics you use should depend on the problem you\u2019re trying to solve and your objectives.", "Fitting and predicting with 3 separate models is somewhat tedious, so we can write a model that wraps the Gradient Boosting Regressors into a single class. It\u2019s derived from a Scikit-Learn model, so we use the same syntax for training / prediction, except now it\u2019s in one call:", "The model also comes with some plotting utilities:", "Please use and adapt the model as you see fit! This is only one method of making uncertainty predictions, but I think it\u2019s useful because it uses the Scikit-Learn syntax (meaning a shallow learning curve) and we can expand on it as needed. In general, this is a good approach to data science problems: start with the simple solution and add complexity only as required!", "The Gradient Boosting Regressor is an ensemble model, composed of individual decision/regression trees. (For the original explanation of the model, see Friedman\u2019s 1999 paper \u201cGreedy Function Approximation: A Gradient Boosting Machine\u201d.) In contrast to a random forest, which trains trees in parallel, a gradient boosting machine trains trees sequentially, with each tree learning from the mistakes (residuals) of the current ensemble. The contribution of a tree to the model is determined by minimizing the loss function of the model\u2019s predictions and the actual targets in the training set.", "With the default loss function \u2014 least squares \u2014 the gradient boosting regressor is predicting the mean. The critical point to understand is that the least squares loss penalizes low and high errors equally:", "In contrast, the quantile loss penalizes errors based on the quantile and whether the error was positive (actual > predicted) or negative (actual < predicted). This allows the gradient boosting model to optimize not for the mean, but for percentiles. The quantile loss is:", "Where \u03b1 is the quantile. Let\u2019s walk through a quick example using an actual value of 10 and our quantiles of 0.1 and 0.9:", "For a quantile < 0.5, if the prediction is greater than the actual value (case 1), the loss is greater than for a prediction an equal distance above the actual value. For a quantile > 0.5, if the prediction is less than the actual value (case 4), the loss is greater than for a prediction an equal distance below the actual value. With a quantile == 0.5, then predictions above and below the actual value result in an equal error and the model optimizes for the median.", "(For the mid model, we can use either loss=\"quantile\", alpha=0.5 for the median, or loss=\"ls\" for the mean).", "The quantile loss is best illustrated in a graph showing loss versus error:", "Quantiles < 0.5 drive the predictions below the median and quantiles > 0.5 drive the predictions above the median. This is a great reminder that the loss function of a machine learning method dictates what you are optimizing for!", "Depending on the output we want, we can optimize for the mean (least squares), median (quantile loss with alpha == 0.5) , or any percentile (quantile loss with alpha == percentile / 100). This is a relatively simple explanation of the quantile loss, but it\u2019s more than enough to get you started generating prediction intervals with the model walkthrough. To go further, check out this article or start on the Wikipedia page and look into the sources.", "Predicting a single number from a machine learning model gives the illusion we have a high level of confidence in the entire modeling process. However, when we remember that any model is only an approximation, we see the need for expressing uncertainty when making estimates. One way to do this is by generating prediction intervals with the Gradient Boosting Regressor in Scikit-Learn. This is only one way to predict ranges (see confidence intervals from linear regression for example), but it\u2019s relatively simple and can be tuned as needed. In this article, we saw a complete implementation and picked up some of the theory behind the quantile loss function.", "Solving data science problems is about having many tools in your toolbox to apply as needed. Generating prediction intervals is a helpful technique, and I encourage you to take this walkthrough and apply it to your problems. (The best way to learn any technique is through practice!) We know machine learning can do some pretty incredible things, but it\u2019s not perfect and we shouldn\u2019t portray it as such. To gain the trust of decision-makers, we often need to present not a single number as our estimate, but rather a prediction range indicating the uncertainty inherent in all models.", "I write about Data Science and occasionally other interesting topics. You can follow me on twitter for useful techniques and tools. If saving the world while helping the bottom line appeals to you, then get in touch with us at Cortex.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fab3899f992ed&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----ab3899f992ed---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab3899f992ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab3899f992ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/photo/adventure-clouds-daylight-fog-551876/", "anchor_text": "(Source)"}, {"url": "https://en.wikipedia.org/wiki/All_models_are_wrong", "anchor_text": "George Box"}, {"url": "https://en.wikipedia.org/wiki/Latent_variable", "anchor_text": "(latent variables)"}, {"url": "https://github.com/WillKoehrsen/Data-Analysis/tree/master/prediction-intervals", "anchor_text": "on GitHub"}, {"url": "https://nbviewer.jupyter.org/github/WillKoehrsen/Data-Analysis/blob/master/prediction-intervals/prediction_intervals.ipynb", "anchor_text": "Jupyter Notebook on nbviewer."}, {"url": "https://drivendata.org", "anchor_text": "DrivenData"}, {"url": "https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/", "anchor_text": "here"}, {"url": "https://github.com/WillKoehrsen/Data-Analysis/tree/master/prediction-intervals/data", "anchor_text": "cleaned-up version in GitHub"}, {"url": "http://get.cortexintel.com", "anchor_text": "Cortex Building Intel!)"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor", "anchor_text": "Gradient Boosting Regressor"}, {"url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html", "anchor_text": "this example"}, {"url": "https://en.wikipedia.org/wiki/Least_squares", "anchor_text": "least squares) which predicts the mean"}, {"url": "https://plot.ly/python/", "anchor_text": "of plotly"}, {"url": "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf", "anchor_text": "Friedman\u2019s 1999 paper \u201cGreedy Function Approximation: A Gradient Boosting Machine\u201d"}, {"url": "https://www.google.com/url?sa=i&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwji_obJmIziAhVhT98KHUEDAbYQjhx6BAgBEAM&url=https%3A%2F%2Fmedium.com%2Fmlreview%2Fgradient-boosting-from-scratch-1e317ae4587d&psig=AOvVaw1gY1KBEhTRQtlp8NI7oSOa&ust=1557413755162687", "anchor_text": "(source)"}, {"url": "https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "anchor_text": "quantile loss"}, {"url": "https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3", "anchor_text": "this article"}, {"url": "https://en.wikipedia.org/wiki/Quantile_regression", "anchor_text": "Wikipedia page"}, {"url": "https://stats.stackexchange.com/questions/85560/shape-of-confidence-interval-for-predicted-values-in-linear-regression", "anchor_text": "confidence intervals from linear regression"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "on twitter"}, {"url": "http://get.cortexintel.com", "anchor_text": "Cortex."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ab3899f992ed---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ab3899f992ed---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ab3899f992ed---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/education?source=post_page-----ab3899f992ed---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/statistics?source=post_page-----ab3899f992ed---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab3899f992ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----ab3899f992ed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab3899f992ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----ab3899f992ed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab3899f992ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fab3899f992ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ab3899f992ed---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ab3899f992ed--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ab3899f992ed--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ab3899f992ed--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}