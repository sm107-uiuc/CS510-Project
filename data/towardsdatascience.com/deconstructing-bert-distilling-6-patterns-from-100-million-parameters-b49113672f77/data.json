{"url": "https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77", "time": 1682994274.906365, "path": "towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77/", "webpage": {"metadata": {"title": "Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters | by Jesse Vig | Towards Data Science", "h1": "Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters", "description": "The year 2018 marked a turning point for the field of Natural Language Processing, with a series of deep-learning models achieving state-of-the-art results on NLP tasks ranging from question\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/jessevig/bertviz", "anchor_text": "BertViz", "paragraph_index": 1}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "transformer", "paragraph_index": 2}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e", "anchor_text": "masked language modeling", "paragraph_index": 2}, {"url": "https://jalammar.github.io/illustrated-bert/", "anchor_text": "The Illustrated Bert", "paragraph_index": 2}, {"url": "https://www.quora.com/What-are-the-best-visualizations-of-machine-learning-algorithms", "anchor_text": "visualization", "paragraph_index": 4}, {"url": "https://www.facebook.com/nipsfoundation/videos/203530960558001/", "anchor_text": "tools", "paragraph_index": 4}, {"url": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization", "anchor_text": "Tensor2Tenso", "paragraph_index": 4}, {"url": "https://github.com/huggingface/pytorch-pretrained-BERT", "anchor_text": "PyTorch implementation", "paragraph_index": 4}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "Github", "paragraph_index": 4}, {"url": "https://github.com/google-research/bert#pre-trained-models", "anchor_text": "BERT-Base, uncased version", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1906.04341", "anchor_text": "this paper", "paragraph_index": 19}, {"url": "https://www.facebook.com/nipsfoundation/videos/203530960558001/", "anchor_text": "It has been said", "paragraph_index": 20}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "Github", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1", "anchor_text": "Part 2", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8", "anchor_text": "most recent article", "paragraph_index": 23}, {"url": "https://jessevig.com", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://twitter.com/jesse_vig", "anchor_text": "jesse_vig", "paragraph_index": 24}, {"url": "https://medium.com/@llionj", "anchor_text": "Llion Jones", "paragraph_index": 25}, {"url": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization", "anchor_text": "visualization tool", "paragraph_index": 25}], "all_paragraphs": ["The year 2018 marked a turning point for the field of Natural Language Processing, with a series of deep-learning models achieving state-of-the-art results on NLP tasks ranging from question answering to sentiment classification. Most recently, Google\u2019s BERT algorithm has emerged as a sort of \u201cone model to rule them all,\u201d based on its superior performance over a wide variety of tasks.", "\ud83d\udd79 Try out an interactive demo with BertViz.", "BERT builds on two key ideas that have been responsible for many of the recent advances in NLP: (1) the transformer architecture and (2) unsupervised pre-training. The transformer is a sequence model that forgoes the recurrent structure of RNN\u2019s for a fully attention-based approach, as described in the instant classic Attention Is All You Need. BERT is also pre-trained; its weights are learned in advance through two unsupervised tasks: masked language modeling (predicting a missing word given the left and right context) and next sentence prediction (predicting whether one sentence follows another). Thus BERT doesn\u2019t need to be trained from scratch for each new task; rather, its weights are fine-tuned. For more details about BERT, check out the The Illustrated Bert.", "Bert is not like traditional attention models that use a flat attention structure over the hidden states of an RNN. Instead, BERT uses multiple layers of attention (12 or 24 depending on the model), and also incorporates multiple attention \u201cheads\u201d in every layer (12 or 16). Since model weights are not shared between layers, a single BERT model effectively has up to 24 x 16 = 384 different attention mechanisms.", "Because of BERT\u2019s complexity, it can be difficult to intuit the meaning of its learned weights. Deep-learning models in general are notoriously opaque, and various visualization tools have been developed to help make sense of them. However, I hadn\u2019t found one that could shed light on the attention patterns that BERT was learning. Fortunately, Tensor2Tensor has an excellent tool for visualizing attention in encoder-decoder transformer models, so I modified this to work with BERT\u2019s architecture, using a PyTorch implementation of BERT. The adapted interface is shown below, and you can run it yourself using the notebooks on Github.", "The tool visualizes attention as lines connecting the position being updated (left) with the position being attended to (right). Colors identify the corresponding attention head(s), while line thickness reflects the attention score. At the top of the tool, the user can select the model layer, as well as one or more attention heads (by clicking on the color patches at the top, representing the 12 heads).", "I used the tool to explore the attention patterns of various layers / heads of the pre-trained BERT model (the BERT-Base, uncased version). I experimented with different input values, but for demonstration purposes, I just use the following inputs:", "Sentence A: I went to the store.", "Sentence B: At the store, I bought fresh strawberries.", "BERT uses WordPiece tokenization and inserts special classifier ([CLS]) and separator ([SEP]) tokens, so the actual input sequence is: [CLS] i went to the store . [SEP] at the store , i bought fresh straw ##berries . [SEP]", "I found some fairly distinctive and surprisingly intuitive attention patterns. Below I identify six key patterns and for each one I show visualizations for a particular layer / head that exhibited the pattern.", "In this pattern, most of the attention at a particular position is directed to the next token in the sequence. Below we see an example of this for layer 2, head 0. (The selected head is indicated by the highlighted square in the color bar at the top.) The figure on the left shows the attention for all tokens, while the one on the right shows the attention for one selected token (\u201ci\u201d). In this example, virtually all of the attention is directed to \u201cwent,\u201d the next token in the sequence.", "On the left, we can see that the [SEP] token disrupts the next-token attention pattern, as most of the attention from [SEP] is directed to [CLS] rather than the next token. Thus this pattern appears to operate primarily within each sentence.", "This pattern is related to the backward RNN, where state updates are made sequentially from right to left. Pattern 1 appears over multiple layers of the model, in some sense emulating the recurrent updates of an RNN.", "In this pattern, much of the attention is directed to the previous token in the sentence. For example, most of the attention for \u201cwent\u201d is directed to the previous word \u201ci\u201d in the figure below. The pattern is not as distinct as the last one; some attention is also dispersed to other tokens, especially the [SEP] tokens. Like Pattern 1, this is loosely related to a sequential RNN, in this case the forward RNN.", "In this pattern, attention is paid to identical or related words, including the source word itself. In the example below, most of the attention for the first occurrence of \u201cstore\u201d is directed to itself and to the second occurrence of \u201cstore\u201d. This pattern is not as distinct as some of the others, with attention dispersed over many different words.", "In this pattern, attention is paid to identical or related words in the other sentence. For example, most of attention for \u201cstore\u201d in the second sentence is directed to \u201cstore\u201d in the first sentence. One can imagine this being particularly helpful for the next sentence prediction task (part of BERT\u2019s pre-training), because it helps identify relationships between sentences.", "In this pattern, attention seems to be directed to other words that are predictive of the source word, excluding the source word itself. In the example below, most of the attention from \u201cstraw\u201d is directed to \u201c##berries\u201d, and most of the attention from \u201c##berries\u201d is focused on \u201cstraw\u201d.", "This pattern isn\u2019t as distinct as some of the others. For example, much of the attention is directed to a delimiter token ([CLS]), which is the defining characteristic of Pattern 6 discussed next.", "In this pattern, most of the attention is directed to the delimiter tokens, either the [CLS] token or the [SEP] tokens. In the example below, most of the attention is directed to the two [SEP] tokens. As discussed in this paper, this pattern serves as a kind of \u201cno-op\u201d: an attention head focuses on the [SEP] tokens when it can\u2019t find anything meaningful in the input sentence to focus on.", "It has been said that data visualizations are a bit like Rorschach tests: our interpretations may be colored by our own beliefs and expectations. While some of the patterns above are quite distinct, others are somewhat subjective, so these interpretations should only be taken as preliminary observations.", "Also, the above 6 patterns describe the coarse attentional structure of BERT and do not attempt to describe the linguistic patterns that attention may capture. For example, there are many different types of \u201crelatedness\u201d that could manifest in Patterns 3 and 4, e.g., synonymy, coreference, etc. It would be interesting to see if different attention heads specialize in different types of semantic and syntactic relationships.", "You can check out the visualization tool on Github. Please play with it and share what you find!", "In Part 2, I extend the visualization tool to show how BERT is able to form its distinctive attention patterns. In my most recent article, I explore OpenAI\u2019s new text generator, GPT-2.", "Learn more about my visualization and interpretability work here. You can find me on Twitter @jesse_vig.", "Big thanks to Llion Jones for creating the original Tensor2Tensor visualization tool!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb49113672f77&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b49113672f77--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b49113672f77--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JesseVig?source=post_page-----b49113672f77--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JesseVig?source=post_page-----b49113672f77--------------------------------", "anchor_text": "Jesse Vig"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7b984a1f8d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&user=Jesse+Vig&userId=c7b984a1f8d1&source=post_page-c7b984a1f8d1----b49113672f77---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb49113672f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb49113672f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "BertViz"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "transformer"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need"}, {"url": "https://towardsdatascience.com/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e", "anchor_text": "masked language modeling"}, {"url": "https://jalammar.github.io/illustrated-bert/", "anchor_text": "The Illustrated Bert"}, {"url": "https://www.quora.com/What-are-the-best-visualizations-of-machine-learning-algorithms", "anchor_text": "visualization"}, {"url": "https://www.facebook.com/nipsfoundation/videos/203530960558001/", "anchor_text": "tools"}, {"url": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization", "anchor_text": "Tensor2Tenso"}, {"url": "https://github.com/huggingface/pytorch-pretrained-BERT", "anchor_text": "PyTorch implementation"}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "Github"}, {"url": "https://github.com/google-research/bert#pre-trained-models", "anchor_text": "BERT-Base, uncased version"}, {"url": "https://arxiv.org/abs/1906.04341", "anchor_text": "this paper"}, {"url": "https://www.facebook.com/nipsfoundation/videos/203530960558001/", "anchor_text": "It has been said"}, {"url": "https://github.com/jessevig/bertviz", "anchor_text": "Github"}, {"url": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8", "anchor_text": "most recent article"}, {"url": "https://jessevig.com", "anchor_text": "here"}, {"url": "https://twitter.com/jesse_vig", "anchor_text": "jesse_vig"}, {"url": "https://medium.com/@llionj", "anchor_text": "Llion Jones"}, {"url": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization", "anchor_text": "visualization tool"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b49113672f77---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----b49113672f77---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b49113672f77---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b49113672f77---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----b49113672f77---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb49113672f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&user=Jesse+Vig&userId=c7b984a1f8d1&source=-----b49113672f77---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb49113672f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&user=Jesse+Vig&userId=c7b984a1f8d1&source=-----b49113672f77---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb49113672f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b49113672f77--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb49113672f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b49113672f77---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b49113672f77--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b49113672f77--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b49113672f77--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b49113672f77--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b49113672f77--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b49113672f77--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b49113672f77--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b49113672f77--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JesseVig?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JesseVig?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jesse Vig"}, {"url": "https://medium.com/@JesseVig/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "987 Followers"}, {"url": "http://jessevig.com", "anchor_text": "jessevig.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7b984a1f8d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&user=Jesse+Vig&userId=c7b984a1f8d1&source=post_page-c7b984a1f8d1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdd746ee1255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77&newsletterV3=c7b984a1f8d1&newsletterV3Id=cdd746ee1255&user=Jesse+Vig&userId=c7b984a1f8d1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}