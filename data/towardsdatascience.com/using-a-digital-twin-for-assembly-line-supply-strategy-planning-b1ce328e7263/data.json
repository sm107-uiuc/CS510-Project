{"url": "https://towardsdatascience.com/using-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263", "time": 1683005029.922304, "path": "towardsdatascience.com/using-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263/", "webpage": {"metadata": {"title": "A Digital Twin for Assembly Line Supply Strategy Planning | Towards Data Science", "h1": "Using a Digital Twin for Assembly Line Supply Strategy Planning", "description": "This article describes how to use reinforcement learning to solve a production-logistics problem. It explains basic concepts of reinforcement learning and industrial digital twins and presents an in-depth case-study on assembly line supply strategy planning."}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/", "anchor_text": "robotics and gaming", "paragraph_index": 4}, {"url": "https://www.researchgate.net/publication/326039529_Optimization_of_global_production_scheduling_with_deep_reinforcement_learning", "anchor_text": "previous studies", "paragraph_index": 5}, {"url": "https://stable-baselines.readthedocs.io/en/master/", "anchor_text": "stable-baselines", "paragraph_index": 7}, {"url": "https://simpy.readthedocs.io/en/latest/", "anchor_text": "SimPy", "paragraph_index": 7}, {"url": "https://openai.com/blog/emergent-tool-use/", "anchor_text": "what the guys from openai are doing", "paragraph_index": 8}, {"url": "https://gym.openai.com/docs/", "anchor_text": "gym-package", "paragraph_index": 9}, {"url": "https://pypi.org/project/stable-baselines/", "anchor_text": "stable-baselines", "paragraph_index": 11}, {"url": "https://www.salabim.org/", "anchor_text": "Salabim", "paragraph_index": 15}, {"url": "https://simpy.readthedocs.io/en/latest/", "anchor_text": "SimPy", "paragraph_index": 15}, {"url": "http://www.bpmn.io", "anchor_text": "Camunda-Modeler", "paragraph_index": 16}, {"url": "https://github.com/microsoft/pyright", "anchor_text": "pyright", "paragraph_index": 16}, {"url": "https://code.visualstudio.com/docs/python/editing", "anchor_text": "vscode", "paragraph_index": 16}, {"url": "https://fladdimir.github.io/post/casymda/", "anchor_text": "this post", "paragraph_index": 16}, {"url": "https://github.com/fladdimir/casymda", "anchor_text": "the repo", "paragraph_index": 16}, {"url": "https://casymda.github.io/page/Webpage/Startpage.html", "anchor_text": "the (German) website", "paragraph_index": 16}, {"url": "https://www.sciencedirect.com/science/article/pii/S2405896319315010", "anchor_text": "improving the tour-building for in-plant milk-run systems", "paragraph_index": 27}, {"url": "https://github.com/fladdimir/tugger-routing/tree/github/tugger_src/gym_env/des_model", "anchor_text": "simulation model", "paragraph_index": 36}, {"url": "https://github.com/fladdimir/tugger-routing", "anchor_text": "repo", "paragraph_index": 40}, {"url": "http://localhost:5000", "anchor_text": "http://localhost:5000", "paragraph_index": 40}, {"url": "https://stable-baselines.readthedocs.io/en/master/guide/algos.html", "anchor_text": "The stable-baselines documentation", "paragraph_index": 56}, {"url": "https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html", "anchor_text": "recommended", "paragraph_index": 65}, {"url": "https://www.linkedin.com/in/wladimir-hofmann-9065a714b/", "anchor_text": "Wladimir Hofmann", "paragraph_index": 71}, {"url": "https://www.linkedin.com/in/clemens-lennart-schwarz-609815186/", "anchor_text": "Clemens L. Schwarz", "paragraph_index": 71}, {"url": "https://www.linkedin.com/in/fredrik-branding-064014163/", "anchor_text": "Fredrik Branding", "paragraph_index": 71}, {"url": "https://fladdimir.github.io/", "anchor_text": "https://fladdimir.github.io/", "paragraph_index": 73}], "all_paragraphs": ["Artificial Intelligence, Digital Twins, Industry 4.0\u2026 you might have heard these buzzwords before.But wouldn\u2019t it be cool to apply them, solving an actual problem?To learn about practical application rather than abstract concepts?To know which challenges to master, and which pitfalls to avoid?", "Great, so this article is for you!It will\u2026", "And it will include code, ready for execution on your machine. Docker-based, no further installation needed.", "Disclaimer: This article is the result of leisure-time work. It represents our individual perspectives on the discussed topics. It\u2019s a report capturing our personal experiences, rather than a purely scientific study. The content, concepts, and their implementation can probably be improved in virtually any imaginable way. We still hope that you enjoy reading.", "Reinforcement learning represents an emerging technique from machine learning. It can autonomously derive complex action sequences in dynamic environments and is successfully applied in various fields, e.g. from robotics and gaming. Instead of explicitly defining a specific solution strategy for a problem, we can just provide an environment. A self-learning agent will then autonomously discover successful strategies just by interaction.", "Needless to say, there is nothing new under the moon and previous studies show the general feasibility of using RL for solving production-logistics problems.", "So why do we think that there is a need for yet another article about this very topic?", "First, there is a lot of active development in RL, as well as in the application of Digital Twins in production/logistics. We believe that there is even more potential in integrating these concepts. This contribution strives to show a working example based on a tool-stack which seamlessly integrates two of the most popular open-source software packages from their respective areas: stable-baselines for RL and SimPy for implementing Digital Twins.", "If you still ask yourself what RL is capable of, we definitely recommend having a look at what the guys from openai are doing.", "Admittedly, that's probably a quite sophisticated and highly engineered example, but it breaks down to a simple interaction between an agent and an environment. Technically, this interaction is defined by an interface (or abstract base-class as Python likes to put it), which is part of the gym-package.", "The graphic below illustrates the exchange of information between agent and environment. First, the agent calls the environment\u2019s step method, providing the action to be executed. The environment then processes the action and returns:", "The interface also prescribes more, such as the formats of action-space and observation_space, as well as render and reset behaviour.The various RL algorithms provided by the stable-baselines-package are ready to work with environments implementing this gym-interface. All that is left to do is creating a compliant environment \u2014 and in the next section, we will show how this can be achieved in the domain of logistics.", "Frankly, Digital Twin is probably the most overused buzzword of all the \u201cLogistics 4.0 / Industry 4.0\u201d stuff that is out there. Even though we could not resist to put it into the title, from now on we\u2019ll prove that we can do better and use the more specific term \u201cDiscrete Event Simulation\u201d (DES).", "Why DES? Discrete Event Simulation is one of the widespread tools for analysis and design of logistics systems. Today\u2019s applications go beyond the traditional usage for systems planning. They include more operational use-cases such as virtual commissioning or short-term forecasts. Simulation models are getting integrated tightly into other IT-systems. This allows to increase process transparency and to improve our means to analyse, control, and optimise system performance in real-time. Doesn\u2019t this sound pretty close to what Digital Twins always promise?", "Most industrial simulation uses are still based on commercial packages.However, there are a couple of open-source alternatives, which are typically closer to general-purpose language programming. Even though they tend to lack some convenient commercial features, there are upsides such as better scalability and simplified interfacing.", "Related to Python we became aware of two popular DES packages: Salabim and SimPy. Both are not only free and open-source, but even built on top of the standard library of one of the world\u2019s most popular programming languages \u2014 let\u2019s see what we can get out of that!", "Based on SimPy, we added bits of complementing functionality to gain some of the modelling convenience of commercial \u201cblock-based\u201d DES-packages. facilitates the usage of BPMN process descriptions to generate corresponding simulation-model python-code. BPMN-files (basically xml) can easily be created with the Camunda-Modeler.This graphical modelling helps to maintain an overview of the high-level model-structure. Generated Casymda models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser. For presentation and debugging, animations can be paused and their speed can be changed dynamically. Solely animation-related events are not scheduled if the simulation is run without visualisation. This maximises the execution speed \u2014 which becomes especially important related to RL, when a high number of runs is necessary.Further features of Casymda include simulated movements along shortest paths in a 2D tile-map space, and gradual typing for development convenience (checkout pyright if you are using vscode). For more info on Casymda have a look at this post, the repo or the (German) website.", "To be able to train an RL-agent inside a simulation model, we need to make the model implementing the Gym-interface described above.", "The following diagram illustrates the coupling concept:", "When the step function of the Gym-Environment is called ( 1), the provided action is propagated to the relevant block of the simulation model ( 1.1). This is realised with help of an ActionHolder, so that a consuming piece of decision logic can dispatch according to the received information.", "Subsequently, the simulation is executed until a next_action_needed-Event is triggered by the simulation model ( 1.2). This is indicating the end of the current step and the need for another action of the agent.", "One Gym-step can thus comprise an arbitrary number of discrete SimPy-steps, each of which can in turn take an arbitrary amount of simulated time.", "Rewards are managed with help of a RewardHolder object, which is wired into the relevant blocks of the simulation model during environment initialisation. At the end of each step, occurred rewards are collected ( 1.3). Depending on the type of the optimisation problem to solve, a post-processing of collected rewards can be applied (e.g. taking into account the amount elapsed time, so that an agent can learn time-efficient behaviour).", "To check whether an episode ended (the done part of the returned information), the current state of the model is checked against configured done_criteria ( 1.4). These can contain e.g. some goals to be reached or a certain amount of time to be simulated.", "To provide the agent with an observation, a model-specific ModelStateToObservationConverter is used to collect relevant information from the model. The created observation conforms to the defined observation_space ( 1.5). This step could include e.g. counting the number of entities in different queues or checking inventory levels and creating a NumPy-array out of this information.", "Finally, collected information is returned to the agent ( 2), which can learn based on the reward and decide for the next action.", "Having the basics covered, let\u2019s see how we get this to work.", "Back in August of last year at the MIM2019 in Berlin, we had the chance to attend an interesting talk of two Bavarian guys presenting their research on improving the tour-building for in-plant milk-run systems. These internal deliveries are commonly used for assembly line supply, and the tours are typically following a very rigid plan. Given the fact that the actual demand at the line tends to vary, their research revealed quite a lot of potential to decrease delivery lead times and to increase systems\u2019 utilisation \u2014 just by making the tour-planning more dynamic.", "Based on this setting we constructed an abstracted and simplified version of an assembly line with a corresponding material supply system to provide a playground for reinforcement learning algorithms.", "The image below shows a schematic layout plan of the system:", "Unfinished products enter the system on the upper right ( I) and are assembled sequentially at 9 different stations, arranged in U-shape ( I-IX). Finished products leave the system after the last assembly step ( IX).Stations require a certain amount of resource of either type A or B to be present in the station's inventory before an assembly step can start.Each station can only hold one product at a time, and finished products can only be forwarded once the following station is empty (thus multiple upstream stations holding already finished products may be blocked by downstream stations which are still processing a product or waiting for material before being able to start processing).Material is supplied by a tugger, able to carry a limited discrete amount (\"boxes\"). The tugger can load material at a stock ( A and/or B, located at the bottom). 1 discrete unit of material (\"box\") can be loaded/unloaded at a time. The goal of the assembly line is achieving the maximal throughput, which also correlates with small lead-times of products.", "The average distance from the stock to an average consuming station in the system is approx. 811m (demand-weighted, since higher demands cause more frequent tours to corresponding stations).", "A tugger speed of 10 m/s, a tugger-capacity of 25 units and an (un-)loading time of 5 seconds per batch of 5 units lead to a cycle-time for the tugger of approx. 212s, and an average delivery pace of approx. 0.12 units/s. This assumes \u201cfull-truck-loads\u201d for the tugger, always moving fully loaded from a single stock to a single station.", "The tugger will therefore at most be able to achieve about 75% of the theoretically possible product throughput (0.12/s delivery vs. 0.16/s demand). The best throughput to be expected should thus be about 1080/24h.", "As we can see, the delivery performance of the tugger represents the limiting factor (bottleneck) of the system, which means that each improvement made here will be directly reflected by a corresponding increase in the overall throughput.For the sake of simplicity, no stochastic model behaviour (such as e.g. randomly distributed loading or movement times) is assumed, hence the simulation model will be deterministic.As stated: the system as a whole is quite abstracted and simplified \u2014 but still capturing at least some of the basic complexity inherent to real-world problems. Will our RL-agent be able to\u2026", "We\u2019ll find out, but let\u2019s first have a look at what the learning environment will look like.", "The simulation model of the system basically consists of 2 processes, both depicted in the graphic below.", "On the left side, products pass through the 9 assembly steps ( ProductStation, rotated U-shape) before leaving the system, occasionally being blocked by downstream stations or waiting for material at a station.", "On the right side the tugger passes through an infinite cycle of movement and loading/unloading process steps (after initial creation at location A by a TuggerSource):", "Note that even unsuccessful loading or unloading attempts are implemented to take a small, fixed amount of time, so that every possible Gym-step is guaranteed to take at least some simulated time (and a time-constrained episode is guaranteed to reach its end eventually).", "Below you can see a process animation, as well as an animation of a tile-map. The agent here follows an explicitly defined simple rule of always delivering a complete load of 25 units to the station with the lowest inventory level. To run the animation just clone the repo, run the command, and visit http://localhost:5000.", "The TuggerEnv implements the Gym-Env interface and wraps the simulation model to be used for RL-agent training. It forwards the decision on where to move the tugger next (action), it keeps track of the collected rewards for finished products arriving at the sink, and it checks whether an episode is done (i.e. the specified time of 24h/86400s elapsed).", "The render method of the Gym-Env is not implemented, since animations at arbitrary moments in time - whenever a Gym-step is finished - do not make much sense for discrete event simulation environments. The animation is controlled separately. The info return value of step is configured to return the number of finished_products which can then be logged.", "The model-specific extraction of the observation from the current model state is carried out by a TuggerEnvModelStateConverter object.", "Specifically, the observation consists of the following information which describes the current state of the system (overall 48 values).", "ProductStation-related observations (5 values x 9 stations = 45 values):", "TuggerEntity observations (3 values x 1 tugger = 3 values):", "Note that parts of a station observation are actually redundant (e.g. a station which is neither busy nor waiting nor empty can only be blocked) \u2014 behind lies the rationale that an intelligent algorithm will (hopefully) learn an importance of different components of an observation, so that we do not have to worry about more than providing all potentially useful information.", "The action_space (of type gym.spaces.Discrete) consists of the 11 possible movement targets (9 stations + 2 stocks, encoded by index).", "As stated above, the defined goal of the assembly line is to achieve the best possible throughput of products, which corresponds to producing as many products as possible e.g. during one episode (24h).", "How do we achieve that? Which kind of incentive is suitable to stimulate such a behaviour? The design of appropriate reward functions is known to be a non-trivial matter. In fact, the design of rewards and incentives even for (arguably more intelligent) humans is a major problem in management and education (remember the last time you studied for passing an exam instead of actually learning useful contents).", "For the environment at hand, we could just think about giving a single reward at the end of each episode, proportionally to the number of achieved products in that fixed amount of time (24h), which would probably properly reflect our aim of maximising the throughput. However, the resulting reward would be quite sparse and therefore greatly decelerate learning speed (taking the average duration of a random action, each episode would take more than 1000 actions to complete before an agent sees any reward).", "Another idea would be to reward every successful delivery of material to any station, which would be possible to be completed within 2 steps (movement to the stock & movement to a suitable station consuming the loaded material). This way we would get less sparse rewards, but also an obvious problem of exploitability, caused by the fact that the delivery of material to one station alone would actually never lead to the completion of any product at all.", "As a compromise, we simply decided to go for a reward of 1 every time a product completes its final assembly step, which is possible be completed within 12 steps (minimum, not necessarily an optimal strategy). Even exhibiting a random behaviour, this would allow an agent to generate a reward of around 50 during one episode, so that there are sufficient \u201crandomly successful\u201d samples to learn from.", "One problem with this reward comes from the fact that the simulated time needed to obtain a reward is not reflected by the reward itself. Since every gym-step can actually eat up a greatly varying amount of simulation time (from 5 seconds to >100), there is a huge implicit impact on the throughput, which the agent is unaware of. To solve this problem we introduced \u201ccosts of time\u201d, which means we simply give a small negative reward every step, proportional to the amount of simulated time that passed. This finally leaves us with the subsequent question of how big these \u201ccosts\u201d should be. If set too high, they would just overrule any of the few actual rewards at the beginning of the training. If put too low, there would not be sufficient stimulus to exhibit time-efficient behaviour at all. Again, as a simple compromise, we implemented the costs to grow proportionally with the highest reward seen so far at the end of an episode, which guarantees a certain balance, and rewards increasing time-efficiency.", "The above described reward that we designed is definitely not \u201cperfect\u201d and also feels a bit like putting too much effort into \u201creward engineering\u201d \u2014 nevertheless its a first solution our agents can hopefully work with.", "The environment presented above is characterised by a Discrete action space and a continuous ( Box) observations space. The stable-baselines documentation lists available RL algorithms and their compatibility.Due to the type of action space, some algorithms are not feasible (i.e. DDPG, SAC, and TD3).", "To train a stable-baselines RL algorithm, the TuggerEnv is vectorised, using a DummyVecEnv and a standard MlpPolicy. To leverage multiple CPUs for training, it can be desirable to use a SubprocVecEnv (but for simpler logging & analysis we did not go with that one here, instead, we did multiple independent training runs in parallel).", "Train an ACER-agent (by default for 10,000 steps only, which should take <1min):", "Plot performance (might require additional setup for mounting the display):", "Below we can see an ACER-agent trained for 1m steps:", "As we can see, the agent manages to fully load the 25 units onto the tugger most of the time, seems to target correct (A/B) stations for material unloading, and the choice of stations with a currently low inventory level seems reasonable too!", "But how does the overall performance look like?", "For comparison, we trained four algorithms (ACER, ACKTR, DQN, and PPO2) with standard settings for both 1 and 3 mio. (Gym-)steps. Training took up to 2.5 hours (DQN, 3mio. steps) on a 2.9GHz Intel i9, using a single-process DummyVecEnv as explained above.The following graph shows the number of produced products per episode (24h) over the course of the training run for each algorithm, as well as the performance of the deterministic lowest-inventory heuristics (yellow line; always delivering a complete load of 25 units to the station with the currently lowest inventory), and the average performance of fully random actions (turquoise line, measured over 100 episodes).", "The number of episodes varies due to the variable number of Gym-steps per episode (24h of simulated time), depending on the simulated time each Gym-step needs. The small number of episodes of the ACER_3mio. training is explained by the up to 17277 Gym-steps per episode, occurring from episode 260 on. Each step of such an episode takes only 5 seconds (the minimum possible time of all available Gym-steps, \u201cachieved\u201d by a repeated visit of the same location). This behaviour might be caused by the defined negative reward per step, proportional to the amount of simulated time the step needed. Apparently, the agent does not remember how to generate a positive reward and only tries to maximise the short-term reward by minimising the step-time. Obviously this behaviour does not lead to any successful delivery, let alone completion of any product.", "It is worth to be mentioned that all training runs were done with default algorithm settings, and that the evaluation of different hyper-parameters is strongly recommended for performance optimisation. Thus, it might not be improbable for an RL agent to close the performance gap towards the theoretically reachable optimum.", "Short version: Our best RL agent reached about 78% of the best possible performance inside our production-logistics environment.", "Ok, now is this good or bad?", "Well, one could be disappointed by the fact that our agent was not able to reach the performance of a hand-coded heuristics approach.But did we believe when we started that we could get a generic piece of code to cope with the non-trivial relations of our specific and fairly complex environment? Certainly not!And this was just a first shot \u2014 we did not yet start with hyper-parameter tuning or the evaluation of alternative rewards.", "What do your experiences with reinforcement learning look like?Which logistics problems did you solve with RL?Did you spot a bug somewhere in the code or do you want to suggest an improvement?Or do you have questions concerning the presented implementation and the tool-stack?", "Just feel free to drop us a note, thanks for reading!", "Wladimir Hofmann \u2014 Clemens L. Schwarz \u2014 Fredrik Branding", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Developer from Hamburg, Germany \u2014 https://fladdimir.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb1ce328e7263&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@fladdi.mir?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fladdi.mir?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "Wladimir Hofmann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8616a4cedfb1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&user=Wladimir+Hofmann&userId=8616a4cedfb1&source=post_page-8616a4cedfb1----b1ce328e7263---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1ce328e7263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1ce328e7263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@pixabay?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pixabay"}, {"url": "https://www.pexels.com/photo/interior-of-empty-parking-lot-257636/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://github.com/fladdimir/tugger-routing", "anchor_text": "https://github.com/fladdimir/tugger-routing"}, {"url": "https://openai.com/blog/", "anchor_text": "robotics and gaming"}, {"url": "https://www.researchgate.net/publication/326039529_Optimization_of_global_production_scheduling_with_deep_reinforcement_learning", "anchor_text": "previous studies"}, {"url": "https://stable-baselines.readthedocs.io/en/master/", "anchor_text": "stable-baselines"}, {"url": "https://simpy.readthedocs.io/en/latest/", "anchor_text": "SimPy"}, {"url": "https://openai.com/blog/emergent-tool-use/", "anchor_text": "what the guys from openai are doing"}, {"url": "https://gym.openai.com/docs/", "anchor_text": "gym-package"}, {"url": "https://pypi.org/project/stable-baselines/", "anchor_text": "stable-baselines"}, {"url": "https://www.salabim.org/", "anchor_text": "Salabim"}, {"url": "https://simpy.readthedocs.io/en/latest/", "anchor_text": "SimPy"}, {"url": "http://www.bpmn.io", "anchor_text": "Camunda-Modeler"}, {"url": "https://github.com/microsoft/pyright", "anchor_text": "pyright"}, {"url": "https://code.visualstudio.com/docs/python/editing", "anchor_text": "vscode"}, {"url": "https://fladdimir.github.io/post/casymda/", "anchor_text": "this post"}, {"url": "https://github.com/fladdimir/casymda", "anchor_text": "the repo"}, {"url": "https://casymda.github.io/page/Webpage/Startpage.html", "anchor_text": "the (German) website"}, {"url": "https://www.sciencedirect.com/science/article/pii/S2405896319315010", "anchor_text": "improving the tour-building for in-plant milk-run systems"}, {"url": "https://github.com/fladdimir/tugger-routing/tree/github/tugger_src/gym_env/des_model", "anchor_text": "simulation model"}, {"url": "https://github.com/fladdimir/tugger-routing", "anchor_text": "repo"}, {"url": "http://localhost:5000", "anchor_text": "http://localhost:5000"}, {"url": "https://stable-baselines.readthedocs.io/en/master/guide/algos.html", "anchor_text": "The stable-baselines documentation"}, {"url": "http://localhost:5000", "anchor_text": "http://localhost:5000"}, {"url": "https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html", "anchor_text": "recommended"}, {"url": "https://www.linkedin.com/in/wladimir-hofmann-9065a714b/", "anchor_text": "Wladimir Hofmann"}, {"url": "https://www.linkedin.com/in/clemens-lennart-schwarz-609815186/", "anchor_text": "Clemens L. Schwarz"}, {"url": "https://www.linkedin.com/in/fredrik-branding-064014163/", "anchor_text": "Fredrik Branding"}, {"url": "https://medium.com/tag/discrete-event-simulation?source=post_page-----b1ce328e7263---------------discrete_event_simulation-----------------", "anchor_text": "Discrete Event Simulation"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----b1ce328e7263---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/logistics?source=post_page-----b1ce328e7263---------------logistics-----------------", "anchor_text": "Logistics"}, {"url": "https://medium.com/tag/openai-gym?source=post_page-----b1ce328e7263---------------openai_gym-----------------", "anchor_text": "Openai Gym"}, {"url": "https://medium.com/tag/python?source=post_page-----b1ce328e7263---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1ce328e7263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&user=Wladimir+Hofmann&userId=8616a4cedfb1&source=-----b1ce328e7263---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1ce328e7263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&user=Wladimir+Hofmann&userId=8616a4cedfb1&source=-----b1ce328e7263---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1ce328e7263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb1ce328e7263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b1ce328e7263---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b1ce328e7263--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b1ce328e7263--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b1ce328e7263--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fladdi.mir?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fladdi.mir?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wladimir Hofmann"}, {"url": "https://medium.com/@fladdi.mir/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://fladdimir.github.io/", "anchor_text": "https://fladdimir.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8616a4cedfb1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&user=Wladimir+Hofmann&userId=8616a4cedfb1&source=post_page-8616a4cedfb1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1bfefda0b9b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-a-digital-twin-for-assembly-line-supply-strategy-planning-b1ce328e7263&newsletterV3=8616a4cedfb1&newsletterV3Id=1bfefda0b9b9&user=Wladimir+Hofmann&userId=8616a4cedfb1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}