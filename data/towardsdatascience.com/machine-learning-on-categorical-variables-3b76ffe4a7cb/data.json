{"url": "https://towardsdatascience.com/machine-learning-on-categorical-variables-3b76ffe4a7cb", "time": 1682997730.3719769, "path": "towardsdatascience.com/machine-learning-on-categorical-variables-3b76ffe4a7cb/", "webpage": {"metadata": {"title": "Machine learning on categorical variables | by Michael Kareev | Towards Data Science", "h1": "Machine learning on categorical variables", "description": "At first blush, categorical variables aren\u2019t that different from numerical ones. But once you start digging deeper and implement your machine learning (and preprocessing) ideas in code, you will stop\u2026"}, "outgoing_paragraph_urls": [{"url": "https://stackoverflow.com/questions/10164608/how-do-you-count-cardinality-of-very-large-datasets-efficiently-in-python", "anchor_text": "cardinality", "paragraph_index": 0}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "excellent housing prices data set", "paragraph_index": 2}, {"url": "https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/205873/data_description.txt?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1564839056&Signature=f5rA1N%2B30QoGsRGpUq8B4PuRuAPXgwHDtJ34IkFb4NGCjxoBvsUnmKuDFTqtK3SJ0JOS31A%2BZmmBlw5aKBoRu6Zjd5870t4LhPYT0SAbUPvfCmEJEPkLyjH55AUfFcP%2BzbcdihTDX47q1NdjDg%2FfBx%2FK2%2FNqd2QthD4J2AXX3BIm0Foxi0bYBUCU%2FQd4jw9yN6grKjRgDWU9eePGciBC%2FDPvqBQEFiqVgZuBW%2BMxOMoy3ElgIVJgs7PyxoACeywYutFtRV01uU5JlwYYsLumJI6M5W4o7hlXWOgZXSxZ0WUGWvacdhqmjqJWyY3ZA7JmHSk%2FSxXRfWNXhqn9J2teOw%3D%3D", "anchor_text": "provided data dictionary", "paragraph_index": 2}, {"url": "https://github.com/nastyh/Feature-Engineering--House-Prices", "anchor_text": "GitHub", "paragraph_index": 3}, {"url": "https://github.com/nastyh/Feature-Engineering--House-Prices/blob/master/Features_for_MLOps.ipynb", "anchor_text": "Features_for_MLOps.ipynb", "paragraph_index": 3}, {"url": "https://github.com/pandas-profiling/pandas-profiling", "anchor_text": "Pandas Profiling Package", "paragraph_index": 7}, {"url": "https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/", "anchor_text": "forest-based", "paragraph_index": 19}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "anchor_text": "extra functionality", "paragraph_index": 19}, {"url": "https://data.library.virginia.edu/understanding-q-q-plots/", "anchor_text": "q-q plot", "paragraph_index": 32}, {"url": "https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html", "anchor_text": "categories", "paragraph_index": 38}], "all_paragraphs": ["At first blush, categorical variables aren\u2019t that different from numerical ones. But once you start digging deeper and implement your machine learning (and preprocessing) ideas in code, you will stop every minute asking questions such as \u201cDo I do feature engineering on both train and test sets?\u201d or \u201cI heard something about cardinality \u2014 what is that and should I Google more about it?\u201d", "Let\u2019s see if we can clear some of this up with an action plan for how you deal with data sets that have a lot of categorical variables and train a couple of models.", "Kaggle will serve as our data source: it has an excellent housing prices data set. Be ready to spend some time going through the provided data dictionary. You can keep it open in a separate browser window. We will also load it into the Jupyter notebook. In this exercise, we will predict values in the column SalePrice, based on various parameters of houses.", "As always, all the code is available on GitHub (you need the workbook Features_for_MLOps.ipynb).It has some additional charts that we don\u2019t cover here but that are useful for better understanding of the process.", "Let\u2019s load the dependencies and the data:", "If you want to have a data dictionary inside your GUI:", "We can now do a quick data profiling:", "As almost always, I recommend the Pandas Profiling Package.", "There\u2019s a good number of missing values. There is no magic bullet to address this except to go through every feature one by one and decide what to do with each of them. We cleaned them the following way:", "Interesting that the test set has missing values where the train set doesn\u2019t. It means that we need to do additional cleaning:", "After this procedure, no NaNs are left:", "This is why you are here. This is the laundry list we will follow:", "If you look at some columns, like MSSubClass, you will realize that, while they contain numeric values (in this case, 20, 30, etc.), they are actually categorical variables. It becomes clear from the data dictionary:", "We suspect that there is more than one column like that. Let\u2019s confirm:", "It returns a list of non-object columns. After reading the description of each of them, we decided to do the following transformations:", "You will see soon why three year-related columns are not converted yet.", "If you have categorical features that exhibit high cardinality, you might face certain problems. Most likely, you will use a one-hot encoder, and your dataset can suddenly become very wide and sparse. It\u2019s bad for computational purposes (especially if the number of columns gets closer to the number of observations), it\u2019s bad for any tree-based methods (most likely, your tree will grow in one direction), and it might lead to overfitting and data leakage.", "You might address the issue conceptually or technically.", "Conceptual approach: Go over each of the variables, do value_counts() on them and decide whether you can sacrifice some of the less frequent values and lump them together under \u201cother.\u201d", "What we just did here: index() returns a position of a given element in a list. In our case, all values of the column that are not in top five in terms of frequency are in \u201cother\u201d now. Ideally, you want to do it for every column. After that, you do one-hot encoding. However, if you are parsimonious with your time, you might be good with a pure vanilla technical approach. Most likely, your computer will be able to handle a very wide dataset and process it relatively quickly. So just call get_dummies() and hope for the best. You might need to forget about forest-based or dimensionality reduction methods, but, in most cases, you can live with it. Sklearn\u2019s OneHotEncoder() provides some extra functionality here that can be useful. It has a parameter n_values() that you can use to specify the maximum number of values every column can keep.", "In this particular dataset, we first investigated whether train and test columns had different cardinality:", "And then decided to delve into this information by exploring bar charts:", "Luckily for us, none of the columns had lots of distinct categorical values in general, nor did the test and train sets exhibit high cardinality. Because of that, we were able to proceed with a plain vanilla one-hot encoding.", "As we said above, the case of high cardinality isn\u2019t that bad and only applies to minor values in respective columns. As such, we will keep them as it is. We still can check how they influence SalePrice, though.", "We will build box plots (it might take some time to render):", "A good idea is to close the charts after:", "No visible outliers can be detected.", "We will correlate numerical features with SalePrice hoping to understand which can be removed and which \u2014 combined. Looking at every feature probably isn\u2019t a good idea, so let\u2019s focus on the top15 (but you can change this number in the code below to anything else) most correlated variables:", "And again \u2014 you\u2019ll need to use some common sense. For example, GarageCars and GarageArea are telling the same story about how big a place where you keep your vehicle(s) is. Information about square footage is spread over different columns and can possibly be aggregated. Square footage of baths can follow the suit. An age of houses and when they were remodeled should also go hand in hand. Let\u2019s implement it:", "We have just created a new column, totSqFt, that combines three existing values. We can check whether it can serve as an correct approximation:", "After it\u2019s done, we can drop columns that go into new variables:", "We are in a good shape with independent variables but let\u2019s take another look at SalePrice.", "This q-q plot shows that very cheap and very expensive houses don\u2019t really follow a normal distribution. An extra check on totSqFt confirms it:", "You can explore these big and expensive (or small and cheap) houses:", "There is nothing special about them, so we should feel relatively safe about removing them from the set:", "After that, q-q plots look more normal.", "This is how you can effectively deal with datasets that have a lot of categorical features. We have done a good amount of data exploration and preprocessing that will help during the ML phase.", "And now the most important part here: you have to do any encoding on the combined dataset! Why? Imagine that you have a column \u201cColor\u201d that in the train set has values \u201cblue,\u201d \u201cgreen,\u201d and \u201cblack.\u201d A the same time the test also has \u201cyellow\u201d and \u201cred.\u201d Your encoder has to see all possible values to adequately work with them.", "There is no reason to retain the object type of categorical columns. Let\u2019s turn them into categories.", "A great way to ensure that everything is done right is to constantly check shape() of your dataframes:", "Here we saved Status separately and removed from X:", "Now we have three pieces: X_encoded (categorical variables after encoding), X_num (numerical variables that haven\u2019t changed) and St (just one column, Status).", "Combining them together (and a final size check):", "Now we can separate the combined set into train and test and continue machine learning:", "This de-facto approach is transparent and is described in various articles and books. Let\u2019s, however, avoid making our dataset too wide. How? By doing frequency encoding.", "Frequency encoding is not that hard to either understand or implement. We count the number of distinct values in a column and then divide by the total length of the column. As a result, we get a \u201cshare\u201d of every value that will play well with any ML algorithm.", "The code below should look familiar: we need to differentiate between train and test sets, then merge them together,", "So that you could compare what type of encoding gives better results, we created dataframes encoded using frequency and one-hot encoding methods:", "This part is explained in length in other sources; in addition, the workbook on GitHub contains a couple of implementations of different models: from a regression using the one-hot encoded dataset to Lasso and XGBoost. Below we will explore Linear Regresion and XGBoost that we ran on the set that underwent frequency encoding.", "After the dependencies are loaded, we can move to the modeling.", "The out of the box results almost matches that from the regression:", "If we optimize parameters, will it help? The code below takes a few minutes to run:", "We can tweak the models further and further but it\u2019s not the main learning outcome. The main one is that by treating categorical features in a wise and accurate manner, we can achieve decent results without extremely fancy machine learning methods or excessive computing power.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Basketball \u2192 Finance \u2192 Data Science"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3b76ffe4a7cb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nasty_haterz?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nasty_haterz?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "Michael Kareev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe9b59afcc29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&user=Michael+Kareev&userId=be9b59afcc29&source=post_page-be9b59afcc29----3b76ffe4a7cb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b76ffe4a7cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b76ffe4a7cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@v2osk?utm_source=medium&utm_medium=referral", "anchor_text": "v2osk"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://stackoverflow.com/questions/10164608/how-do-you-count-cardinality-of-very-large-datasets-efficiently-in-python", "anchor_text": "cardinality"}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "excellent housing prices data set"}, {"url": "https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/205873/data_description.txt?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1564839056&Signature=f5rA1N%2B30QoGsRGpUq8B4PuRuAPXgwHDtJ34IkFb4NGCjxoBvsUnmKuDFTqtK3SJ0JOS31A%2BZmmBlw5aKBoRu6Zjd5870t4LhPYT0SAbUPvfCmEJEPkLyjH55AUfFcP%2BzbcdihTDX47q1NdjDg%2FfBx%2FK2%2FNqd2QthD4J2AXX3BIm0Foxi0bYBUCU%2FQd4jw9yN6grKjRgDWU9eePGciBC%2FDPvqBQEFiqVgZuBW%2BMxOMoy3ElgIVJgs7PyxoACeywYutFtRV01uU5JlwYYsLumJI6M5W4o7hlXWOgZXSxZ0WUGWvacdhqmjqJWyY3ZA7JmHSk%2FSxXRfWNXhqn9J2teOw%3D%3D", "anchor_text": "provided data dictionary"}, {"url": "https://github.com/nastyh/Feature-Engineering--House-Prices", "anchor_text": "GitHub"}, {"url": "https://github.com/nastyh/Feature-Engineering--House-Prices/blob/master/Features_for_MLOps.ipynb", "anchor_text": "Features_for_MLOps.ipynb"}, {"url": "https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/205873/data_description.txt?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1564407075&Signature=Iduf4UDvx2Cei5S9B7A%2B%2Fz3u%2Ff8GG0RxvpfMu5IHRtJOFBsjq806B2sSr6zucZBwJeBNSOuIpOssfa4i%2BYS8ybrJgaHnA%2Fqkcox6ZsD8BLIl3yTHjwmfkie2ohGSI0bdZLiXblBWps8xJ8sGZPnmTegLYLhFgrA7O0BEF5dIXrFVYufTcndkOeOyYm3fopGjTablaxWOUyhmd43WfOxADJInaMqUk37SBzVD4jD1bj%2F%2B%2FJkK7OeTvUIBJOR3EXij97rhVqcZNdxTttF91t0W3HFcqJrRhrw5%2BKvZmHNzsT5AO164QSjlFqT5kU3dZWoZqxdDOxImVvr%2Fw2m4IRZGCw%3D%3D'", "anchor_text": "https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/205873/data_description.txt?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1564407075&Signature=Iduf4UDvx2Cei5S9B7A%2B%2Fz3u%2Ff8GG0RxvpfMu5IHRtJOFBsjq806B2sSr6zucZBwJeBNSOuIpOssfa4i%2BYS8ybrJgaHnA%2Fqkcox6ZsD8BLIl3yTHjwmfkie2ohGSI0bdZLiXblBWps8xJ8sGZPnmTegLYLhFgrA7O0BEF5dIXrFVYufTcndkOeOyYm3fopGjTablaxWOUyhmd43WfOxADJInaMqUk37SBzVD4jD1bj%2F%2B%2FJkK7OeTvUIBJOR3EXij97rhVqcZNdxTttF91t0W3HFcqJrRhrw5%2BKvZmHNzsT5AO164QSjlFqT5kU3dZWoZqxdDOxImVvr%2Fw2m4IRZGCw%3D%3D'"}, {"url": "https://github.com/pandas-profiling/pandas-profiling", "anchor_text": "Pandas Profiling Package"}, {"url": "https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/", "anchor_text": "forest-based"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "anchor_text": "extra functionality"}, {"url": "https://data.library.virginia.edu/understanding-q-q-plots/", "anchor_text": "q-q plot"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html", "anchor_text": "categories"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3b76ffe4a7cb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3b76ffe4a7cb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/sklearn?source=post_page-----3b76ffe4a7cb---------------sklearn-----------------", "anchor_text": "Sklearn"}, {"url": "https://medium.com/tag/features?source=post_page-----3b76ffe4a7cb---------------features-----------------", "anchor_text": "Features"}, {"url": "https://medium.com/tag/python?source=post_page-----3b76ffe4a7cb---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b76ffe4a7cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&user=Michael+Kareev&userId=be9b59afcc29&source=-----3b76ffe4a7cb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b76ffe4a7cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&user=Michael+Kareev&userId=be9b59afcc29&source=-----3b76ffe4a7cb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b76ffe4a7cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3b76ffe4a7cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3b76ffe4a7cb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3b76ffe4a7cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nasty_haterz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nasty_haterz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Kareev"}, {"url": "https://medium.com/@nasty_haterz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "150 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe9b59afcc29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&user=Michael+Kareev&userId=be9b59afcc29&source=post_page-be9b59afcc29--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F99ef10ff6a10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-on-categorical-variables-3b76ffe4a7cb&newsletterV3=be9b59afcc29&newsletterV3Id=99ef10ff6a10&user=Michael+Kareev&userId=be9b59afcc29&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}