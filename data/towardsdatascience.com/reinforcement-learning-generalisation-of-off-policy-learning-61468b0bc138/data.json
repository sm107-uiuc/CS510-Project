{"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138", "time": 1682997752.917601, "path": "towardsdatascience.com/reinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Generalisation of Off-Policy Learning | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Generalisation of Off-Policy Learning", "description": "Till now, we have extended our reinforcement learning topic from discrete state to continuous state and have elaborated a bit on applying tile coding to on-policy learning, that is the learning\u2026"}, "outgoing_paragraph_urls": [{"url": "https://stats.stackexchange.com/questions/410131/in-rl-why-using-a-behavior-policy-instead-of-the-target-policy-for-an-episode-i", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://stats.stackexchange.com/questions/237085/how-to-correctly-compute-rho-in-reinforcement-learning-with-importance-sampli?rq=1", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://medium.com/@zhangyue9306/importance-sampling-introduction-e76b2c32e744", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0", "anchor_text": "here", "paragraph_index": 3}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/BairdExample/BairdCounterExample.py", "anchor_text": "implementation", "paragraph_index": 22}], "all_paragraphs": ["Till now, we have extended our reinforcement learning topic from discrete state to continuous state and have elaborated a bit on applying tile coding to on-policy learning, that is the learning process follows the trajectory the agent takes. Now let\u2019s have a talk of off-policy learning in continuous settings. While in discrete settings, on-policy learning can easily be generalised to off-policy learning(say, from Sarsa to Q-learning), in continuous settings, the generalisation can be a little troublesome, and in some scenarios can cause divergence issues. In this article, I will:", "The most prominent consequence of off-policy learning is it may not necessarily converge in continuous settings. The major reason is caused by the distribution of updates in the off-policy case is not according to the on-policy distribution, that is the state, action being used to update might not be the state, action the agent takes. Let\u2019s get into the update formula and further talk about the problem. In continuous state of off-policy learning, the 1-step update process follows:", "Notice that comparing to on-policy generalisation, there is one more term \u03c1, which is called importance sampling ratio. \u03c1 is computed by target policy \u03c0 divided by behaviour policy b , in control, the target policy is typically the deterministic greedy policy with respect to the current estimate of the action-value function. This policy becomes a deterministic optimal policy while the behaviour policy remains stochastic and more exploratory, for example, an \u03f5-greedy policy. Recall that in discrete state space of Q learning, the temporal difference is max{Q(S', A')} \u2014 Q(S, A) (disregarding reward), so that at each state update, the maximum Q value(coming from greedy policy) is used to update instead of the value of actual state, action taken by the agent(behaviour policy). In continuous space, importance sampling is leveraged to approach the target policy value using behaviour policy. For more explanation of \u03c1 , you can refer to here and here(For more explanation of importance sampling, please refer to here).", "The rest part of the formula is same as on-policy learning, where w is weight vector, and \u03b4 is temporal difference(notice the \u03b4 is slightly difference depending on whether the problem is episodic or not, for non-episodic case, please refer to here).", "The divergence of off-policy learning, referring to Sutton\u2019s description in his book, is caused by:", "One transition occurs repeatedly without w being updated on other transitions. This is possible under off-policy training because the behaviour policy might select actions on those other transitions which the target policy never would.", "No worries if you get a little confused here, let\u2019s proceed to an example and see how the weights diverge.", "This is a famous and concise example in illustrating the divergence feature of off-policy learning.", "The dashed action takes the system to one of the six upper states with equal probability, whereas the solid action takes the system to the seventh state. The behaviour policy b selects the dashed and solid actions with probabilities 6/7 and 1/7, so that the next-state distribution under it is uniform (the same for all nonterminal states), which is also the starting distribution for each episode. The target policy \u03c0 always takes the solid action, and so the on-policy distribution (for \u03c0) is concentrated in the seventh state. The reward is 0 on all transitions. The discount rate is \u03b3 = 0.99 .(The target policy is set fixed only for simplicity and illustration reason)", "Notice that inside each circle is the representation of each state using weight vector.", "Despite some general setting, note that self.features is the representation of each state, and self.weights is the weight vector(The initial value is purposely set). The multiplication of weights and features are the values of each state.", "The action choosing and taking follows the rules.", "The value function, as stated above, returns the value of a state by taking multiplication of feature and weight.", "With the above preps, we are ready to let the agent learn. At each round, the agent repeat the process of state, action, next state, next action, \u2026, and because we set the target policy to always choose solid line, when the agent takes action dash , the importance sampling ratio will be 0, and 1/self.prob when it takes solid action. The sarsa argument used to control whether use on-policy learning or not(in on-policy learning, target policy is always the behaviour policy, thus \u03c1 = 1), and this is for result comparison only.", "See that in off-policy learning, the weights diverge to infinity, while on-policy method(Sarsa in this case) converges. Now let\u2019s look at the image again and explain the reason:", "For example, when the agent takes a solid action from the leftmost state(2*w1+w8) to the bottom state(w7+2*w8), if w8 increase its value through the formula above, this will result in value increase of both states, as they share weight w8. However, from the bottom state, by taking dash action would not contribute to off-policy learning, as \u03c1 is 0 in this case, this leads to the value of w8 always increase but never decrease. This is what Sutton said:", "One transition occurs repeatedly without w being updated on other transitions. This is possible under off-policy training because the behaviour policy might select actions on those other transitions which the target policy never would.", "In terms of solutions, the first one is to not use off-policy learning on continuous space settings, always use on-policy instead. Another way is to change the target error function. Till now, all function approximation updates are based on the target error of (true_value \u2014 current_value)^2 , but to make the training process converge, Sutton in his book suggested to minimise the Projected Bellman Error(PBE). For detailed explanation please refer to his book here, chapter 11.", "Here I give an simple implementation of this method and comparing it with classic off-policy learning. By minimising PBE, one gets an update formula of:", "Where \u03b2 is another step parameter and vt*xt (xt is the feature vector) is an approximation of \u03c1*\u03b4 . This method is called TD with gradient correction(TDC)", "The only difference lies in the run function:", "The process follows exactly the formula above, and with TDC, we get an learning process of:", "See that the weights converge slowly to the optimal value. There are also other methods introduced to solve the problem, you can find out yourself if you are interested(Check out full implementation).", "Lastly, referring to the ending section from Sutton\u2019s book:", "The whole area of off-policy learning is relatively new and unsettled. Which methods are best or even adequate is not yet clear. Are the complexities of the new methods introduced at the end of this chapter really necessary? Which of them can be combined effectively with variance reduction methods? The potential for off-policy learning remains tantalizing, the best way to achieve it still a mystery.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F61468b0bc138&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://meatba11.medium.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----61468b0bc138---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F61468b0bc138&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----61468b0bc138---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F61468b0bc138&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&source=-----61468b0bc138---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://stats.stackexchange.com/questions/410131/in-rl-why-using-a-behavior-policy-instead-of-the-target-policy-for-an-episode-i", "anchor_text": "here"}, {"url": "https://stats.stackexchange.com/questions/237085/how-to-correctly-compute-rho-in-reinforcement-learning-with-importance-sampli?rq=1", "anchor_text": "here"}, {"url": "https://medium.com/@zhangyue9306/importance-sampling-introduction-e76b2c32e744", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/BairdExample/BairdCounterExample.py", "anchor_text": "implementation"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/BairdExample/BairdCounterExample.py", "anchor_text": "implementation"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----61468b0bc138---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----61468b0bc138---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----61468b0bc138---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F61468b0bc138&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----61468b0bc138---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F61468b0bc138&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----61468b0bc138---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F61468b0bc138&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----61468b0bc138---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----61468b0bc138---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Written by Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----61468b0bc138---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-generalisation-of-off-policy-learning-61468b0bc138&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----61468b0bc138---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----61468b0bc138----0---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----61468b0bc138----0---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----61468b0bc138----0---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/geekculture?source=author_recirc-----61468b0bc138----0---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----61468b0bc138----0---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Using ChatGPT in PythonPractical Examples of Using ChatGPT SDK"}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----61468b0bc138----0---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "\u00b74 min read\u00b7Dec 20, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Feeaed9847e72&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fusing-chatgpt-in-python-eeaed9847e72&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----eeaed9847e72----0-----------------clap_footer----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----61468b0bc138----0---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feeaed9847e72&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fusing-chatgpt-in-python-eeaed9847e72&source=-----61468b0bc138----0-----------------bookmark_preview----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----61468b0bc138----1---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----61468b0bc138----1---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----61468b0bc138----1---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----61468b0bc138----1---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----61468b0bc138----1---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----61468b0bc138----1---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----61468b0bc138----1---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----61468b0bc138----1-----------------bookmark_preview----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----61468b0bc138----2---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----61468b0bc138----2---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----61468b0bc138----2---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----61468b0bc138----2---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----61468b0bc138----2---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----61468b0bc138----2---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----61468b0bc138----2---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----61468b0bc138----2-----------------bookmark_preview----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----61468b0bc138----3---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----61468b0bc138----3---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----61468b0bc138----3---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Jeremy Zhang"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----61468b0bc138----3---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----61468b0bc138----3---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "UNet Line by Line ExplanationExample UNet Implementation"}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----61468b0bc138----3---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": "\u00b74 min read\u00b7Oct 18, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b191c76baf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funet-line-by-line-explanation-9b191c76baf5&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----9b191c76baf5----3-----------------clap_footer----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----61468b0bc138----3---------------------44db4f83_8af3_4942_8c41_237d261a3e0e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b191c76baf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funet-line-by-line-explanation-9b191c76baf5&source=-----61468b0bc138----3-----------------bookmark_preview----44db4f83_8af3_4942_8c41_237d261a3e0e-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "See all from Jeremy Zhang"}, {"url": "https://towardsdatascience.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----61468b0bc138----0-----------------bookmark_preview----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----61468b0bc138----1-----------------bookmark_preview----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "AI Anyone Can Understand: Part 2 \u2014 The Bellman EquationMake sure you check out the rest of the AI Anyone Can Understand Series I have written and plan to continue to write on"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F614846383eb7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7&user=Andrew+Austin&userId=42d388912d13&source=-----614846383eb7----0-----------------clap_footer----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----61468b0bc138----0---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F614846383eb7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7&source=-----61468b0bc138----0-----------------bookmark_preview----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----1-----------------clap_footer----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----61468b0bc138----1---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----61468b0bc138----1-----------------bookmark_preview----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----61468b0bc138----2---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----61468b0bc138----2---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----61468b0bc138----2---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----61468b0bc138----2---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----61468b0bc138----2---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----61468b0bc138----2---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----2-----------------clap_footer----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----61468b0bc138----2---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----61468b0bc138----2-----------------bookmark_preview----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----61468b0bc138----3---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----61468b0bc138----3---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----61468b0bc138----3---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Aniruddha Mukherjee"}, {"url": "https://medium.com/dsckiit?source=read_next_recirc-----61468b0bc138----3---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "GDSC KIIT"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----61468b0bc138----3---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "Reinforcement Learning: An Introduction and Guide to its FundamentalsPolicies, Rewards, the Bellman Equation, and the Markov Decision Process (MDP)"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----61468b0bc138----3---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": "5 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdsckiit%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&user=Aniruddha+Mukherjee&userId=68f97387c191&source=-----467c6a2ed25e----3-----------------clap_footer----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----61468b0bc138----3---------------------e89e7003_3606_4815_8133_ffee4b5f375d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&source=-----61468b0bc138----3-----------------bookmark_preview----e89e7003_3606_4815_8133_ffee4b5f375d-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----61468b0bc138--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----61468b0bc138--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}