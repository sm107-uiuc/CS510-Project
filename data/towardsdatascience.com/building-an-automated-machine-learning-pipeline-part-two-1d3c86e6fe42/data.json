{"url": "https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42", "time": 1683007877.5047302, "path": "towardsdatascience.com/building-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42/", "webpage": {"metadata": {"title": "Building an Automated Machine Learning Pipeline: Part Two | by Ceren Iyim | Towards Data Science", "h1": "Building an Automated Machine Learning Pipeline: Part Two", "description": "Setting the Evaluation Metric & Establishing a Baseline, Selecting the Algorithm and Performing Hyperparameter Tuning steps"}, "outgoing_paragraph_urls": [{"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/blob/master/notebooks/WineRatingPredictor-2.ipynb", "anchor_text": "in this notebook", "paragraph_index": 8}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/tree/master/notebooks/transformed", "anchor_text": "notebooks/transformed", "paragraph_index": 10}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle", "paragraph_index": 13}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html", "anchor_text": "Linear regression", "paragraph_index": 30}, {"url": "https://render.githubusercontent.com/view/ipynb?commit=800ae104b19cb48b4e403ee24aa541e723e1972e&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f636572656e6979696d2f57696e652d526174696e672d507265646963746f722d4d4c2d4d6f64656c2f383030616531303462313963623438623465343033656532346161353431653732336531393732652f6e6f7465626f6f6b732f57696e65526174696e67507265646963746f722d322e6970796e62&nwo=cereniyim%2FWine-Rating-Predictor-ML-Model&path=notebooks%2FWineRatingPredictor-2.ipynb&repository_id=257017095&repository_type=Repository#Normalize-Datasets-for-KNN-and-SVM", "anchor_text": "here in the notebook", "paragraph_index": 31}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html", "anchor_text": "K-nearest neighbors regressor", "paragraph_index": 32}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html", "anchor_text": "Support vector regressor", "paragraph_index": 33}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html", "anchor_text": "Random forest regressor", "paragraph_index": 34}, {"url": "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor", "anchor_text": "Light gradient boosting regressor", "paragraph_index": 35}, {"url": "https://render.githubusercontent.com/view/ipynb?commit=8f4a85d2c45f18c97f086c631778358717a0b0da&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f636572656e6979696d2f57696e652d526174696e672d507265646963746f722d4d4c2d4d6f64656c2f386634613835643263343566313863393766303836633633313737383335383731376130623064612f6e6f7465626f6f6b732f57696e65526174696e67507265646963746f722d322e6970796e62&nwo=cereniyim%2FWine-Rating-Predictor-ML-Model&path=notebooks%2FWineRatingPredictor-2.ipynb&repository_id=257017095&repository_type=Repository#Tune-Hyperparameters-of-the-Models", "anchor_text": "here in the notebook", "paragraph_index": 36}, {"url": "https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/", "anchor_text": "this quotation", "paragraph_index": 41}, {"url": "https://machinelearningmastery.com/", "anchor_text": "Machine Learning Mastery", "paragraph_index": 41}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV", "paragraph_index": 50}, {"url": "https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-part-one-5c70ae682f35?source=friends_link&sk=8de05327eedb3d0dadcfa4b1a8e8cc75", "anchor_text": "first article", "paragraph_index": 65}, {"url": "https://www.docker.com/", "anchor_text": "Docker", "paragraph_index": 69}, {"url": "https://luigi.readthedocs.io/en/stable/", "anchor_text": "Luigi", "paragraph_index": 69}, {"url": "https://twitter.com/cereniyim", "anchor_text": "Twitter", "paragraph_index": 70}, {"url": "https://www.linkedin.com/in/ceren-iyim", "anchor_text": "Linkedin", "paragraph_index": 70}, {"url": "http://linkedin.com/in/ceren-iyim/", "anchor_text": "linkedin.com/in/ceren-iyim/", "paragraph_index": 73}, {"url": "http://github.com/cereniyim", "anchor_text": "github.com/cereniyim", "paragraph_index": 73}], "all_paragraphs": ["In this article series, we set our course to build a 9-step machine learning (ML) pipeline (we are calling it the wine rating predictor) and automate it. Eventually, we will observe how each step congregates and runs in production systems.", "We are working on a supervised regression problem. We want to develop a performant, understandable and good wine rating predictor that can predict the points, a quality measure of wine.", "In the first article, we defined the problem and our motivation behind building the wine rating predictor. Then, we had a detailed look at the data by visualizing the relationships between the features and the target along with the Understand & Clean & Format Data and Exploratory Data Analysis steps.", "In the Feature Engineering & Pre-processing step, we added new and more useful features. Besides, we prepared the training and test datasets to use during training and evaluation of the model. As the final step of the first article, we created validation dataset out of the training dataset for model selection.", "In this article, we are going to complete the following steps:", "4. Set Evaluation Metric & Establish Baseline", "5. Select an ML Model based on the Evaluation Metric", "6. Perform Hyperparameter Tuning on the Selected Model", "The code behind this article can be found in this notebook. The complete project is available on GitHub:", "Feel free to share, fork, and utilize this repo for your projects!", "The datasets that we are going to use is available in notebooks/transformed", "We are going to need the following libraries of Python in this notebook:", "Let\u2019s load the datasets into dataframes and convert to an array using below functions:", "If this was a Kaggle competition, we would skip this step of the pipeline because we would be given with the evaluation metric.", "However, in real-world applications of data science/machine learning, the evaluation metric is set by data scientists in line with the stakeholder\u2019s expectations from the ML model. That is why this is an important step.", "After we decide on our evaluation metric, to quantify our initial motive \u2014 building a good wine rating predictor and to compare our model\u2019s performance against, we are going to form a baseline.", "The mean square error (MSE) is set as the evaluation metric. It is the average of the sum of squared residuals where a \u200bresidual is the subtraction of the actual values from the predicted values of the target variable.", "In other words, evaluation of the model is done by looking at the measure of how large the squared errors (residuals) are spread out.", "We selected MSE because it is interpretable and analogous to variance, and it is a widely-used optimization criterion among ML models. (E.g. linear regression, random forest)", "A \u200bbaseline\u200b can be explained as generating a naive guess of the target value by using expert knowledge or a few lines of code. It also helps to measure the performance of the ML model.", "If the built-model (wine rating predictor) cannot beat this baseline, then the selected ML algorithm may not be the best approach to solve this problem or we might want to revisit the previous steps of the pipeline.", "Recall that the points (target) has a normal distribution between 80 and 100. The mean is 88.45 and the variance is 9.1.", "We are going to form the baseline by:", "It is not a coincidence that the variance of the points and baseline error is almost equal. You can think of this baseline MSE as the manually calculated variance with a smaller set from our dataset.", "This number (9.01) will accompany us in the next step \u2014 Select an ML Model based on the Evaluation Metric while we are testing different ML algorithms.", "I always find useful trying out several algorithms with different rationales behind, because I believe this whole process involves experiment as well! (\u201cscientist\u201d part of being a data scientist makes sense now \ud83d\ude43)", "While searching for the best algorithm, we are going to observe both the improvement of the MSE and the run time of the algorithms (with %%time magic at the beginning of the cells) and compare the algorithm\u2019s MSE to our baseline MSE. At the same time, we will keep in mind the understandable and performant requirements of the wine rating predictor.", "We are going to experiment with one linear algorithm, two distance-based algorithm and two tree-based algorithms, ordered from the simplest to most complex:", "We are going to train them with the training set, and compare their generalization performances with the validation set. Below function will do the work for us.", "At the end of this step, we are going to elaborate on how the selected algorithm works.", "Linear regression slightly decreased the baseline metric, showing that it is not a candidate to be a good predictor.", "Distance-based models use the euclidian distance (or other distance measures) for training, thus varying ranges causes distance-based models to generate inaccurate predictions. To apply distance-based algorithms, we scaled the datasets with normalization before here in the notebook.", "K-nearest neighbors regressor performed better than the linear regression. However, MSE is still high, showing that this algorithm is not a good predictor as well.", "Support vector regressor performed better than the k-nearest neighbors regressor at a higher run-time. All in all, MSE decreased 35% showing that this algorithm might be a candidate for building a good predictor.", "Random forest regressor performed better than the support vector regressor at lesser run-times. It decreased the MSE 44% and replaced support vector regressor in the good-predictor list.", "Light gradient boosting regressor (Light GBM) showed the best performance of all tried models. It also lowered the baseline MSE 45% and showed that it is a potential candidate for a good predictor at a lower run time.", "It might not be a fair selection of algorithms since we only train them with the default hyperparameters. Nevertheless, this is the experimental step of the pipeline, that is why both light GBM and random forest algorithm has been given a chance for further improvements in the Perform Hyperparameter Tuning on the Selected Model step. (You can find it here in the notebook) In this article, we will use random forest regressor due to the understandable and performant requirements and we will only elaborate on it.", "Before moving on to the next step, let\u2019s understand how random forest regressor works:", "Random forest regressor \u200bis an ensemble algorithm that builds multiple decision trees once and trains them on various sub-samples and various subsets of the features of the dataset. Its random selection of the dataset and feature subsamples makes this algorithm more robust.", "A Decision tree\u200b uses a tree-like structure to make predictions. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches, each representing values for the feature tested. Leaf node represents a final decision on the target.", "A hyperparameter is the set of defined parameter(s) by the data scientist or ML engineer whose values are not affected by the training process of the model. On the other hand, a parameter of a model is searched and optimized by the model during the training process and affected by the dataset.", "I loved this quotation from Jason Brownlee on Machine Learning Mastery to prevent confusion of the two:", "\u201cIf you have to specify a model parameter manually then, it is probably a model hyperparameter.\u201d", "An example of a parameter from our simplest model: Coefficients of the linear regression model, which are optimized through the model training.", "An example of a hyperparameter from our selected model: The number of trees constructed in a random forest model, which is specified by us or by scikit-learn.", "Hyperparameter tuning is the process of defining, searching and perhaps improving the performance of the model further. We are going to search for the best set of parameters with the random search and k-fold cross-validation.", "Random search is the process of searching for the combination of the defined parameters randomly and comparing the defined score (mean squared error, for this problem) in each iteration. It is fast and run-time-efficient, but you may not always find the most optimal set of hyperparameters due to the searching of a random combination of defined hyperparameters.", "We are going to search for the below hyperparameters of random forest regressor with the hyperparameter_grid dictionary:", "K-fold cross-validation is the method used to assess the performance of the model on the complete training dataset. Rather than splitting the dataset into two static subsets of training and validation set, the dataset is divided equally for the given K. Then the model is trained with K-1 subsets and tested on Kth subset iteratively. This process makes the model more robust to overfitting \u2014 more on that at the end of the article.", "To perform hyperparameter tuning with random search and k-fold cross-validation we are going to add training and validation datasets and continue with one training set from now on.", "Since we have less than 10.000 rows in the training dataset, we are going to perform 4 fold cross-validation to have sufficient number of data points in each fold. We are going to consolidate random search and k-fold cross-validation in the RandomizedSearchCV object:", "With the fit method, we initiate the search for the random combination of the values defined for each hyperparameter in the hyperparameter_grid. At the same time, \"neg_mean_squared_error\" is calculated for each fold in each iteration. We can observe the determined best set of parameters by calling the best_estimator_ method:", "After hyperparameter tuning, the best set of hyperparameters are determined as:", "Let\u2019s see if those hyperparameters will help us to improve the MSE of the random forest regressor further.", "The MSE is decreased from 5.41 to 4.99 and tuned model\u2019s run-time resulted in 1.12 seconds which is lower than the run-time of the initial random forest model (1.89 seconds). Hyperparameter tuning has not only improved the evaluation metric but also lowered the run-time in our case.", "Given the sample dataset, the determined set of features and the tuned random forest regressor we have successfully built a good wine rating predictor without falling into the underfitting and overfitting areas!", "Although this topic is another article by itself, I think it is important to address here since we mentioned these concepts.", "Overfitting happens when the ML model perfectly fits (or memorizes) the training dataset rather than grasping the common patterns between the features and the target. An overfit model will have a high variance, I find a resemblance between this situation and a fragile glass home. It is perfectly built for its current conditions, but it is less likely to survive if the conditions are changed.", "Did you remember that cross-validation makes our model more robust to overfitting?", "The reason behind is that our model sees a different dataset in each fold during training \u2014 like a house already encounters different weather conditions. This improves the generalization performance which makes the model more resistant to overfitting.", "Underfitting happens when the ML model fails to grasp the relationships between the features and the target due to not having enough data points or features. It may perform as bad as a random guess about the predictions. An underfit model will have a high bias, I think of this situation as a half-built shed. It should have to be undergone some construction to serve its purpose.", "Did you remember the linear regression model that only showed a slight improvement on the baseline MSE?", "That was an example of an underfitting. Linear regression model resulted in almost the same MSE as the baseline MSE and failed to understand the relationship between the features and the target.", "All in all, both overfitting and underfitting decrease the generalization performance of the machine learning models, and results in unsatisfactory levels of evaluation metric. Although we did not test the tuned random forest regressor specifically the overfitting case, the 45% improvement shows that we are at the fine line between underfitting and overfitting.", "Before moving on to the conclusions let\u2019s save our model in the directory so that we can pick up where we left off in the last article and notebook.", "In this article, we completed the intermediate steps of the machine learning pipeline. After some quick recap on the first article and objectives, we", "If we consider our initial motivation (building a good wine predictor) and the baseline MSE (9.01), we are going on the right path as we approach the end of the pipeline.", "We have significantly lowered the baseline MSE, from 9.01 to 4.99 which results in 45% improvement!", "The third article will start by loading the fine-tuned random forest regressor model and will focus on the evaluation of the model with the test set, as well as the results of it. (steps 7, 8, and 9).", "The last article will automate this pipeline with Docker and Luigi.", "For comments or constructive feedback, you can reach out to me on responses, Twitter or Linkedin!", "Stay safe and healthy until then \ud83d\udc4b", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer and Data Scientist | linkedin.com/in/ceren-iyim/ | github.com/cereniyim"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d3c86e6fe42&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@cereniyim?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cereniyim?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "Ceren Iyim"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F287e9909d3b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&user=Ceren+Iyim&userId=287e9909d3b5&source=post_page-287e9909d3b5----1d3c86e6fe42---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d3c86e6fe42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d3c86e6fe42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/machine-learning/home", "anchor_text": "MACHINE LEARNING"}, {"url": "https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-part-one-5c70ae682f35?source=friends_link&sk=8de05327eedb3d0dadcfa4b1a8e8cc75", "anchor_text": "Part 1: Understand, clean, explore, process data"}, {"url": "https://medium.com/p/building-an-automated-machine-learning-pipeline-a74acda76b98?source=email-287e9909d3b5--writer.postDistributed&sk=1790d8dd404126a45828c3905f47432c", "anchor_text": "Part 3: Train, evaluate and interpret model"}, {"url": "https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-part-four-787cdc50a12d?source=friends_link&sk=7068bbe294f0aa37f247336afcf3eac0", "anchor_text": "Part 4: Automate your pipeline using Docker and Luigi"}, {"url": "https://unsplash.com/@zachhagy?utm_source=medium&utm_medium=referral", "anchor_text": "Zachariah Hagy"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/blob/master/notebooks/WineRatingPredictor-2.ipynb", "anchor_text": "in this notebook"}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model", "anchor_text": "cereniyim/Wine-Rating-Predictor-ML-ModelIn this procject, I built a wine rating predictor for an online wine seller. This wine predictor aims to show good\u2026github.com"}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/tree/master/notebooks/transformed", "anchor_text": "notebooks/transformed"}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/blob/master/notebooks/transformed/X_train.csv", "anchor_text": "X_train"}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/blob/master/notebooks/transformed/y_train.csv", "anchor_text": "y_train"}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/blob/master/notebooks/transformed/X_valid.csv", "anchor_text": "X_valid"}, {"url": "https://github.com/cereniyim/Wine-Rating-Predictor-ML-Model/blob/master/notebooks/transformed/y_valid.csv", "anchor_text": "y_valid"}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle"}, {"url": "https://www.dataquest.io/blog/understanding-regression-error-metrics/", "anchor_text": "Dataquest.io"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html", "anchor_text": "Linear regression"}, {"url": "https://render.githubusercontent.com/view/ipynb?commit=800ae104b19cb48b4e403ee24aa541e723e1972e&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f636572656e6979696d2f57696e652d526174696e672d507265646963746f722d4d4c2d4d6f64656c2f383030616531303462313963623438623465343033656532346161353431653732336531393732652f6e6f7465626f6f6b732f57696e65526174696e67507265646963746f722d322e6970796e62&nwo=cereniyim%2FWine-Rating-Predictor-ML-Model&path=notebooks%2FWineRatingPredictor-2.ipynb&repository_id=257017095&repository_type=Repository#Normalize-Datasets-for-KNN-and-SVM", "anchor_text": "here in the notebook"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html", "anchor_text": "K-nearest neighbors regressor"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html", "anchor_text": "Support vector regressor"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html", "anchor_text": "Random forest regressor"}, {"url": "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor", "anchor_text": "Light gradient boosting regressor"}, {"url": "https://render.githubusercontent.com/view/ipynb?commit=8f4a85d2c45f18c97f086c631778358717a0b0da&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f636572656e6979696d2f57696e652d526174696e672d507265646963746f722d4d4c2d4d6f64656c2f386634613835643263343566313863393766303836633633313737383335383731376130623064612f6e6f7465626f6f6b732f57696e65526174696e67507265646963746f722d322e6970796e62&nwo=cereniyim%2FWine-Rating-Predictor-ML-Model&path=notebooks%2FWineRatingPredictor-2.ipynb&repository_id=257017095&repository_type=Repository#Tune-Hyperparameters-of-the-Models", "anchor_text": "here in the notebook"}, {"url": "https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/", "anchor_text": "this quotation"}, {"url": "https://machinelearningmastery.com/", "anchor_text": "Machine Learning Mastery"}, {"url": "https://scikit-learn.org/stable/modules/cross_validation.html", "anchor_text": "scikit-learn"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV"}, {"url": "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/", "anchor_text": "Overfitting and Underfitting"}, {"url": "https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-part-one-5c70ae682f35?source=friends_link&sk=8de05327eedb3d0dadcfa4b1a8e8cc75", "anchor_text": "first article"}, {"url": "https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-a74acda76b98", "anchor_text": "Building an Automated Machine Learning PipelineTraining and evaluating the model, interpretation of model results and final conclusionstowardsdatascience.com"}, {"url": "https://www.docker.com/", "anchor_text": "Docker"}, {"url": "https://luigi.readthedocs.io/en/stable/", "anchor_text": "Luigi"}, {"url": "https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-part-four-787cdc50a12d", "anchor_text": "Building an Automated Machine Learning Pipeline: Part FourAutomate your pipeline with Docker and Luigitowardsdatascience.com"}, {"url": "https://twitter.com/cereniyim", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/ceren-iyim", "anchor_text": "Linkedin"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1d3c86e6fe42---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----1d3c86e6fe42---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1d3c86e6fe42---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----1d3c86e6fe42---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----1d3c86e6fe42---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d3c86e6fe42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&user=Ceren+Iyim&userId=287e9909d3b5&source=-----1d3c86e6fe42---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d3c86e6fe42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&user=Ceren+Iyim&userId=287e9909d3b5&source=-----1d3c86e6fe42---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d3c86e6fe42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d3c86e6fe42&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d3c86e6fe42---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d3c86e6fe42--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cereniyim?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cereniyim?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ceren Iyim"}, {"url": "https://medium.com/@cereniyim/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "504 Followers"}, {"url": "http://linkedin.com/in/ceren-iyim/", "anchor_text": "linkedin.com/in/ceren-iyim/"}, {"url": "http://github.com/cereniyim", "anchor_text": "github.com/cereniyim"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F287e9909d3b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&user=Ceren+Iyim&userId=287e9909d3b5&source=post_page-287e9909d3b5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff788c50c2bf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-an-automated-machine-learning-pipeline-part-two-1d3c86e6fe42&newsletterV3=287e9909d3b5&newsletterV3Id=f788c50c2bf3&user=Ceren+Iyim&userId=287e9909d3b5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}