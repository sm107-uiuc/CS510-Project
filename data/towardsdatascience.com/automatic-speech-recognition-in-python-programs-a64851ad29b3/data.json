{"url": "https://towardsdatascience.com/automatic-speech-recognition-in-python-programs-a64851ad29b3", "time": 1683003878.6750479, "path": "towardsdatascience.com/automatic-speech-recognition-in-python-programs-a64851ad29b3/", "webpage": {"metadata": {"title": "Speech Recognition with Python. Learn which of the 9 most prominent\u2026 | by Satish Chandra Gupta | Towards Data Science", "h1": "Speech Recognition with Python", "description": "Google Speech-to-Text, Amazon Transcribe, Microsoft Azure Speech, Watson, Nuance, CMU Sphinx, Kaldi, DeepSpeech, Facebook wav2letter."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/slanglabs/voice-in-apps-youtube-25bcc288ac4c", "anchor_text": "YouTube", "paragraph_index": 0}, {"url": "https://medium.com/slanglabs/voice-in-apps-gaana-1f6e2d8b026b", "anchor_text": "Gana", "paragraph_index": 0}, {"url": "https://medium.com/slanglabs/voice-in-apps-paytm-travel-5bee6aea76dc", "anchor_text": "Paytm Travel", "paragraph_index": 0}, {"url": "https://medium.com/slanglabs/voice-in-apps-my-jio-5dc8f2e298d", "anchor_text": "My Jio", "paragraph_index": 0}, {"url": "https://medium.com/slanglabs/what-is-voice-augmented-experience-1003a28b6e5", "anchor_text": "augment existing apps with voice experiences", "paragraph_index": 0}, {"url": "https://docs.slanglabs.in/", "anchor_text": "Android and Web SDKs", "paragraph_index": 2}, {"url": "https://colab.research.google.com/github/scgupta/ml4devs-notebooks/blob/master/speech/asr/python_speech_recognition_notebook.ipynb", "anchor_text": "Colab notebook", "paragraph_index": 12}, {"url": "https://people.csail.mit.edu/hubert/pyaudio/", "anchor_text": "PyAudio", "paragraph_index": 13}, {"url": "http://www.portaudio.com/", "anchor_text": "PortAudio", "paragraph_index": 13}, {"url": "https://colab.research.google.com/github/scgupta/ml4devs-notebooks/blob/master/speech/asr/python_speech_recognition_notebook.ipynb", "anchor_text": "Colab", "paragraph_index": 17}, {"url": "https://cloud.google.com/speech-to-text/docs", "anchor_text": "speech-to-text", "paragraph_index": 22}, {"url": "https://cloud.google.com/speech-to-text/docs/reference/libraries", "anchor_text": "libraries", "paragraph_index": 22}, {"url": "https://developers.google.com/accounts/docs/application-default-credentials", "anchor_text": "Google Cloud Credentials", "paragraph_index": 23}, {"url": "https://azure.microsoft.com/en-in/services/cognitive-services/speech-services/", "anchor_text": "Speech Services", "paragraph_index": 28}, {"url": "https://azure.microsoft.com/en-in/services/cognitive-services/speech-to-text/", "anchor_text": "Speech to Text", "paragraph_index": 28}, {"url": "https://portal.azure.com/", "anchor_text": "Microsoft Azure portal", "paragraph_index": 29}, {"url": "https://azure.microsoft.com/en-in/free/ai/", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://www.ibm.com/in-en/cloud/watson-speech-to-text", "anchor_text": "Watson Speech to Text", "paragraph_index": 34}, {"url": "https://cloud.ibm.com/apidocs/speech-to-text/speech-to-text?code=python", "anchor_text": "Python", "paragraph_index": 34}, {"url": "https://cloud.ibm.com/docs/services/text-to-speech?topic=text-to-speech-gettingStarted", "anchor_text": "sign up/in", "paragraph_index": 35}, {"url": "https://aws.amazon.com/transcribe/", "anchor_text": "Amazon Transcribe", "paragraph_index": 38}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/getting-started.html", "anchor_text": "speech-to-text", "paragraph_index": 38}, {"url": "https://aws.amazon.com/transcribe/resources/", "anchor_text": "libraries", "paragraph_index": 38}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/getting-started-python.html", "anchor_text": "batch", "paragraph_index": 38}, {"url": "https://docs.aws.amazon.com/cli/latest/reference/transcribe/start-transcription-job.html", "anchor_text": "command line", "paragraph_index": 38}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/websocket.html", "anchor_text": "WebSocket", "paragraph_index": 38}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/how-streaming.html", "anchor_text": "HTTP/2", "paragraph_index": 38}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/getting-started-streaming.html", "anchor_text": "example with AWS Java SDK", "paragraph_index": 38}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html", "anchor_text": "low-level event stream encoding", "paragraph_index": 38}, {"url": "https://www.nuance.com/dragon.html", "anchor_text": "commercial speech recognition products", "paragraph_index": 40}, {"url": "https://nuancedev.github.io/samples/http/python/", "anchor_text": "Python bindings", "paragraph_index": 40}, {"url": "https://github.com/NuanceDev/ndev-python-http-cli/blob/master/ndev/asr.py", "anchor_text": "code sample", "paragraph_index": 40}, {"url": "https://cmusphinx.github.io/", "anchor_text": "CMUSphinx", "paragraph_index": 42}, {"url": "https://github.com/cmusphinx/pocketsphinx-python", "anchor_text": "PocketSphinx", "paragraph_index": 42}, {"url": "http://www.swig.org/", "anchor_text": "swig", "paragraph_index": 43}, {"url": "https://hacks.mozilla.org/2019/12/deepspeech-0-6-mozillas-speech-to-text-engine/", "anchor_text": "DeepSpeech 0.6", "paragraph_index": 51}, {"url": "https://github.com/mozilla/DeepSpeech/releases/tag/v0.6.0", "anchor_text": "APIs", "paragraph_index": 51}, {"url": "https://deepspeech.readthedocs.io/en/v0.6.0/Python-API.html", "anchor_text": "Python", "paragraph_index": 51}, {"url": "https://kaldi-asr.org/doc/about.html", "anchor_text": "Kaldi", "paragraph_index": 58}, {"url": "https://pykaldi.github.io/", "anchor_text": "PyKaldi", "paragraph_index": 59}, {"url": "https://pykaldi.github.io/api/kaldi.asr.html", "anchor_text": "Python bindings", "paragraph_index": 59}, {"url": "https://github.com/pykaldi/pykaldi", "anchor_text": "README", "paragraph_index": 59}, {"url": "https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/", "anchor_text": "wav2letter@anywhere", "paragraph_index": 61}, {"url": "https://github.com/facebookresearch/wav2letter/wiki/Python-bindings", "anchor_text": "Python bindings", "paragraph_index": 61}, {"url": "https://github.com/facebookresearch/wav2letter/wiki/Inference-Framework", "anchor_text": "inference framework", "paragraph_index": 61}, {"url": "https://github.com/facebookresearch/wav2letter/", "anchor_text": "source", "paragraph_index": 62}, {"url": "https://pypi.org/project/SpeechRecognition/", "anchor_text": "SpeechRecognition", "paragraph_index": 63}, {"url": "https://github.com/Uberi/speech_recognition", "anchor_text": "CMU Sphinx, Google, Microsoft, IBM, Houndify, and Wit", "paragraph_index": 64}, {"url": "https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py", "anchor_text": "this example", "paragraph_index": 65}, {"url": "https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py", "anchor_text": "batch", "paragraph_index": 66}, {"url": "https://github.com/Uberi/speech_recognition/blob/master/examples/background_listening.py", "anchor_text": "background", "paragraph_index": 66}, {"url": "https://www.ml4devs.com/articles/how-to-build-python-transcriber-using-mozilla-deepspeech/", "anchor_text": "How to Build Python Transcriber Using Mozilla DeepSpeech", "paragraph_index": 67}, {"url": "https://www.ML4Devs.com", "anchor_text": "https://www.ML4Devs.com", "paragraph_index": 69}], "all_paragraphs": ["Speech recognition technologies have been evolving rapidly for the last couple of years, and are transitioning from the realm of science to engineering. With the growing popularity of voice assistants like Alexa, Siri, and Google Assistant, several apps (e.g., YouTube, Gana, Paytm Travel, My Jio) are beginning to have functionalities controlled by voice. At Slang Labs, we are building a platform for programmers to easily augment existing apps with voice experiences.", "Automatic Speech Recognition (ASR) is the necessary first step in processing voice. In ASR, an audio file or speech spoken to a microphone is processed and converted to text, therefore it is also known as Speech-to-Text (STT). Then this text is fed to a Natural Language Processing/Understanding (NLP/NLU) to understand and extract key information (such as intentions, sentiments), and then appropriate action is taken. There are also stand-alone applications of ASR, e.g. transcribing dictation, or producing real-time subtitles for videos.", "We are interested in ASR and NLU in general, and their efficacy in the voice-to-action loop in apps in particular. Our Android and Web SDKs provide simple APIs suitable from the perspective of app programmers, while the Slang platform handles the burden of the complexity of stitching together ASR, NLU, and Text-to-Speech (TTS). But, naturally, we are curious about the state of art in ASR, NLU, and TTS even though we do not expose these parts of our tech stack as separate SaaS offerings. This exploration of existing ASR solutions is the result of that curiosity.", "There are two possibilities: make calls to Speech-to-Text SaaS on the cloud or host one of the ASR software packages in your application.", "Service is the easiest way to start. You have to sigh-up for a SaaS and get key/credentials. Then you are all set to use it in your code, either through HTTP endpoints or libraries in the programming languages of your choice. However, for reasonably large usage, it typically costs more money.", "Software packages offer you full control as you are hosting it, and also the possibility of creating smaller models tailored for your application, and deploying it on-device/edge without needing network connectivity. But it requires expertise and upfront efforts to train and deploy the models.", "It is a reversible choice. For example, you can start with a cloud service, and if needed, move to your own deployment of a software package; and vice versa. You can design your code to limit the blast radius of such reversal, as well as in case if you migrate to another SaaS or software package.", "You need to determine whether your application requires batch ASR or streaming ASR.", "Batch: If you have audio recordings that need to transcribe it offline, then batch processing will suffice as well more economical. In batch API, an audio file is passed as a parameter, and speech-to-text transcribing is done in one shot.", "Streaming: If you need to process speech in real-time (e.g. in voice-controlled applications, video subtitles), you will need a streaming API. In the case of streaming API, it is repeatedly invoked with available chunks of the audio buffer. It may send interim results, but the final result is available at the end.", "All services and software packages have batch APIs, but some lack streaming APIs at the moment. So if you have a streaming application, that eliminates some of the choices.", "Most speech services provide libraries in popular programming languages. In the worst case, you can always use HTTP endpoints. The same is true for speech packages, these come with bindings in various programming languages. In the worst case, you can create bindings yourself. So there is no constraint of using Python.", "I am choosing Python for this article because most speech cloud services and ASR software packages have Python libraries. Also, you can run code snippets of the article using its companion Colab notebook in the browser, without requiring anything to be installed on your computer.", "One common use case is to collect audio from the microphone and pass on the buffer (batch or streaming) to the speech recognition API. Invariably, in such transcribers, the microphone is accessed through PyAudio, which is implemented over PortAudio. But since the microphone is not accessible on Colab, we simplify it. We will use a complete audio file to examine batch API. And for streaming API, we will break an audio file into chunks and simulate stream.", "Following services and software packages are covered.", "Code samples are not provided for Amazon Transcribe, Nuance, Kaldi, and Facebook wav2letter due to some peculiarity or limitation (listed in their respective sections). Instead, links to code samples and resources are given.", "The next section has the common utility functions and test cases. The last section covers the Python SpeechRecognition package that provides an abstraction over batch API of several could services and software packages.", "If you want to have an overview of all services and software packages, then please open the Colab, and execute the code as you read this post. If you are interested only in a specific service or package, directly jump to that section. But in either case, do play with the code in Colab to explore it better.", "Download the audio files we will use for testing Speech Recognition services and software packages:", "It has three audio files. Define test cases with needed metadata:", "Also, write some utility functions. The read_wav_file() takes the path to the audio file, and returns the buffer bytes and sample rate:", "The simulate_stream() is useful for simulating steam to try streaming APIs. Usually, there will be an audio source like a microphone. At regular intervals, the microphone will generate a speech chunk, which has to be passed to the streaming API. The simulate_stream() function helps to avoid all that complexity and to focus on the APIs. It takes an audio buffer and batch size, and generates chunks of that size. Notice the yield buf statement in the following:", "Google has speech-to-text as one of the Google Cloud services. It has libraries in C#, Go, Java, JavaScript, PHP, Python, and Ruby. It supports both batch and stream modes.", "You will need your Google Cloud Credentials. You will need to setup GOOGLE_APPLICATION_CREDENTIALS environment variable pointing to the cred file:", "Using batch speech-to-text-API is straightforward. You need to create a SpeechClient, create a config with audio metadata and call recognize() method of the speech client.", "When you run this, you will see the text of each of the audio test files in the output:", "Google\u2019s streaming API is also quite simple. For processing audio stream, you can repeatedly call the streaming API with the available chunk of audio, and it will return you interim results:", "In the output, you can see that the result improves as more audio is fed:", "Microsoft Azure Cognitive Services is a family of AI services and cognitive APIs. The Speech Services include Speech to Text, Text to Speech, Speech Translation services.", "You can enable Speech service and find credentials for your account at Microsoft Azure portal. You can open a free account here. Service credentials:", "Azure\u2019s batch API is simple too. It takes a config and audio input, and returns the text:", "The output will be as following:", "Azure has several kinds of streaming API. By creating different types of audio sources, one can either push the audio chunks, or pass a callback to Azure to pull the audio chunk. It fires several types of speech recognition events to hookup callbacks. Here is how you can wire a push audio stream with the audio stream generator:", "The output for the first test case looks like:", "IBM Watson Speech to Text is an ASR service with .NET, Go, JavaScript, Python, Ruby, Swift, and Unity API libraries, as well as HTTP endpoints. It has rich documentation.", "You will need to sign up/in, and get API key credential and service URL, and fill it below.", "The batch API is predictably simple:", "Watson\u2019s streaming API works over WebSocket, and takes a little bit of work to set it all up. It has the following steps:", "Amazon Transcribe is a speech-to-text AWS cloud service with libraries in C#, Go, Java, JavaScript, PHP, Python, and Ruby. It has a batch speech-to-text API (also available as command line), but it requires the audio file to be either in an S3 bucket, or be available over HTTP. It also has a streaming API on WebSocket and HTTP/2. Here is an example with AWS Java SDK, but no Python bindings (of course, a Python socket library can be used, but it will require getting into low-level event stream encoding).", "Amazon Transcribe Python APIs currently do not facilitate use cases covered in this article, and therefore code samples are not included here.", "Nuance is most probably the oldest commercial speech recognition products, even customized for various domains and industries. They do have Python bindings for a speech recognition service. Here is a code sample in their GitHub repo.", "I could not figure out a way to create a developer account. I hope there is a way to get a limited period of free trial credits similar to other products, and get the credentials needed to access the services.", "CMUSphinx has been around for quite some time, and has been adapting to advancements in ASR technologies. PocketSphinx is a speech-to-text decoder Python package.", "First, install swig. On macOS, you can install using brew:", "On Linux, you can use apt-get:", "And then install pocketsphinx using pip:", "Whether you use batch or streaming API, you will require a decoder object:", "Batch API is expectedly simple, just a couple of lines of code:", "And you will see the now familiar output:", "Notice the errors in transcription. With more training data, it typically improves.", "Streaming APIs are also quite simple, but there is no hook to get intermediate results:", "Mozilla released DeepSpeech 0.6 software package in December 2019 with APIs in C, Java, .NET, Python, and JavaScript, including support for TensorFlow Lite models for use on edge devices.", "You can install DeepSpeech with pip (make it deepspeech-gpu==0.6.0 if you want to use GPU in Colab runtime or on your machine):", "Download and unzip the models (this will take a while):", "Test that it all works. Examine the output of the last three commands, and you will see results \u201cexperience proof less\u201d, \u201cwhy should one halt on the way\u201d, and \u201cyour power is sufficient i said\u201d respectively. You are all set.", "The first step is to read the model files and create a DeepSpeech model object.", "It takes just a couple of line of code for doing batch speech-to-text:", "DeepSpeech streaming API requires creating a stream context and use it repeatedly to feed chunks of audio:", "Kaldi is a very popular software toolkit for speech recognition among the research community. It is designed to experiment with different research ideas and possibilities. It has a rich collection of various possible techniques and alternatives. The learning curve is steeper compared to other alternatives discussed in the code lab.", "PyKaldi provides Python bindings. Please do take a look at the README of their GitHub repo.", "There is no pre-build PyPI ready-to-use package, and you have to build it either from source or from Conda. Neither options suit the Colab environment.", "Facebook released wav2letter@anywhere in January 2020. It boasts a fully convolutional (CNN) acoustic model instead of a recurrent neural network (RNN) that is used by other solutions. It is very promising, include for use in edge devices. It has Python bindings for its inference framework.", "Like Kaldi, this also does not provide a PyPI package, and needs to build and install from the source.", "The SpeechRecognition package provides a nice abstraction over several solutions. We already explored using Google service and CMU Sphinxpackage. Now we will use these through SpeechRecognition package APIs. It can be installed using pip:", "SpeechRecognition has only batch API. The first step to create an audio record, either from a file or from a microphone, and the second step is to call recognize_<speech engine name> function. It currently has APIs for CMU Sphinx, Google, Microsoft, IBM, Houndify, and Wit. Let's checkout using one cloud service (Google) and one software package (Sphinx) through SpeechRecognition abstraction.", "For other speech recognition providers, you will need to create API credentials, which you have to pass to recognize_<speech engine name> function, you can check out this example.", "It also has a nice abstraction for Microphone, implemented over PyAudio/PortAudio. Check out examples to capture input from the microphone in batch and continuously in the background.", "Want to write a Python transcriber that converts microphone input to text? Check this out: How to Build Python Transcriber Using Mozilla DeepSpeech.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Cofounder @SlangLabs. Ex Amazon, Microsoft Research. I learn, do, and write about Machine Learning in production. https://www.ML4Devs.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa64851ad29b3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://scgupta.medium.com/?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": ""}, {"url": "https://scgupta.medium.com/?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "Satish Chandra Gupta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2e6816bf92a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&user=Satish+Chandra+Gupta&userId=d2e6816bf92a&source=post_page-d2e6816bf92a----a64851ad29b3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa64851ad29b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa64851ad29b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://slanglabs.in", "anchor_text": "Slang Labs"}, {"url": "https://medium.com/slanglabs/voice-in-apps-youtube-25bcc288ac4c", "anchor_text": "YouTube"}, {"url": "https://medium.com/slanglabs/voice-in-apps-gaana-1f6e2d8b026b", "anchor_text": "Gana"}, {"url": "https://medium.com/slanglabs/voice-in-apps-paytm-travel-5bee6aea76dc", "anchor_text": "Paytm Travel"}, {"url": "https://medium.com/slanglabs/voice-in-apps-my-jio-5dc8f2e298d", "anchor_text": "My Jio"}, {"url": "https://medium.com/slanglabs/what-is-voice-augmented-experience-1003a28b6e5", "anchor_text": "augment existing apps with voice experiences"}, {"url": "https://docs.slanglabs.in/", "anchor_text": "Android and Web SDKs"}, {"url": "https://colab.research.google.com/github/scgupta/ml4devs-notebooks/blob/master/speech/asr/python_speech_recognition_notebook.ipynb", "anchor_text": "Colab notebook"}, {"url": "https://people.csail.mit.edu/hubert/pyaudio/", "anchor_text": "PyAudio"}, {"url": "http://www.portaudio.com/", "anchor_text": "PortAudio"}, {"url": "https://colab.research.google.com/github/scgupta/ml4devs-notebooks/blob/master/speech/asr/python_speech_recognition_notebook.ipynb", "anchor_text": "Colab"}, {"url": "https://cloud.google.com/speech-to-text/docs", "anchor_text": "speech-to-text"}, {"url": "https://cloud.google.com/speech-to-text/docs/reference/libraries", "anchor_text": "libraries"}, {"url": "https://developers.google.com/accounts/docs/application-default-credentials", "anchor_text": "Google Cloud Credentials"}, {"url": "https://azure.microsoft.com/en-in/services/cognitive-services/speech-services/", "anchor_text": "Speech Services"}, {"url": "https://azure.microsoft.com/en-in/services/cognitive-services/speech-to-text/", "anchor_text": "Speech to Text"}, {"url": "https://portal.azure.com/", "anchor_text": "Microsoft Azure portal"}, {"url": "https://azure.microsoft.com/en-in/free/ai/", "anchor_text": "here"}, {"url": "https://www.ibm.com/in-en/cloud/watson-speech-to-text", "anchor_text": "Watson Speech to Text"}, {"url": "https://cloud.ibm.com/apidocs/speech-to-text/speech-to-text?code=python", "anchor_text": "Python"}, {"url": "https://cloud.ibm.com/docs/services/text-to-speech?topic=text-to-speech-gettingStarted", "anchor_text": "sign up/in"}, {"url": "https://aws.amazon.com/transcribe/", "anchor_text": "Amazon Transcribe"}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/getting-started.html", "anchor_text": "speech-to-text"}, {"url": "https://aws.amazon.com/transcribe/resources/", "anchor_text": "libraries"}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/getting-started-python.html", "anchor_text": "batch"}, {"url": "https://docs.aws.amazon.com/cli/latest/reference/transcribe/start-transcription-job.html", "anchor_text": "command line"}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/websocket.html", "anchor_text": "WebSocket"}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/how-streaming.html", "anchor_text": "HTTP/2"}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/getting-started-streaming.html", "anchor_text": "example with AWS Java SDK"}, {"url": "https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html", "anchor_text": "low-level event stream encoding"}, {"url": "https://www.nuance.com/dragon.html", "anchor_text": "commercial speech recognition products"}, {"url": "https://nuancedev.github.io/samples/http/python/", "anchor_text": "Python bindings"}, {"url": "https://github.com/NuanceDev/ndev-python-http-cli/blob/master/ndev/asr.py", "anchor_text": "code sample"}, {"url": "https://cmusphinx.github.io/", "anchor_text": "CMUSphinx"}, {"url": "https://github.com/cmusphinx/pocketsphinx-python", "anchor_text": "PocketSphinx"}, {"url": "http://www.swig.org/", "anchor_text": "swig"}, {"url": "https://hacks.mozilla.org/2019/12/deepspeech-0-6-mozillas-speech-to-text-engine/", "anchor_text": "DeepSpeech 0.6"}, {"url": "https://github.com/mozilla/DeepSpeech/releases/tag/v0.6.0", "anchor_text": "APIs"}, {"url": "https://deepspeech.readthedocs.io/en/v0.6.0/Python-API.html", "anchor_text": "Python"}, {"url": "https://kaldi-asr.org/doc/about.html", "anchor_text": "Kaldi"}, {"url": "https://pykaldi.github.io/", "anchor_text": "PyKaldi"}, {"url": "https://pykaldi.github.io/api/kaldi.asr.html", "anchor_text": "Python bindings"}, {"url": "https://github.com/pykaldi/pykaldi", "anchor_text": "README"}, {"url": "https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/", "anchor_text": "wav2letter@anywhere"}, {"url": "https://github.com/facebookresearch/wav2letter/wiki/Python-bindings", "anchor_text": "Python bindings"}, {"url": "https://github.com/facebookresearch/wav2letter/wiki/Inference-Framework", "anchor_text": "inference framework"}, {"url": "https://github.com/facebookresearch/wav2letter/", "anchor_text": "source"}, {"url": "https://pypi.org/project/SpeechRecognition/", "anchor_text": "SpeechRecognition"}, {"url": "https://github.com/Uberi/speech_recognition", "anchor_text": "CMU Sphinx, Google, Microsoft, IBM, Houndify, and Wit"}, {"url": "https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py", "anchor_text": "this example"}, {"url": "https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py", "anchor_text": "batch"}, {"url": "https://github.com/Uberi/speech_recognition/blob/master/examples/background_listening.py", "anchor_text": "background"}, {"url": "https://www.ml4devs.com/articles/how-to-build-python-transcriber-using-mozilla-deepspeech/", "anchor_text": "How to Build Python Transcriber Using Mozilla DeepSpeech"}, {"url": "https://www.ml4devs.com/newsletter/", "anchor_text": ""}, {"url": "https://twitter.com/intent/follow?user_id=29633907", "anchor_text": ""}, {"url": "https://www.linkedin.com/in/scgupta/", "anchor_text": ""}, {"url": "https://medium.com/tag/voice-assistant?source=post_page-----a64851ad29b3---------------voice_assistant-----------------", "anchor_text": "Voice Assistant"}, {"url": "https://medium.com/tag/speech-recognition?source=post_page-----a64851ad29b3---------------speech_recognition-----------------", "anchor_text": "Speech Recognition"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a64851ad29b3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a64851ad29b3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----a64851ad29b3---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa64851ad29b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&user=Satish+Chandra+Gupta&userId=d2e6816bf92a&source=-----a64851ad29b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa64851ad29b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&user=Satish+Chandra+Gupta&userId=d2e6816bf92a&source=-----a64851ad29b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa64851ad29b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa64851ad29b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a64851ad29b3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a64851ad29b3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a64851ad29b3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a64851ad29b3--------------------------------", "anchor_text": ""}, {"url": "https://scgupta.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://scgupta.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Satish Chandra Gupta"}, {"url": "https://scgupta.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.3K Followers"}, {"url": "https://www.ML4Devs.com", "anchor_text": "https://www.ML4Devs.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2e6816bf92a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&user=Satish+Chandra+Gupta&userId=d2e6816bf92a&source=post_page-d2e6816bf92a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe35e99930c83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-speech-recognition-in-python-programs-a64851ad29b3&newsletterV3=d2e6816bf92a&newsletterV3Id=e35e99930c83&user=Satish+Chandra+Gupta&userId=d2e6816bf92a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}