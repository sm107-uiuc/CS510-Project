{"url": "https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6", "time": 1682994626.649652, "path": "towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6/", "webpage": {"metadata": {"title": "Various Optimization Algorithms For Training Neural Network | by Sanket Doshi | Towards Data Science", "h1": "Various Optimization Algorithms For Training Neural Network", "description": "Optimizers are an important aspect of a network convergence. Based on the optimizers used the time taken by the network may change drastically."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1412.6980.pdf", "anchor_text": "Adam", "paragraph_index": 18}], "all_paragraphs": ["Many people may be using optimizers while training the neural network without knowing that the method is known as optimization. Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.", "How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.", "We\u2019ll learn about different types of optimizers and their advantages:", "Gradient Descent is the most basic but most used optimization algorithm. It\u2019s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm.", "Gradient descent is a first-order optimization algorithm which is dependent on the first order derivative of a loss function. It calculates that which way the weights should be altered so that the function can reach a minima. Through backpropagation, the loss is transferred from one layer to another and the model\u2019s parameters also known as weights are modified depending on the losses so that the loss can be minimized.", "It\u2019s a variant of Gradient Descent. It tries to update the model\u2019s parameters more frequently. In this, the model parameters are altered after computation of loss on each training example. So, if the dataset contains 1000 rows SGD will update the model parameters 1000 times in one cycle of dataset instead of one time as in Gradient Descent.", "As the model parameters are frequently updated parameters have high variance and fluctuations in loss functions at different intensities.", "It\u2019s best among all the variations of gradient descent algorithms. It is an improvement on both SGD and standard gradient descent. It updates the model parameters after every batch. So, the dataset is divided into various batches and after every batch, the parameters are updated.", "All types of Gradient Descent have some challenges:", "Momentum was invented for reducing high variance in SGD and softens the convergence. It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by \u2018\u03b3\u2019.", "The momentum term \u03b3 is usually set to 0.9 or a similar value.", "Momentum may be a good method but if the momentum is too high the algorithm may miss the local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was developed. It is a look ahead method. We know we\u2019ll be using \u03b3V(t\u22121) for modifying the weights so, \u03b8\u2212\u03b3V(t\u22121) approximately tells us the future location. Now, we\u2019ll calculate the cost based on this future parameter rather than the current one.", "One of the disadvantages of all the optimizers explained is that the learning rate is constant for all parameters and for each cycle. This optimizer changes the learning rate. It changes the learning rate \u2018\u03b7\u2019 for each parameter and at every time step \u2018t\u2019. It\u2019s a type second order optimization algorithm. It works on the derivative of an error function.", "\u03b7 is a learning rate which is modified for given parameter \u03b8(i) at a given time based on previous gradients calculated for given parameter \u03b8(i).", "We store the sum of the squares of the gradients w.r.t. \u03b8(i) up to time step t, while \u03f5 is a smoothing term that avoids division by zero (usually on the order of 1e\u22128). Interestingly, without the square root operation, the algorithm performs much worse.", "It makes big updates for less frequent parameters and a small step for frequent parameters.", "It is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it. Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w. In this exponentially moving average is used rather than the sum of all the gradients.", "We set \u03b3 to a similar value as the momentum term, around 0.9.", "Adam (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don\u2019t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients M(t).", "M(t) and V(t) are values of the first moment which is the Mean and the second moment which is the uncentered variance of the gradients respectively.", "Adam is the best optimizers. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.", "For sparse data use the optimizers with dynamic learning rate.", "If, want to use gradient descent algorithm than min-batch gradient descent is the best option.", "I hope you guys liked the article and were able to give you a good intuition towards the different behaviors of different Optimization Algorithms.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Currently working as a Backend Developer. Exploring how to make machines smarter than me."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F59450d71caf6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----59450d71caf6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59450d71caf6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sdoshi579?source=post_page-----59450d71caf6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=post_page-----59450d71caf6--------------------------------", "anchor_text": "Sanket Doshi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a8e3136747c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&user=Sanket+Doshi&userId=4a8e3136747c&source=post_page-4a8e3136747c----59450d71caf6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1412.6980.pdf", "anchor_text": "Adam"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----59450d71caf6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----59450d71caf6---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----59450d71caf6---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----59450d71caf6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/adam?source=post_page-----59450d71caf6---------------adam-----------------", "anchor_text": "Adam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&user=Sanket+Doshi&userId=4a8e3136747c&source=-----59450d71caf6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&user=Sanket+Doshi&userId=4a8e3136747c&source=-----59450d71caf6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59450d71caf6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----59450d71caf6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----59450d71caf6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----59450d71caf6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----59450d71caf6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----59450d71caf6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----59450d71caf6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----59450d71caf6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----59450d71caf6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----59450d71caf6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sanket Doshi"}, {"url": "https://medium.com/@sdoshi579/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "577 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a8e3136747c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&user=Sanket+Doshi&userId=4a8e3136747c&source=post_page-4a8e3136747c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F157262d7e358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&newsletterV3=4a8e3136747c&newsletterV3Id=157262d7e358&user=Sanket+Doshi&userId=4a8e3136747c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}