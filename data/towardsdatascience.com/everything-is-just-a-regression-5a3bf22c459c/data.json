{"url": "https://towardsdatascience.com/everything-is-just-a-regression-5a3bf22c459c", "time": 1683002839.6986759, "path": "towardsdatascience.com/everything-is-just-a-regression-5a3bf22c459c/", "webpage": {"metadata": {"title": "Everything is a Regression. In search of unifying paradigms in\u2026 | by Kevin Hannay | Towards Data Science", "h1": "Everything is a Regression", "description": "In college my first love was biology, but eventually I was turned off by the classes because of their emphasis on memorization. I was drawn to mathematics as a study because I didn\u2019t have to memorize\u2026"}, "outgoing_paragraph_urls": [{"url": "http://khannay.com/data/NBA_Draft_Data.csv", "anchor_text": "NBA draft picks", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/F-test", "anchor_text": "F-test", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc", "anchor_text": "logistic regression", "paragraph_index": 58}, {"url": "https://en.wikipedia.org/wiki/Likelihood-ratio_test", "anchor_text": "ikelihood ratio test (LRT)", "paragraph_index": 66}, {"url": "https://lindeloev.github.io/tests-as-linear/", "anchor_text": "Jonas Lindel\u00f8v\u2019s", "paragraph_index": 72}, {"url": "https://xcelab.net/rm/statistical-rethinking/", "anchor_text": "recommend Statistical Rethinking", "paragraph_index": 72}, {"url": "https://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/1441923225", "anchor_text": "All of Statistics", "paragraph_index": 72}, {"url": "https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?crid=123S4MQDKXJO6&keywords=gelman+regression&qid=1578605925&sprefix=Gelman+regr%2Cstripbooks%2C179&sr=8-1", "anchor_text": "Gelman and Hill", "paragraph_index": 72}], "all_paragraphs": ["In college my first love was biology, but eventually I was turned off by the classes because of their emphasis on memorization. I was drawn to mathematics as a study because I didn\u2019t have to memorize anything. In math, the main point is how we know something. If you can\u2019t prove a result yourself, then you don\u2019t understand it.", "For some reason statistics is still often taught by listing out tests to apply in a variety of scenarios. Complicated flow charts with ever more specific statistical tests.", "This leads to confusion and misuse of the statistical tests. It is okay though, because I have some good news. Basic statistics can be explained in three parts:", "For people who have a background in data science and/or machine learning this is great news. Since regression is the starting place for most studies of machine learning it means you have already been doing statistical hypothesis testing- maybe without knowing it. In this article I will focus on the last step of this by giving some examples of how common statistical tests can be thought of in terms of regression.", "In linear regression we build a linear model for the relationship between our feature x and our response variable y.", "The model is statistical because epsilon is assumed to be stochastic.", "This is a mathematical model. It is a way for us to specify precisely what we are assuming about the data set. Whenever we are building a model it is useful to keep the famous quote in mind:", "All models are wrong, but some are useful. \u2014 George Box", "Models are an abstraction and simplification of the complexity of the real world. Since they are a simplification they are always false, but they just might capture some important ideas.", "As an example let\u2019s consider a data set with NBA draft picks (you can download this data set from my website) from 1991\u20132018. We might look at the relationship between the pick number and the career points per game average. This is shown in the below scatter plot along with a best fit regression line to the data.", "The best fit regression line here is given by:", "The best fit for linear regression is usually found by minimizing the squared residual terms.", "I won\u2019t go into how the actual minimization of the cost function works here. It will suffice to say that we can very quickly find the global minimum of this cost function using some tricks from linear algebra. This gives us some best fit parameters for the slope and intercept.", "We can also perform regression for features which are categorical in nature. The trick here is do a so-called one hot encoding of the categorical variables. The idea is to convert out categorical levels into indicator variables (\u03b4). These \u03b4\u2019s will be one if the input belongs to its designated level and zero otherwise.", "For example, for the NBA Draft data lets split the pick number column into Lottery picks (\u226414) and non lottery picks (NL).", "Then we can look for a difference in the mean points per game for these two groups of players.", "Here is a plot of this data with the regression line shown.", "Now the slope term \u03b2 gives the difference in the mean points per game between the non lottery picks (x=0) and lottery picks (x=1). The slight upward trend here indicates that we have some evidence that the lottery picks tend to average more points over the course of their careers.", "Of course, this trend is based on a random sampling of players so this upward trend might just disappear if we collected a new sample. To account for this variation due to random sampling we can form a confidence interval for the slope.", "For the lottery pick example above we find the following 95% confidence intervals for the parameters:", "This tells us that the mean difference in the career PPG for lottery picks minus non lottery picks is likely to be between 3.48 and 4.78. This gives us an idea of both the effect direction (positive) and the magnitude of the effect (3.5\u20134.8 points).", "We can see this interval for the slope does not contain zero. This tells us that we are unlikely to see a reversal of this trend if we re-sample the data. If we increase the confidence level from 95% to say 99% we will see that the interval width will grow. To commit fewer errors we need a larger range of values.", "We could now play the game of growing the (100-p)% confidence interval until the interval just hits 0 on the left side. This value is called the p-value.", "P-values give P(D|H\u2080) the probability of observing the data given that the Null hypothesis is true (i.e. H\u2080 : \u03b2=0).", "In some cases we might only care about the effect direction and not the magnitude of the effect. This falls into the domain of the statistical test called a two-sample t-test. In basic statistics classes we are taught to use a two sample t-test to evaluate data collected under two conditions for evidence of a difference in mean values. This is the classic control versus experimental group.", "To perform this test in R, I am first going to make a smaller data set from our rather large draft data set. The below commands just generate a random subset of 100 players for us to compare. I also create a lottery column in the data set for good measure.", "Now we are ready to run our t-test in R.", "Now notice the p-value in our results. The relatively small value here indicates that the data is unlikely given the null hypothesis.", "Now lets do this as a linear regression with a categorical feature.", "Here I will show the results from a summary command in R for our simple linear regression.", "I have highlighted the important line. Compare this with the results we found using the two sample t test. The t value (up to the sign) and p-value are the same!", "Also, while we are looking at the regression summary in R, notice that the same p-value is repeated in the last line of the summary. This is the result of an F-test being run on our entire regression model.", "This test tells us if any of our features in the model depart from zero in a statistically significant way. Our course, in this simple case we have only one feature so the F-test on the model, and the t-test on the Lottery feature are exactly the same thing. In a multiple regression (more than one feature) these will be different.", "Let\u2019s say we want to evaluate the effect of the players position on their career scoring average. First, we should clean-up the levels of the position column in our data set.", "We can then make a plot of the position are career scoring by position:", "We might want to know if the means of these groups are truly different or if the differences observed could also be explained by sampling error. In classical statistics we would say we want to conduct a one-way ANOVA (analysis of variance). This can be done very easily in R:", "Now we can also approach this as a regression. Our regression model has the form:", "The alpha intercept value now tells us the mean for the centers and the two slopes tell us the points relative to the center\u2019s value.", "This is all done under the hood in R. Here is the output:", "Compare the last line (F-test) in the regression output with the ANOVA output. Once again we see the same results!", "This tells us a one way ANOVA is just a linear regression model with a categorical feature-with more than two levels. The test used has a null hypothesis that all slopes are zero.", "In a two way ANOVA we use two categorical features to predict a continuous response variable.", "Let\u2019s do this with our draft data set using the Tm (Drafting Team) and Pos (Position) columns. A two-way ANOVA needs more data to fit the model, so we will use the full data set instead of our trimmed down one. To begin I ran the two commands below to clean-up the two categorical features levels.", "Our linear model in this case takes the form:", "The first sum is over the dummy coded team variables and the second the position categories. All of this is nicely done under the hood in R. To perform our analysis we can build our linear model using:", "The only change here is that we should use the anova() command on our model instead of the usual summary(). This will show the following results:", "The two lines tell us that we have evidence that the position matters for career points, but we don\u2019t have sufficient evidence to conclude that the team matters. You can probably see how to perform a N-way ANOVA using multiple regression now.", "If we add a continuous feature to our regression this now goes by yet another name (ANCOVA= analysis of covariance). The motivation here might be that we have seen that position matters to the PPG for NBA players, however this might just be because some positions play more minutes than others.", "We can control for the effect of the minutes played by including this as a feature in our model. To start off I am going to re-scale the minutes played column (MP) such that the mean is zero and the standard deviation is set to one. The motivation for this is that otherwise our intercept in the linear model is truly useless-as it would give the career PPG for a center who averaged 0 minutes per game. Now the intercept will have the interpretation of the mean PPG for a center who played average minutes per game. Maybe a mouthful but much more meaningful.", "Here is a plot the relationship between minutes played and points per game with the positions shown as colors.", "Now we let\u2019s build the linear model:", "The first line tells us that the position has a statistically significant effect on the career PPG even after controlling for the minutes played.", "We can also formulate other basic statistical procedures as regressions. However, we will need to make use of so-called generalized linear models (GLM) to go much further.", "To start I am going to generate a fake data set for us to work with. The below commands make a R data frame which stores the free throw outcomes and player name for an imaginary contest where players A and B each shoot 100 free throws.", "Player B only shoots 77% and player A shoots 80%, although this is subject to sampling error. We can see the results of the random draw using the table command in R:", "Here we might want to perform a two-sample proportion test to test the hypothesis that the percentages differ between the two players.", "This can be done in R using the prop.test command.", "Now for the regression approach. As mentioned above since our response variable is no longer continuous we need to adjust our regression to deal with binary outputs. Really, we want our model to produce a probability p\u1d62", "This can be done using logistic regression. Our usual regression takes the form:", "In logistic regression our outputs Y\u1d62 should give the probability that Y\u1d62 takes the value 1 given the X\u2c7c features. As written we have a problem as the right hand side of our model above will produce values in \u211d=(-\u221e, \u221e) while the left hand side should live in [0,1].", "Therefore to use a model like this we need to transform our outputs from [0,1] to the whole real line \u211d. The logit function is useful for this purpose as it maps logit: [0,1] \u2192 \u211d.", "Thus, we can use our multiple regression techniques if treat our outputs as being produced by a logit function Y\u1d62=logit(p\u1d62). This is the basic idea of logistic regression:", "Finally, we can invert the logit function to get the actual probability:", "A logistic regression is one example in a family of techniques called generalized linear models (GLM). GLMs involve a linear predictor function \u03b1+ \u2211 \u03b2\u2c7c X\u1d62\u2c7c and a link function g() which maps the linear predictor to the response variable.", "This very easy to do in R. We just change from a lm() function to glm() and specify the link function we want to use.", "In the above case we actually fit two logistic regressions. The first is the model we actually want to build and the second is the equivalent of the null hypothesis for the two sample proportion test. By fitting only the intercept (~1 above) we are saying the percentage of made free throws must be the same for the two players.", "We then use the anova() function to compare our model with the null hypothesis model. I also specified that the anova should use a likelihood ratio test (LRT). Of course, as usual notice that the p-values are the same as the proportion test above.", "Also, this example shows us how to use regression models and the anova command to do a model selection. Really, we have been doing this all along as the null hypothesis has always been a model- we just haven\u2019t had to specify it before in the anova command. This also shows us how we might begin to compare our models against more complex null models.", "Model selection allows us to compare models and move beyond trivial null models.", "A super fancy diagram for the natural progression of a mathematical model is shown below:", "When we are forced to recognize the statistical tests as mathematical models it brings the assumptions to the forefront. The hope is that this keeps us in the first two stages of the model life cycle for a little bit longer. Also, this means we can stop trying to memorize a bunch of statistical tests.", "Stop memorizing and start building models", "If you want to keep learning about this topic I highly recommend you check out Jonas Lindel\u00f8v\u2019s site. For a textbook in this area I recommend Statistical Rethinking. For basic statistics reading I love All of Statistics. For some more advanced regression analysis I recommend taking a look at Gelman and Hill.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Applied Math PhD, Machine Learning Engineer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5a3bf22c459c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@moleculeboy24?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@moleculeboy24?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "Kevin Hannay"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aee0cbca1e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&user=Kevin+Hannay&userId=2aee0cbca1e1&source=post_page-2aee0cbca1e1----5a3bf22c459c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a3bf22c459c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a3bf22c459c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://khannay.com/StatsBook/introduction-to-sampling-distributions.html", "anchor_text": "Sampling"}, {"url": "http://khannay.com/StatsBook/confidence-intervals.html", "anchor_text": "Confidence Intervals"}, {"url": "http://khannay.com/StatsBook/introduction-to-linear-regression.html", "anchor_text": "Regression"}, {"url": "http://khannay.com/data/NBA_Draft_Data.csv", "anchor_text": "NBA draft picks"}, {"url": "https://en.wikipedia.org/wiki/F-test", "anchor_text": "F-test"}, {"url": "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc", "anchor_text": "logistic regression"}, {"url": "https://en.wikipedia.org/wiki/Likelihood-ratio_test", "anchor_text": "ikelihood ratio test (LRT)"}, {"url": "https://lindeloev.github.io/tests-as-linear/", "anchor_text": "Jonas Lindel\u00f8v\u2019s"}, {"url": "https://xcelab.net/rm/statistical-rethinking/", "anchor_text": "recommend Statistical Rethinking"}, {"url": "https://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/1441923225", "anchor_text": "All of Statistics"}, {"url": "https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?crid=123S4MQDKXJO6&keywords=gelman+regression&qid=1578605925&sprefix=Gelman+regr%2Cstripbooks%2C179&sr=8-1", "anchor_text": "Gelman and Hill"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5a3bf22c459c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5a3bf22c459c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----5a3bf22c459c---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/probability?source=post_page-----5a3bf22c459c---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/tag/r-programming?source=post_page-----5a3bf22c459c---------------r_programming-----------------", "anchor_text": "R Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a3bf22c459c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&user=Kevin+Hannay&userId=2aee0cbca1e1&source=-----5a3bf22c459c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a3bf22c459c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&user=Kevin+Hannay&userId=2aee0cbca1e1&source=-----5a3bf22c459c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a3bf22c459c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5a3bf22c459c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5a3bf22c459c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5a3bf22c459c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@moleculeboy24?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@moleculeboy24?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kevin Hannay"}, {"url": "https://medium.com/@moleculeboy24/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "301 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aee0cbca1e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&user=Kevin+Hannay&userId=2aee0cbca1e1&source=post_page-2aee0cbca1e1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffe6194563141&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feverything-is-just-a-regression-5a3bf22c459c&newsletterV3=2aee0cbca1e1&newsletterV3Id=fe6194563141&user=Kevin+Hannay&userId=2aee0cbca1e1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}