{"url": "https://towardsdatascience.com/can-a-neural-network-train-other-networks-cf371be516c6", "time": 1683014720.349074, "path": "towardsdatascience.com/can-a-neural-network-train-other-networks-cf371be516c6/", "webpage": {"metadata": {"title": "Can a neural network train other networks? | by Tivadar Danka | Towards Data Science", "h1": "Can a neural network train other networks?", "description": "If you have ever used a neural network to solve a complex problem, you know that they can be enormous in size, containing millions of parameters. For instance, the famous BERT model has about ~110\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.stateof.ai/", "anchor_text": "State of AI Report 2020", "paragraph_index": 1}, {"url": "https://www.twitter.com/nathanbenaich", "anchor_text": "Nathan Benaich", "paragraph_index": 1}, {"url": "https://www.twitter.com/soundboy", "anchor_text": "Ian Hogarth", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Logit", "anchor_text": "logits", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1802.06509", "anchor_text": "Sanjeev Arora, Nadav Cohen, and Elad Hazan in their paper On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Hinton et al.", "paragraph_index": 25}, {"url": "https://dl.acm.org/doi/10.1145/1150402.1150464", "anchor_text": "Cristian Bucilu\u01ce, Rich Caruana, and Alexandru Niculescu-Mizil in their 2006 paper titled Model compression", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Distilling the Knowledge in a Neural Network", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1711.09784", "anchor_text": "Distilling a Neural Network Into a Soft Decision Tree", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 33}, {"url": "https://cloud.google.com/tpu/pricing#pod-pricing", "anchor_text": "currently available TPU pod pricing per hour", "paragraph_index": 34}, {"url": "https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/", "anchor_text": "not mentioning the environmental costs like carbon emissions", "paragraph_index": 34}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "paragraph_index": 37}, {"url": "https://www.tivadardanka.com/blog", "anchor_text": "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!", "paragraph_index": 42}], "all_paragraphs": ["If you have ever used a neural network to solve a complex problem, you know that they can be enormous in size, containing millions of parameters. For instance, the famous BERT model has about ~110 million.", "To illustrate the point, this is the number of parameters for the most common architectures in NLP, as summarized in the recent State of AI Report 2020 by Nathan Benaich and Ian Hogarth.", "In Kaggle competitions, the winner models are often ensembles, composed of several predictors. Although they can beat simple models by a large margin in terms of accuracy, their enormous computational costs make them utterly unusable in practice.", "Is there any way to somehow leverage these powerful but massive models to train state of the art models, without scaling the hardware?", "Currently, there are three main methods out there to compress a neural network while preserving the predictive performance:", "In this post, my goal is to introduce you to the fundamentals of knowledge distillation, which is an incredibly exciting idea, building on training a smaller network to approximate the large one.", "Let\u2019s imagine a very complex task, such as image classification for thousands of classes. Often, you can\u2019t just slap on a ResNet50 and expect it to achieve 99% accuracy. So, you build an ensemble of models, balancing out the flaws of each one. Now you have a huge model, which, although performs excellently, there is no way to deploy it into production and get predictions in a reasonable time.", "However, the model generalizes pretty well to the unseen data, so it is safe to trust its predictions. (I know, this might not be the case, but let\u2019s just roll with the thought experiment for now.)", "What if we use the predictions from the large and cumbersome model to train a smaller, so-called student model to approximate the big one?", "This is knowledge distillation in essence, which was introduced in the paper Distilling the Knowledge in a Neural Network by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.", "In broad strokes, the process is the following.", "To visualize the process, you can think of the following.", "Let\u2019s focus on the details a bit. How is the knowledge obtained?", "In classifier models, the class probabilities are given by a softmax layer, converting the logits to probabilities:", "are the logits produced by the last layer. Instead of these, a slightly modified version is used:", "where T is a hyperparameter called temperature. These values are called soft targets.", "If T is large, the class probabilities are \u201csofter\u201d, that is, they will be closer to each other. In the extreme case, when T approaches infinity,", "If T = 1, we obtain the softmax function. For our purposes, the temperature is set to higher than 1, thus the name distillation.", "Hinton, Vinyals, and Dean showed that a distilled model can perform as good as an ensemble composed of 10 large models.", "You might ask, why not train a smaller network from the start? Wouldn\u2019t it be easier? Sure, but it wouldn\u2019t work necessarily.", "Empirical evidence suggests that more parameters result in better generalization and faster convergence. For instance, this was studied by Sanjeev Arora, Nadav Cohen, and Elad Hazan in their paper On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization.", "For complex problems, simple models have trouble learning to generalize well on the given training data. However, we have much more than the training data: the teacher model\u2019s predictions for all the available data.", "This benefits us in two ways.", "First, the teacher model\u2019s knowledge can teach the student model how to generalize via available predictions outside the training dataset. Recall that we use the teacher model\u2019s predictions for all available data to train the student model, instead of the original training dataset.", "Second, the soft targets provide more useful information than class labels: it indicates if two classes are similar to each other. For instance, if the task is to classify dog breeds, information like \u201cShiba Inu and Akita are very similar\u201d is extremely valuable regarding model generalization.", "As noted by Hinton et al., one of the earliest attempts to compress models by transferring knowledge was to reuse some layers of a trained ensemble, as done by Cristian Bucilu\u01ce, Rich Caruana, and Alexandru Niculescu-Mizil in their 2006 paper titled Model compression.", "In the words of Hinton et al.,", "\u201c\u2026we tend to identify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we can change the form of the model but keep the same knowledge. A more abstract view of the knowledge, that frees it from any particular instantiation, is that it is a learned mapping from input vectors to output vectors.\u201d \u2014 Distilling the Knowledge in a Neural Network", "Thus, the knowledge distillation doesn\u2019t use the learned weights directly, as opposed to transfer learning.", "If you want to compress the model even further, you can try using even simpler models like decision trees. Although they are not as expressive as neural networks, their predictions can be explained by looking at the nodes individually.", "This was done by Nicholas Frosst and Geoffrey Hinton, who studied this in their paper Distilling a Neural Network Into a Soft Decision Tree.", "They showed that distilling indeed helped a little, although even simpler neural networks have outperformed them. On the MNIST dataset, the distilled decision tree model achieved 96.76% test accuracy, which was an improvement from the baseline 94.34% model. However, a straightforward two-layer deep convolutional network still reached 99.21% accuracy. Thus, there is a trade-off between performance and explainability.", "So far, we have only seen theoretical results instead of practical examples. To change this, let\u2019s consider one of the most popular and useful models in recent years: BERT.", "Originally published in the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin et al. from Google, it soon became widely used for various NLP tasks like document retrieval or sentiment analysis. It was a real breakthrough, pushing state of the art in several fields.", "There is one issue, however. BERT contains ~110 million parameters and takes a lot of time to train. The authors reported that the training required 4 days using 16 TPU chips in 4 pods. Calculating with the currently available TPU pod pricing per hour, training costs would be around 10000 USD, not mentioning the environmental costs like carbon emissions.", "One successful attempt to reduce the size and computational cost of BERT was made by Hugging Face. They used knowledge distillation to train DistilBERT, which is 60% the original model\u2019s size while being 60% faster and keeping 97% of its language understanding capabilities.", "The smaller architecture requires much less time and computational resources: 90 hours on 8 16GB V100 GPUs.", "If you are interested in more details, you can read the original paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, or the summarizing article was written by one of the authors. This is a fantastic read, so I strongly recommend you to do so!", "Knowledge distillation is one of the three main methods to compress neural networks and make them suitable for less powerful hardware.", "Unlike weight pruning and quantization, the other two powerful compression methods, knowledge distillation does not reduce the network directly. Rather, it uses the original model to train a smaller one called the student model. Since the teacher model can provide its predictions even on unlabelled data, the student model can learn how to generalize like the teacher.", "Here, we have looked at two key results: the original paper, which introduced the idea, and a follow-up, showing that simple models such as decision trees can be used as student models.", "If you are interested in a broader overview of the field, I recommend the following article, which was published right here in Towards Data Science:", "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I want to democratize machine learning. Math PhD with an INTJ personality. Chaotic good."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcf371be516c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf371be516c6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf371be516c6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tivadar.danka?source=post_page-----cf371be516c6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=post_page-----cf371be516c6--------------------------------", "anchor_text": "Tivadar Danka"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F26fd873de5f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&user=Tivadar+Danka&userId=26fd873de5f2&source=post_page-26fd873de5f2----cf371be516c6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf371be516c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf371be516c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.stateof.ai/", "anchor_text": "State of AI Report 2020"}, {"url": "https://www.twitter.com/nathanbenaich", "anchor_text": "Nathan Benaich"}, {"url": "https://www.twitter.com/soundboy", "anchor_text": "Ian Hogarth"}, {"url": "https://www.stateof.ai/", "anchor_text": "State of AI Report 2020"}, {"url": "https://www.twitter.com/nathanbenaich", "anchor_text": "Nathan Benaich"}, {"url": "https://www.twitter.com/soundboy", "anchor_text": "Ian Hogarth"}, {"url": "https://towardsdatascience.com/can-you-remove-99-of-a-neural-network-without-losing-accuracy-915b1fab873b", "anchor_text": "weight pruning"}, {"url": "https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7", "anchor_text": "quantization"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://en.wikipedia.org/wiki/Logit", "anchor_text": "logits"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://arxiv.org/abs/1802.06509", "anchor_text": "Sanjeev Arora, Nadav Cohen, and Elad Hazan in their paper On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization"}, {"url": "https://arxiv.org/abs/1802.06509", "anchor_text": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization"}, {"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Hinton et al."}, {"url": "https://dl.acm.org/doi/10.1145/1150402.1150464", "anchor_text": "Cristian Bucilu\u01ce, Rich Caruana, and Alexandru Niculescu-Mizil in their 2006 paper titled Model compression"}, {"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://arxiv.org/abs/1711.09784", "anchor_text": "Distilling a Neural Network Into a Soft Decision Tree"}, {"url": "https://arxiv.org/abs/1711.09784", "anchor_text": "Distilling a Neural Network Into a Soft Decision Tree"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://cloud.google.com/tpu/pricing#pod-pricing", "anchor_text": "currently available TPU pod pricing per hour"}, {"url": "https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/", "anchor_text": "not mentioning the environmental costs like carbon emissions"}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"url": "https://medium.com/huggingface/distilbert-8cf3380435b5", "anchor_text": "\ud83c\udfce Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERTYou can find the code to reproduce the training of DilBERT along with pre-trained weights for DilBERT here.medium.com"}, {"url": "https://towardsdatascience.com/knowledge-distillation-a-survey-through-time-187de05a278a", "anchor_text": "Knowledge Distillation \u2014 A Survey Through TimeThrough this blog you will review Knowledge Distillation (KD) and six follow-up papers.towardsdatascience.com"}, {"url": "https://www.tivadardanka.com/blog", "anchor_text": "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!"}, {"url": "https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7", "anchor_text": "How to accelerate and compress neural networks with quantizationGoing from floats to integerstowardsdatascience.com"}, {"url": "https://towardsdatascience.com/can-you-remove-99-of-a-neural-network-without-losing-accuracy-915b1fab873b", "anchor_text": "Can you remove 99% of a neural network without losing accuracy?An introduction to weight pruningtowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cf371be516c6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cf371be516c6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cf371be516c6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----cf371be516c6---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf371be516c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&user=Tivadar+Danka&userId=26fd873de5f2&source=-----cf371be516c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf371be516c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&user=Tivadar+Danka&userId=26fd873de5f2&source=-----cf371be516c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf371be516c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf371be516c6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcf371be516c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cf371be516c6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf371be516c6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cf371be516c6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cf371be516c6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cf371be516c6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cf371be516c6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cf371be516c6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cf371be516c6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cf371be516c6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tivadar Danka"}, {"url": "https://medium.com/@tivadar.danka/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F26fd873de5f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&user=Tivadar+Danka&userId=26fd873de5f2&source=post_page-26fd873de5f2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F471a6c88bdce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-a-neural-network-train-other-networks-cf371be516c6&newsletterV3=26fd873de5f2&newsletterV3Id=471a6c88bdce&user=Tivadar+Danka&userId=26fd873de5f2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}