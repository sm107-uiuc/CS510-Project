{"url": "https://towardsdatascience.com/idea-behind-lime-and-shap-b603d35d34eb", "time": 1683002292.7164521, "path": "towardsdatascience.com/idea-behind-lime-and-shap-b603d35d34eb/", "webpage": {"metadata": {"title": "Idea Behind LIME and SHAP. Intuition behind ML interpretation\u2026 | by ashutosh nayak | Towards Data Science", "h1": "Idea Behind LIME and SHAP", "description": "In machine learning, there has been a trade-off between model complexity and model performance. Complex machine learning models e.g. deep learning (that perform better than interpretable models e.g\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=w9O0fkfMkx0", "anchor_text": "this", "paragraph_index": 7}, {"url": "https://www.youtube.com/watch?v=qcLZMYPdpH4", "anchor_text": "this", "paragraph_index": 7}, {"url": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf", "anchor_text": "LIME", "paragraph_index": 11}, {"url": "https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf", "anchor_text": "SHAP", "paragraph_index": 11}, {"url": "https://christophm.github.io/interpretable-ml-book/shapley.html", "anchor_text": "must-read before LIME", "paragraph_index": 12}, {"url": "https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83", "anchor_text": "must-read before SHAP", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b", "anchor_text": "Medium blog on LIME", "paragraph_index": 12}, {"url": "https://www.youtube.com/watch?v=Q8rTrmqUQsU&t=1s", "anchor_text": "H20 video", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27", "anchor_text": "Scott Lundberg\u2019s blog", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/machine-learning-interpretability-techniques-662c723454f3", "anchor_text": "White box vs Black box", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/how-to-avoid-the-machine-learning-blackbox-with-shap-da567fc64a8b", "anchor_text": "Medium blog", "paragraph_index": 14}, {"url": "https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80", "anchor_text": "Medium blog", "paragraph_index": 14}], "all_paragraphs": ["In machine learning, there has been a trade-off between model complexity and model performance. Complex machine learning models e.g. deep learning (that perform better than interpretable models e.g. linear regression) have been treated as black boxes. Research paper by Ribiero et al (2016) titled \u201cWhy Should I Trust You\u201d aptly encapsulates the issue with ML black boxes. Model interpretability is a growing field of research. Please read here for the importance of machine interpretability. This blog discusses the idea behind LIME and SHAP.", "This blog is not about how to use/code/interpret LIME or SHAP (there are plenty of good resources on this). These resources show how to use the LIME and SHAP values given to the independent variables, individualistically (one LIME/SHAP value for one variable for one data point). This blog is about understanding how do we get these values and what are we actually doing when using the LIME and SHAP library. Before jumping to LIME and then SHAP, few primers for better understanding.", "In linear regression models, Beta coefficients explain the prediction for all the data points (if variable value increases by 1, prediction increases by Beta for every data point). This is global fidelity. In a causal analysis, it is called \u201caverage\u201d causal analysis. However, this does not explain individual differences (heterogeneity). The effect of a variable for one user could be different from another user. This is local fidelity (explanation for individual data point or local subsection of the joint distribution of independent variables). Local function explanations (expectedly) could be more accurate than global explanations because, in a local region, the function (higher probability) is linear and monotonic. Even local regions could be highly non-linear and local explainability would fail \u2014 limitations of such methods. (This also is a part of smoothness assumption \u2014 without this, it is a cannot do many situations as many optimization models would fail). LIME and SHAP explore and use the property of local explainability to build surrogate models to black-box machine learning models to provide them interpretability. Few examples in Figure 1.", "LIME and SHAP are surrogate models (Figure 1). It means they still use the black-box machine learning models. They tweak the input slightly (like we do in sensitivity tests) and test the changes in prediction. This tweak has to be small so that it is still close to the original data point (or in the local region). LIME and SHAP models are surrogate models that model the changes in the prediction (on the changes in the input). For example, if the model prediction does not change much by tweaking the value of a variable, that variable for that particular data point may not be an important predictor. Since surrogate models still treat the ML models as a black box, these are model agnostic.", "A data point has to be converted into a format that is easier to work with for building surrogate models (sampling data points in the neighborhood of the original data point). This representation is called interpretable (in the LIME paper) as it is understandable to humans (it converts the data in binary). As shown in Figure 2, x: d-dimensional data is converted into x\u2019: d\u2019-dimensional binary vector data for its interpretable representation. Some of the examples are given in Figure 2. Figure 2 also shows how LIME uses interpretable data format in surrogate models for interpretability. The objective (loosely) is to minimize the difference in response (prediction) between x and its neighbor.", "Now we have the background to understand LIME. If we select a point x, we can draw samples (z\u2019) around x by switching off some of the binary dimensions (from x\u2019) representation (weighted by proximity measure). Once we get the sample, recover a variable z from z\u2019. For example, (1) let f(z) be the deep learning model that detects if a sentence is hateful or not. Our dataset contains sentences. (2) If we consider a data point (a sentence x), we first convert it into a format of x\u2019 (words present or absent). (3) From x\u2019, we sample z\u2019 around the neighborhood of x\u2019 (by switching off some of the binary vectors uniformly). (4) These samples are converted to z (the sentence is recovered). In z, some of the words may not be present. (5) Using z as input, get the value f(z). Figure 3 shows an example of sampling. It also covers what is LIME \u201cvalue\u201d for the individual data point (and for each variable) and how we get it. It shows how do we get global importance from the local importance (explainability) of a variable. It loosely means adding up the local weights such that even the switched off dimensions are maximally covered (useful to keep in mind as we go through SHAP). As mentioned before, there are plenty of good resources that tell us how to use (or interpret) these values so this blog will not cover it.", "Although LIME has the desirable property of additivity (sum of the individual impact is equal to the total impact), it has got some criticism on lack of stability, consistency (changing the model does not decrease the attribution of a variable if its contribution increases or remains the same) and missingness (missing variable should have 0 attribution). All three properties are fulfilled by SHAP (hence more commonly used). Also, LIME needs to define \u201clocal\u201d.", "SHAP values use a similar concept to LIME. SHAP provides theoretical guarantees based on the game theory concept of Shapley values. Please see this short video on Shapley value before reading further to understand SHAP. You can also see this for the theoretical background of Shapley value.", "SHAP stands for SHapley Additive exPlanation. \u201cAdditive\u201d is an important key term. Like LIME, SHAP has additive attribution property. The sum of SHAP values of all the variables for a data point is equal to the final prediction. SHAP can be understood keeping LIME in mind as explained in Figure 4. Figure 4 shows how SHAP values are calculated. In SHAP, we do not have to build a local model (like linear regression in LIME), rather the same function f(x) is used to calculate the shapley values for each dimension.", "Shapley value guarantees a fair distribution of contribution for each of the variables (LIME do not provide the guarantee). LIME assumes that the local model is linear, SHAP does not have any such assumptions. SHAP value calculation is very time expensive (as it checks all the possible combinations: it does it through monte Carlo simulations rather than brute force). SHAP value is NOT the difference between the prediction with and without a variable, rather it is a contribution of a variable to the difference between the actual prediction and the mean prediction. The variable importance at a global level is given by adding the absolute value of the SHAP values for each individual data point. Although SHAP uses all the variables, we can select some variables with higher variable importance, drop other variables and rerun SHAP if we want to reduce the number of variables (because of the property like consistency, the order of variable importance will not change so less important variables can be ignored).", "I hope the background of what we are doing in LIME and SHAP is clearer. Both are model agnostic and the library is available for standard machine learning models. Due to its theoretical guarantees and simplicity, SHAP is widely used and maybe more acceptable [6]. In LIME, we need to define how we are considering a \u201cneighbor\u201d. Also, we build a linear local model which might not be linear in a very complicated decision surface (even at a local level). In SHAP, we can use the same model we trained using the training data.", "The blog is drawn from these papers[1] LIME by Ribeiro et al[2] SHAP by Lundberg and Lee", "The following are in the same line as this blog, explaining LIME and SHAP at a lower level:[3] must-read before LIME[4] must-read before SHAP[5] Medium blog on LIME by Lar", "Additional sources on why should we consider interpretability:[6] H20 video[7] Scott Lundberg\u2019s blog[8] White box vs Black box", "This blog is NOT about how to use the LIME and SHAP library or how to interpret their results. You can find how to use them at:[9] Medium blog by Migeul[0] Medium blog by Peter", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Postdoc, UC Davis (OR+ML in marketing)|Purdue|IIT Kgp"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb603d35d34eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashutoshnayakkgp?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashutoshnayakkgp?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "ashutosh nayak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F107b80e9d9d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&user=ashutosh+nayak&userId=107b80e9d9d4&source=post_page-107b80e9d9d4----b603d35d34eb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb603d35d34eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb603d35d34eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=w9O0fkfMkx0", "anchor_text": "this"}, {"url": "https://www.youtube.com/watch?v=qcLZMYPdpH4", "anchor_text": "this"}, {"url": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf", "anchor_text": "LIME"}, {"url": "https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf", "anchor_text": "SHAP"}, {"url": "https://christophm.github.io/interpretable-ml-book/shapley.html", "anchor_text": "must-read before LIME"}, {"url": "https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83", "anchor_text": "must-read before SHAP"}, {"url": "https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b", "anchor_text": "Medium blog on LIME"}, {"url": "https://www.youtube.com/watch?v=Q8rTrmqUQsU&t=1s", "anchor_text": "H20 video"}, {"url": "https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27", "anchor_text": "Scott Lundberg\u2019s blog"}, {"url": "https://towardsdatascience.com/machine-learning-interpretability-techniques-662c723454f3", "anchor_text": "White box vs Black box"}, {"url": "https://towardsdatascience.com/how-to-avoid-the-machine-learning-blackbox-with-shap-da567fc64a8b", "anchor_text": "Medium blog"}, {"url": "https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80", "anchor_text": "Medium blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b603d35d34eb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b603d35d34eb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----b603d35d34eb---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/predictive-analytics?source=post_page-----b603d35d34eb---------------predictive_analytics-----------------", "anchor_text": "Predictive Analytics"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b603d35d34eb---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb603d35d34eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&user=ashutosh+nayak&userId=107b80e9d9d4&source=-----b603d35d34eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb603d35d34eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&user=ashutosh+nayak&userId=107b80e9d9d4&source=-----b603d35d34eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb603d35d34eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb603d35d34eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b603d35d34eb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b603d35d34eb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b603d35d34eb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b603d35d34eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashutoshnayakkgp?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashutoshnayakkgp?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "ashutosh nayak"}, {"url": "https://medium.com/@ashutoshnayakkgp/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "309 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F107b80e9d9d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&user=ashutosh+nayak&userId=107b80e9d9d4&source=post_page-107b80e9d9d4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc1f1695dc3f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidea-behind-lime-and-shap-b603d35d34eb&newsletterV3=107b80e9d9d4&newsletterV3Id=c1f1695dc3f9&user=ashutosh+nayak&userId=107b80e9d9d4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}