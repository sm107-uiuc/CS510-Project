{"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "time": 1682993666.028642, "path": "towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9/", "webpage": {"metadata": {"title": "Finding the Cost Function of Neural Networks | by Chi-Feng Wang | Towards Data Science", "h1": "Finding the Cost Function of Neural Networks", "description": "Without understanding the math behind deep learning, we cannot really appreciate all the intricacies behind the code. This series will walk you through the math of minimizing loss of neural networks."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1802.01528", "anchor_text": "The Matrix Calculus You Need for Deep Learning", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1802.01528", "anchor_text": "the original paper", "paragraph_index": 1}, {"url": "https://www.khanacademy.org/math/ap-calculus-ab", "anchor_text": "Khan Academy videos", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/the-beginners-guide-to-gradient-descent-c23534f808fd", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-ac15e178bbd", "anchor_text": "Part 2", "paragraph_index": 14}], "all_paragraphs": ["Terence Parr and Jeremy Howard\u2019s paper, The Matrix Calculus You Need for Deep Learning, provided a lot of insights into how deep learning libraries like Tensorflow or PyTorch really worked. Without understanding the math behind deep learning, all we are doing is writing a few lines of abstract code \u2014 build a model, compile it, train it, evaluate it \u2014 without really learning to appreciate all the complex intricacies that support all these functions.", "This (and the next few) articles will be the reflections of what I learned from Terence and Jeremy\u2019s paper. This article will introduce the problem, which we will solve in the upcoming articles. I\u2019ll explain most of the math and add in some of my insights, but for more information, definitely check out the original paper.", "These articles (and the paper) both assume a basic knowledge of high-school level calculus (i.e. the derivative rules and how to apply them). You can check out the Khan Academy videos if you want to revisit them.", "Here\u2019s our problem. We have a neural network with just one layer (for simplicity\u2019s sake) and a loss function. That one layer is a simple fully-connected layer with only one neuron, numerous weights w\u2081, w\u2082, w\u2083\u2026, a bias b, and a ReLU activation. Our loss function is the commonly used Mean Squared Error (MSE). Knowing our network and our loss function, how can we tweak the weights and biases to minimize the loss?", "In order to minimize loss, we use the concept of gradient descent. As explained here (read this if you aren\u2019t familiar with how gradient descent works), gradient descent calculates the slope of the loss function, then shifts the weights and biases according to that slope to a lower loss.", "We need to find the slope of our loss function. Before we do that, however, let us define our loss function. MSE simply squares the difference between every network output and true label, and takes the average. Here\u2019s the MSE equation, where C is our loss function (also known as the cost function), N is the number of training images, y is a vector of true labels (y = [target(x\u2081), target(x\u2082)\u2026target(x\ud835\udc5b)]), and o is a vector of network predictions. (In case you haven\u2019t noticed already, variables in bold are vectors.)", "We can further expand this equation. What are the network outputs? We feed in a vector input \u2014 let\u2019s call that x \u2014 to our fully-connected layer. The activations of that layer are our network outputs. What are the activations of our fully-connected layer? Each item in our vector input is multiplied by a certain weight each. Then, all these products are added together, and a bias added on top of that. Finally, that value is passed through a ReLu to form the activation for our one neuron in the fully-connected layer.", "Summing up the products of each input value and each weight is essentially a vector dot product between the input x and a vector of weights (let\u2019s call that w). A ReLU is simply a function that converts any negative values to 0. Let\u2019s rename that as the max(0,z) function, which returns z if z is positive and 0 if z is negative.", "Put that altogether, and we get the equation of our activation for the neuron:", "Now let\u2019s substitute that into our loss function. Because we train with more than one input, let us define X as a collection of all our inputs:", "Since we only have one layer (with one neuron), the activation of this neuron is the prediction of our model. Hence, we can substitute our activation in for o in our loss function:", "and then substitute the activation function:", "This is the loss function that we have to find the slope to.", "In order to find the slope, we have to find the loss function\u2019s derivative. Not just any derivative, however \u2014 it has to be the partial derivative with respect to the weights and with respect to the biases (since these are the values we are tweaking).", "Check out Part 2 to learn how to calculate partial derivatives!", "If you like this article, don\u2019t forget to leave some claps! Do leave a comment below if you have any questions or suggestions :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Student at UC Berkeley; Machine Learning Enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F490dc1f3cfd9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@reina.wang?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "Chi-Feng Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ddaaec52a09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=post_page-9ddaaec52a09----490dc1f3cfd9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F490dc1f3cfd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F490dc1f3cfd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/en/math-symbols-blackboard-classroom-1500720/", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1802.01528", "anchor_text": "The Matrix Calculus You Need for Deep Learning"}, {"url": "https://arxiv.org/abs/1802.01528", "anchor_text": "the original paper"}, {"url": "https://www.khanacademy.org/math/ap-calculus-ab", "anchor_text": "Khan Academy videos"}, {"url": "https://towardsdatascience.com/the-beginners-guide-to-gradient-descent-c23534f808fd", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1802.01528.pdf", "anchor_text": "Source"}, {"url": "https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-ac15e178bbd", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd", "anchor_text": "Part 2: Partial Derivatives"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3: Vector Calculus"}, {"url": "https://towardsdatascience.com/calculating-gradient-descent-manually-6d9bee09aa0b", "anchor_text": "Part 4: Putting It All Together"}, {"url": "https://arxiv.org/abs/1802.01528", "anchor_text": "here"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----490dc1f3cfd9---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----490dc1f3cfd9---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/derivatives?source=post_page-----490dc1f3cfd9---------------derivatives-----------------", "anchor_text": "Derivatives"}, {"url": "https://medium.com/tag/matrix?source=post_page-----490dc1f3cfd9---------------matrix-----------------", "anchor_text": "Matrix"}, {"url": "https://medium.com/tag/calculus?source=post_page-----490dc1f3cfd9---------------calculus-----------------", "anchor_text": "Calculus"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F490dc1f3cfd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=-----490dc1f3cfd9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F490dc1f3cfd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=-----490dc1f3cfd9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F490dc1f3cfd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F490dc1f3cfd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----490dc1f3cfd9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----490dc1f3cfd9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chi-Feng Wang"}, {"url": "https://medium.com/@reina.wang/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ddaaec52a09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=post_page-9ddaaec52a09--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F827df2c647b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-490dc1f3cfd9&newsletterV3=9ddaaec52a09&newsletterV3Id=827df2c647b&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}