{"url": "https://towardsdatascience.com/adventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16", "time": 1683014445.849371, "path": "towardsdatascience.com/adventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16/", "webpage": {"metadata": {"title": "The Math Behind Training of a Neural Network with an Example Case | Towards Data Science", "h1": "The Math Behind Training of a Neural Network", "description": "Neural Networks are widely used in the scope of Artificial Intelligence. It can be a solution for most of the predictive problems thanks to its scalability and flexibility. You can solve complex\u2026"}, "outgoing_paragraph_urls": [{"url": "https://youtu.be/AXqhWeUEtQU", "anchor_text": "watch this video", "paragraph_index": 24}, {"url": "https://youtu.be/AXqhWeUEtQU", "anchor_text": "this video", "paragraph_index": 29}], "all_paragraphs": ["Neural Networks are widely used in the scope of Artificial Intelligence. It can be a solution for most of the predictive problems thanks to its scalability and flexibility. You can solve complex problems on regression, classification, forecasting, object recognition, speech recognition, NLP and so on. So, what is Neural Networks, what makes them capable of all these problems and how does it learn to make predictions? In order to understand these, we need to know more about neurons and the math behind them. In this article, I am going to explain how neurons learn from given data and used in prediction. We will inspect the theory behind the neural networks and training process, from A to Z. You will get the answers of the following questions in this article;", "Neurons are essential components of a Neural Network. More precisely, Neural Networks are formed by connecting one neuron to every other neuron. Unlike the human neural system, neurons in neural networks are connected to each other through the layers. Processing time would be extremely high if we consider that every neuron would be connected to every other neuron. In order to reduce processing time and save computer\u2019s computational power, layers are used in the Neural Network. Thanks to layers, every neuron in a layer is connected to neurons in the next layer. The figure below represents the basic structure of a neural network with three layers.", "As you can see from the figure above, there are 3 layers as Input, Hidden and Output. These three layers are the main layers of a neural network. Neurons in the input layer represent each variable in the dataset. Neuron in the output layer represents the final predicted value after input values pass into every neuron in the hidden layer. While there is only one input and output layer, the number of hidden layers can be increased. Therefore, performance of the neural networks depends on the number of layers and number of neurons in each layer. Predictive performance between a network with 3 hidden layers with 3 neurons in every layer and network with one layer-one neuron might be different. So, what does the network look like if we add another hidden layer with 3 neurons? Figure below shows a network with 2 hidden layers.", "We can add more hidden layers and neurons to the network. There is not any limit for it. But, we shouldn\u2019t forget that adding more layers and neurons doesn\u2019t mean that our network will predict better. However, when we increase the number of hidden layers and neurons, the training time will increase due to the calculations in each neuron. What we need to do is find the best network structure for our network.", "Neural networks work over iterations and every iteration trains the model to reach the best prediction. So, feeding the neurons is the main movement that trains our network. This movement is called \u201cfeed forward\u201d in neural networks. What it means is taking data from previous connected neurons, doing calculations with that data and sending the result to the next connected neurons. When the final calculation is done in the output neuron, another observation is taken from the data set and goes for feeding again. This process keeps going until our network\u2019s prediction is really close to the actual value that has been predicted. Most important thing here is what calculations are performed in neurons. We can basically call weighting to these calculations. We shouldn\u2019t forget that we are going to make predictions by using this network. Therefore, we need neurons to be weighted to make the best prediction.", "Okay, let\u2019s inspect this feeding process in depth. Let\u2019s assume that we have a neural network with 1 input layer with 2 input neurons, 1 hidden layer with 2 neurons and 1 output layer with 1 neuron. So, first we can inspect the feeding process of a neuron in the hidden layer;", "In the figure above, there are two neurons in the input layer. It means that we have two variables in our data set that we are going to use for training. As you can see, every connection between the input neurons and the neuron in the hidden layer have \u201cW\u201d values. These \u201cW\u201d values represent the values which a single neuron keeps for every neuron that feeds itself. At the first step \u201cW\u201d and \u201cb\u201d values can be generated randomly between 0 and 1. During the iterations (steps) these values will be updated in order to reach the best predicted value. We will see these iterations in the next sections.", "The H1 neuron is fed by X1 and X2 neurons so it has W1 and W2 weight for X1 and X2. If you see the calculation part, each neuron\u2019s value that comes from the input layer multiplies with its weight and we sum all of it. However, there is also one more value which is represented as \u201cb\u201d. This value is the intercept of the neuron and we sum this value with multiplied weights. It can also be called \u201cbias\u201d. After the sum operation, the result passes from the activation function. Activation function makes a kind of transformation operation. The obtained value from the activation function will be the final value of this neuron at the current step .", "There are different activation functions that we can choose according to the value that we want to predict. Let\u2019s assume that we want to predict 2 classes indicated as 0 and 1. So, what we need is a probability value between 0 and 1 in order to decide the predicted class. If the predicted value is less than 0.5 (which means it is close to 0 class) we can say is predicted as 0. If it is greater than 0.5 we can say it is predicted as 1. But, without activation function it is possible to get predicted values out of 0 and 1. In order to sort out this problem, we can use an activation function that keeps neuron values between 0 and 1. This activation function is called \u201cSigmoid\u201d. Other activation functions which are commonly used are presented in the figure below.", "I already explained when we should use the sigmoid function. If you look at the figure above there are also other activation functions. For example, hyperbolic tangent function can be used when we need output between -1 and 1. The ReLU function can be used if the output should not be less than 0 and identify function returns output as it was. It means the values which are calculated in neurons will be as itself. According to the variable which is going to be predicted, one of these functions should be chosen.", "Okay! We can set the whole network with an example case. Let\u2019s assume that we are going to train a network to predict tree ages by using the length and the width of the tree. It means that we will use two variables as predictors and one variable as predicted. The data set that is going to be used for training is shown below.", "As we mentioned before, neural networks get trained over iterations. Therefore, in every iteration (step) it takes samples from the data set and does the calculations in the neurons. Only one observation or small batches can be taken. In this example we will take one sample (observation) for one step and it will be the first observation of our dataset.", "We should also decide which activation function is going to be used in the network. As it mentioned, it depends on what you want to predict for using this network. If we look up to our data set, age, which is a dependent variable, includes numeric values. So, we won\u2019t predict the type of trees, we will predict how old they are. This inference leads us to activation functions whose output will be linear. The first option is ReLU and the second option is Identity. Because, these functions do not transform the values to between 0 and 1. The difference between them while Identity function returns inputed value as it was, ReLU function returns value 0 if the input is negative and returns value as it was if it is not negative. However, the age can not be negative so using ReLU function makes sense. But I want to keep this tutorial as simple as it can be. That\u2019s why I want to use the Identity function as activation. It is much easier to understand in this format. It sounds weird but using the Identity function means that you won\u2019t use any activation function. Therefore, in the following figures I didn\u2019t add any activation function process (Because, it returns calculated value as it was).", "Figure below represents the neural network and calculations for the first iteration. As you can see, there are two neurons in the Input layer as \u201cWidth\u201d and \u201cLength\u201d. Because it is the first iteration, the values of \u201cWidth\u201d and \u201cLength\u201d come from the first observation as 5 and 7. There are also 2 neurons in the hidden layer and one neuron in the output layer. If you look carefully every connection between neurons has \u201cw\u201d weight values. As we mentioned before, weights can be chosen randomly between 0 and 1. It can be out of this range but it is better to set between 0 and 1 for the first step. In the following iterations they are going to be changed anyway. The values of these weights that have been randomly assigned to connections are also shown above the neurons. We can use the saying which is -these weights for every connection- sometimes, but they are stored in the neurons. We do not assign weights to connection in practice. It means that every neuron stores \u201cw\u201d values for every connection that comes from the previous layer in matrix structure. Actually, we could also use the ReLU activation function. Because, we want to predict the age of the trees and ages can not be negative. But I just want to keep this example as simple as it can be. Therefore I decided to use the Identity Activation Function and there is not any activation function process in the figure below. Because identity functions do not have any transformation for the values which are calculated in the neurons. It keeps them as they were.", "After the calculations in the neurons, 6.25 has been found as the final result in the first iteration. Technically we can say that it was the first predicted \u201cAge\u201d value according to given first observation\u2019s Width and Length values. Next thing that we should do is to compare this result with the actual Age for that observation. Age of the tree for the first observation is 5. It means that we are 1.5 ages far from the actual value and we need to update the neurons\u2019 weights according to this distance. This process is called Back-propagation and we will see it in the next section. But before that, I want to show you what would happen if we used 2 hidden layers with 2 neurons in each in this artificial neural network. Because you might wonder how we would do the calculation in such a scenario.", "In the figure above, you can see that there is one more layer added into the previous network. Therefore, our predicted value has changed as 5.68 as expected. This was just for an example we will use the previous network structure in the following sections.", "This section may be the most important section of this article. Because I will explain how neurons\u2019 weights get changed to make good predictions. I named this section as \u201cUpgrading Neurons to Up Levels\u201d because after every iteration in the network, we change the weight values of neurons. So, we upgrade them to up level to make better predictions.", "As it mentioned in the previous section, neurons\u2019 weights and biases (intercepts) need to be updated by considering the difference between predicted and actual value after one iteration has been completed. This process is called Back-propagation in artificial neural networks and the algorithm that we are going to use for that is Stochastic Gradient Descent. After every feedforward (iteration) back-propagation has to be completed too during the training. With help of back-propagation networks get trained. Following figure represents the back-propagation process.", "In order to update the weights and biases first we should find the difference between actual and predicted value. There are two functions which are used often for calculating the difference between actual and predicted. These are called Mean Square Error and Cross-Entropy. In some cases Sum of Squared Error is used too. These functions are called loss function or cost function in general. While Means Square Error can be prefered for regression problems, Cross-Entropy can be prefered for classification problems. In this example we are trying to estimate tree ages, not type of trees. That\u2019s why we can use Mean Square Error.", "The figure below shows the function of Mean Squared Error;", "Now, we can calculate the difference between actual value and predicted value in the previous example by using MSE. The predicted age was found as 6.25 for first observation and the actual age was 5. Because we take only one sample for one iteration the \u201cn\u201d will be equal to 1.", "According to the above mathematical operations MSE found as 1.5625. We can use the term of loss for this value. Additionally, if we would have taken 32 observations for each iteration, we would have to calculate MSE from 32 observations predicted values and actual values.", "Okay, we have the loss result and what is next? Now, we have to figure out how we are going to change the weights and biases by considering their effects on loss. Therefore, we need a method that can do it. This method is called partial derivative in math. Partial derivatives of loss function L (MSE) with weights and biases can provide the information that how weights and biases influence the Loss. So, it is what we are looking for. We need to do the following partial derivative operation;", "The figure above shows the partial derivatives for each weight and bias. When we have partial derivatives we should multiply it with the Learning Rate. After that we should subtract the result from its weights. The Learning Rate here represents how weights are going to change. Changes in weight with a high learning rate will be much more than low rate learning rate. Therefore, we need to set the optimum learning rate for our network.", "Now it is time to learn how we are going to find the partial derivatives. It\u2019s a little deep in math and if you don\u2019t have any idea about partial derivatives you can watch this video. We will go on derivative operations for W1 (dL/dW1). In order to find this partial derivative we can do a transition which is shown in the formula below. H1 represents the value of H1 neuron after weighting operation, Yprediction represents the result of O1 neuron and w1 is the weight value which is for in the H1 neuron for tree Width (first neuron in the input layer).", "In order to solve the above equilibrium we need to go on the formulas of Yprediction, H1 and L loss. After that we can obtain the partial derivatives of all. If we look to the first partial derivative YPrediction formula we can see it\u2019s the final calculation of predicted value in the O1 neuron.", "And the formula that we found the value of H1 neuron is;", "The last formula that we need is the formula of loss function MSE (L). We shouldn\u2019t forget that we take only 1 observation in each step. So, the \u201cn\u201d value will be equal to 1;", "Okay! Now, we are good to go. We can start to take partial derivatives one by one. We can start with the first partial derivative which is \u201cdL/dYPrediction\u201d;", "The partial derivative operations have been performed in the figure above. The basic idea behind this operation is to find how Loss is influenced from Yprediction. As it mentioned before, you can watch this video in order to understand these operations. The next partial derivative is \u201cdYpred/dH1\u201d. Normally, H1 is a function which returns the value over activation function. In such cases while taking partial derivative, we should take derivative of H1. But, in this case we used identity function and the derivative of identity function f(x) = x is equal to 1. So, we can pretend like H1 is just a parameter value of YPrediction. If we would use the Sigmoid function we would should have taken derivative of it (It is another scenario).", "Now, we have all equilibriums to find the partial derivative of Loss according to \u201cW1\u201d weight. All we need to do is bring the pieces together.", "Next thing that we should do is placed the values of YTrue, YPrediction, w5 and x1 in the formula above;", "Yeap! We know how w1 value influences the loss. Next thing that we should do is decide how we are going to change the \u201cW1\u201d value in order to minimize the loss. Therefore, we\u2019ll use the algorithm called Stochastic Gradient Descent (SGD). Actually, partial derivative operation was part of the gradient descent that we have done in the previous section. We can basically say that we are training our network by using back propagation as a stochastic gradient computing technique. In SGD we take single samples from data, pass them into the network, find out how we are going to change the weights and intercepts in order to minimize loss. This process goes in every iteration until we reach the minimum threshold. It means if we can\u2019t get any decrease more than threshold value in the loss after a while, we can stop training. In order to understand it we can see following figure;", "In SGD; after we find the partial derivatives for all weights and intercepts, we should multiply them with Learning Rate and subtract from their old weights and intercepts. We only found partial derivative of \u201cw1\u201d so far. Therefore, we can do following operations in order to find out new updated \u201cw1\u201d value;", "As you can see from the above operations, our new W1 value has been found as -0.1125. It means that we will replace the old \u201cW1\u201d value (which is located in the H1 neuron) with -0.1125 and we will use this value in the next iteration. We only found the new W1 value so far. We also must find the new values of other W values and intercepts. In the figure above there other partial derivatives for W and Intercept values;", "Now, we can calculate new weights in order to complete the first back propagation process;", "We have all new \u201cw\u201d and \u201cb\u201d values after first back-propagation. Let\u2019s look at our network with new values and its result after feed forwarding step;", "As you can see from the result, the predicted value has been found as -3.33 which is negative. Do we need to worry? Answer is nop! We just completed the first iteration with the first sample of data set. There are more samples in our data that the network has to process. Besides, neural networks require much more iteration with other samples to get trained. The main questions might be; How many iteration has to be completed or when should our training process stop? The answer is hiding behind the SDG.", "The purpose of the iterations is to reach the threshold of minimum decrease in loss. We should define this threshold value before the training phase of our network. Let\u2019s say we defined the threshold value as 0.05 and the maximum iteration as 10000. It means we plan to reach 0.05 decrease in max 10000 iterations. Decreasing in loss will be higher in the first iterations. But after a while, decreasing will start to change not that much and when the network reaches the minimum threshold training will stop.", "For example, at 8909. iteration we took our sample and passed into the network and found the loss 0.3559, at 8910. iteration we took another example and found the loss 0.3315, at 89111. iteration we took another sample and found loss 0.35080. The average decrease in loss is between 0.0244 and 0.0026. These values are less than our threshold which is 0.05. So, we can stop training. However, we can also define a minimum error that stops training of our network. Network keeps training to reach a given minimum error a.k.a loss (of course we also define max iterations).", "There is also one more problem that we need to worry about. It is an overfitting problem. This problem occurs when our ML model performs perfectly on the training set and doesn\u2019t perform well on the new data (test data). It means our network can reach the minimum loss after thousands of iterations but it does not perform well on the testing set which we keep out from the training set in order to evaluate the performance. In such cases we can define a validation part from our data and control the loss amount for that part too. If the loss for validation set keeps increasing while the loss for training is decreasing, we can stop training too. We can understand it from the figure below;", "In this article we learned how neural network models\u2019 training process goes. Of course, it was just a basic example with basic network structure. The purpose of this article was to learn the idea and the math behind neural networks. More complex network structures can be set. With the help of programming languages, math operations can be done easily in these structures. But they also will require much more computational power in the machine. If you have knowledge on programming languages, you can apply these steps and create a neural network model from scratch.", "What about for after? After you understand completely the operations in this article you can go further. You can check other back-propagation and gradient descent approaches, different types of neural networks such as CNN, RNN, LSTM and so on. Every other algorithm or approach has been developed in order to solve different problems.", "I hope it was helpful, please don\u2019t hesitate to ask questions\u2026", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Statistician, Python and R Developer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5d19c594ca16&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sergencansiz.medium.com/?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "Sergen Cansiz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a38abffb19e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&user=Sergen+Cansiz&userId=5a38abffb19e&source=post_page-5a38abffb19e----5d19c594ca16---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d19c594ca16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d19c594ca16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@roman_lazygeek?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Roman Mager"}, {"url": "https://unsplash.com/s/photos/math?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://youtu.be/AXqhWeUEtQU", "anchor_text": "watch this video"}, {"url": "https://youtu.be/AXqhWeUEtQU", "anchor_text": "this video"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5d19c594ca16---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----5d19c594ca16---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5d19c594ca16---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neurons?source=post_page-----5d19c594ca16---------------neurons-----------------", "anchor_text": "Neurons"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5d19c594ca16---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d19c594ca16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----5d19c594ca16---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d19c594ca16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----5d19c594ca16---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d19c594ca16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5d19c594ca16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5d19c594ca16---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5d19c594ca16--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5d19c594ca16--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5d19c594ca16--------------------------------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sergen Cansiz"}, {"url": "https://sergencansiz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "414 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a38abffb19e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&user=Sergen+Cansiz&userId=5a38abffb19e&source=post_page-5a38abffb19e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6b5ddf93b82a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadventure-of-the-neurons-theory-behind-the-neural-networks-5d19c594ca16&newsletterV3=5a38abffb19e&newsletterV3Id=6b5ddf93b82a&user=Sergen+Cansiz&userId=5a38abffb19e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}