{"url": "https://towardsdatascience.com/finding-similar-names-using-cosine-similarity-a99eb943e1ab", "time": 1683004908.553528, "path": "towardsdatascience.com/finding-similar-names-using-cosine-similarity-a99eb943e1ab/", "webpage": {"metadata": {"title": "Finding Similar Names Using Cosine Similarity | by Leon Lok | Towards Data Science", "h1": "Finding Similar Names Using Cosine Similarity", "description": "A simple method for finding similar names using Python and cosine similarity."}, "outgoing_paragraph_urls": [{"url": "http://youtube.com/c/LeonLokk", "anchor_text": "youtube.com/c/LeonLokk", "paragraph_index": 44}, {"url": "http://leonlok.co.uk/newsletter", "anchor_text": "leonlok.co.uk/newsletter", "paragraph_index": 44}], "all_paragraphs": ["Humans are prone to making mistakes. With manually entered data, it\u2019s only a matter of time before something goes wrong. And this is especially true for customer data. There\u2019s usually two reasons for this:", "Thus errors are bound to happen. These errors could be textual ones such as typos when entering a name. They could be misclicks such as selecting the wrong address from a dropdown menu after entering a postcode. As a result, multiple entries of the same customer could appear as two distinct customers especially if they\u2019re a returning customer. Even if you analyse the data correctly, you could draw wrong and misleading conclusions from the analysis due to poor data.", "Gathering incorrect customer information is a very common issue. It can be overwhelming trying to find textual errors and anomalies in a large dataset. That being said, there\u2019s already a lot of techniques available for finding names/documents that look similar. Cosine similarity is one of such techniques. It\u2019s relatively straight forward to implement, and provides a simple solution for finding similar text.", "Because of how we usually gather customer data, incorrect customer information is a very common issue. It can be overwhelming trying to find textual errors and anomalies in a large dataset. Luckily, there\u2019s already a lot of methods available for finding names/documents that look similar. One such method is called the cosine similarity. It\u2019s relatively straight forward to implement, and provides a simple solution for finding similar text.", "As with many natural language processing (NLP) techniques, this method calculates a numerical value. Hence this technique only works with vectors.", "The idea is simple. Cosine similarity calculates a value known as the similarity by taking the cosine of the angle between two non-zero vectors. This ranges from 0 to 1, with 0 being the lowest (the least similar) and 1 being the highest (the most similar).", "To demonstrate, if the angle between two vectors is 0\u00b0, then the similarity would be 1. Conversely, if the angle between two vectors is 90\u00b0, then the similarity would be 0. For two vectors with an angle greater than 90\u00b0, then we also consider those to be 0.", "However, it\u2019s important to note cosine similarity does not consider the magnitude of the vectors. Only the angle between the two vectors matter. This can be useful in certain contexts.", "For example, let\u2019s say we\u2019re comparing two documents where each dimension of their respective vector represents a unique word. If one document is much larger in size and has a much higher frequency of a particular word, they could still be considered similar.", "We can do the same thing when comparing names instead of words. We just need to have each vector dimension representing a character instead. Say we had two names with a similar spelling but are different in character length, like Conor and Connor, then this would still result in a high similarity score.", "The mathematical definition of the cosine similarity is as follows:", "where \u03b8 is the angle between two non-zero vectors A and B. Although the formula may look complicated, it\u2019s simply a rearrangement of the definition of the dot product of two vectors. We take the dot product of vectors A and B and divide it by the product of the magnitude of both vectors. Each i represents a dimension of the vectors; in this context, it could be letters or words. If there\u2019s no matching letter or word, then we disregard those (i.e. consider them as 0).", "Our first task is to define a function that can take words as inputs and convert them into a vector representation. The code will be in Python 3, but feel free to use whichever language you feel most comfortable with \u2014 the underlying concept should be the same.", "What we\u2019re aiming to do is to create a function called word2vec() that\u2019ll break down a word into its character components so that we can enter this information as input for the cosine_similarity() function later.", "First, we use the Counter() method from the collections library to count the number of occurrences of each letter in our inputted word, as shown in the count_characters variable. This is just a dictionary where each key represents a character. The value of the key is the number of occurrences of that character.", "We then get the set of characters used in our inputted word by using the set() function so that we can get the intersection of characters between two words, and we\u2019ll name that variable set_characters . We need this for later when we\u2019re calculating the cosine similarity.", "The final part is to calculate the \u2018length\u2019 of the word. This is the magnitude of the word vector as shown previously in the cosine similarity formula. Remember that the reason we calculate the length this way is because we\u2019re representing these words as vectors, where each used character represents a dimension of the vector.", "These are all returned as a tuple so that we can easily extract the information. We also add the input word to the end of the output tuple to easily figure out what the original word was if we wanted to use the function to iterate a list of names.", "We can now create the cosine_similarity() function, as shown below. The inputs here will be the outputs of the word2vec() function; that is, vector1 and vector2 are calculated by applying the word2vec() function.", "First, we need to get a list of characters that are common to both words by using the intersection() function, and we call this variable common_characters .", "Referring back to the formula, we then have to calculate the dot product. This is done by iterating through the list of common_characters and getting the count value of each iterated character, as seen in the product_summation variable.", "Then, we calculate the product of the lengths of both vectors and call this variable length (looking back, I probably should have used a less confusing variable name).", "We\u2019re now ready to plug everything into the formula. However, cosine similarity only allows non-zero vectors due to the division involved. Hence, we need to add a condition that if length = 0 , then the similarity is 0; otherwise, calculate the cosine similarity as usual. The round() function is also used, and this is where we make use of the ndigits argument to round the similarity score to the desired number of decimal places.", "With the necessary functions created, we can put everything together to create a function called find_similar() that scans through a list of names and returns a dataframe of results. The results dataframe will be a pandas one (so make sure pandas is installed and aliased as pd) and it\u2019ll consist of three columns:", "Below is the Python code for this function. It takes a list of names as input and a \u2018similarity threshold\u2019. This threshold sets the minimum required for including pairs of names in the final results dataframe. Doing this removes the uninteresting comparisons from the results. Then, like the cosine_similarity() function, there\u2019s an argument for the number of decimal places to calculate to called ndigits.", "First, we initialise a results list to store our results in. We then use the word2vec() function in a list comprehension for each name in the names list.", "Now we need a way to compare each name with every other name only once; this can be done using two for loops, and we apply the cosine_similarity() function with each iteration. Since we don\u2019t want comparisons that result in a lower similarity score than our specified input threshold, we include a if statement in the inside for loop. This basically says that we will append the similarity score to the results_list if it\u2019s above the threshold.", "We then convert the list into a pandas dataframe. At the end, there\u2019s an if statement to check if the dataframe is empty. I personally haven\u2019t added anything in the else statement for this, but an error could be added here for empty results if desired. It then returns the final dataframe as the output.", "Say we had 4 names: Conor, Connor, Jon, and John. We can calculate the cosine similarity by comparing each name with every other name. Now that we\u2019ve got the functions ready, it\u2019s just a simple case of plugging everything in!", "First, we\u2019ll create a list of names.", "hen, we need to decide on the similarity threshold and the number of decimal places we want. Let\u2019s use a similarity threshold of 0 for now. Doing this will give every comparison pair, and we\u2019ll also set the desired decimal place to be 3.", "Let\u2019s put these in and see what we get.", "Immediately, we should notice that we\u2019ve got 6 comparisons. This is what we\u2019d expect since the number of unique pairs in a set of N items is N*(N-1)/2. We can also see that the pairs with high similarity scores are the Conor and Connor pair, as well as the Jon and John pair, which is also what we\u2019d expect.", "If we changed the similarity threshold to 0.8, then we should expect only the mentioned pairs to remain. Let\u2019s give that a go.", "Yep, as we expected. However, we can do an additional check by using names where we already know what the outcome will be. For example, we know that if we compared exactly the same names, then we\u2019d expect a similarity score of 1. Conversely, if we compared names that were completely different (no common characters at all), then we\u2019d expect a similarity score of 0.", "Let\u2019s do the same by using a different list of names. Say we had 2 names: Connor and Lee. Now we can create a new names list.", "If we run the find_similar() function using this new names list with a similarity threshold of 0 and the same number of decimal places, then we get the following result.", "Check passed! This is exactly the scores we\u2019d expect. If we wanted to, we can use this as a check when unit testing, but I won\u2019t delve into that. This just gives us some confidence that things are working as intended.", "We\u2019ve seen that it can be quite simple to find names that are similar by converting them into vectors and calculating the cosine similarity. If we applied this to real customer data, we could also apply additional checks on other attributes such as gender, date of birth, etc. for when we do find names that are similar. For example, Leon and Leona would achieve a high similarity score, but Leon would most likely be a male whereas Leona would most likely be a female; if this is the case, it\u2019s unlikely that the similarity is due to an error like a typo. We want to exclude these. Having some additional checks like this would make it less likely to return such comparisons.", "We also might not want to include those that achieve a similarity score of 1 since it\u2019s possible for people to have the same name. Again, we could add some checks on other attributes like gender and date of birth. For example, if there are two Leon records, we can check whether they have the same gender and date of birth. If they do, then it\u2019s likely a duplicate record. Regardless, whether or not you want to include similarity scores of 1 can easily be changed in the if statement in the find_similar() function.", "The word2vec() function also doesn\u2019t take into account anagrams, as some readers have pointed out. You could fix this by looking at pairs (or more) of letters across each word like a sliding window. If we\u2019re comparing the inspector with inspectar and inceptors, the algorithm would consider inspector to be more similar to inspectar. This is because even though inceptors is an anagram of inspector, the order of the", "Lastly, we might want to add a process before applying the find_similar() function to clean the names first, such as removing capital letters and hyphens, concatenating the first and last names of each customer, etc.", "And that\u2019s it! Hopefully, this provides a simple starting point for finding possible errors in a dataset based on name. If anybody has any other ideas on how to improve this, or want to discuss on how to implement it in a real setting or just have general queries about the code, please feel free to contact me.I\u2019d love to have a chat!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data science, machine learning, and the boring bits in between | youtube.com/c/LeonLokk | leonlok.co.uk/newsletter"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa99eb943e1ab&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://leonlok.medium.com/?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": ""}, {"url": "https://leonlok.medium.com/?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "Leon Lok"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F36167b835f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&user=Leon+Lok&userId=36167b835f0&source=post_page-36167b835f0----a99eb943e1ab---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa99eb943e1ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa99eb943e1ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@vsmilelx?utm_source=medium&utm_medium=referral", "anchor_text": "\u6d6e\u840d \u95ea\u7535"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral", "anchor_text": "chuttersnap"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@joaosilas?utm_source=medium&utm_medium=referral", "anchor_text": "Jo\u00e3o Silas"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a99eb943e1ab---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----a99eb943e1ab---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/analytics?source=post_page-----a99eb943e1ab---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/tag/data?source=post_page-----a99eb943e1ab---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----a99eb943e1ab---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa99eb943e1ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&user=Leon+Lok&userId=36167b835f0&source=-----a99eb943e1ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa99eb943e1ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&user=Leon+Lok&userId=36167b835f0&source=-----a99eb943e1ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa99eb943e1ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa99eb943e1ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a99eb943e1ab---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a99eb943e1ab--------------------------------", "anchor_text": ""}, {"url": "https://leonlok.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://leonlok.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Leon Lok"}, {"url": "https://leonlok.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "253 Followers"}, {"url": "http://youtube.com/c/LeonLokk", "anchor_text": "youtube.com/c/LeonLokk"}, {"url": "http://leonlok.co.uk/newsletter", "anchor_text": "leonlok.co.uk/newsletter"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F36167b835f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&user=Leon+Lok&userId=36167b835f0&source=post_page-36167b835f0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbf82098bdd77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffinding-similar-names-using-cosine-similarity-a99eb943e1ab&newsletterV3=36167b835f0&newsletterV3Id=bf82098bdd77&user=Leon+Lok&userId=36167b835f0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}