{"url": "https://towardsdatascience.com/linear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0", "time": 1683017030.445473, "path": "towardsdatascience.com/linear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0/", "webpage": {"metadata": {"title": "Linear regression and gradient descent for absolute beginners | by Lily Chen | Towards Data Science", "h1": "Linear regression and gradient descent for absolute beginners", "description": "*Note: I used \u201cpredict/prediction\u201d in this article. However, a reader pointed out in the comment below that the correct terminology is \u201cestimate/estimation.\u201d Before we dig into gradient descent\u2026"}, "outgoing_paragraph_urls": [{"url": "https://datascience.stackexchange.com/questions/52157/why-do-we-have-to-divide-by-2-in-the-ml-squared-error-cost-function", "anchor_text": "this", "paragraph_index": 31}, {"url": "https://www.ritchieng.com/one-variable-linear-regression/", "anchor_text": "link", "paragraph_index": 33}, {"url": "https://www.linkedin.com/in/yilingchen405/", "anchor_text": "https://www.linkedin.com/in/yilingchen405/", "paragraph_index": 50}, {"url": "https://lilychencodes.com/", "anchor_text": "https://lilychencodes.com/", "paragraph_index": 52}], "all_paragraphs": ["Let\u2019s say we have a fictional dataset of pairs of variables, a mother and her daughter\u2019s heights:", "Given a new mother height, 63, how do we predict* her daughter\u2019s height?", "The way you do it is by linear regression.", "First, you find the line of best fit. Then you use that line for your prediction*.", "*Note: I used \u201cpredict/prediction\u201d in this article. However, a reader pointed out in the comment below that the correct terminology is \u201cestimate/estimation.\u201d", "Linear regression is about finding the line of best fit for a dataset. This line can then be used to make predictions.", "How do you find the line of best fit?", "This is where gradient descent comes in.", "Gradient descent is a tool to arrive at the line of best fit", "Before we dig into gradient descent, let\u2019s first look at another way of computing the line of best fit.", "Statistics way of computing line of best fit:", "A line can be represented by the formula: y = mx + b.", "The formula for slope m of the regression line is:", "Translation: correlation coefficient between x and y values (r), multiplied by the standard deviation of y values (SD of y) divided by standard deviation of x values (SD of x).", "The standard deviation of mothers\u2019 heights in the data above is approximately 4.07. The standard deviation of daughters\u2019 heights is approximately 5.5. The correlation coefficient between these two sets of variable is about 0.89.", "So the line of best fit, or regression line is:", "We know that the regression line crosses the point of averages, so one point on the line is (average of x values, average of y values), or (63.5, 63.33)", "Therefore, the regression line as calculated using correlation coefficient and standard deviations is approximately:", "Now, let\u2019s dig into gradient descent.", "Gradient descent way of computing line of best fit:", "In gradient descent, you start with a random line. Then you change the parameters of the line (i.e. slope and y-intercept) little by little to arrive at the line of best fit.", "How do you know when you arrived at the line of best fit?", "For every line you try \u2014 line A, line B, line C, etc \u2014 you calculate the sum of squares of the errors. If line B has a smaller value than line A, then line B is a better fit, etc.", "Error is your actual value minus your predicted value. The line of best fit minimizes the sum of the squares of all the errors. In linear regression, the line of best fit we computed above using correlation coefficient also happens to be the least squared error line. That\u2019s why the regression line is called the LEAST SQUARE REGRESSION LINE.", "Line of best fit is the least square regression line", "In the image below, line C is better fit than line B, which is better fit than line A.", "At a high level, this is how gradient descent works:", "You start with a random line, let\u2019s say line A. You compute the sum of squared errors for that line. Then, you adjust your slope and y-intercept. You compute the sum of squared errors again for your new line. You continue adjusting until you reach a local minimum, where the sum of squared errors is the smallest and additional tweaks does not produce better result. The way you adjust your slope and intercept will be covered in more details momentarily.", "Gradient descent is an algorithm that approaches the least squared regression line via minimizing sum of squared errors through multiple iterations.", "In machine learning terminology, the sum of squared error is called the \u201ccost\u201d. This cost equation is:", "This equation is therefore roughly \u201csum of squared errors\u201d as it computes the sum of predicted value minus actual value squared.", "The 1/2mis to \u201caverage\u201d the squared error over the number of data points so that the number of data points doesn\u2019t affect the function. See this explanation for why we divide by 2.", "In gradient descent, the goal is to minimize the cost function. We do this by trying different values of slope and intercept. But which values to try and how do you go about changing those values?", "We change their values according to the gradient descent formula, which comes from taking the partial derivative of the cost function. The exact math can be found in this link.", "By taking the partial derivative, you arrive at the formula:", "This formula computes by how much you change your theta with each iteration.", "The alpha (\u03b1) is called the learning rate. The learning rate determines how big the step would be on each iteration. It\u2019s critical to have a good learning rate because if it\u2019s too large your algorithm will not arrive at the minimum, and if it\u2019s too small, your algorithm will take forever to get there. For my example, I picked the alpha to be 0.001", "Here\u2019s my implementation for simple linear regression using gradient descent.", "I started at 0,0 for both the slope and intercept.", "Note: In machine learning, we use theta to represent the vector [y-intercept, slope]. Theta0 = y-intercept. Theta1=slope. That\u2019s why you see theta as variable name in the implementation below.", "Using this algorithm and the dataset above for mothers and daughters heights, I got to a cost of 3.4 after 500 iterations.", "With an estimate of [0,0] as initial value for [y-intercept, slope], it\u2019s impractical to get to y = 1.2x -12.87 . To get close to that without tons and tons of iterations, you\u2019d have to start with a better estimate.", "Adjusting parameters like learning rate and starting estimate is commonplace in the world of machine learning.", "There it is, the gist of gradient descent in linear regression.", "Gradient descent is an algorithm that approaches the least squared regression line via minimizing sum of squared errors through multiple iterations.", "So far, I\u2019ve talked about simple linear regression, where you only have 1 independent variable (i.e. one set of x values). Theoretically, gradient descent can handle n number of variables.", "I\u2019ve refactored my previous algorithm to handle n number of dimensions below.", "Everything is the same, the only exception is that instead of using mx + b (i.e. slope times variable x plus y-intercept) directly to get your prediction, you do a matrix multiplication. See def get_prediction in the gist above.", "With dot product, your algorithm can take in n number of variables to compute a prediction.", "Thank you for reading! Comment below if you have questions!", "I\u2019m Lily, I\u2019m a software engineer who recently started taking machine learning courses. You can find my LinkedIn here: https://www.linkedin.com/in/yilingchen405/", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior software engineer at Datadog. I write about tech and life. Portfolio: https://lilychencodes.com/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Feef9574eadb0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://lilychencodes.medium.com/?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": ""}, {"url": "https://lilychencodes.medium.com/?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "Lily Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1190b9d7fa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&user=Lily+Chen&userId=b1190b9d7fa0&source=post_page-b1190b9d7fa0----eef9574eadb0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feef9574eadb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feef9574eadb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://datascience.stackexchange.com/questions/52157/why-do-we-have-to-divide-by-2-in-the-ml-squared-error-cost-function", "anchor_text": "this"}, {"url": "https://www.ritchieng.com/one-variable-linear-regression/", "anchor_text": "link"}, {"url": "https://algebra1course.wordpress.com/2013/02/19/3-matrix-operations-dot-products-and-inverses/", "anchor_text": "https://algebra1course.wordpress.com/2013/02/19/3-matrix-operations-dot-products-and-inverses/"}, {"url": "https://www.linkedin.com/in/yilingchen405/", "anchor_text": "https://www.linkedin.com/in/yilingchen405/"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----eef9574eadb0---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----eef9574eadb0---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----eef9574eadb0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----eef9574eadb0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----eef9574eadb0---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feef9574eadb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&user=Lily+Chen&userId=b1190b9d7fa0&source=-----eef9574eadb0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feef9574eadb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&user=Lily+Chen&userId=b1190b9d7fa0&source=-----eef9574eadb0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feef9574eadb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Feef9574eadb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----eef9574eadb0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----eef9574eadb0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----eef9574eadb0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----eef9574eadb0--------------------------------", "anchor_text": ""}, {"url": "https://lilychencodes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://lilychencodes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lily Chen"}, {"url": "https://lilychencodes.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6.9K Followers"}, {"url": "https://lilychencodes.com/", "anchor_text": "https://lilychencodes.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1190b9d7fa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&user=Lily+Chen&userId=b1190b9d7fa0&source=post_page-b1190b9d7fa0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6d2f8554dd77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0&newsletterV3=b1190b9d7fa0&newsletterV3Id=6d2f8554dd77&user=Lily+Chen&userId=b1190b9d7fa0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}