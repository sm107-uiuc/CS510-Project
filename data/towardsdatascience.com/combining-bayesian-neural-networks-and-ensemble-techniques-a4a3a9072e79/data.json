{"url": "https://towardsdatascience.com/combining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79", "time": 1683008487.649271, "path": "towardsdatascience.com/combining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79/", "webpage": {"metadata": {"title": "Combining Bayesian Neural Networks and Ensemble techniques | by Dylan Almeida | Towards Data Science", "h1": "Combining Bayesian Neural Networks and Ensemble techniques", "description": "In the field of Machine Learning, the pursuit of greater and greater accuracy drives innovation. However to make machine learning practical an equally important characteristic is to be able to define\u2026"}, "outgoing_paragraph_urls": [{"url": "https://machinelearningmastery.com/maximum-a-posteriori-estimation/", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/what-uncertainties-tell-you-in-bayesian-neural-networks-6fbd5f85648e", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc", "anchor_text": "marginalisation", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275", "anchor_text": "dropout units", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1506.02158", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1703.04977", "anchor_text": "here", "paragraph_index": 29}, {"url": "http://bayesiandeeplearning.org/2018/papers/78.pdf", "anchor_text": "here", "paragraph_index": 34}, {"url": "https://brilliant.org/wiki/gaussian-mixture-model/", "anchor_text": "here", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1905.10659", "anchor_text": "Cobb et al. 2019", "paragraph_index": 44}, {"url": "https://arxiv.org/abs/1705.07832", "anchor_text": "here", "paragraph_index": 46}, {"url": "https://math.stackexchange.com/questions/1742578/law-of-total-variance-intuition", "anchor_text": "law of total variance", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1811.12188", "anchor_text": "Pearce et al. 2018", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/1704.02030", "anchor_text": "Yao et al. 2018", "paragraph_index": 53}, {"url": "https://arxiv.org/abs/1507.02646", "anchor_text": "pareto smoothed importance sampling", "paragraph_index": 62}], "all_paragraphs": ["In the field of Machine Learning, the pursuit of greater and greater accuracy drives innovation. However to make machine learning practical an equally important characteristic is to be able to define uncertainty of a given prediction. In this article, we will discuss a variety of techniques to achieve these ends and also give practical examples in literature where a selection of these techniques have been combined with proven success. The general format is that of a Bayesian deep learning framework that seeks to unify the accuracy and robustness of ensemble predictions with the uncertainty estimates available in Bayesian modelling. We will therefore split the article up as:", "MAP Ensemble techniquesBayesian Neural NetworksRandomized MAP samplingGaussian Mixture Models", "\u2018An Ensemble of Bayesian Neural Networks for Exoplanetary Atmospheric Retrieval\u2019\u2018Uncertainty in Neural Networks: Approximately Bayesian Ensembling\u2019\u2018Using Stacking to Average Bayesian Predictive Distributions\u2019", "f\u1d42(X) \u2014 neural network modelp(W|X,Y) \u2014 posterior probability of NN weights W given data X,Yp(Y|X,W) \u2014 maximum likelihood of NN weights W from data X,Yp(W) \u2014 prior probability of NN weightsp(Y|X) \u2014 marginal probability of data ~N(\u03bc,\u03c3\u00b2 ) \u2014 normally distributed with mean \u03bc and variance \u03c3\u00b2 N \u2014 training sample size M \u2014 ensemble size(All equations shown below are taken from their respective references in the footnotes.)", "A neural network (NN) model denoted, f\u1d42(X), consists of a set of interconnected weights, W, which allow us to predict outputs, Y, from inputs X. This is done by optimizing the weights using gradient descent and backpropagation using a loss function, L. In statistical terminology, this leads to a frequentist maximum likelihood estimate (MLE) of the weights given the data, p(Y|X,W).", "Maximum a posteriori estimatesAn alternative way of using NN\u2019s is to maximize the maximum a posteriori (MAP) estimate. In other words, this is finding the parameters that best explains our data, p(W|X,Y). This can be found using Bayes Rule which states: p(W|X,Y) = p(Y|X,W)p(W)/p(Y|X). If we are optimizing for p(W|X,Y) we can ignore the normalizing p(Y|X) and restate this as optimizing for p(W|X,Y) and is equivalent to optimizing for p(Y|X,W)*p(W). We must place a prior distribution over our weights for the p(W) part which is our prior belief (if this is uniform we can observe that our MAP and MLE estimates are identical). In practice this is equivalent to training a NN with an added regularisation term to our loss function (an L2 term is equivalent to a Gaussian prior and L1 to a laplacian prior) as shown here\u00b9. This gives us our MAP point estimate predictions.", "Ensemble techniques consist of training multiple models (weak learners) to solve the same problem which then get combined to get better, more robust predictions which have a lower variance and/or lower bias. They can be largely broken down into three different types training processes: bagging, boosting and stacking.", "Bagging consists of training the same base model N different times over N different sub-datasets. These sub-datasets are generated from the initial dataset using bootstrapping sampling methods. This effectively randomly samples the initial larger dataset with replacement (making sure the size of the sampled sub-datasets are sufficiently smaller than the initial dataset). Depending on the model, the features chosen to train on from the initial dataset can also be bootstrap sampled. To make a prediction on an input, these N different models all make their respective predictions based on the input and then the resulting N different predictions get averaged.", "Boosting is similar to bagging but consists of training N (same base) models sequentially. Each consecutive model focuses on the data which the previous models predicted erroneously. The final ensemble consists of all the weak learners each appropriately weighted given their performance. Examples include adaboost and gradient boosting.", "Bagging is non-expensive as it can be trained in parallel while boosting due to its continual improvement leads to a lower bias.", "Stacking consists of training different base models whose predictions then get combined by passing them into another \u2018meta-model\u2019 which produces our final prediction. We split our initial dataset into one for training the weak learners and into another which the weak learners predict on and the meta-model trains on the weak learners predictions to finally output a single prediction.", "An excellent explanation in detail can be found here\u00b2.", "When making regression predictions in Neural Networks, the typical use case is of point estimates. However, a very useful complementary extension to this is also being able to gauge how confident or uncertain we are in our predictions. To do this we utilise what is known as Bayesian Neural Networks (BNN).", "Uncertainty typesLet us first discuss the different types of uncertainty. There are two types of uncertainty: aleatoric and epistemic. Aleatoric uncertainty arises from noisiness inherent in the dataset such that for a given x value we might get multiple y values (a distribution of y values). Aleatoric uncertainty can be split into two types: uncertainty levels stay the same across all x values (homoscedastic) or uncertainty levels vary across x values (heteroscedastic) as seen below.", "There also exists epistemic uncertainty which arises from inadequacies in our modelling. This can be improved upon with better modelling and more data.", "A full explanation of the different types of uncertainties can be found here\u00b3.", "Modelling Epistemic uncertaintyTo model epistemic uncertainty we use a BNN which we will now explain. This is similar to an NN except instead of exact parameters, our weights W are now modeled as a distribution.", "Now in regular NN\u2019s, we first initialise our weights then solve an optimisation task to get our final weights. However in BNN\u2019s (which we will also denote f\u1d42(X)), initially we place a prior distribution over our weights (usually gaussian prior), p(W). Then we perform bayesian inference to find the posterior distribution of the weights p(W|X,Y) from bayes theorem as shown below.", "Note here that p(Y|X,W) is our likelihood. To get p(Y|X) we need to average our likelihood over all possible weights (marginalisation) therefore leading to the integral.", "Obtaining the posterior distribution, p(W|X,Y), however in BNN\u2019s is often intractable and therefore we use approximation procedures to try and solve this. Variational inference is one such method. This consists of fitting a simple distribution q*(W) parametrised by \u03b8 to the posterior distribution. This therefore replaces the intractable marginalisation with an optimisation task over the parameters of the simple distribution which best minimize the KL divergence between q*(W) and the true model posterior p(W|X,Y).", "An example implementation method is dropout variational inference. Here, to approximate the posterior distribution, we set up the NN model to include dropout units\u2074 (this randomly sets weights to 0 or 1) in every weight layer. The NN is then trained normally. However for predictions, we again use dropout (known as monte carlo dropout) to randomly set some weights to 0 which lets us sample from the approximate posterior. It was shown here\u2075 that using dropout like this is equivalent to variational inference using Bernoulli distribution for q*(W). The loss function in this case would be:", "Here N is the number of data points, the probability of dropout is denoted p and \u03b8 is the simple distributions parameters. Our weights are sampled from our simple distribution W\u1d62 ~ q*(W).", "Modelling heteroscedastic aleatoric uncertaintyTo model aleatoric uncertainty, we can just use a normal NN. However instead of just predicting our outputs Y, we will predict a distribution by changing our outputs to our mean, \u03bc (which is essentially a normal NN\u2019s prediction), and our variance \u03c3\u00b2. The distributions variance, \u03c3\u00b2, gives us an idea of the level of uncertainty. For heteroscedasticity, this variance depends on the input, \u03c3(X\u1d62). We train the NN as usual with an amended loss function as:", "A weight decay L2 regularisation term may also be added.", "A key point to note is that we are just performing normal optimisation here not variational inference. This produces MAP point estimates for our models weights, W, rather than the distributions q*(W) seen previously.", "Modelling epistemic and aleatoric uncertaintyIn this case we will use the heteroscedastic NN and turn it into a BNN or in other words we model the NN\u2019s weights as a distribution. Our model needs to infer the posterior distribution of weights which maps our inputs X to outputs (prediction y/\u03bc and variance \u03c3\u00b2). We train this BNN with loss function:", "This resembles the purely aleatoric NN. D is the number of samples, y\u1d62 is the real output, y\u1d62 with a hat is the predicted output and \u03c3\u1d62 with a hat is the predicted variance. When implementing this loss function we manipulate it such that instead of \u03c3\u1d62 we predict s\u1d62 := log \u03c3\u1d62\u00b2 as:", "This is because this loss function proves more numerically stable when training since it avoids divisions by zero. The loss consists of two parts: the first part is a mean squared residual obtained with a random sample of the models output making use of uncertainty over the parameters and the second part is an uncertainty regularization. We don\u2019t need to specify uncertainty labels as the model implicitly learns this from the cost function. The second term prevents the network from predicting infinite uncertainty and therefore zero loss.", "We therefore get T samples of [y,\u03c3\u00b2] which each draw weights W~q(W) from approximate distribution q(W) and the total variance is given by:", "A good discussion of this can be found here\u2076.", "In randomized MAP sampling our aim is to try and model epistemic uncertainty using an ensemble of NN\u2019s to predict a posterior distribution. We can imagine our neural network as usual which we denote f\u1d42(X). However to find our models weights, instead of just training the NN using the normal MAP loss function (usually L2 regularised loss function), we also add noise into our loss term.", "We train multiple models with each different instance of the noisy loss function and this gives us different optimised weights for each model. When making predictions using all the models we therefore get a distribution of MAP solutions which estimate our true output distribution. Now we must think about how to add the noise into the problem which we use a markov chain monte carlo (MCMC) procedure for.", "First let us state the problem. For the sake of explanation we will assume a prior distribution and likelihood that is multivariate normal, N(\u03bc\u209a\u1d63\u1d62\u2092\u1d63,\u03a3\u209a\u1d63\u1d62\u2092\u1d63),N(\u03bc\u2097\u1d62\u2096\u2091,\u03a3\u2097\u1d62\u2096\u2091). To solve for \u03bc\u209a\u2092\u209b\u209c in this case there exists a standard solution as:", "We must add noise such that the constraint Var(\u03bc\u209a\u2092\u209b\u209c) = \u03a3\u209a\u2092\u209b\u209c is enforced or in other words the variance of the mean of our output distribution is equal to the variance of the output distribution. In practical terms we inject this noise by modelling the \u03bc\u209a\u1d63\u1d62\u2092\u1d63 (the mean of our prior distribution) as a random noisy variable \u03b8\u2080 leading to a loss function as below:", "For a sufficiently wide NN, it is shown here\u2077 that our random noisy variable fitting the constraint can be effectively modeled as \u03b8\u2080 ~ N(\u03bc\u209a\u1d63\u1d62\u2092\u1d63,\u03a3\u209a\u1d63\u1d62\u2092\u1d63).", "Gaussian mixture models (GMM) are a methodology for parametrising normally distributed subsets within an overall dataset. It is therefore a form of unsupervised learning. In our circumstances we can consider two different situations.", "An example of this would be considering a normally distributed set of weights for men and women. Given only the weight data and not the gender affiliation, we could find the overall distribution as the sum of two scaled (change in variance) and shifted (change in mean) normal distributions which is a form of GMM modeling. Our GMM (mathematical model shown below) is therefore parametrized by two types of values: its component weights and components means and variances.", "Our GMM\u2019s probability is denoted p(x). In the first line above we can see this is modeled as a weighted sum of multiple normal distributions (our components). The second line just defines the classic normal distribution formula and the third line gives us our constraints on our weights such that they add up to one.", "The task of GMM modeling is to figure out its component weights \u03d5\u1d62, component means, \u03bc\u1d62 and component variances, \u03c3\u1d62. If the component weights aren\u2019t learned they can be viewed as a prior distribution over the components as p(x generated by component C\u2096) = \u03d5\u2096. If they are learned, then they are the posterior estimates of the component probabilities given the data.", "To learn the models weights we use the maximum likelihood technique known as expectation maximization. The EM algorithms is as shown below:", "To start off the algorithm we randomly assign samples from the dataset without replacement to our component mean estimate \u03bc\u2081\u2026.\u03bc\u2096 and set all component variance estimates to the sample variances:", "We also set all component weights to a uniform distribution = 1/K where K is the number of components.", "Then in the expectation step as seen above we find the expectation of each component for each data point given its current parameters p(C\u2096|x\u1d62,\u03d5\u1d62,\u03bc\u1d62,\u03c3\u1d62). Then in the maximization step we maximize these expectations with respect to the model parameters, \u03d5\u1d62, \u03bc\u1d62 and \u03c3\u1d62.", "This concept is more fully explained here\u2078.", "An example taken from the paper Cobb et al. 2019\u2079 shows a combination of the various techniques implemented above proving successful and state-of-the-art. In the paper, they attempted to account for both epistemic and aleatoric uncertainties.", "Firstly, they used a BNN which outputted mean and covariance matrices (to model aleatoric uncertainty) with an architecture of 4 dense concrete dropout layers with each layer consisting of 1024 units. A batch size of 512 and adam optimizer was used. A negative log-likelihood loss function is used with early stopping and best checkpoints saved during training. 5 of these models were then used in an ensemble averaging process.", "Let\u2019s discuss some of the nuances. Firstly, a gaussian prior was assumed over the BNN\u2019s weights. Monte carlo dropout approximation was used for variational inference during training which allowed for wide NN layers. During the prediction phase, they forward propagated with dropout through the network T times to get T predictions forming our empirical output distribution which encapsulates our epistemic uncertainty. Concrete dropout layers are used which is a technique explained here\u00b9\u2070 which efficiently optimizes for the proportion of units to be dropped in each layer.", "This was then integrated into an ensemble of 5 models (5 chosen through empirical performance) to increase its robustness to weights initialisations and optimisation paths. As each ensemble predicts a distribution, these were combined together by using a meta Gaussian Mixture Model with each components weight equaling 1/M where M is the number of models which gave the final output distribution as \u03b8\u2091\u2099\u209b ~ N(\u03bc\u2091\u2099\u209b, \u03a3\u2091\u2099\u209b). \u03bc\u2091\u2099\u209b is found by averaging all the component means and using the law of total variance\u00b9\u00b9, \u03a3\u2091\u2099\u209b is given as:", "An example taken from the paper Pearce et al. 2018\u00b9\u2074 shows the ability of applying an ensemble method to emulate bayesian techniques. In general ensemble methods do not follow any formal bayesian modelling techniques.", "The first step is to reformulate our machine learning from an MLE to an MAP problem which leads us to a regularised loss function as:", "We then implement the randomised MAP sampling method described above in which we assume we add noise into our prior distribution leading to the anchored loss function above.", "This random MAP sampling method is run with a 100 unit single layer NN. A small number of NN\u2019s is sufficient as we perform inference in parameter space rather than output space which requires relatively fewer number of ensemble NN\u2019s. In the papers test, randomized MAP sampling proved to be good approximations of the gold standards (Gaussian Process and Hamiltonian Monte Carlo) bayesian inferences. It has a tendency to overpredict the variance but performs better in capturing uncertainty in interpolated regions when compared with mean field variational inference or MC dropout which don\u2019t account for correlations. For actual plots and results, refer to the article link above.", "This proves the validity of the method in particular its simplicity and its similarity with the normal working of NN\u2019s with the added complexity of regularising around values from a prior distribution and using ensemble techniques to resample.", "An example taken from Yao et al. 2018\u00b9\u2074 seeks to combine the stacking methodology usually used from point estimation literature with Bayesian modelling of predictive distributions and compares its usefulness with the usual Bayesian Model Averaging (BMA).", "The problem formulation is as follows, given a dataset X following a multidimensional distribution, can we then from a range of candidate models, generate a new model that would predict the output distribution, Y, as close to the true output data generating distribution as possible. This problem would consist of 3 different cases.", "As a one line summary for the most basic type of BMA, for a given data point it picks a single model from a range of candidate models that best predicts it. This however is often wasteful of knowledge and unstable and is appropriate for the M-closed case.", "Stacking uses the list of candidate models, M, and gets an LOO prediction (leave-one-out prediction: fit a model on all data points except one and predict on that) for each data point for each model. The models are then combined to make a prediction by getting a weighted sum of all the models predictions. The weights generated for each model are determined by minimizing an LOO scoring rule:", "with k iterating through the models and i iterating through the data points. This equation says we find the optimal weights which minimize the residual between a weighted sum of the models and the actual output. The prediction is then given by a weighted sum of the models:", "To convert this into stacking of predictive distributions we need to first come up with a scoring rule.", "Scoring rulesA scoring rule is some function that acts on a probability space and an integer and returns a value on how well the probability space predicts the integer. Examples of scoring rules include: Quadratic score, logarithmic score, continuous-ranked probability score, energy score and scoring rules dependent on the first and second moment of the distributions. The scores for quadratic and logarithmic scoring are given by: QS(p,y) = 2p(y) \u2014 ||p||\u2082\u00b2 and LogS(p,y) = log(p(y)).", "The generalized stacking problem can be stated as:", "Prediction C is optimal according to some scoring rule criterion between the distribution and point its trying to estimate and following constraints that the model weights should add up to 1 and the model weights must be non-negative.", "In our case we use an LOO version of the scoring rule by replacing the predictive distribution p(y\u2092\u1d64\u209c|y,M\u2096) with the LOO predictive distribution. We can do this as for a sample size \u2192 \u221e, the L2 error between the score of the LOO predictive distribution and the full predictive distribution tends to zero therefore proving stacking gives the optimal combination of weights asymptotically. In practice however LOO is still to onerous and so PSIS-LOO (pareto smoothed importance sampling\u00b9\u2074) is used.", "Taking an example, if we were to apply this to a situation such as trying to predict a true underlying model of the form y ~ N(3.4,1) with 8 candidate solutions of the form N(\u03bc\u2096,1) with 1 \u2264 k \u2264 8. BMA would choose a single model at k = 3 and stacking would always give us 1/n \u03a3\u207f\u1d62\u208c\u2081 y\u1d62. For a stacking of predictive distributions result we need to optimise our weights according to the loss and constraints:", "The paper shows that this wholly outperforms a mean squared error trained point estimate prediction. This shows that two distributions close in KL divergence are close in each moment while the reverse is not necessarily true. This illustrates the necessity of matching the distributions rather than matching moments (such as the means).", "Overall, this article is trying to show the theoretical possibility and practical plausibility of a different approach in tackling Machine Learning problems.", "Graduated Aeronautical Engineer from Imperial College London, working as Data Scientist. Enthusiastic about the combination of Mathematics, Engineering and ML."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa4a3a9072e79&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@d.a.dylanalmeida?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@d.a.dylanalmeida?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Dylan Almeida"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8390b5f7850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=post_page-e8390b5f7850----a4a3a9072e79---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa4a3a9072e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=-----a4a3a9072e79---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4a3a9072e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&source=-----a4a3a9072e79---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://machinelearningmastery.com/maximum-a-posteriori-estimation/", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/what-uncertainties-tell-you-in-bayesian-neural-networks-6fbd5f85648e", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc", "anchor_text": "marginalisation"}, {"url": "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275", "anchor_text": "dropout units"}, {"url": "https://arxiv.org/abs/1506.02158", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1703.04977", "anchor_text": "here"}, {"url": "http://bayesiandeeplearning.org/2018/papers/78.pdf", "anchor_text": "here"}, {"url": "https://brilliant.org/wiki/gaussian-mixture-model/", "anchor_text": "https://brilliant.org/wiki/gaussian-mixture-model/"}, {"url": "https://brilliant.org/wiki/gaussian-mixture-model/", "anchor_text": "https://brilliant.org/wiki/gaussian-mixture-model/"}, {"url": "https://brilliant.org/wiki/gaussian-mixture-model/", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1905.10659", "anchor_text": "Cobb et al. 2019"}, {"url": "https://arxiv.org/abs/1705.07832", "anchor_text": "here"}, {"url": "https://math.stackexchange.com/questions/1742578/law-of-total-variance-intuition", "anchor_text": "law of total variance"}, {"url": "https://arxiv.org/abs/1811.12188", "anchor_text": "Pearce et al. 2018"}, {"url": "https://arxiv.org/abs/1704.02030", "anchor_text": "Yao et al. 2018"}, {"url": "https://arxiv.org/abs/1507.02646", "anchor_text": "pareto smoothed importance sampling"}, {"url": "https://machinelearningmastery.com/maximum-a-posteriori-estimation/", "anchor_text": "https://machinelearningmastery.com/maximum-a-posteriori-estimation/"}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205"}, {"url": "https://towardsdatascience.com/what-uncertainties-tell-you-in-bayesian-neural-networks-6fbd5f85648e", "anchor_text": "https://towardsdatascience.com/what-uncertainties-tell-you-in-bayesian-neural-networks-6fbd5f85648e"}, {"url": "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275", "anchor_text": "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Gal%2C+Y", "anchor_text": "Y. Gal"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Ghahramani%2C+Z", "anchor_text": "Z. Ghahramani"}, {"url": "https://arxiv.org/abs/1506.02158", "anchor_text": "https://arxiv.org/abs/1506.02158"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Kendall%2C+A", "anchor_text": "A. Kendall"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Gal%2C+Y", "anchor_text": "Y. Gal"}, {"url": "https://arxiv.org/abs/1703.04977", "anchor_text": "https://arxiv.org/abs/1703.04977"}, {"url": "http://bayesiandeeplearning.org/2018/papers/78.pdf", "anchor_text": "http://bayesiandeeplearning.org/2018/papers/78.pdf"}, {"url": "https://brilliant.org/profile/john-9143r5/about/", "anchor_text": "J. McGonagle"}, {"url": "https://brilliant.org/profile/geoff-bfbkja/", "anchor_text": "G. Pilling"}, {"url": "https://brilliant.org/profile/andrei-vjp2fw/about/", "anchor_text": "A. Dobre"}, {"url": "https://brilliant.org/wiki/gaussian-mixture-model/", "anchor_text": "https://brilliant.org/wiki/gaussian-mixture-model/"}, {"url": "https://arxiv.org/abs/1905.10659", "anchor_text": "https://arxiv.org/abs/1905.10659"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Gal%2C+Y", "anchor_text": "Y. Gal"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Hron%2C+J", "anchor_text": "J. Hron"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Kendall%2C+A", "anchor_text": "A. Kendall"}, {"url": "https://arxiv.org/abs/1705.07832", "anchor_text": "https://arxiv.org/abs/1705.07832"}, {"url": "https://math.stackexchange.com/questions/1742578/law-of-total-variance-intuition", "anchor_text": "Law of total variance intuition"}, {"url": "https://math.stackexchange.com/questions/1742578/law-of-total-variance-intuition", "anchor_text": "https://math.stackexchange.com/questions/1742578/law-of-total-variance-intuition"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Pearce%2C+T", "anchor_text": "T. Pearce"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Zaki%2C+M", "anchor_text": "M. Zaki"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Neely%2C+A", "anchor_text": "A. Neely"}, {"url": "https://arxiv.org/abs/1811.12188", "anchor_text": "https://arxiv.org/abs/1811.12188"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Yao%2C+Y", "anchor_text": "Y. Yao"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Vehtari%2C+A", "anchor_text": "A. Vehtari"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Simpson%2C+D", "anchor_text": "D. Simpson"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Gelman%2C+A", "anchor_text": "A. Gelman"}, {"url": "https://arxiv.org/abs/1704.02030", "anchor_text": "https://arxiv.org/abs/1704.02030"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Vehtari%2C+A", "anchor_text": "A. Vehtari"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Simpson%2C+D", "anchor_text": "D. Simpson"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Gelman%2C+A", "anchor_text": "A. Gelman"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Yao%2C+Y", "anchor_text": "Y. Yao"}, {"url": "https://arxiv.org/search/stat?searchtype=author&query=Gabry%2C+J", "anchor_text": "J. Gabry"}, {"url": "https://arxiv.org/abs/1507.02646", "anchor_text": "https://arxiv.org/abs/1507.02646"}, {"url": "https://medium.com/tag/deeplearing?source=post_page-----a4a3a9072e79---------------deeplearing-----------------", "anchor_text": "Deeplearing"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----a4a3a9072e79---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/ensemble-learning?source=post_page-----a4a3a9072e79---------------ensemble_learning-----------------", "anchor_text": "Ensemble Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a4a3a9072e79---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/random-forest?source=post_page-----a4a3a9072e79---------------random_forest-----------------", "anchor_text": "Random Forest"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa4a3a9072e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=-----a4a3a9072e79---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa4a3a9072e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=-----a4a3a9072e79---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4a3a9072e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@d.a.dylanalmeida?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8390b5f7850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=post_page-e8390b5f7850----a4a3a9072e79---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe8390b5f7850%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=-----a4a3a9072e79---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@d.a.dylanalmeida?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Written by Dylan Almeida"}, {"url": "https://medium.com/@d.a.dylanalmeida/followers?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "33 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8390b5f7850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=post_page-e8390b5f7850----a4a3a9072e79---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe8390b5f7850%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79&user=Dylan+Almeida&userId=e8390b5f7850&source=-----a4a3a9072e79---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a4a3a9072e79----0---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a4a3a9072e79----0---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a4a3a9072e79----0---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a4a3a9072e79----0---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a4a3a9072e79----0---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a4a3a9072e79----0---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----0-----------------clap_footer----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a4a3a9072e79----0---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----a4a3a9072e79----0-----------------bookmark_preview----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a4a3a9072e79----1---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----a4a3a9072e79----1---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----a4a3a9072e79----1---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a4a3a9072e79----1---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a4a3a9072e79----1---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a4a3a9072e79----1---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a4a3a9072e79----1---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----a4a3a9072e79----1-----------------bookmark_preview----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a4a3a9072e79----2---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a4a3a9072e79----2---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a4a3a9072e79----2---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a4a3a9072e79----2---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a4a3a9072e79----2---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a4a3a9072e79----2---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a4a3a9072e79----2---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----a4a3a9072e79----2-----------------bookmark_preview----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----a4a3a9072e79----3---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----a4a3a9072e79----3---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----a4a3a9072e79----3---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a4a3a9072e79----3---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----a4a3a9072e79----3---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "Time-Series Forecasting: Deep Learning vs Statistics \u2014 Who Wins?A comprehensive guide on the ultimate dilemma"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----a4a3a9072e79----3---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": "\u00b714 min read\u00b7Apr 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----c568389d02df----3-----------------clap_footer----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----a4a3a9072e79----3---------------------ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&source=-----a4a3a9072e79----3-----------------bookmark_preview----ec9e7f28_2bd8_45ad_a2a6_49f97c0806c3-------", "anchor_text": ""}, {"url": "https://medium.com/@d.a.dylanalmeida?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "See all from Dylan Almeida"}, {"url": "https://towardsdatascience.com/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----a4a3a9072e79----0-----------------bookmark_preview----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----a4a3a9072e79----1-----------------bookmark_preview----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----0-----------------clap_footer----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----a4a3a9072e79----0---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----a4a3a9072e79----0-----------------bookmark_preview----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://zainbaq.medium.com/?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://zainbaq.medium.com/?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Zain Baquar"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Time Series Forecasting with Deep Learning in PyTorch (LSTM-RNN)An in depth tutorial on forecasting a univariate time series using deep learning with PyTorch"}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "\u00b712 min read\u00b7Feb 9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ba339885f0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c&user=Zain+Baquar&userId=d16fc4a70186&source=-----1ba339885f0c----1-----------------clap_footer----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----a4a3a9072e79----1---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ba339885f0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c&source=-----a4a3a9072e79----1-----------------bookmark_preview----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----a4a3a9072e79----2---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----a4a3a9072e79----2---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----a4a3a9072e79----2---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a4a3a9072e79----2---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----a4a3a9072e79----2---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Understanding NeRFsA massive breakthrough in scene representation"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----a4a3a9072e79----2---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "\u00b711 min read\u00b73 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----2a082e13c6eb----2-----------------clap_footer----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----a4a3a9072e79----2---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&source=-----a4a3a9072e79----2-----------------bookmark_preview----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----a4a3a9072e79----3---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----a4a3a9072e79----3---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----a4a3a9072e79----3---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Coucou Camille"}, {"url": "https://medium.com/codex?source=read_next_recirc-----a4a3a9072e79----3---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----a4a3a9072e79----3---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "Time Series Prediction Using LSTM in PythonImplementation of Machine Learning Algorithm for Time Series Data Prediction."}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----a4a3a9072e79----3---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": "\u00b76 min read\u00b7Feb 10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&user=Coucou+Camille&userId=d796c2fbb274&source=-----19b1187f580f----3-----------------clap_footer----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----a4a3a9072e79----3---------------------00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&source=-----a4a3a9072e79----3-----------------bookmark_preview----00cb6160_6cad_4aec_9baf_3e73b4d20c3f-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----a4a3a9072e79--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}