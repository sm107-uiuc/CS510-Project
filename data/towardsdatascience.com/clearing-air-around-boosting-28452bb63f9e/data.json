{"url": "https://towardsdatascience.com/clearing-air-around-boosting-28452bb63f9e", "time": 1682995434.255681, "path": "towardsdatascience.com/clearing-air-around-boosting-28452bb63f9e/", "webpage": {"metadata": {"title": "Clearing air around \u201cBoosting\u201d. Understanding the current go-to\u2026 | by Puneet Grover | Towards Data Science", "h1": "Clearing air around \u201cBoosting\u201d", "description": "Boosting is an ensemble meta-algorithm primarily for reducing bias and variance in supervised learning."}, "outgoing_paragraph_urls": [{"url": "https://github.com/PuneetGrov3r/MediumPosts/tree/master/Concepts", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Michael_Kearns_(computer_scientist)", "anchor_text": "Kearns", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Leslie_Valiant", "anchor_text": "Valiant", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Robert_Schapire", "anchor_text": "Robert Schapire", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Boosting_(machine_learning)#cite_note-Schapire90-5", "anchor_text": "paper", "paragraph_index": 6}, {"url": "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf", "anchor_text": "paper", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting#Stochastic_gradient_boosting", "anchor_text": "Stochastic Gradient Boosting", "paragraph_index": 33}, {"url": "http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf", "anchor_text": "here", "paragraph_index": 34}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.2861&rep=rep1&type=pdf", "anchor_text": "regression", "paragraph_index": 35}, {"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor Series Approximation", "paragraph_index": 43}, {"url": "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf", "anchor_text": "paper", "paragraph_index": 63}], "all_paragraphs": ["Note: Although this post is a little bit math oriented, still you can understand core working of Boosting and Gradient Boosting by only reading first two sections i.e. Introduction and History. Sections after that are explanation of different Gradient Boosting algorithm\u2019s papers.", "This is one of the post from my posts from Concepts category, which can be found on my github repo here.", "Boosting is an ensemble meta-algorithm primarily for reducing bias and variance in supervised learning.", "Boosting algorithms, today, are one of the most used algorithms for getting state of the art results in a wide varieties of contexts/problems. And it has become a go to method for any machine learning problem or contest to get best results. Now,", "I will try to answer all these questions, and many more, through this post.", "Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \u201cCan a set of weak learners create a single strong learner?\u201d.", "Robert Schapire\u2019s affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.", "With Bagging there are a few other ensemble methods which came around the same time and you can say they are subset of modern Gradient Boosting algorithms, which are:", "Bagging is an ensemble meta-algorithm which helps in increasing stability and accuracy. It also helps in reducing variance and thus in reducing over-fitting.", "In bagging, if we have N data points and we want to make \u2018m\u2019 models, then we will take some fraction of data [mostly, (1\u20131/e)\u224863.2%] from data \u2018m\u2019 times and repeat some of the rows in them to make their length equal to N data points (though some of them are redundant). Now, we will train those \u2018m\u2019 models on these \u2018m\u2019 datasets and then get \u2018m\u2019 set of predictions. Then we aggregate those predictions to get final prediction.", "It can be used with Neural Networks, Classification and Regression trees, and subset selection in Linear Regression.", "Random forests (or random decision forests) method is an ensemble learning method that operates by constructing a multitude of decision trees. It helps reduce overfitting which is common in Decision Tree models (with high depth value).", "Random Forest combines \u201cbagging\u201d idea and random selection of features in order to construct a collection of Decision Trees to counter variance. As in bagging, we make bootstrapped training sets for decision trees, but now for each split that we make in tree generation process we only select a fraction of total number of features [mostly \u221an or log2(n)].", "These methods, bootstrapping and subset selection, makes trees more uncorrelated with each other and helps reduce variance more, and thus reduces overfitting in more generalize-able manner.", "Above methods were using averaging of mutually exclusive models in order to reduce variance. Boosting is a little bit different. Boosting is a sequential ensemble method. For \u2018n\u2019 number of total trees, we add trees predictions in a sequential method (i.e. we add second tree to improve performance of first tree or you can say to tries to right the wrong of first tree, and so on). So what we do is, we subtract the prediction of first model multiplied by a constant (0<\u03bb\u22641) from the target values and then taking these values as target value we fit second model, and so on. We can see it as: new models trying to correct previous models/previous model\u2019s mistakes. Boosting can be nearly summarized by one formula:", "I.e. final predictions are summation of predictions of all the models, each multiplied by a small constant (0<\u03bb\u22641). This is another way of looking at Boosting algorithm.", "So, actually we try to learn small amount of information about target from each learner (which are all weak learners) which are trying to improve upon previous models and then sum them all to get final predictions (this is possible because we are fitting only to residuals from previous models). So, each sequential learner is trying to predict:", "Each tree predictor can also have different \u03bb value based on their performance.", "In gradient boosting, we take a loss function that is evaluated at every cycle of fitting (like in Deep Learning). First paper on Gradient Boosting by Jerome H. Friedman focused on additive expansion of the final function, similar to what we saw above.", "We first predict a target value (say, \u03b3), a constant which gives least error (i.e. first prediction is F0 = \u03b3). After that we calculate gradients for every point in our dataset w.r.t. our previous output. So, we calculate gradient of error function with error calculated between prediction and sum of all outputs of previous models, w.r.t. sum of all outputs of previous models, which in case of Square Error will be:", "By this we have gradients w.r.t. all outputs of previous models.", "Why are we calculating gradients like this?", "In Neural Networks it is straightforward to calculate gradients w.r.t. all the parameters (i.e. all neural nodes) because Neural Network is just linear (or some function whose gradient is easy to calculate) combination in all layers, so its easier to calculate gradients with Backpropagation (w.r.t. all parameters) and updating them. But here we can\u2019t calculate gradient of output w.r.t. any parameters of a Decision Tree (like depth, number of leaves, split point \u2026), because its not straightforward (actually its abstract).", "J. H. Friedman used gradients of error (error function) w.r.t. every output and fitted model to those gradients.", "How can this help in getting a better model?", "What gradient w.r.t. each output represents is in which direction we should move and by how much to get to a better outcome for that particular data point/row, converting a Tree Fitting problem into Optimization Problem. We can use any kind of Loss Function, which is differentiable and which can be tailored specific to our task at hand, which was not possible with normal Boosting. For example, we can use MSE for regression tasks and Log Loss for classification tasks.", "How are gradients used here to get better results?", "Unlike in Boosting, where at ith iteration we learned some part of the output (target) and (i+1)th tree would try to predict what is left to be learned, i.e. we fitted to residuals, here in Gradient Boosting we calculate gradients w.r.t. all data points / rows, which tells us about the direction in which we want to move( negative gradients) and by how much (can be thought of as absolute value of gradient), and fit a tree on these gradients.", "These gives us regions of gradients of data points which will need similar updates as they will have similar gradients. Now we will find best value for that region which will give least error for that region. (i.e. Tree Fitting)", "After this we add these predictions to our previous predictions to get this stage\u2019s final prediction.", "Which gives us new predictions for every data point/row.", "The basic difference between normal Boosting and GradientBoosting is that, in normal Boosting we fit our next model to residuals, whereas in GradientBoosting we fit our next model to gradients of residuals. Err\u2026 GradientBoosting actually uses a Loss Function, so we fit our next model to gradient of that loss function (where we find gradients by using previous predictions).", "Current Gradient Boosting libraries does much more than this, such as:", "These methods (some of them) are implemented in Boosting algorithm called Stochastic Gradient Boosting.", "This is the first Boosting algorithm which made a huge mark in ML world. It was developed by Freund and Schapire (1997), and here is the paper.", "In addition to sequentially adding model\u2019s predictions (i.e. Boosting) it adds weights to each prediction. It was originally designed for classification problems, where they increased weights of all miss-classified examples and reduced weights of all data points classified correctly (though it can be applied to regression too). So, the next model will have to focus more on examples with more weight and less on examples with less weight.", "They also have a constant to shrink every tree\u2019s predictions, whose values is calculated during fitting and depend on error they get after fitting. More the error less is the value of constant for that tree. This makes predictions more accurate as we are learning less from less accurate models and learning more from more accurate learners.", "It was originally for two class classification and chose outputs as {-1, +1}, where:", "4. Finally current round\u2019s predictions are added to previous predictions after multiplying with \u03b1. As it was originally to classify as +1 or -1, it takes sign of predictions after last round as final prediction:", "XGBoost tries to improve upon previous Gradient Boosting algorithms, to give better, faster and more generalizable results.", "It uses some different and some new additions to Gradient Boosting, such as:", "As in many other objective function\u2019s implementations, here it proposes to add an extra function to the loss function to penalize complexity of the model(like in LASSO, Ridge, etc), known as the regularization term. This helps models not to overfit the data.", "Now for Gradient Boosting, which is additive in nature, we can write our \u2018t\u2019 th prediction as F_t(x) = F_t-1(x) + y_hat_t(x), i.e. this rounds predictions plus sum of all previous predictions. Loss Function:", "which after Taylor Series Approximation can be written as:", "Gradient Boosting with only first order gradient faces convergence issues (convergence only possible with small step sizes). And it is pretty good approximation of loss function, plus we don\u2019t want to increase our computations.", "Then it finds optimal value of loss function by putting gradient of loss function equal to zero and thus finding a function via Loss Function for making a split. (i.e. function for change in loss after split)", "It also adds shrinkage for every tree, to decrease influence of one particular tree, and subsampling of columns to combat overfitting and decrease variance as discussed in History section.", "Gradient Boosting algorithms goes through all possible splits to find the best split at that level. However, this can be a expensive bottleneck if our data is very large, so many algorithms use some kind of approximation or some other trick to find, not best but, a particularly good split. So, XGBoost looks at a particular feature\u2019s distribution and selects some percentiles (or quantiles) as splitting points.", "It proposes to selects a value, lets call it q, now from quantile range of [0, 100] nearly every qth quantile value is selected as candidate split point for splitting. There will be roughly 100/q candidate points.", "It has also added Sparsity Aware split finding, which can be helpful in sparse BigData arrays.", "d) For speed enhancement and space efficiency:", "It proposes to divide data into blocks, in-memory blocks. Data in each block is stored in Compressed Column (CSC) format, in which each column is stored by corresponding feature value. So, a linear search of the column in block is sufficient to get all split points of that column, for that block.", "Block format makes it easy to find all splits in linear time, but when it is turn to get gradient statistics for those points, it becomes a non-continuous fetches of gradient statistics (because gradients are still in previous format, where block-values have pointer to their gradient) which can lead to cache-misses. To overcome this problem, they made a Cache Aware algorithm for gradient accumulations. In it, every thread is given an internal buffer. This buffer is used to get gradients in mini-batch manner and accumulate them, in contrast to accessing some gradient from here and then some gradient from there in order.", "Finding the best size of block is also a problem which can help use parallelism the best and reduces cache-misses the most.", "It also proposes something called Block Sharding. It writes data on multiple disks alternatively (if you have those). So, when it wants to read some data, this setup can help read multiple blocks at the same time. For example if you have 4 disks, then those four disks can read 4 blocks in one unit time, giving 4x speedup.", "This paper has proposed two techniques to speed up the overall Boosting process.", "For first one, it proposes a method in which they won\u2019t have to use all data points for a particular model, without loosing much information gain. It\u2019s named Gradient Based One Side Sampling (GOSS). In it, they calculate gradients of the loss function and then sort them by their absolute value. It also has a proof to prove that values having larger value of gradient contribute more to information gain, so it proposes to ignore many data points with low gradient for that particular model.", "So, take some fraction of top gradients and a different fraction from remaining (randomly from remaining gradients) for a particular model, with some low weight applied to random set of gradients as they have lower value of gradients and should not contribute much to our current model.", "One thing to note here is that LightGBM uses histogram-based algorithms, which bucket continuous feature values into discrete bins. This speeds up training and reduces memory usage.", "Also, they use a different kind of Decision Tree which optimizes leaf wise instead of depth wise that normal Decision Tree does. (i.e. it enumerates all possible leaves and selects the one with least error)", "For the second one, it proposes a method in which to combine many features to make one new feature, thus reducing dimentionality of the data without much information loss. This method is called Exclusive Feature Bundling (EFB). It says that, in world of High Dimentional data there are a lot of columns that are mutually exclusive. How? As, high dimentional data has many columns which are highly sparse, there can be many columns present in data which are not taking any value at the same time (i.e. only one of them is taking a non-zero value most of the time, i.e. mutually exclusive). So, they have proposed to bundle such features into one, which don\u2019t have conflict above some pre-specified value (i.e. they don\u2019t have some non-zero value at same data point for many points. i.e. they are not fully mutually-exclusive, but mutually-exclusive till some level). Still to distinguish each value from different features, it proposes to add a different constant to values coming from different features, so values from one feature will be in one particular range and values from other features will not be in that range. For example, say we have 3 features to combine and all are between 0\u2013100. So, we will add 100 to second feature and 200 to third feature to get three ranges for 3 features, equal to [0, 100), [100, 200) and [200, 300). And in tree based model this is acceptable, as it won\u2019t affect information gain by splitting.", "Making these bundles is actually NP-Hard problem and is similar to Graph Coloring problem which is also NP-Hard. So, as in Graph Coloring problem, it has opted for a good approximation algorithm instead of an optimal solution.", "Although these two methods were the main highlight of this paper, it also provide those improvements to Gradient Boosting algorithms, like sub-sampling, max_depth, learning_rate, num_leaves etc., which we discussed above, in their package.", "Overall this one is a quite mathematical paper. If you are interested in proofs, you should look into this paper.", "This paper focuses on one of the problem with Boosting suffers, i.e. leakage, target leakage. In Boosting, fitting of many models on training examples relies on target values (for calculating residuals). This leads to shift in target values in test set, i.e. prediction shift. So, it proposes a method to bypass this problem.", "Plus, it also proposes a method for converting categorical features into target statistics (TS) (which can lead to target leakage if done wrong).", "It has proposed an algorithm called Ordered Boosting which helps in preventing target leakage, and an algorithm for processing categorical features. Though both uses something called Ordering Principle.", "Firstly, for converting categorical features into target statistics (TS). If you know about mean encoding or target encoding of categorical features, specially K-Fold mean encoding, it will be easy to understand as this is just a little twist to that. What they did to avoid target leakage, but still be able to do target encoding, is that for i-th element they took (i-1) elements above it, in dataset, to get feature value for this element (i.e. if 7 elements are above i-th element, of same category as the i-th element, then they took mean of target for those values to get feature value for i-th element).", "Secondly, for making algorithm prediction shift proof, it has proposed an algorithm which they named Ordered Boosting. At every iteration, it samples a new dataset D_t independently and obtain unshifted residuals (as this is sufficiently/somewhat(depends) different dataset) by applying current model to this dataset and fit a new model. Practically, they add new data points to previous points, so, it gives unshifted residuals atleast for the new data points which are added in current iteration.", "With this algorithm we can make \u2019n\u2019 models if there are \u2019n\u2019 examples. But we only make log_2(n) models, for time considerations. So, by this, first model is fitted to 2 examples, then second is fitted to 4 and so on.", "CatBoost too uses a different kind of Decision Tree, called Oblivious Trees. In such trees the same splitting criterion is used across an entire level of the tree. Such trees are balanced and less prone to overfitting.", "In oblivious trees each leaf index can be encoded as a binary vector with length equal to the depth of the tree. This fact is widely used in CatBoost model evaluator: it first binarizes all float features and all one-hot encoded features, and then uses these binary features to calculate model predictions. This helps in predicting at very fast speed."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F28452bb63f9e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://techelfpuneet.medium.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Puneet Grover"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfddda6ba487&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&user=Puneet+Grover&userId=dfddda6ba487&source=post_page-dfddda6ba487----28452bb63f9e---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28452bb63f9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&user=Puneet+Grover&userId=dfddda6ba487&source=-----28452bb63f9e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28452bb63f9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&source=-----28452bb63f9e---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@spacex?utm_source=medium&utm_medium=referral", "anchor_text": "SpaceX"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/PuneetGrov3r/MediumPosts/tree/master/Concepts", "anchor_text": "here"}, {"url": "https://nbviewer.jupyter.org/github/PuneetGrov3r/MediumPosts/blob/master/Concepts/Boosting.ipynb", "anchor_text": "ClearingAirAroundBoosting"}, {"url": "https://unsplash.com/@andriklangfield?utm_source=medium&utm_medium=referral", "anchor_text": "Andrik Langfield"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Michael_Kearns_(computer_scientist)", "anchor_text": "Kearns"}, {"url": "https://en.wikipedia.org/wiki/Leslie_Valiant", "anchor_text": "Valiant"}, {"url": "https://en.wikipedia.org/wiki/Robert_Schapire", "anchor_text": "Robert Schapire"}, {"url": "https://en.wikipedia.org/wiki/Boosting_(machine_learning)#cite_note-Schapire90-5", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/Bootstrapping_(statistics)", "anchor_text": "Bootstrapping"}, {"url": "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting#Stochastic_gradient_boosting", "anchor_text": "Stochastic Gradient Boosting"}, {"url": "https://unsplash.com/@mehrshadr?utm_source=medium&utm_medium=referral", "anchor_text": "Mehrshad Rajabi"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf", "anchor_text": "here"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.2861&rep=rep1&type=pdf", "anchor_text": "regression"}, {"url": "https://unsplash.com/@viktortheo?utm_source=medium&utm_medium=referral", "anchor_text": "Viktor Theo"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor Series Approximation"}, {"url": "https://unsplash.com/@sdmk?utm_source=medium&utm_medium=referral", "anchor_text": "Severin D."}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf", "anchor_text": "paper"}, {"url": "https://unsplash.com/@alexiby?utm_source=medium&utm_medium=referral", "anchor_text": "Alex Iby"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://statweb.stanford.edu/~tibs/ElemStatLearn/download.html", "anchor_text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"}, {"url": "https://en.wikipedia.org/wiki/Boosting_(machine_learning)", "anchor_text": "Wikipedia \u2014 Boosting"}, {"url": "http://statweb.stanford.edu/~tibs/ElemStatLearn/download.html", "anchor_text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"}, {"url": "http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf", "anchor_text": "A Short Introduction to Boosting \u2014 Yoav Freund, Robert E. Schapire"}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "XGBoost: A Scalable Tree Boosting System \u2014 Tianqi Chen, Carlos Guestrin (2016)"}, {"url": "https://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal", "anchor_text": "Stack Exchange \u2014 Need help understanding XGBoost\u2019s appropriate split points proposal"}, {"url": "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf", "anchor_text": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree \u2014 Ke, Meng et al."}, {"url": "https://arxiv.org/pdf/1706.09516.pdf", "anchor_text": "CatBoost: unbiased boosting with categorical features \u2014 Prokhorenkova, Gusev et al."}, {"url": "http://learningsys.org/nips17/assets/papers/paper_11.pdf", "anchor_text": "CatBoost: gradient boosting with categorical features support \u2014 Dorogush, Ershov, Gulin"}, {"url": "https://arxiv.org/pdf/1609.05610.pdf", "anchor_text": "Enhancing LambdaMART Using Oblivious Trees \u2014 Modr\u00b4y, Ferov (2016)"}, {"url": "https://www.youtube.com/watch?v=8o0e-r0B5xQ", "anchor_text": "CatBoost \u2014 the new generation of gradient boosting \u2014 Anna Veronika Dorogush"}, {"url": "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/", "anchor_text": "Gentle Intro to Gradient Boosting Algos"}, {"url": "https://medium.com/tag/boosting?source=post_page-----28452bb63f9e---------------boosting-----------------", "anchor_text": "Boosting"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----28452bb63f9e---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/lightgbm?source=post_page-----28452bb63f9e---------------lightgbm-----------------", "anchor_text": "Lightgbm"}, {"url": "https://medium.com/tag/catboost?source=post_page-----28452bb63f9e---------------catboost-----------------", "anchor_text": "Catboost"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----28452bb63f9e---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28452bb63f9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&user=Puneet+Grover&userId=dfddda6ba487&source=-----28452bb63f9e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28452bb63f9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&user=Puneet+Grover&userId=dfddda6ba487&source=-----28452bb63f9e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28452bb63f9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfddda6ba487&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&user=Puneet+Grover&userId=dfddda6ba487&source=post_page-dfddda6ba487----28452bb63f9e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8beb028904ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&newsletterV3=dfddda6ba487&newsletterV3Id=8beb028904ce&user=Puneet+Grover&userId=dfddda6ba487&source=-----28452bb63f9e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Written by Puneet Grover"}, {"url": "https://techelfpuneet.medium.com/followers?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "408 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfddda6ba487&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&user=Puneet+Grover&userId=dfddda6ba487&source=post_page-dfddda6ba487----28452bb63f9e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8beb028904ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclearing-air-around-boosting-28452bb63f9e&newsletterV3=dfddda6ba487&newsletterV3Id=8beb028904ce&user=Puneet+Grover&userId=dfddda6ba487&source=-----28452bb63f9e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1?source=author_recirc-----28452bb63f9e----0---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=author_recirc-----28452bb63f9e----0---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=author_recirc-----28452bb63f9e----0---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Puneet Grover"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----28452bb63f9e----0---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1?source=author_recirc-----28452bb63f9e----0---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Speed Up your Algorithms Part 2\u2014 NumbaGet C++/Fortran like speed for your functions with Numba"}, {"url": "https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1?source=author_recirc-----28452bb63f9e----0---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "8 min read\u00b7Oct 12, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F293e554c5cc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeed-up-your-algorithms-part-2-numba-293e554c5cc1&user=Puneet+Grover&userId=dfddda6ba487&source=-----293e554c5cc1----0-----------------clap_footer----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1?source=author_recirc-----28452bb63f9e----0---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F293e554c5cc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeed-up-your-algorithms-part-2-numba-293e554c5cc1&source=-----28452bb63f9e----0-----------------bookmark_preview----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----28452bb63f9e----1---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----28452bb63f9e----1---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----28452bb63f9e----1---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----28452bb63f9e----1---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----28452bb63f9e----1---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----28452bb63f9e----1---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----28452bb63f9e----1---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----28452bb63f9e----1-----------------bookmark_preview----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----28452bb63f9e----2---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----28452bb63f9e----2---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----28452bb63f9e----2---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----28452bb63f9e----2---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----28452bb63f9e----2---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----28452bb63f9e----2---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----28452bb63f9e----2---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----28452bb63f9e----2-----------------bookmark_preview----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef?source=author_recirc-----28452bb63f9e----3---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=author_recirc-----28452bb63f9e----3---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=author_recirc-----28452bb63f9e----3---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Puneet Grover"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----28452bb63f9e----3---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef?source=author_recirc-----28452bb63f9e----3---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "Speeding up your Algorithms Part 4\u2014 DaskRun your Pandas/Numpy/Sklearn/Python code in parallel with Dask"}, {"url": "https://towardsdatascience.com/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef?source=author_recirc-----28452bb63f9e----3---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": "10 min read\u00b7Nov 27, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7c6ed79994ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeeding-up-your-algorithms-part-4-dask-7c6ed79994ef&user=Puneet+Grover&userId=dfddda6ba487&source=-----7c6ed79994ef----3-----------------clap_footer----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef?source=author_recirc-----28452bb63f9e----3---------------------1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7c6ed79994ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeeding-up-your-algorithms-part-4-dask-7c6ed79994ef&source=-----28452bb63f9e----3-----------------bookmark_preview----1f5c55e7_fefe_4c76_9e3b_9fbd8eb7a1c0-------", "anchor_text": ""}, {"url": "https://techelfpuneet.medium.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "See all from Puneet Grover"}, {"url": "https://towardsdatascience.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----28452bb63f9e----0-----------------bookmark_preview----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Erdogan Taskesen"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "From Data to Clusters; When is Your Clustering Good Enough?Sensible clusters and hidden gems can be found using clustering approaches but you need the right cluster evaluation method!"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "\u00b717 min read\u00b75 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&user=Erdogan+Taskesen&userId=4e636e2ef813&source=-----5895440a978a----1-----------------clap_footer----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&source=-----28452bb63f9e----1-----------------bookmark_preview----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Mate Pocs"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Understanding L1 Regularisation in Gradient Boosted Decision TreesA thorough look with an example in LightGBM and R"}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "\u00b716 min read\u00b7Nov 1, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf4f0ba9d32a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a&user=Mate+Pocs&userId=686b78ddcf4b&source=-----af4f0ba9d32a----0-----------------clap_footer----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----28452bb63f9e----0---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf4f0ba9d32a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a&source=-----28452bb63f9e----0-----------------bookmark_preview----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Prof Bill Buchanan OBE"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Similarity Hashing and Perceptual HashesSean McKeown [here] and myself have just published a paper to arXiv that will be presented at DFRWS (Digital Forensics Research Conference)\u2026"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "\u00b79 min read\u00b7Dec 17, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&user=Prof+Bill+Buchanan+OBE&userId=e680fcaf274b&source=-----963fba36c8b5----1-----------------clap_footer----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----28452bb63f9e----1---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&source=-----28452bb63f9e----1-----------------bookmark_preview----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----28452bb63f9e----2---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----28452bb63f9e----2---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----28452bb63f9e----2---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----28452bb63f9e----2---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----28452bb63f9e----2---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----28452bb63f9e----2---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----28452bb63f9e----2---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----28452bb63f9e----2-----------------bookmark_preview----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://medium.com/coriers/data-engineering-vs-machine-learning-pipelines-82d0e1be410c?source=read_next_recirc-----28452bb63f9e----3---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://medium.com/@SeattleDataGuy?source=read_next_recirc-----28452bb63f9e----3---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://medium.com/@SeattleDataGuy?source=read_next_recirc-----28452bb63f9e----3---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Ben Rogojan"}, {"url": "https://medium.com/coriers?source=read_next_recirc-----28452bb63f9e----3---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "SeattleDataGuy By SeattleDataGuy"}, {"url": "https://medium.com/coriers/data-engineering-vs-machine-learning-pipelines-82d0e1be410c?source=read_next_recirc-----28452bb63f9e----3---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "Data Engineering Vs Machine Learning PipelinesWhat\u2019s the difference?"}, {"url": "https://medium.com/coriers/data-engineering-vs-machine-learning-pipelines-82d0e1be410c?source=read_next_recirc-----28452bb63f9e----3---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": "\u00b78 min read\u00b7Apr 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcoriers%2F82d0e1be410c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcoriers%2Fdata-engineering-vs-machine-learning-pipelines-82d0e1be410c&user=Ben+Rogojan&userId=41cd8f154e82&source=-----82d0e1be410c----3-----------------clap_footer----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://medium.com/coriers/data-engineering-vs-machine-learning-pipelines-82d0e1be410c?source=read_next_recirc-----28452bb63f9e----3---------------------b2830a3e_cad9_4e55_a360_3cdad2fda02e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F82d0e1be410c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcoriers%2Fdata-engineering-vs-machine-learning-pipelines-82d0e1be410c&source=-----28452bb63f9e----3-----------------bookmark_preview----b2830a3e_cad9_4e55_a360_3cdad2fda02e-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----28452bb63f9e--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}