{"url": "https://towardsdatascience.com/multi-collinearity-in-regression-fe7a2c1467ea", "time": 1683007679.572588, "path": "towardsdatascience.com/multi-collinearity-in-regression-fe7a2c1467ea/", "webpage": {"metadata": {"title": "Multicollinearity in Regression. Why it is a problem? How to track and\u2026 | by Songhao Wu | Towards Data Science", "h1": "Multicollinearity in Regression", "description": "Why Multi-Collinearity is a problem and how do we check and fix multi-collinearity"}, "outgoing_paragraph_urls": [{"url": "http://linkedin.com/in/songhaowu/", "anchor_text": "linkedin.com/in/songhaowu/", "paragraph_index": 23}], "all_paragraphs": ["Multicollinearity happens when independent variables in the regression model are highly correlated to each other. It makes it hard to interpret of model and also creates an overfitting problem. It is a common assumption that people test before selecting the variables into the regression model.", "I encountered a serious multicollinearity issue before when I built the regression model for time series data. I created multiple features based on different time periods like 1-month total return, 6-month total return, and 1-year total return to have more input variables. However, these features are highly correlated with each other. For example, if one stock has performed well for the past year, then it is very likely to have done well for the recent month. I would need to either drop some of these variables or find a way to make them less correlated. I will explain later in the article different ways to solve the problem.", "When independent variables are highly correlated, change in one variable would cause change to another and so the model results fluctuate significantly. The model results will be unstable and vary a lot given a small change in the data or model. This will create the following problems:", "Depending on the situation, it may not be a problem for your model if only slight or moderate collinearity issue occurs. However, it is strongly advised to solve the issue if severe collinearity issue exists(e.g. correlation >0.8 between 2 variables or Variance inflation factor(VIF) >20 )", "The first simple method is to plot the correlation matrix of all the independent variables.", "I used the housing data from the Kaggle competition. The goal of the competition is to use the housing data input to correctly predict the sales price. I have selected a few numerical variables to be included in my model here.", "After plotting the correlation matrix and color scaling the background, we can see the pairwise correlation between all the variables. I have included the dependent variable \u2018SalePrice\u2019 here as well. This is because it is a secret trick for me when I try to select the independent variables to be included in the model.", "When you are clueless about which variables to include in the model, just do a correlation matrix and select those independent variables with high correlation with dependent variable.", "Back to the Multi-Collinearity issue, we can see that from the correlation matrix, quite a few variables are correlated to each other. There is one pair of independent variables with more than 0.8 correlation which are total basement surface area and first-floor surface area. Houses with larger basement areas tend to have bigger first-floor areas as well and so a high correlation should be expected.", "The second method to check multi-collinearity is to use the Variance Inflation Factor(VIF) for each independent variable. It is a measure of multicollinearity in the set of multiple regression variables. The higher the value of VIF the higher correlation between this variable and the rest.", "If the VIF value is higher than 10, it is usually considered to have a high correlation with other independent variables. However, the acceptance range is subject to requirements and constraints. From the results, we can see that most features are highly correlated with other independent variables and only two features can pass the below 10 threshold.", "The most straightforward method is to remove some variables that are highly correlated to others and leave the more significant ones in the set. For example, when we plot the correlation matrix with \u2018SalePrice\u2019 included, we can see that Overall Quality and Ground living area have the two highest correlations with the dependent variable \u2018SalePrice\u2019 and thus I will try to include them in the model.", "We can see that using simple elimination, we are able to reduce the VIF value significantly while keeping the important variables. However, some of the variables like Overall Quality and Years of Built still have high VIF value and they are important in predicting housing price. How? Sometimes we can use small tricks as described in the second method later to transform the variable.", "The second method is to transform some of the variables to make them less correlated but still maintain their feature. What do I mean by this? In the housing model example, I can transfer \u2018years of built\u2019 to \u2018age of the house\u2019 by subtracting current year by years of built. For example, if the year of building is1994, then the age of the house is 2020\u20131994=26 years.", "After I convert the years of building to house age, the VIF for the new \u2018House_age\u2019 factor drops to an acceptable range and the VIF value for overall quality also drops.", "In the example of time series analysis which I mentioned at the beginning, I also converted variables to make them less correlated. For example, the total return for the past 1 month is highly correlated with the past 6 months' total return. I subtracted the past 1-month return from the past 6-month return to get the new variable on the previous 5-month return which does not include the past month. The correlation results are much more acceptable and I was able to include both variables as my model features.", "Principal Component Analysis(PCA) is commonly used to reduce the dimension of data by decomposing data into a number of independent factors. It has many applications like simplifying model calculation by reducing the number of predicting factors. However, in our case here, we will just use the character of variable independence for PCA to remove the multi-collinearity issue in the model.", "I still keep the same number of variables compared to the original data and we can see that now the 6 variables are not correlated to each other at all. We can use the new 6 variables as the independent variables to predict housing prices.", "The drawback of this method is also very obvious. After PCA transformation, we do not have the identity for each variable and it will be hard to interpret the results.", "We should check the issue of Multi-Collinearity every time before we build the regression model. VIF would be an easy way to look at each independent variable to see whether they have a high correlation with the rest. A correlation matrix would be useful to select important factors when you are not sure which variables to select for the model. The correlation matrix also helps to understand why certain variables have high VIF values.", "In terms of methods to fix the multi-collinearity issue, I personally do not prefer PCA here because model interpretation will be lost and when you want to apply the model to another set of data you need to PCA transform again. Thus, we should try our best to reduce the correlation by selecting the right variables and transform them if needed. It is your call to decide whether to keep the variable or not when it has a relatively high VIF value but also important in predicting the result. Trial and error is always the case to include different sets of variables, build the model and test it against testing data to see whether there is any overfitting.", "Do you think you are finally done with all the checks with statistical assumptions before constructing a model? No, you are not! If you want to know more about other statistical assumptions in a regression model, refer to my other article below on Normality Assumption in the regression model.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Enthusiast | Let's have this data journey together! | linkedin.com/in/songhaowu/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffe7a2c1467ea&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://songhaowu.medium.com/?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": ""}, {"url": "https://songhaowu.medium.com/?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "Songhao Wu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a26b3749a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&user=Songhao+Wu&userId=33a26b3749a7&source=post_page-33a26b3749a7----fe7a2c1467ea---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe7a2c1467ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe7a2c1467ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/PDxYfXVlK2M", "anchor_text": "unsplash"}, {"url": "https://towardsdatascience.com/is-normal-distribution-necessary-in-regression-how-to-track-and-fix-it-494105bc50dd", "anchor_text": "Is Normal Distribution Necessary in Regression? How to track and fix it?Box-Cox Transformation, Shapiro-Wilk test, QQ Plottowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fe7a2c1467ea---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fe7a2c1467ea---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/regression?source=post_page-----fe7a2c1467ea---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/tag/multicollinearity?source=post_page-----fe7a2c1467ea---------------multicollinearity-----------------", "anchor_text": "Multicollinearity"}, {"url": "https://medium.com/tag/vif?source=post_page-----fe7a2c1467ea---------------vif-----------------", "anchor_text": "Vif"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe7a2c1467ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&user=Songhao+Wu&userId=33a26b3749a7&source=-----fe7a2c1467ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe7a2c1467ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&user=Songhao+Wu&userId=33a26b3749a7&source=-----fe7a2c1467ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe7a2c1467ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffe7a2c1467ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fe7a2c1467ea---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fe7a2c1467ea--------------------------------", "anchor_text": ""}, {"url": "https://songhaowu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://songhaowu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Songhao Wu"}, {"url": "https://songhaowu.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "433 Followers"}, {"url": "http://linkedin.com/in/songhaowu/", "anchor_text": "linkedin.com/in/songhaowu/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a26b3749a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&user=Songhao+Wu&userId=33a26b3749a7&source=post_page-33a26b3749a7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7a191443ce50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-collinearity-in-regression-fe7a2c1467ea&newsletterV3=33a26b3749a7&newsletterV3Id=7a191443ce50&user=Songhao+Wu&userId=33a26b3749a7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}