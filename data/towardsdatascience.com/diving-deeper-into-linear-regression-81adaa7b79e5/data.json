{"url": "https://towardsdatascience.com/diving-deeper-into-linear-regression-81adaa7b79e5", "time": 1682992954.621563, "path": "towardsdatascience.com/diving-deeper-into-linear-regression-81adaa7b79e5/", "webpage": {"metadata": {"title": "Diving Deeper into Linear Regression | by Sujan Dutta | Towards Data Science", "h1": "Diving Deeper into Linear Regression", "description": "When I say \u201clinear regression\u201d, most of the people start thinking about the good old Ordinary Least Square(OLS) regression. If you are not familiar with the term, these equations might help\u2026 Did you\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["When I say \u201clinear regression\u201d, most of the people start thinking about the good old Ordinary Least Square(OLS) regression. If you are not familiar with the term, these equations might help\u2026", "Did you also think about OLS? If yes then you are on the right track. But there\u2019s more to linear regression than just OLS! First, let us look at OLS a bit more closely.", "The name of this technique came from the cost function. Here, we take the sum of squared errors (the difference between ground truths and predictions) and try to minimize this. By minimizing the cost function we achieve the optimal value of the vector \u03b2 (contains bias and weights). In the below plot, the contour (concentric ellipses) of the cost function is shown. After the minimization, we get \u03b2 as the point at the center.", "At first, it seems like OLS is enough for any regression problem. But as we increase the number of features and the complexity of data OLS tends to overfit the training data. The concept of overfitting is vast and deserves a separate article (you can find plenty of them) so I\u2019m going to give you a brief. Overfitting means the model has learned the training data so well that it fails to generalize. In other words, the model has learned even the small scale (insignificant) variations in the train data so it fails to produce good predictions on unseen (validation and test) data. To tackle the problem of overfitting we can use many techniques. Adding a regularization (penalty) term to our cost function is one such technique. But what term should we use? We generally use one of the following two methods.", "In this case, we add the sum of squares of weights to our least square cost function. So now it looks something like this\u2026", "But how does this term prevent overfitting? Adding this term is equivalent to adding an extra constraint on the possible values of \u03b2. Because to achieve the minimum cost, the sum of \u03b2\u00b2_j\u2019s must not exceed a certain value (say r). This technique prevents the model from assigning very large weights to some features over the others thus tackling overfitting. Mathematically,", "In other words, \u03b2 should lie inside(or on) the circle with radius \u221ar centered at the origin. Here\u2019s the visualization\u2026", "Notice that because of the constraint (red circle), the final value of \u03b2 is closer to the origin than it was in the OLS.", "The only difference between Ridge and Lasso is the regularization term. Here, we add the sum of absolute values of the weights to our least square cost function. So the cost function becomes\u2026", "In this case, the constraint can be written as\u2026", "Now we can visualize the constraint as a square instead of a circle.", "It is worth noting that, if the contours hit a corner of the square then one feature is completely neglected (weight becomes 0). For higher-dimensional feature space, we can use this trick to reduce the number of features.", "Note: In the regularization term we are not using bias (\u03b2_0) because only the very large weights (\u03b2_i\u2019s for i>0) corresponding to the features contribute to the overfitting. Bias term is just an intercept hence does not have much to do with the overfitting.", "Phew\u2026that was a lot about regularization. The common thing among the above methods was: they all have residuals/errors (ground truth-prediction) in their cost function. These errors are parallel to the y-axis. We could also consider errors along the x-axis and proceed similarly. See the plot below.", "What if we use a different kind of error?", "In this case, we consider errors in both directions (x-axis and y-axis). The sum of the square of perpendicular distances between the observed data points and the predicted line is to be minimized. Let\u2019s visualize this by taking only one feature.", "Then the regression coefficients can be obtained by minimizing", "This is very similar to the above method with a slight change. Here, we minimize the sum of areas of the rectangle formed by (X_i, Y_i) and (x_i, y_i).", "The total area extended by n data points is,", "The constraints here are the same as orthogonal regression.", "One should go for orthogonal and reduced major axis regressions when the uncerttainties are present in study (y) and explanatory (x) variables both.", "One interesting thing in orthogonal regression is, it produces symmetrical fit w.r.t y-errors and x-errors. But in OLS we don\u2019t get the symmetry for we minimize either y-errors or x-errors, not both.", "I hope you enjoyed the reading. Until next time\u2026Happy learning!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student | Ex-AI/ML intern at Apple | Normalized Nerd (58k+ subscribers)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F81adaa7b79e5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sujan99dutta?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sujan99dutta?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "Sujan Dutta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F74827b0a297&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&user=Sujan+Dutta&userId=74827b0a297&source=post_page-74827b0a297----81adaa7b79e5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81adaa7b79e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81adaa7b79e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@artemverbo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Artem Verbo"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----81adaa7b79e5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----81adaa7b79e5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----81adaa7b79e5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----81adaa7b79e5---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----81adaa7b79e5---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81adaa7b79e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&user=Sujan+Dutta&userId=74827b0a297&source=-----81adaa7b79e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81adaa7b79e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&user=Sujan+Dutta&userId=74827b0a297&source=-----81adaa7b79e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81adaa7b79e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F81adaa7b79e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----81adaa7b79e5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----81adaa7b79e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sujan99dutta?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sujan99dutta?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sujan Dutta"}, {"url": "https://medium.com/@sujan99dutta/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "133 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F74827b0a297&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&user=Sujan+Dutta&userId=74827b0a297&source=post_page-74827b0a297--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1ba0f1a8337e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-linear-regression-81adaa7b79e5&newsletterV3=74827b0a297&newsletterV3Id=1ba0f1a8337e&user=Sujan+Dutta&userId=74827b0a297&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}