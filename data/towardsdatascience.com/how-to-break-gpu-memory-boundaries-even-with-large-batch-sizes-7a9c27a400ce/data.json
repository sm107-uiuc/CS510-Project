{"url": "https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce", "time": 1683003050.5813062, "path": "towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce/", "webpage": {"metadata": {"title": "Batch size and GPU memory limitations in neural networks | Towards Data Science", "h1": "How to Break GPU Memory Boundaries Even with Large Batch Sizes", "description": "The problem of batch size and available GPU memory in training neural networks, how gradient accumulation can solve GPU memory limitations in deep learning"}, "outgoing_paragraph_urls": [{"url": "http://www.run.ai", "anchor_text": "Run:AI product", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa?source=friends_link&sk=28226e1d0ffa7e450d7dffa8d5b9cff6", "anchor_text": "another post", "paragraph_index": 22}, {"url": "https://github.com/run-ai/runai/tree/master/runai/ga", "anchor_text": "GitHub", "paragraph_index": 23}], "all_paragraphs": ["In this article, we\u2019ll talk about batch sizing issues one may encounter while training neural networks using large batch sizes and being limited by GPU memory.", "When building deep learning models, we have to choose batch size \u2014 along with other hyperparameters. Batch size plays a major role in the training of deep learning models. It has an impact on the resulting accuracy of models, as well as on the performance of the training process.", "The range of possible values for the batch size is limited today by the available GPU memory. As the neural network gets larger, the maximum batch size that can be run on a single GPU gets smaller. Today, as we find ourselves running larger models than ever before, the possible values for the batch size become smaller and might be far from the optimal values.", "Gradient accumulation is a way to enable running batch sizes that do not fit into the GPU memory in a trivial way.", "The batch size is the number of samples (e.g. images) used to train a model before updating its trainable model variables \u2014 the weights and biases. That is, in every single training step, a batch of samples is propagated through the model and then backward propagated to calculate gradients for every sample. The gradients of all samples will then be averaged or summed up and this value will be used as an input to a formula (depending on the chosen optimizer) that calculates the updates for the trainable model variables. Only after updating the parameters will the next batch of samples go through the same process.", "Batch size has a critical impact on the convergence of the training process as well as on the resulting accuracy of the trained model. Typically, there is an optimal value or range of values for batch size for every neural network and dataset. Different neural networks and different datasets may have different optimal batch sizes.", "There might be critical consequences when using different batch sizes that should be taken into consideration when choosing one. Let\u2019s cover two of the main potential consequences of using small or large batch sizes:", "With all that in mind, we have to choose a batch size that will be neither too small nor too large but somewhere in between. The main idea here is that we should play around with different batch sizes until we find one that would be optimal for the specific neural network and dataset we are using.", "While traditional computers have access to a lot of RAM, GPUs have much less, and although the amount of GPU memory is growing and will keep growing in the future, sometimes it\u2019s not enough. The training batch size has a huge impact on the required GPU memory for training a neural network. In order to further understand this, let\u2019s first examine what\u2019s being stored in GPU memory during training:", "NOTE: While (1) and (4) are always required, (2) and (3) are required only in training mode.", "So, the larger the batch size, the more samples are being propagated through the neural network in the forward pass. This results in larger intermediate calculations (e.g. layer activation outputs) that need to be stored in GPU memory. Technically speaking, the size of the activations is linearly dependent on the batch size.", "It is now clearly noticeable that increasing the batch size will directly result in increasing the required GPU memory. In many cases, not having enough GPU memory prevents us from increasing the batch size. Let\u2019s now see how we could break the GPU memory boundaries and still use larger batch sizes.", "One way to overcome the GPU memory limitations and run large batch sizes is to split the batch of samples into smaller mini-batches, where each mini-batch requires an amount of GPU memory that can be satisfied. These mini-batches can run independently, and their gradients should be averaged or summed before calculating the model variable updates. There are two main ways to implement this:", "Data-parallelism \u2014 use multiple GPUs to train all mini-batches in parallel, each on a single GPU. The gradients from all mini-batches are accumulated and the result is used to update the model variables at the end of every step.", "Gradient accumulation \u2014 run the mini-batches sequentially, while accumulating the gradients. The accumulated results are used to update the model variables at the end of the last mini-batch.", "Data-parallelism gradient accumulation have many characteristics and constraints in common:", "While both of the options are pretty similar, gradient accumulation may be done sequentially using a single GPU, making it more attractive for users who cannot access more than a single GPU, or users that want to minimize resource usage.", "Additionally, the two can be used together. In this way, we would use several GPUs, run a few steps and accumulate the gradients on each GPU, and reduce the accumulated results from all the GPUs at the end of the step.", "We have run some experiments this way and refer to this as elasticity. The Run:AI product utilizes this feature to increase the utilization of GPU clusters and improve the productivity of data science teams. We will share some more details on these concepts in future posts.", "Although there are similarities between gradient accumulation and data-parallelism, their implementations are completely different. We will be focusing on gradient accumulation in the next posts.", "As a deep-learning model gets larger the maximum batch size that can be run on a single GPU gets smaller.", "While data scientists aim to find the optimal batch size for a specific neural network and dataset, finding the right batch size and then being limited by GPU memory is a common occurrence and one we tried and managed to overcome.", "In another post, we\u2019ll go through the technical and algorithmic details of gradient accumulation and further demonstrate how it is used to solve the problem of batch size limitations.", "An open-source gradient accumulation tool, as well as usage examples and more resources, are available on GitHub.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Programmer. I like technology, music, and too many more things."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c27a400ce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@raz.rotenberg?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "Raz Rotenberg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f915c067327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&user=Raz+Rotenberg&userId=4f915c067327&source=post_page-4f915c067327----7a9c27a400ce---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a9c27a400ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a9c27a400ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@anniespratt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Annie Spratt"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://www.run.ai", "anchor_text": "Run:AI product"}, {"url": "https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa?source=friends_link&sk=28226e1d0ffa7e450d7dffa8d5b9cff6", "anchor_text": "another post"}, {"url": "https://github.com/run-ai/runai/tree/master/runai/ga", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7a9c27a400ce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7a9c27a400ce---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7a9c27a400ce---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gpu?source=post_page-----7a9c27a400ce---------------gpu-----------------", "anchor_text": "Gpu"}, {"url": "https://medium.com/tag/keras?source=post_page-----7a9c27a400ce---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a9c27a400ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&user=Raz+Rotenberg&userId=4f915c067327&source=-----7a9c27a400ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a9c27a400ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&user=Raz+Rotenberg&userId=4f915c067327&source=-----7a9c27a400ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a9c27a400ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7a9c27a400ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7a9c27a400ce---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7a9c27a400ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raz.rotenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raz Rotenberg"}, {"url": "https://medium.com/@raz.rotenberg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "107 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f915c067327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&user=Raz+Rotenberg&userId=4f915c067327&source=post_page-4f915c067327--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F68478d8b203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce&newsletterV3=4f915c067327&newsletterV3Id=68478d8b203&user=Raz+Rotenberg&userId=4f915c067327&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}