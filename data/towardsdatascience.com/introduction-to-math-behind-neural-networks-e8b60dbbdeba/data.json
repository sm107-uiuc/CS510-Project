{"url": "https://towardsdatascience.com/introduction-to-math-behind-neural-networks-e8b60dbbdeba", "time": 1683014794.125516, "path": "towardsdatascience.com/introduction-to-math-behind-neural-networks-e8b60dbbdeba/", "webpage": {"metadata": {"title": "An Introduction To Mathematics Behind Neural Networks | Towards Data Science", "h1": "A Gentle Introduction To Math Behind Neural Networks", "description": "Today, with open source machine learning software libraries such as TensorFlow, Keras, or PyTorch we can create a neural network, even with high structural complexity, with just a few lines of code\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 0}, {"url": "https://keras.io/", "anchor_text": "Keras", "paragraph_index": 0}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Dot_product#Algebraic_definition", "anchor_text": "dot product", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Chain_rule", "anchor_text": "chain rule", "paragraph_index": 14}, {"url": "https://www.linkedin.com/in/dasaradhsk/", "anchor_text": "https://www.linkedin.com/in/dasaradhsk/", "paragraph_index": 25}], "all_paragraphs": ["Today, with open source machine learning software libraries such as TensorFlow, Keras, or PyTorch we can create a neural network, even with high structural complexity, with just a few lines of code. Having said that, the mathematics behind neural networks is still a mystery to some of us, and having the mathematics knowledge behind neural networks and deep learning can help us understand what\u2019s happening inside a neural network. It is also helpful in architecture selection, fine-tuning of deep learning models, hyperparameters tuning, and optimization.", "I had ignored understanding the mathematics behind neural networks and deep learning for a long time as I didn\u2019t have good knowledge of algebra or differential calculus. A few days ago, I decided to start from scratch and derive the methodology and mathematics behind neural networks and deep learning, to know how and why they work. I also decided to write this article, which would be useful to people like me, who find it difficult to understand these concepts.", "Perceptrons \u2014 invented by Frank Rosenblatt in 1958, are the simplest neural network that consists of n number of inputs, only one neuron, and one output, where n is the number of features of our dataset. The process of passing the data through the neural network is known as forward propagation and the forward propagation carried out in a perceptron is explained in the following three steps.", "Step 1: For each input, multiply the input value x\u1d62 with weights w\u1d62 and sum all the multiplied values. Weights \u2014 represent the strength of the connection between neurons and decides how much influence the given input will have on the neuron\u2019s output. If the weight w\u2081 has a higher value than the weight w\u2082, then the input x\u2081 will have a higher influence on the output than w\u2082.", "The row vectors of the inputs and weights are x = [x\u2081, x\u2082, \u2026 , x\u2099] and w =[w\u2081, w\u2082, \u2026 , w\u2099] respectively and their dot product is given by", "Hence, the summation is equal to the dot product of the vectors x and w", "Step 2: Add bias b to the summation of multiplied values and let\u2019s call this z. Bias \u2014 also known as the offset is necessary in most of the cases, to move the entire activation function to the left or right to generate the required output values.", "Step 3: Pass the value of z to a non-linear activation function. Activation functions \u2014 are used to introduce non-linearity into the output of the neurons, without which the neural network will just be a linear function. Moreover, they have a significant impact on the learning speed of the neural network. Perceptrons have binary step function as their activation function. However, we shall use sigmoid \u2014 also known as logistic function as our activation function.", "where \u03c3 denotes the sigmoid activation function and the output we get after the forward prorogation is known as the predicted value y\u0302.", "The learning algorithm consists of two parts \u2014 backpropagation and optimization.", "Backpropagation: Backpropagation, short for backward propagation of errors, refers to the algorithm for computing the gradient of the loss function with respect to the weights. However, the term is often used to refer to the entire learning algorithm. The backpropagation carried out in a perceptron is explained in the following two steps.", "Step 1: To know an estimation of how far are we from our desired solution a loss function is used. Generally, mean squared error is chosen as the loss function for regression problems and cross entropy for classification problems. Let\u2019s take a regression problem and its loss function be mean squared error, which squares the difference between actual (y\u1d62) and predicted value ( y\u0302\u1d62 ).", "Loss function is calculated for the entire training dataset and their average is called the Cost function C.", "Step 2: In order to find the best weights and bias for our Perceptron, we need to know how the cost function changes in relation to weights and bias. This is done with the help of the gradients (rate of change) \u2014 how one quantity changes in relation to another quantity. In our case, we need to find the gradient of the cost function with respect to the weights and bias.", "Let\u2019s calculate the gradient of cost function C with respect to the weight w\u1d62 using partial derivation. Since the cost function is not directly related to the weight w\u1d62, let\u2019s use the chain rule.", "Now we need to find the following three gradients", "Let\u2019s start with the gradient of the cost function (C) with respect to the predicted value ( y\u0302 )", "Let y = [y\u2081 , y\u2082 , \u2026 y\u2099] and y\u0302 =[ y\u0302\u2081 , y\u0302\u2082 , \u2026 y\u0302\u2099] be the row vectors of actual and predicted values. Hence the above equation is simplified as", "Now let\u2019s find the gradient of the predicted value with respect to the z. This will be a bit lengthy.", "The gradient of z with respect to the weight w\u1d62 is", "What about Bias? \u2014 Bias is theoretically considered to have an input of constant value 1. Hence,", "Optimization: Optimization is the selection of the best element from some set of available alternatives, which in our case, is the selection of best weights and bias of the perceptron. Let\u2019s choose gradient descent as our optimization algorithm, which changes the weights and bias, proportional to the negative of the gradient of the cost function with respect to the corresponding weight or bias. Learning rate (\u03b1) is a hyperparameter which is used to control how much the weights and bias are changed.", "The weights and bias are updated as follows and the backpropagation and gradient descent is repeated until convergence.", "I hope that you\u2019ve found this article useful and understood the mathematics behind the neural networks and deep learning. I have explained the working of a single neuron in this article. However, these basic concepts are applicable to all kinds of neural networks with some modifications.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Full Stack Developer @ Zoho Chennai, India | https://www.linkedin.com/in/dasaradhsk/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe8b60dbbdeba&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://skdasaradh.medium.com/?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": ""}, {"url": "https://skdasaradh.medium.com/?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "Dasaradh S K"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb4a0d3b48c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&user=Dasaradh+S+K&userId=db4a0d3b48c1&source=post_page-db4a0d3b48c1----e8b60dbbdeba---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8b60dbbdeba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8b60dbbdeba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.diagrams.net/", "anchor_text": "Diagrams.Net"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://en.wikipedia.org/wiki/Dot_product#Algebraic_definition", "anchor_text": "dot product"}, {"url": "https://en.wikipedia.org/wiki/Chain_rule", "anchor_text": "chain rule"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e8b60dbbdeba---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e8b60dbbdeba---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e8b60dbbdeba---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----e8b60dbbdeba---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/technology?source=post_page-----e8b60dbbdeba---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8b60dbbdeba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&user=Dasaradh+S+K&userId=db4a0d3b48c1&source=-----e8b60dbbdeba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8b60dbbdeba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&user=Dasaradh+S+K&userId=db4a0d3b48c1&source=-----e8b60dbbdeba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8b60dbbdeba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe8b60dbbdeba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e8b60dbbdeba---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e8b60dbbdeba--------------------------------", "anchor_text": ""}, {"url": "https://skdasaradh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://skdasaradh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dasaradh S K"}, {"url": "https://skdasaradh.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "306 Followers"}, {"url": "https://www.linkedin.com/in/dasaradhsk/", "anchor_text": "https://www.linkedin.com/in/dasaradhsk/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb4a0d3b48c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&user=Dasaradh+S+K&userId=db4a0d3b48c1&source=post_page-db4a0d3b48c1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F256553b5e296&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-math-behind-neural-networks-e8b60dbbdeba&newsletterV3=db4a0d3b48c1&newsletterV3Id=256553b5e296&user=Dasaradh+S+K&userId=db4a0d3b48c1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}