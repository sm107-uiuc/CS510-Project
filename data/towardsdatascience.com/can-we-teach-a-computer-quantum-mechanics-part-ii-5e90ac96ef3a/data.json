{"url": "https://towardsdatascience.com/can-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a", "time": 1682996953.642144, "path": "towardsdatascience.com/can-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a/", "webpage": {"metadata": {"title": "Can we teach a computer quantum mechanics? (Part II) | by Max Kelsen | Towards Data Science", "h1": "Can we teach a computer quantum mechanics? (Part II)", "description": "Authors: Kaiah Steven, Matthew Rose, Tyler Jones and Xavier Poncini (Quantum Machine Learning Research Team, Max Kelsen) In Part I of this series we introduced many prerequisite concepts essential to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://maxkelsen.com", "anchor_text": "Max Kelsen", "paragraph_index": 0}, {"url": "https://medium.com/@maxkelsen/can-we-teach-a-computer-quantum-mechanics-part-i-c3e724e31e1a", "anchor_text": "Part I", "paragraph_index": 1}, {"url": "https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://github.com/Qiskit/qiskit-api-py", "anchor_text": "IBMQs Qiskit Python library", "paragraph_index": 17}, {"url": "http://qutip.org/", "anchor_text": "Qutip", "paragraph_index": 17}, {"url": "https://openai.com/blog/openai-baselines-ppo/", "anchor_text": "Proximal Policy Optimisation", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1104.3787", "anchor_text": "Projective Simulation", "paragraph_index": 18}, {"url": "https://deepmind.com/research/dqn/", "anchor_text": "Deep Q Networks", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1602.01783.pdf", "anchor_text": "Asychronous Actor-Critic Agents", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/quant-ph/0603161", "anchor_text": "Nielsen et al.", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1808.01649", "anchor_text": "Girolami", "paragraph_index": 22}, {"url": "https://github.com/MaxKelsen/quantumcircuit-gym", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://maxkelsen.com", "anchor_text": "https://maxkelsen.com", "paragraph_index": 35}], "all_paragraphs": ["Authors: Kaiah Steven, Matthew Rose, Tyler Jones and Xavier Poncini (Quantum Machine Learning Research Team, Max Kelsen)", "In Part I of this series we introduced many prerequisite concepts essential to quantum computation. In this part we will detail our work in applying reinforcement learning to the problem of state preparation.", "We reside in an era where quantum computation is limited to noisy intermediate-scale quantum computers (NISQ). This technology \u2014 much like the revelation that striking rocks together creates sparks \u2014 lays the foundations for unimagined innovation and potential.", "Currently, these devices are limited by the small number of available qubits and short coherence times; thus the current goal is to extract as much \u201cheat\u201d and \u201clight\u201d from this burgeoning technology as possible. With limited resources, the extent of what is possible with NISQ is subject to their efficient use. Hence the optimal gate(qubit operations) configuration for achieving a particular process is highly desirable. However, finding this configuration is no easy task, especially when attempting to carry out useful quantum computations.", "The application of machine learning breakthroughs to this dilemma is a promising avenue towards lighting a fire with these sparks and achieving practical advantages from today\u2019s quantum computers.", "Before we, begin let\u2019s provide a quick introduction to reinforcement learning (RL). Popularised in the AI scene by some breakthrough achievements such as OpenAI Five RL based DOTA2 team, which beat a team of professional players, and Google\u2019s AlphaZero defeating Lee Sedol and Ke Jie (both multiple world championship winners) in Go.", "RL has gained a reputation for being a technique with the potential to only \u2018solve\u2019 video games. It is our hope however, that quantum computing is a domain outside the realm of games where the power of RL can be effectively harnessed.", "The general idea behind reinforcement learning is to teach an agent (embodied by a policy) how to reach some defined goal or objective. This can be understood through the description of two main elements \u2014 the agent and the environment.", "The environment in RL can be thought of as the virtual world our agent lives within and interacts with. This interaction and feedback allows the agent to learn from the environment, in much the same way humans learn through interactions with others and the world around them.", "At each step in the time period (or \u2018episode\u2019), our agent is allowed to observe the world around them, partially or entirely, to gain some understanding about its current state. The agent then chooses an action from some predefined action space that facilitates further interaction. This action alters the state of the environment and returns a reward to the agent based on the difference between the previous state of the world and its new state.", "The backbone behind the choice of any action by an agent is that agent\u2019s policy \u2014 some probabilistic distribution that outputs a valid action based on the current observed state of the environment. In Deep RL, this policy is manifested by a neural network that takes the current state as input and provides an action as output.", "A na\u00efve approach may be to simply choose the action with the most substantial immediate reward. However, this may lead the agent into falling into holes or hitting dead ends \u2014 reaching local extrema. It is common to take into account discounted rewards from actions taken in the future in combination with the most immediate action as a way to learn how to reach the final objective.", "After one episode is played out by our agent, the policy is updated with the stored information about states, actions and rewards. This update serves as a way of teaching the agent which actions are preferable in certain states and which should be avoided.", "A major hurdle in building useful, functioning RL agents is constructing a reward function that best maps to your specific use-case. Such a function is the sole supervisor of the agent within its environment, which should only provide rewards directing the agent towards its goal, and in a way that avoids pitfalls.", "The field of RL research is littered with examples of misaligned and misdirected agents \u2014 most of which seem humorous from our perspective but are perfectly reasonable to the agent. For example:", "See here for a growing catalogue of literature using reinforcement learning in games, and some rather entertaining faulty reward functions. Having outlined preliminaries to both quantum computation and reinforcement learning, we are now in a position to detail our efforts in applying RL to the preparation of quantum states.", "Now armed with the ability to exploit reinforcement learning to optimise the construction of quantum circuits, we began exploring our options in terms of what set of actions to use, what to define as our state, possible reward functions, and which actual RL algorithm to train.", "IBMQs Qiskit Python library was used to construct and visualise quantum circuits as well as perform relevant quantum operations in conjunction with fellow Python-based quantum computing library \u2014 Qutip. Neural networks implemented with TensorFlow were used as the basis for policies for the agents.", "After exploring the current literature regarding RL algorithms and examining state-of-the-art performances, the widespread success and applicability of Proximal Policy Optimisation (PPO) led us to utilise this algorithm in our testing. Other options examined include Projective Simulation, Deep Q Networks and Asychronous Actor-Critic Agents (A3C).", "Our first effort was to utilise the quantum state of the specified qubits as the state of the environment, along with defining an action space consisting of all legal qubit-gate pairs (e.g. an X gate on qubit one was specified as a different action to an X gate on qubit three).", "The initial reward function operated on the sole basis of fidelity, which is a measure of one\u2019s ability to distinguish between two quantum states. Basically, the more overlap between the environment\u2019s current state and the goal state, the higher the reward. To encourage synthesis of smaller circuits, the reward was also decreased for each gate used.", "Although fidelity was effective in determining whether the goal had been achieved, it quickly became apparent that it was a faulty reward function. Faulty in the sense that there were many local maxima in the fidelity function over this discrete gate space. For simple circuits in particular, a near-term increase in fidelity often increased the total number of gates needed to complete the circuit, and in some cases even led the circuit into a dead-end.", "Our second take on a reward function was driven by the ambition to have some measure of the number of gates remaining to complete the circuit. Two papers stood out from the literature as they were tackling the problem of deriving this measure. One (Nielsen et al.) leveraged ideas from geometry to represent the space of quantum operations as a surface, where the optimal circuit was the shortest path (geodesic) across this surface. The other (Girolami), derived an expression for a lower bound of the number of gates needed to drive some qubits from one state to another.", "Unfortunately for us, both of these options seemed to have limiting factors that prevented them from being useful in our framework. Geodesics turned out to be computationally expensive to generate and solve, and Girolami\u2019s lower bound was only proven for commuting gates, which removes universality from the set of actions.", "A hesitant solution to the reward function problem was the introduction of a sparse reward scheme \u2014 providing only a positive reward (inversely proportional to the number of gates used) in the case where the final circuit reached the goal. At this stage of experimentation, we altered the action space to contain only the separate gate operations and set the agent to loop over the qubits instead of allowing it to choose.", "As a consequence of this change, the identity gate was added to the action space so that the agent could \u2018skip over\u2019 a qubit if it deemed it necessary. Doing so ensured consistency across different qubits for each operation.", "The results of these changes were promising, with the model able to design optimal simple three-qubit circuits with consistency, and in a reasonable time (e.g. the below circuit was synthesised in around 40 training epochs, or 40 seconds on the laptop used for testing).", "Following the confidence boost from witnessing our agent at work, we wanted to explore the generalisability of the model. A curriculum of different goal states was generated, ranging from simple one gate goal unitaries through to more complex four gate unitaries. There was clear evidence of improvement from training over sets of different goals multiple times, suggesting some ability for the agent to learn a policy accounting for multiple different goals at once.", "The success of PPO without any guiding reward function represented a promising result, especially after hyper-parameter optimisation led to significantly shortened convergence times (within five seconds for the simplest circuits and minutes later in the curriculum). This improvement of convergence with pre-trained models represented the strongest suggestion yet that our neural network could learn to navigate in the challenging quantum circuit environment.", "As such, we decided to test the waters of other reward functions, replacing our universal non-commuting gate set with the non-universal commuting IQP gate set. We then made use of Girolami\u2019s lower bound result to form our reward function. This model was able to piggyback off the success of our sparse reward while nudging the agent towards the goal, and (hopefully) avoiding any local maxima.", "The use of a step-by-step reward function displays relatively rapid convergence, with circuits more complex than those used to produce Figure 3. The success of this commuting-gate reward function in encouraging the agent towards goals with relatively quick convergence times represents a strong proof of concept. With the function generalised to non-commuting gates, a more general policy could potentially be developed.", "Our efforts in training a reinforcement learning agent to synthesise optimal quantum circuits relied upon the creation of a custom environment in which our agent was able to play around in. This environment can be found here and utilises features such as supporting different gate sets, taking into account the physical connectivity of qubits, and generating different sized circuits. All experimentation throughout this series was completed using this environment.", "We feel that these results encourage the ongoing efforts in finding the holy grail, optimal quantum circuit construction. In our pursuit, we found that the reward function (a reliable metric for the distance between the current and target state) is a crucial ingredient in applying machine learning to determining efficient quantum circuits. Subsequent improvements in the cost function would significantly enhance our agent\u2019s performance.", "Some exciting related avenues that we are currently exploring include the extension of this reinforcement learning approach to a continuous control space \u2014 shaping the very pulses that are used to implement quantum gates in a circuit. Stay tuned!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "We are an Artificial Intelligence and Machine Learning consultancy that delivers competitive advantage for government and enterprise. https://maxkelsen.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5e90ac96ef3a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://maxkelsen.medium.com/?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": ""}, {"url": "https://maxkelsen.medium.com/?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "Max Kelsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa70db411556b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&user=Max+Kelsen&userId=a70db411556b&source=post_page-a70db411556b----5e90ac96ef3a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e90ac96ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e90ac96ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://maxkelsen.com", "anchor_text": "Max Kelsen"}, {"url": "https://medium.com/@maxkelsen/can-we-teach-a-computer-quantum-mechanics-part-i-c3e724e31e1a", "anchor_text": "Part I"}, {"url": "https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml", "anchor_text": "here"}, {"url": "https://github.com/Qiskit/qiskit-api-py", "anchor_text": "IBMQs Qiskit Python library"}, {"url": "http://qutip.org/", "anchor_text": "Qutip"}, {"url": "https://openai.com/blog/openai-baselines-ppo/", "anchor_text": "Proximal Policy Optimisation"}, {"url": "https://arxiv.org/abs/1104.3787", "anchor_text": "Projective Simulation"}, {"url": "https://deepmind.com/research/dqn/", "anchor_text": "Deep Q Networks"}, {"url": "https://arxiv.org/pdf/1602.01783.pdf", "anchor_text": "Asychronous Actor-Critic Agents"}, {"url": "https://arxiv.org/abs/quant-ph/0603161", "anchor_text": "Nielsen et al."}, {"url": "https://arxiv.org/abs/1808.01649", "anchor_text": "Girolami"}, {"url": "https://github.com/MaxKelsen/quantumcircuit-gym", "anchor_text": "here"}, {"url": "https://medium.com/tag/quantum-computing?source=post_page-----5e90ac96ef3a---------------quantum_computing-----------------", "anchor_text": "Quantum Computing"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5e90ac96ef3a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5e90ac96ef3a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/science?source=post_page-----5e90ac96ef3a---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5e90ac96ef3a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e90ac96ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&user=Max+Kelsen&userId=a70db411556b&source=-----5e90ac96ef3a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e90ac96ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&user=Max+Kelsen&userId=a70db411556b&source=-----5e90ac96ef3a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e90ac96ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5e90ac96ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5e90ac96ef3a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5e90ac96ef3a--------------------------------", "anchor_text": ""}, {"url": "https://maxkelsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://maxkelsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Kelsen"}, {"url": "https://maxkelsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "159 Followers"}, {"url": "https://maxkelsen.com", "anchor_text": "https://maxkelsen.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa70db411556b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&user=Max+Kelsen&userId=a70db411556b&source=post_page-a70db411556b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F54cf8f4e5223&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-teach-a-computer-quantum-mechanics-part-ii-5e90ac96ef3a&newsletterV3=a70db411556b&newsletterV3Id=54cf8f4e5223&user=Max+Kelsen&userId=a70db411556b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}