{"url": "https://towardsdatascience.com/how-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb", "time": 1682994164.954512, "path": "towardsdatascience.com/how-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb/", "webpage": {"metadata": {"title": "Building a convolutional neural network for natural language processing | by David Bressler | Towards Data Science", "h1": "Building a convolutional neural network for natural language processing", "description": "Recurrent neural networks (RNNs) with LSTM or GRU units are the go-to tools for NLP researchers, and provide state-of-the-art results on many different NLP tasks, including language modeling (LM)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://thegradient.pub/nlp-imagenet/", "anchor_text": "using pretrained models", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9", "anchor_text": "faster softmax", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNNs", "paragraph_index": 0}, {"url": "https://research.fb.com/category/facebook-ai-research/", "anchor_text": "Facebook\u2019s AI Research (FAIR) group", "paragraph_index": 0}, {"url": "https://github.com/DavidWBressler/GCNN", "anchor_text": "Pytorch implementation", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1612.08083", "anchor_text": "\u201cLanguage Modeling with Gated Convolutional Networks\u201d", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet architecture", "paragraph_index": 8}, {"url": "https://www.coursera.org/learn/convolutional-neural-networks/lecture/XAKNO/why-resnets-work", "anchor_text": "built much deeper", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet architecture", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9", "anchor_text": "blog post", "paragraph_index": 10}, {"url": "https://github.com/DavidWBressler/GCNN/blob/master/GCNN.ipynb", "anchor_text": "Jupyter notebook", "paragraph_index": 11}, {"url": "https://docs.fast.ai/", "anchor_text": "fastai", "paragraph_index": 11}, {"url": "https://course.fast.ai/", "anchor_text": "fast.ai course", "paragraph_index": 11}, {"url": "https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset", "anchor_text": "Wikitext-2 dataset", "paragraph_index": 11}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe pre-trained word vectors", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9", "anchor_text": "adaptive softmax", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "attentional", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1705.03122", "anchor_text": "convolutional", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "He, Kaiming, et al. \u201cDeep residual learning for image recognition.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani, Ashish, et al. \u201cAttention is all you need.\u201d Advances in Neural Information Processing Systems. 2017.", "paragraph_index": 21}], "all_paragraphs": ["Recurrent neural networks (RNNs) with LSTM or GRU units are the go-to tools for NLP researchers, and provide state-of-the-art results on many different NLP tasks, including language modeling (LM), neural machine translation (NMT), sentiment analysis, and so on. However, a major drawback of RNNs is that they are notoriously slow to train, and so much of the effort over the past few years has been focused on trying to speed them up. There have been a variety of approaches to do this, including using pretrained models, using a faster softmax, and using different architectures such as convolutional neural networks (CNNs), quasi-recurrent neural networks (QRNNs), or transformer architectures. In this blog post, I\u2019ll tell you about recent research from Facebook\u2019s AI Research (FAIR) group, who for the first time ever are able to get state-of-the-art results on a language modeling task using a CNN. I\u2019ll explain their approach, and then walk you step by step through a Pytorch implementation I wrote.", "As you probably know, language modeling involves predicting the next word in a sequence, when given a sequence of words as an input. A major reason why RNNs are slow is that each word in that input string must be processed sequentially: in the sentence \u201cParallelization is fun\u201d, the words \u2018parallelization\u2019 and \u2018is\u2019 must be processed before the word \u2018fun\u2019. In contrast, all the elements in a CNN are processed simultaneously, which can speed up processing immensely. Previous attempts at using a CNN for language modeling significantly underperform results from RNNs, but in a recent paper called \u201cLanguage Modeling with Gated Convolutional Networks\u201d, Dauphin et al. (2016) build a convolutional language model that produces results competitive with RNNs, and can drastically cut down computational cost.", "A quick reminder of how information flows through a RNN language model will be helpful. The input to the model is a sequence of words represented by word embeddings X of size [seq_length, emb_sz], where seq_length is the sequence length, and emb_sz is the dimensionality of your embeddings. After X is passed through multiple LSTM layers, the output of the final layer is a hidden state representation H of size [seq_length, c_out], where c_out is the dimensionality of that final layer. Then, a softmax step produces predictions over the entire vocabulary, for each step in the sequence.", "Dauphin et al.\u2019s CNN similarly takes embedding activations of size [seq_length, emb_sz] as input, but then uses multiple layers of gated convolutions to produce the final hidden state output H (also of size [seq_length, c_out]). Each layer is composed of 1) a convolutional block that produces two separate convolutional outputs, and 2) a gating block that uses one convolutional output to gate the other.", "The convolutional block performs \u201ccausal convolutions\u201d on the input (which for the first layer will be size [seq_length, emb_sz]). Whereas a normal convolution has a window of width k that is centered on the current timestep (and therefore includes inputs from both future and past timesteps), a causal convolution has a window that overlaps only the current and previous timesteps (see figure below). This is accomplished by simply computing a normal convolution on an input that has been zero-padded with k-1 elements on the left. Causal convolutions are necessary because it would be cheating if the CNN was able to \u201csee\u201d information from the future timesteps that it is trying to predict.", "As mentioned, the convolutional block will actually produce two separate convolutional outputs, A and B, one of which will gate the other through element-wise multiplication. The first convolutional output is computed as A= X*W+b. Since X is a tensor of size [seq_length, emb_sz], after left-padding with zeroes it will be of size [seq_length+k-1, emb_sz]. W is a convolutional filter of size [k, in_c, out_c], where in_c is the number of input channels (here equal to emb_sz for the first layer), and out_c is the number of output channels. b is a bias vector of length out_c. After applying the convolutions, the output A will be size [seq_length, out_c]. Finally, you do the same steps with B= X*V+c\u2026 i.e. produce a second output B using a different convolutional filter V and bias vector c.", "The two outputs of the convolutional block, A and B, are then passed to the gating block. At this point its helpful to be reminded of what happens in CNNs for image processing\u2026 the convolutional output A would likely be put through an activation mechanism such as ReLU: ReLU(A), a.k.a. ReLU(X*W+b). For the GCNN\u2019s gating block however, Dauphin et al. use a mechanism they call a \u201cgated linear unit\u201d (GLU), which involves element-wise multiplying A by sigmoid(B):", "Here, B contains the \u2018gates\u2019 that control what information from A is passed up to the next layer in the hierarchy. Conceptually, the gating mechanism is important because it allows selection of the words or features that are important for predicting the next word, and provides a mechanism to learn and pass along just the relevant info. Similar to ReLU, the gating mechanism also provides the layer with non-linear capabilities, while providing a linear path for the gradient during backpropagation (thereby diminishing the vanishing gradient problem).", "Dauphin et al. also add a residual skip connection to each layer (see figure above), similar to the ResNet architecture. Residual connections minimize the vanishing gradient problem, allowing networks to be built much deeper (more layers). The input to the layer (X) is added to convolutional output A, after it is gated by convolutional output B.", "Dauphin et al. sometimes use a bottleneck structure within a layer, also similar to the ResNet architecture. The purpose of a bottleneck structure is to reduce the computational cost of a convolutional operation by first reducing the dimensionality of the data. Suppose you\u2019d like to perform a k=4 convolution on an input of size [seq_length=70, in_c=600], with 600 output channels. This would normally require a convolutional weights matrix of size [4, 600, 600]. In contrast, a bottleneck layer first reduces the dimensionality of the data to e.g. [70, 30], then performs the k=4 convolution in the lower-dimensional space, then finally increase the dimensionality of the data back to [70,600]. This approach requires a convolutional weights matrix size of only [4, 30, 30], which is much more computationally efficient. The bottleneck increases and decreases dimensionality through the use of k=1 convolutions (see Pytorch implementation below).", "Regardless of whether a bottleneck structure is used within a layer, the GCNN layers can be stacked, making sure at every layer to left-pad the outputs of the previous layer. The input to each layer will be size [seq_length+k-1, in_c] and the output of each layer will be size [seq_length, out_c]. Dauphin et al. stack at least 5 of these layers on top each other, to produce the final hidden state output H. They then use the adaptive softmax to choose which word to select, from a huge list of words in the language model\u2019s vocabulary (check out my blog post on how to speed up the time-consuming softmax step by up to 1000%).", "After some searching, I was not able to find a working Pytorch implementation of the paper, so I decided to build one from scratch. Check out the accompanying Jupyter notebook to follow along. For preprocessing you will need fastai, a deep learning library that runs on top of Pytorch that simplifies training neural networks. [For those who want to learn state-of-the-art deep learning techniques, I highly recommend Jeremy Howard\u2019s fast.ai course, which is available online for free]. I decided to use the Wikitext-2 dataset, which is a relatively small dataset that contains ~2 million tokens and ~33 thousand words in the vocabulary, and is a subset of the larger Wikitext-103 dataset used in the paper.", "As in the paper, I divide the data into paragraphs. For the sake of having a somewhat standardized sequence length, I remove paragraphs that have less than 10 or more than 300 words. Once the data has been properly formatted in csv files, fastai makes it easy to quickly tokenize and numericalize it. I created customized dataset, sampler, and collating functions, which properly format the data into data loaders. Each minibatch randomly samples paragraphs of approximately the same sequence length. I also downloaded GloVe pre-trained word vectors that are used for the models\u2019 word embeddings. Finally, I created a modeler class that handles training. During training, gradient clipping allows the use of a large learning rate (lr=1), which along with Nesterov\u2019s momentum and weight normalization helps training to converge quickly.", "As mentioned above, the sequence length differs between minibatches, but just assume for the rest of this post that seq_length equals 70. After the embeddings and resizing, the input x to the first layer is size [bs=50, emb_sz=300, seq_length=70, 1], where bs is batch size, and emb_sz is the embedding dimensionality. I use a convolutional kernel size k of 4, so after left-padding the input with k-1=3 elements, the input size is [50, 300, 73, 1].", "I use a bottleneck structure in each layer of my architecture. In the first layer, (called with self.inlayer = GLUblock(k, emb_sz, nh, downbot) ), I produce two convolutional outputs, x1 and x2. To produce x1, the first k=1 convolution decreases the dimensionality of input x from [50, 300, 73, 1] to [50, 15, 73, 1]. Then the k=4 convolution is performed in this lower dimensional space, to produce output of size [50, 15, 70, 1]. Finally, a k=1 convolution increases the dimensionality of x1 to size [50, 600, 70, 1]. The same three steps are used to produce x2, which is then passed through a sigmoid and used to gate x1 through element-wise multiplication, to produce output x (also of size [50, 600, 70, 1]). The residual (which was set equal to x at the beginning of the block), is transformed from size [50, 300, 70, 1] to size [50, 600, 70, 1] through the use of a k=1 convolution, and added to the output x.", "I next pass the data x through 4 gated convolutional layers. Each layer has an input of size [50, 600, 70, 1], and produces an output of the same size. Since the dimensionality of the input and the output is the same (i.e. in_c==out_c), the residual can simply be copied and set aside until the end of the block. As described above, the input is left-padded to be size [50, 600, 73, 1], bottleneck convolutional outputs x1 and x2 are produced (both of size [50, 600, 70, 1]), and x1 is gated by sigmoid(x2) before the residual is added to produce the layer\u2019s output. After the 4 convolutional layers, the final output is then passed through the adaptive softmax to produce the model\u2019s predictions.", "I ran this network on the Wikitext-2 dataset for 50 epochs, which produced a minimum loss of 4.34 (perplexity=76.6) on epoch 49. This compares pretty well with the results from Dauphin et al., who achieve a final perplexity of 37.2 on the larger Wikitext-103 dataset. My model\u2019s performance would likely be closer to that found in the paper by increasing the model capacity (more layers, more hidden units per layer), by training for much longer, by using a less aggressive dimensionality decrease in the bottleneck layers, and by using a larger dataset (e.g. Wikitext-103).", "Dauphin et al.\u2019s results are competitive those of state-of-the-art models, either nearly matching (Google Billion Word dataset) or surpassing (Wikitext-103) the best published results from LSTM-based models. Moreover, their approach greatly reduces computational cost. Their use of the adaptive softmax speeds training by ~500% compared to LSTM models with the full softmax, so that it requires significantly fewer operations to reach a given perplexity value. Since the convolutional approach is parallelizable over both sequences and the tokens within a sequence, the GCNN model reduces the latency to process a single sentence by an order of magnitude*.", "Dauphin et al.\u2019s paper is one of many recent publications that completely abandon RNNs for NLP tasks, relying instead on either purely attentional or convolutional structures. Other approaches such as the quasi-recurrent neural network (QRNN), partially abandon recurrence and speed training by an order of magnitude. It\u2019s exciting to see these new approaches reach state-of-the-art results while drastically reducing computational costs!", "*However, the authors note that since the GPU accelerated functionality of cuDNN is optimized for an LSTM but not for the type of convolutions needed for the GCNN, the overall time required to process a corpus of text is still similar between the RNN and GCNN approaches. Improvements to cuDNN\u2019s convolution implementation could potentially lead to much better computational performance for the GCNN.", "He, Kaiming, et al. \u201cDeep residual learning for image recognition.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.", "Vaswani, Ashish, et al. \u201cAttention is all you need.\u201d Advances in Neural Information Processing Systems. 2017.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD in Visual Neuroscience from UC Berkeley, now working on applying deep learning to natural language processing (NLP). Twitter: @davidwbressler"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5ba3ee730bfb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dwbressler?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dwbressler?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "David Bressler"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd543b0e79d50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&user=David+Bressler&userId=d543b0e79d50&source=post_page-d543b0e79d50----5ba3ee730bfb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ba3ee730bfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ba3ee730bfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://thegradient.pub/nlp-imagenet/", "anchor_text": "using pretrained models"}, {"url": "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9", "anchor_text": "faster softmax"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNNs"}, {"url": "https://research.fb.com/category/facebook-ai-research/", "anchor_text": "Facebook\u2019s AI Research (FAIR) group"}, {"url": "https://github.com/DavidWBressler/GCNN", "anchor_text": "Pytorch implementation"}, {"url": "https://vimeo.com/238222385]", "anchor_text": "https://vimeo.com/238222385"}, {"url": "https://arxiv.org/abs/1612.08083", "anchor_text": "\u201cLanguage Modeling with Gated Convolutional Networks\u201d"}, {"url": "https://vimeo.com/238222385", "anchor_text": "https://vimeo.com/238222385"}, {"url": "https://vimeo.com/238222385", "anchor_text": "https://vimeo.com/238222385"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet architecture"}, {"url": "https://www.coursera.org/learn/convolutional-neural-networks/lecture/XAKNO/why-resnets-work", "anchor_text": "built much deeper"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet architecture"}, {"url": "https://vimeo.com/238222385", "anchor_text": "https://vimeo.com/238222385"}, {"url": "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9", "anchor_text": "blog post"}, {"url": "https://github.com/DavidWBressler/GCNN/blob/master/GCNN.ipynb", "anchor_text": "Jupyter notebook"}, {"url": "https://docs.fast.ai/", "anchor_text": "fastai"}, {"url": "https://course.fast.ai/", "anchor_text": "fast.ai course"}, {"url": "https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset", "anchor_text": "Wikitext-2 dataset"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe pre-trained word vectors"}, {"url": "https://vimeo.com/238222385", "anchor_text": "Language Modeling with Gated Convolutional Networks --- Yann Dauphin, Angela Fan, Michael Auli\u2026vimeo.com"}, {"url": "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9", "anchor_text": "adaptive softmax"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "attentional"}, {"url": "https://arxiv.org/abs/1705.03122", "anchor_text": "convolutional"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "QRNN"}, {"url": "https://arxiv.org/abs/1611.01576", "anchor_text": "Bradbury, James, et al. \u201cQuasi-recurrent neural networks.\u201d arXiv preprint arXiv:1611.01576 (2016)."}, {"url": "https://arxiv.org/abs/1612.08083", "anchor_text": "Dauphin, Yann N., et al. \u201cLanguage modeling with gated convolutional networks.\u201d arXiv preprint arXiv:1612.08083 (2016)."}, {"url": "https://arxiv.org/abs/1705.03122", "anchor_text": "Gehring, Jonas, et al. \u201cConvolutional sequence to sequence learning.\u201d arXiv preprint arXiv:1705.03122 (2017)."}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "He, Kaiming, et al. \u201cDeep residual learning for image recognition.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani, Ashish, et al. \u201cAttention is all you need.\u201d Advances in Neural Information Processing Systems. 2017."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5ba3ee730bfb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5ba3ee730bfb---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----5ba3ee730bfb---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5ba3ee730bfb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----5ba3ee730bfb---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ba3ee730bfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&user=David+Bressler&userId=d543b0e79d50&source=-----5ba3ee730bfb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ba3ee730bfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&user=David+Bressler&userId=d543b0e79d50&source=-----5ba3ee730bfb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ba3ee730bfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5ba3ee730bfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5ba3ee730bfb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5ba3ee730bfb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dwbressler?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dwbressler?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David Bressler"}, {"url": "https://medium.com/@dwbressler/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "136 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd543b0e79d50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&user=David+Bressler&userId=d543b0e79d50&source=post_page-d543b0e79d50--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2e78ca83b4c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb&newsletterV3=d543b0e79d50&newsletterV3Id=2e78ca83b4c5&user=David+Bressler&userId=d543b0e79d50&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}