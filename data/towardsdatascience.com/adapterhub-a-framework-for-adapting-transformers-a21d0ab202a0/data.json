{"url": "https://towardsdatascience.com/adapterhub-a-framework-for-adapting-transformers-a21d0ab202a0", "time": 1683011138.414753, "path": "towardsdatascience.com/adapterhub-a-framework-for-adapting-transformers-a21d0ab202a0/", "webpage": {"metadata": {"title": "AdapterHub: A Framework for Adapting Transformers | by Nicole Nair | Towards Data Science", "h1": "AdapterHub: A Framework for Adapting Transformers", "description": "This blog post is an introduction to AdapterHub, a new framework released by Pfeiffer et al (2020b), that enables you to perform transfer learning of generalized pre-trained transformers such as\u2026"}, "outgoing_paragraph_urls": [{"url": "https://adapterhub.ml/", "anchor_text": "AdapterHub", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/2007.07779v1.pdf", "anchor_text": "Pfeiffer et al (2020b)", "paragraph_index": 0}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://adapterhub.ml/", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/", "anchor_text": "transformers and how they are usually fine-tuned", "paragraph_index": 2}, {"url": "http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf", "anchor_text": "Houlsby et al (2019)", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/2007.07779v1.pdf", "anchor_text": "AdapterHub paper", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/2007.07779", "anchor_text": "AdapterHub paper", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/2007.07779v1.pdf", "anchor_text": "AdapterHub paper", "paragraph_index": 14}, {"url": "https://adapterhub.ml/", "anchor_text": "AdapterHub.ml", "paragraph_index": 32}, {"url": "https://adapterhub.ml/", "anchor_text": "AdapterHub.ml", "paragraph_index": 33}, {"url": "https://adapterhub.ml/", "anchor_text": "here", "paragraph_index": 39}], "all_paragraphs": ["This blog post is an introduction to AdapterHub, a new framework released by Pfeiffer et al (2020b), that enables you to perform transfer learning of generalized pre-trained transformers such as BERT, RoBERTa, and XLM-R to downstream tasks such as question-answering, classification, etc. using adapters instead of fine-tuning.", "AdapterHub was built on top of the popular transformer package supplied by HuggingFace. You can find the HuggingFace transformers package here, and AdapterHub\u2019s modification here.", "To properly understand this article, I recommend that you first read up on transformers and how they are usually fine-tuned!", "I go into the details of this in the section \u2018The Benefits of Adapters\u2019 but as a sneak peek:", "Houlsby et al (2019) introduced something called an adapter. Adapters serve the same purpose as fine-tuning but do it by stitching in layers to the main pre-trained model, and updating the weights \u03a6 of these new layers, whilst freezing the weights \u03b8 of the pre-trained model.", "In contrast, you will recall that in fine-tuning, we are required to update the pre-trained weights as well.", "As you might imagine, this makes adapters much more efficient, both in terms of time and storage, compared to fine-tuning. Adapters have also been shown to be able to match the performance of state-of-the-art fine-tuning methods!", "I will simplify the contents of the AdapterHub paper which accompanied the release of the framework, to make it easier for you to start developing.", "The AdapterHub framework is important because prior to this framework, stitching in adapters or sharing adapters that one has trained was difficult and involved manually modifying the transformer architecture. This framework enables a", "dynamic \u201cstitching-in\u201d of pre-trained adapters for different tasks and languages", "In short, it makes using adapters for transfer learning much, much easier.", "In this post, we will review the various benefits of adapters as discussed in the AdapterHub paper and explain the various features of the new AdapterHub framework.", "The features will be accompanied by some sample code from the paper to get you started!", "If you are already familiar with adapters & their various benefits, you can skip straight to the section \u2018Key Features of AdapterHub.\u2019", "The benefits listed here correspond to a simplified version of those listed in Section 2.2 of the AdapterHub paper.", "As alluded to earlier, when comparing the performance of fine-tuning vs adapters on the GLUE benchmark (popular in natural language processing circles), there was no large difference in performance.", "This means that adapters can achieve the state-of-the-art on par with fine-tuning, whilst preserving the time and space efficiency listed as the next feature!", "To fully fine-tune a model, we need to \u2018store a copy\u2019 of the model for each task. This also \u2018impedes itemizing and parallelizing training.\u2019", "In contrast, adapters require MUCH LESS storage. To illustrate, Pfeiffer et al(2020b) provide the following example:", "for the popular Bert-Base model with a size of 440Mb, storing 2 fully fine-tuned models amounts to the same storage space required by 125 models with adapters, when using a bottleneck size of 48 and adapters of Pfeiffer et al. (2020a)", "A sub-benefit of this is that we can add many more tasks to an application by simply adding small adapters instead of hefty fine-tuning.", "Reproducibility across researchers is another wonderful result of the reduced storage requirements.", "When we stitch-in adapters, we fix the representations of the rest of the transformer, which means these adapters are encapsulated and can be stacked or moved or combined with other adapters.", "This modularity allows us to combine adapters from various tasks \u2014 something super important as NLP tasks get more complex.", "Natural language processing often involves sharing information across tasks. We often use something called Multi-Task Learning (MTL), but MTL suffers from two issues:", "With adapters, we train the adapter for each task separately, meaning that we overcome both issues above.", "Great, now let\u2019s look at the key features of this framework!", "Adapters in Transformer Layers + How to Train an Adapter", "To add the adapters, the authors used something called \u2018Mix-Ins\u2019 which are inherited by the HuggingFace transformer, so as to keep the codebases reasonably separate.", "In practice, here is how you add an adapter layer:", "You\u2019ll notice that the code mostly corresponds to regular HuggingFace transformers, and we just add to two lines to add & train the adapter.", "Something special about this AdapterHub framework is that you can dynamically configure the adapters, and change the architectures. Whilst you can use adapters directly from the literature \u2014 for example from Pfeiffer et al(2020a) or Houlsby et al (2020), you can also modify these architectures quite easily using a configuration file. In the code above, we use the default Pfeiffer (2020a) configuration.", "You can push the adapters that you train to AdapterHub.ml, as well as benefit from adapters other people have pre-trained. Unlike fine-tuning, where the entire large model must be shared, these adapters are lightweight and easily shareable!", "The search function of AdapterHub.ml works hierarchically:", "The website also helps you to identify compatible adapters depending on the pre-trained transformer that you specify.", "Use the following code to stitch in a pre-trained adapter, as opposed to training on your own (as detailed earlier):", "To be perfectly clear, the pre-trained adapter is loaded in the third line of code, whilst the pre-trained transformer is loaded in the second line of code.", "Just use regular HuggingFace code for inference! You may optionally load prediction heads when loading adapter weights.", "Recall that combining adapters for combined tasks etc is very much possible with this awesome framework!", "I\u2019d encourage you to try out the AdapterHub framework for yourself here.", "This framework is so exciting and I hope that this post helped you to start your journey of adapting transformers!", "Do let me know if you detect any errors in the post, or if you have any comments/critiques!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist. Digital Humanist. Reader of Books."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa21d0ab202a0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nicarina98?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicarina98?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "Nicole Nair"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8dddb92a2f6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&user=Nicole+Nair&userId=8dddb92a2f6e&source=post_page-8dddb92a2f6e----a21d0ab202a0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa21d0ab202a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa21d0ab202a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@wesleyphotography?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Wesley Tingey"}, {"url": "https://unsplash.com/s/photos/adapter?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://adapterhub.ml/", "anchor_text": "AdapterHub"}, {"url": "https://arxiv.org/pdf/2007.07779v1.pdf", "anchor_text": "Pfeiffer et al (2020b)"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "here"}, {"url": "https://adapterhub.ml/", "anchor_text": "here"}, {"url": "https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/", "anchor_text": "transformers and how they are usually fine-tuned"}, {"url": "http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf", "anchor_text": "Houlsby et al (2019)"}, {"url": "https://arxiv.org/pdf/2007.07779v1.pdf", "anchor_text": "AdapterHub paper"}, {"url": "https://arxiv.org/abs/2007.07779", "anchor_text": "AdapterHub paper"}, {"url": "https://arxiv.org/pdf/2007.07779v1.pdf", "anchor_text": "AdapterHub paper"}, {"url": "https://adapterhub.ml/", "anchor_text": "AdapterHub.ml"}, {"url": "https://adapterhub.ml/", "anchor_text": "AdapterHub.ml"}, {"url": "https://adapterhub.ml/", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a21d0ab202a0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----a21d0ab202a0---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a21d0ab202a0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----a21d0ab202a0---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a21d0ab202a0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa21d0ab202a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&user=Nicole+Nair&userId=8dddb92a2f6e&source=-----a21d0ab202a0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa21d0ab202a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&user=Nicole+Nair&userId=8dddb92a2f6e&source=-----a21d0ab202a0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa21d0ab202a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa21d0ab202a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a21d0ab202a0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a21d0ab202a0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicarina98?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicarina98?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nicole Nair"}, {"url": "https://medium.com/@nicarina98/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "55 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8dddb92a2f6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&user=Nicole+Nair&userId=8dddb92a2f6e&source=post_page-8dddb92a2f6e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4482935f49a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadapterhub-a-framework-for-adapting-transformers-a21d0ab202a0&newsletterV3=8dddb92a2f6e&newsletterV3Id=4482935f49a5&user=Nicole+Nair&userId=8dddb92a2f6e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}