{"url": "https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c", "time": 1682993780.445218, "path": "towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c/", "webpage": {"metadata": {"title": "Entropy is a measure of uncertainty | by Sebastian Kwiatkowski | Towards Data Science", "h1": "Entropy is a measure of uncertainty", "description": "Suppose you are talking with three patients in the waiting room of a doctor\u2019s office. All three of them have just completed a medical test which, after some processing, yields one of two possible\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.mtm.ufsc.br/~taneja/book/node43.html", "anchor_text": "other entropies", "paragraph_index": 11}, {"url": "https://math.stackexchange.com/questions/84719/why-is-h-used-for-entropy", "anchor_text": "Greek capital letter Eta", "paragraph_index": 13}, {"url": "https://books.google.de/books/about/Mathematical_Foundations_of_Information.html?id=0uvKF-LT_tMC&redir_esc=y", "anchor_text": "Khinchin (1957)", "paragraph_index": 33}], "all_paragraphs": ["Suppose you are talking with three patients in the waiting room of a doctor\u2019s office. All three of them have just completed a medical test which, after some processing, yields one of two possible results: the disease is either present or absent. Let\u2019s assume we are dealing with curious and data-oriented individuals. They\u2019ve researched the probabilities for their specific risk profiles in advance and are now eager to find out the result.", "Patient A knows that, statistically, there is a 95% chance that he has the disease in question. For Patient B, the probability of being diagnosed with illness is 30%. Patient C, in contrast, faces a 50/50 probability.", "I would like to focus on a simple question. All other things being equal, which of the three patients is confronted with the greatest degree of uncertainty?", "I think the answer is clear: patient C. Not only is he experiencing \u201ca lot of uncertainty\u201d. What he is going through is the greatest degree of uncertainty possible under the circumstances: a dramatic medical version of a flip with a fair coin.", "Compare this with patient A. Sure, the overall situation looks quite grim, but at least this patient is experiencing little uncertainty with regard to his medical prospects.", "Intuitively speaking, what can we say about patient B? Perhaps that her situation falls \u201csomewhere in the middle\u201d?", "This is where entropy comes in. Describing a situation as \u201csomewhere in the middle\u201d might be good enough for waiting room talk, but it\u2019s certainly too coarse a description for machine learning purposes.", "Entropy allows us to make precise statements and perform computations with regard to one of life\u2019s most pressing issues: not knowing how things will turn out.", "Entropy, in other words, is a measure of uncertainty.", "(It is also a measure of information, but, personally, I prefer the uncertainty interpretation. It might just be me, but things seemed a lot clearer when I no longer attempted to impose my preconceived notion of information on the equations.)", "In a way, saying that entropy is \u201ca measure of uncertainty\u201d is an understatement. Given certain assumptions (and foreshadowing an important result mentioned below), entropy is the measure of uncertainty.", "By the way, when I use the term entropy, I\u2019m referring to Shannon entropy. There are quite a few other entropies, but I think it\u2019s safe to assume that Shannon entropy is the one that is used most frequently in natural language processing and machine learning.", "So without further ado, here it is, the entropy formula for an event X with n possible outcomes and probabilities p_1, \u2026, p_n:", "If you are anything like me when I first looked at this formula, you might be asking yourself questions such as: Why the logarithm? Why is this a good measure of uncertainty at all? And, of course, why the letter H? (Apparently, the use of the English letter H evolved from the the Greek capital letter Eta, although the history appears to be quite complicated.)", "One thing I\u2019ve learned over time is that a good starting point \u2014 here and in many other cases \u2014 is to ask two questions: (1) Which desirable properties does the mathematical construct I\u2019m trying to understand have? And (2) Are they any competing constructs that have all of the these desirable properties?", "In short, the answers for Shannon entropy as a measure of uncertainty are: (1) many and (2) no.", "Let\u2019s proceed with a wish list.", "If your goal is to minimize uncertainty, stay away from uniform probability distributions.", "Quick reminder: A probability distribution is a function that assigns a probability to every possible outcome such that the probabilities add up to 1. A distribution is uniform when all of the outcomes have the same probability. For example, fair coins (50% tails, 50% tails) and fair dice (1/6 probability for each of the six faces) follow uniform distributions.", "A good measure of uncertainty achieves its highest values for uniform distributions. Entropy satisfies the criterion. Given n possible outcomes, maximum entropy is maximized by equiprobable outcomes:", "Here is the plot of the Entropy function as applied to Bernoulli trials (events with two possible outcomes and probabilities p and 1-p):", "Let A and B be independent events. In other words, knowing the outcome of event A does not tell us anything about the outcome of event B.", "The uncertainty associated with both events \u2014 this is another item on our wish list \u2014 should be the sum of the individual uncertainties:", "Let\u2019s use the example of flipping two coins to make this more concrete. We can either flip both coins simultaneously or first flip one coin and then flip the other one. Another way to think about this is that we can either report the outcome of the two coin flips at once or separately. The uncertainty is the same in either case.", "To make this even more concrete, consider two particular coins. The first coin lands heads (H) up with an 80% probability and tails (T) up with a probability of 20%. The probabilities for the other coin are 60% and 40%. If we flip both coins simultaneously, there are four possible outcomes: HH, HT, TH and TT. The corresponding probabilities are given by [ 0.48, 0.32, 0.12, 0.08 ].", "Plugging the numbers into the entropy formula, we see that:", "Suppose (a) you win whenever outcome #1 occurs and (b) you can choose between two probability distributions, A and B. Distribution A has two outcomes: say, 80% and 20%. Distribution B has three outcomes with probabilities 80%, 20% and 0%.", "Given the options A and B, which one would you choose? An appropriate reaction at this point would be to shrug your shoulders or roll your eyes. The inclusion of the third outcome neither increases nor decreases the uncertainty associated with the game. A or B, who cares. It doesn\u2019t matter.", "The entropy formula agrees with this assessment:", "In words, adding an outcome with zero probability has no effect on the measurement of uncertainty.", "The last of the basic properties is continuity.", "Famously, the intuitive explanation of a continuous function is that there are no \u201cgaps\u201d or \u201choles\u201d. More precisely, arbitrarily small changes in the output (uncertainty, in our case) should be achievable through sufficiently small changes in the input (probabilities).", "Logarithm functions are continuous at every point for which they are defined. So are sums and products of a finite number of functions that are continuous on a subset. It follows that the entropy function is continuous in its probability arguments.", "Khinchin (1957) showed that the only family of functions satisfying the four basic properties described above is of the following form:", "where \u03bb is a positive constant. Khinchin referred to this as the Uniqueness Theorem. Setting \u03bb = 1 and using the binary logarithm gives us the Shannon entropy.", "To reiterate, entropy is used because it has desirable properties and is the natural choice among the family functions that satisfy all items on the basic wish list (properties 1\u20134). (I might discuss the proof in a separate article in the future.)", "Entropy has many other properties beyond the four basic ones used in Khinchin\u2019s Uniqueness Theorem. Let me just mention some of them here.", "Suppose you have the choice between a fair coin and a fair die:", "And let\u2019s say you win if the coin lands heads up or the die lands on face 1.", "Which of the two options would you choose? A if you are a profit maximizer and B if you prefer with more variety and uncertainty.", "As the number of equiprobable outcomes increases, so should our measure of uncertainty.", "And, in general, if we let L(k) be the entropy of a uniform distribution with k possible outcomes, we have", "Do you know what negative uncertainty is? Neither do I.", "A user-friendly measure of uncertainty should always return a non-negative quantity, no matter what the input is.", "This is yet another criterion that is satisfied by entropy. Let\u2019s take another look at the formula:", "Probabilities are, by definition, in the range between 0 and 1 and, therefore, non-negative. The logarithm of a probability is non-positive. Multiplying the logarithm of a probability with a probability doesn\u2019t change the sign. The sum of non-positive products is non-positive. And finally, the negative of a non-positive value is non-negative. Entropy is, thus, non-negative for every possible input.", "Suppose you are in possession of a magical coin. No matter how you flip the coin, it always lands head up.", "How would you quantify the uncertainty about the magical or any other situation in which one outcome is certain to occur? Well, there is none. So the natural answer\u2014 I think, you will agree \u2014 is 0.", "Does entropy agree with this intuition? Of course.", "Suppose that outcome i certain is certain to occur. It follows that p_i, the probability of outcome i, is equal to 1. H(X), thus, simplifies to:", "This is another obviously desirable property. Consider two cases. In the first case, the probability of heads and tails are 80% and 20%. In the second case, the probabilities are reversed: heads 20%, tails 80%.", "In more general terms, for the case of two outcomes, we have:", "This fact applies to any number of outcomes. We can position the arguments (i.e., the probabilities of a distribution) in any order we like. The result of the entropy function is always the same.", "To recap, Shannon entropy is a measure of uncertainty.", "It is widely used because its satisfies certain criteria (and because life is full of uncertainty). The Uniqueness Theorem tells us that only one family of functions has all the four basic properties we\u2019ve mentioned. Shannon entropy is the natural choice among this family.", "In addition to other facts, entropy is maximal for uniform distributions (property #1), additive for independent events (#2), increasing in the number of outcomes with non-zero probabilities (#3 and #5), continuous (#4), non-negative (#6), zero for certain outcomes (#7) and permutation-invariant (#8).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe2c000301c2c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sekwiatkowski?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sekwiatkowski?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "Sebastian Kwiatkowski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe50114ad83ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&user=Sebastian+Kwiatkowski&userId=e50114ad83ae&source=post_page-e50114ad83ae----e2c000301c2c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2c000301c2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2c000301c2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.mtm.ufsc.br/~taneja/book/node43.html", "anchor_text": "other entropies"}, {"url": "https://math.stackexchange.com/questions/84719/why-is-h-used-for-entropy", "anchor_text": "Greek capital letter Eta"}, {"url": "https://books.google.de/books/about/Mathematical_Foundations_of_Information.html?id=0uvKF-LT_tMC&redir_esc=y", "anchor_text": "Khinchin (1957)"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e2c000301c2c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e2c000301c2c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----e2c000301c2c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e2c000301c2c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e2c000301c2c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2c000301c2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&user=Sebastian+Kwiatkowski&userId=e50114ad83ae&source=-----e2c000301c2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2c000301c2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&user=Sebastian+Kwiatkowski&userId=e50114ad83ae&source=-----e2c000301c2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2c000301c2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe2c000301c2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e2c000301c2c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e2c000301c2c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e2c000301c2c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e2c000301c2c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sekwiatkowski?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sekwiatkowski?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sebastian Kwiatkowski"}, {"url": "https://medium.com/@sekwiatkowski/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe50114ad83ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&user=Sebastian+Kwiatkowski&userId=e50114ad83ae&source=post_page-e50114ad83ae--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56560efdc5ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-is-a-measure-of-uncertainty-e2c000301c2c&newsletterV3=e50114ad83ae&newsletterV3Id=56560efdc5ac&user=Sebastian+Kwiatkowski&userId=e50114ad83ae&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}