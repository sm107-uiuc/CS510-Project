{"url": "https://towardsdatascience.com/learning-to-plan-with-value-iteration-networks-5768e80df2c3", "time": 1682995290.998304, "path": "towardsdatascience.com/learning-to-plan-with-value-iteration-networks-5768e80df2c3/", "webpage": {"metadata": {"title": "Learning to Plan with Value Iteration Networks | by Or Rivlin | Towards Data Science", "h1": "Learning to Plan with Value Iteration Networks", "description": "The first major achievement of deep reinforcement learning was the famous DQN algorithm\u2019s human level performance in various Atari video games, in which a neural network learned to play the game\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1812.02341.pdf", "anchor_text": "known to struggle with", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/1805.11199.pdf", "anchor_text": "been improved", "paragraph_index": 26}, {"url": "https://arxiv.org/pdf/1706.02416.pdf", "anchor_text": "more general domains such as graphs", "paragraph_index": 26}], "all_paragraphs": ["The first major achievement of deep reinforcement learning was the famous DQN algorithm\u2019s human level performance in various Atari video games, in which a neural network learned to play the game using the raw screen pixels as input. In reinforcement learning we wish to learn a policy that maps states to actions, such that it maximizes the accumulated rewards. In the DQN paper for example, the neural network is a Convolutional Neural Network that takes the screen image as the input and outputs scores for the possible actions.", "While reinforcement learning algorithms are designed so that this policy should learn to pick actions that have a long-term benefit, the information we get from our policy applies to the current state only. This is called a reactive policy, which is a policy that maps the current state to the action that should be taken right now, or to a probability distribution over the actions.", "While reactive policies dominate most of the reinforcement learning literature, and have led to some amazing and well publicized achievements, sometimes we need a more informative answer from our policy and the current recommended action is not enough. Sometimes our application requires that we can look ahead further than a single step and validate that our policy will take us on a \u201csafe\u201d trajectory, subject to scrutiny by other components of our system. In those cases, rather than having the current best action to take, what we would like is a complete plan, or at least a plan of some specified horizon into the future.", "But if our great RL algorithm can learn very good reactive policies, why would we want to bother with a complete plan? One possible reason is that RL policies often require a large number of attempts before successfully learning the task and are therefore usually trained in a simulated environment, in which the policy can crash the car as much as it likes, or shoot friendly forces and just try again if it fails. Simulations are not a perfect representation of the real world, and we would like our system to work despite those differences, something that RL agents are known to struggle with.", "Having a complete plan allows us to use external knowledge to evaluate it and prevent dangerous actions from being taken. For example, if an autonomous car wishes to change lanes but suddenly a car approaches very fast, faster than the simulated cars were during training, an external program can predict that the current planned trajectory is headed for a collision and abort the maneuver. This is much more difficult to do with a reactive policy, in which it might be difficult to predict how the scenario will end before it was played out.", "Another reason for desiring a complete plan is that it might make our policy perform better. Perhaps by forcing it to plan ahead we might constrain our policy to be more consistent and be able to adjust better with unseen situations, which is what we want.", "A very common model for planning problems is a Markov Decision Process, or MDP. In MDPs, we define the world as a set of states S, a set of possible actions to take A, a reward function R and a transition model P. together they comprise the tuple:", "Which defines the MDP. Let\u2019s look at a concrete example:", "In this example an agent must navigate the 2D map and get to the green tile, for which it gets a +1 reward, and avoid the red tile which will penalize it with -1 reward if stepped on. The rest of the states do not return a reward (or return 0 equivalently). As per our definition of MDP, the states S are the set of tiles on the map except the black tile, which is an obstacle. The set of actions are the four directions, and transition probabilities are given by the right illustration. The transition model specifies the probability of transitioning from the current state to another state given that a specific action was taken. In our example, if our agent chooses the action UP, it will have 80% chance of moving up, and 10% chance of moving either left or right. If the agent chooses the action RIGHT, it has an 80% chance of moving right, and 10% chance of either moving up or down.", "Suppose our agent is placed in the bottom left corner of the map, and must navigate safely to the green tile. A distinction must be made between planning a trajectory or finding a policy. If we plan a trajectory, we will get a sequence that specifies the order in which actions should be taken, for example: (UP,UP,RIGHT,RIGHT,RIGHT). If our problem was deterministic and choosing a direction would move our agent in that direction with a 100% chance, then this would correspond to the trajectory:", "However, since our problem is not deterministic, and we might diverge from the path we would like to take, this sequence of actions is not enough for us. What we would really like, is a complete plan that maps every state to the desired action, like this:", "This is in fact exactly what our RL policy represents, a mapping from states to actions. The difference is of course that in this simple problem we can lay out the entire state space before us and observe the policy. In complex real-world problems we might know our current state, but figuring out possible outcomes of future actions and states might be unrealistic., especially when our inputs are high dimensional sensor observations such as images or videos.", "But returning to our example, how can we find the optimal policy like the one described in the above illustration? A classic algorithm exists for that kind of problems called Value Iteration. What this algorithm does is calculate the long-term benefit that can be achieved by currently being in some state, by asking the question \u201cwhat is the best profit I can get if I start from this state?\u201d. This quantity is termed the state Value in MDP jargon, and intuitively it\u2019s easy to see that if we knew the Value of each state, we could try to always move to states that have higher Value and reap the benefits of their promise.", "Suppose we know the optimal Value for all the states in our problem; V*(s), we can now define the value of taking a specific action from our state and acting optimally henceforth; Q*(s,a).", "The state-action value is the reward we get from taking the action at this state in addition to the optimal value of the next state. But, since our problem is stochastic, we must take an expectation over it so that the different probabilities of getting to future states are considered. But since V*(s) is the optimal value we can get from the current state, this means that this equality must hold true:", "Which gives us a recursive definition of the optimal value function:", "If we guess a value for each state and it turns out that this recursive equation is satisfied for all of them, we know that we have an optimal value function. But more than that, we can guess any initial value function for our states, and use our equation to update it like this:", "And by using some math we can even prove that if at every iteration we perform this for all the states in our problem, eventually our value function will converge to the optimal value function. Once we have an optimal value function, we can easily extract the optimal policy:", "This entire process is known as Value Iteration.", "But this obviously cannot be done for complex problems in which we don\u2019t even have access to a description of the transition model and perhaps not to the reward function, so how can we use this in more interesting problems?", "A very interesting paper published in NIPS 2016 by researchers from Berkeley (they won the best paper award for it) attempts to solve this in a very elegant manner, by endowing a neural network with the ability to perform a similar kind of process inside it. The idea goes like this: since we don\u2019t really know the underlying MDP of our real problem, we can let our neural network learn some other MDP, that is different from the real one but that provides useful plans for the real problem. They demonstrate their approach on a 2D navigation problem, where the input is an image of the map that contains the goal position and the obstacles.", "The input image is fed to a convolutional neural network that outputs an image of the same size, that represents the \u201creward\u201d of different positions in the map, ant to another CNN that maps it to an initial value map. Then, for M iterations a convolutional layer is applied to the concatenated value and reward images to produce a Q value image, that has K channels that each represents an \u201caction\u201d in the learned MDP. Max pooling is performed along the dimension of the channels, essentially performing a very similar operation to that of the Value Iteration algorithm we have seen in the previous section.", "The final value map (or specific parts of it) is fed to a reactive policy that chooses the actions to perform in our real problem. It is important to note that at no stage is the model given ground truth labels of the true reward maps or value maps, we simply hope that by learning to act using this value iteration network (VIN) our neural network will learn a useful MDP.", "This whole model is trained by either imitation learning using given trajectories, or reinforcement learning from scratch, and as it turns out, what comes out is pretty awesome indeed:", "Without being given them, the neural network learned reward and value maps that seem exactly what we would think they should be. The reward map has high values at the goal position and adjacent to it, and negative values at the obstacles. The value map appears to have a gradient that directs points towards the goal position and away from the obstacles, which makes perfect sense.", "While this 2D navigation task seems simple, it is actually deceptively difficult. Training a standard CNN policy on a set of training maps works, but generalizes relatively poorly to unseen maps. Using VIN, the authors showed a much-improved generalization ability to unseen maps. In addition to the simple grid-world navigation task they also demonstrated their algorithm on a navigation problem using lunar surface elevation images in which a rover must navigate safely among non-traversable features, and on a web navigation problem using natural language inputs.", "I find it quite amazing that the model could learn these purely from the image input, and we can attribute this success to the inherent inductive bias produced by the architecture, which forces the model to perform computations in a way similar to a planning algorithm. This work has since then been improved and extended to more general domains such as graphs, but remains a very impressive achievement.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5768e80df2c3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://or-rivlin-mail.medium.com/?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": ""}, {"url": "https://or-rivlin-mail.medium.com/?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "Or Rivlin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6ea8553654c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&user=Or+Rivlin&userId=d6ea8553654c&source=post_page-d6ea8553654c----5768e80df2c3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5768e80df2c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5768e80df2c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://: http://karpathy.github.io/assets/rl/policy.png", "anchor_text": "source"}, {"url": "https://cdn-images-1.medium.com/max/1000/1*umJ5XJ3PS828PE0cr8nCmg.png", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1812.02341.pdf", "anchor_text": "known to struggle with"}, {"url": "https://i.stack.imgur.com/jkJ5b.png", "anchor_text": "source"}, {"url": "https://mpatacchiola.github.io/blog/images/reinforcement_learning_example_r004.png", "anchor_text": "source"}, {"url": "https://mpatacchiola.github.io/blog/images/reinforcement_learning_example_r004.png", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1602.02867.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1602.02867.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1602.02867.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1602.02867.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1805.11199.pdf", "anchor_text": "been improved"}, {"url": "https://arxiv.org/pdf/1706.02416.pdf", "anchor_text": "more general domains such as graphs"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5768e80df2c3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5768e80df2c3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5768e80df2c3---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5768e80df2c3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----5768e80df2c3---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5768e80df2c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&user=Or+Rivlin&userId=d6ea8553654c&source=-----5768e80df2c3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5768e80df2c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&user=Or+Rivlin&userId=d6ea8553654c&source=-----5768e80df2c3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5768e80df2c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5768e80df2c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5768e80df2c3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5768e80df2c3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5768e80df2c3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5768e80df2c3--------------------------------", "anchor_text": ""}, {"url": "https://or-rivlin-mail.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://or-rivlin-mail.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Or Rivlin"}, {"url": "https://or-rivlin-mail.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "793 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6ea8553654c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&user=Or+Rivlin&userId=d6ea8553654c&source=post_page-d6ea8553654c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8de554e94ad7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-plan-with-value-iteration-networks-5768e80df2c3&newsletterV3=d6ea8553654c&newsletterV3Id=8de554e94ad7&user=Or+Rivlin&userId=d6ea8553654c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}