{"url": "https://towardsdatascience.com/gradient-centralization-bdb333a53bcd", "time": 1683007953.279658, "path": "towardsdatascience.com/gradient-centralization-bdb333a53bcd/", "webpage": {"metadata": {"title": "Gradient Centralization. How to achieve more efficient training\u2026 | by Lady Pythonista | Towards Data Science", "h1": "Gradient Centralization", "description": "A recently published paper proposes a simple answer: Gradient Centralization, a new optimization technique which can be easily integrated in the gradient-based optimization algorithm you already use\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/2004.01461.pdf", "anchor_text": "paper", "paragraph_index": 0}, {"url": "https://medium.com/@tomelisse/gradient-centralization-in-keras-9e4e34a8b895", "anchor_text": "next article", "paragraph_index": 1}, {"url": "https://medium.com/@tomelisse/gradient-centralization-in-keras-9e4e34a8b895", "anchor_text": "next article", "paragraph_index": 31}, {"url": "https://instagram.com/ladypythonista?igshid=ZDdkNTZiNTM=", "anchor_text": "https://instagram.com/ladypythonista?igshid=ZDdkNTZiNTM=", "paragraph_index": 34}], "all_paragraphs": ["A recently published paper proposes a simple answer: Gradient Centralization, a new optimization technique which can be easily integrated in the gradient-based optimization algorithm you already use. It\u2019s a nice alternative to widely used Batch Normalization. All you need to do is centralize your gradient vectors so that they have zero mean!", "Here I will explore the theoretical details of the proposed method. The practical approach is discussed in my next article.", "The GC operation is defined as follows: from every column of the gradient matrix, we subtract the mean value of the column. For example if the gradient matrix looks like this:", "after the transformation it will look like this:", "since the respective column mean values are 4, 3, 5, 0.", "In other words we transform each gradient of loss function w.r.t. to the weight vector so that its mean is equal to zero. We can denote it using a fancy notation:", "If you want this to look even more advanced, you can introduce a cool P operator and use the matrix formulation:", "By means of direct matrix multiplication you can easily check that the above equation holds up:", "so indeed from each column, we subtract the mean value of this column.", "Now, the goal of this notation is not to make simple things more obscure, but to expose an elegant geometrical interpretation of the whole procedure. If you know a little bit about algebra, you perhaps can see already that the original gradients are being projected on a certain hyperplane in the weight space by means of using a projection operator P.", "To make sure that we are on the same page, let\u2019s review some basic facts about projection operators looking at a simple example \u2014 let\u2019s consider the following matrix P and what happens when it\u2019s applied to any vector v in a 3-dimensional space:", "As you can see, v loses its z component \u2014 in other words, it is being projected on the XY plane:", "What happens if you apply the projection operator again? Nothing, the vector cannot get more projected to the XY plane. This is one of the defining properties of a projection operator:", "If the projection is orthogonal another condition must be satisfied:", "If a vector e is perpendicular to the the projection plane, it is also perpendicular to a projection of any vector v to that plane:", "e.g. for our previous example with projection to XY plane, we can see that any vertical vector e satisfies the above equation:", "Now, if you check the above-mentioned conditions (1) and (2) for the previously defined GC operator P, you will conclude that it is an orthogonal projection.", "What is more, you can check that:", "which means that e is a normal vector of the hyperplane on which gradients are being projected.", "If we take any old weights represented by a vector w in the weight space and correct it by the projected gradient of the loss function (rather than the gradient itself), we will obtain the following new weights vector w\u2019:", "Applying transposed normal vector e to both sides of the above equation leads to:", "which means that for the weight computed in successive training iterations (0, 1, 2, \u2026, t) the following is true:", "The optimization problem can be therefore written as:", "which \u2014 in the words of the authors of the article \u2014 means that", "GC can be viewed as a projected gradient descent method with a constrained loss function.", "This constraint on the weight vectors regularizes the solution space of w leading to better generalization capacities of a trained model.", "Apart from regularizing the weight space, the proposed approach regularizes the output feature space. Given an input vector x, the output activation in the step t is computed as:", "Let\u2019s consider an input vector x\u2019 which differs from x only by a constant intensity change \u03b3:", "In can be shown (for details check the Appendix of the original article) that:", "Therefore, by choosing small initial weights we can ensure that output activations are not sensitive to intensity changes of input features.", "The article also mentions that the proposed technique accelerates the training process by means of optimization landscape smoothing and gradient explosion suppression, but I will not discuss these here.", "In the next article I show how to actually use the proposed technique.", "For more tips on Python programming & data science following me on Instagram:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Python developer | AI researcher | I love cats | https://instagram.com/ladypythonista?igshid=ZDdkNTZiNTM="], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbdb333a53bcd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ladypythonista?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ladypythonista?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "Lady Pythonista"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797ac7782277&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&user=Lady+Pythonista&userId=797ac7782277&source=post_page-797ac7782277----bdb333a53bcd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdb333a53bcd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdb333a53bcd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/JESHOOTS-com-264599/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585", "anchor_text": "Jan Va\u0161ek"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585", "anchor_text": "Pixabay"}, {"url": "https://arxiv.org/pdf/2004.01461.pdf", "anchor_text": "paper"}, {"url": "https://medium.com/@tomelisse/gradient-centralization-in-keras-9e4e34a8b895", "anchor_text": "next article"}, {"url": "https://arxiv.org/pdf/2004.01461.pdf", "anchor_text": "https://arxiv.org/pdf/2004.01461.pdf"}, {"url": "https://medium.com/@tomelisse/gradient-centralization-in-keras-9e4e34a8b895", "anchor_text": "next article"}, {"url": "https://www.instagram.com/ladypythonista/?igshid=ZDdkNTZiNTM%3D", "anchor_text": "https://www.instagram.com/ladypythonista/?igshid=ZDdkNTZiNTM%3D"}, {"url": "https://medium.com/tag/ai?source=post_page-----bdb333a53bcd---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----bdb333a53bcd---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----bdb333a53bcd---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----bdb333a53bcd---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bdb333a53bcd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbdb333a53bcd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&user=Lady+Pythonista&userId=797ac7782277&source=-----bdb333a53bcd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbdb333a53bcd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&user=Lady+Pythonista&userId=797ac7782277&source=-----bdb333a53bcd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdb333a53bcd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbdb333a53bcd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bdb333a53bcd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bdb333a53bcd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ladypythonista?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ladypythonista?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lady Pythonista"}, {"url": "https://medium.com/@ladypythonista/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "128 Followers"}, {"url": "https://instagram.com/ladypythonista?igshid=ZDdkNTZiNTM=", "anchor_text": "https://instagram.com/ladypythonista?igshid=ZDdkNTZiNTM="}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797ac7782277&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&user=Lady+Pythonista&userId=797ac7782277&source=post_page-797ac7782277--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F623477d155e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-centralization-bdb333a53bcd&newsletterV3=797ac7782277&newsletterV3Id=623477d155e3&user=Lady+Pythonista&userId=797ac7782277&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}