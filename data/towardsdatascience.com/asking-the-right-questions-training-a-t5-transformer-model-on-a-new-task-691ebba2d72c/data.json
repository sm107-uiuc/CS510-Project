{"url": "https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c", "time": 1683007845.9939132, "path": "towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c/", "webpage": {"metadata": {"title": "Asking the Right Questions: Training a T5 Transformer Model on a New task | by Thilina Rajapakse | Towards Data Science", "h1": "Asking the Right Questions: Training a T5 Transformer Model on a New task", "description": "I\u2019ve been itching to try the T5 (Text-To-Text Transfer Transformer) ever since it came out way, way back in October 2019 (it\u2019s been a long couple of months). I messed around with open-sourced code\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/google-research/text-to-text-transfer-transformer", "anchor_text": "code", "paragraph_index": 0}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 0}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 0}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "article", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "paper link", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/datasets/catalog/c4", "anchor_text": "Colossal Clean Crawled Corpus", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "paper", "paragraph_index": 4}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5", "anchor_text": "examples", "paragraph_index": 8}, {"url": "https://nijianmo.github.io/amazon/index.html", "anchor_text": "Amazon Review Data (2018)", "paragraph_index": 9}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 13}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 13}, {"url": "https://nijianmo.github.io/amazon/index.html", "anchor_text": "page", "paragraph_index": 16}, {"url": "https://www.wandb.com/", "anchor_text": "Weights & Biases", "paragraph_index": 27}, {"url": "https://app.wandb.ai/thilina/Question%20Generation%20with%20T5?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb", "anchor_text": "notebook", "paragraph_index": 32}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 49}], "all_paragraphs": ["I\u2019ve been itching to try the T5 (Text-To-Text Transfer Transformer) ever since it came out way, way back in October 2019 (it\u2019s been a long couple of months). I messed around with open-sourced code from Google a couple of times, but I never managed to get it to work properly. Some of it went a little over my head (Tensorflow \ud83d\ude2b ) so I figured I\u2019ll wait for Hugging Face to ride to the rescue! As always, the Transformers implementation is much easier to work with and I adapted it for use with Simple Transformers.", "Before we get to the good stuff, a quick word on what the T5 model is and why it\u2019s so exciting. According to the article on T5 in the Google AI Blog, the model is a result of a large-scale study (paper link) on transfer learning techniques to see which works best. The T5 model was pre-trained on C4 (Colossal Clean Crawled Corpus), a new, absolutely massive dataset, released along with the model.", "Pre-training is the first step of transfer learning in which a model is trained on a self-supervised task on huge amounts of unlabeled text data. After this, the model is fine-tuned (trained) on smaller labelled datasets tailored to specific tasks, yielding far superior performance compared to simply training on the small, labelled datasets without pre-training. Further information on pre-training language models can be found in my post below.", "A key difference in the T5 model is that all NLP tasks are presented in a text-to-text format. On the other hand, BERT-like models take a text sequence as an input and output a single class label or a span of text from the input. A BERT model is retrofitted for a particular task by adding a relevant output layer on top of the transformer model. For example, a simple linear classification layer is added for classification tasks. T5, however, eschews this approach and instead reframes any NLP task such that both the input and the output are text sequences. This means that the same T5 model can be used for any NLP task, without any aftermarket changes to the architecture. The task to be performed can be specified via a simple prefix (again a text sequence) prepended to the input as demonstrated below.", "The T5 paper explores many of the recent developments in NLP transfer learning. It is well worth a read!", "However, the focus of this article on adapting the T5 model to perform new NLP tasks. Thanks to the unified text-to-text approach, this turns out to be (surprisingly) easy. So, let\u2019s get to the aforementioned good stuff!", "The T5 model is trained on a wide variety of NLP tasks including text classification, question answering, machine translation, and abstractive summarization. The task we will be teaching our T5 model is question generation.", "Specifically, the model will be tasked with asking relevant questions when given a context.", "You can find all the scripts used in this guide in the examples directory of the Simple Transformers repo.", "We will be using the Amazon Review Data (2018) dataset which contains (among other things) descriptions of the various products on Amazon and question-answer pairs related to those products.", "The descriptions and the question-answer pairs must be downloaded separately. You can either download the data manually by following the instructions in the Descriptions and Question-Answer Pairs below, or you can use the provided shell script. The list of categories used in this study is given below.", "Alternatively, the shell script below should download all the necessary files by reading the links from the two text files also given below (place the text files in the same directory data/ as the shell script). It will also rename meta_ALL_Beauty.json.gz to meta_Beauty.json.gz to match the name in the question-answer file.", "With the data files in place, we can start training our model!", "We will be using the Simple Transformers library (based on the Hugging Face Transformers) to train the T5 model.", "The instructions given below will install all the requirements.", "We can process the data files and save them in a convenient format using the script given below. This will also split the data into train and evaluation sets.", "Adapted from the helpful scripts given in the Amazon Review Data page.", "Check whether you have the train_df.tsv and eval_df.tsv files in your data/ directory.", "The input data to a T5 model should be a Pandas DataFrame containing 3 columns as shown below.", "Internally, Simple Transformers will build the properly formatted input and target sequences (shown below) from the Pandas DataFrame.", "The input to a T5 model has the following pattern;", "The target sequence has the following pattern;", "The prefix value specifies the task we want the T5 model to perform. To train a T5 model to perform a new task, we simply train the model while specifying an appropriate prefix. In this case, we will be using the prefix ask_question. I.e. All the rows in our DataFrame will have the value ask_question in the prefix column.", "Training the model is quite straightforward with Simple Transformers.", "As you might observe from the training script, we are using the t5-large pre-trained model. Using these parameters with the t5-large model takes about 12 hours of training with a single Titan RTX GPU. Depending on your GPU resources, you can either increase the train_batch_size to speed up training or you can decrease it to fit a GPU with less VRAM (Titan RTX has 24 GB).", "Note that you can offset the effect of a small batch size by increasing the gradient_accumulation_steps. The effective batch size is roughly equal to train_batch_size * gradient_accumulation_steps.", "You can also significantly improve the training speed and GPU memory consumption by opting for the t5-base model. This will likely result in comparatively worse (but by no means poor) model.", "This training script will also automatically log the training progress using the Weights & Biases framework. You can see my logs here.", "Evaluating a language generation model is a little more complicated than evaluating something like a classification model. This is because there is no right answer you can compare against like you could with a classification model. The evaluation dataset contains descriptions and the questions that people have asked about those products, but that doesn\u2019t mean that those are the only right questions you can ask.", "Therefore, one of the best ways to evaluate a language generation model is to generate text and have it evaluated by an actual person (or several people).", "Speaking of generating text, impressive developments in decoding algorithms over the past few years has led to models capable of generating quite realistic text sequences. (Decoding algorithms are used to generate text)", "The following section gives a brief overview of the popular decoding algorithms currently in use.", "This section is based heavily on the Hugging Face notebook on text generation. I highly recommend going through that notebook to gain a more in-depth understanding of decoding algorithms as it does an excellent job of explaining the algorithms and showing how they can be used.", "We will be using a combination of both Top-K and Top-p sampling techniques to generate questions with our T5 model. This strategy typically leads to more natural-looking text.", "The predict() method of a Simple Transformers T5 model is used to generate the predictions or, in our case, the questions.", "Here, we are generating 3 questions for each description in the eval_df dataset.", "Let\u2019s take a look at some of the samples.", "Just for fun, I\u2019ve shuffled the generated questions with the actual question from the dataset. There are 4 questions for each description, 3 of which are generated and one is the original. See if you can tell which is which! I would love to see your guesses in the comments. \ud83d\ude09", "The Smart Solar San Rafael II Solar Mission Lantern will provide elegant ambiance to any outdoor setting and is ideal for your patio, deck or garden: made from all-weather poly-plastic with a seeded glass effect, the 15-inch lantern sits on any surface, or can be hung using the integrated hanging loop. The Rafael II is illuminated by two warm white LEDs in the top with a pillar candle inside the lantern that has an amber LED for a warm glowing effect. Powered by an integral mono-crystalline solar panel and rechargeable Ni-MH battery, the Rafael II requires no wiring or operating costs. The lantern automatically turns on at dusk and off at dawn. Smart Living Home & Garden offers a 1 year limited manufacturers warranty from the original date of purchase on full products bought from authorized distributors and retailers. Established in 2002, Smart Solar offers a wide selection of exclusively solar powered products. We design, manufacture, and customize all of our own items for your patio and garden. Enjoy our solar powered, energy efficient, and environmental friendly lighting solutions, water features, and outdoor decor. We are confident you will love solar living \u2014 that\u2019s why we\u2019ve been creating solar products and growing the solar lifestyle for nearly 15 years.", "Durable dog ball with treat hole", "Petco River Rock Shallow Creek Aquarium GravelPetco Aquarium Gravel is ideal for freshwater and safe for marine aquariums. This high quality gravel has colorful, durable coatings specifically developed for their permanence and non-toxicity. The gravel is processed to remove potentially harmful debris and materials. It will not affect the water\u2019s chemistry, nor harm any fish, invertebrates or plants. Can be used in aquariums, ponds, water gardens and terrariums.", "Enter a world of building fun with the LEGO City Starter Set featuring 3 iconic vehicles. Catch the robber with the policeman on his motorcycle! Put out the fire with the firemans speedy fire truck. Then race to help the fallen skater boy in the ambulance. Create endless play possibilities with all the inspiration a young builder needs to explore fun ways of saving the day! Includes 5 minifigures with accessories: robber, policeman, fireman, rescuer and a skater boy. 272 pcs. Ages 5 yrs. +.\u2019, \u201cEnter a world of building fun with the LEGO City Starter Set featuring 3 iconic vehicles. Catch the robber with the policeman on his motorcycle. Put out the fire with the fireman\u2019s speedy fire truck. Then race to help the fallen skater boy in the ambulance. Create endless play possibilities with all the inspiration a young builder needs to explore fun ways of saving the day. Includes 5 minifigures with accessories: robber, policeman, fireman, rescuer and a skater boy.", "Elegant and sleek, this TV Stand a new look to your home. Finished in a dark Espresso color. Two sliding doors. Four Sections for storage.", "Did we say cotton? You bet we did. The Men\u2019s Charged Cotton Longsleeve T-shirt may feel like a regular cotton T-shirt, but it\u2019s anything but ordinary. Its unique fabrication combines the classic comfort of cotton with the built-in water-resistance of all-weather gear to create the world\u2019s first true performance cotton T-shirt. It feels soft but dries faster than regular cotton, so you\u2019ll never be weighed down. Lightweight comfort. Stretchable mobility. This is the most powerful cotton T-shirt you\u2019ll ever put on. After all, it is Under Armour.", "Connect the Dotters!\u2019, \u201cConnect the Dotters! Dotters, our 10 happy-faced Dalmatian dog, is made of our super-soft Pluffies material that\u2019s not only cuddly, but machine washable!", "You can also test your model on other product descriptions. The script below uses a random description I found on eBay.", "For me, the most intriguing aspect of the T5 model is the ability to train it for an entirely new task by merely changing the prefix. In this article, we\u2019ve trained the model to generate questions by looking at product descriptions. However, it is entirely possible to have this same model trained on other tasks and switch between the different tasks by simply changing the prefix.", "This flexibility opens up a whole new world of possibilities and applications for a T5 model. I can\u2019t wait to see what comes next!", "You can also significantly improve the training speed and GPU memory consumption by opting for the t5-base model. This will likely result in comparatively poorer (but by no means poor) performance.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F691ebba2d72c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----691ebba2d72c---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F691ebba2d72c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----691ebba2d72c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F691ebba2d72c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&source=-----691ebba2d72c---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://pixabay.com/users/825545-825545/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=662711", "anchor_text": "Katrin B."}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=662711", "anchor_text": "Pixabay"}, {"url": "https://github.com/google-research/text-to-text-transfer-transformer", "anchor_text": "code"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "article"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "paper link"}, {"url": "https://www.tensorflow.org/datasets/catalog/c4", "anchor_text": "Colossal Clean Crawled Corpus"}, {"url": "https://towardsdatascience.com/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d", "anchor_text": "Understanding ELECTRA and Training an ELECTRA Language ModelHow does a Transformer Model learn a language? What\u2019s new in ELECTRA? How do you train your own language model on a\u2026towardsdatascience.com"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "paper"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5", "anchor_text": "examples"}, {"url": "https://nijianmo.github.io/amazon/index.html", "anchor_text": "Amazon Review Data (2018)"}, {"url": "http://deepyeti.ucsd.edu/jianmo/amazon/index.html", "anchor_text": "reviews URL"}, {"url": "http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_AMAZON_FASHION.json.gz", "anchor_text": "http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_AMAZON_FASHION.json.gz"}, {"url": "http://deepx.ucsd.edu/public/jmcauley/qa/", "anchor_text": "qa URL"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "here"}, {"url": "https://github.com/NVIDIA/apex", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/installation/#installation-steps", "anchor_text": "docs"}, {"url": "https://nijianmo.github.io/amazon/index.html", "anchor_text": "page"}, {"url": "https://www.wandb.com/", "anchor_text": "Weights & Biases"}, {"url": "https://app.wandb.ai/thilina/Question%20Generation%20with%20T5?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb", "anchor_text": "notebook"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----691ebba2d72c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----691ebba2d72c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----691ebba2d72c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----691ebba2d72c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F691ebba2d72c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----691ebba2d72c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F691ebba2d72c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----691ebba2d72c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F691ebba2d72c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----691ebba2d72c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----691ebba2d72c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Written by Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "1.6K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----691ebba2d72c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fasking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----691ebba2d72c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5?source=author_recirc-----691ebba2d72c----0---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=author_recirc-----691ebba2d72c----0---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=author_recirc-----691ebba2d72c----0---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----691ebba2d72c----0---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5?source=author_recirc-----691ebba2d72c----0---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Multi-Label Classification using BERT, RoBERTa, XLNet, XLM, and DistilBERT with Simple TransformersLearn how to use Transformer Models to perform Multi-Label Classification in just 3 lines of code with Simple Transformers."}, {"url": "https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5?source=author_recirc-----691ebba2d72c----0---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "\u00b74 min read\u00b7Nov 9, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3e0cda12ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----b3e0cda12ce5----0-----------------clap_footer----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5?source=author_recirc-----691ebba2d72c----0---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3e0cda12ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5&source=-----691ebba2d72c----0-----------------bookmark_preview----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----691ebba2d72c----1---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----691ebba2d72c----1---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----691ebba2d72c----1---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----691ebba2d72c----1---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----691ebba2d72c----1---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----691ebba2d72c----1---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----691ebba2d72c----1---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----691ebba2d72c----1-----------------bookmark_preview----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----691ebba2d72c----2---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----691ebba2d72c----2---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----691ebba2d72c----2---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----691ebba2d72c----2---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----691ebba2d72c----2---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----691ebba2d72c----2---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----691ebba2d72c----2---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----691ebba2d72c----2-----------------bookmark_preview----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04?source=author_recirc-----691ebba2d72c----3---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=author_recirc-----691ebba2d72c----3---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=author_recirc-----691ebba2d72c----3---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/swlh?source=author_recirc-----691ebba2d72c----3---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "The Startup"}, {"url": "https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04?source=author_recirc-----691ebba2d72c----3---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "A Simple Guide On Using BERT for Text Classification.The A-to-Z of how you can use Google\u2019s BERT for binary text classification tasks with Python and PyTorch."}, {"url": "https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04?source=author_recirc-----691ebba2d72c----3---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": "\u00b710 min read\u00b7Jun 9, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fswlh%2Fbbf041ac8d04&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Fa-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----bbf041ac8d04----3-----------------clap_footer----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04?source=author_recirc-----691ebba2d72c----3---------------------7e5bfb34_40c7_4136_aec3_540dae482b96-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbbf041ac8d04&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Fa-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04&source=-----691ebba2d72c----3-----------------bookmark_preview----7e5bfb34_40c7_4136_aec3_540dae482b96-------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "See all from Thilina Rajapakse"}, {"url": "https://towardsdatascience.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----691ebba2d72c----0-----------------bookmark_preview----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----1-----------------clap_footer----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----691ebba2d72c----1-----------------bookmark_preview----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/ask-and-you-shall-receive-using-bert-for-question-answering-with-the-world-happiness-report-3fa8bf4df536?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://huonglanchu.medium.com/?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://huonglanchu.medium.com/?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Lan Chu"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/ask-and-you-shall-receive-using-bert-for-question-answering-with-the-world-happiness-report-3fa8bf4df536?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Ask, and You Shall Receive: Building a question-answering system with BertLet\u2019s see which country is the happiest country in the world according to Bert!"}, {"url": "https://pub.towardsai.net/ask-and-you-shall-receive-using-bert-for-question-answering-with-the-world-happiness-report-3fa8bf4df536?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "\u00b710 min read\u00b7Jan 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F3fa8bf4df536&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fask-and-you-shall-receive-using-bert-for-question-answering-with-the-world-happiness-report-3fa8bf4df536&user=Lan+Chu&userId=3916743f0e10&source=-----3fa8bf4df536----0-----------------clap_footer----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/ask-and-you-shall-receive-using-bert-for-question-answering-with-the-world-happiness-report-3fa8bf4df536?source=read_next_recirc-----691ebba2d72c----0---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fa8bf4df536&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fask-and-you-shall-receive-using-bert-for-question-answering-with-the-world-happiness-report-3fa8bf4df536&source=-----691ebba2d72c----0-----------------bookmark_preview----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----691ebba2d72c----1---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----691ebba2d72c----1-----------------bookmark_preview----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/towards-stand-alone-self-attention-in-vision-3d0561c6aee5?source=read_next_recirc-----691ebba2d72c----2---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://medium.com/@ju2ez?source=read_next_recirc-----691ebba2d72c----2---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://medium.com/@ju2ez?source=read_next_recirc-----691ebba2d72c----2---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Julian Hatzky"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----691ebba2d72c----2---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/towards-stand-alone-self-attention-in-vision-3d0561c6aee5?source=read_next_recirc-----691ebba2d72c----2---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Towards Stand-Alone Self-Attention in VisionA deep dive into the application of the transformer architecture and its self-attention operation for vision"}, {"url": "https://towardsdatascience.com/towards-stand-alone-self-attention-in-vision-3d0561c6aee5?source=read_next_recirc-----691ebba2d72c----2---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "\u00b714 min read\u00b74 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d0561c6aee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-stand-alone-self-attention-in-vision-3d0561c6aee5&user=Julian+Hatzky&userId=e24e3594d8a&source=-----3d0561c6aee5----2-----------------clap_footer----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/towards-stand-alone-self-attention-in-vision-3d0561c6aee5?source=read_next_recirc-----691ebba2d72c----2---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d0561c6aee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-stand-alone-self-attention-in-vision-3d0561c6aee5&source=-----691ebba2d72c----2-----------------bookmark_preview----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----691ebba2d72c----3---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----691ebba2d72c----3---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----691ebba2d72c----3---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Benjamin Marie"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----691ebba2d72c----3---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----691ebba2d72c----3---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "Translate with GPT-3Machine translation but without a machine translation system"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----691ebba2d72c----3---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": "\u00b718 min read\u00b7Nov 22, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&user=Benjamin+Marie&userId=ad2a414578b3&source=-----9903c4a6f385----3-----------------clap_footer----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----691ebba2d72c----3---------------------7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&source=-----691ebba2d72c----3-----------------bookmark_preview----7c63a0b3_ccc4_4aa3_aa9b_a3b2de3d9bb8-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----691ebba2d72c--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}