{"url": "https://towardsdatascience.com/neurips-2020-10-essentials-you-shouldnt-miss-845723f3add6", "time": 1683017193.841471, "path": "towardsdatascience.com/neurips-2020-10-essentials-you-shouldnt-miss-845723f3add6/", "webpage": {"metadata": {"title": "NeurIPS 2020 \u2014 10 essentials you shouldn\u2019t miss | by Sergi Castella i Sap\u00e9 | Towards Data Science", "h1": "NeurIPS 2020 \u2014 10 essentials you shouldn\u2019t miss", "description": "The Conference in Neural Information Processing Systems is always exciting because it serves as a collection of the best the field has offered in the preceeding year. Despite going fully virtual for\u2026"}, "outgoing_paragraph_urls": [{"url": "https://search.zeta-alpha.com", "anchor_text": "AI Research Navigator", "paragraph_index": 1}, {"url": "https://www.zeta-alpha.com", "anchor_text": "Zeta Alpha", "paragraph_index": 1}, {"url": "https://nips.cc/virtual/2020/public/workshop_16146.html", "anchor_text": "its own full workshop", "paragraph_index": 2}, {"url": "https://openreview.net/forum?id=ByeL1R4FvS", "anchor_text": "rejected at ICLR 2020", "paragraph_index": 6}, {"url": "https://papers.nips.cc/paper/2020/hash/29539ed932d32f1c56324cded92c07c2-Abstract.html", "anchor_text": "Self-Supervised Relational Reasoning for Representation Learning", "paragraph_index": 14}, {"url": "https://search.zeta-alpha.com/?q=self-supervision&retrieval_method=keyword&retrieval_unit=document&years_range=2020%2C2020&doc_sources=Advances+in+Neural+Information+Processing+Systems&sort_by=relevance", "anchor_text": "a more comprehensive selection", "paragraph_index": 14}, {"url": "https://search.zeta-alpha.com?doc_ids=8146fcf9aafb5c96a2b35822ec9337c00c4825ce", "anchor_text": "Efficient Transformers Survey\u2075", "paragraph_index": 17}, {"url": "https://papers.nips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html", "anchor_text": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping", "paragraph_index": 23}, {"url": "https://papers.nips.cc/paper/2020/hash/9ed27554c893b5bad850a422c3538c15-Abstract.html", "anchor_text": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers", "paragraph_index": 23}, {"url": "https://papers.nips.cc/paper/2020/hash/1325cdae3b6f0f91a1b629307bf2d498-Abstract.html", "anchor_text": "Deep Transformers with Latent Depth", "paragraph_index": 23}, {"url": "https://search.zeta-alpha.com?doc_ids=068dbb97ed246e21eec65fc39d03cab99f425821,bbc53b06f54ab365ba19e6c0ff739d293fcb40c5,fc182f0a7facc9e425b322754ce2c2b0de6d8597,90560b9d04d7d2d10988b69734cb07015cbf410e,46d52802c16f82dc212e55d12084e659729ddb2c,325df8a0c1aa055882d86046cb558b8cb80ca528,ed47a838ebf4f30c39f04529913e9bcc8db3cd58,6bdbb0128e0b09ebc9677ab70de4bbc7eb8c7b87,cd7b042d68bfca545facc2359e8dfad4e83515e7,56e181716900991a8af537ea269f2de2019a2e71,c4cdf427b9512abb21c96b4279fe44d63b63e4be,85473222e0d814fe96eb7317848ad09ac00a4116,ab4a485c81cc41b2878433cde860e8823557494d", "anchor_text": "a more comprehensive list", "paragraph_index": 23}, {"url": "http://ogb.stanford.edu", "anchor_text": "ogb.stanford.edu", "paragraph_index": 29}, {"url": "https://papers.nips.cc/paper/2020/hash/75877cb75154206c4e65e76b88a12712-Abstract.html", "anchor_text": "Can Graph Neural Networks Count Substructures?", "paragraph_index": 30}, {"url": "https://papers.nips.cc/paper/2020/hash/1fc30b9d4319760b04fab735fbfed9a9-Abstract.html", "anchor_text": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games", "paragraph_index": 30}, {"url": "https://papers.nips.cc/paper/2020/hash/61c66a2f4e6e10dc9c16ddf9d19745d6-Abstract.html", "anchor_text": "Factor Graph Neural Networks", "paragraph_index": 30}, {"url": "https://papers.nips.cc/paper/2020/hash/bf15e9bbff22c7719020f9df4badc20a-Abstract.html", "anchor_text": "BONGARD-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning", "paragraph_index": 31}, {"url": "https://papers.nips.cc/paper/2020/hash/e5a90182cc81e12ab5e72d66e0b46fe3-Abstract.html", "anchor_text": "A Benchmark for Systematic Generalization in Grounded Language Understanding", "paragraph_index": 31}, {"url": "https://papers.nips.cc/paper/2020/hash/51200d29d1fc15f5a71c1dab4bb54f7c-Abstract.html", "anchor_text": "RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning", "paragraph_index": 31}, {"url": "https://search.zeta-alpha.com?doc_ids=97d01f5511ee05e187efc223e096f01d481b8626,aa46f6f1af235d5a7fc620c26fb2c0134ac039ae,907d0b551d1692ec2a15c043007412285fb565b0,e0718b33607667c2e9ee901ec3d6a308807a5aac,1e0b52415fd332aae1f808ac074d286fa51f5d8c,92d809c9833daafaa62e08c315cd43765dc0619b,3e9abb147255a53ad98660b1010362da0850212a,941a7309b2ebcc8a5fe225e6beb525ec2e7c22b8", "anchor_text": "a more comprehensive list", "paragraph_index": 31}, {"url": "https://twitter.com/ZetaVector", "anchor_text": "@zetavector", "paragraph_index": 32}, {"url": "http://bit.ly/3ITDNNE", "anchor_text": "bit.ly/3ITDNNE", "paragraph_index": 41}, {"url": "http://linkedin.com/in/sergicastella", "anchor_text": "linkedin.com/in/sergicastella", "paragraph_index": 41}], "all_paragraphs": ["The Conference in Neural Information Processing Systems is always exciting because it serves as a collection of the best the field has offered in the preceeding year. Despite going fully virtual for the first time, this year is no different; I mean, look at the top 25 already cited papers published\ud83d\udc47", "Making sense of this impressive lineup is no easy feat, but with some help from the AI Research Navigator at Zeta Alpha, we went through the most relevant NeurIPS papers by citations, spotlight presentations and some recommendations from the platform and we identified some really cool works we\u2019d like to highlight; some are already well known, and some are more of a hidden gem. Of course these picks do not aim to be a comprehensive overview \u2014 we\u2019ll be missing on many topics such as multimodal ML, Federated Learning, GANs, Reinforcement Learning, ML theory, ML for ODEs, among others \u2014 but hey, I\u2019ve heard it\u2019s often better to choose sparse and deeply than broad and shallow; so here\u2019s my top-10, enjoy!", "Leaving behind the costly reliance on labelled data has been one of the main focus in recent years\u2019 ML agenda, it even has its own full workshop at NeurIPS this year.", "\u2753Why: the results on this paper seem bizzarre, which is why it\u2019s so interesting. How can one learn representations with only positive samples and not collapse into a trivial solution?", "\ud83d\udca1Key insights: the method is fairly similar to a standard contrastive learning setting for computer vision, where augmentations are applied to images and a contrastive loss forces the images coming from the same source to come together and pushes the rest away. However, in this paper, there are no negative samples. Instead, there are two encoders:", "The training procedure consists of encoding representations of different views of an image through T and T\u2019 and maximize the dot product of these representations. The fact that this method doesn\u2019t collapse into a trivial representation is already impressive, but the results on ImageNet don\u2019t fall short either.", "\u2753Why: consistency training has a lot of potential to be a generic procedure that improves weak supervision in many tasks. As an extra fun fact, the paper was rejected at ICLR 2020, but is now at NeurIPS with an already strong citation record.", "\ud83d\udca1Key insights: in a nutshell, the unsupervised consistency loss consists on an agreement loss on different variations of an input (like back translation for text or random augmentation for images). The intuition is: different variations of an input need to have the same output classification, despite not knowing which one, which is a valid learning signal for a classification model M. Under this setting, remarkably few true labels are needed to learn a good classifier.", "The results are nothing short of impressive both in Computer Vision and Natural Language Processing, where as little as 20 labels are enough to get decent performance in tasks like sentiment analysis on the IMDb dataset\u00b9.", "\u2753Why: contrastive learning can be understood from the lens of Information Theory, and this paper is an excellent combination of empirical and theoretical results which are helpful to better understand the fundamentals of this family of methods.", "\ud83d\udca1Key insights: Constrastive Learning in Computer Vision often implies generating different views of an image \u2014 such as croppings, filtering or other transformations \u2014 and learn a model that is able to discriminate between views from this image and the rest. Interestingly, this can be formulated as maximizing the mutual information between views of the image. Diving deeper into this framework, the paper shows:", "\u2753Why: similarly as in the previous suggestion, contrastive learning is one of the pillars of self-supervised representation learning, but regarding hard negatives, their impact in the quality of learned representations is not well understood.", "\ud83d\udca1Key insights: the authors propose a new method for adding synthetic hard negatives during training which are cheap computationally: MoCHi (Mixing of Contrastive Hard Negatives). The method creates synthetic hard negatives directly in the embedding space by:", "Surprisingly, this simple method improves self-supervised representation learning on images, and broad ablations are performed to understand their effect.", "Others: Self-Supervised Relational Reasoning for Representation Learning and a more comprehensive selection.", "\u2753Why: around 2017 and 2018, seq2seq models went from being RNN almost across the board (GRUs\u00b2, LSTMs\u00b3) to being fully Attention based (Transformers\u2074). But isn\u2019t recurrence still a valid inductive bias in NNs? Can we shed some light into self-attentive RNNs, in the sense of what general principles makes them good for learning? This paper provides a theoretical framework to think about it.", "\ud83d\udca1Key insights: full self-attention has the problem that it scales badly with sequence length (quadratic), and recurrence has the problem that information flow fails to travel \u201clong temporal distances\u201d due to the well known vanishing gradient effect, for which only heuristic-based solutions exist. This paper formalizes this tradeoff and shows how attention sparsity and gradient flow depth bound the computational complexity and the information flow in these types of networks. Somewhere within this tradeoff interesting things happen, such as an intriguingly good generalization in RL.", "\u2753Why: while BigBird is neither the first nor the last reincarnation of an efficient Transformer \u2014 see the zoo of approaches in the fantastic Efficient Transformers Survey\u2075 \u2014 this version contains neat engineering tricks and has solid results.", "\ud83d\udca1Key idea: combine 3 different attention forms: window, global and random. With these tricks, the number of operations needed for the attention mechanism can be linear with respect to the sequence length. While this is by no means a tiny model \u2014 the window attention for their experiments is already 512 tokens, just like the OG BERT\u2076 \u2014 this attention mode enables modelling of much longer sequences, such as those required in genomics, for which this publication provides some results on.", "\u2753Why: the main appeal is the use of fully non-parametric memory, which while not novel, has the potential to allow for question answering systems that don\u2019t need to be retrained to adapt to new or changing knowledge because they completely rely on external knowledge.", "\ud83d\udca1Key idea: retrieve documents as evidence, whose text is used as context for text generation. A part from results being state of the art, they show results on question answering over changing knowledge and show how RAG can answer questions for which it wasn\u2019t trained on by swapping the collection of documents it gets knowledge from (without any re-training). Moreover, factual correctness seems to be a strong feature of this approach, although it still falls short to be called truly reliable.", "\u2753Why: (aka GPT-3) a lot has been said about GPT-X series\u2077 and there\u2019s no doubt the latest iteration has impressed the most skeptic people in the field. Originally released in July, it\u2019s now worth reading this streamlined version of the work.", "\ud83d\udca1Key insight: size, size, size. Scaling up models keeps improving performance and leading to surprising results; the ceiling seems to still be far away\u2026 GPT-3 trains a 175 Billion parameter model which shows surprising results in few shot learning, where the model only needs a couple of examples to learn any language task to an astonishing degree. Still, many concerns arise such as the cost an environmental impact of such models as well as the biases they reveal.", "Others: Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping, O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers, Deep Transformers with Latent Depth and a more comprehensive list.", "\u2753Why: sometimes measuring performance in a task is just as hard if not harder than solving the task itself. Summarization is a good example: works often rely on measures such as ROUGE\u2078 which correlate with human judgements only to a certain degree; and when models are close to that boundary, the measure ceases to be useful.", "\ud83d\udca1Key idea: 3 steps that can be repeated iteratively", "This evaluation seems to correlate better with human judgements, although this comes at the cost of this metric being less universal and explainable.", "\u2753Why: graphs have been specially hot in the field for a couple of years, and graphs need their gold benchmark backed by heavyweights in the field. This one is a strong contender.", "\ud83d\udca1Keys: the main defining features of this benchmark are diverse sizes (from 100k to 100M nodes), coverage of many domains and multiple task categories (node, link and property predictions). Moreover, the authors claim that their experiments so far show significant challenges of scalability and out-of-distribution generalization, which resonate strongly with the challenges that real-world data presents.", "The steering committee backing it up includes giants such as Tommi Jaakkola, Yoshua Bengio and Max Welling, and the benchmark does not only include data, but also a pipeline for managing it (loading, evaluating, etc.) which can be found at ogb.stanford.edu and provides graph objects compatible with PyTorch, PyTorch Geometric and Deep Graph Library.", "Other relevant papers on Graphs at NeurIPS: Can Graph Neural Networks Count Substructures?, Learning Dynamic Belief Graphs to Generalize on Text-Based Games, Factor Graph Neural Networks.", "Others benchmarks at NeurIPS: BONGARD-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning, A Benchmark for Systematic Generalization in Grounded Language Understanding, RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning; and a more comprehensive list.", "What an exciting set of papers, it was honestly really hard to narrow it down to 10. As a closing note, I\u2019d like to mention how much of a pleasure it is to read NeurIPS papers, as they\u2019re way more polished than your average arxiv.org publication. Regardless, this little NeurIPS collection ends here, but there\u2019s still so much more to explore for the conference and I\u2019m really looking forward to it. The team will be reporting interesting insights live from our company twitter feed at @zetavector, so tune in if you don\u2019t want to miss a thing.", "What about you? What are you most looking forward about the conference? Feel free to share some suggestions down in the comments\ud83d\udc47", "[1] Learning Word Vectors for Sentiment Analysis, Maas et al. 2011.", "[2] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, Kyunghyun Cho et al. 2014.", "[6] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, et al. 2018.", "[7] Improving Language Understanding by Generative Pre-Training, Alec Radford et al. 2018.", "[8] ROUGE: a Package for Automatic Evaluation of Summaries, Chin-Yew Lin 2004.", "[9] Proximal Policy Optimization Algorithms, John Schulman et al. 2017.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\ud83d\udd0e Working on search technology at Zeta Alpha // \ud83c\udf99 Neural IR Podcast: bit.ly/3ITDNNE // \ud83d\udc54 Linkedin: linkedin.com/in/sergicastella"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F845723f3add6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----845723f3add6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----845723f3add6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sergicastella?source=post_page-----845723f3add6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=post_page-----845723f3add6--------------------------------", "anchor_text": "Sergi Castella i Sap\u00e9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e27e64320ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=post_page-1e27e64320ad----845723f3add6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F845723f3add6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F845723f3add6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@mbenna?utm_source=medium&utm_medium=referral", "anchor_text": "Mike Benna"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://search.zeta-alpha.com", "anchor_text": "AI Research Navigator"}, {"url": "https://www.zeta-alpha.com", "anchor_text": "Zeta Alpha"}, {"url": "https://nips.cc/virtual/2020/public/workshop_16146.html", "anchor_text": "its own full workshop"}, {"url": "https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html", "anchor_text": "Bootstrap Your Own Latent, A New Approach to Self-Supervised Learning"}, {"url": "https://nips.cc/virtual/2020/public/poster_4c2e5eaae9152079b9e95845750bb9ab.html", "anchor_text": "Virtual Poster"}, {"url": "https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html", "anchor_text": "Unsupervised Data Augmentation for Consistency Training"}, {"url": "https://nips.cc/virtual/2020/public/poster_44feb0096faa8326192570788b38c1d1.html", "anchor_text": "Virtual Poster"}, {"url": "https://openreview.net/forum?id=ByeL1R4FvS", "anchor_text": "rejected at ICLR 2020"}, {"url": "https://papers.nips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf"}, {"url": "https://search.zeta-alpha.com?doc_ids=180aaa74e1b97e1efbe58bb41ab570b56fc787eb", "anchor_text": "What Makes for Good Views for Contrastive Learning?"}, {"url": "https://nips.cc/virtual/2020/public/poster_4c2e5eaae9152079b9e95845750bb9ab.html", "anchor_text": "Virtual Poster"}, {"url": "https://proceedings.neurips.cc/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf", "anchor_text": "https://proceedings.neurips.cc/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html", "anchor_text": "Hard Negative Mixing for Contrastive Learning"}, {"url": "https://nips.cc/virtual/2020/public/poster_f7cade80b7cc92b991cf4d2806d6bd78.html", "anchor_text": "Virtual Poster"}, {"url": "https://papers.nips.cc/paper/2020/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/29539ed932d32f1c56324cded92c07c2-Abstract.html", "anchor_text": "Self-Supervised Relational Reasoning for Representation Learning"}, {"url": "https://search.zeta-alpha.com/?q=self-supervision&retrieval_method=keyword&retrieval_unit=document&years_range=2020%2C2020&doc_sources=Advances+in+Neural+Information+Processing+Systems&sort_by=relevance", "anchor_text": "a more comprehensive selection"}, {"url": "https://papers.nips.cc/paper/2020/hash/e2065cb56f5533494522c46a72f1dfb0-Abstract.html", "anchor_text": "Untangling tradeoffs between recurrence and self-attention in neural networks"}, {"url": "https://nips.cc/virtual/2020/public/poster_e2065cb56f5533494522c46a72f1dfb0.html", "anchor_text": "Virtual Poster"}, {"url": "https://papers.nips.cc/paper/2020/file/e2065cb56f5533494522c46a72f1dfb0-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/e2065cb56f5533494522c46a72f1dfb0-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html", "anchor_text": "Big Bird: Transformers for Long Sequences"}, {"url": "https://nips.cc/virtual/2020/public/poster_c8512d142a2d849725f31a9a7a361ab9.html", "anchor_text": "Virtual Poster"}, {"url": "https://search.zeta-alpha.com?doc_ids=8146fcf9aafb5c96a2b35822ec9337c00c4825ce", "anchor_text": "Efficient Transformers Survey\u2075"}, {"url": "https://papers.nips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html", "anchor_text": "Retrieval-Augmented Generation for Knowledge-Intensive NLP tasks"}, {"url": "https://nips.cc/virtual/2020/public/poster_6b493230205f780e1bc26945df7481e5.html", "anchor_text": "Virtual Poster"}, {"url": "https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf", "anchor_text": "https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html", "anchor_text": "Language Models are Few -Shot Learners"}, {"url": "https://nips.cc/virtual/2020/public/poster_1457c0d6bfcb4967418bfb8ac142f64a.html", "anchor_text": "Virtual Poster"}, {"url": "https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html", "anchor_text": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"url": "https://papers.nips.cc/paper/2020/hash/9ed27554c893b5bad850a422c3538c15-Abstract.html", "anchor_text": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"url": "https://papers.nips.cc/paper/2020/hash/1325cdae3b6f0f91a1b629307bf2d498-Abstract.html", "anchor_text": "Deep Transformers with Latent Depth"}, {"url": "https://search.zeta-alpha.com?doc_ids=068dbb97ed246e21eec65fc39d03cab99f425821,bbc53b06f54ab365ba19e6c0ff739d293fcb40c5,fc182f0a7facc9e425b322754ce2c2b0de6d8597,90560b9d04d7d2d10988b69734cb07015cbf410e,46d52802c16f82dc212e55d12084e659729ddb2c,325df8a0c1aa055882d86046cb558b8cb80ca528,ed47a838ebf4f30c39f04529913e9bcc8db3cd58,6bdbb0128e0b09ebc9677ab70de4bbc7eb8c7b87,cd7b042d68bfca545facc2359e8dfad4e83515e7,56e181716900991a8af537ea269f2de2019a2e71,c4cdf427b9512abb21c96b4279fe44d63b63e4be,85473222e0d814fe96eb7317848ad09ac00a4116,ab4a485c81cc41b2878433cde860e8823557494d", "anchor_text": "a more comprehensive list"}, {"url": "https://papers.nips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html", "anchor_text": "Learning to summarize with human feedback"}, {"url": "https://nips.cc/virtual/2020/public/poster_1f89885d556929e98d3ef9b86448f951.html", "anchor_text": "Virtual Poster"}, {"url": "https://papers.nips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html", "anchor_text": "Open Graph Benchmark: Datasets for Machine Learning on Graphs"}, {"url": "https://nips.cc/virtual/2020/public/poster_fb60d411a5c5b72b2e7d3527cfc84fd0.html", "anchor_text": "Virtual Poster"}, {"url": "http://ogb.stanford.edu", "anchor_text": "ogb.stanford.edu"}, {"url": "https://papers.nips.cc/paper/2020/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf", "anchor_text": "https://papers.nips.cc/paper/2020/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf"}, {"url": "https://papers.nips.cc/paper/2020/hash/75877cb75154206c4e65e76b88a12712-Abstract.html", "anchor_text": "Can Graph Neural Networks Count Substructures?"}, {"url": "https://papers.nips.cc/paper/2020/hash/1fc30b9d4319760b04fab735fbfed9a9-Abstract.html", "anchor_text": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games"}, {"url": "https://papers.nips.cc/paper/2020/hash/61c66a2f4e6e10dc9c16ddf9d19745d6-Abstract.html", "anchor_text": "Factor Graph Neural Networks"}, {"url": "https://papers.nips.cc/paper/2020/hash/bf15e9bbff22c7719020f9df4badc20a-Abstract.html", "anchor_text": "BONGARD-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning"}, {"url": "https://papers.nips.cc/paper/2020/hash/e5a90182cc81e12ab5e72d66e0b46fe3-Abstract.html", "anchor_text": "A Benchmark for Systematic Generalization in Grounded Language Understanding"}, {"url": "https://papers.nips.cc/paper/2020/hash/51200d29d1fc15f5a71c1dab4bb54f7c-Abstract.html", "anchor_text": "RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning"}, {"url": "https://search.zeta-alpha.com?doc_ids=97d01f5511ee05e187efc223e096f01d481b8626,aa46f6f1af235d5a7fc620c26fb2c0134ac039ae,907d0b551d1692ec2a15c043007412285fb565b0,e0718b33607667c2e9ee901ec3d6a308807a5aac,1e0b52415fd332aae1f808ac074d286fa51f5d8c,92d809c9833daafaa62e08c315cd43765dc0619b,3e9abb147255a53ad98660b1010362da0850212a,941a7309b2ebcc8a5fe225e6beb525ec2e7c22b8", "anchor_text": "a more comprehensive list"}, {"url": "https://twitter.com/ZetaVector", "anchor_text": "@zetavector"}, {"url": "https://medium.com/tag/neurips-2020?source=post_page-----845723f3add6---------------neurips_2020-----------------", "anchor_text": "Neurips 2020"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----845723f3add6---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/nlp?source=post_page-----845723f3add6---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----845723f3add6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----845723f3add6---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F845723f3add6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=-----845723f3add6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F845723f3add6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=-----845723f3add6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F845723f3add6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----845723f3add6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F845723f3add6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----845723f3add6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----845723f3add6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----845723f3add6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----845723f3add6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----845723f3add6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----845723f3add6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----845723f3add6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----845723f3add6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----845723f3add6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sergi Castella i Sap\u00e9"}, {"url": "https://medium.com/@sergicastella/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.2K Followers"}, {"url": "http://bit.ly/3ITDNNE", "anchor_text": "bit.ly/3ITDNNE"}, {"url": "http://linkedin.com/in/sergicastella", "anchor_text": "linkedin.com/in/sergicastella"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e27e64320ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=post_page-1e27e64320ad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce123c69a0ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneurips-2020-10-essentials-you-shouldnt-miss-845723f3add6&newsletterV3=1e27e64320ad&newsletterV3Id=ce123c69a0ef&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}