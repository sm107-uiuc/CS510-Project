{"url": "https://towardsdatascience.com/digging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29", "time": 1683012514.572524, "path": "towardsdatascience.com/digging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29/", "webpage": {"metadata": {"title": "Digging deep into YOLO V3 - A hands-on guide Part 1 | by Manogna Mantripragada | Towards Data Science", "h1": "Digging deep into YOLO V3 - A hands-on guide Part 1", "description": "The field of artificial intelligence has seen dramatic changes over the past few years leading to many new techniques. Computer vision is one such field that has gained a lot of momentum in recent\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 8}, {"url": "https://www.streamlit.io/", "anchor_text": "Streamlit", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/@ayoosh?source=post_page-----53fb7d3bfe6b----------------------", "anchor_text": "Ayoosh Kathuria", "paragraph_index": 27}], "all_paragraphs": ["The field of artificial intelligence has seen dramatic changes over the past few years leading to many new techniques. Computer vision is one such field that has gained a lot of momentum in recent times which aims at training the machines to read and interpret the visual world.", "Computer vision deals with many challenging use-cases right from classifying images to recognizing faces. One such challenge that I would be addressing today is object detection.", "Object detection is a technique that encompasses two tasks of object classification and object localization. It is a model trained to detect the presence and location of multiple classes of objects. This can be used on static images or even in real-time on videos.", "Object detection locates the object and classifies into different classes and localizes it by drawing bounding boxes around it. There have been many use cases for object detection. For eg. A self-driving car driving needs to be able to identify other objects on the road while driving. An AI radiologist reading X-rays needs to be able to locate the lesion (abnormal tissue). To address these use cases many state-of-the-art algorithms are being used to detect objects eg. R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, SSD, YOLO, etc. But I am specifically interested in YOLO today.", "YOLO (You Only Look Once) is considered to be one of the most powerful object detection algorithms. Invented by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi (2015), so far it already has 4 different versions, with YOLO V4 being the latest which released in April 2020, but in this post, we\u2019re going to focus on YOLOv3 and try to understand all the hype around it. Along with YOLO v3, we would be using the latest version of Tensorflow, i.e TF 2.0 released by Google in September 2019.", "With extra time on my hands these days due to being quarantined, I decided to explore Object detection using YOLO V3 and understand the dynamics of it before I jumped onto the latest YOLO versions. I came across a lot of articles but most of them explained the high-level architecture and direct implementation of this very complex YOLO v3 model and that left me in a confused state. I was looking for more and decided I wanted to understand each line of the code and what was happening in the background. I came across Ethan Yanjia Li, Ayoosh Kathuria, and Rahmad Sadli's amazing articles which helped me understand better. And what better way to ensure I really did understand something, than to try explaining it to someone else, hence I came up with this article.", "This post is my understanding of YOLO V3, a complex algorithm, and a naive attempt to put it into simple words.", "This post works with assumptions that you are:", "\u00b7 Good understanding of TensorFlow libraries", "\u00b7 Have a basic understanding of Convolutional Neural Networks (CNN) and Object detection.", "\u00b7 Have a basic understanding of Streamlit and what is it used for.", "This tutorial is divided into two parts.", "Part 1 explains the architecture and key concepts for understanding how YOLO v3 works.", "Part 2 gets onto a hands-on implementation of this algorithm right from understanding the configuration files to being able to create a user interface to take input images from the user and perform object detection on the same.", "Let\u2019s start by understanding what YOLO is in the first place.", "Many object detection models take in and process the image multiple times to be able to detect all the objects present in the images. But YOLO, as the name suggests just looks at the object once. It applies a single forward pass to the whole image and predicts the bounding boxes and their class probabilities. This makes YOLO a super-fast real-time object detection algorithm.", "Too much jargon, eh? Let us understand it step-by-step.", "Yolo V3 is an improvement over the previous two YOLO versions where it is more robust but a little slower than its previous versions. This model features multi-scale detection, a stronger feature extraction network, and a few changes in the loss function.", "For understanding the network architecture on a high-level, let\u2019s divide the entire architecture into two major components: Feature Extractor and Feature Detector (Multi-scale Detector). The image is first given to the Feature extractor which extracts feature embeddings and then is passed on to the feature detector part of the network that spits out the processed image with bounding boxes around the detected classes.", "Let\u2019s get a little deep into these components.", "The previous YOLO versions have used Darknet-19 (a custom neural network architecture written in C and CUDA) as a feature extractor which was of 19 layers as the name suggests. YOLO v2 added 11 more layers to Darknet-19 making it a total 30-layer architecture. Still, the algorithm faced a challenge while detecting small objects due to downsampling the input image and losing fine-grained features.", "YOLO V3 came up with a better architecture where the feature extractor used was a hybrid of YOLO v2, Darknet-53 (a network trained on the ImageNet), and Residual networks(ResNet). The network uses 53 convolution layers (hence the name Darknet-53) where the network is built with consecutive 3x3 and 1x1 convolution layers followed by a skip connection (introduced by ResNet to help the activations propagate through deeper layers without gradient diminishing).", "The 53 layers of the darknet are further stacked with 53 more layers for the detection head, making YOLO v3 a total of a 106 layer fully convolutional underlying architecture. thus leading to a large architecture, though making it a bit slower as compared to YOLO v2, but enhancing the accuracy at the same time.", "If the aim was to perform classification as in the ImageNet, then the Average pool layer, 1000 fully connected layers, and a SoftMax activation function would be added as shown in the image, but in our case, we would like to detect the classes along with the locations, so we would be appending a detection head to the extractor. The detection head is a multi-scale detection head hence, we would need to extract features at multiple scales as well.", "We\u2019ll take a deeper look at understanding how these layers work in the next section once we familiarize ourselves with the high-level architecture.", "For visualizing how the multi-scale extractor would look like, I\u2019m taking an example of a 416x416 image. A stride of a layer is defined as the ratio by which it downsamples the input, and hence the three scales in our case would be 52x52, 26x26, and 13x13 where 13x13 would be used for larger objects and 26x26 and 52x52 would be used for medium and smaller objects.", "An important feature of the YOLO v3 model is its multi-scale detector, which means that the detection for an eventual output of a fully convolutional network is done by applying 1x1 detection kernels on feature maps of three different sizes at three different places. The shape of the kernel is 1x1x(B*(5+C)).", "Ayoosh Kathuria made a very elaborate diagram that beautifully explains the complete architecture of YOLO v3 (Combining both, the extractor and the detector).", "As seen in the above image where we take an example of a 416x416 image, the three scales where the detections are made are at the 82nd layer, 94th layer, and 106th layer.", "For the first detection, the first 81 layers are downsampled such that the 81st layer has a stride of 32 (as mentioned earlier, a stride of a layer is defined as the ratio by which it downsamples the input) resulting in our first feature map of size 13x13 and the first detection is made with a 1x1 kernel, leading to our detection 3D tensor of size 13x13x255.", "For the second detection, the 79th layer onwards is subjected to convolutional layers before upsampling to dimensions 26x26. This feature map is then depth concatenated with the feature map from layer 61 to form a new feature map which is further fused with the 61st layer with the help of 1x1 convolution layers. The second detection layer is at the 94th layer with a 3D tensor of size 26x26x255.", "For the final(third) detection layer, the same process is followed as that of the second detection where the feature map of the 91st layer is subjected to convolution layers before being depth concatenated and fused with a feature map from 36th layer. The final detection is made at the 106th layer with a feature map of size 52x52x255.", "The multi-scale detector is used to ensure that the small objects are also being detected unlike in YOLO v2, where there was constant criticism regarding the same. Upsampled layers concatenated with the previous layers end up preserving the fine-grained features which help in detecting small objects.", "The details of how this kernel looks in our model is described below in the next section.", "The YOLO v3 network aims to predict bounding boxes (region of interest of the candidate object) of each object along with the probability of the class which the object belongs to.", "For this, the model divides every input image into an SxS grid of cells and each grid predicts B bounding boxes and C class probabilities of the objects whose centers fall inside the grid cells. The paper states that each bounding box may specialize in detecting a certain kind of object.", "Bounding boxes \u201cB\u201d is associated with the number of anchors being used. Each bounding box has 5+C attributes, where \u20185\u2019 refers to the five bounding box attributes (eg: center coordinates(bx, by), height(bh), width(bw), and confidence score) and C is the number of classes.", "Our output from passing this image into a forward pass convolution network is a 3-D tensor because we are working on an SxS image. The output looks like [S, S, B*(5+C)].", "Let\u2019s just understand this better using an example.", "In the above example, we see that our input image is divided into 13 x 13 grid cells. Now, let us understand what happens with taking just a single grid cell.", "Due to the multi-scale detection feature of YOLO v3, a detection kernel of three different sizes is applied at three different places, hence the 3 boxes(i.e B=3). YOLO v3 was trained on the COCO dataset with 80 object categories or classes, hence C=80.", "In the earlier years for detecting an object, scientists used the concept of the sliding window and ran an image classification algorithm on each window. Soon they realized this didn\u2019t make sense and was very inefficient so they moved on to using ConvNets and running the entire image in a single shot. Since the ConvNet outputs square matrices of feature values (i.e something like a 13x13 or 26x26 in case of YOLO) the concept of \u201cgrid\u201d came into the picture. We define the square feature matrix as a grid but the real problem came when the objects to detect were not in square shapes. These objects could be in any shape (mostly rectangular). Thus, anchor boxes were started being used.", "Anchor boxes are pre-defined boxes that have an aspect ratio set. These aspect ratios are defined beforehand even before training by running a K-means clustering on the entire dataset. These anchor boxes anchor to the grid cells and share the same centroid. YOLO v3 uses 3 anchor boxes for every detection scale, which makes it a total of 9 anchor boxes.", "There is a chance that after the single forward pass, the output predicted would have multiple bounding boxes for the same object since the centroid would be the same, but we only need one bounding box which is best suited for all the.", "For this, we can use a method called non-maxim suppression (NMS) which basically cleans up after these detections. We can define a certain threshold that would act as a constraint for this NMS method where it would ignore all the other bounding boxes whose confidence is below the threshold mentioned, thus eliminating a few. But this wouldn\u2019t eliminate all, so the next step in the NMS would be implemented, i.e to arrange all the confidences of the bounding boxes in descending order and choose the one with the highest score as the most appropriate one for the object. Then we find all the other boxes with high Intersection over union (IOU) with the bounding box with maximum confidence and eliminate all those as well.", "Now that we know all the terminologies and architecture used in the YOLO v3, in the next part (Part 2) we would dive deep into the implementation.", "Since this is my very first attempt at writing a medium article and trying to articulate my thoughts, I would love to hear back suggestions for improvement.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist at Greenlink Analytics. Working towards using data science to create a sustainable environment and make the world a better place to live."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F78681f2c7e29&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mantrimanogna?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mantrimanogna?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "Manogna Mantripragada"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5846d4e2e364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&user=Manogna+Mantripragada&userId=5846d4e2e364&source=post_page-5846d4e2e364----78681f2c7e29---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78681f2c7e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78681f2c7e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://www.streamlit.io/", "anchor_text": "Streamlit"}, {"url": "https://arxiv.org/pdf/1804.02767.pdf", "anchor_text": "YOLOv3: An Incremental Improvement"}, {"url": "https://towardsdatascience.com/@ayoosh?source=post_page-----53fb7d3bfe6b----------------------", "anchor_text": "Ayoosh Kathuria"}, {"url": "https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b", "anchor_text": "https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b"}, {"url": "https://machinelearningspace.com/yolov3-tensorflow-2-part-1/#nms-unique", "anchor_text": "https://machinelearningspace.com/yolov3-tensorflow-2-part-1/#nms-unique"}, {"url": "https://arxiv.org/pdf/1804.02767.pdf", "anchor_text": "https://arxiv.org/pdf/1804.02767.pdf"}, {"url": "https://towardsdatascience.com/@ethanyanjiali?source=post_page-----9e3d2666280e----------------------", "anchor_text": "Ethan Yanjia Li"}, {"url": "https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e", "anchor_text": "https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e"}, {"url": "https://towardsdatascience.com/@ayoosh?source=post_page-----53fb7d3bfe6b----------------------", "anchor_text": "Ayoosh Kathuria"}, {"url": "https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b", "anchor_text": "https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b"}, {"url": "https://mc.ai/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-1/", "anchor_text": "https://mc.ai/the-beginners-guide-to-implementing-yolo-v3-in-tensorflow-2-0-part-1/"}, {"url": "https://machinelearningmastery.com/object-recognition-with-deep-learning/", "anchor_text": "https://machinelearningmastery.com/object-recognition-with-deep-learning/"}, {"url": "https://medium.com/tag/yolov3?source=post_page-----78681f2c7e29---------------yolov3-----------------", "anchor_text": "Yolov3"}, {"url": "https://medium.com/tag/object-detection?source=post_page-----78681f2c7e29---------------object_detection-----------------", "anchor_text": "Object Detection"}, {"url": "https://medium.com/tag/tensorflow2?source=post_page-----78681f2c7e29---------------tensorflow2-----------------", "anchor_text": "Tensorflow2"}, {"url": "https://medium.com/tag/python3?source=post_page-----78681f2c7e29---------------python3-----------------", "anchor_text": "Python3"}, {"url": "https://medium.com/tag/streamlit?source=post_page-----78681f2c7e29---------------streamlit-----------------", "anchor_text": "Streamlit"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78681f2c7e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&user=Manogna+Mantripragada&userId=5846d4e2e364&source=-----78681f2c7e29---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78681f2c7e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&user=Manogna+Mantripragada&userId=5846d4e2e364&source=-----78681f2c7e29---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78681f2c7e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F78681f2c7e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----78681f2c7e29---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----78681f2c7e29--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----78681f2c7e29--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----78681f2c7e29--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mantrimanogna?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mantrimanogna?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manogna Mantripragada"}, {"url": "https://medium.com/@mantrimanogna/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "18 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5846d4e2e364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&user=Manogna+Mantripragada&userId=5846d4e2e364&source=post_page-5846d4e2e364--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1094ebed800&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdigging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29&newsletterV3=5846d4e2e364&newsletterV3Id=b1094ebed800&user=Manogna+Mantripragada&userId=5846d4e2e364&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}