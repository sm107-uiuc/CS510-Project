{"url": "https://towardsdatascience.com/introduction-to-graph-neural-networks-with-deepwalk-f5ac25900772", "time": 1683011559.398531, "path": "towardsdatascience.com/introduction-to-graph-neural-networks-with-deepwalk-f5ac25900772/", "webpage": {"metadata": {"title": "Introduction to Graph Neural Networks with DeepWalk | by Mohit Mayank | Towards Data Science", "h1": "Introduction to Graph Neural Networks with DeepWalk", "description": "Graph Neural Networks are the current hot topic [1]. And this interest is surely justified as GNNs are all about latent representation of the graph in vector space. Representing an entity as a vector\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/phanein/deepwalk", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://easyai.tech/en/blog/gnn-ai-future/", "anchor_text": "EasyAI \u2014 GNN may be the future of AI", "paragraph_index": 22}, {"url": "http://konect.cc/networks/ucidata-zachary/", "anchor_text": "The KONECT Project", "paragraph_index": 23}, {"url": "https://ronxin.github.io/wevi/", "anchor_text": "Wevi", "paragraph_index": 24}, {"url": "https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html", "anchor_text": "Lil\u2019Log \u2014 Learning word embedding", "paragraph_index": 25}], "all_paragraphs": ["Graph Neural Networks are the current hot topic [1]. And this interest is surely justified as GNNs are all about latent representation of the graph in vector space. Representing an entity as a vector is nothing new. There are many examples like word2vec and Gloves embeddings in NLP which transforms a word into a vector. What makes such representation powerful are (1) these vectors incorporate a notion of similarity among them i.e. two words who are similar to each other tend to be closer in the vector space (dot product is large), and (2) they have application in diverse downstream problems like classification, clustering, etc. This is what makes GNN interesting, as while there are many solutions to embed a word or image as a vector, GNN laid the foundation to do so for graphs. In this post, we will discuss one of the initial and basic approaches to do so \u2014 DeepWalk [2]", "Graph or Networks is used to represent relational data, where the main entities are called nodes. A relationship between nodes is represented by edges. A graph can be made complex by adding multiple types of nodes, edges, direction to edges, or even weights to edges.", "One example of a graph is shown in Figure 1. The graph is the Karate dataset [4] which represents the social information of the members of a university karate club. Each node represents a member of the club, and each edge represents a tie between two members of the club. The left info bar states several graph properties like a number of nodes, edges, density, degree, etc. Network repository [3] contains many such networks from different fields and domains and provides visualization tools and basic stats as shown above.", "As the idea behind vector embedding is to highlight similarities, we will consider some definitions for similar nodes. Two nodes can be called similar by several ways, like if they have a similar \u2014 in-degree count, out-degree count, avg-degree, or no of neighbors, etc. One interesting notion is to consider the neighbors of nodes and the more common neighbors they share, more similar they are. In plain text, a node is defined by the company it keeps. If two nodes have very similar company, they are very similar. This idea of representing an entity by its locality is not new. The base of word embedding in NLP is based on the motto that, \u201ca word is represented by the context it keeps\u201d. With this much similarity between the two fields, its obvious that the first instinct was to leverage the existing techniques in NLP, port it to graph domains by somehow converting the idea of context of words to neighbors of nodes. One such existing technique is word2vec, which we will discuss briefly.", "A detour with word2vec (w2v) is required to completely appreciate and understand the idea behind DeepWalk. Word2Vec is a word embedding technique that represents a word as a vector. Each vector can be thought of as a point in $R^{D}$ space, where $D$ is the dimension of each vector. One thing to note is that these vectors are not randomly spread out in the vector space. They follow certain properties such that, words who are similar like cat and tiger are relatively closer to each other than a completely unrelated word like tank. In the vector space, this means their cosine similarity score is higher. Along with this, we can even observe famous analogies like king - man + woman = queen which can be replicated by vector addition of these word's representation vector.", "While such representation is not unique to w2v, its major contribution was to provide a simple and faster neural network based word embedder. To do so, w2v transformed the training as a classification problem where given one word the networks try to answer which word is most probable to be found in the context of the given word. This technique is formally called Skip-gram, where input is the middle word and output is context word. This is done by creating a 1-layer deep neural network where the input word is fed in one-hot encoded format and output is softmax with ideally large value to context word.", "The training data is prepared by sliding a window (of some window size) across the corpus of large text (which could be articles or novels or even complete Wikipedia), and for each such window the middle word is the input word and the remaining words in the context are output words. For each input word in vector space, we want the context words to be close but the remaining words far. And if two input words will have similar context words, their vector will also be close. This is the intuition behind Word2Vec which it does by using negative sampling. After training we can observe something interesting \u2014 the weights between the Input-Hidden layer of NN now represent the notions we wanted in our word embeddings, such that words with the same context have similar values across vector dimension. And these weights are used as word embeddings.", "The result in Figure 4 is from training 5D word embeddings from a cool interactive w2v demo Wevi [5]. As obvious words like (juice, milk, water) and (orange, apple) have similar kinds of vectors (some dimensions are equally lit \u2014 red or blue). Interested readers can go to [7] for a detailed understanding of the architecture and maths. Also [5] is suggested for excellent visualization of the engine behind word2vec.", "DeepWalk employs the same training technique as of w2v i.e. skip-gram. But one important thing remaining is to create training data that captures the notion of context in graphs. This is done by random walk technique, where we start from one node and randomly go to one of its neighbors. We repeat this process $L$ time which is the length of the random walk. After this, we restart the process again. If we do this for all nodes (and $M$ times for each node) we have in some sense transformed the graph structure into a text like corpus used to train w2v where each word is a node and its context defines its neighbor.", "The DeepWalk authors provide a python implementation here. Installation details with other pre-requisite are provided in the readme (windows user be vary of some installation and execution issues). The CLI API exposes several algorithmic and optimization parameters like,", "Authors have also provided example graphs, one of which is our Karate club dataset. It's stored in the format of the adjacency list.", "Now let\u2019s read the graph data and create node embeddings by,", "This performs start-to-end analysis by taking care of \u2014 loading the graph from the file, generating random walks, and finally training skip-gram model on the walk data. By running this with additional --max-memory-data-size 0 param, the script also stores the walk data as shown below.", "Finally, we get the output embedding file which contains vector embedding for each node in the graph. The file looks as,", "A much simpler API is provided by newly released python implementation \u2014 KarateClub [6]. To do the same set of actions, all we need to do is following.", "The DeepWalk class also extends the same parameters exposed by the author's code and can be tweaked to do the desired experiment.", "To see DeepWalk in action, we will pick one graph and visualize the network as well as the final embeddings. For better understanding, I created a union of 3 complete graphs with some additional edges to connect each graph.", "Now, we will create DeepWalk embeddings of the graph. For this, we can use the KarateClub package and by running DeepWalk on default settings we get embeddings of 128 dimensions. To visualize this I use dimensionality reduction technique PCA, which scaled-down embeddings from R\u00b9\u00b2\u2078 to R\u00b2. I will also plot the 128D heatmap of the embedding on the side.", "There is a clear segregation of nodes in the left chart which denotes the vector space of the embedding. This showcase how DeepWalk can transform a graph from force layout visualization to vector space visualization while maintaining some of the structural properties. The heatmap plot also hints to a clear segregation of graph into 3 clusters.", "Another important thing to note is when the graph is not so complex, we can get by with lower dimension embedding as well. This not only reduces the dimensions but also improves the optimization and convergence as there are fewer parameters in skip-gram to train. To prove this we will create embedding of only size 2. This can be done by setting the parameter in DeepWalk object dw = DeepWalk(dimensions=2) . We will again visualize the same plots.", "Both the plots again hint towards the same number of clusters in the graph, and all this by only using 1% of the previous dimensions (from 128 to 2 i.e. ~1%).", "As the answer to this analogy NLP - word2vec + GraphNeuralNetworks = ? can arguably be DeepWalk (is it? \ud83d\ude42 ), it leads to two interesting points, (1) DeepWalk's impact in GNN can be analogous to Word2Vec's in NLP. And it's true as DeepWalk was one of the first approaches to use NN for node embeddings. It was also a cool example of how some proven SOTA technique from another domain (here, NLP) can be ported to and applied in a completely different domain (here, graphs). This leads to the second point, (2) As DeepWalk was published a while ago (in 2014 - only 6 years but a lifetime in AI research), currently, there are lots of other techniques which can be applied to do the job in a better way like Node2Vec or even Graph convolution networks like GraphSAGE, etc. That said, as to start with NN based NLP, word2vec is the best starting point, I think DeepWalk is in the same sense a good beginning for NN based graph analysis. And hence the topic of this article.", "[1] EasyAI \u2014 GNN may be the future of AI", "[4] Zachary karate club \u2014 The KONECT Project", "[5] Wevi \u2014 word embedding visual inspector", "[7] Lil\u2019Log \u2014 Learning word embedding", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior Data Scientist | AI/ML Researcher | Creator of \u201cJaal\u201d | Author of \u201cLazy Data Science Guide\u201d | Linkedin & Twitter: @imohitmayank"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff5ac25900772&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5ac25900772--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5ac25900772--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mohitmayank.medium.com/?source=post_page-----f5ac25900772--------------------------------", "anchor_text": ""}, {"url": "https://mohitmayank.medium.com/?source=post_page-----f5ac25900772--------------------------------", "anchor_text": "Mohit Mayank"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c8a84675e86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&user=Mohit+Mayank&userId=5c8a84675e86&source=post_page-5c8a84675e86----f5ac25900772---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5ac25900772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5ac25900772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@urielsc26", "anchor_text": "urielsc26"}, {"url": "https://unsplash.com/photos/11KDtiUWRq4", "anchor_text": "Unsplash"}, {"url": "https://github.com/phanein/deepwalk", "anchor_text": "here"}, {"url": "https://easyai.tech/en/blog/gnn-ai-future/", "anchor_text": "EasyAI \u2014 GNN may be the future of AI"}, {"url": "https://arxiv.org/abs/1403.6652", "anchor_text": "Paper"}, {"url": "https://github.com/phanein/deepwalk", "anchor_text": "Code"}, {"url": "http://networkrepository.com/graphvis.php", "anchor_text": "Network repository"}, {"url": "http://konect.cc/networks/ucidata-zachary/", "anchor_text": "The KONECT Project"}, {"url": "https://ronxin.github.io/wevi/", "anchor_text": "Wevi"}, {"url": "https://arxiv.org/pdf/2003.04819.pdf", "anchor_text": "Paper"}, {"url": "https://karateclub.readthedocs.io/en/latest/index.html", "anchor_text": "Code"}, {"url": "https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html", "anchor_text": "Lil\u2019Log \u2014 Learning word embedding"}, {"url": "https://medium.com/tag/graph?source=post_page-----f5ac25900772---------------graph-----------------", "anchor_text": "Graph"}, {"url": "https://medium.com/tag/ai?source=post_page-----f5ac25900772---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f5ac25900772---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----f5ac25900772---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f5ac25900772---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5ac25900772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&user=Mohit+Mayank&userId=5c8a84675e86&source=-----f5ac25900772---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5ac25900772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&user=Mohit+Mayank&userId=5c8a84675e86&source=-----f5ac25900772---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5ac25900772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5ac25900772--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff5ac25900772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f5ac25900772---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5ac25900772--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f5ac25900772--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f5ac25900772--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f5ac25900772--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f5ac25900772--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f5ac25900772--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f5ac25900772--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f5ac25900772--------------------------------", "anchor_text": ""}, {"url": "https://mohitmayank.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mohitmayank.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mohit Mayank"}, {"url": "https://mohitmayank.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "930 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c8a84675e86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&user=Mohit+Mayank&userId=5c8a84675e86&source=post_page-5c8a84675e86--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7840ae6d0ff6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-graph-neural-networks-with-deepwalk-f5ac25900772&newsletterV3=5c8a84675e86&newsletterV3Id=7840ae6d0ff6&user=Mohit+Mayank&userId=5c8a84675e86&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}