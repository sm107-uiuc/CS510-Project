{"url": "https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350", "time": 1683013850.990647, "path": "towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350/", "webpage": {"metadata": {"title": "A Comprehensive Guide to Neural Machine Translation using Seq2Seq Modelling using PyTorch. | by Balakrishnakumar V | Towards Data Science", "h1": "A Comprehensive Guide to Neural Machine Translation using Seq2Seq Modelling using PyTorch.", "description": "It was one of the hardest problems for computers to translate from one language to another with a simple rule-based system because they were not able to capture the nuances involved in the process\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/towards-artificial-intelligence/create-your-own-mini-word-embedding-from-scratch-c7b32bd84f8e", "anchor_text": "Word Embedding", "paragraph_index": 15}, {"url": "https://medium.com/towards-artificial-intelligence/create-your-own-mini-word-embedding-from-scratch-c7b32bd84f8e", "anchor_text": "word embedding", "paragraph_index": 30}, {"url": "https://www.figma.com/", "anchor_text": "Figma", "paragraph_index": 65}, {"url": "https://docs.google.com/drawings/", "anchor_text": "Google Drawing", "paragraph_index": 65}], "all_paragraphs": ["Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.", "It was one of the hardest problems for computers to translate from one language to another with a simple rule-based system because they were not able to capture the nuances involved in the process. Then shortly we were using statistical models but after the entry of deep learning the field is collectively called Neural Machine Translation and now it has achieved State-Of-The-Art results.", "I want this post to be beginner-friendly, so a specific kind of architecture (Seq2Seq) showed a good sign of success, is what we are going to implement here.", "So the Sequence to Sequence (seq2seq) model in this post uses an encoder-decoder architecture, which uses a type of RNN called LSTM (Long Short Term Memory), where the encoder neural network encodes the input language sequence into a single vector, also called as a Context Vector.", "This Context Vector is said to contain the abstract representation of the input language sequence.", "This vector is then passed into the decoder neural network, which is used to output the corresponding output language translation sentence, one word at a time.", "Here I am doing a German to English neural machine translation. But the same concept can be extended to other problems such as Named Entity Recognition (NER), Text Summarization, even other language models, etc.", "For getting the data in the best way we want, I am using SpaCy (Vocabulary Building), TorchText (text Pre-processing) libraries, and multi30k dataset which contains the translation sequences for English, German and French languages", "Torch text is a powerful library for making the text data ready for a variety of NLP tasks. It has all the tools to perform preprocessing on the textual data.", "Let\u2019s see some of the processes it can do,", "1. Train/ Valid/ Test Split: partition your data into a specified train/ valid/ test set.", "2. File Loading: load the text corpus of various formats (.txt,.json,.csv).", "3. Tokenization: breaking sentences into a list of words.", "4. Vocab: Generate a list of vocabulary from the text corpus.", "5. Words to Integer Mapper: Map words into integer numbers for the entire corpus and vice versa.", "6. Word Vector: Convert a word from a higher dimension to a lower dimension (Word Embedding).", "7. Batching: Generate batches of the sample.", "So once we get to understand what can be done in torch text, let\u2019s talk about how it can be implemented in the torch text module. Here we are going to make use of 3 classes under torch text.", "Here our source language (SRC \u2014 Input) is German and the target language (TRG \u2014 Output) is English. We also add 2 extra tokens \u201cstart of sequence\u201d <sos> and \u201cend of sequence\u201d <EOS> for effective model training.", "After setting the language pre-processing criteria, the next step is to create batches of training, testing, and validation data using iterators.", "Creating batches is an exhaustive process, luckily we can make use of TorchText\u2019s iterator library.", "Here we are using BucketIterator for effective padding of source and target sentences. We can access the source (german) batch of data using the .src attribute and it's corresponding (English) batch of data using the .trg attribute. Also, we can see the data before tokenizing it.", "I just experimented with a batch size of 32 and a sample batch is shown below. The sentences are tokenized into a list of words and indexed according to the vocabulary. The \u201cpad\u201d token gets an index of 1.", "Each column corresponds to a sentence, indexed into numbers and we have 32 such sentences in a single target batch and the number of rows corresponds to the maximum length of that sentence. Short sentences are padded with 1's to compensate for the length.", "The table below (Idx.csv) contains the numerical indices of the batch, which is later fed into the word embedding and converted into dense representation for Seq2Seq processing.", "The table below (Words.csv) contains the corresponding words mapped with the numerical indices of the batch.", "The above picture shows the units present under a single LSTM Cell. I will add some references to learn more about LSTM in the last and why it works well for long sequences.", "But to simply put, Vanilla RNN, Gated Recurrent Unit (GRU) is not able to capture the long term dependencies due to its nature of design and suffers heavily by the Vanishing Gradient problem, which makes the rate of change in weights and bias values negligible, resulting in poor generalization.", "Inside the LSTM cell, we have a bunch of mini neural networks with sigmoid and TanH activations at the final layer and few vector adder, Concat, multiplications operations.", "Sigmoid NN \u2192 Squishes the values between 0 and 1. Say a value closer to 0 means to forget and a value closer to 1 means to remember.", "Embedding NN \u2192 Converts the input word indices into word embedding.", "TanH NN \u2192 Squishes the values between -1 and 1. Helps to regulate the vector values from either getting exploded to the maximum or shrank to the minimum.", "But LSTM has some special units called gates (Remember (Add) gate, Forget gate, Update gate), which helps to overcome the problems stated before.", "The hidden state and the cell state are referred to here as the context vector, which are the outputs from the LSTM cell. The input is the sentence\u2019s numerical indexes fed into the embedding NN.", "Before moving to build the seq2seq model, we need to create an Encoder, Decoder, and create an interface between them in the seq2seq model.", "Let\u2019s pass the german input sequence \u201cIch Liebe Tief Lernen\u201d which translates to \u201cI love deep learning\u201d in English.", "For a lighter note, let\u2019s explain the process happening in the above image. The Encoder of the Seq2Seq model takes one input at a time. Our input German word sequence is \u201cich Liebe Tief Lernen\u201d.", "Also, we append the start of sequence \u201cSOS\u201d and the end of sentence \u201cEOS\u201d tokens in the starting and in the ending of the input sentence.", "And the first block in the Encoder architecture is the word embedding layer [shown in green block], which converts the input indexed word into a dense vector representation called word embedding (sizes \u2014 100/200/300).", "Then our word embedding vector is sent to the LSTM cell, where it is combined with the hidden state (hs), and the cell state (cs) of the previous time step and the encoder block outputs a new hs and a cs which is passed to the next LSTM cell. It is understood that the hs and cs captured some vector representation of the sentence so far.", "At time step-0, the hidden state and cell state are either initialized fully of zeros or random numbers.", "Then after we sent pass all our input German word sequence, a context vector [shown in yellow block] (hs, cs) is finally obtained, which is a dense representation of the word sequence and can be sent to the decoder\u2019s first LSTM (hs, cs) for corresponding English translation.", "In the above figure, we use 2 layer LSTM architecture, where we connect the first LSTM to the second LSTM and we then we obtain 2 context vectors stacked on top as the final output. This is purely experimental, you can manipulate it.", "It is a must that we design identical encoder and decoder blocks in the seq2seq model.", "The above visualization is applicable for a single sentence from a batch.", "Say we have a batch size of 5 (Experimental), then we pass 5 sentences with one word at a time to the Encoder, which looks like the below figure.", "The decoder also does a single step at a time.", "The Context Vector from the Encoder block is provided as the hidden state (hs) and cell state (cs) for the decoder\u2019s first LSTM block.", "The start of sentence \u201cSOS\u201d token is passed to the embedding NN, then passed to the first LSTM cell of the decoder, and finally, it is passed through a linear layer [Shown in Pink color], which provides an output English token prediction probabilities (4556 Probabilities) [4556 \u2014 as in the total vocabulary size of English language], hidden state (hs), Cell State (cs).", "The output word with the highest probability out of 4556 values is chosen, hidden state (hs), and Cell State (cs) is passed as the inputs to the next LSTM cell and this process is executed until it reaches the end of sentences \u201cEOS\u201d.", "The subsequent layers will use the hidden and cell state from the previous time steps.", "In addition to other blocks, you will also see the block shown below in the Decoder of the Seq2Seq architecture.", "While model training, we send the inputs (German Sequence) and targets (English Sequence). After the context vector is obtained from the Encoder, we send them Vector and the target to the Decoder for translation.", "But during model Inference, the target is generated from the decoder based on the generalization of the training data. So the output predicted words are sent as the next input word to the decoder until a <EOS> token is obtained.", "So in model training itself, we can use the teach force ratio (tfr), where we can actually control the flow of input words to the decoder.", "Sending either of the word (actual target word or predicted target word) can be regulated with a probability of 50%, so at any time step, one of them is passed during the training.", "This method acts like a Regularization. So that the model trains efficiently and fastly during the process.", "The above visualization is applicable for a single sentence from a batch. Say we have a batch size of 4(Experimental), then we pass 4sentences at a time to the Encoder, which provides 4 sets of Context Vectors, and they all are passed into the Decoder, which looks like the below figure.", "The final seq2seq implementation for a single input sentence looks like the figure below.", "The above visualization is applicable for a single sentence from a batch. Say we have a batch size of 4 (Experimental), then we pass 4 sentences at a time to the Encoder, which provide 4 sets of Context Vectors, and they all are passed into the Decoder, which looks like the below figure.", "Now let us compare our trained model with that of SOTA Google Translate.", "Not bad, but clearly the model is not able to comprehend complex sentences. So in the upcoming series of posts, I will be enhancing the above model\u2019s performance by altering the model\u2019s architecture, like using Bi-directional LSTM, adding attention mechanism, or replacing LSTM with the Transformers model to overcome these apparent shortcomings.", "I hope I was able to provide some visual understanding of how the Seq2Seq model processes the data, let me know your thoughts in the comment section.", "Check out the Notebooks that contains the entire code implementation and feel free to break it.", "Complete Code Implementation is available at,", "For those who are curious, visualizations in this article were made possible by Figma & Google Drawing.", "Complete Visualization files created on Figma (.fig) [LSTM, ENCODER+DECODER, SEQ2SEQ] is available @ Github.", "Until then, see you next time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F41c9b84ba350&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://balakrishnakumar-v.medium.com/?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": ""}, {"url": "https://balakrishnakumar-v.medium.com/?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "Balakrishnakumar V"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ff242bc8b38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&user=Balakrishnakumar+V&userId=4ff242bc8b38&source=post_page-4ff242bc8b38----41c9b84ba350---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41c9b84ba350&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41c9b84ba350&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/towards-artificial-intelligence/create-your-own-mini-word-embedding-from-scratch-c7b32bd84f8e", "anchor_text": "Word Embedding"}, {"url": "https://medium.com/towards-artificial-intelligence/create-your-own-mini-word-embedding-from-scratch-c7b32bd84f8e", "anchor_text": "word embedding"}, {"url": "https://github.com/bala-codes/Natural-Language-Processing-NLP/blob/master/Neural%20Machine%20Translation/1.%20Seq2Seq%20%5BEnc%20%2B%20Dec%5D%20Model%20for%20Neural%20Machine%20Translation%20%28Without%20Attention%20Mechanism%29.ipynb", "anchor_text": "bala-codes/Natural-Language-Processing-NLPPermalink Dismiss GitHub is home to over 50 million developers working together to host and review code, manage\u2026github.com"}, {"url": "https://colab.research.google.com/github/bala-codes/Natural-Language-Processing-NLP/blob/master/Neural%20Machine%20Translation/1.%20Seq2Seq%20%5BEnc%20%2B%20Dec%5D%20Model%20for%20Neural%20Machine%20Translation%20%28Without%20Attention%20Mechanism%29.ipynb", "anchor_text": "Google ColaboratoryEdit descriptioncolab.research.google.com"}, {"url": "https://www.kaggle.com/balakrishcodes/seq2seq-model-for-neural-machine-translation#12.-Seq2Seq-Model-Inference", "anchor_text": "Seq2Seq Model for Neural Machine TranslationExplore and run machine learning code with Kaggle Notebooks | Using data from no data sourceswww.kaggle.com"}, {"url": "https://www.figma.com/", "anchor_text": "Figma"}, {"url": "https://docs.google.com/drawings/", "anchor_text": "Google Drawing"}, {"url": "https://github.com/bala-codes/Natural-Language-Processing-NLP/blob/master/Neural%20Machine%20Translation/Visualizations_Figma_Files/FIGURES%20OF%20LSTM%2C%20ENCODER%2C%20DECODER%2C%20SEQ2SEQ.fig", "anchor_text": "bala-codes/Natural-Language-Processing-NLPYou can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or\u2026github.com"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTM"}, {"url": "https://medium.com/towards-artificial-intelligence/create-your-own-mini-word-embedding-from-scratch-c7b32bd84f8e", "anchor_text": "WORD_EMBEDDING"}, {"url": "https://medium.com/datadriveninvestor/deployment-of-deep-learning-models-in-aws-using-aws-lambda-aws-api-gateway-aws-elastic-file-a48fdeb2c140", "anchor_text": "DEEP_LEARNING_MODEL_DEPLOYMENT_ON_AWS"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----41c9b84ba350---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----41c9b84ba350---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/machine-translation?source=post_page-----41c9b84ba350---------------machine_translation-----------------", "anchor_text": "Machine Translation"}, {"url": "https://medium.com/tag/lstm?source=post_page-----41c9b84ba350---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/encoder-decoder?source=post_page-----41c9b84ba350---------------encoder_decoder-----------------", "anchor_text": "Encoder Decoder"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41c9b84ba350&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&user=Balakrishnakumar+V&userId=4ff242bc8b38&source=-----41c9b84ba350---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41c9b84ba350&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&user=Balakrishnakumar+V&userId=4ff242bc8b38&source=-----41c9b84ba350---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41c9b84ba350&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F41c9b84ba350&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----41c9b84ba350---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----41c9b84ba350--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----41c9b84ba350--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----41c9b84ba350--------------------------------", "anchor_text": ""}, {"url": "https://balakrishnakumar-v.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://balakrishnakumar-v.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Balakrishnakumar V"}, {"url": "https://balakrishnakumar-v.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "104 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ff242bc8b38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&user=Balakrishnakumar+V&userId=4ff242bc8b38&source=post_page-4ff242bc8b38--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F265ba2f32c44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350&newsletterV3=4ff242bc8b38&newsletterV3Id=265ba2f32c44&user=Balakrishnakumar+V&userId=4ff242bc8b38&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}