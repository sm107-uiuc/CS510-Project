{"url": "https://towardsdatascience.com/net-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a", "time": 1682994304.296652, "path": "towardsdatascience.com/net-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a/", "webpage": {"metadata": {"title": "Net upvote prediction and subreddit-based sentence completion for Reddit comments: | by Rishabh Rai | Towards Data Science", "h1": "Net upvote prediction and subreddit-based sentence completion for Reddit comments:", "description": "As the one of most popular websites on the internet, Reddit is a treasure trove of information. Since its inception in 2005, Reddit has recorded the interactions of people all around the world on\u2026"}, "outgoing_paragraph_urls": [{"url": "https://files.pushshift.io/reddit/", "anchor_text": "accumulated comment dataset", "paragraph_index": 5}, {"url": "https://www.reddit.com/user/Stuck_In_the_Matrix", "anchor_text": "redditor", "paragraph_index": 5}, {"url": "https://github.com/davidzchen-ut/subreddit-predictive-keyboard", "anchor_text": "Demo Here", "paragraph_index": 78}], "all_paragraphs": ["This project was made by Rishabh Rai, Jayanth Shenoy, David Chen, Wei Wang, and Shruthi Krish", "As the one of most popular websites on the internet, Reddit is a treasure trove of information. Since its inception in 2005, Reddit has recorded the interactions of people all around the world on just about any topic imaginable.", "These interactions can be found in the form of comments, with each comment receiving a score from the users who either grant it an upvote or a downvote. Comments are grouped by post, but their more general domain is specified by the subreddit in which they reside.", "Because Reddit users are both consumers and contributors for the website, user experience for contributors is key for Reddit\u2019s growth. A better experience when writing comments would encourage more interactions on Reddit, and we aim to create this via a combination of models.", "Redditors place great value on their karma, which is gained by way of upvotes. By creating a model that predicts the net number of upvotes, we can help commenters get a better idea of the projected quality of their comment. Additionally, because of the diverse nature of subreddits, by creating a sentence completion system that varies based on subreddit, we can help users contribute more quickly and easily.", "Using an excellent accumulated comment dataset created by a redditor, we are able to create models for comments.", "The process of building a net upvote regressor for Reddit comments involved training many different model to find which one would best predict the upvote count based on the features.", "The full comment dataset contains all the comments on Reddit since 2005. Since dealing with such an enormous amount of data (hundreds of gigabytes of text) is beyond the scope of our resources, we limited the training and test sets to the comments from February 1st and 2nd, 2018, respectively. Each day\u2019s set contains data on more than 3 million comments, and after removing invalid data we are left with 2.8 and 2.4 million training and test examples, respectively. Though we would have preferred to have more data spread out over a time period, the ample amount of data for each set allowed us to be fairly certain of our", "Let\u2019s take a look at the features present in the dataset, and what we can get out of them:", "We decided that neither a syntactic nor a semantic interpretation of the author name would yield useful features. Though we considered the presence of \u201cthrowaway\u201d accounts, which are temporary account that tend to have gibberish usernames, whether or not a comment is made by a throwaway account is unlikely to have an effect on how many upvotes it gets. The same author can comment on a variety of topics, so the username of author likely has nothing to do with the content of the comment either. If we were to consider the history of the author\u2019s comments and how successful they were, we may be able to use that information, but due to a lack of time and computational resources, doing so was not feasible. Additionally, there is a high chance that including such information would result in noise (since, in practice, the popularity of previous comments says very little about that of a new one) or would result in the model being biased against users who have little to no comment history.", "The birthday of the user is also definitely unrelated to how many upvotes their comments get.", "Author Flair CSS Class and Author Flair CSS Text", "The author flair css class and text refer to tags relating to the subreddit. These flairs provide information to redditors with domain knowledge, and they often require domain knowledge to create. In a straightforward instance, on the NBA subreddit the flair can mark the user as a fan of a certain team. In the examples from r/TheLastAirbender below, although it is immediately obvious to anyone that the first user is a fan of someone named Asami, only a user with domain knowledge would understand that the second user is a fan of the teachings of a giant lion turtle.", "Though the actual semantics of the flair may not necessarily relate to the success of the comment, its presence means that the user is more active on this subreddit and has greater domain knowledge. This domain knowledge likely means a higher quality comment that is more likely to get upvotes.", "The body of the comment refers to its actual text content. This is the most important feature as it is the source of the upvotes and many features can be engineered from it.", "Using the NLTK VADER library, we determined the magnitude of positive, neutral, negative, and compound sentiment in the comment. The sentimental polarity of the comment is likely a strong indicator of its success.", "Number of User Mentions and Number of Subreddit Mentions", "We parsed the comment bodies for mentions of other users or subreddits, since directing voters to an outside source may encourage or discourage upvotes.", "Number of Reddit Links and Number of Outside Links", "The number of links to outside content or to other Reddit content may mean more responses to a post because of useful redirecting, or less responses because of spam advertising. Either way, both of these features could be useful and so we parsed the bodies for them.", "We determined the readability of each comment using the textstat library, which contains a metric that is a conglomerate of a variety of standard readability metrics. Doing so allowed us to reach a readability consensus for a post despite a wide variety of available metrics.", "The ratio of uppercase characters to the total number of characters in a post can serve as a metric for the excitement or urgency of a comment. A high ratio also attracts attention to a comment.", "The length of a post is one of the first aspects a reader notices. It often acts as a deciding factor of whether or not a comment will be read, so it is worthwhile to include this as a feature.", "Toxic and inflammatory comments are likely to be controversial. At the least, they garner response. To determine toxicity we tried applying a pre built model to the dataset, but the lack of necessary computational resources forced us to skip this feature for the time being.", "More questions in the comment body may lead to more responses to a comment, and therefore could be an indicator of upvotes.", "A mapping to a semantic space is crucial to determining a comment\u2019s success. This can be done using Doc2Vec, which represents a document\u2019s semantics as an embedding vector. Doc2Vec maps documents to a semantic space by adding document tags while learning word embeddings, and then creating a document embedding of a specified dimension. We started with training a dm model that creates embedding vectors of size 300, trained over 125 epochs. Though this model was accurate, the sheer size of the vector data for all of the comments proved to be too much for our resources. As a result, we trained a new 50 dimensional embedding model over 50 epochs. This model was small enough for us to use for predictions, and, after qualitatively inspecting a number of examples, it seemed that the overall accuracy of the embeddings hadn\u2019t been affected greatly. Looking at the Doc2Vec embeddings in two dimensions, we can see a clustering of popular comments. This may be due to common semantics, but it could very well just be the result of more activity on certain subreddits.", "A comment that has been gilded has been given some amount of money in the Reddit world. We discarded this feature since it is determined after a comment is made, and will not be available to someone before their comment has been made.", "This refers to whether or not a comment can be gilded. Because gilded comments are given more exposure and promotion on a thread, a comment that can be gilded has a chance at becoming gilded and gaining even more upvotes.", "Controversial comments are those that have many upvotes as well as downvotes. We discarded this feature because it is determined after the comments gets its votes, and so cannot be used to make predictions.", "We decided to discard the time of creation because despite having a large North American user base, Reddit is used internationally. This means that the time of day that a comment is made will likely be noise rather than an indicator of success. Additionally, we only have a single day\u2019s data, so we cannot try to use date as a distinguishing factor.", "A stickied comment has been pinned to the top of the thread, which results in greater exposure and more upvotes. However, since comments can be stickied by other users after they are made, we cannot use this as a predictive feature.", "Distinguished comments add a \u201c[M]\u201d next to the username when a comment is made by a moderator. Because distinguishing has no effect on the ranking of a comment and the tag doesn\u2019t offer a passerby any insight like a flair would, whether or not a comment is distinguished is unlikely to affect its success.", "Because an edited comment may mean that the comment has garnered attention, and because an edited comment can be treated as a new comment, the edited feature can be used as an indicator of future success.", "Approved submitters are allowed to post as many times as they want, which means that the person has considerable experience commenting and will likely garner more upvotes.", "The permalink is a link to the comment. It contains the title of the post.", "We extracted the title from the link and used a dictionary to fix the words that originally had apostrophes (contractions can\u2019t be part of a url). Though we planned on applying the Doc2Vec model to these titles and calculating cosine distance of the title embedding from the comment embedding do determine relevance, we were unable to fully implement this feature due to time constraints.", "Comments responding to a question might get more upvotes, but the only way to know for sure is to include this as a feature extracted from the post title.", "This acts as the label, so it\u2019s definitely necessary.", "Using the Viterbi algorithm for word segmentation, we segmented the combined words in the subreddit names to create documents that we could apply the Doc2Vec model on. Again, the goal was to calculate the relevance of the comment to the subreddit. However, we were unable to fully implement this feature because of time constraints.", "The retrieved date and time is important for determining the period over which we estimate the net upvote count. The training set had a period of 11 days whereas the test set had a period of 15 days. We do not see this being a large issue issue in prediction evaluation as the small difference in number of days makes little difference in upvotes (since comments probably get most of their votes near the start of their existence), especially considering the scale of years that Reddit content often functions on.", "ID, Link ID, Parent ID, and Subreddit ID", "The IDs of the comment definitely don\u2019t have anything to do with its success.", "Building and Evaluating a Net Upvote Regressor", "When tuning the hyperparameters for our models it was important to decide on a good evaluation metric for scoring the models. Based on the score distribution of the training data, the vast majority of comments have very few upvotes, meaning that the data is skewed right. As a result, using certain regression evaluation metrics such as RMSE, which give high weight to large errors, could potentially result in bias towards comments with a high number of upvotes. Drawing inspiration from the Zillow housing price kaggle competition, which involved a regression problem with a similar right-skewed dataset, we decided on using Mean Absolute Error (MAE) as the evaluation metric as it would not place bias on outlying comments.", "Hyperparameter tuning for the gradient boosted tree models above was the most time consuming aspect of building the upvote predictor. Tuning the model was done on google cloud instance with 40GB RAM and GPUs. We tried several different techniques to optimize our models\u2019 parameters. While parameter tuning, all models were cross validated using a 4-fold of the training set.", "Hyperopt is a sophisticated way of searching parameter spaces through statistical methods. By means of intelligent random sampling, we were able to use HyperOpt to properly minimize the mean absolute errors of the gradient boosted tree models.", "To do a sanity check on our bayesian optimization search results, we also performed randomized searches to get an idea of what the ballpark values were for top performing fits.", "Gradient Boosted Decision Trees using XGBoost", "Gradient boosted decision trees generally work well in practice for tabular data (as evidenced by their success in many Kaggle competitions involving regression), so we decided to build several different models from gradient boosted trees. We started with by tuning in XGBoostRegressor as a default model. All categorical features were one-hot encoded before the model was trained.", "Although XGBoost may perform well in general, our XGBRegressor model was the worst model in this scenario. The model\u2019s loss was calculated to be 12.34 CV MAE, and a 10.33 for testing MAE. The scatter plot of the test predictions is shown below. Predicted upvote values range from -200 to 2000. Based on the scatter plot, the model seems to be over-shooting many scores for comments with low scores in actuality.", "Gradient Boosted Decision Trees using CatBoost", "CatBoost is a gradient boosted tree model developed by Yandex that provides an input for categorical features. Before training, CatBoost applies a sophisticated algorithm for properly encoding specified categorical features. Because our training data had a few categorical features that potentially created issues for the XGBoost model, we attempted to build a CatBoost regressor that could better deal with these features. All categorical features were CatBoost encoded before the model was trained.", "The results were significantly better compared to the XGBoost model. Unlike the predictions of XGBoost, the range of values for the CatBoost predictions were much smaller. As a result, the model was unable to predict any of the outlying comments with extremely high scores. Despite this, the model\u2019s loss was calculated to be 8.32 CV MAE, and 8.21 for testing MAE, showing how specialized encoding of categorical features was a step in the right direction. Based on the model\u2019s feature importance scores, the categorical feature has_flair seemed to have had the greatest impact on the model, while can_guilded and edited were not even considered. Now that we had two interesting feature importance plots to work with, it was time to start removing noisy features that could potentially increase MAE.", "Having access to cloud instances and the desire to diversify from gradient boosted trees, we decided to try several models based on neural networks. Our best one involved the architecture below, a dense sequential network with a 67 unit input, two hidden layers of 50 units each which use a ReLU activation function, and a final output layer with a linear activation. For training we used the Adam optimizer and a MAE evaluation metric.", "Additionally, we decided to try reducing model noise from the previous models by cutting out certain features that were not well suited for neural networks and that were low scoring in terms of feature importance for the previous two models.", "Quite surprisingly, the neural network yielded much better results than expected. We were only able to train it for a few epochs, which can clearly be seen by the learning and validation curves below. The model still hasn\u2019t fully converged with the few iterations that we trained it for, but it still managed to obtain a CV MAE of 7.27 and a test MAE of 7.17 since the general curve trend remained the same. What\u2019s unusual, is that the range of values predicted is growing even smaller based on the scatter plot. It seems that this model is biased towards comments with high values, despite our initial choosing of MAE as the evaluation metric. However, it seems that removing some of our generated features have helped reduce noise in the model.", "For our final model, we wanted to try something with a lot of predictive power in hopes of reducing our MAE test score even further. Essentially, the model we designed was a 4-fold combination of gradient boosted trees and 4-fold combination sequential neural networks. The predictions from these primary models would be stacked and predicted by a supervisor XGBoost Regressor model, which would return the final set of score predictions. Our thought process on designing this supervisor model was to combine the good and bad of our previous tuned models to see how the results would end up.", "Unfortunately, although we were able to fully code our ensemble model, our computing capabilities prevented us from actually training the model. During the k-fold training phases for the neural network and CatBoost models, our google cloud instance GPU continually kept running out of memory. We tried the experiment several times, but the results of training failure on the cloud server were consistent.", "Above we can see a comparison of the results we\u2019ve gotten. As we experimented with new models using our experience with previous ones as a guide our efforts, we saw our error decrease steadily. We believe that successfully implementing the goal model will continue this trend and aim to do so in the future using greater computational resources.", "To create a full suite of tools for Reddit users to use when commenting, we built a customized predictive keyboard for different subreddits. In general, a predictive keyboard observes past typing habits and vocabulary usage as well as the current context to approximate what words have a high likelihood of being typed and suggests those to the user.", "In most natural language processing problems the first step is to process the text to eliminate any character noise. In our case we decided to take a more hands off approach and modify the text as little as possible to preserve as much information from comments as possible. Regardless, some syntactic elements like punctuation and capitalization were adding more noise than was justifiable so we decided to remove those.", "Along with typical preprocessing, we also face the more novel problem of generating a proper corpus. Most NLP models require a corpus to train on so the model can first have an understanding of what patterns there may be in the text. In our case, the raw data came from millions of different comments instead of one centralized corpus, so we had to generate our own corpus out of what we had. To do this, we decided to simply append all the comments together while using special delimiting characters distinguish between different comments.", "With all of our data in a workable form and a simple understanding of how predictive keyboards work, we decided to just jump right in to implementing our first Markov chain based model.", "A Markov chain model seemed to be a good fit for our problem because it could take into account both the context of words through n-grams and keep track of past history by memorizing probabilities of words showing up. Markov chains can be thought of as finite state machines where each each state has a certain probability of transitioning to other stages based on past history. When applied to text, this means that every group of words is one possible state and all of the edges are formed based on the probability that the next word occurs as shown below. For our model, we decided to use bi-grams when separating out the sentences so instead of looking at individual words we looked at two words at a time allowing us to take into account some the context of certain words, while not making our training data too sparse.", "After training the Markov model to our data we found that it was very good at generate large amounts of readable text. The performance of this model was especially fast because it just needs one pass through all the data to memorize all the probabilities. This model was able to take in very short starting phrases and was able to extrapolate a lot from there. For example, when the model is trained on the subreddit r/Fitness and we make the model keep generating words until it thinks the comment should end, starting with the words \u201cI am\u201d generates the following commnet: \u201cI am inclined to think you can\u2019t be gung ho every day not have any muscle injuries\u201d.", "At first glance this may seem perfect, but upon closer inspection this auto generated comment reveals some major advantages and drawbacks of our model. On one hand, the comment is very readable and more importantly, fits inside the context of a fitness subreddit, so our Markov chain seems to be able to generate subreddit specific comments quite effectively. However, our ideal use case would focus more on suggesting shorter phrases or single words with a high degree of accuracy to what the user expects. So if we were to run the model again, but make it stop after one word we would get the phrase \u201cI am inclined\u201d. This phrase may be perfectly fine grammatically, but it may not actually be too accurate of a prediction because the word \u201cinclined might not actually follow the phrase \u201cI am\u201d too often. Of course, it would be basically impossible to predict the next word perfectly especially given that we only have two words to get context off of. One solution to this problem is to use a model that\u2019s better at taking context into account (we later use an LSTM for this), the other more immediate solution to this is to feed more data into our current model. We decided to try this second approach to improving our accuracy by only trying to predict words halfway through typing them, so using the same example as before, we would only predict \u201cI am inclined\u201d if the user had already typed \u201cI am inc\u201d. To get this desired effect we needed to modify our Markov model to be character based.", "This was an extreme change, as it just meant that instead of predicting for individual words we looked at characters. As a part of this, we also decided to group together 4 letters at a time in an attempt to get more readable text. This allowed us to get the mid-word prediction that we desired, but it came with the downside of dramatically worse sentences, with even proper words being a rarity. We decided that this was a result of the model being too naive and not being good enough at interpreting context. Because of this, we decided to revisit our first solution of finding a different model that would better take context into account.", "How we can improve on Markov Models \u2014 Neural Language Modeling", "Let\u2019s think about how humans process and interpret sentences. Humans generally go over the text, left to right, and after each word gain some context of what the text is about until the end is reached; thus we cumulatively understand the entire text. Given this is the way humans read and understand sentences, we can gain some insight on how humans would predict the next words or characters in a sentence given the words that are given: we read and gain information cumulatively from words in beginning of the sentence until we reach the end, then given the context of those words we can determine what would make sense to go next. The idea behind Recurrent Neural Networks (RNNs) is very similar.", "In other neural networks, inputs to the network are assumed to be independent of each other. For example, your standard multilayer perceptron (MLP) or feedforward neural networks pass inputs through neurons which perform some mathematical function on the inputs to the model. By training these models, you are performing a mathematical operation on these inputs, seeing how off you were, then re-scaling the weights of the neurons in the network through a process known as back-propagation. This is great for some use cases, however, in natural language processing our inputs are generally not independent of each other. Inputs from the past could help us recognize a better output for the current input, and that\u2019s exactly what RNNs try to do: make use of sequential information.", "Recurrent Neural Networks is a variant of a recursive artificial neural network where connections between neurons make directed cycles. Its output depends not just on current inputs but also on previous steps\u2019 states; this memory allows the network to perform better on problems where context of previous inputs are important. This attribute is the key factor that makes RNNs a very powerful tool to work on Natural Language Processing problems, and we will try to use RNNs to create a better predictive keyboard.", "With these results in mind, we decided to try to develop a new model that could match the subreddit behavior while also introducing mid-word predictions. Originally we tried to simply convert our Markov chain model into ac character based one, this lead to poor results because the model was not sophisticated enough to generate real words from the characters it predicted.", "As a result, we created a long short-term memory (LSTM) recurrent neural network. Regular recurrent neural networks suffer from vanishing and the exploding gradients problem so they are rarely used in practice. Instead, we used a LSTM recurrent neural network (shown below) which uses gates to flow gradients and reduce the vanishing gradient problem.", "Given our data is comments from a subreddit, we need to format the data in a specific way that works well with the neural network. Passing each comment as an array of strings would not work well in a neural network, what would be much better is either representing them in as categorical features or numerical features. In the case of our character LSTM, we first create a mapping of characters to indices and indices to characters, which we will later use to one hot encode our sequences. We also keep track of the size of our character set, which came out to around 200 unique characters.", "Next, we need to map our comments to some representation that a LSTM will accept. We can do this by creating sequences of characters with length 40 for our entire text.While we create these sequences, for each character in our sequence we create an one hot encoded vector to represent each character. This is useful as neural networks generally perform well for multi-class classification with categorical features. After we finish creating these sequences, we are left with a three-dimensional matrix (our X): the first dimension represents the sequence number, the second dimension represents the characters in the sequence, and the third dimension is an one-hot vector representing a specific character. At the same time when we create our sequence, we create our next letter matrix (our Y). For sequence N, we are given characters 1 to 30 and we try to predict Y, which would be character 31. Our Y array should be a two-dimensional array: the first dimension represents the sequence number, the second dimension is an one-hot vector representing the specific character our model is trying to predict.", "Then, we create our model used to predict the next character for input sequences. We used a Keras sequential model with a LSTM layer with size 128, into a full connected Dense layer of size our character set size, and finally we put those results into a softmax activation layer to output an one hot vector whose singular True index represents the character that we are trying to predict. We compile this model with a RMSprop optimizer (an optimizer generally good for recurrent neural networks) with learning rate of 0.01 and with a loss function of categorical_crossentropy (good for neural networks in multi-class classification). We fit this model to our X and Y matrices described above for 20 epochs and see the below accuracy and loss.", "We can see our model can achieve around 0.55 accuracy for our validation data with a loss of around 1.9. This means given a sequence of characters of length 30, there\u2019s a 55% chance the model will correctly guess the next character in the sequence. For our model trained on /r/2007scape, it performs pretty well since it can learn some of the words used only in those specific subreddits. For example, given an input sequence \u201cOld School Runesc\u201d, our model can predict [\u201ccape\u201d, \u201chools\u201d, \u201cores\u201d, \u201cscenity\u201d, \u201c and\u201d] which would make the prediction \u201cOld School Runescape\u201d. This highlights some of the advantages and disadvantages our model has compared with the Markov model: our model is now able to predict incomplete words with decent accuracy given past context. However, given very vague context such as \u2018O\u2019, our model predicts [\u201cOne\u201d, \u201cOther\u201d, \u201cOre\u201d, \u201cOin\u201d, \u201cOent\u201d], showing us our model isn\u2019t guaranteed to give us words that are valid. Also our model does not work well if we give it sequences with complete words already, for example \u201cI want to eat\u201d gives us an output of [\u201ction\u201d, \u201ch\u201d, \u201c the\u201d, \u201cer\u201d, \u201co\u201d] so our model doesn\u2019t actually know complete words nor does it really understand the concept of grammar. Below is a quick look at the web application we built to demo our model.", "In summary, our model works well for incomplete words that need to be autocompleted into words that are used in the subreddit but not elsewhere (e.g. game cities, bosses, monsters, etc.). However, for predicting next words, our model does not perform as well because it doesn\u2019t really understand the concept of words/grammar as it is trained on characters rather than words. Next, we will propose a model that is supposed to work well for predicting next words.", "LSTM word2vec embedding model (Demo Here)", "Our previous character neural language model worked well for predicting incomplete words and words specific to a subreddit. However, is there a way for a model that performs better when predicting words and conforms to the rules of grammar? Now we try to create a model that uses LSTM RNNs and word embedding models in order to try to predict words given a sequence of words. Again, we use LSTM RNNs over regular RNNs for their ability to deal with the vanishing and exploding gradient problem, and because we\u2019re trying to predict using sequential features.", "Before we go into what we tried, it\u2019s important to go over what advantages word embeddings would bring to the model. Word embedding is the process of converting strings to vectors. The process involves mathematically embedding one dimension per word to a continuous vector space of much lower dimension. This allows us to reduce the \u201ccurse of dimensionality\u201d as one hot encoding a word vocabulary set could result in vocabulary sets with lengths over one hundred thousand. Specifically, we explored using Word2Vec, which is a model that creates unique word embedding vectors for each unique word. The special thing about Word2Vec is that the word vectors it produces are positioned in a way such that words that share common contexts in the corpus are located close to each other in the vector space. We try to take advantage of this property for our model, as we think the context captured by the Word2Vec vector will give the model more context and as a result improve the output of the model. Below is a visualization of word embeddings generated by a Word2Vec model.", "Our input is still the same corpus of words, so we need to transform this data into something our model will accept. We start by tokenizing our corpus into an array of strings, then we create and train a gensim Word2Vec model. After training our model, we create our 30 word long sliding window sequence on the entire dataset by creating a similar three-dimensional array (our X matrix): the first dimension still represents the sequence number, the second dimension represents the sequence of words, and the third dimension represents the word (by looking up the word in the Word2Vec model and using the word embedding). We also create a two-dimensional array that represents our Y matrix: the first dimension representing the sequence number and the second dimension representing the word we are trying to predict (as a word vector).", "The model we use is very similar to our character based neural language model. We use a Keras sequential model that uses a LSTM neural network layer with size 128, into a fully connected Dense layer with the size of our Word2Vec output dimensions, and finally we output the results. We compiled this model with an AdamOptimizer with learning rate 0.01 with the loss function of mean_absolute_error. We fit this model with X and Y for 150 epochs and see what results we get.", "Unfortunately, the results of our model are pretty bad. After we convert the results of our model back into words, we see that our model keeps outputting either one or two words the model thinks is the mostly to be the correct word. Given examples we see online, we think our model is underfitting. Given more time and resources, if we kept training this model, we believe that this model would output word embeddings that can be converted into actual words and should be grammatically correct.", "Our goal for this project was to improve the commenting experience on Reddit. Using one day\u2019s comments from a dataset created by a redditor, we built two different models to assist a user in the commenting process. The first part of our project was creating and upvote predictor. The sequential neural network predictor was the best model we managed to fully create and train; it had a mean absolute error of 7.17 on the testing set. The second part of our project was generating predictive keyboards specific to each subreddit. One constant issue we faced throughout the process was having more data than we could manage.", "To create the best model, we would finish training our word embedding neural language model with more resources and time. Then, we would combine our character language model and the word embedding model to create an autocomplete keyboard that can predict the current word you\u2019re typing and give accurate suggestions for words that should come next given the context of the words you just typed. This model would be more similar to Gmail\u2019s new smart compose feature. Another consideration to create a more accurate model is to try a seq2seq model that uses two LSTMs, one LSTM for encoding the current context and a second LSTM to decode that sequence and generate predictions given the encoder weights. On Google\u2019s AI blog, we discovered that Google considered using a seq2seq model but its performance was too slow. However, they found that the seq2seq model had a better accuracy, so we can also consider using this model if our accuracy for the word embedding neural language model is not sufficient. The computational complexity of training this seq2seq model is higher than training the word embedding neural language model, so in practice this is likely unfeasible.", "There are a number of directions to explore when improving the upvote predictor model. At a glance, the hyperparameters can almost certainly be improved for all of the models we tried. Feeding the models into an XGBoost-based supervisor models would almost certainly lead to an improvement, if we could somehow overcome the GPU memory issues we faced when training that model. Exploring different neural network architectures could also be useful, especially since the embeddings might need to be treated differently from independent features. There were also a number of features that we had to forego due to lack of computing power, time, or both, and it could be very helpful to include those in the next iteration of the model. Regardless of what steps we take, we will undoubtedly need access to greater computational resources.", "Though both models can separately help improve the user experience for Redditors, a combined model could do even more for a user. The upvote predictor could be used to measure the projected quality of the comment and this information could be fed into the keyboard for completions that help increase comment quality. This would mean a better experience for contributors as well as consumers."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F98668b510a1a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@rishabh_r?source=post_page-----98668b510a1a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rishabh_r?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Rishabh Rai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb02a7cc924d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=post_page-fb02a7cc924d----98668b510a1a---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98668b510a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=-----98668b510a1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98668b510a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&source=-----98668b510a1a---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral", "anchor_text": "Brett Jordan"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://files.pushshift.io/reddit/", "anchor_text": "accumulated comment dataset"}, {"url": "https://www.reddit.com/user/Stuck_In_the_Matrix", "anchor_text": "redditor"}, {"url": "http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html", "anchor_text": "http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Colah\u2019s blog"}, {"url": "https://github.com/davidzchen-ut/subreddit-predictive-keyboard", "anchor_text": "Demo Here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----98668b510a1a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----98668b510a1a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reddit?source=post_page-----98668b510a1a---------------reddit-----------------", "anchor_text": "Reddit"}, {"url": "https://medium.com/tag/nlp?source=post_page-----98668b510a1a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----98668b510a1a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98668b510a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=-----98668b510a1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98668b510a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=-----98668b510a1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98668b510a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@rishabh_r?source=post_page-----98668b510a1a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb02a7cc924d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=post_page-fb02a7cc924d----98668b510a1a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffb02a7cc924d%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=-----98668b510a1a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@rishabh_r?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Written by Rishabh Rai"}, {"url": "https://medium.com/@rishabh_r/followers?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "7 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb02a7cc924d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=post_page-fb02a7cc924d----98668b510a1a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffb02a7cc924d%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnet-upvote-prediction-and-subreddit-based-sentence-completion-for-reddit-comments-98668b510a1a&user=Rishabh+Rai&userId=fb02a7cc924d&source=-----98668b510a1a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----98668b510a1a----0---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----98668b510a1a----0---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----98668b510a1a----0---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----98668b510a1a----0---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----98668b510a1a----0---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----98668b510a1a----0---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----0-----------------clap_footer----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----98668b510a1a----0---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----98668b510a1a----0-----------------bookmark_preview----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----98668b510a1a----1---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----98668b510a1a----1---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----98668b510a1a----1---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----98668b510a1a----1---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----98668b510a1a----1---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----98668b510a1a----1---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----98668b510a1a----1---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----98668b510a1a----1-----------------bookmark_preview----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----98668b510a1a----2---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----98668b510a1a----2---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----98668b510a1a----2---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----98668b510a1a----2---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----98668b510a1a----2---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----98668b510a1a----2---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----98668b510a1a----2---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----98668b510a1a----2-----------------bookmark_preview----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----98668b510a1a----3---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----98668b510a1a----3---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----98668b510a1a----3---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----98668b510a1a----3---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----98668b510a1a----3---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "Time-Series Forecasting: Deep Learning vs Statistics \u2014 Who Wins?A comprehensive guide on the ultimate dilemma"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----98668b510a1a----3---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": "\u00b714 min read\u00b7Apr 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----c568389d02df----3-----------------clap_footer----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----98668b510a1a----3---------------------ccfe0535_3b2e_48a2_ad08_14db30259111-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&source=-----98668b510a1a----3-----------------bookmark_preview----ccfe0535_3b2e_48a2_ad08_14db30259111-------", "anchor_text": ""}, {"url": "https://medium.com/@rishabh_r?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "See all from Rishabh Rai"}, {"url": "https://towardsdatascience.com/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----98668b510a1a----0-----------------bookmark_preview----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----98668b510a1a----1-----------------bookmark_preview----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----98668b510a1a----0---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----98668b510a1a----0-----------------bookmark_preview----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----98668b510a1a----1---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----98668b510a1a----1-----------------bookmark_preview----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----98668b510a1a----2---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----98668b510a1a----2---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----98668b510a1a----2---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Josep Ferrer"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----98668b510a1a----2---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----98668b510a1a----2---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Stop doing this on ChatGPT and get ahead of the 99% of its usersUnleash the Power of AI Writing with Effective Prompts"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----98668b510a1a----2---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "\u00b78 min read\u00b7Mar 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&user=Josep+Ferrer&userId=8213af8f3ccf&source=-----f3441bf7a25a----2-----------------clap_footer----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----98668b510a1a----2---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "71"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&source=-----98668b510a1a----2-----------------bookmark_preview----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----98668b510a1a----3---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----98668b510a1a----3---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----98668b510a1a----3---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----98668b510a1a----3---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----98668b510a1a----3---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----98668b510a1a----3---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----3-----------------clap_footer----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----98668b510a1a----3---------------------0c899749_ce78_41bb_90e1_06940ae178eb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----98668b510a1a----3-----------------bookmark_preview----0c899749_ce78_41bb_90e1_06940ae178eb-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----98668b510a1a--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----98668b510a1a--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}