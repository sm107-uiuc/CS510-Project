{"url": "https://towardsdatascience.com/the-many-uses-of-input-gradient-regularization-e2af244e6950", "time": 1683003748.89376, "path": "towardsdatascience.com/the-many-uses-of-input-gradient-regularization-e2af244e6950/", "webpage": {"metadata": {"title": "The Many Uses of Input Gradient Regularization | by Eugen Lindwurm | Towards Data Science", "h1": "The Many Uses of Input Gradient Regularization", "description": "Regularization is one of machine learning\u2019s fundamental techniques. The idea is that we make assumptions about the structure of the ideal model and modify our training process to encourage producing\u2026"}, "outgoing_paragraph_urls": [{"url": "http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf", "anchor_text": "dropout", "paragraph_index": 0}, {"url": "https://nyuscholars.nyu.edu/en/publications/improving-generalization-performance-using-double-backpropagation", "anchor_text": "indeed old", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "integrated gradients", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1312.6034", "anchor_text": "saliency maps", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1704.03296v3", "anchor_text": "iterated gradient", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1703.03717", "anchor_text": "Ross et al.", "paragraph_index": 5}, {"url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17337/15866", "anchor_text": "It has been shown", "paragraph_index": 8}, {"url": "https://asross.github.io/publications/RossLageDoshiVelez2017.pdf", "anchor_text": "this has already been done for us", "paragraph_index": 11}], "all_paragraphs": ["Regularization is one of machine learning\u2019s fundamental techniques. The idea is that we make assumptions about the structure of the ideal model and modify our training process to encourage producing models meeting these expectations. Very often, regularizers are part of the loss function. Perhaps the most common regularizers in machine learning are the L1 norm (assumption: sparse weights) and L2 norm (assumption: small weights) on the model weights, leading to lasso and ridge regression, respectively. Comparably new is the concept of dropout where we favor redundancy in our network.", "In this article, I discuss regularizing the input gradient. Concretely, this means imposing some structural constraints on how the gradient of the input features x wrt. our loss function behaves. Naturally, this technique can only be applied to training fully differentiable models, such as neural networks.", "The idea of imposing constraints on the input gradient is indeed old, but it seems to have gotten traction only over the last years. To understand its usefulness, we need to understand the meaning of the input gradient. Intuitively, the input gradient tells us how the model loss changes under very small perturbations of the input. The input gradient is the direction in input space that results in the larges change in loss, locally.", "This property makes it the central piece of many explainable ML techniques, trying to give explanations of the output in terms of inputs. (See e.g. integrated gradients, saliency maps, iterated gradient.) The idea is that knowing which parts of the inputs drastically change the output under small perturbations tells us what inputs are responsible for the current output.", "In a very broad sense of the word, the input gradient tells us what the network attends to. Regularizing the input gradient thus gives us the power of changing how our model sees the world.", "The most natural way of restricting the input gradient is to tell the model which areas of the input are important and which to ignore. Ross et al. implemented a simple penalty preventing a neural network from looking at certain parts of the input.", "In image processing applications, noise or spurious correlations can potentially be avoided by training the network to only attend to foreground objects and having zero gradient on background elements.", "If successfully applied, this regularization can potentially improve generalization (ignore spurious correlations) and learn faster (using only important parts of the input).", "Another interesting application is to use input regularization for adversarial robustness. It has been shown that an L2 regularization of the input gradient can avoid large gradients on the inputs \u2014 precisely what makes networks vulnerable to adversarial attacks.", "One can think of this regularizer as changing the network's perception of the input in such a way that it has to pay attention to many features a bit, rather than relying on a few features a lot. In that sense, this regularization has a similar motivation to dropout.", "Interestingly, adversarial attacks created for an input gradient-regularized network can successfully be transferred to other networks trained on the same dataset. This makes this simple regularizer not only a tool for defense but potentially for creating attacks without knowing the target network.", "After looking at the uses of an L2 regularizer on the input gradient, it is only natural to explore the utility of L1 regularization. Luckily, this has already been done for us. And indeed, using the L1 norm shifts the network\u2019s perception of the data in such a way that it only deems few inputs as relevant at any time.", "This can be useful when feature acquisition is costly and one would prefer models that can work with few features only. Perhaps, it could also be used as an analysis tool for analyzing feature relevance, similar to conventional neural attention mechanisms.", "The beauty of regularization methods is their simplicity. You can apply input gradient regularization to any network with little effort (but some computational cost).", "I hope you learned something useful!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in Machine Learning. Interested in social and environmental issues."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe2af244e6950&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2af244e6950--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2af244e6950--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----e2af244e6950--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----e2af244e6950--------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00----e2af244e6950---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2af244e6950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2af244e6950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@pjswinburn?utm_source=medium&utm_medium=referral", "anchor_text": "Philip Swinburn"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf", "anchor_text": "dropout"}, {"url": "https://nyuscholars.nyu.edu/en/publications/improving-generalization-performance-using-double-backpropagation", "anchor_text": "indeed old"}, {"url": "https://arxiv.org/abs/1703.01365", "anchor_text": "integrated gradients"}, {"url": "https://arxiv.org/abs/1312.6034", "anchor_text": "saliency maps"}, {"url": "https://arxiv.org/abs/1704.03296v3", "anchor_text": "iterated gradient"}, {"url": "https://arxiv.org/abs/1703.03717", "anchor_text": "Ross et al."}, {"url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17337/15866", "anchor_text": "It has been shown"}, {"url": "https://asross.github.io/publications/RossLageDoshiVelez2017.pdf", "anchor_text": "this has already been done for us"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e2af244e6950---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e2af244e6950---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e2af244e6950---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/explainable-ai?source=post_page-----e2af244e6950---------------explainable_ai-----------------", "anchor_text": "Explainable Ai"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e2af244e6950---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2af244e6950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----e2af244e6950---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2af244e6950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----e2af244e6950---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2af244e6950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2af244e6950--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe2af244e6950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e2af244e6950---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2af244e6950--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e2af244e6950--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e2af244e6950--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e2af244e6950--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e2af244e6950--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e2af244e6950--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e2af244e6950--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e2af244e6950--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/@pflaenzchen/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "135 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56b4244ba55f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-many-uses-of-input-gradient-regularization-e2af244e6950&newsletterV3=cb9e52bc6a00&newsletterV3Id=56b4244ba55f&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}