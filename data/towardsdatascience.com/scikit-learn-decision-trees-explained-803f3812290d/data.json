{"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d", "time": 1682995143.018049, "path": "towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d/", "webpage": {"metadata": {"title": "Scikit-Learn Decision Trees Explained | by Frank Ceballos | Towards Data Science", "h1": "Scikit-Learn Decision Trees Explained", "description": "Decision trees are the most important elements of a Random Forest. They are capable of fitting complex data sets while allowing the user to see how a decision was taken. While searching the web I was\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Iris_flower_data_set", "anchor_text": "iris flower data se", "paragraph_index": 2}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html", "anchor_text": "export_graphviz", "paragraph_index": 6}, {"url": "http://graphviz.org/", "anchor_text": "graphviz", "paragraph_index": 6}, {"url": "http://graphviz.org/", "anchor_text": "graphviz", "paragraph_index": 7}, {"url": "https://www.youtube.com/watch?v=RYm8lskGiYk", "anchor_text": "YouTube tutorial", "paragraph_index": 7}, {"url": "https://www.linkedin.com/pulse/white-box-black-choosing-machine-learning-model-your-vidyadhar-ranade/", "anchor_text": "article", "paragraph_index": 8}, {"url": "https://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "parameters the DecisionTreeClassifier", "paragraph_index": 19}, {"url": "https://stats.stackexchange.com/questions/324370/references-on-number-of-features-to-use-in-random-forest-regression", "anchor_text": "Here is an article that recommends how to set max_features.", "paragraph_index": 25}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn", "paragraph_index": 29}, {"url": "https://www.frank-ceballos.com/", "anchor_text": "personal blog", "paragraph_index": 29}], "all_paragraphs": ["Decision trees are the most important elements of a Random Forest. They are capable of fitting complex data sets while allowing the user to see how a decision was taken. While searching the web I was unable to find one clear article that could easily describe them, so here I am writing about what I have learned so far. It\u2019s important to note, a single decision tree is not a very good predictor; however, by creating an ensemble of them (a forest) and collecting their predictions, one of the most powerful machine learning tools can be obtained \u2014 the so called Random Forest.", "Make sure you have installed pandas and scikit-learn on your machine. If you haven't, you can learn how to do so here.", "Let\u2019s start by creating decision tree using the iris flower data set. The iris data set contains four features, three classes of flowers, and 150 samples.", "Features: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)", "Numerically, setosa flowers are identified by zero, versicolor by one, and virginica by two.", "For simplicity, we will train our decision tree using all features and setting the depth to two.", "Of course we still do not know how this tree classifies samples, so let\u2019s visualize this tree by first creating a dot file using Scikit-Learn export_graphviz module and then processing it with graphviz.", "This will create a file named tree.dot that needs to be processed on graphviz. Here is a YouTube tutorial that shows you how to process such a file with graphviz. The end result should be similar to the one shown in Figure-1; however, a different tree might sprout even if the training data is the same!", "A single decision tree is the classic example of a type of classifier known as a white box. The predictions made by a white box classifier can easily be understood. Here is an excellent article about black and white box classifiers.", "In Figure-1, you can see that each box contains several characteristics. Let\u2019s start by describing the content of the top most node, most commonly referred to as the root node. The root node is at a depth of zero, see Figure-2. A node is a point along the decision tree where a question is asked. This action divides the data into smaller subsets.", "petal length (cm) <=2.45: The first question the decision tree ask is if the petal length is less than 2.45. Based on the result, it either follows the true or the false path.", "gini = 0.667: The gini score is a metric that quantifies the purity of the node/leaf (more about leaves in a bit). A gini score greater than zero implies that samples contained within that node belong to different classes. A gini score of zero means that the node is pure, that within that node only a single class of samples exist. You can find out more about impurity measures here. Notice that we have a gini score greater than zero; therefore, we know that the samples contained within the root node belong to different classes.", "samples = 150: Since the iris flower data set contains 150 samples, this value is set to 150.", "value = [50, 50, 50]: The value list tells you how many samples at the given node fall into each category. The first element of the list shows the number of samples that belong to the setosa class, the second element of the list shows the number of samples that belong to the versicolor class, and the third element in the list shows the number of samples that belong to the virginica class. Notice how this node is not a pure one since different types of classes are contained within the same node. We knew this already from the gini score, but it\u2019s nice to actually see it.", "class = setosa: The class value shows the prediction a given node will make and it can be determined from the value list. Whichever class occurs the most within the node will be selected as the class value. If the decision tree were to end at the root node, it would predict that all 150 samples belonged to the setosa class. Of course this makes no sense, since there is an equal number of samples for each class. It seems to me that the decision tree is programmed to choose the first class on the list if there is an equal number of samples for each class.", "To determine which feature to use to make the first split \u2014 that is, to make the root node \u2014 the algorithm chooses a feature and makes a split. It then looks at the subsets and measures their impurity using the gini score. It does this for multiple thresholds and determines that the best split for the given feature is the one that produces the purest subsets. This is repeated for all the features in the training set. Ultimately, the root node is determined by the feature that produces a split with purest subsets. Once the root node is decided, the tree is grown to a depth of one. The same process is repeated for the other nodes in the tree.", "Suppose we have a flower with petal_length = 1 and petal_width = 3. If we follow the logic of the decision tree shown on Figure-1, we will see that we will end up in the orange box. In Figure-1, if the question a node asks turns out to be true (false), we will move to the left (right). The orange box is at a depth of one, see Figure-2. Since there is nothing growing out of this box, we will refer to it as a leaf node. Notice the resemblance this has to an actual tree, see Figure-3. Moreover, note that the gini score is zero \u2014 which makes it a pure leaf. The total number of samples is 50. Out of the 50 samples that end up on the orange leaf node, we can see that all of them belong to the setosa class, see the value list for this leaf. Therefore, the tree will predict that the sample is a setosa flower.", "Let us pick a more interesting sample. For instance, petal_length = 2.60 and petal_width = 1.2 . We start at the root node which asks whether the petal length is less than 2.45. This is false; therefore we move to the internal node on the right, where the gini score is 0.5 and the total number of samples is 100. This internal node at a depth of one will ask the question \u201cIs the petal width less than 1.75?\u201d In our case, this is true, so we move to the left and end up in the green colored leaf node which is at a depth of 2. The decision tree will predict that this sample is a versicolor flower. You can see that this is most likely the case because 49 out of the 54 samples that end up in the green leaf node were versicolor flowers, see the value list for this leaf.", "Now that we know how our decision tree works, let us make predictions. The input should be in a list and ordered as [sepal length, sepal width, petal length, petal width] where the sepal length and sepal width wont affect the predictions made by the decision tree shown in Figure-1; therefore, we will can assign them an arbitrary value.", "If you take a look at the parameters the DecisionTreeClassifier can take, you might be surprised so, let\u2019s look at some of them.", "criterion : This parameter determines how the impurity of a split will be measured. The default value is \u201cgini\u201d but you can also use \u201centropy\u201d as a metric for impurity.", "splitter: This is how the decision tree searches the features for a split. The default value is set to \u201cbest\u201d. That is, for each node, the algorithm considers all the features and chooses the best split. If you decide to set the splitter parameter to \u201crandom,\u201d then a random subset of features will be considered. The split will then be made by the best feature within the random subset. The size of the random subset is determined by the max_features parameter. This is partly where a Random Forest gets its name.", "max_depth: This determines the maximum depth of the tree. In our case, we use a depth of two to make our decision tree. The default value is set to none. This will often result in over-fitted decision trees. The depth parameter is one of the ways in which we can regularize the tree, or limit the way it grows to prevent over-fitting. In Figure-4, you can see what happens if you don\u2019t set the depth of the tree \u2014 pure madness!", "min_samples_split: The minimum number of samples a node must contain in order to consider splitting. The default value is two. You can use this parameter to regularize your tree.", "min_samples_leaf: The minimum number of samples needed to be considered a leaf node. The default value is set to one. Use this parameter to limit the growth of the tree.", "max_features: The number of features to consider when looking for the best split. If this value is not set, the decision tree will consider all features available to make the best split. Depending on your application, it\u2019s often a good idea to tune this parameter. Here is an article that recommends how to set max_features.", "For syntax purposes, lets set some of these parameters:", "Now you know how create a decision tree using Scikit-learn. More importantly, you should be able to visualize it and understand how it classifies samples. It\u2019s important to note that one needs to limit the liberty of a decision tree. There are several parameters that can regularized a tree. By default, the max_depth is set to none. Therefore, a tree will grow fully, which often results in over-fitting. Moreover, a single decision tree is not a very powerful predictor.", "The real power of decision trees unfolds more so when cultivating many of them \u2014 while limiting the way they grow \u2014 and collecting their individual predictions to form a final conclusion. In other words, you grow a forest, and if your forest is random in nature, using the concept of bagging and with splitter = \"random\", we call this a Random Forest. Many of the parameters used in Scikit-Learn Random Forest are the same ones explained in this article. So it\u2019s a good idea to understand what a single decision tree is and how it works, before moving on to using the big guns.", "You can find me in LinkedIn or visit my personal blog.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F803f3812290d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----803f3812290d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----803f3812290d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----803f3812290d--------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----803f3812290d--------------------------------", "anchor_text": "Frank Ceballos"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe----803f3812290d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@szmigieldesign?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Lukasz Szmigiel"}, {"url": "https://unsplash.com/search/photos/tree?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Iris_flower_data_set", "anchor_text": "iris flower data se"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html", "anchor_text": "export_graphviz"}, {"url": "http://graphviz.org/", "anchor_text": "graphviz"}, {"url": "http://graphviz.org/", "anchor_text": "graphviz"}, {"url": "https://www.youtube.com/watch?v=RYm8lskGiYk", "anchor_text": "YouTube tutorial"}, {"url": "https://www.linkedin.com/pulse/white-box-black-choosing-machine-learning-model-your-vidyadhar-ranade/", "anchor_text": "article"}, {"url": "https://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php", "anchor_text": "here"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "parameters the DecisionTreeClassifier"}, {"url": "https://stats.stackexchange.com/questions/324370/references-on-number-of-features-to-use-in-random-forest-regression", "anchor_text": "Here is an article that recommends how to set max_features."}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn"}, {"url": "https://www.frank-ceballos.com/", "anchor_text": "personal blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----803f3812290d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----803f3812290d---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----803f3812290d---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&user=Frank+Ceballos&userId=176ad7e37afe&source=-----803f3812290d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&user=Frank+Ceballos&userId=176ad7e37afe&source=-----803f3812290d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----803f3812290d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F803f3812290d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----803f3812290d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----803f3812290d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----803f3812290d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----803f3812290d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----803f3812290d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----803f3812290d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----803f3812290d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----803f3812290d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----803f3812290d--------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Frank Ceballos"}, {"url": "https://frank-ceballos.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "856 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3961b72dd61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscikit-learn-decision-trees-explained-803f3812290d&newsletterV3=176ad7e37afe&newsletterV3Id=f3961b72dd61&user=Frank+Ceballos&userId=176ad7e37afe&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}