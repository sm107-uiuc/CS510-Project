{"url": "https://towardsdatascience.com/the-transformer-a-quick-run-through-ce9b21b4f3ed", "time": 1683006682.6971881, "path": "towardsdatascience.com/the-transformer-a-quick-run-through-ce9b21b4f3ed/", "webpage": {"metadata": {"title": "The Transformer: A Quick Run Through | by Mandar Deshpande | Towards Data Science", "h1": "The Transformer: A Quick Run Through", "description": "In the previous post, we looked at how ELMo and ULMFiT boosted the prominence of language model pre-training in the community. This blog assumes that you have read through the previous two parts of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/language-modelingii-ulmfit-and-elmo-d66e96ed754f?source=friends_link&sk=8419eea77e21bb3f93b91a0f6447ca92", "anchor_text": "5 part series", "paragraph_index": 0}, {"url": "https://medium.com/p/d66e96ed754f", "anchor_text": "previous post", "paragraph_index": 1}, {"url": "https://medium.com/p/d66e96ed754f", "anchor_text": "previous two parts", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "The Transformer", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "original paper", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/language-modeling-c1cf7b983685", "anchor_text": "Link to Part I", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/language-modelingii-ulmfit-and-elmo-d66e96ed754f", "anchor_text": "Link to Part II", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/the-transformer-a-quick-run-through-ce9b21b4f3ed", "anchor_text": "Link to Part III", "paragraph_index": 25}, {"url": "https://twitter.com/mandroid_6", "anchor_text": "https://twitter.com/mandroid_6", "paragraph_index": 27}, {"url": "http://jalammar.github.io", "anchor_text": "Jay Alammar\u2019s blog", "paragraph_index": 28}], "all_paragraphs": ["This is Part 3 of the 5 part series on language modeling.", "In the previous post, we looked at how ELMo and ULMFiT boosted the prominence of language model pre-training in the community. This blog assumes that you have read through the previous two parts of this series and thus builds upon that knowledge.", "The Transformer has been seen as the model which finally removed the limitations of sequence model training through recurrent neural networks. The idea picked up in language modeling and machine translation around the use of encoder-decoder stacking turned out to be valuable learning in the process of building this architecture. The Transformer is a simple network architecture solely based on attention mechanism and giving away any kind of recurrence and convolutions entirely. It has been shown to generalize well to other language understanding and modeling tasks, with large and limited training data. It also achieved the state of the art results on the English-to-German translation task and anchored itself as the go-to architecture for future advancements in model pre-training in NLP.", "In this model, multiple encoders are stacked on top of each other, and similarly, decoders are stacked together. Usually, each encoder/decoder comprises recurrent connections and convolution, and the hidden representation from each encoder stage is passed ahead to be used by the next layer. Most seq2seq tasks can easily be solved using such a stack of encoders-decoders which processes each word in the input sequence in order.", "Since attention mechanism has become an integral part of sequence modeling and transduction models in various tasks allow modeling dependencies without regard to their distance in the input or output sequences. To put it in simple terms; the attention mechanism helps us tackle long-range dependency issues in neural networks without the use of recurrent neural networks (RNN). This solves the exact purpose addressed by hidden state shared across all time steps in RNN, through the use of encoder-decoder based architecture. The attention model focuses on the relevant part of the input text sequence or image as per the task being solved.", "In a regular RNN, context is passed in terms of the final hidden state produced by the encoders and uses it to produce the next token of the translation or text.", "Steps involved in generating the Context Vector:", "Once the context vector has been fully updated, it is passed to the decoder as an additional input to the word/token being translated. The context vector is a useful abstraction, except that it acts as a bottleneck for the representation of the entire meaning of the input sequence.", "Instead of passing a single context vector to the decoder, the attention mechanism passes all the intermediate hidden states within a stack of encoders to the decoder. This enables the decoder to focus on different parts of the input sequence as per the relevance of the current word/token being processed.", "Unlike the previous seq2seq models, attention models perform 2 extra steps:", "The Transformer consists of 6 stacked encoders and 6 stacked decoders to form the main architecture of the model. This number can be variable as per the use-case, but 6 has been used in the original paper.", "Let us consider a single encoder and decoder stack to simplify our understanding of the working.", "Each encoder consists of a Self-Attention layer followed by the Feed Forward network. Usually in attention mechanisms, hidden states from the previous states are utilized for the calculation of attention. Instead, self-attention uses trained embeddings from the same layer to compute the attention vector. To elucidate, self-attention could be thought of as a mechanism for coreference resolution within a sentence:", "\u201cThe man was eating his meal while he was thinking about his family\u201d", "In the above sentence, the model needs to build an understanding of what he refers to, and that it is a coreference to the man.This is enabled by the self-attention mechanism in the Transformer. A detailed discussion on self-attention (using multiple heads) is beyond the scope of this blog and can be found in the original paper.", "The decoder also has the same two layers as the encoder, except that additional encoder-decoder attention is introduced in between to help the model extract relevant features from attention vectors from the encoder.", "Point-wise Feed-Forward NetworksIt is important to notice that each word in the input sequence shares the computation in the self-attention layer, but each word flows through a separate feed-forward network. The output from the feed-forward network is passed on to the next encoder in the stack which utilizes this learned context from previous encoders.", "Positional EncodingTo embed a sense of time in the input sequence, each word is concatenated with a positional encoding. This augmented input word embedding is passed as input to Encoder 1. Since the model doesn\u2019t use any recurrence or convolution, positional encodings encode some information about the relative position in the input sentence.", "Residual Connections with NormalizationThe output from the self-attention layer is added with the original word embedding using residual connections and layer normalization. A similar scheme is followed by the feed-forward layer.", "Fully Connected Linear with Softmax Once a point vector is given out by the final decoder in the stack, it needs to be converted into the translated word. Now that we already have all the required information embedded as floats in this output vector, we just need to convert it to a probability over possible next word in the translation.", "The fully connected linear network converts the float vector into scores which are transformed into probability values using the softmax function. The index with the highest softmax value is chosen and retrieved from the output vocabulary learned from the training set.", "The training is supervised i.e. uses labeled training dataset which can be used as a benchmark for comparison and correction of output word probabilities.", "Essentially, each word in the translated output vocabulary is converted into a one-hot vector that is 1, only at the index where the word is present and 0 everywhere else. Now, once we receive the softmax output vector comprised of normalized probability values, we can compare it with the one-hot vectors to improve model parameters/weights.", "These two vectors can be compared by using some similarity metrics like cosine similarity, cross-entropy, and/or Kullback-Leibler divergence. At the beginning of the training process, the output probability distribution is much further off than the ground truth one-hot vector. As training proceeds and the weights get optimized, the output word probabilities closely track the ground truth vectors.", "I hope this blog helped you understand how the Transformer works, and I plan to explore the BERT model in the next post.", "Link to Part I: Language Modeling ILink to Part II: Language Modeling II: ELMo and ULMFiTLink to Part III : The Transformer: A Quick Run Through", "Keep learning and growing till then!", "Connect with me on Twitter: https://twitter.com/mandroid_6", "All images above have been created by me and I have the rights to use them. I have taken inspiration from Jay Alammar\u2019s blog for the visualizations", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Engineer @awsai. Grad @ UCLA. Prev: ML Engineer at Citi, GSoC @TensorFlow, gensim, Scilab 17'"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fce9b21b4f3ed&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@razzormandar?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@razzormandar?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "Mandar Deshpande"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd08a98ebec56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&user=Mandar+Deshpande&userId=d08a98ebec56&source=post_page-d08a98ebec56----ce9b21b4f3ed---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce9b21b4f3ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce9b21b4f3ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/language-modelingii-ulmfit-and-elmo-d66e96ed754f?source=friends_link&sk=8419eea77e21bb3f93b91a0f6447ca92", "anchor_text": "5 part series"}, {"url": "https://translate.google.com/?hl=en&tab=TT", "anchor_text": "translate.google.com"}, {"url": "https://medium.com/p/d66e96ed754f", "anchor_text": "previous post"}, {"url": "https://medium.com/p/d66e96ed754f", "anchor_text": "previous two parts"}, {"url": "https://www.linkedin.com/in/mandardeshpande1995/", "anchor_text": "(Mandar Deshpande)"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "The Transformer"}, {"url": "https://www.linkedin.com/in/mandardeshpande1995/", "anchor_text": "(Mandar Deshpande)"}, {"url": "https://www.linkedin.com/in/mandardeshpande1995/", "anchor_text": "(Mandar Deshpande)"}, {"url": "https://www.linkedin.com/in/mandardeshpande1995/", "anchor_text": "(Mandar Deshpande)"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "original paper"}, {"url": "https://www.linkedin.com/in/mandardeshpande1995/", "anchor_text": "(Mandar Deshpande)"}, {"url": "https://www.linkedin.com/in/mandardeshpande1995/", "anchor_text": "(Mandar Deshpande)"}, {"url": "https://towardsdatascience.com/language-modeling-c1cf7b983685", "anchor_text": "Link to Part I"}, {"url": "https://towardsdatascience.com/language-modelingii-ulmfit-and-elmo-d66e96ed754f", "anchor_text": "Link to Part II"}, {"url": "https://towardsdatascience.com/the-transformer-a-quick-run-through-ce9b21b4f3ed", "anchor_text": "Link to Part III"}, {"url": "https://twitter.com/mandroid_6", "anchor_text": "https://twitter.com/mandroid_6"}, {"url": "http://jalammar.github.io", "anchor_text": "Jay Alammar\u2019s blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ce9b21b4f3ed---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ce9b21b4f3ed---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ce9b21b4f3ed---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-translation?source=post_page-----ce9b21b4f3ed---------------machine_translation-----------------", "anchor_text": "Machine Translation"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ce9b21b4f3ed---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce9b21b4f3ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&user=Mandar+Deshpande&userId=d08a98ebec56&source=-----ce9b21b4f3ed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce9b21b4f3ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&user=Mandar+Deshpande&userId=d08a98ebec56&source=-----ce9b21b4f3ed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce9b21b4f3ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fce9b21b4f3ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ce9b21b4f3ed---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ce9b21b4f3ed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@razzormandar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@razzormandar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mandar Deshpande"}, {"url": "https://medium.com/@razzormandar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "186 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd08a98ebec56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&user=Mandar+Deshpande&userId=d08a98ebec56&source=post_page-d08a98ebec56--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe4ea3b3d0051&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-a-quick-run-through-ce9b21b4f3ed&newsletterV3=d08a98ebec56&newsletterV3Id=e4ea3b3d0051&user=Mandar+Deshpande&userId=d08a98ebec56&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}