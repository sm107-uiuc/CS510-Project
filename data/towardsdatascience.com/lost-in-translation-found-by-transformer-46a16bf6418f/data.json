{"url": "https://towardsdatascience.com/lost-in-translation-found-by-transformer-46a16bf6418f", "time": 1683000419.490575, "path": "towardsdatascience.com/lost-in-translation-found-by-transformer-46a16bf6418f/", "webpage": {"metadata": {"title": "Lost in Translation. Found by Transformer. BERT Explained. | by Michel Kana, Ph.D | Towards Data Science", "h1": "Lost in Translation. Found by Transformer. BERT Explained.", "description": "Tackle the mystery of Transformer model used by GPT-2, BERT"}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2 from OpenAI", "paragraph_index": 0}, {"url": "https://www.technologyreview.com/s/614143/nvidia-just-made-it-easier-to-build-smarter-chatbots-and-slicker-fake-news/", "anchor_text": "AI writing assistants, dialog agents", "paragraph_index": 0}, {"url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954", "anchor_text": "generating misleading news articles", "paragraph_index": 0}, {"url": "https://www.technologyreview.com/s/613111/an-ai-for-generating-fake-news-could-also-help-detect-it/", "anchor_text": "to detect fake content", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is All You Need", "paragraph_index": 1}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Sutskever et al., 2014", "paragraph_index": 3}, {"url": "http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf", "anchor_text": "Cho et al., 2014", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "anchor_text": "previous article", "paragraph_index": 5}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "the Tensor2Tensor notebook", "paragraph_index": 9}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "Tensor2Tensor notebook", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "layer normalization", "paragraph_index": 23}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Check it up!", "paragraph_index": 26}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "AlphaStar", "paragraph_index": 27}], "all_paragraphs": ["Throughout this article, you will learn why it is possible to generate fake news. Transformer-based language models such as GPT-2 from OpenAI have recently enabled computers to generate realistic and coherent continuations about a topic of their choosing. Applications can be found in AI writing assistants, dialog agents, speech recognition systems. Transformer-based models are also successful at generating misleading news articles, impersonate others online, creating faked content to post on social media, upscale the production of phishing content. Surprisingly the same Transformer-based approach can be also used to detect fake content.", "The Transformer is a machine learning model that was proposed in the paper Attention is All You Need by Vaswani et al. It outperformed the Google Neural Machine Translation model in specific tasks, thanks to parallelization, in opposite of the sequential nature of sequence-to-sequence models.", "In order to understand how Transformer works, let\u2019s have a look at how it performs machine translation.", "When sequence-to-sequence models were invented by Sutskever et al., 2014, Cho et al., 2014, there was quantum jump in the quality of machine translation. We recently wrote about sequence-to-sequence models at work.", "Thanks to sequence-to-sequence approach, for the first time, we were able to encode the source text into an internal fixed-length context vector. Once encoded, different decoding systems could be used to translate the context into different languages.", "Sequence-to-sequence models are usually enhanced by connecting the encoder and decoder through an attention mechanism, as we demonstrated in our previous article.", "Transformer is a different architecture for transforming one sequence into another one with the help of two parts, Encoder and Decoder. It however goes beyond sequence-to-sequence paradigm, that was ruling the machine translation world since 2014.", "The Transformer is fully built on the attention mechanism. That is, it does not imply any recurrent networks: Attention Is All What You Need.", "The original paper suggests the encoding component as a stack of six encoders and six decoders as illustrated below. All six encoders are identical, however they do not share weights. The same applies for the six decoders.", "Each encoder has two layers: a self-attention layer and a feed-forward layer. As we can see, no recurrent layer is used. Given an input word (its embedding, with recommended size 512), the self-attention layer looks at other words in the input sentence for clues that can help lead to a better encoding for this word. In order to get a better grasp of this mechanism, you might want to play around with the animation in the Tensor2Tensor notebook.", "Transformer calculates self-attention using 64-dimension vectors. For each input word, there is a query vector q, a key vector k, and a value vector v, which are maintained. Put all together they build the matrices Q, K and V. These matrices are created by multiplying the embedding of the input words X by three matrices Wq, Wk, Wv which are initialized and learned during training process.", "In the illustration below, X has 3 rows, each representing 1 word from the input sentence \u201cje suis \u00e9tudiant\u201d. With padding, the matrix X would be extended to 512. Each word is encoded by 4-dimensional embedding for illustration purpose. The original Transformer uses 512-dimensional embedding. In our example, the W matrices should have shape (4, 3) where 4 is the embedding dimension and 3 is the self-attention dimension. The original Transformer uses 64. Therefore Q, K, V are (3, 3)-matrices, where the first 3 corresponds to the number of words and the second 3 corresponds to the self-attention dimension.", "Let\u2019s assume that we have some values in those vectors q, k, v for now. We need to score each word of the input sentence against the current input word. Let\u2019s score the word \u201cje\u201d which means \u201cI\u201d in French. The score determines how much focus to place on other parts \u201csuis\u201d and \u201c\u00e9tudiant\u201d of the input sentence \u201cje suis \u00e9tudiant\u201d.", "The score is simply the dot product of the query vector q_1 with the key vector k_1 of the word \u201cje\u201d. This gives 112 in the illustrative example below. We do the same for the second word \u201csuis\u201d to obtain its score 96. We also do this for the last word \u201c\u00e9tudiant\u201d.", "The authors of Transformer found out that, when we divide the scores by the square root of the dimension of the key vectors (sqrt of 64, which is 8), this leads to having more stable gradients.", "The result is passed through softmax operation to normalize the scores so they are all positive and add up to 1 (0.84 for \u201cje\u201d, 0.12 for \u201csuis\u201d, 0.04 for \u201c\u00e9tudiant\u201d). Clearly the word \u201csuis\u201d is more relevant to \u201cje\u201d than the word \u201c\u00e9tudiant\u201d. Now we can drown-out irrelevant words, such as \u201c\u00e9tudiant\u201d, and reduce the attention on \u201csuis\u201d, by multiply each value vector by the softmax score.", "The output z_1 of the self-attention layer for \u201cje\u201d is finally obtained by summing up the weighted value vectors. The resulting vector is sent along to the feed-forward neural network.", "This self-attention calculation is repeated for each single word in the sequence, in matrix form, which is very fast. Q is a matrix that contains all queries vectors of all words in the sequence, K is the matrix of keys and V is the matrix of all values.", "The whole calculation of self-attention is nicely presented below.", "Now, we know how to calculate self-attention. You have probably noticed that it could happen that attention is drawn towards the same words over and over. It is like with that colleague who receives all the attention from the boss. What if we kind of reshuffle things to avoid biased attention. Just imagine, we have more of those Wq, Wk, Wv matrices, which were used to calculate the Q, K and V matrices, which were further used to compute self-attention for all words. We recall that Wq, Wk, Wv are learned during training. So, if we have multiple of such matrices, we could clone the self-attention calculation, each time leading to a different view of our text. Thus, we project the input embedding into a different representation subspace. For example, if we do the same self-attention calculation, 8 different times with 8 different Wq, Wk, Wv weight matrices, we end up with 8 different Z matrices. Actually it is what the author of Transformer did. They concatenated all eight Z matrices, then multiplied the resulting huge matrix by an additional weights matrix Wo.", "The result is amazing. Below, let\u2019s have a look at a graphical example from the Tensor2Tensor notebook. It contains an animation of where the 8 attention heads are looking at within each of the 6 encoder layers. The image below shows two attention heads in layer 5 when coding the word \u201cit\u201d. Clearly, \u201cit\u201d refers to \u201canimal\u201d according to the orange attention head, and more to \u201ctired\u201d according to the green one. Therefore the model representation of \u201cit\u201d includes the representation of both \u201canimal\u201d and \u201ctired\u201d.", "The encoding part of Transformer does additional magical things, we will only introduce here.", "Learning the position of each word or the distance between words can improve translation, especially for a language like German, where verbs come at the very end of the sentence many times. Transformer generates and learn a special positional vector that is added to the input embedding before it is fed into the first encoder layer.", "Within each encoder, the Z output from the Self-Attention layer goes through a layer normalization using the input embedding (after adding the positional vector). It has a similar effect as Batch Normalization, and reduces training and inference time.", "The Z output from the layer normalization is fed into feed forward layers, one per word. Finally the result from the feed forward layers are collected through a layer normalization to produce the inputs representations for the next encoder layer. This process happens in each of the 6 encoder layers. The original full architecture of the Transformer is depicted below.", "After last encoder layer has produced K and V matrices, the decoder can start. Each decoder has an encoder-decoder attention layer for focusing on appropriate places in the input sequence in the source language. The encoder-decoder attention layer uses queries Q from the previous decoder layer, and the memory keys K and values V from the output of the last encoder layer. In the first decoding time step, the decoder produces the first target word \u201cI\u201d in our example, as translation for \u201cje\u201d in French. In the second step, \u201cI\u201d is used as input to the first decoder layer, as well as K, V from the encoder to predict the second target word \u201cam\u201d. In the third decoding step, \u201cI am\u201d is used as input to predict \u201ca\u201d. In the fourth step, \u201cI am a\u201d is used to predict \u201cstudent\u201d. Finally, \u201cI am a student\u201d is used to predict \u201c<end of sentence>\u201d.", "Below you can see a fantastic animation created by Jay Alammar. His blog explains Transformer in a very creative and simplistic way. It has inspired us while writing this article. Check it up!", "The simple idea of focusing on salient parts of input by taking a weighted average of them, has proven to be the key factor of success for DeepMind AlphaStar, the model that defeated a top professional Starcraft player. AlphaStar\u2019s behavior is generated by a Transformer-based model that receives input data from the raw game interface (a list of units and their properties). The model outputs a sequence of instructions that constitute an action within the game.", "In this article we gently explained how Transformers work and why it has been successfully used for sequence transduction tasks. In our next article below, we will apply this approach to the natural language understanding task that deals with intent classification.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Husband & Dad. Mental health advocate. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F46a16bf6418f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michel-kana.medium.com/?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----46a16bf6418f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46a16bf6418f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46a16bf6418f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2 from OpenAI"}, {"url": "https://www.technologyreview.com/s/614143/nvidia-just-made-it-easier-to-build-smarter-chatbots-and-slicker-fake-news/", "anchor_text": "AI writing assistants, dialog agents"}, {"url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954", "anchor_text": "generating misleading news articles"}, {"url": "https://www.technologyreview.com/s/613111/an-ai-for-generating-fake-news-could-also-help-detect-it/", "anchor_text": "to detect fake content"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is All You Need"}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Sutskever et al., 2014"}, {"url": "http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf", "anchor_text": "Cho et al., 2014"}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "Natural Language Understanding with Sequence to Sequence ModelsHow to predict the intent behind a customer query. Seq2Seq models explained. Slot filling demonstrated on ATIS dataset\u2026towardsdatascience.com"}, {"url": "https://github.com/tensorflow/nmt", "anchor_text": "source"}, {"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "anchor_text": "Practical guide to attention mechanism for NLU tasksTested hands-on strategies to tackle attention for improving sequence to sequence modelstowardsdatascience.com"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "Jay. Alammar"}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "the Tensor2Tensor notebook"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "Jay. Alammar"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "Jay. Alammar"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "Jay. Alammar"}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "Tensor2Tensor notebook"}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "layer normalization"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Vaswani, et. al. (2017)"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Check it up!"}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "AlphaStar"}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03", "anchor_text": "BERT for dummies \u2014 Step by Step TutorialDIY Practical guide on Transformer. Hands-on proven PyTorch code for Intent Classification with BERT fine-tuned.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----46a16bf6418f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----46a16bf6418f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----46a16bf6418f---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/transformers?source=post_page-----46a16bf6418f---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----46a16bf6418f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F46a16bf6418f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----46a16bf6418f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F46a16bf6418f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----46a16bf6418f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F46a16bf6418f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F46a16bf6418f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----46a16bf6418f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----46a16bf6418f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----46a16bf6418f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----46a16bf6418f--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://michel-kana.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69e95067d2a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flost-in-translation-found-by-transformer-46a16bf6418f&newsletterV3=cb0b01fe20d2&newsletterV3Id=69e95067d2a1&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}