{"url": "https://towardsdatascience.com/linguistic-rule-writing-for-nlp-ml-64d9af824ee8", "time": 1683001652.4863071, "path": "towardsdatascience.com/linguistic-rule-writing-for-nlp-ml-64d9af824ee8/", "webpage": {"metadata": {"title": "Writing Linguistic Rules for Natural Language Processing | by Jenny Lee | Towards Data Science", "h1": "Writing Linguistic Rules for Natural Language Processing", "description": "When I first started exploring data science towards the end of my Ph.D. program in linguistics, I was pleased to discover the role of linguistics \u2014 specifically, linguistic features \u2014 in the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://nlp.stanford.edu/projects/socialsent/", "anchor_text": "SocialSent", "paragraph_index": 2}, {"url": "https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c", "anchor_text": "these ones", "paragraph_index": 2}, {"url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197816", "anchor_text": "article", "paragraph_index": 2}, {"url": "https://link.springer.com/referenceworkentry/10.1007%2F978-3-319-02240-6_7", "anchor_text": "how a child acquires language", "paragraph_index": 5}, {"url": "https://pdfs.semanticscholar.org/6e2f/9f203db02be6d7a90d7b6c6819c1c65ab6ae.pdf", "anchor_text": "this one", "paragraph_index": 11}, {"url": "https://spacy.io/", "anchor_text": "spaCy", "paragraph_index": 18}, {"url": "https://spacy.io/usage/spacy-101#features", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://spacy.io/usage/models", "anchor_text": "your choice", "paragraph_index": 18}, {"url": "https://explosion.ai/demos/displacy", "anchor_text": "displaCy", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Wh-movement#Pied-piping", "anchor_text": "pied-piped", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Preposition_stranding", "anchor_text": "\u201cstrands\u201d a preposition", "paragraph_index": 33}], "all_paragraphs": ["When I first started exploring data science towards the end of my Ph.D. program in linguistics, I was pleased to discover the role of linguistics \u2014 specifically, linguistic features \u2014 in the development of NLP models. At the same time, I was a bit perplexed by why there was relatively little talk of exploiting syntactic features (e.g., level of clausal embedding, presence of coordination, type of speech act, etc.) compared to other types of features, such as how many times a certain word occurs in the text (lexical features), word similarity measures (semantic features), and even where in the document a word or phrase occurs (positional features).", "For example, in a sentiment analysis task, we may use a list of content words (adjectives, nouns, verbs, and adverbs) as features to predict the semantic orientation of user feedback (i.e., positive or negative). In another feedback classification task, we may curate domain-specific lists of words or phrases to train a model that can direct user comments to an appropriate division of support, e.g., billing, technical, or customer service.", "Both these tasks require pre-built lexicons for feature extraction, and there are many such lexicons out there for public use, such as SocialSent and these ones curated by a Medium writer. (Also check out this research article which discusses different methods of building sentiment lexicons). While lexical features have played an important role in the development of many NLP applications, there has been relatively little work involving the use of syntactic features (one big exception being perhaps in the domain of grammar error detection).", "This was baffling (and a little bit irksome) to me as someone coming from a tradition of linguistics that gives syntax something of \u201csupremacy\u201d over semantics and phonology in the general architecture of grammar. This, of course, isn\u2019t to say that linguists perceive syntax to be in some ways more important than the other branches of linguistics \u2014 but simply that it occupies a position of primacy in many models of grammar that posit that syntactic form is derived before a semantic or phonetic interpretation is achieved. Accustomed to this \u201ccentrality\u201d of the syntactic module in linguistic theory, I struggled to make sense of the lukewarm attitude toward syntactic structure in NLP, which sharply contrasted with the great confidence that NLP developers put in lexical content as features.", "Language, as imagined by NLP, made me think of a fragmented Mr. Potato Head at times \u2014 two-dimensional body parts strewn on a page of a coloring book, words without relations to each other.", "Some of the unease I felt surrounding this apparent oversimplification of language (as a bag of words in my earliest understanding of it) did eventually dissipate when I caught on to the \u201cmagic\u201d of machine learning/deep learning: Even without explicit coding, \u201cit learns the syntax, too.\u201d (And ironically, that is also how a child acquires language \u2014 without explicit instruction.)", "As a linguist (specifically a morphosyntactician), though, I am still honestly a bit hung up on the reality that syntactic features have not garnered as much attention as lexical features in NLP/ML, and I am inclined to attribute this asymmetry to the following reasons:", "As students of NLP, we are repeatedly introduced to an NLP pipeline that includes preprocessing techniques like stop words removal, lemmatization/stemming, and bag-of-words representation. These techniques are valuable in many cases but are inherently subtractive: They remove chunks of syntactic (and/or morphological) information that are present in the original text. Stop words removal eliminates function words which encode information about the relations between constituents/phrases, e.g., into, from, to, the, and, etc. Lemmatization and stemming removes important inflectional features such as tense and agreement as well as derivational affixes that may be crucial to determining the precise meaning of a word (e.g., -er in teacher, -ment in -embezzlement). And of course, bag-of-words representations disregard word order, a significant loss for a language like English which does not use morphological case to indicate word order. When these steps become normal and recurring parts of our everyday NLP pipeline, it has the potential to obscure or minimize the role that syntactic structure actually does play in natural language processing, consequently hindering or discouraging any efforts to systematically measure its impact on NLP models.", "I also speculate that the relative obscurity of syntactic features is in large part due to the practical challenges of writing sophisticated enough rules. NLP experts come into the field with varying degrees of knowledge in syntax, and without a firm grasp of the fundamentals and universals of syntactic structure that underlie every language as well as, to some extent, constraints on cross-linguistic variation, it may be difficult to know what syntactic features are relevant for a particular language, let alone go about writing heuristics to extract them. Unlike lexical features which in many cases amount to simple word counts, crafting well-thought-out linguistic rules at the sentence (syntactic) level is a much more challenging task that requires considerable domain knowledge.", "I also came to see that some of our most popular and commercialized NLP products do not require very complex syntactic representations of input text or speech \u2014 making syntactic features less than indispensable in the development of the models that power them. I think the reasons are (1) human speech in machine-human interaction tends to be oversimplified to begin with (\u201cHey Google, SF weather\u201d vs. \u201cHey John, would you mind telling me what the weather is in San Francisco today?\u201d) and (2) simple parses are more amenable to less computationally expensive systems than complex parses, so we\u2019re compelled to work with those constraints.", "To be clear, the three things I outlined above are challenges/hindrances to using syntactic features, not justifications for excluding them, in feature engineering. While I\u2019m not sure how much syntactic features can contribute to the performance of a model compared to other features, I do believe that as we seek to build machines that can interact with humans in much more subtle, natural, and sophisticated ways, syntactic input and representation will play an increasingly prominent role in the future of NLP. I think we would benefit greatly by doing these three things in response:", "In all three of these tasks, I think we would get the most mileage by exploiting, specifically, a rule-driven system. Syntax is an area that requires precise knowledge about a specific language and therefore lends itself well to rule-based representation (in NLP, that is), perhaps more so than any other area of grammar. The proposal to take a rule-based approach competes with taking an entirely ML approach to build feature sets. In the latter, you train a model that predicts the value of a specific feature to feed into a model that predicts the output of some target variable. This process is likely to be very time-consuming and expensive, however, because you\u2019re training a model to train a model. And that is why I would recommend using rules as part of a hybrid system in almost all cases. Hybrid approaches combine an ML algorithm-driven base model with a rule-based system that serves as a kind of post-processing filter, and they are old news. There have been many successful applications of them, like this one and at my own job.", "Another advantage of rules and rule-based systems is that they can be used flexibly in a variety of ways and therefore bring value and rigor to almost every stage of the ML pipeline. In addition to using them to extract features during the feature engineering stage, they can also be used to:", "Therefore, it is hardly appropriate to pit rules or rule-based systems against ML algorithms as some might be inclined to do. Rather, we must acknowledge that they complement each other in some very compelling ways, adding a serious edge to the overall architecture.", "In this final section, I will provide a practical guide to writing some syntactic rules for feature extraction. I will use spaCy to do this, focusing on extracting question type. I chose question type because this category can illustrate particularly well the incredible value of linguistic knowledge in building a partially rule-driven NLP model. Moreover, there are probably many use cases for question type extraction (and by extension, sentence type), so hopefully, someone will benefit from the code below. Some applications I can think of are: a fine-grained classification of questions (is the sentence a wh-question, a yes or no question, or a tag question?), speech act detection (is the comment a question, request, or demand?), some comparative speech analyses (which individual or group uses more politeness strategies in their speech?), etc.", "With this concluding section, I ultimately hope to have driven home both the relevance of linguistic knowledge for NLP developers and the relevance of rules or rule-based systems in NLP/ML tasks.", "In writing linguistic rules, I generally follow these six steps:", "Often these steps will not be perfectly sequential. For example, identifying the categories you want to target (step 1) basically amounts to stating some generalizations about them (step 2), so you may do these steps at the same time. Before I illustrate how this flow can guide our rule writing, let\u2019s dig into spaCy a bit.", "spaCy is a high-performance Python library that boasts of numerous features and capabilities to help you build your favorite NLP application, including tokenization, named entity recognition, part-of-speech tagging, dependency parsing, pre-trained word vectors and similarity, and built-in visualizers. Check out its full suite of features here, but here we\u2019ll only be concerned with two of its processing components tbat will enable us to represent the syntactic structure of any sentence, namely, tagger and parser. The basic flow of using spaCy goes like this: First, load a model of your choice (e.g., \"en_core_web_sm\"). You can then examine the pipeline components by calling pipeline or pipeline_name on nlp.", "Next, call nlp by passing in a string and converting it to a spaCy Doc, a sequence of tokens (line 2 below). The syntactic structure of Doc can be constructed by accessing the following attributes of its component tokens:", "I find it helpful to print out the values of these attributes along with token names and indices in tuples, like this:", "If you\u2019d like to visualize the parse, type your sentence into spaCy\u2019s built-in visualizer displaCy:", "The arrows indicate the head-dependent relations between tokens and each token is labeled with a coarse-level POS tag (fine-level (tag_) is not shown here).", "These two steps often go hand in hand. In coming up with rules for question type extraction, we must recognize that there are two types of questions in any language: wh-questions and polar questions. Read these generalizations carefully:", "You may find it helpful to visualize these question types using displaCy. The last screenshot contains a parse for a wh-question and here\u2019s one for a polar question:", "In addition to a visual parse, I like being able to see the pos-tag-dep triplet for each token, like this:", "Using your knowledge of Penn Treebank annotation conventions (because that\u2019s what spaCy uses), you can try writing v1 functions that would capture the generalizations above. I usually prefer to write separate functions for the different categories rather than writing a single function that contains complicated if/then logic. Here\u2019s a quick first version attempting to capture the generalizations in (1)-(2).", "Coming up with counterexamples to your initial generalizations is essential because they will challenge your basic assumptions about the linguistic phenomenon under consideration (here, question formation) and force you to identify a common thread between the different examples to express their behavior and characteristics in a unified, generalized way. Ultimately, that will help help you write simpler but more robust and comprehensive rules.", "The two generalizations given above are reproduced here to save you scrolling:", "Here\u2019s the first pair of counterexamples to the generalization in (1):", "We see that (1) is not borne out by these sentences because despite beginning with a wh-word, they are not wh-questions! They are what linguists call wh-clefts, or pseudoclefts. Pseudoclefts are often described as giving emphasis, or \u201cfocus\u201d, to part of the sentence and have the form in (5):", "A free relative is a wh-relative clause that (apparently) lacks a head noun (so \u201cwhat you say\u201d instead of \u201cthe thing you say\u201d). This part of the pseudocleft introduces a topic (what the sentence is about) and is linked to a focused constituent which introduces new information (\u201cunbelievable\u201d) by the verb \u201cbe\u201d.", "Here are some additional counterexamples to (1):", "These sentences fail to bear out (1) for a different reason: While they don\u2019t start with a wh-phrase as the generalization says, they still are wh-questions in that they can be answered with specific information (e.g., an answer to (6) can be \u201cI read the article to my sister.\u201d). These sentences are so-called pied-piped constructions and are characterized by a wh-word appearing along with a preposition, as though it has \u201cdragged\u201d the other to the front of the sentence. (6) in particular can be contrasted with another sentence that \u201cstrands\u201d a preposition: \u201cWho did you read the article to?\u201d.", "Finally, here are two counterexamples to the generalization about polar questions, stated in (2):", "These sentences show that in addition to auxiliary and modal verbs, the copula (\u201cbe\u201d) as a main verb (vs. an auxiliary) should be included in the category of verbs that can appear at the beginning of polar questions. Stated more simply, a verb and a subject undergo inversion in polar questions when the verb is an auxiliary, a modal, or a form of \u201cbe\u201d that serves as the main verb. This brings us to the following revised generalizations:", "I think this step demonstrates quite palpably that linguistic knowledge is indispensable to writing sophisticated feature-extracting rules. Knowing that English employs various strategies to signal interrogative sentences such as subject-verb inversion and wh-movement, as well as recognizing (and having names for) interesting linguistic phenomena, like pied-piping and clefting, can help you formulate generalizations that can in turn help you write comprehensive rules that are not overly specific.", "We can now modify the earlier versions of the functions, is_wh_question_v1 and is_polar_question_v1, to conform to the new generalizations (1') and (2'). This part will require some time and practice before we can feel very confident and comfortable, so be patient! Below I\u2019ll highlight a few core things that characterize each of the v2 functions:", "In is_polar_question_v2, we need to account for two situations:", "Finally, to put all of the above together, we can write a test function that takes a list of example sentences and prints the question type for each sentence.", "When I run get_question_type(sentences), I get the following output:", "Everything is correct except for the third to last sentence, \u201cWhatever you want.\u201d. With the v2 functions, we have achieved an accuracy of 18/19 = 94.7%. Not bad! Note that these rules are truly sensitive to the underlying syntactic structure of each sentence and not to some superficial properties like the presence of a question mark (as illustrated by the correct classification of the example \u201cWhose child is that.\u201d).", "As a final step, you will want to fine-tune your rules by addressing any false positives to increase precision (e.g., \u201cWhatever you want.\u201d was incorrectly classified as a wh-question) and testing with more examples (and more complex examples) to increase recall. When you are satisfied, apply the rules to new data and use the results at any stage of your workflow that you deem appropriate. For example, you may choose to incorporate them as features during feature engineering or use them as early as the data pipeline to generate noisy data for annotation.", "Linguistic rule writing for NLP/machine learning is a rich, iterative process that requires a deep understanding of language and the ability to codify that knowledge in the form of general heuristics in a programming language. While rules are probably not adequate to stand alone in most applications of NLP and good rule writing almost always requires expert knowledge, they have many advantages and are a great complement to ML algorithms. If I have convinced you at all of their value, please give spaCy a try and feel free to suggest a v3 function below or add a request for rules for other features. :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F64d9af824ee8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dzenilee?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dzenilee?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "Jenny Lee"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8231fa2cbef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&user=Jenny+Lee&userId=8231fa2cbef7&source=post_page-8231fa2cbef7----64d9af824ee8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F64d9af824ee8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F64d9af824ee8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.latimes.com/entertainment/la-et-toy-story-4-don-rickles-mr-potato-head-rex-20190619-story.html", "anchor_text": "article"}, {"url": "https://nlp.stanford.edu/projects/socialsent/", "anchor_text": "SocialSent"}, {"url": "https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c", "anchor_text": "these ones"}, {"url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197816", "anchor_text": "article"}, {"url": "https://www.pixar.com/feature-films/toy-story", "anchor_text": "https://www.pixar.com/feature-films/toy-story"}, {"url": "https://link.springer.com/referenceworkentry/10.1007%2F978-3-319-02240-6_7", "anchor_text": "how a child acquires language"}, {"url": "https://pdfs.semanticscholar.org/6e2f/9f203db02be6d7a90d7b6c6819c1c65ab6ae.pdf", "anchor_text": "this one"}, {"url": "https://www.newstalkflorida.com/featured/review-in-the-joyous-toy-story-4-the-toys-evolve-too/", "anchor_text": "review"}, {"url": "https://spacy.io/", "anchor_text": "spaCy"}, {"url": "https://spacy.io/usage/spacy-101#features", "anchor_text": "here"}, {"url": "https://spacy.io/usage/models", "anchor_text": "your choice"}, {"url": "https://explosion.ai/demos/displacy", "anchor_text": "displaCy"}, {"url": "https://en.wikipedia.org/wiki/Wh-movement#Pied-piping", "anchor_text": "pied-piped"}, {"url": "https://en.wikipedia.org/wiki/Preposition_stranding", "anchor_text": "\u201cstrands\u201d a preposition"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----64d9af824ee8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----64d9af824ee8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/linguistics?source=post_page-----64d9af824ee8---------------linguistics-----------------", "anchor_text": "Linguistics"}, {"url": "https://medium.com/tag/nlp?source=post_page-----64d9af824ee8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/spacy?source=post_page-----64d9af824ee8---------------spacy-----------------", "anchor_text": "Spacy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F64d9af824ee8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&user=Jenny+Lee&userId=8231fa2cbef7&source=-----64d9af824ee8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F64d9af824ee8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&user=Jenny+Lee&userId=8231fa2cbef7&source=-----64d9af824ee8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F64d9af824ee8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F64d9af824ee8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----64d9af824ee8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----64d9af824ee8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----64d9af824ee8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----64d9af824ee8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dzenilee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dzenilee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jenny Lee"}, {"url": "https://medium.com/@dzenilee/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "445 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8231fa2cbef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&user=Jenny+Lee&userId=8231fa2cbef7&source=post_page-8231fa2cbef7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F630d12d64891&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinguistic-rule-writing-for-nlp-ml-64d9af824ee8&newsletterV3=8231fa2cbef7&newsletterV3Id=630d12d64891&user=Jenny+Lee&userId=8231fa2cbef7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}