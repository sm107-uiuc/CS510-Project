{"url": "https://towardsdatascience.com/linear-regression-70cca59678ec", "time": 1682997422.828334, "path": "towardsdatascience.com/linear-regression-70cca59678ec/", "webpage": {"metadata": {"title": "Linear Regression. A simpler intuitive explanation. | by Abhishek Kumar | Towards Data Science", "h1": "Linear Regression", "description": "Linear Regression is a famous supervised learning algorithm used to predict a real-valued output. The model is a linear combination of the features of the input examples."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Linear Regression is a famous supervised learning algorithm used to predict a real-valued output. The linear regression model is a linear combination of the features of the input examples.", "As discussed in the definition, linear regression is a supervised learning algorithm, therefore, has a set of N labelled examples, represented as :", "Here, x_{i} represents a set of properties corresponding to the i_{^th} example. These set of properties are collectively called a feature vector. All the examples from i=1,2,3,\u2026,n have a corresponding real-valued y, which denotes a physical quantity such as cost, temperature, or any other continuous value.", "Now, as we have our examples ready we want to make our model f(x) that will help us to predict the output y for an unseen x.", "The job of the model is to predict a real-value y for an unseen value of the feature vector x. But, we want to find a model such that it does the best job in predicting the values of y, therefore, we want to find values of w and b such that the predictions are as close as possible to the actual answers. It is obvious that different values of w and b result in producing different models, of varying capabilities. Therefore, our job is to find the optimal set of values w* and b* which will minimize the error between the predictions made by the model f(x) and the actual results y for the training set.", "As discussed earlier we have N examples and a model f(x) for which we need to find the optimal values of w and b. Let us use all these N examples for finding the optimal values of w and b, popularly called as training our model. We need to find values of w and b such that the following expression is minimum.", "This is our objective function as we are going to minimize it. Learning algorithms have functions which we try to minimize or maximise. These functions are called as loss function or the cost function. This particular form is called the mean squared error loss function.", "If you observe the loss function:", "To understand this better let us assume John, recently appeared for an examination having 10 mathematical questions and the answer key has been published. Now John decides to find out how well has he performed? so, he compares his answer, f(x)_{i} with the corresponding answer y_{i} on the answer key. If the difference between John\u2019s answer and the actual answer f(x)_{i}-y_{i} is 0 he answered that question correctly. If he answered all the questions correctly then the average will also be 0 which corresponds to the best performance, implying the best model. Squaring the error helps to accentuate the error of the model. We could have also taken a cube or higher power but then the derivatives would have been more difficult to work out. We worry about the derivatives of the cost function as setting them to zero gives the optimal value w* and b* for the model.", "Let us discuss a few questions that perplexed me while studying about linear regression. But, before we start let\u2019s take a look at a very primitive example of linear regression.", "So, John and his friends decided to start studying linear regression from scratch so they began by collecting examples themselves. The examples they collected are shown below.", "After having collected the data, John decides to fit a linear regression model to it.", "This is a model of form f(x)=wx+b where w is a scalar, as x, the feature vector is one dimensional. A better comprehension of this model is to compare this to the equation of a straight line y=mx+c where m is analogous to w and c to b. This is a linear model.", "But, can we do better? Can we come up with a model that performs better than the current one? Yes, we can. It is a common confusion that linear regression only comprises of models that are straight lines. However, we can also fit curves to our data by transforming the data. Let\u2019s transform our feature vector by squaring each x_{i} value.", "After having transformed our feature vector let us try to fit a model on the new feature vector x\u00b2 and the output y (original feature vector x is not considered for training the model instead, it\u2019s transformation x_{^ 2} has been used to train the model).", "So, now we have predicted a polynomial model that is better than the linear one by transforming the original feature vector x_{i} to its square. The new model corresponds to f(x)=wx\u00b2+b.", "The capability of the model to predict better results has increased by transforming the feature vectors but we need to be aware of over fitting. Over fitting happens when the model predicts too well during the training phase but makes an error while predicting unseen examples. Over fitting does not reflect the real-world scenario of being dynamic. It does not produce generalised models.", "Let\u2019s say that the feature vector is R-dimensional. We have seen the case where R=1 and also predicted a linear and a polynomial model. If R=2 a plane is predicted as the model. Generally linear regression models a hyper plane for a data set with R-dimensional feature vector, x and 1-dimensional output, y.", "Hyper plane is a subspace with one less dimension than that of its surrounding space. In case of a 1 dimensional line the point is a hyper plane, in case of a 2 dimensional region a line is a hyper plane, in case of a 3 dimensional space the plane is a hyper plane and so on.", "Let\u2019s discuss the utility of the bias term. Consider the equation of a straight line y=mx. In this case m controls the slope of the line and can rotate the line anywhere but only about the origin.", "Suppose you decide to use this model for a trivial linear regression problem. However, any hypothesis that you generate will always pass through the origin and might fail to generalise. Adding the bias term will result in the hypothesis y=mx+c thereby, allowing you to move your line anywhere in the plane. The bias term helps in generalising the hypothesis.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I enjoy to read, write, develop, and listen to music."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F70cca59678ec&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----70cca59678ec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70cca59678ec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://abhieshekumar.medium.com/?source=post_page-----70cca59678ec--------------------------------", "anchor_text": ""}, {"url": "https://abhieshekumar.medium.com/?source=post_page-----70cca59678ec--------------------------------", "anchor_text": "Abhishek Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1245ce379ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&user=Abhishek+Kumar&userId=e1245ce379ff&source=post_page-e1245ce379ff----70cca59678ec---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70cca59678ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70cca59678ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----70cca59678ec---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----70cca59678ec---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/supervised-learning?source=post_page-----70cca59678ec---------------supervised_learning-----------------", "anchor_text": "Supervised Learning"}, {"url": "https://creativecommons.org/publicdomain/mark/1.0/", "anchor_text": "Public domain."}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70cca59678ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&user=Abhishek+Kumar&userId=e1245ce379ff&source=-----70cca59678ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70cca59678ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&user=Abhishek+Kumar&userId=e1245ce379ff&source=-----70cca59678ec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70cca59678ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70cca59678ec--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F70cca59678ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----70cca59678ec---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----70cca59678ec--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----70cca59678ec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----70cca59678ec--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----70cca59678ec--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----70cca59678ec--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----70cca59678ec--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----70cca59678ec--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----70cca59678ec--------------------------------", "anchor_text": ""}, {"url": "https://abhieshekumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://abhieshekumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhishek Kumar"}, {"url": "https://abhieshekumar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "143 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1245ce379ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&user=Abhishek+Kumar&userId=e1245ce379ff&source=post_page-e1245ce379ff--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F919dadc87f64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-70cca59678ec&newsletterV3=e1245ce379ff&newsletterV3Id=919dadc87f64&user=Abhishek+Kumar&userId=e1245ce379ff&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}