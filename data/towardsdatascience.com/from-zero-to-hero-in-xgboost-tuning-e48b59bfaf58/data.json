{"url": "https://towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58", "time": 1682996329.014849, "path": "towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58/", "webpage": {"metadata": {"title": "From Zero to Hero in XGBoost Tuning | by Florencia Leoni | Towards Data Science", "h1": "From Zero to Hero in XGBoost Tuning", "description": "XGBoost or Extreme Gradient Boosting is an optimized implementation of the Gradient Boosting algorithm. Since its introduction in 2014 XGBoost has been the darling of machine learning hackathons and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 0}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem", "anchor_text": "Condorcet\u2019s Jury Theorem", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "A Scalable Tree Boosting System", "paragraph_index": 4}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 4}, {"url": "https://stackoverflow.com/questions/473137/a-simple-example-of-a-cache-aware-algorithm", "anchor_text": "cache-aware", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d", "anchor_text": "systems optimization and algorithmic enhancements", "paragraph_index": 4}, {"url": "http://blog.kaggle.com/2015/08/26/avito-winners-interview-1st-place-owen-zhang/", "anchor_text": "Words of Wisdom", "paragraph_index": 5}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 6}, {"url": "https://www.kaggle.com/zalando-research/fashionmnist", "anchor_text": "Fashion MNIST", "paragraph_index": 6}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn\u2019s", "paragraph_index": 9}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html", "anchor_text": "StandardScaler", "paragraph_index": 9}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "XGBoostClassifier", "paragraph_index": 10}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "XGBoostClassifier", "paragraph_index": 10}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 15}, {"url": "https://gabrieltseng.github.io/2018/02/25/XGB.html", "anchor_text": "prevent overfitting", "paragraph_index": 15}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn\u2019s", "paragraph_index": 17}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV", "paragraph_index": 17}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "GridSearchCV", "paragraph_index": 17}, {"url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html", "anchor_text": "lower run-time and better performance", "paragraph_index": 17}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV", "paragraph_index": 17}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "XGBClassifier", "paragraph_index": 18}, {"url": "https://github.com/dmlc/xgboost/issues/4287#issuecomment-494055233", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV", "paragraph_index": 20}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV", "paragraph_index": 21}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV", "paragraph_index": 25}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn", "paragraph_index": 25}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "wrapper interface", "paragraph_index": 25}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 25}], "all_paragraphs": ["XGBoost or Extreme Gradient Boosting is an optimized implementation of the Gradient Boosting algorithm. Since its introduction in 2014 XGBoost has been the darling of machine learning hackathons and competitions because of its prediction performance and processing time.", "Boosting is a type of Ensemble technique. Ensemble Learning borrows from Condorcet\u2019s Jury Theorem the idea of the wisdom of crowds. Thus Ensemble techniques combine the results of different models to improve the overall results and performance", "A majority vote classifies correctly with higher probability than any person (model), and as the number of people (models) becomes large, the accuracy of the majority vote approaches 100% \u2014 Scott Page, The Model Thinker.", "In decision-tree based machine learning, Boosting algorithms implement a sequential process where each model attempts to correct the mistakes of the previous models. The idea is to convert many weak learners into one strong learner. Gradient Boosting employs the gradient descent algorithm to minimize errors in sequential models. The major inefficiency in Gradient Boosting is that it creates one decision tree at a time.", "To overcome this, Tianqi Chen and Carlos Guestrin built A Scalable Tree Boosting System \u2014 XGBoost can be thought of as Gradient Boosting on steroids. It features parallelized tree building, cache-aware access, sparsity awareness, regularization, and weighted quantile sketch as some of its systems optimization and algorithmic enhancements.", "When in doubt, use XGBoost \u2014 Words of Wisdom by Owen Zhang.", "To walk you through XGBoost and its hyperparameters, we\u2019ll build a simple classification model using the Fashion MNIST dataset.", "The Fashion MNIST dataset consists of a training set of 60,000 28x28 greyscale images associated with 10 classification labels, and a test set of 10,000 images. The labels are the following:", "Let\u2019s take a look at the first and last image.", "Each instance (image) in the dataset has 784 features (one for each pixel) and the value in each feature ranges from 0 to 255, hence we\u2019ll use Scikit-Learn\u2019s StandardScaler to rescale the values to a smaller range with mean zero and unit variance.", "We are going to use an XGBoostClassifier to predict the label of a given image. Building an XGBoostClassifier is pretty straightforward. We\u2019ll start with a simple baseline model and move on from there.", "For this first iteration, we\u2019ll only specify one hyperparameter: objective and set it to \u201cmulti:softmax\u201d. The objective is part of the Learning Task hyperparameters, and it specifies the learning task (regression, classification, ranking, etc) and function to be used.", "Let\u2019s backtrack for a second. What are hyperparameters? \u2014 They are the parameters that are initialized before training a model because they cannot be learned from the algorithm. They control the behavior of the training algorithm and have a high impact on the performance of a model. The typical metaphor goes like this: hyperparameters are the knobs one turns to tweak a machine learning model. They are essential to optimization and to improve evaluation metrics.", "And now, let\u2019s take a look at our model and the results.", "Not too shabby! But we can try to beat these first scores with some tweaking and knob-turning.", "One of XGBoost\u2019s strongest advantages is the degree of customization available, i.e. an intimidatingly long list of hyperparameters you can tune, which were designed mostly to prevent overfitting.", "The million dollar question, in this case, is what to tune and how? There are no benchmarks as to what the ideal hyperparameters are since these will depend on your specific problem, your data, and what you\u2019re optimizing for. But once you understand the concepts of even the most obscure hyperparameters you\u2019ll be well on your way to tuning like a boss!", "To find our best hyperparameters we can use Scikit-Learn\u2019s RandomizedSearchCV or GridSearchCV. The difference between the two is a trade-off between lower run-time and better performance, respectively. Taking the size of the dataset into consideration RandomizedSearchCV is the way to go this time around.", "We start by creating an XGBClassifier object, just as with the baseline model, we are passing the hyperparameters tree_method, predictor, verbosity and eval_metric, in addition to the objective, directly rather than through the parameter grid. The first two allow us to access GPU capabilities directly, verbosity gives us visibility as to what the model is running in real-time, and eval_metric is the evaluation metric to be used for validation data \u2014 as you can see, you can pass multiple evaluation metrics in the form of a Python list.", "Unfortunately, using tree_method and predictor we kept getting the same error over and over. You can track the status of this error here.", "Given that we couldn\u2019t fix the root issue (pun intended), the model had to be run on CPU. So the final code for our RandomizedSearchCV looked like this:", "RandomizedSearchCV allows us to find the best combination of hyperparameters from the options given of the parameter grid. We can then access these through model_xgboost.best_estimator_.get_params() so we can use them on the next iteration of the model. Below are the best estimators for this model.", "Our accuracy score went up by 2%! Not bad for a model that was running for over 60 hours!", "Now, let\u2019s take a look at each hyperparameter individually.", "But wait! There\u2019s more. There are other hyperparameters you can modify, depending on the problem you are trying to solve or what you\u2019re trying to optimize for:", "Note: we performed RandomizedSearchCV using the Scikit-Learn wrapper interface for XGBoost.", "Once you have your model\u2019s best estimators, you could do a number of different things. My recommendation would be to use them as hyperparameters on XGBoost\u2019s built-in Cross Validation so you can make use of the early_stopping_rounds functionality \u2014 another step towards optimization and overfit prevention. But that\u2019s a story for another day!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Lawyer and Economist exploring data science. Passionately curious, avid notetaker, and impulsive book buyer. Living for the love of knowledge. Just Add Data!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe48b59bfaf58&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@florenleoni?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@florenleoni?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "Florencia Leoni"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa72195637ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&user=Florencia+Leoni&userId=a72195637ec6&source=post_page-a72195637ec6----e48b59bfaf58---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe48b59bfaf58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe48b59bfaf58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://cican17.com/an-overview-of-boosted-trees/", "anchor_text": "Chen Shikun"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem", "anchor_text": "Condorcet\u2019s Jury Theorem"}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "A Scalable Tree Boosting System"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://stackoverflow.com/questions/473137/a-simple-example-of-a-cache-aware-algorithm", "anchor_text": "cache-aware"}, {"url": "https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d", "anchor_text": "systems optimization and algorithmic enhancements"}, {"url": "http://blog.kaggle.com/2015/08/26/avito-winners-interview-1st-place-owen-zhang/", "anchor_text": "Words of Wisdom"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://www.kaggle.com/zalando-research/fashionmnist", "anchor_text": "Fashion MNIST"}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn\u2019s"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html", "anchor_text": "StandardScaler"}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "XGBoostClassifier"}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "XGBoostClassifier"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://gabrieltseng.github.io/2018/02/25/XGB.html", "anchor_text": "prevent overfitting"}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn\u2019s"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "GridSearchCV"}, {"url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html", "anchor_text": "lower run-time and better performance"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV"}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "XGBClassifier"}, {"url": "https://github.com/dmlc/xgboost/issues/4287#issuecomment-494055233", "anchor_text": "here"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV"}, {"url": "https://stats.stackexchange.com/questions/354484/why-does-xgboost-have-a-learning-rate", "anchor_text": "lessen the effect of each additional tree"}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "A Scalable Tree Boosting System"}, {"url": "https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f", "anchor_text": "robust the model will be in preventing overfitting"}, {"url": "https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6", "anchor_text": "Lagrangian Multiplier,"}, {"url": "https://gabrieltseng.github.io/appendix/2018-02-25-XGB.html", "anchor_text": "by how much the loss has to be reduced when considering a split"}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "A Scalable Tree Boosting System"}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "A Scalable Tree Boosting System"}, {"url": "https://medium.com/@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5", "anchor_text": "useful in tree-models"}, {"url": "https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance", "anchor_text": "feature_importances_"}, {"url": "https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance", "anchor_text": "method"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV"}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn"}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "wrapper interface"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e48b59bfaf58---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e48b59bfaf58---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/classification?source=post_page-----e48b59bfaf58---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----e48b59bfaf58---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/python?source=post_page-----e48b59bfaf58---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe48b59bfaf58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&user=Florencia+Leoni&userId=a72195637ec6&source=-----e48b59bfaf58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe48b59bfaf58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&user=Florencia+Leoni&userId=a72195637ec6&source=-----e48b59bfaf58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe48b59bfaf58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe48b59bfaf58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e48b59bfaf58---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e48b59bfaf58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@florenleoni?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@florenleoni?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Florencia Leoni"}, {"url": "https://medium.com/@florenleoni/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "247 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa72195637ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&user=Florencia+Leoni&userId=a72195637ec6&source=post_page-a72195637ec6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F659d9cdd1235&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-zero-to-hero-in-xgboost-tuning-e48b59bfaf58&newsletterV3=a72195637ec6&newsletterV3Id=659d9cdd1235&user=Florencia+Leoni&userId=a72195637ec6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}