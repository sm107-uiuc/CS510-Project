{"url": "https://towardsdatascience.com/strategies-for-model-debugging-aa822f1097ce", "time": 1683001244.526616, "path": "towardsdatascience.com/strategies-for-model-debugging-aa822f1097ce/", "webpage": {"metadata": {"title": "Real-World Strategies for Model Debugging | by Patrick Hall | Towards Data Science", "h1": "Real-World Strategies for Model Debugging", "description": "So, you\u2019ve trained a machine learning (ML) model. You did everything right: cross-validation, early stopping, grid search, monotonicity constraints, and regularization. It\u2019s accurate and stable in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pair-code.github.io/what-if-tool/index.html", "anchor_text": "What-If Tool", "paragraph_index": 9}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 11}, {"url": "https://github.com/h2oai/h2o-3", "anchor_text": "H2O", "paragraph_index": 11}, {"url": "https://github.com/microsoft/LightGBM", "anchor_text": "LightGBM", "paragraph_index": 11}, {"url": "https://cran.r-project.org/web/packages/ALEPlot/index.html", "anchor_text": "ALEPlot", "paragraph_index": 13}, {"url": "https://cran.r-project.org/web/packages/DALEX/index.html", "anchor_text": "DALEX", "paragraph_index": 13}, {"url": "https://cran.r-project.org/web/packages/iml/index.html", "anchor_text": "iml", "paragraph_index": 13}, {"url": "https://github.com/SauceCat/PDPbox", "anchor_text": "PDPbox", "paragraph_index": 14}, {"url": "https://github.com/AustinRochford/PyCEbox", "anchor_text": "PyCEbox", "paragraph_index": 14}, {"url": "https://cran.r-project.org/web/packages/ICEbox/index.html", "anchor_text": "ICEbox", "paragraph_index": 14}, {"url": "https://bgreenwell.github.io/pdp/index.html", "anchor_text": "pdp", "paragraph_index": 14}, {"url": "https://github.com/tensorflow/cleverhans", "anchor_text": "cleverhans", "paragraph_index": 17}, {"url": "https://github.com/bethgelab/foolbox", "anchor_text": "foolbox", "paragraph_index": 17}, {"url": "https://cran.r-project.org/web/packages/DALEX/index.html", "anchor_text": "DALEX", "paragraph_index": 26}, {"url": "https://github.com/cosmicBboy/themis-ml", "anchor_text": "themis-ml", "paragraph_index": 26}, {"url": "https://github.com/ModelOriented/auditor", "anchor_text": "auditor", "paragraph_index": 26}, {"url": "https://github.com/dssg/aequitas", "anchor_text": "aequitas", "paragraph_index": 27}, {"url": "http://aif360.mybluemix.net", "anchor_text": "AIF360", "paragraph_index": 27}, {"url": "https://github.com/LASER-UMASS/Themis", "anchor_text": "Themis", "paragraph_index": 27}, {"url": "https://github.com/slundberg/shap", "anchor_text": "shap", "paragraph_index": 30}, {"url": "https://github.com/microsoft/interpret", "anchor_text": "GA2M", "paragraph_index": 40}, {"url": "http://aif360.mybluemix.net", "anchor_text": "AIF360", "paragraph_index": 45}, {"url": "https://github.com/cosmicBboy/themis-ml", "anchor_text": "themis-ml", "paragraph_index": 45}, {"url": "https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html", "anchor_text": "When a Computer Keeps You in Jail", "paragraph_index": 54}, {"url": "https://developers.google.com/machine-learning/testing-debugging", "anchor_text": "Testing and Debugging Machine Learning Models", "paragraph_index": 55}, {"url": "https://ai.google/research/pubs/pub43146", "anchor_text": "Machine Learning: The High Interest Credit Card of Technical Debt", "paragraph_index": 55}, {"url": "https://github.com/tensorflow/model-analysis", "anchor_text": "Model analysis tools for TensorFlow", "paragraph_index": 55}, {"url": "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients", "anchor_text": "Default of credit card clients dataset", "paragraph_index": 56}, {"url": "http://www.vias.org/tmdatanaleng/cc_ann_extrapolation.html", "anchor_text": "Teach me Data Analysis", "paragraph_index": 57}, {"url": "https://christophm.github.io/interpretable-ml-book/shapley.html", "anchor_text": "Interpretable Machine Learning", "paragraph_index": 58}, {"url": "http://www.cs.yale.edu/homes/jf/Dwork.pdf", "anchor_text": "Fairness Through Awareness", "paragraph_index": 59}, {"url": "https://scholar.google.com/scholar?cites=15887350027958465759&as_sdt=20005&sciodt=0,9&hl=en", "anchor_text": "related work", "paragraph_index": 59}, {"url": "https://www.fatml.org/", "anchor_text": "Fairness, Accountability, and Transparency in Machine Learning", "paragraph_index": 60}, {"url": "https://www.oreilly.com/ideas/proposals-for-model-vulnerability-and-security", "anchor_text": "Proposals for Model Vulnerability and Security", "paragraph_index": 61}, {"url": "https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf", "anchor_text": "Warning Signs: The Future of Security and Privacy in an Age of Machine Learning", "paragraph_index": 62}, {"url": "https://machinelearningmastery.com/controlled-experiments-in-machine-learning/", "anchor_text": "Controlled Experiments in Machine Learning", "paragraph_index": 63}, {"url": "http://gendershades.org/", "anchor_text": "How well do IBM, Microsoft, and Face++ AI services guess the gender of a face?", "paragraph_index": 64}, {"url": "https://cs.stanford.edu/~matei/papers/2018/mlsys_model_assertions.pdf", "anchor_text": "Model Assertions for Debugging Machine Learning", "paragraph_index": 65}, {"url": "https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/", "anchor_text": "How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did", "paragraph_index": 66}, {"url": "https://www.youtube.com/watch?v=rToFuhI6Nlw", "anchor_text": "Responsible Data Science: Identifying and Fixing Biased AI", "paragraph_index": 67}, {"url": "https://www.nature.com/articles/d41586-019-03228-6", "anchor_text": "Millions of Black People Affected by Racial Bias in Health-care Algorithms", "paragraph_index": 68}, {"url": "https://github.com/jphall663/interpretable_machine_learning_with_python", "anchor_text": "Interpretable Machine Learning with Python", "paragraph_index": 69}, {"url": "http://bnh.ai", "anchor_text": "bnh.ai", "paragraph_index": 71}], "all_paragraphs": ["So, you\u2019ve trained a machine learning (ML) model. You did everything right: cross-validation, early stopping, grid search, monotonicity constraints, and regularization. It\u2019s accurate and stable in out-of-time test data, and better than the linear model it\u2019s replacing. You even wrapped your model in a Docker container with all its dependencies for your data engineering and information technology (IT) pals. Time to deploy? Not so fast. Current best practices for ML model training and assessment just don\u2019t tell us how to find and fix all the nasty things that can blow up in a real-world ML system. If only these systems could be tested and debugged like regular applications \u2026 enter: model debugging. Model debugging is an emergent discipline focused on finding and fixing problems in ML systems. Model debugging attempts to test ML models like code (because they are usually code) and to probe complex ML response functions and decision boundaries to systematically detect and correct accuracy, fairness, and security problems in ML systems.", "In ML, trust and understanding are similar but not the same. Unpacking that difference helps me think through model debugging and how it relates to other parts of a ML workflow. As shown in Figure 1, there are lots of tools available today to promote human trust and understanding of ML. Some techniques, such as model debugging and social bias testing and remediation, help us make ML more accurate, fair, and secure without telling us very much about how the model works. These help us trust a model, more than they help us understand it. Other techniques, like interpretable ML models and post-hoc explanations, increase our understanding directly by revealing model mechanisms or by summarizing model decisions. These techniques only increase trust as a side-effect, when we like the model or explanations.", "This post will focus on mathy aspects of debugging ML and promoting trust in ML. However, it\u2019s crucial to consider these two additional aspects of a responsible ML workflow:", "Even if you value trust, and in particular accuracy, more than fairness, interpretability, or security, making ML systems understandable is needed to enable logical appeal or operator override of wrong ML decisions. It\u2019s very hard to argue against a black-box.[1] You won\u2019t like it when your children are kept out of their preferred college by an accurate, but inscrutable, ML system. Nor would you enjoy being denied healthcare by the same kind of black-box system. You\u2019d like the opportunity to appeal wrong ML decisions that affect your life, and that\u2019s why business- or life-critical ML systems should always be understandable. Figure 2 proposes a workflow that includes steps for increasing both trust and understanding in ML systems. Model debugging likely works best when used along with the other techniques proposed in Figure 2.", "Many of you likely know more than me about testing and hardening systems that surface ML models to consumers. I won\u2019t say much more than that kind of work is still necessary. Just because a system houses a ML model doesn\u2019t exempt it from testing. Moreover, Google (and probably others) have already put forward some workable thoughts and frameworks on the subject.[2]", "Now, let\u2019s get into how to debug the guts of ML models. First we\u2019ll discuss the example problem and dataset used in this post, then how to detect bugs, and finally, how to squash them.", "Some examples below are based on the well-known Taiwanese credit card dataset from the University of California Irvine (UCI) ML repository.[3] In this dataset, we try to predict if someone will pay, DEFAULT_NEXT_MONTH = 0, or default, DEFAULT_NEXT_MONTH = 1, on their next credit card payment. Variables about payments are used to generate probabilities of default, or p_DEFAULT_NEXT_MONTH. I\u2019ll use one of my favorite types of models, an interpretable monotonically constrained gradient boosting machine (M-GBM), to make these predictions. In the M-GBM, p_DEFAULT_NEXT_MONTH must only increase, or only decrease, as a certain input variable increases. This makes explaining and debugging the model much, much easier and does not really affect the overall accuracy of the model for this dataset. The M-GBM is trained on payment variables including PAY_0 \u2014 PAY_6, or a customer\u2019s most recent through six months prior repayment statuses (higher values are late payments), PAY_AMT1 \u2014 PAY_AMT6, or a customer\u2019s most recent through six months prior payment amounts, and BILL_AMT1 \u2014 BILL_AMT6, or a customer\u2019s most recent through six months prior bill amounts. All monetary values are reported in Taiwanese Dollars (NT$).", "Some of the example results also contain the variables LIMIT_BAL and r_DEFAULT_NEXT_MONTH. LIMIT_BAL is a customer\u2019s credit limit. r_DEFAULT_NEXT_MONTH is a logloss residual value, or a numeric measure of how far off the M-GBM prediction, p_DEFAULT_NEXT_MONTH, is from the known correct answer, DEFAULT_NEXT_MONTH. I\u2019ll also use demographic variables in the dataset, like SEX, to test for unwanted sociological bias. For the most part, this post treats the example credit lending problem as a general predictive modeling exercise, and does not consider applicable regulations. (Of course, noncompliance penalties probably make model debugging more commercially appealing.)", "How do we find math bugs in ML models? I can think of at least four major ways: sensitivity analysis, residual analysis, benchmark models, and ML security audits. You can probably think of others too.", "Sensitivity analysis, sometimes called \u201cWhat-if\u201d analysis, is a simple and powerful idea. Just simulate data that represents interesting scenarios, then see what kind of predictions your model makes in those scenarios. Because it\u2019s almost impossible to know how a nonlinear ML model will react to data it did not see during training, it\u2019s really important to conduct sensitivity analysis on all of our important ML models.[4] You might have some ideas about what kinds of scenarios to test. If so, that\u2019s great and just do it. Maybe you just want to play with your model too? Also great. For those cases, definitely checkout the What-If Tool, which provides an interactive sandbox experience for certain types of ML models.", "It can also help to have a more structured approach to sensitivity analysis. This subsection will present three strategies for structured sensitivity analysis:", "Before we jump into those, it\u2019s good to know which variables are most important to your model. I always focus my testing efforts on those variables first. Figure 3 is an accurate and consistent variable importance plot calculated with Shapley values using XGBoost.[5] (Shapley variable importance measurements are also available natively in H2O and LightGBM.) Figure 3 shows us that PAY_0 is very important, and probably too important as we will see later.", "Partial dependence sets all the values of a column of interest, say PAY_0, in a dataset of interest, say the example validation dataset, to a value of interest, say missing or NaN, or any other reasonable value. This new dataset is run back through the trained model, creating a prediction for every row. Taking the average of all those predictions is the estimated partial dependence for that variable, for that dataset, for that value, and for that model. Now, we repeat that process over several different values that we\u2019re interested in to create a partial dependence curve to plot. In Figure 4, that plot shows us the rough average behavior of PAY_0 in the M-GBM model.", "Though it\u2019s pretty easy to understand, partial dependence is far from perfect. It\u2019s known to show untrustworthy results when there are strong interactions or correlations in a dataset. We have at least two options to improve partial dependence, ALE and ICE. Sounds fun right? ALE is almost a direct replacement for partial dependence. It\u2019s more efficient to calculate and typically more accurate. ALE is available in R packages like: ALEPlot, DALEX, and iml.", "ICE is often used along with partial dependence. ICE is a very similar calculation to partial dependence. Follow the same steps as described above, except the dataset of interest is just one row. That, and we usually calculate ICE curves for many different rows in a dataset. When the ICE curves, representing many different real or simulated individuals, follow along with the overall average behavior represented by partial dependence, this is a good sign that partial dependence is accurate enough. When ICE curves diverge from partial dependence, this potentially indicates the presence of interactions in the model (that are likely averaged-out of the partial dependence). In short, ICE can tell us about the behavior of interesting real or simulated individuals under our model, if partial dependence is trustworthy, and if we should be on the lookout for any strong interactions in our model. Combinations and variants of partial dependence and ICE are available in several of open source packages like PDPbox, PyCEbox, ICEbox, and pdp.", "Figure 4 combines partial dependence, ICE, and a histogram to provide a lot of insight about the most important variable, PAY_0, in the M-GBM model. First we can see the training data is really sparse for PAY_0 > 1. This is usually a bad sign. ML models need lots of data to learn. This model has almost no data to learn about people who are more than 1 month late on their most recent repayment. Looking at partial dependence we can see a few other potential problems. The model gives the lowest average probability of default for missing, i.e. NaN, data. Does this make business sense? I don\u2019t see how. Also, it\u2019s kind of scary from a security perspective. If I want a good score from this model, I might only need to hack an adversarial example with a missing value for PAY_0 into the M-GBM\u2019s production scoring queue. Also, we can see a huge swing in predictions from PAY_0 = 1 to PAY_0 = 2. Does this make sense from a business perspective? Maybe so and that\u2019s fine, but it\u2019s another thing to be aware of from a security and data drift perspective. If I want to conduct a denial of service attack for a customer of this model, I just need to change their PAY_0 value to larger than 1. For this model, we might want to let our IT pals know to monitor for adversarial example attacks involving PAY_0 = NaN and PAY_0 > 1. Also, if our market changes toward recession conditions, and customers have more late bills, it\u2019s important to remember how sensitive this M-GBM is to values of PAY_0 > 1.", "On the more reassuring side, the partial dependence and ICE curves show the monotonicity constraint for PAY_0 held on average and for many individuals. The monotonicity constraints luckily helped us deal with the data sparsity issue too. Due to the monotonicity constraints, the model held the probability of default from PAY_0 = 2, where there is some training data, all the way out to PAY_0 > 8, where there is no training data. Without the constraint, model predictions in this domain of PAY_0 would probably just be random noise. Finally, because ICE and partial dependence are mostly aligned, we can see that the partial dependence curve for PAY_0, this dataset, and the M-GBM model is probably pretty trustworthy. Now, you just need to perform this analysis for the rest of your most important variables. For brevity\u2019s sake, I\u2019ll move onto the next sensitivity analysis debugging strategy, adversarial example searches.", "Adversarial examples are rows of data that make a model produce unexpected results. Looking for adversarial examples is a great debugging technique. The search process allows us to see how our model performs in many different scenarios. Finding and understanding actual adversarial examples can direct us toward ways to make our models more robust and tell us about anomalies to look for when the model moves into a production setting. If you\u2019re working in the Python deep learning space, look into cleverhans and foolbox for finding adversarial examples. For structured data, there\u2019s less freely available software to help us out, but I have a little heuristic search method you can try or modify. Figure 5 shows the results of that heuristic search.", "For the M-GBM model and data, the heuristic search starts with PAY_0, calculating ICE like I did in Figure 4, and finding the ICE curve with the largest swing in predictions. For this dataset and model, that is the ICE curve at the 90th percentile of p_DEFAULT_NEXT_MONTH. Then the row of data at the 90th percentile of p_DEFAULT_NEXT_MONTH is perturbed 10,000 times, through ten different values for four important variables: PAY_0, PAY_3, PAY_AMT1, and PAY_AMT2. (These variables were selected due to their wide range of Shapley values, not directly from the variable importance plot in Figure 3.) This dataset of 10,000 perturbed instances of the row at the 90th percentile of p_DEFAULT_NEXT_MONTH was then rescored by the M-GBM model, and the results are plotted in Figure 5.", "Figure 5 shows us several interesting things about the M-GBM model. First, we can see the monotonicity constraints held even for combinations of several variables. Second, we may have discovered a logical flaw in the M-GBM model. It appears that no matter how large someone\u2019s most recent or second most recent payments are, the model will generate high probabilities of default if their most recent payment is more than one month late. This means the M-GBM model may not be able to account for prepayment, or for someone making large payments to make up for late payments, without issuing default predictions. If I did want to account for these conditions in my M-GBM model or in a ML-based credit lending system based on the M-GBM model, I could consider editing the M-GBM model or using model assertions to enable the credit lending system to handle these more complex scenarios. (See Subsections 3.3 and 3.4.) Third, this search did turn up at least one adversarial example. Extremely low values of PAY_AMT1 and PAY_AMT2, when combined with other values in the row used to initialize the search, will cause the M-GBM model to generate surprisingly high probabilities of default. These values are something for which to monitor when the M-GBM model is moved into production. They could indicate the model is under adversarial attack.", "If you think the proposed adversarial search method seems useful, give it a try. The heuristic search process can be summarized as follows.", "5. Plot and analyze the results.", "Random attacks are conducted by exposing a model to all kinds of random data. Think: double-byte character sets, datasets with one row and one column, datasets with 10 million columns and one row, etc. etc. etc. Random attacks can find both conventional IT bugs and math bugs. Consider that upon being exposed to a dataset with 10 million columns and 1 row, your API misbehaves by coughing up a stacktrace with too much private or internal information. Or maybe it just fails in a really ugly, service-crashing way? Maybe your API and model treat double-byte characters like missing values, and always assign records containing them a low probability of default? Who knows? Not you \u2026 if you don\u2019t test for this kind of stuff. Also, if you\u2019re totally clueless about where to start with your model debugging efforts, start with a random attack. I bet you will find some very interesting things.", "Residual analysis has long been a cornerstone of linear model diagnostics, and that should remain true in an age of ML. Residuals refer to the difference between the known true outcome and what a model predicted that outcome to be. There are lots of ways to calculate residuals, but usually a large residual value means a model was wrong, and small residual value means a model was right. Residual plots put all your input data and predictions into a two-dimensional visualization where influential outliers and other types of math bugs can become plainly visible. The only draw-back of residual analysis is that to calculate residuals, we need true outcomes. So sometimes we can\u2019t work with residuals in real-time if we\u2019re making predictions where the true outcome won\u2019t be available for a period of time. (Say in mortgage lending.)", "Figure 6 displays the logloss residuals of the M-GBM model plotted by the levels of the important variable, PAY_0. Magenta residuals are from customers who actually defaulted. Blue residuals are from customers who did not default. Sadly, Figure 6 paints a damning portrait of my M-GBM model.", "In Figure 6, we can see that for desirable values of PAY_0 < 1, i.e. NO CONSUMPTION (-2), PAID DULY (-1), or USE OF REVOLVING CREDIT (0), there are a lot of large magenta residuals. This means that the model basically fails to predict default when someone made their most recent payment. For PAY_0 > 1, model errors are driven by the large blue residuals, meaning that when a customer has an undesirable value for PAY_0, i.e. several months late, the M-GBM model cannot predict on-time payment. Combining this information with the variable importance plot in Figure 3 shows that the M-GBM is pathologically over-dependent on PAY_0. I could likely deploy a single business rule: IF PAY_0 > 1 THEN DEFAULT_NEXT_MONTH = 1, and have the same accuracy as the M-GBM, or almost any GBM. That way I would only be unleashing one brittle rule into the wild, instead of thousands or millions of broken, potentially biased and hackable rules.", "We can try to fix this dangerous bug with data augmentation, strong regularization, model editing, or model assertions, all of which are discussed in the remediation section of this post. But one thing is clear, this model is broken, untrustworthy, and unfit for use in the real-world. This problem does not show up in fit statistics, lift, or AUC plots, and I never would have seen it so clearly it without looking at the residuals. In fact, it can be shocking what residual plots show us about seemingly healthy models.[6] Hopefully I\u2019ve convinced you plotting residuals is a high-impact debugging technique. It\u2019s usually easy to do yourself with any language you like and a decent plotting library, but packages like DALEX, themis-ml, and auditor provide this functionality right out-of-the-box.", "Disparate impact (DI) roughly refers to unintentional discrimination in decision-making systems. DI testing methods are a well-known way to find some types of unwanted sociological bias in training data and predictive modelling results. Are they perfect? No. Are they the least you can do to stop your ML model from perpetuating or exacerbating unwanted sociological bias? Probably. There are also several open source packages that can help you do your DI testing, including aequitas, AIF360, and Themis. Basic DI testing methods look at accuracy and error rates across demographic variables. Ideally we want these accuracy and error rates to be roughly equal across different demographic groups. If they\u2019re not, this a strong indication your model is perpetuating or exacerbating unwanted sociological bias. In Figure 7, we can see with respect to the variable SEX, accuracy and error rates look fairly similar for men and women. This is a good sign, but it does not mean our model is free of unwanted sociological bias, even with respect to SEX.", "All models have the ability to treat similar people differently based small changes in their input data, potentially resulting in instances of local bias, or a lack of individual fairness. An example of local bias could be granting a credit extension to a young woman with a good payment history and an income of NT$100,000, but denying credit to a very similar young woman with an income NT$99,999. We know that the NT$1 difference in income makes no real difference, but a ML model can arbitrarily place these two similar individuals on different sides of a nonlinear decision boundary. Worse still, standard DI testing usually can\u2019t catch local bias problems. How do we ensure fairness on an individual level? That\u2019s still a somewhat open question as of today, that many stellar researchers are trying to answer.[7] One practical suggestion I can give is to take a hard look at individuals closest to your model\u2019s decision boundary or probability cutoff. In most cases, very similar individuals should not be on different sides of that boundary. Now, before moving onto discuss disparate accuracy and error rates as a general bug detection tool, it\u2019s important to say fairness in ML goes far beyond the small discussion here. If you want to learn more about it, checkout the Fairness, Accountability, and Transparency in ML (FATML) conference and associated resources and try out some of those packages linked above. [8]", "Traditional DI testing methods can also be applied to categorical variables in general, and I find this to be an excellent bug detection method. Figure 7 displays numerous accuracy and error metrics across the different categorical levels of the important variable, PAY_0. Here we can see the dramatic difference between the M-GBM\u2019s performance for PAY > 1, likely driven by sparsity of training data in that domain. This table does a good job of presenting just how brittle the model performance is in this domain, and how different the model performance is for PAY > 1. This bug detection technique can also be applied to numeric variables, simply by binning them.", "The last few years have brought us numerous techniques to explain ML model predictions. These techniques can also be used to improve our residual analysis. It\u2019s possible to create local interpretable model-agnostic explanations (LIMEs), partial dependence, or individual conditional expectation plots of residuals. Recent additions to the shap package, make it possible to calculate Shapley contributions to residuals, meaning you can get an accurate picture of which variables are driving errors, both locally, i.e. for single rows, and globally, i.e. over an entire dataset. Another good option for explaining residuals is to fit a model to them. Figure 8 is a decision tree fit to the M-GBM residuals when DEFAULT_NEXT_MONTH = 1. Figure 8 shows a model of why the M-GBM model missed future defaults, that would likely cause write-offs in the real-world.", "The decision tree in Figure 8 has a cross-validated R-square of 0.89 and mean absolute percentage error of about 17% for DEFAULT_NEXT_MONTH = 1 residuals. So it\u2019s fairly accurate, and this means there are strong patterns in the M-GBM\u2019s wrong guesses. Moreover, the decision tree can help us see what those patterns might be. The largest residuals appear for PAY_0 < 0.5 AND PAY_AMT2 < NT$ 2802.5 AND PAY_4 < 1 AND PAY_AMT2 \u2265 NT$ 1312.5, or for customers with good most recent and fourth most recent repayment statuses, and smaller second most recent payments between NT$1300 and NT$2800. Under these conditions, we know the M-GBM model often fails to predict future defaults. This decision policy for residuals echoes earlier results that point to an over-emphasis of PAY_0 by the M-GBM, but adds an additional hint to look further into customers with second most recent payments between NT$1300 and NT$2800. This extracted decision policy for high-residual customers could also be used to create a model assertion, or a post-prediction business rule. (See Section 3.4.) The model assertion could send customers with these characteristics for processing by human case workers or to more specific models. It could possibly be used to correct wrong predictions in real-time too!", "Benchmark models are stable, trusted, transparent models, typically something like a linear model, single decision tree, simple rule-based model, or a previously existing and well-understood ML model. It\u2019s always a good idea to check that your new ML model does actually out-perform a known benchmark in out-of-time test data. If your new ML model doesn\u2019t out-perform a more traditional or transparent model, please don\u2019t use it.", "Once you\u2019ve established your ML model is at least more accurate than a simpler or pre-existing benchmark, that benchmark model can be a solid debugging tool. I use benchmark models to ask questions like: who did my ML model get wrong that my benchmark model got right? If you can isolate the erroneous ML model behavior, you can consider blending benchmark model predictions with ML model predictions to make better predictions for these cases. Also, you can probably reason through why a transparent model is behaving better for some subset of data, and develop potential remediation strategies. For instance, when comparing misbehaving ML models to linear models, one probable cause of ML model inaccuracy is overemphasis of non-robust interactions in the ML model. Additionally, benchmark models can be used to spot anomalies in real-time. In most low signal-to-noise ratio, human-oriented ML problems, simple model and ML model predictions should not be extremely different. Comparing benchmark model and ML model predictions in real-time can help you catch some accuracy, fairness, or security anomalies as they are happening.", "There are several known attacks against ML models that can lead to altered, harmful model outcomes or exposure of sensitive training data.[9][10] Figure 9 outlines some of the most well-known ML attacks. Unfortunately, traditional model assessment measures don\u2019t tell us much about whether a model is secure. In addition to other debugging steps, it may be prudent to add some or all of the known ML attacks into any white-hat hacking exercises or red-team audits your organization is already conducting.", "Now that we\u2019ve gone over some systematic ways to find accuracy, fairness, and security problems in ML-based systems, let\u2019s consider some remediation strategies to fix any detected problems.", "If your model debugging exposes sparse places in your training data or logical errors in your model related to a lack of data, then you probably need to get more data. You may be able to simulate the needed data, add it back into your training data, retrain your model, and re-test your model. More likely, you\u2019ll have to go back to the whiteboard, rethink how your training data is being collected, and wait until more complete data becomes available. To help avoid these kinds of problems in the future, consider experimental design techniques.[11] In the examples here, collecting information about debt-to-income ratio or employment status may have helped to de-emphasize PAY_0 in the M-GBM model.", "Data augmentation can be a remediation strategy for unwanted sociological bias in ML models as well. One major source of sociological bias in ML is demographically unbalanced training data. If your model is going to be used on all kinds of people, it\u2019s best to ensure your training data has a representative distribution of all kinds of people too. The Gender Shades line of research is both a cautionary tale and a success story about, among other important things, the necessity of demographically balanced training data.[12]", "Most of us use L1 and L2 regularization in our ML models today. We should probably continue to do so. Unfortunately standard types of regularization may not overcome strong and pathological biases, correlations, or dependencies in our training data. This is the case with PAY_0 in the discussed examples. A potential fix is to crank up the L1 and L2 regularization. If that remedy is not strong enough, you might consider additional measures like L\u221e regularization, dropout, weight-clipping, or noise-injection techniques.", "Going to such extremes to fix your training data may also be indicative of a data collection problem. If one variable is driving your entire model, you\u2019re probably missing at least one confounding input variable. In this case, consider data augmentation as another remediation strategy.", "Some ML models are designed to be interpretable, so you can understand how they work directly. Some of these models, like variants of decision trees or GA2M (a.k.a. explainable boosting machines, EBM), can be directly editable by human users. If you see something you don\u2019t like in the inner workings of a GA2M model, it\u2019s not very hard to change the model equation to get rid of what you don\u2019t like. Other models might not be as easy to edit as GA2M or decision trees, but if they generate human-readable scoring code, they can be edited. If there are a lot of wrong rules in your M-GBM scoring code, maybe you can fix them or maybe you can remove them. Definitely, try out GA2M, but also consider editing for other models as a viable bug-fixing strategy.", "One thing to consider about model editing is it will probably make your model look worse in training or validation data. Whatever was in your model was there because it made the training and validation error better. So, if you edit a model, you need to have solid reasoning backing up your decision.", "Model assertions, a.k.a. post-prediction business rules, are assertion statements on model predictions that can help correct erroneous or problematic model predictions.[13] In my examples, I saw my M-GBM model cannot account for pre-paying or over-paying after a customer is more than one month late on their most recent payment. Before issuing a default decision, it might be wise to check if the customer has prepaid or overpaid on their last payment. A model assertion for the M-GBM model in production could say: IF ((PAY_0 > 1) AND (PAY_AMT1 > LIMIT_BAL)) THEN DEFAULT_NEXT_MONTH = 0", "(A facetious, ripped-from-the-headlines model assertion might be more along the lines of: check to ensure a customer is not a minor before sending pregnancy-related coupons to their family home.[14])", "There are a lot of human and technical ways to fix unwanted sociological bias in ML models today. Many human fixes involve promoting diversity of opinion and experience on data science teams, and ensuring diverse intellects are involved in all stages of model building. Technical remediation methods fall into three basic buckets:", "Prediction post-processing: Changing model predictions after training, like reject-option classification in AIF360 or themis-ml.", "(Of course, giving two paragraphs to the subject of fixing unwanted sociological bias in ML models in 2019 borders on the absurd and offensive. Many techniques exist to fix unwanted sociological bias today. Learn more about them. There\u2019s really no excuse to deploy a racist model today, but it keeps happening, even in high-stakes life-critical scenarios.[16])", "Know how many models you have, who trained them and when. Document them like other software assets. Also, monitor important deployed ML model inputs and predictions. Watch for concerning trends and anomalies. Don\u2019t focus solely on accuracy. Think about fairness and security problems too.", "Most data scientists now understand models are typically trained on data that is a snapshot of reality. Over time, reality changes. New data drifts away from that snapshot, and model accuracy can degrade on that new data. This kind of model drift would likely be evident in statistical trends in the model\u2019s inputs and predictions. Can this kind of drift affect the fairness characteristics of a model as well? Probably. So consider real-time tests for DI in addition to any input or prediction monitoring you might be doing today.", "Finally, when an older model is taken down to be replaced, the old model should be decommissioned, meaning stored carefully for any future diagnostic, forensic, or litigation needs. Important models should not just be deleted.", "Anomalous inputs and predictions are always worrisome, and can be indicative of an adversary attacking your model. In the example M-GBM model, we saw that it\u2019s particularly sensitive to missing values for PAY_0, high values for PAY_0, and extremely low values of PAY_AMT1 and PAY_AMT3. Missing and other illogical values should probably not even be allowed into the M-GBM\u2019s production scoring queue in this case. Real-time monitoring for new data with high values for PAY_0 or with extremely low values of PAY_AMT1 and PAY_AMT3 could potentially divert those customers to processing by human case workers or to even more transparent models.", "To catch anomalous predictions in real-time, think about traditional statistical process control methodologies, comparing ML predictions to stable and transparent benchmark model predictions, or monitoring how new data flow through your model. (The latter is sometimes referred to as activation analysis.) For benchmark models, compare ML model predictions to benchmark model predictions. If they\u2019re very different, take a closer look before issuing a prediction or just use the benchmark model predictions for these data. For activation analysis, new data should likely not commonly flow through model mechanisms (e.g. decision tree nodes or neural network hidden units) that were not activated frequently during model training. If that\u2019s happening a lot, it\u2019s probably safest to investigate why.", "This post presents several debugging strategies for ML models, mostly focusing on structured data and standard commercial data mining use cases. If you\u2019re interested, you can dig deeper into the code used to create my examples on GitHub.[17] To get a flavor for what\u2019s going on with model debugging in the ML research community, checkout the International Conference on Learning Representations (ICLR) Debugging Machine Learning Models workshop proceedings.[18]", "While practitioner and research buzz is building around model debugging, the discipline is likely still in its infancy. Where it goes from here is largely up to us. Whenever ML is adopted for business- or life-critical decisions, I personally hope to see a lot more trustworthy ML and model debugging.", "[1] See: When a Computer Keeps You in Jail", "[2] E.g., Testing and Debugging Machine Learning Models, Machine Learning: The High Interest Credit Card of Technical Debt, or Model analysis tools for TensorFlow", "[3] See: Default of credit card clients dataset", "[4] See: Teach me Data Analysis", "[5] See: Interpretable Machine Learning, Section 5.9", "[7] For instance, Fairness Through Awareness and related work", "[8] Fairness, Accountability, and Transparency in Machine Learning", "[9] Proposals for Model Vulnerability and Security", "[10] Warning Signs: The Future of Security and Privacy in an Age of Machine Learning", "[11] Controlled Experiments in Machine Learning", "[12] How well do IBM, Microsoft, and Face++ AI services guess the gender of a face?", "[13] Model Assertions for Debugging Machine Learning", "[14] How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did", "[15] Responsible Data Science: Identifying and Fixing Biased AI", "[16] Millions of Black People Affected by Racial Bias in Health-care Algorithms", "[17] Interpretable Machine Learning with Python", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Principal scientist at bnh.ai, visiting faculty at GWU Decision Sciences"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faa822f1097ce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jphall_22520?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jphall_22520?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "Patrick Hall"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F45328ad3cf8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&user=Patrick+Hall&userId=45328ad3cf8d&source=post_page-45328ad3cf8d----aa822f1097ce---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa822f1097ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa822f1097ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pair-code.github.io/what-if-tool/index.html", "anchor_text": "What-If Tool"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://github.com/h2oai/h2o-3", "anchor_text": "H2O"}, {"url": "https://github.com/microsoft/LightGBM", "anchor_text": "LightGBM"}, {"url": "https://cran.r-project.org/web/packages/ALEPlot/index.html", "anchor_text": "ALEPlot"}, {"url": "https://cran.r-project.org/web/packages/DALEX/index.html", "anchor_text": "DALEX"}, {"url": "https://cran.r-project.org/web/packages/iml/index.html", "anchor_text": "iml"}, {"url": "https://github.com/SauceCat/PDPbox", "anchor_text": "PDPbox"}, {"url": "https://github.com/AustinRochford/PyCEbox", "anchor_text": "PyCEbox"}, {"url": "https://cran.r-project.org/web/packages/ICEbox/index.html", "anchor_text": "ICEbox"}, {"url": "https://bgreenwell.github.io/pdp/index.html", "anchor_text": "pdp"}, {"url": "https://github.com/tensorflow/cleverhans", "anchor_text": "cleverhans"}, {"url": "https://github.com/bethgelab/foolbox", "anchor_text": "foolbox"}, {"url": "https://cran.r-project.org/web/packages/DALEX/index.html", "anchor_text": "DALEX"}, {"url": "https://github.com/cosmicBboy/themis-ml", "anchor_text": "themis-ml"}, {"url": "https://github.com/ModelOriented/auditor", "anchor_text": "auditor"}, {"url": "https://github.com/dssg/aequitas", "anchor_text": "aequitas"}, {"url": "http://aif360.mybluemix.net", "anchor_text": "AIF360"}, {"url": "https://github.com/LASER-UMASS/Themis", "anchor_text": "Themis"}, {"url": "https://github.com/slundberg/shap", "anchor_text": "shap"}, {"url": "https://github.com/jphall663/secure_ML_ideas", "anchor_text": "https://github.com/jphall663/secure_ML_ideas"}, {"url": "https://github.com/microsoft/interpret", "anchor_text": "GA2M"}, {"url": "http://aif360.mybluemix.net", "anchor_text": "AIF360"}, {"url": "http://aif360.mybluemix.net", "anchor_text": "AIF360"}, {"url": "http://aif360.mybluemix.net", "anchor_text": "AIF360"}, {"url": "https://github.com/cosmicBboy/themis-ml", "anchor_text": "themis-ml"}, {"url": "https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html", "anchor_text": "When a Computer Keeps You in Jail"}, {"url": "https://developers.google.com/machine-learning/testing-debugging", "anchor_text": "Testing and Debugging Machine Learning Models"}, {"url": "https://ai.google/research/pubs/pub43146", "anchor_text": "Machine Learning: The High Interest Credit Card of Technical Debt"}, {"url": "https://github.com/tensorflow/model-analysis", "anchor_text": "Model analysis tools for TensorFlow"}, {"url": "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients", "anchor_text": "Default of credit card clients dataset"}, {"url": "http://www.vias.org/tmdatanaleng/cc_ann_extrapolation.html", "anchor_text": "Teach me Data Analysis"}, {"url": "https://christophm.github.io/interpretable-ml-book/shapley.html", "anchor_text": "Interpretable Machine Learning"}, {"url": "https://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/Residual_Surrealism_TAS_2007.pdf", "anchor_text": "Residual (Sur)Realism"}, {"url": "http://www.cs.yale.edu/homes/jf/Dwork.pdf", "anchor_text": "Fairness Through Awareness"}, {"url": "https://scholar.google.com/scholar?cites=15887350027958465759&as_sdt=20005&sciodt=0,9&hl=en", "anchor_text": "related work"}, {"url": "https://www.fatml.org/", "anchor_text": "Fairness, Accountability, and Transparency in Machine Learning"}, {"url": "https://www.oreilly.com/ideas/proposals-for-model-vulnerability-and-security", "anchor_text": "Proposals for Model Vulnerability and Security"}, {"url": "https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf", "anchor_text": "Warning Signs: The Future of Security and Privacy in an Age of Machine Learning"}, {"url": "https://machinelearningmastery.com/controlled-experiments-in-machine-learning/", "anchor_text": "Controlled Experiments in Machine Learning"}, {"url": "http://gendershades.org/", "anchor_text": "How well do IBM, Microsoft, and Face++ AI services guess the gender of a face?"}, {"url": "https://cs.stanford.edu/~matei/papers/2018/mlsys_model_assertions.pdf", "anchor_text": "Model Assertions for Debugging Machine Learning"}, {"url": "https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/", "anchor_text": "How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did"}, {"url": "https://www.youtube.com/watch?v=rToFuhI6Nlw", "anchor_text": "Responsible Data Science: Identifying and Fixing Biased AI"}, {"url": "https://www.nature.com/articles/d41586-019-03228-6", "anchor_text": "Millions of Black People Affected by Racial Bias in Health-care Algorithms"}, {"url": "https://github.com/jphall663/interpretable_machine_learning_with_python", "anchor_text": "Interpretable Machine Learning with Python"}, {"url": "https://debug-ml-iclr2019.github.io/", "anchor_text": "Debugging Machine Learning Models"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----aa822f1097ce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/privacy?source=post_page-----aa822f1097ce---------------privacy-----------------", "anchor_text": "Privacy"}, {"url": "https://medium.com/tag/security?source=post_page-----aa822f1097ce---------------security-----------------", "anchor_text": "Security"}, {"url": "https://medium.com/tag/fairness?source=post_page-----aa822f1097ce---------------fairness-----------------", "anchor_text": "Fairness"}, {"url": "https://medium.com/tag/data-science?source=post_page-----aa822f1097ce---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa822f1097ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&user=Patrick+Hall&userId=45328ad3cf8d&source=-----aa822f1097ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa822f1097ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&user=Patrick+Hall&userId=45328ad3cf8d&source=-----aa822f1097ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa822f1097ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faa822f1097ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----aa822f1097ce---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----aa822f1097ce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----aa822f1097ce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----aa822f1097ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jphall_22520?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jphall_22520?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Patrick Hall"}, {"url": "https://medium.com/@jphall_22520/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "60 Followers"}, {"url": "http://bnh.ai", "anchor_text": "bnh.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F45328ad3cf8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&user=Patrick+Hall&userId=45328ad3cf8d&source=post_page-45328ad3cf8d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F45328ad3cf8d%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstrategies-for-model-debugging-aa822f1097ce&user=Patrick+Hall&userId=45328ad3cf8d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}