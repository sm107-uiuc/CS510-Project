{"url": "https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55", "time": 1683013864.500267, "path": "towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55/", "webpage": {"metadata": {"title": "Understanding Reinforcement Learning Hands-on: Non-Stationarity | by Alejandro Aristizabal | Sep, 2020 | Towards Data Science | Towards Data Science", "h1": "Understanding Reinforcement Learning Hands-on: Non-Stationary", "description": "Welcome to the third entry of a series on Reinforcement Learning. On the previous article we explored the first on a series of many scenarios we\u2019re going to tackle, the Multi-Armed Bandits. In this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc", "anchor_text": "previous article", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e", "anchor_text": "article about Gradient Descent", "paragraph_index": 18}], "all_paragraphs": ["Welcome to the third entry of a series on Reinforcement Learning. On the previous article we explored the first on a series of many scenarios we\u2019re going to tackle, the Multi-Armed Bandits. In this situation, we\u2019re presented with an environment with a fixed number of actions, and are tasked on finding the action that yields the greatest reward. We presented some strategies, and measured their performance on this simple task.", "On this article, we\u2019re going to modify the previously presented environment, and make it a little bit more dynamic. We will see how our previous strategies deal with non-stationary environments, and how we can do better.", "Last time we began our story on a Casino, filled with bandits at our disposal. Using this example, we built a simplified environment, and developed a strong strategy to obtain high rewards, the \u025b-greedy Agent. Using this strategy, we were able to find the best action given enough time, and therefore earn tons of reward. Our agent performed well because it had a good balance between exploring the environment and exploiting its knowledge. This balance allowed the agent to learn how the environment behaves, while also receiving high rewards along the way. But, there\u2019s a small assumption our agent is doing to be able to behave so optimally, and that is that the environment is static, non-changing. What do we mean by this, and where is our agent making such assumption?", "When we mention the word \u201cstationary\u201d, we\u2019re talking about the underlying behavior of our environment. If you remember from last time, the environment is defined to have a set of actions that, upon interaction, yield a random reward. Even though the reward is random, they are generated from a central or mean value that every action has, which we called the true value. To see what I\u2019m talking about, let\u2019s have a look at one of the animated interactions we saw on the previous article.", "Observe how the true values (red dots) are static. Even though the environment generates random rewards, each action has a true expected value which is never-changing. This is a big assumption to make, and one that almost no valuable real-world scenario will follow. If, for example, our casino analogy was static, then casinos would be quickly out of business! So, how can we portray a more realistic scenario without making the problem that more complex?", "Making the multi-armed bandit scenario non-stationary is actually pretty simple, all we need to do is make sure the expected value for all our actions is always changing. Now, this change should not be completely erratic, since no strategy would be able to deal with a completely random scenario. Instead, we want the expected value to slowly drift away. This is so our agent can still consider previous experience meaningful for future decision-making. Here\u2019s how we can implement such environment", "There are some subtle changes. Now, every time our agent interacts with the environment, the true value for each action changes by a small, random amount. How much it changes can be controlled when we create the environment. This is so we can experiment with it later on. How does the environment behave now? We can make the same visualization as before, but with our new environment. Since we\u2019re not interested in the agent, we\u2019re not going to plot its interaction (yet).", "The previous plot was generated with a somewhat high amount of non-stationary for the sake of illustration. Note that on some instances the highest-valued action changes due to the random movements. Since we\u2019re interested on an agent that behaves as optimally as possible, we would expect that the agent can observe this changes and act accordingly.", "Having declared our new environment, let\u2019s see how the \u025b-greedy Agent performs here. Knowing that our agent has room for exploration, it should only be a matter of time before the agent notices that the environment changed.", "To make visualization clearer, I\u2019ve added some colors to show which is the action with the greatest value (orange dot) and which action the agent considers to be best (blue bar). For this animations, the amount of elapsed time was extended to 800 steps. At first, the agents are able to rapidly adapt to changes in the environment, but after a while, the estimated values stagnate, and tend to move slow. This makes it hard to catch up to changes in the future, and causes the agent to stay on a sub-optimal action for longer. As always, let\u2019s plot the optimality of this strategy by averaging the performance of many experiments.", "As we can see, at first our agent rapidly adapts to the environment, but as more time goes on, its performance starts to decrease. Even though our agent is always gathering experience, and is always exploring the environment, it seems like it can\u2019t handle a dynamic world. How can we ensure that our agent will adapt to changes even further in the future?", "Going back to the previous article, we exposed a way in which we can easily evaluate how valuable it is to take an action on the multi-armed bandit scenario. This evaluation was done using a basic average, which would converge to the expected value of the action after a given time. If you\u2019ve ever dealt with taking averages, you should know that the more items you add up to the operation, the more robust the result is. Let\u2019s say we want to take the average of a list of three values:", "Then the average would equal 4. Given such a small list, a change on any of the inputs will vary the resulting average by some amount.", "If we instead use a larger list of values, then the same change in the input will cause a smaller variation on the output.", "To sum up, the more information we have to calculate the average, the less prone the result is to outliers or variations. This effect can also be seen in the Incremental Average Update Rule we implemented previously:", "The expression of 1/n causes the same effect, given that as n gets larger, then the Q value changes less and less. This is what causes our agent to stagnate. Once enough experience has been collected, it\u2019s very hard to make our agent adapt and change its mind.To fix this, we must modify the update rule, so that later experience isn\u2019t discarded or ignored.", "Those that have some experience or knowledge with Machine Learning in general might have already seen a pattern behind the update rule presented above. If you haven\u2019t, let me give a brief explanation:", "Generally, for a Machine Learning model to learn, we use a measure called the Error or Loss function. This measurement determines how off our model\u2019s performance is compared to the expected results. To improve our model, we update its parameters by moving them against the error. How much should we move or modify our parameters? That\u2019s determined by the learning rate. Here\u2019s an overly simplified demonstration of the above statement.", "Looks familiar, right? h(x) represents the output of our model, while \u03b1 represents the learning rate. This is a grossly simplified version of Gradient Descent, which is the standard method for improving a model in most machine learning tasks. If interested on this topic, I think this is a great article about Gradient Descent.", "Comparing the incremental average update rule and gradient descent, our intuition would say that 1/n is equivalent to the learning rate \u03b1, and that would be correct. We can therefore modify the update rule we use to a more generalized version.", "What does this do? Well, now we can define \u03b1 to be anything other than 1/n. Using a constant for the learning rate is a simple but effective method used in other machine learning paradigms, and so we could try it as well here. Let\u2019s implement our new agent!", "We\u2019re basing our new implementation on the \u025b-greedy Agent. There are some minor changes. First, we\u2019re adding a new parameter called ss_func, which allows us to change the step-size function if desired. By default, this function returns a value of 0.1, which will be the constant step-size used here. Also, at the time of updating the estimates, we execute the ss_func function, and use the return value as our step size. Just to make things clear, the next line of code would be equivalent to declaring an \u025b-greedy Agent using this new implementation.", "Let\u2019s see how this new agent performs on our dynamic multi-armed bandit scenario, compared to the previous \u025b-greedy strategy.", "And just like that, our agent is now capable of adapting to a dynamic world! By using a constant step-size (or learning rate), the agent will no longer stagnate. Instead, every new experience is valued the same, and so it\u2019s possible to overcome previous experiences with new knowledge.", "As a final note, I want to briefly go into understanding this new parameter \u03b1, how it behaves and what values are reasonable for it. We\u2019ve already seen that a value of 0.1 gave great results on this scenario, but how would it behave with other values?", "The step-size parameter can be considered as a measurement of confidence in new knowledge. A high value for \u03b1 is analogous to saying that we trust our recent experience to be a good representation of the problem at hand. As such, the step-size parameter should lie between 0 and 1, where 1 is total confidence and 0 is no confidence on how representative the previous interaction is towards understanding the underlying problem. Let\u2019s see how this two values affect the learning outcome.", "A confidence of 1 would turn our update rule into this:", "Here, we\u2019re basically saying that our estimates should equal the reward recently obtained, disregarding any previous experience. This would only work if we know (or are really confident) that the reward we receive is deterministic and stationary. Since this is not the case in the real world (because interactions are usually stochastic, and the ones that aren\u2019t don\u2019t usually require learning algorithms), a value of 1 for the step-size should not be considered.", "A confidence of 0 would cancel out the update part of our update rule:", "With a step-size of zero, we\u2019re removing the possibility for our agent to learn from experience. For this reason, a constant step-size of 0 is meaningless for Reinforcement Learning.", "As for other values inside the range of 0 and 1, they determine how quickly or slowly our agent responds to variance, and how fast it would converge. Values close to 1 would quickly update the estimated values, and try to keep up with changes in the environment, but it would also be susceptible to noise. On the other hand, small values would take longer to converge, and react slower to a dynamic environment. Let\u2019s compare some values on this scenario.", "As predicted, the extremes are not well suited for the problem, low values converge slower and higher values converge faster. Different to other fields of Machine Learning, in which the learning-rate or step-size affects mostly convergence time and accuracy towards optimal results, in Reinforcement Learning the step-size is tightly linked to how dynamic the environment is. A really dynamic world (one that changes often and rapidly) would require high values for our step size, or else our agent will simply not be fast enough to keep up with the variability of the world.", "In this article we covered the idea of non-stationary, and implemented it onto the Multi-Armed Bandit scenario.We then explored how our previous contestant, the \u025b-greedy Agent, performed in this new situation, and exposed what made it behave sub-optimally. Then, by borrowing some concepts from other machine learning areas, we defined a new way of evaluating our actions. This new update rule allowed for a constant step-size, which was key to solve the problem of non-stationary. Lastly, a brief explanation of how the step-size parameter affects the learning outcome was demonstrated.", "This article was intended to touch on a few topics that were too long to be added to the previous one, but that still had enough value to be presented on the series. The Generalized Update Rule will play a role later on, and for that reason it had to be covered. With this, we can say goodbye to the Multi-Armed Bandit, and start touching on other topics. Next article will cover Markov Decision Processes (MDPs) and build the foundation for the famous Bellman Equation. These two concepts are the backbone of Reinforcement Learning, and understanding them could literally change for perspective of the world. See you then!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Musician, Sound Engineer and Programmer. Interested in Sound Design and Machine Learning for the arts."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F544ed094b55&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----544ed094b55--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----544ed094b55--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alejandro.aristizabal24?source=post_page-----544ed094b55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alejandro.aristizabal24?source=post_page-----544ed094b55--------------------------------", "anchor_text": "Alejandro Aristizabal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd49e13de55bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=post_page-d49e13de55bb----544ed094b55---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F544ed094b55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F544ed094b55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@grizzlybear?utm_source=medium&utm_medium=referral", "anchor_text": "Jonathan Petersson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-1-introduction-44e3b011cf6", "anchor_text": "Introduction"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc", "anchor_text": "Multi-Armed Bandits"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Multi-Armed%20Bandits.ipynb", "anchor_text": "Notebook"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Non-Stationarity.ipynb", "anchor_text": "Notebook"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-markov-decision-processes-7d8469a8a782", "anchor_text": "Markov Decision Processes"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Markov%20Decision%20Processes.ipynb", "anchor_text": "Notebook"}, {"url": "https://medium.com/@alejandro.aristizabal24", "anchor_text": "The Bellman Equation pt. 1"}, {"url": "https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e", "anchor_text": "article about Gradient Descent"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-1-introduction-44e3b011cf6", "anchor_text": "Introduction"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc", "anchor_text": "Multi-Armed Bandits"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Multi-Armed%20Bandits.ipynb", "anchor_text": "Notebook"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Non-Stationarity.ipynb", "anchor_text": "Notebook"}, {"url": "https://medium.com/@alejandro.aristizabal24/understanding-reinforcement-learning-hands-on-markov-decision-processes-7d8469a8a782", "anchor_text": "Markov Decision Processes"}, {"url": "https://github.com/aristizabal95/Understanding-Reinforcement-Learning-Hands-On/blob/master/Markov%20Decision%20Processes.ipynb", "anchor_text": "Notebook"}, {"url": "https://medium.com/@alejandro.aristizabal24", "anchor_text": "The Bellman Equation pt. 1"}, {"url": "https://www.coursera.org/specializations/reinforcement-learning", "anchor_text": "Reinforcement Learning Specialization"}, {"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "Reinforcement learning: An introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----544ed094b55---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----544ed094b55---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/multi-armed-bandit?source=post_page-----544ed094b55---------------multi_armed_bandit-----------------", "anchor_text": "Multi Armed Bandit"}, {"url": "https://medium.com/tag/non-stationarity?source=post_page-----544ed094b55---------------non_stationarity-----------------", "anchor_text": "Non Stationarity"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----544ed094b55---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F544ed094b55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=-----544ed094b55---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F544ed094b55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=-----544ed094b55---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F544ed094b55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----544ed094b55--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F544ed094b55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----544ed094b55---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----544ed094b55--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----544ed094b55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----544ed094b55--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----544ed094b55--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----544ed094b55--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----544ed094b55--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----544ed094b55--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----544ed094b55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alejandro.aristizabal24?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alejandro.aristizabal24?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alejandro Aristizabal"}, {"url": "https://medium.com/@alejandro.aristizabal24/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "50 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd49e13de55bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=post_page-d49e13de55bb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F97b8debaf8c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-hands-on-part-3-non-stationarity-544ed094b55&newsletterV3=d49e13de55bb&newsletterV3Id=97b8debaf8c7&user=Alejandro+Aristizabal&userId=d49e13de55bb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}