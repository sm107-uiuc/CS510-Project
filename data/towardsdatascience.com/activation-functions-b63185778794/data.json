{"url": "https://towardsdatascience.com/activation-functions-b63185778794", "time": 1682993342.551177, "path": "towardsdatascience.com/activation-functions-b63185778794/", "webpage": {"metadata": {"title": "Activation Functions. So why do we need Activation functions\u2026 | by Dhaval Dholakia | Towards Data Science", "h1": "Activation Functions", "description": "This blog attempts to outline my understanding of different activation functions and how to select which one to use."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Welcome to my first post! I am a Data Scientist and have been an active reader of Medium blogs. Now, I am planning to use Medium blog to document my journey in learning Deep Learning and share my experiences through the projects I have been working on. Hopefully, by sharing my views on the subjects I can also learn from the fantastic data science/deep learning community on Medium! I would love to hear your feedback on my first post here. With that said, let\u2019s get started \u2026", "The basic idea of how a neural network learns is \u2014 We have some input data that we feed it into the network and then we perform a series of linear operations layer by layer and derive an output. In a simple case for a particular layer is that we multiply the input by the weights, add a bias and apply an activation function and pass the output to the next layer. We keep repeating the process until we reach the last layer. The final value is our output. We then compute the error between the \u201ccalculated output\u201d and the \u201ctrue output\u201d and then calculate the partial derivatives of this error with respect to the parameters in each layer going backwards and keep updating the parameters accordingly!", "Neural networks are said to be universal function approximators. The main underlying goal of a neural network is to learn complex non-linear functions. If we do not apply any non-linearity in our multi-layer neural network, we are simply trying to seperate the classes using a linear hyperplane. As we know, in the real-world nothing is linear!", "Also, imagine we perform simple linear operation as described above, namely; multiply the input by weights, add a bias and sum them across all the inputs arriving to the neuron. It is likely that in certain situations, the output derived above, takes a large value. When, this output is fed into the further layers, they can be transformed to even larger values, making things computationally uncontrollable. This is where the activation functions play a major role i.e. squashing a real-number to a fix interval (e.g. between -1 and 1).", "Let us see different types of activation functions and how they compare against each other:", "The sigmoid activation function has the mathematical form `sig(z) = 1/ (1 + e^-z)`. As we can see, it basically takes a real valued number as the input and squashes it between 0 and 1. It is often termed as a squashing function as well. It aims to introduce non-linearity in the input space. The non-linearity is where we get the wiggle and the network learns to capture complicated relationships. As we can see from the above mathematical representation, a large negative number passed through the sigmoid function becomes 0 and a large positive number becomes 1. Due to this property, sigmoid function often has a really nice interpretation associated with it as the firing rate of the neuron; from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). However, sigmoid activation functions have become less popular over the period of time due to the following two major drawbacks:", "The python implementation looks something similar to:", "The tanh or hyperbolic tangent activation function has the mathematical form `tanh(z) = (e^z \u2014 e^-z) / (e^z + e^-z)`. It is basically a shifted sigmoid neuron. It basically takes a real valued number and squashes it between -1 and +1. Similar to sigmoid neuron, it saturates at large positive and negative values. However, its output is always zero-centered which helps since the neurons in the later layers of the network would be receiving inputs that are zero-centered. Hence, in practice, tanh activation functions are preffered in hidden layers over sigmoid.", "The ReLU or Rectified Linear Unit is represented as `ReLU(z) = max(0, z)`. It basically thresholds the inputs at zero, i.e. all negative values in the input to the ReLU neuron are set to zero. Fairly recently, it has become popular as it was found that it greatly accelerates the convergence of stochastic gradient descent as compared to Sigmoid or Tanh activation functions. Just to give an intuition, the gradient is either 0 or 1 depending on the sign of the input. Let us discuss some of the advantages of ReLU:", "However, it has a drawback in terms of a problem called as dying neurons.", "The Leaky ReLU is just an extension of the traditional ReLU function. As we saw that for values less than 0, the gradient is 0 which results in \u201cDead Neurons\u201d in those regions. To address this problem, Leaky ReLU comes in handy. That is, instead of defining values less than 0 as 0, we instead define negative values as a small linear combination of the input. The small value commonly used is 0.01. It is represented as `LeakyReLU(z) = max(0.01 * z, z)`. The idea of Leaky ReLU can be extended even further by making a small change. Instead of multiplying `z` with a constant number, we can learn the multiplier and treat it as an additional hyperparameter in our process. This is known as Parametric ReLU. In practice, it is believed that this performs better than Leaky ReLU.", "Thank you for reading. In this article I tried to lay down my understanding of some of the most commonly used activation functions, why we use them in the first place and which activation function should one use. Keep chilling and keep innovating!", "If you are working in Data Science or Deep Learning field do not hesitate to reach out if you think there is an opportunity to collaborate. I am looking for full-time opportunities and would love to discuss.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning @Wayfair - Computer Vision."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb63185778794&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b63185778794--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b63185778794--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dhaval.dholakia04?source=post_page-----b63185778794--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhaval.dholakia04?source=post_page-----b63185778794--------------------------------", "anchor_text": "Dhaval Dholakia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c5597884b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&user=Dhaval+Dholakia&userId=4c5597884b5&source=post_page-4c5597884b5----b63185778794---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb63185778794&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb63185778794&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://cs231n.github.io/neural-networks-1/#actfun", "anchor_text": "http://cs231n.github.io/neural-networks-1/#actfun"}, {"url": "http://www.deeplearningbook.org/", "anchor_text": "http://www.deeplearningbook.org/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b63185778794---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b63185778794---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----b63185778794---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/activation-functions?source=post_page-----b63185778794---------------activation_functions-----------------", "anchor_text": "Activation Functions"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb63185778794&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&user=Dhaval+Dholakia&userId=4c5597884b5&source=-----b63185778794---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb63185778794&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&user=Dhaval+Dholakia&userId=4c5597884b5&source=-----b63185778794---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb63185778794&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b63185778794--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb63185778794&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b63185778794---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b63185778794--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b63185778794--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b63185778794--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b63185778794--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b63185778794--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b63185778794--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b63185778794--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b63185778794--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhaval.dholakia04?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhaval.dholakia04?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dhaval Dholakia"}, {"url": "https://medium.com/@dhaval.dholakia04/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "73 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c5597884b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&user=Dhaval+Dholakia&userId=4c5597884b5&source=post_page-4c5597884b5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4c5597884b5%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-b63185778794&user=Dhaval+Dholakia&userId=4c5597884b5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}