{"url": "https://towardsdatascience.com/a-puzzle-for-ai-eb7a3cb8e599", "time": 1683015497.6723452, "path": "towardsdatascience.com/a-puzzle-for-ai-eb7a3cb8e599/", "webpage": {"metadata": {"title": "A puzzle for AI. How I taught AI to play 2048 | by Rinat Maksutov | Towards Data Science", "h1": "A puzzle for AI", "description": "Hi! My name is Rinat Maksutov, I work at Intelligent Engineering Services group of Accenture Technology in Russia, and I lead custom development projects. Over the course of my long career at\u2026"}, "outgoing_paragraph_urls": [{"url": "https://play2048.co/", "anchor_text": "https://play2048.co/", "paragraph_index": 4}, {"url": "https://www.alexirpan.com/2018/02/14/rl-hard.html", "anchor_text": "A wonderful article on this topic from", "paragraph_index": 15}, {"url": "https://github.com/voice32/2048_RL", "anchor_text": "my code on GitHub", "paragraph_index": 41}], "all_paragraphs": ["Hi! My name is Rinat Maksutov, I work at Intelligent Engineering Services group of Accenture Technology in Russia, and I lead custom development projects. Over the course of my long career at Accenture, I have experienced many different areas: mobile development, front-end, back-end, and even data science with machine learning. However, my story will not be about work, but about a hobby. I really enjoy learning and exploring new areas via my personal pet projects. Today I will tell you about one of them \u2014 how I taught a Reinforcement learning (RL) agent to play the famous puzzle game called \u201c2048\u201d. The article deliberately will not contain code, mathematics, state-of-the-art approaches and the latest discoveries in the field, so people who are well acquainted with RL will not discover anything new for themselves. This article is a story for the general public about how I set myself an unusual goal and achieved it.", "Our company invests heavily in ongoing employee training. For example, last year a program was launched where employees can take one of the Nanodegree on Udacity for free (Nanodegree is a set of several courses with a final project). I have already done Deep Learning Nanodegree on this platform, so this time I decided to take the course on reinforcement learning.", "The course reveals the basics of RL very well, but has one big drawback: the educational projects that are offered on the course are based on ready-made tasks \u2014 in which the environment the agent operates in has already been written for you by someone. That is, you just have to implement the learning algorithm and tweak the hyperparameters in order to reach the target score and pass.", "Therefore after completing the course, you will not be able to fully apply RL and solve your own problems, because you have studied only part of this area. And the questions of how to properly build an environment for an agent, how to formalize a task for it, how to correctly assign rewards for various actions \u2014 remain outside the brackets, and you will have to figure it out on your own (what all these terms mean, I will explain below).", "To close this gap, I tried to solve some problem that had not been solved by anyone before (at least with the help of RL), and use it to study various aspects of building environments for agents. As such a task, a mechanically simple puzzle game 2048 was chosen (you can play it in the browser here: https://play2048.co/ or search on Google Play or App Store for your smartphone). In this game, the player, by moving cells in one of four directions (up, down, right, left), needs to combine cells with the same value, and try to collect a cell with the maximum possible value. When you make a shift, a new two (with the probability of 0.9) or a four (with the probability of 0.1) appears on a random free cell. The game ends when there are no empty cells left on the field and the player cannot combine any cells.", "Despite the name, 2048 is not the maximum possible cell value in the game. With proper training, you can get to values 4096, 8192 cells, and so on. The maximum cell value theoretically possible is 131,072, that is 2 ^ 17:", "But getting this value is very difficult. This is because there is no strategy for this game that is guaranteed to lead to the maximum possible number of points. There is only a strategy that increases your chances of achieving it. Roughly speaking, it consists in defining two priority perpendicular directions (for example, down and to the right), and the majority of moves moving cells only in these directions, and very rarely in the rest. Thus, large values \u200b\u200bwill \u201caccumulate\u201d close to each other in one of the four corners, and it will be much more convenient to combine them to get even larger values.", "Why doesn\u2019t this strategy guarantee victory?", "Thus, the agent\u2019s task will be to learn this strategy in order to choose at each step the action that is most likely to allow in the future to obtain the maximum possible value of the cell in the game.", "Above I wrote that I will not immerse you in the theory of RL, but it is still worth going over the main ideas briefly. Reinforcement learning is one of the areas of machine learning, in which the goal is to teach an agent to perform the necessary actions in some environment. An agent here is a virtual entity (program, model) that receives as input the current state of the environment in which it operates. Then, depending on this state, it chooses the most valuable action, communicates it to the environment, and receives feedback and a new state from the environment. And so on in a loop until the environment comes to its final state.", "Under the hood of an agent there may be some mathematical model that determines which action will be best based on the current state of the environment. Initially, the agent, in general, does not know anything about what kind of environment is given to him, and how its actions affect it. The agent receives feedback from the environment through the so-called \u201creward\u201d. This is some number by which the agent can judge how correct the chosen action was. The agent\u2019s task is to learn a \u201cpolicy\u201d \u2014 an action strategy that maximizes the agent\u2019s reward for the entire period of the agent\u2019s existence in the environment. This reward is the very \u201creinforcement\u201d, which means that the right actions are supported by a positive reward, and the wrong ones are discouraged by negative ones. So, through multiple iterations, the agent (or rather, the model) begins to more accurately predict the potential reward from each possible action and choose the most valuable action.", "Udacity has an interesting analogue with puppy training. You give it a command, and at first it does not understand what you want. It begins to perform random actions in response to your commands: barking, bouncing, lying on its back, running around. The moment it randomly chooses the right action, you give him a treat. The dog realizes that most likely you want it to do exactly that action on this command. This is repeated several times \u2014 command \u2014 action \u2014 delicious, after which the dog is convinced of the correctness of the chosen action.", "A significant number of large media stories about artificial intelligence in recent years have been associated with reinforcement learning: the victory of AlphaGo algorithm, playing StarCraft at the professional level, and others. But you need to understand that in the field of reinforcement learning there are still a lot of unsolved problems, and one of the main ones is the poor transferability of the learned skills to other tasks and the high instability of learning, which is affected by even the slightest changes in the environment. That is, if the algorithm learns to play well in one game, then most likely it will show poor results in another. There is some progress in this direction, but there is still a long way to go to relatively universal algorithms, and most likely, it is not economically feasible to develop them yet.", "Another important point is related to the fact that in practical terms, very few tasks make sense to solve with the help of reinforcement learning. It is often much easier, faster and more reliable to implement a previously known algorithm for solving the problem than to spend time training an agent. For this reason, reinforcement learning has not yet received as much attention as other areas of machine learning.", "The fact is that traditionally machine learning is used to automate tasks that: 1) are difficult to algorithmize, 2) are easily performed by a sufficiently trained person, but 3) require a significant amount of time from a person to complete a task. For example, in image recognition tasks, a person can easily determine whether the desired object is on it or not, but it is very difficult to write a sufficiently reliable and more or less universal algorithm for determining an object, and at the same time, for a large number of images, a person will do this work for a very long time. Or, for example, fraud detection: a trained person will be able to identify suspicious activity by the sequence of transactions, but their search and analysis will take an enormous amount of time, and this task is also difficult to describe with universally predefined sets of rules.", "Now let\u2019s look at reinforcement learning from this angle. The aim of this area is to teach an agent to act in a certain environment, that is, to make decisions depending on the current situation. Not many tasks fit this idea \u2014 mainly in the field of controlling something, for example, a robotic arm on an assembly line, a car on the road, a crowd of zergs in StarCraft, and so on. But usually people already know the rules or strategies by which the desired result is achieved. We would not want a manipulator on an assembly line to first learn to put a part in the right place for tens of thousands of cycles and ruin tens of thousands of products before it starts doing the right thing. It is much safer to write a program that precisely positions the arm at every step of the assembly process. We would not want the car to travel millions of kilometers and learn what each of the road signs means via the fines that came to it in a week \u2014 it is much faster and more reliable to input this knowledge initially. That is why reinforcement learning is still a very experimental field, and there are few practical applications for it. A wonderful article on this topic from three years ago is still relevant and describes the problem very well, I advise you to read it. This concludes the introduction, we return to our task.", "Reaching cell 2048 (although for a more or less experienced player, 2048 is not an achievement at all) is a long journey of trial, error, disappointment, the stupidest bugs, inspiration and joy.", "At first, everything seemed pretty simple: we take the ready-made code that implements Deep Q-network from my homework on Udacity, adapt it a bit to our own environment, and everything will work out. So na\u00efve.", "So that you understand what 3 months of experiments were spent on (if nothing is clear, you can just be amazed by the number of bullets and scroll further):", "And if the neural network configuration and training parameters are pretty standard things that one has to deal with every time, when you solve a problem in deep learning, then the others are specific to reinforcement learning.", "The first thing I had to face was the lack of progress in training the agent AT ALL. And this was related to what the agent was receiving from the environment.", "Writing the logic of the board for the game turned out to be quite simple, and took a couple of hours. A few simple matrix tricks \u2014 and the engine is ready. It performs \u201ccollapsing\u201d and shifting of cells depending on the selected action, filling a new cell and counting points. Thus, the current state of the environment was described by a simple 4x4 matrix, where each cell contained the value of the corresponding cell. Since I used a regular fully-connected neural network, before sending the state of the environment to the neural network, it had to be transformed into a 1x16 vector:", "And here came the first problem. The quality of training stopped growing when the agent got to cell 512. It could not collect more than that, no matter how much I trained it. The problem here turned out to be in the values \u200b\u200bof the cells, which have a colossal range: from 0 to hundreds of thousands. Let the experts forgive me, but here is a very loose explanation of the reason for this behavior \u2014 I deliberately simplify so that the general idea is clear.", "Each input value is a signal for a neural network. The strength of this signal is proportional to the input value, and the error at the output of the neural network will also be proportional to it. And the larger the cells the agent reaches, the stronger the signal from these large cells will be, and the weaker \u2014 from the smaller ones. However, small cells are found orders of magnitude more often than large ones, and they are of more significant importance than large ones, since it is by correctly collecting small values \u200b\u200bthat large ones can be obtained. It turns out that the larger the values \u200b\u200bthe agent learns to collect, the worse it starts to work with smaller cells, because their signals become weaker and weaker for the neural network.", "It turned out to be quite simple to solve this problem. I just brought the values \u200b\u200bto a more uniform scale: the log2 of the cell value. Then the exponentially growing values \u200b\u200bturned into an ordinary sequence:", "That is, before each move, the log2 for each cell of the current board was calculated, and then this matrix was fed into the neural network. This trick made it possible to overcome the ceiling of the value 512 and get to 1024. But the learning process was still very slow. It was obvious that this kind of environment was not informative enough for a neural network.", "At some point, I thought that the agent doesn\u2019t really care what values \u200b\u200bthe cells have. The mechanics of transforming one values \u200b\u200binto others is what matters, that is, that a new value is born from another two identical values. We could supplement our table with the following row:", "And for the agent, the only meaning would be that a + a = b, b + b = c, etc., and not what values \u200b\u200bare hidden behind a, b and c. (\u201c+\u201d In this case is not addition, but the same \u201ccollapsing\u201d). This means that the values \u200b\u200bof the cells can be considered not as numerical values, but as categorical ones. And since we know the maximum possible value of a cell, each cell can be represented as a one-hot encoded vector. That is, for each cell, not its value was used, but a vector of dimension 18, in which all values \u200b\u200bwere zeros, and only one value, the position of which corresponds to one of our possible values, was equal to one. And such vectors \u2014 by the number of cells. It\u2019s still not clear to me why, but it was this, and not the numeric representation that helped the agent reach higher values \u200b\u200bmuch faster.", "Initially, the reward was calculated simply as the sum of the values \u200b\u200bof all cells on the board. It seemed that this score would be the very engine of progress for the agent, because by the gradual increase in the score, one can judge whether the right actions are chosen or not, and this is what can be used as a reward. It turned out not. And the reason is in the mechanics of the game.", "Take a very simple game like Space Invaders. Google trained its agents on it a few years ago.", "In this game, the score increases every time you hit an alien. That is, there is a direct relationship between the action (\u201cshot\u201d), the result in the environment (\u201chit\u201d) and the score.", "In 2048, this approach did not work. And here\u2019s why. Let\u2019s say you have 2 cells with the same values \u200b\u200bnext to each other. You collapse them, and \u2026 the score on the board has not changed. Because their value individually equals the value of the new cell. That is, having performed the correct action, the agent will not receive positive reinforcement and will not learn anything. Moreover, after each action, a new random cell is filled with a value of 2 or 4. So no matter what action the agent performs, it will always receive a value in response that equals [count to step + 2 or 4]. Obviously, this information is not enough to understand how well the agent chose the action. And precisely because of this, the training practically did not progress.", "Therefore, the award had to be realized in a different way. At first I tried to give it not the current value of cells on the board, but the value of collapsed cells since the beginning of the game. Now the agent had a more reliable reference point, and the training went faster: the agent saw which of the actions greatly increased the score, and which did not. But even so, the training did not go as quickly as I would like, so the idea came to present an even more specific indicator: to give as a reward only the total value of collapsed cells at the current step. And this has allowed it to clearly understand what actions should lead to what results, and significantly accelerate the learning process.", "But there is another detail related to the mechanics of the game. The reward for moving in two opposite directions will be the same, but the boards will be in different states, and will lead to different consequences in the next steps. But what is even more important, if you remember, is that the shifts should occur predominantly in the chosen directions. That is, if we collect cells in the lower right corner, then the left shift should be performed only in exceptional cases. Therefore, we can assume that a shift to the right has a larger expected reward, and a shift to the left has a smaller expected reward. This means that we need to predict the expected reward not only at the current step, but also at the next one.", "To train the model to predict the reward, I used the history of \u201cplayed\u201d games \u2014 each step of the previous games was saved in a special buffer and used for training. To begin with, I tried using the sum of all rewards from the current step to the end of the game as the target value to predict. Moreover, the farther a step is from the current one, the less weight its reward should be summed up. To do this, I multiplied it by the corresponding ordinal value of a geometric decreasing progression. But this approach, due to too many variations of moves, gave bad predictions. Then I limited the prediction depth to the current step with a weight of 1.0 and the next step with a weight of 0.1. Looking at the logs, I saw that as time went on, the predictions of potential rewards and most valuable actions actually became closer to what was actually happening. But still, the model too often made moves in non-priority directions, and thus spoiled the situation on the board for itself. It was necessary to somehow \u201cdissuade\u201d the agent from doing actions harmful to itself, and this was done with the help of penalties.", "This approach is used quite often in RL so that the agent not only learns how to perform a task, but also does it the leanest way. And to show it which of the moves are lean and which are wasteful, you need to put this information within the reward. In my case, I decided to penalize the agent at every move for all cells that have moved (that is, changed their position) after the selected action. And the more cells were shifted, the higher the penalty was. That is, if it accumulates large values \u200b\u200bin the lower right corner, and continues to make moves to the right or down, then only the values \u200b\u200bof new cells will be a penalty for it at each step. If it suddenly decides to make a move up or to the left, then not only the new cell will move, but also all those that were concentrated in the lower right corner will move, and the penalty will be huge. Over time, the agent realized that the greatest reward is obtained not only when it collapses larger cells, but also when it moves the least number of cells \u2014 and this forces to adhere to the chosen strategy, and only in exceptional cases take steps in \u201cnon-priority\u201d directions \u2014 when the expected the reward, including fines, is indeed higher.", "On this picture we see how the strategy gradually changed in the right direction: in the first games the directions of the shifts were chosen evenly, but then \u201cpriority\u201d ones appeared \u2014 to the right and down.", "There was another funny moment in the agent\u2019s behavior, which greatly slowed down the training. As I wrote above, the agent was not given knowledge of the rules of the game. Accordingly, it did not know that it is impossible to make a move, for example, to the right, if you already have all the cells pressed to the right side of the board. That is, even if you make such a move, nothing on the board will change. At some point, the neural network for some reason decided that the most profitable move is the one in which not a single cell moves, because if nothing has moved, then there will be no penalty. Perhaps because of their size, the shift penalties were worse for it than not receiving a reward for collapsed cells. And since after such a move the board did not change, then input for the next step remained the same. And, of course, the same action was chosen. And so on for thousands of iterations, until the \u201crandom\u201d move happened (at each iteration, with a very low probability \u2014 in tenths or hundredths of a percent \u2014 a random move was made instead of the calculated one, in order, among other things, to get out of such loops). This problem could have been solved by tweaking the penalties and rewards, but instead I just decided to very heavily penalize the agent for \u201cimpossible\u201d moves, and it quickly weaned off doing it.", "The green graph shows the maximum cell value in each game. The outlier \u2014 in one of the games the maximum value of the cell was 2048.", "In order to learn to assemble the cell 2048, the agent had to play a little more than 60 thousand games. However, an important detail is that a single achievement does not mean that the agent has learned to do it consistently. Look at the beginning of the green graph, where you can see how the agent learned to reach cell 1024. At first there was the same outburst, then 1024 appeared more and more often, and then somewhere after 30 thousand games the agent began to reach 1024 quite confidently. So if we talk about the agent really \u201clearning\u201d how to collect 2048, then, extrapolating the pattern, we can estimate that the agent will need more than a million games to consolidate this skill, and rush to the next goal \u2014 4096.", "You are probably already tired of reading, so I\u2019m finishing my longread with a 20-minute video of cell 2048 gathering (starting at 16:40):", "I performed the training on my laptop, so it took about two days to get this result. But as I wrote above, 2048 is far from the limit in this game. Therefore, if you have computing power and time, it is quite possible to reach much larger cells \u2014 take my code on GitHub and try to train your own agent! If you manage to beat my achievement, send the gifs and links to your results in the comments. Thanks for reading!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Technology consultant with experience in mobile and web development, artificial intelligence and systems architecture."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Feb7a3cb8e599&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rinat-maksutov.medium.com/?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": ""}, {"url": "https://rinat-maksutov.medium.com/?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "Rinat Maksutov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9483c7e0e6ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&user=Rinat+Maksutov&userId=9483c7e0e6ed&source=post_page-9483c7e0e6ed----eb7a3cb8e599---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb7a3cb8e599&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb7a3cb8e599&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://play2048.co/", "anchor_text": "https://play2048.co/"}, {"url": "https://ru.wikipedia.org/wiki/2048_(%D0%B8%D0%B3%D1%80%D0%B0)#/media/%D0%A4%D0%B0%D0%B9%D0%BB:2048_Final_Position.jpg", "anchor_text": "Wikipedia"}, {"url": "https://www.alexirpan.com/2018/02/14/rl-hard.html", "anchor_text": "A wonderful article on this topic from"}, {"url": "https://en.wikipedia.org/wiki/File:SpaceInvaders-Gameplay.gif#/media/File:SpaceInvaders-Gameplay.gif", "anchor_text": "Wikipedia"}, {"url": "https://github.com/voice32/2048_RL", "anchor_text": "my code on GitHub"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----eb7a3cb8e599---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----eb7a3cb8e599---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----eb7a3cb8e599---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feb7a3cb8e599&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&user=Rinat+Maksutov&userId=9483c7e0e6ed&source=-----eb7a3cb8e599---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feb7a3cb8e599&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&user=Rinat+Maksutov&userId=9483c7e0e6ed&source=-----eb7a3cb8e599---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb7a3cb8e599&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Feb7a3cb8e599&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----eb7a3cb8e599---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----eb7a3cb8e599--------------------------------", "anchor_text": ""}, {"url": "https://rinat-maksutov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rinat-maksutov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rinat Maksutov"}, {"url": "https://rinat-maksutov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "219 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9483c7e0e6ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&user=Rinat+Maksutov&userId=9483c7e0e6ed&source=post_page-9483c7e0e6ed--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5b3a5c8e1171&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-puzzle-for-ai-eb7a3cb8e599&newsletterV3=9483c7e0e6ed&newsletterV3Id=5b3a5c8e1171&user=Rinat+Maksutov&userId=9483c7e0e6ed&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}