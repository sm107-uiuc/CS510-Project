{"url": "https://towardsdatascience.com/recommendation-system-with-reinforcement-learning-3362cb4422c8", "time": 1683002389.579384, "path": "towardsdatascience.com/recommendation-system-with-reinforcement-learning-3362cb4422c8/", "webpage": {"metadata": {"title": "Recommendation System with Reinforcement Learning | by Yizhou Wang | Towards Data Science", "h1": "Recommendation System with Reinforcement Learning", "description": "Recommendation system can be a vital competitive edge for service providers such as Spotify, who mainly grows business through user subscriptions. Accurate recommendations help improve user\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1901.09851", "anchor_text": "\u201cMusic Streaming Sessions Dataset\u201d (MSSD)", "paragraph_index": 3}, {"url": "https://www.capstone.iacs.seas.harvard.edu/", "anchor_text": "IACS 297R research project", "paragraph_index": 26}], "all_paragraphs": ["Team members: Sophie Zhao, Yizhou Wang, Feng Qian", "Recommendation system can be a vital competitive edge for service providers such as Spotify, who mainly grows business through user subscriptions. Accurate recommendations help improve user experience and strengthen customer loyalty.", "Traditional recommendation methods include modeling user-item interaction with supervised learning such as classification, memory-based content-filtering from user history and many more. These ideas overlook the dependency across consecutive time steps. Inspired by the progress of reinforcement learning in other domains, such as playing Atari game, we apply a state of the art model, the Deep Deterministic Gradient Policy (DDPG), to model music recommendations as a sequential decision process. In this setup, action of the DDPG learner is a song selected from a huge pool. By representing each song using a set of continuous features and subsequently expanding the action space from discrete to continuous, our agent successfully scales up the number of candidate songs it can accommodate, while maintaining satisfying recommendation accuracy and diversity.", "We use the \u201cMusic Streaming Sessions Dataset\u201d (MSSD), originally published for a competition by Spotify. The dataset contains both listening session data and a lookup table for song features.", "Above is a plot of the structure of the data we have. We have two data files: one file with rows of sessions and the other file with rows of track features. Tracks are songs and a session is a sequence of tracks that a single user listens to. The maximum of the maximum length of a session is limited to 20 by Spotify.", "Inside of a session, actions between and within tracks are recorded. Actions between tracks include: skip_very_briefly, skip_briefly, mostly_played_before_skip, no_skip, indicating the pattern a user transfer from one track to the next. Actions within a song mainly include move_forward, move_backward, no_move, indicating user behavior while listening to a track; and, no_pause_before_play, short_pause_before_play, long_pause_before_play indicating user pausing behavior. The above responses can be interpreted as user preferences on each track.", "We also have feature data on each track and the features are given by Spotify through either manual or automatic means. Such features include acousticness, beat_strentgh, danceability, etc. and vary between [0,1]. We can incorporate these features into our model.", "MSSD is a huge dataset with more than 20 millions of songs and 17 millions of sessions, comprising around 600 GB of data. In order to accommodate our model to this dataset, we take a subset of songs and sessions as we explain below.", "To select a subset of MSSD we do as follows: first, we sample the session data, then, we sample session track data. In the original data, We decided to sample 100 thousand sessions from the data. Since the data is divided into 10 zip files with similar sizes, we need to sample 10 thousand sessions from each zip file. Each zip file is comprised of N data files so we need to sample 10k/N thousand sessions from each data file. Since each data file contains a different number of sessions, we need to sample from each data file with different probabilities. For example, if there is $M$ session in data file $f_1$, we accept each session in file f1 randomly with a probability of 10k/NM.", "After sampling all sessions, we need to find out all tracks that appear in our sampled data. It could be easy if the size of the data is small: we could use a set data structure to find a set of track ids. However, since the size of the data is too big to store in memory, we decided to use a database (Mysql on a Macbook Air 2014) to let the database maintain the tree-structure for us. Using the database, we could find tracks we need easily.", "A reinforcement learning model has these components: Agent, Environment, State, Reward Function, Value Function and Policy. To simplify the problem, we assume a hypothetical user whose experience is pooled from all the actual users. Our recommender model is going to be the agent of the system that processes songs to this hypothetical user that will skip/not-skip the recommendation. The user behaves as the system\u2019s environment, responding to the system\u2019s recommendation depending to the state of the system. User feedback determines our reward, that is, one score only if the user does not skip. Action of the agent is song recommended. Our state is defined as the song features and corresponding user reactions of past 5 steps, excluding the current step. Therefore, feedback and action together give us the next state. The goal of the agent is to learn a policy that maximizes accumulated rewards in 15 steps. We set the length of prediction to be 15 to avoid a cold start, that is, predicting when there isn\u2019t sufficient history, given our original session length is 20 and history length is 5.", "More formally, the mathematical definition is as follows:", "Our data first go through an autoencoder with 2 parts: a numerical compressor and a time compressor. Each data point is originally 5 x 21 corresponding to 20 song features for 5 songs and the observed action of user (skip/no-skip). We compress the 20 song features with a feedforward autoencoder which transforms the input into 5 x 8. We concatenate the user response at the end to make this 5 x 9. We then input this latent representation to an LSTM autoencoder that compresses along the time dimension into a single vector of dimension 1 x 9.", "During training, we first build an autoencoder and train it on a song feature dataset.", "We then fix the encoding layers and decoding layers and connect them with the LSTM time compressor. The encoding layers are responsible for preprocessing the numerical features, and the decoding layers are responsible for extending the output of time decoder to full length.", "Time compressor tries to compress 5 x 9 input into 1 x 9 and then decode it into 5 x 9. In order to accommodate mixed-type data, we use two time decoders, one with MSE loss to recover the numerical, and the other with cross entropy loss to recover binary user skipping behavior. The final loss is a linear combination of these two losses and the weights assigned affect model performance in these two tasks. Despite using two decoders, they share a single length 9 latent representation as input. During prediction, one addition step is that the numerical decoder takes the output of time decoder and restore the original dimension for numerical features.", "Instead of compressing numerical features only, the former \u201cnumerical compressor\u201d compresses both song features and binary user responses, which we refer to as the \u201ccombinational compressor\u201d in order to distinguish it from the \u201cnumerical compressor\u201d. By releasing the burden of compressing categorical response from our time compressor, this structure can be further improved. In this project, we have yet to connect the combinational compressor with the time compressor.", "Our numerical features are normalized to be within 0 to 1. The MSE for restructuring 5 x 20 from 1 x 9 latent is 0.0111, about 1% of the feature range. The Accuracy of reconstructing binary user behavior is 88.89%. The MSE from training the numerical compressor is 0.0016. All statistics are calculated on the test set. The last two rows are performance statistics of the experimental combinational compressor. The experimental combinational compressor has an accuracy of one retrieving binary user skipping behaviors on the test set, and a slightly increased numerical reconstruction MSE at 0.0064.", "When given a pair of state and action, the environment is supposed to return the reward. However, in this problem we cannot observe real-time user response. What we will do is using the existing data to estimate the skip/not skip behavior.", "DDPG, short for Deep Deterministic Policy Gradient, is a model-free off-policy actor-critic algorithm, combining DPG with DQN. The original DQN works in discrete space, and DDPG extends it to continuous action space with the actor-critic framework while learning a deterministic policy.", "There are four neural networks in this algorithm: actor, critic, actor target and critic target. Actor network learns the policy function, while critic network approximates the Q-function.", "Since our model requires the information of the previous five songs, the agent will make 15 recommendations for each session. Therefore we run 15 steps in each episode.", "The maximum score the recommender system can accumulate from our truncated music sessions is 15. The not-skip behaviour takes up 34% of the whole data, which translates to a benchmark score of around 5. As we can see from the left plot, upon training on merely a few hundreds episodes, our agent reaches a score of around 11, showcasing much better performance than the benchmark. On the right is the diversity score. If the distance between the current action and previous action is beyond a certain threshold (0.4 times the standard deviation), we get a diversity score of 1, otherwise we get 0. This is calculated for each step, hence the maximum score is also 15. From the plot, we can see that at the beginning, our agent tends to recommend similar songs. But after around 300 episodes, it learns to take into account the diversity of recommendations.", "In this project, we successfully use reinforcement learning to capture user-song interaction and the time dependency between current and past decisions. Future research can extend on our discovery by relaxing many of the simplifying assumptions: first, instead of assuming a single hypothetical user, perform a customer stratification and accommodate this variable into the model. Second, instead of truncating history at five, we can take into account full user history since the start of the session. In addition, we can test whether the experimental combined compressor outperforms the current model by connecting it to time compressor and downstream DDPG agent.", "In terms of training, we are training different model components separately due to constraints of computational resources and time. It would be a good idea to train the model end to end so the latent may be better suited for reinforcement learning task. Finally, possibly the most significant modification one can make on top of our research is to provide the reinforcement learning agent with a real environments. Our current dataset is highly biased towards skipping behaviors (with 34% non-skipping rate) and may not reflect realistic customer behaviors. Therefore, instead of simulating an environment using the dataset (600G), a better way to make the model production-ready is to recruit some participants to test how they react to the recommendations. This way the agent can fully explore recommendation space and make more trustworthy recommendation.", "We would like to give our most sincere gratitude to: Javier Zazo, our mentor, expert in reinforcement learning, and a great friend who has always been giving us valuable suggestions and encouragement we need to proceed. Pavlos Protopapas, the head of the 297R research project, the instructor who makes this unforgettable research journey possible, and Aparna Kumar, Spotify senior researcher and data scientist, a great supporting manager to report to.", "The research project is under the IACS 297R research project.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science @ Harvard. Previously Peking University."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3362cb4422c8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wangyizhou30?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wangyizhou30?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "Yizhou Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f1656660d00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&user=Yizhou+Wang&userId=7f1656660d00&source=post_page-7f1656660d00----3362cb4422c8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3362cb4422c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3362cb4422c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1901.09851", "anchor_text": "\u201cMusic Streaming Sessions Dataset\u201d (MSSD)"}, {"url": "https://www.capstone.iacs.seas.harvard.edu/", "anchor_text": "IACS 297R research project"}, {"url": "https://sophieyanzhao.github.io/AC297r_2019_SpotifyRL/2019-12-14-Spotify-Reinforcement-Learning-Recommendation-System/", "anchor_text": "https://sophieyanzhao.github.io"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----3362cb4422c8---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/recommendation-system?source=post_page-----3362cb4422c8---------------recommendation_system-----------------", "anchor_text": "Recommendation System"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----3362cb4422c8---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3362cb4422c8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3362cb4422c8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3362cb4422c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&user=Yizhou+Wang&userId=7f1656660d00&source=-----3362cb4422c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3362cb4422c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&user=Yizhou+Wang&userId=7f1656660d00&source=-----3362cb4422c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3362cb4422c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3362cb4422c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3362cb4422c8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3362cb4422c8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3362cb4422c8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3362cb4422c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wangyizhou30?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wangyizhou30?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yizhou Wang"}, {"url": "https://medium.com/@wangyizhou30/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "69 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f1656660d00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&user=Yizhou+Wang&userId=7f1656660d00&source=post_page-7f1656660d00--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe079681524b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-with-reinforcement-learning-3362cb4422c8&newsletterV3=7f1656660d00&newsletterV3Id=e079681524b7&user=Yizhou+Wang&userId=7f1656660d00&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}