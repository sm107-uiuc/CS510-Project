{"url": "https://towardsdatascience.com/introduction-to-evolution-strategy-1b78b9d48385", "time": 1683006731.838717, "path": "towardsdatascience.com/introduction-to-evolution-strategy-1b78b9d48385/", "webpage": {"metadata": {"title": "Introduction to Evolution Strategy | by Abhijeet Biswas | Towards Data Science", "h1": "Introduction to Evolution Strategy", "description": "In this post we will learn to train a neural network without back-propagation using Evolution Strategies (ES) in Python from scratch on MNIST Handwritten Digit dataset. This simple implementation\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1703.03864", "anchor_text": "paper", "paragraph_index": 4}, {"url": "https://www.digdeepml.com", "anchor_text": "Dig Deep ML", "paragraph_index": 16}], "all_paragraphs": ["In this post we will learn to train a neural network without back-propagation using Evolution Strategies (ES) in Python from scratch on MNIST Handwritten Digit dataset. This simple implementation will help us understand the concept better and apply it to other suitable settings. Let\u2019s get started!", "Table of Content1. Numerical Optimization2. Evolution Strategies3. Vanilla Implementation4. Python Implementation from scratch5. Ending note", "Almost every machine learning algorithm can be posed as an optimization problem. In an ML algorithm, we update the model\u2019s parameters to minimize the loss. For example, every supervised learning algorithm can be written as, \u03b8_estimate = argmin \ud835\udd3c[L(y,f(x,\u03b8))], where x and y represent the features and the target respectively, \u03b8 represents model parameters, f represents the function we are trying to model and L represents the Loss function, which measures how good our fit is. Gradient Descent algorithm also known as steepest descent has proven to solve such problems well in most of the cases. It is a first-order iterative algorithm for finding the local minimum of a differentiable function. We take steps proportional to the negative of the gradient of the Loss function at the current point, i.e. \u03b8_new = \u03b8_old \u2014 \u03b1*\u2207 L(y, f(x, \u03b8_old)). Newton\u2019s Method is another second-order iterative method which converges in fewer iterations but is computationally expensive as the inverse of second-order derivative of the loss function (Hessian matrix) needs to be calculated, i.e. \u03b8_new = \u03b8_old \u2014 [\u2207\u00b2 L(y, f(x, \u03b8_old))]^(-1) * \u2207 L(y, f(x, \u03b8_old)). We are searching for parameter using the gradients as we believe that it will lead us in the direction where loss will get reduced. But can we search for optimal parameters without calculating any gradients? Actually, there are many ways to solve this problem! There are bunch of different Derivitive-free optimization algorithms (also known as Black-Box optimization).", "Gradient descent might not always solve our problems. Why? The answer is local optimum in short. For example in case of sparse reward scenarios in reinforcement learning where agent receives reward at the end of episode, like in chess with end reward as +1 or -1 for winning or losing the game respectively. In case we lose the game, we won\u2019t know whether we played horribly wrong or just made a small mistake. The reward gradient signal is largely uninformative and can get us stuck. Rather than using noisy gradients to update our parameters we can resort to derivative-free techniques such as Evolution Strategies (ES). ES works out well in such cases and also where we don\u2019t know the precise analytic form of an objective function or cannot compute the gradients directly.", "In this paper by OpenAI, they show that ES is easier to implement and scale in a distributed computational environment, it does not suffer in case of sparse rewards and has fewer hyper-parameters. Moreover, they found out that ES discovered more diverse policies compared to traditional Reinforcement learning algorithm.", "ES are nature-inspired optimization methods which use random mutation, recombination, and selection applied to a population of individuals containing candidate solutions in order to evolve iteratively better solutions. It is really useful for non-linear or non-convex continuous optimization problems.", "In ES, we don\u2019t care much about the function and its relationship with the inputs or parameters. Some million numbers (parameters of the model) go into the algorithm and it spits out 1 value (e.g. loss in supervised setting; reward in case of Reinforcement Learning). We try to find the best set of such numbers which returns good values for our optimization problem. We are optimizing a function J(\u03b8) with respect to the parameters \u03b8, just by evaluating it without making any assumptions about the structure of J, and hence the name \u2018black-box optimization\u2019. Let\u2019s dig deep into the implementation details!", "To start with, we randomly generate the parameters and tweak it such that the parameters work better slightly. Mathematically, at each step we take a parameter vector \u03b8 and generate a population of, say, 100 slightly different parameter vectors \u03b8\u2081,\u03b8\u2082\u2026\u03b8\u2081\u2080\u2080 by jittering \u03b8 with Gaussian noise. We then evaluate each one of the 100 candidates independently by running the model and based on the output value evaluate the loss or the objective function. We then select top N best performing elite parameters, N can be say 10, and take the mean of these parameters and call it our best parameter so far. We then repeat the above process by again generating 100 different parameters by adding Gaussian noise to our best parameter obtained so far.", "Thinking in terms of natural selection, we are creating a population of parameters (species) randomly and selecting the top parameters that perform well based on our objective function (also known as fitness function). We then take combine the best qualities of these parameters by taking their mean (this is a crude way but it still works!) and call it our best parameter. We then recreate the population by mutating this parameter by adding random noise and repeat the whole process till convergence.", "Let\u2019s go through a simple example in Python to get a better understanding. I tried to add details related to numerical stability as well for few of the things. Please read the comments! We will start by loading the required libraries and the MNIST Handwritten digit dataset.", "This is how the images look like,", "We will start by defining our model, which will be a single layer neural network with only forward pass.", "We will now define our function which will take a model as input and update its parameters.", "Results: After training for 200 iterations the test accuracy was ~ 85% and cross-entropy loss was ~ 0.28. This is comparable to a single layer neural network trained with back propagation. Note that we haven\u2019t even used decay here as n_iter was set to 1.", "ES are very simple to implement and don\u2019t require gradients. Just by injecting noise into our parameters we are able to search the parameter space. Even though we have solved it for a supervised problem for the ease of understanding, it is more suited for Reinforcement learning scenarios where one has to estimate the gradient of the expected reward by sampling.", "Hope you enjoyed reading this post!", "Also for more technical blogs check my website: Dig Deep ML", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Aerospace Engineer | Artist. I can talk about Machine Learning all day long!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1b78b9d48385&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://abhijeetbiswas2212-ab.medium.com/?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": ""}, {"url": "https://abhijeetbiswas2212-ab.medium.com/?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "Abhijeet Biswas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd659cd425508&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&user=Abhijeet+Biswas&userId=d659cd425508&source=post_page-d659cd425508----1b78b9d48385---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b78b9d48385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b78b9d48385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1703.03864", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/CMA-ES", "anchor_text": "https://en.wikipedia.org/wiki/CMA-ES"}, {"url": "https://www.deeplearningbook.org/contents/numerical.html", "anchor_text": "https://www.deeplearningbook.org/contents/numerical.html"}, {"url": "https://www.digdeepml.com", "anchor_text": "Dig Deep ML"}, {"url": "https://openai.com/blog/evolution-strategies/", "anchor_text": "OpenAI Blog post"}, {"url": "https://blog.otoro.net/2017/10/29/visual-evolution-strategies/", "anchor_text": "Otoro\u2019s blog"}, {"url": "https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html", "anchor_text": "Lilian\u2019s blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1b78b9d48385---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1b78b9d48385---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1b78b9d48385---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1b78b9d48385---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/evolutionary-algorithms?source=post_page-----1b78b9d48385---------------evolutionary_algorithms-----------------", "anchor_text": "Evolutionary Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b78b9d48385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&user=Abhijeet+Biswas&userId=d659cd425508&source=-----1b78b9d48385---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b78b9d48385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&user=Abhijeet+Biswas&userId=d659cd425508&source=-----1b78b9d48385---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b78b9d48385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1b78b9d48385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1b78b9d48385---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1b78b9d48385--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1b78b9d48385--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1b78b9d48385--------------------------------", "anchor_text": ""}, {"url": "https://abhijeetbiswas2212-ab.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://abhijeetbiswas2212-ab.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhijeet Biswas"}, {"url": "https://abhijeetbiswas2212-ab.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "16 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd659cd425508&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&user=Abhijeet+Biswas&userId=d659cd425508&source=post_page-d659cd425508--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd659cd425508%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-evolution-strategy-1b78b9d48385&user=Abhijeet+Biswas&userId=d659cd425508&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}