{"url": "https://towardsdatascience.com/pyspark-data-manipulation-tutorial-8c62652f35fa", "time": 1683016190.0415099, "path": "towardsdatascience.com/pyspark-data-manipulation-tutorial-8c62652f35fa/", "webpage": {"metadata": {"title": "Pyspark Data Manipulation Tutorial | by Armando Rivero | Towards Data Science", "h1": "Pyspark Data Manipulation Tutorial", "description": "This tutorial is meant for data people with some Python experience that are absolute Spark beginners. It will help you installing Pyspark and launching your first script. You\u2019ll learn about Resilient\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ArmandoRiveroPi/pyspark_tutorial/blob/master/tutorial_part_1_data_wrangling.py", "anchor_text": "tutorial_part_1_data_wrangling.py", "paragraph_index": 7}, {"url": "https://stackoverflow.com/questions/42540169/pyspark-pass-multiple-columns-in-udf", "anchor_text": "they can take several columns as input", "paragraph_index": 33}, {"url": "https://stackoverflow.com/questions/39699107/spark-rdd-to-dataframe-python", "anchor_text": "this Stack Overflow question", "paragraph_index": 37}, {"url": "https://archive.ics.uci.edu/ml/datasets/adult", "anchor_text": "https://archive.ics.uci.edu/ml/datasets/adult", "paragraph_index": 46}, {"url": "https://github.com/ArmandoRiveroPi/pyspark_tutorial", "anchor_text": "repository", "paragraph_index": 47}, {"url": "https://jdbc.postgresql.org/download.html", "anchor_text": "driver", "paragraph_index": 53}, {"url": "https://github.com/ArmandoRiveroPi/pyspark_tutorial/blob/master/tutorial_part_1_reading_from_database.py", "anchor_text": "different file", "paragraph_index": 54}], "all_paragraphs": ["This tutorial is meant for data people with some Python experience that are absolute Spark beginners. It will help you installing Pyspark and launching your first script. You\u2019ll learn about Resilient Distributed Datasets (RDDs) and dataframes, the main data structures in Pyspark. We discuss some foundational concepts cause I think they will spare you confusion and debugging later. You\u2019ll learn to do data transformations and read from files or databases.", "The main reason to learn Spark is that you will write code that could run in large clusters and process big data. This tutorial only talks about Pyspark, the Python API, but you should know there are 4 languages supported by Spark APIs: Java, Scala, and R in addition to Python. Since Spark core is programmed in Java and Scala, those APIs are the most complete and native-feeling.", "The advantage of Pyspark is that Python has already many libraries for data science that you can plug into the pipeline. That, together with the fact that Python rocks!!! can make Pyspark really productive. For instance, if you like pandas, know you can transform a Pyspark dataframe into a pandas dataframe with a single method call. You could then do stuff to the data, and plot it with matplotlib.", "Over time you might find Pyspark nearly as powerful and intuitive as pandas or sklearn and use it instead for most of your work. Just give Pyspark a try and it could become the next big thing in your career.", "There are many articles on how to create Spark clusters, configure Pyspark to submit scripts to them and so on. All of this is needed to do high performance computation on Spark. However, in most companies they\u2019ll have data or infrastructure engineers that will maintain the clusters and you will only code and run the scripts. So you probably will never be installing a Spark cluster, but if you ever need to, you can deal with the problem when it becomes a problem. OTOH, what if I told you that you can get started in a few minutes with a single command?", "This will download and install Pyspark on your machine so you can learn the ropes and then, when you get a cluster, it will be relatively easy to use it.", "To support this tutorial I created this GitHub repository:", "Most of the code in the examples is better organized on the tutorial_part_1_data_wrangling.py file.", "Before getting up to speed a little gotcha. I have found Pyspark will throw errors if I don\u2019t also set some environment variables at the beginning of my main Python script.", "After that you can create the spark session", "Trust me now, this is pretty much all you need to get started.", "Alright, so we can get into the real matter here. First, you must know that Spark is complex. Not only it includes a lot of code for doing many data processing tasks, but it also promises to be highly efficient and error free while executing code distributed across hundreds of machines. You don\u2019t really need this complexity at the beginning, what you need is to abstract things away and forget the gritty details. Something important though is the distinction between the driver and the workers. The driver is the Python (single) process where you issue the orders for workers. The workers are Spark processes that follow the driver\u2019s orders. You might think of the workers as being \u201cin the cloud\u201d (the Spark cluster). Spark assumes that big data will be distributed among the workers which together have enough memory and processing capacity to deal with it. The driver is not expected have enough resources to hold this amount of data. That\u2019s why you need to explicitly say when you want to move data to the driver. We\u2019ll get to that in a moment but beware that in real applications you should be very careful with the amount of data you bring to the driver.", "The RDDs (Resilient Distributed Datasets) are one of the most important data structures in Spark, and the basis of dataframes. You can think of them as \u201cdistributed\u201d arrays. In many regards they behave like lists, with a few details we\u2019ll discuss bellow.", "So, how to create an RDD? The most straightforward way is to \u201cparallelize\u201d a Python array.", "To start interacting with your RDD, try things like:", "This will bring the first 2 values of the RDD to the driver. The count method will return the length of the RDD", "If you want to send all the RDD data to the driver as an array you can use collect", "Be careful though, as we said before, in real applications this could make the driver collapse, as the RDD could be gigabytes in size. In general prefer to use take passing the amount of rows you want and invoke collect only when you\u2019re sure the RDD or dataframe is not too big.", "If you know pandas or R dataframes you\u2019ll have a very good idea of what Spark dataframes stand for. They represent tabular (matrix) data with named columns. Deep inside, they are implemented with an RDD of Row objects, which are somewhat similar to a Python named tuple. The greatest power of dataframes is they make you able to put your SQL thinking right into action. We\u2019ll talk about dataframe manipulation later, but let\u2019s start creating a dataframe so you can play with it.", "The data matrix comes first and the column names later.", "You can try the take, count and collect methods as in the RDD case; take and collect will give you a list of Row objects. But to me the most user friendly display method would be show:", "It will print a table representation of the dataframe with the first n rows.", "One key difference with Python lists is that RDDs, (and also dataframes), are immutable. Immutable data is often required in concurrent applications and functional languages.", "Let\u2019s discuss what immutability means in Python. Say you do this:", "Here the interpreter first creates a name \u201ca\u201d that points to a list object and in the second line it modifies that very same object. However, when you do", "the interpreter first creates the name \u201cst\u201d that points to a string object, then it creates a brand new string object (at a different location in memory) with \u201cmy string is pretty\u201d and points the name \u201cst\u201d to that new object. This happens because strings are immutable, you can\u2019t modify them in place.", "In the same way RDDs and dataframes can\u2019t be modified in place, so when you do", "my_rdd stays the same. You will need to do", "To get the name \u201cmy_rdd\u201d pointed to the transformed RDD object.", "A more shocking difference with ordinary Python might confuse you at the beginning. Sometimes you\u2019ll notice that a very heavy operation happens instantly. But later you do something little (like printing the first value of the RDD) and it seems to take forever.", "Let me try to explain this problem simplifying a lot. In Spark there\u2019s a distinction between transformations and actions. When you change a dataframe that\u2019s a transformation, however, when you actually consume the data (eg df.show(1)) that\u2019s an action. Transformations are lazy loaded, they don\u2019t run when you call them. They are executed when you consume their results via an action. Then all the needed transformations will be planned, optimized and run together. So when you see operations that look instantaneous, even though they\u2019re heavy, it\u2019s because they are transformations and they\u2019ll be run later.", "User Defined Functions let you use Python code to operate on dataframe cells. You create a regular Python function, wrap it in a UDF object and pass it to Spark, it will care of making your function available in all the workers and scheduling its execution to transform the data.", "First you create a Python function, it could be a method in an object, that\u2019s also a function. Then you create an UDF object. The annoying part is that you need to define the output type, something we aren\u2019t really used to in Python. To be really effective with UDFs you\u2019ll need to learn those types, specially the composite MapType (like dictionaries) and ArrayType (like lists). The benefit is that then you can pass this UDF to the dataframe, tell it which column it will be operating on, and you\u2019ll get fantastic things done without leaving the comfort of your old Python.", "One of the main limitations with UDFs, though, is that although they can take several columns as input, they can\u2019t change the row as a whole. If you want to work with the whole row, you\u2019ll need the RDD map.", "It\u2019s a lot like using a UDF, you also pass a regular Python function. But in this case, the function will be receiving a full Row object instead of column values. It will be expected to return a full Row as well. This will give you the ultimate power over your rows, with a couple of caveats. First: Row object are immutable, so you need to create a whole new Row and return it. Second: you need to convert the dataframe to an RDD and back again. Fortunately neither of these problems are hard to overcome.", "Let me show you a function that will logarithmically transform all the columns in your dataframe. As a nice touch it also transforms each column name as \u2018log(column_name)\u2019. I find the easiest to start by getting the row into a dictionary with row.asDict(). The function has a bit of Python magic like dictionary comprehensions and key-word arguments packing (the double stars). Hope you understand all of it.", "This by itself won\u2019t do anything. You need to execute the map.", "You\u2019ll notice this is a chained method call. First you call rdd, it will give you the underlying RDD where the dataframe rows are stored. Then you apply map on this RDD, where you pass your function. To close you call toDF() that transforms an RDD of rows into a dataframe. See this Stack Overflow question for further discussion.", "Since dataframes represent tables, naturally they are endowed with SQL-like operations. I\u2019ll be referring just a few of them to get you excited, but you can expect to find almost isomorphic functionality. Calling select will return a dataframe with only some of the original columns.", "This call to where will return a dataframe with only the rows where the value for column1 is 3.", "This call to join will return a dataframe that is, well\u2026, a join of df and df1 through column1 in the same way INNER JOIN would do in SQL.", "In case you need to perform right or left joins, in our example df is like the left table and df1, the right one. Outer joins are also possible.", "But on top of that, in Spark you can execute SQL much more directly. You can create a temporal view out of a dataframe", "And then perform a query on top of that view", "These queries will return a new dataframe with the corresponding column names and values.", "The column abstractions supports several direct operations like arithmetic, for instance, say you want to add column1 plus the multiplication of colum2 and column3. Then you can", "For this example we will need to read from a CSV file. I downloaded the adult.data file from https://archive.ics.uci.edu/ml/datasets/adult", "I moved the file to the data folder in my repository and renamed it to adult.data.csv. I copied the column names from adult.names into a list:", "After reading from the file I set the column names using this list, since the file lacks headers.", "After that you can try the following for some quick descriptive statistics", "To get an aggregation use groupBy together with the agg method. For instance, the following will get you a dataframe with the work hours average and standard deviation by age group. We also sort the dataframe by age.", "It\u2019s very likely that you keep your data in a relational database, handled by a RDBMS like MySQL or PostgreSQL. If that\u2019s the case, don\u2019t worry, Spark has ways of interacting with many kinds of data storage. I bet you can google your way through most IO needs you may have.", "To connect to a RDBMS you need a JDBC driver, that would be a jar file Spark can use to talk to the database. I will show you a PostgreSQL example.", "First you need to download the driver.", "In my GitHub repo I put the JDBC driver in the bin folder, but you can have it in any path. Also, you should know the code in this example is at a different file in the repo. I had to do this because unlike the other file, this one will throw errors, as the database credentials are of course fake. You\u2019d need to replace them with your database\u2019s to make it run.", "Now you\u2019ll start Pyspark with an extra step:", "This creates a session aware of the driver. You can then read from the database like this (replacing the fake configs with your real ones):", "Then you can create a transformed dataframe any way you want and write the data back to the database (maybe at a different table).", "The writing modes according to the documentation are:", "I hope you got encouraged to start learning Pyspark right away. I mean, it will take a few weeks to get productive, depending on how much time per day you can afford, but after that you could perform a lot of your data processing with Pyspark. In case you can\u2019t choose what you use at work, at least you can learn it in your free time. My intro barely scratches the surface and mostly hopes to give you an enticing bite. It\u2019s not much more difficult than pandas or sklearn.", "Become fluent at Spark, and it could open you the gates of big data.", "Part 2 will cover basic Classification and Regression.", "PySpark Recipes by Raju Kumar Mishra. Apress, 2018.", "\u201cLearning is the new knowing\u201d Physicist by training, in love with programming."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c62652f35fa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://armando-rivero.medium.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Armando Rivero"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3fb636c333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&user=Armando+Rivero&userId=b3fb636c333&source=post_page-b3fb636c333----8c62652f35fa---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c62652f35fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&user=Armando+Rivero&userId=b3fb636c333&source=-----8c62652f35fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c62652f35fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&source=-----8c62652f35fa---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://pixabay.com/photos/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=839831", "anchor_text": "Free-Photos"}, {"url": "https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=839831", "anchor_text": "Pixabay"}, {"url": "https://github.com/ArmandoRiveroPi/pyspark_tutorial", "anchor_text": "ArmandoRiveroPi/pyspark_tutorialIntroduction to data manipulation and machine learning in pyspark GitHub is home to over 50 million developers working\u2026github.com"}, {"url": "https://github.com/ArmandoRiveroPi/pyspark_tutorial/blob/master/tutorial_part_1_data_wrangling.py", "anchor_text": "tutorial_part_1_data_wrangling.py"}, {"url": "https://stackoverflow.com/questions/42540169/pyspark-pass-multiple-columns-in-udf", "anchor_text": "they can take several columns as input"}, {"url": "https://stackoverflow.com/questions/39699107/spark-rdd-to-dataframe-python", "anchor_text": "this Stack Overflow question"}, {"url": "https://archive.ics.uci.edu/ml/datasets/adult", "anchor_text": "https://archive.ics.uci.edu/ml/datasets/adult"}, {"url": "https://github.com/ArmandoRiveroPi/pyspark_tutorial", "anchor_text": "repository"}, {"url": "https://jdbc.postgresql.org/download.html", "anchor_text": "driver"}, {"url": "https://github.com/ArmandoRiveroPi/pyspark_tutorial/blob/master/tutorial_part_1_reading_from_database.py", "anchor_text": "different file"}, {"url": "https://spark.apache.org/docs/latest/api/python/", "anchor_text": "https://spark.apache.org/docs/latest/api/python/"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----8c62652f35fa---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8c62652f35fa---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----8c62652f35fa---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-manipulation?source=post_page-----8c62652f35fa---------------data_manipulation-----------------", "anchor_text": "Data Manipulation"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----8c62652f35fa---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c62652f35fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&user=Armando+Rivero&userId=b3fb636c333&source=-----8c62652f35fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c62652f35fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&user=Armando+Rivero&userId=b3fb636c333&source=-----8c62652f35fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c62652f35fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3fb636c333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&user=Armando+Rivero&userId=b3fb636c333&source=post_page-b3fb636c333----8c62652f35fa---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F22b233be652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&newsletterV3=b3fb636c333&newsletterV3Id=22b233be652a&user=Armando+Rivero&userId=b3fb636c333&source=-----8c62652f35fa---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Written by Armando Rivero"}, {"url": "https://armando-rivero.medium.com/followers?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "38 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3fb636c333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&user=Armando+Rivero&userId=b3fb636c333&source=post_page-b3fb636c333----8c62652f35fa---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F22b233be652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpyspark-data-manipulation-tutorial-8c62652f35fa&newsletterV3=b3fb636c333&newsletterV3Id=22b233be652a&user=Armando+Rivero&userId=b3fb636c333&source=-----8c62652f35fa---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/implementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6?source=author_recirc-----8c62652f35fa----0---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=author_recirc-----8c62652f35fa----0---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=author_recirc-----8c62652f35fa----0---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Armando Rivero"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8c62652f35fa----0---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/implementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6?source=author_recirc-----8c62652f35fa----0---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Implementing a data pipeline by chaining Python iteratorsIn this article I\u2019ll talk about how to process a collection of items in Python through several steps with relative efficiency and\u2026"}, {"url": "https://towardsdatascience.com/implementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6?source=author_recirc-----8c62652f35fa----0---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "\u00b79 min read\u00b7Mar 16, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&user=Armando+Rivero&userId=b3fb636c333&source=-----b5887c2f0ec6----0-----------------clap_footer----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/implementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6?source=author_recirc-----8c62652f35fa----0---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=-----8c62652f35fa----0-----------------bookmark_preview----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c62652f35fa----1---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----8c62652f35fa----1---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----8c62652f35fa----1---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8c62652f35fa----1---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c62652f35fa----1---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c62652f35fa----1---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8c62652f35fa----1---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----8c62652f35fa----1-----------------bookmark_preview----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8c62652f35fa----2---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----8c62652f35fa----2---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----8c62652f35fa----2---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8c62652f35fa----2---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8c62652f35fa----2---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8c62652f35fa----2---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8c62652f35fa----2---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----8c62652f35fa----2-----------------bookmark_preview----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/building-a-fast-wordpress-development-stack-with-docker-and-bedrock-2944efd18f12?source=author_recirc-----8c62652f35fa----3---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=author_recirc-----8c62652f35fa----3---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=author_recirc-----8c62652f35fa----3---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Armando Rivero"}, {"url": "https://armando-rivero.medium.com/building-a-fast-wordpress-development-stack-with-docker-and-bedrock-2944efd18f12?source=author_recirc-----8c62652f35fa----3---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "Building a fast WordPress development stack with Docker and Bedrockimprove your WordPress coding practices"}, {"url": "https://armando-rivero.medium.com/building-a-fast-wordpress-development-stack-with-docker-and-bedrock-2944efd18f12?source=author_recirc-----8c62652f35fa----3---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": "7 min read\u00b7Mar 19, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F2944efd18f12&operation=register&redirect=https%3A%2F%2Farmando-rivero.medium.com%2Fbuilding-a-fast-wordpress-development-stack-with-docker-and-bedrock-2944efd18f12&user=Armando+Rivero&userId=b3fb636c333&source=-----2944efd18f12----3-----------------clap_footer----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/building-a-fast-wordpress-development-stack-with-docker-and-bedrock-2944efd18f12?source=author_recirc-----8c62652f35fa----3---------------------0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2944efd18f12&operation=register&redirect=https%3A%2F%2Farmando-rivero.medium.com%2Fbuilding-a-fast-wordpress-development-stack-with-docker-and-bedrock-2944efd18f12&source=-----8c62652f35fa----3-----------------bookmark_preview----0317e225_8bec_4ab0_b1e3_d7ddcbd1370d-------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "See all from Armando Rivero"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/@antoniolui?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/@antoniolui?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Tony Lui"}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Applying Custom Functions in PySparkHow to Use Spark UDFs and Row-wise RDD Operations"}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "\u00b75 min read\u00b7Nov 10, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F337d63bf32a5&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40antoniolui%2Fapplying-custom-functions-in-pyspark-337d63bf32a5&user=Tony+Lui&userId=a2544693db86&source=-----337d63bf32a5----0-----------------clap_footer----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/@antoniolui/applying-custom-functions-in-pyspark-337d63bf32a5?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F337d63bf32a5&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40antoniolui%2Fapplying-custom-functions-in-pyspark-337d63bf32a5&source=-----8c62652f35fa----0-----------------bookmark_preview----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-test-pyspark-etl-data-pipeline-1c5a6ab6a04b?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/@edwin.tan?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/@edwin.tan?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Edwin Tan"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-test-pyspark-etl-data-pipeline-1c5a6ab6a04b?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "How to Test PySpark ETL Data PipelineValidate big data pipeline with Great Expectations"}, {"url": "https://towardsdatascience.com/how-to-test-pyspark-etl-data-pipeline-1c5a6ab6a04b?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "\u00b76 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c5a6ab6a04b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-test-pyspark-etl-data-pipeline-1c5a6ab6a04b&user=Edwin+Tan&userId=484e02c96aa7&source=-----1c5a6ab6a04b----1-----------------clap_footer----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-test-pyspark-etl-data-pipeline-1c5a6ab6a04b?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c5a6ab6a04b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-test-pyspark-etl-data-pipeline-1c5a6ab6a04b&source=-----8c62652f35fa----1-----------------bookmark_preview----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8c62652f35fa----0---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----8c62652f35fa----0-----------------bookmark_preview----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://syal-anuj.medium.com/?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://syal-anuj.medium.com/?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Anuj Syal"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "12 Must-Have Skills to become a Data EngineerThe Essential Skills for a Successful Data Engineering Career"}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "\u00b78 min read\u00b7Jan 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F35b100dbee0a&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2F12-must-have-skills-to-become-a-data-engineer-35b100dbee0a&user=Anuj+Syal&userId=df3997c527b4&source=-----35b100dbee0a----1-----------------clap_footer----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----8c62652f35fa----1---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35b100dbee0a&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2F12-must-have-skills-to-become-a-data-engineer-35b100dbee0a&source=-----8c62652f35fa----1-----------------bookmark_preview----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----8c62652f35fa----2---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/?source=read_next_recirc-----8c62652f35fa----2---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/?source=read_next_recirc-----8c62652f35fa----2---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Josue Luzardo Gebrim"}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----8c62652f35fa----2---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Data Quality in Python Pipelines!Discover What It Is And How To Achieve Data Quality In Your Data Streams!"}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----8c62652f35fa----2---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "\u00b714 min read\u00b7Mar 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4ad1e8eb6603&operation=register&redirect=https%3A%2F%2Fjlgjosue.medium.com%2Fdata-quality-in-python-pipelines-4ad1e8eb6603&user=Josue+Luzardo+Gebrim&userId=9f59dfc0edf7&source=-----4ad1e8eb6603----2-----------------clap_footer----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----8c62652f35fa----2---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4ad1e8eb6603&operation=register&redirect=https%3A%2F%2Fjlgjosue.medium.com%2Fdata-quality-in-python-pipelines-4ad1e8eb6603&source=-----8c62652f35fa----2-----------------bookmark_preview----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----8c62652f35fa----3---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://liamjhartley.medium.com/?source=read_next_recirc-----8c62652f35fa----3---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://liamjhartley.medium.com/?source=read_next_recirc-----8c62652f35fa----3---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Liam Hartley"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----8c62652f35fa----3---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----8c62652f35fa----3---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "The Data Engineering Interview GuideThe best preparation tool for your Data Engineering interview with questions and answers by a former Data Engineering Manager"}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----8c62652f35fa----3---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": "\u00b712 min read\u00b7Dec 27, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2F710b621f3ff1&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fthe-data-engineering-interview-guide-710b621f3ff1&user=Liam+Hartley&userId=1ad19b8961a6&source=-----710b621f3ff1----3-----------------clap_footer----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/the-data-engineering-interview-guide-710b621f3ff1?source=read_next_recirc-----8c62652f35fa----3---------------------58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F710b621f3ff1&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Fthe-data-engineering-interview-guide-710b621f3ff1&source=-----8c62652f35fa----3-----------------bookmark_preview----58cac3f1_99d7_4382_9de3_ef4fdb46b6cc-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----8c62652f35fa--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}