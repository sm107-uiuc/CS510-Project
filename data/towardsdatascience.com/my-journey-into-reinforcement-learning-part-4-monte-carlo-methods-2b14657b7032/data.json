{"url": "https://towardsdatascience.com/my-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032", "time": 1683003494.701086, "path": "towardsdatascience.com/my-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032/", "webpage": {"metadata": {"title": "Monte Carlo Methods. Representing and exploring real-world\u2026 | by Reuben Kavalov | Towards Data Science", "h1": "Monte Carlo Methods", "description": "Welcome to another chapter to my studies of reinforcement learning. Up until this point, we\u2019ve laid out the foundations of reinforcement learning, describing known environments using finite Markov\u2026"}, "outgoing_paragraph_urls": [{"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "Reinforcement Learning: An Introduction by Sutton and Barto", "paragraph_index": 28}, {"url": "https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-", "anchor_text": "RL Course by David Silver", "paragraph_index": 29}], "all_paragraphs": ["Welcome to another chapter to my studies of reinforcement learning. Up until this point, we\u2019ve laid out the foundations of reinforcement learning, describing known environments using finite Markov Decision Processes (MDPs). This is all fine and dandy, until we stumble across a situation where everything about the environment might not be known. This is more interesting because it is much more realistic and akin to real life environments (if only things were as simple as gridworld). I\u2019ll post resources at the bottom of this blog, as usual. Let\u2019s dive in.", "Monte Carlo (MC) methods do not require the entire environment to be known in order to find optimal behavior. The term \u201cMonte Carlo\u201d is broadly used for any estimation method that involves a significant random component. In our case, all they rely on is experience \u2014 repeated sequences of states, actions, and rewards \u2014 from interaction with the environment. We divide these interactions into episodes, in order to be able to define beginnings and ends to these sequences. We can use the same concept from the last blog to evaluate a policy, but the difference is key: whereas last time we computed value functions based on knowledge of an MDP, this time we learn value functions from sample returns with the MDP. As we did before, we\u2019ll start with the computation of the value function and action-value function for a fixed arbitrary policy \u03c0, then move to policy improvement, and then to the control problem and its solution by general policy iteration.", "Recall that the value of a state is the expected cumulative future discounted reward, starting from that state. To estimate it from experience, we can simply average the returns observed after visits to that state. According to the law of large numbers, as many returns are observed, the average shall eventually converge to the expected value. Now, this average can be obtained both from the first visit to the particular state, or from every visit. These two MC methods are very similar but have slight nuanced properties. Let\u2019s take a look at first-visit MC prediction.", "Let\u2019s use a real-world situation to assist in implementing this. Blackjack is a card game familiar to most, and is simple to learn. At the start of each round, the player and the dealer are both dealt two cards. The player can see only one of the dealers cards until the end of the round. The object of the game is to get the value of the cards to be as high as possible, without going over 21. All face cards count as 10, and an ace can count as 1 or 11. The player may \u201chit\u201d, receiving another card, or may \u201cstick\u201d, to stop accepting cards, which begins the dealers turn. The dealer plays by a fixed strategy: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes \u201cbust\u201d, or over 21, the player wins. Otherwise, the outcome (win, lose, or draw) is determined by who is closer to 21.", "An important note is that if the player holds an ace that can be counted as 11 without going bust, then the ace is considered usable. In this case, it\u2019s always counted as 11 because counting it as 1 would make the sum 11 or less, which the player should hit on every time. Therefore, the player makes the decision based on the following observations: his current sum (12\u201321), the dealer\u2019s showing card (ace-10), and whether or not he holds a usable ace. This totals up to 200 possible states.", "We can think of this as an episodic finite MDP, which each round being an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a round are zero, which means those terminal rewards are considered the returns:", "We won\u2019t be discounting in our case, so \u03b3 = 1.", "Let\u2019s consider we are given a policy that sticks if the player\u2019s sum is 20 or 21, and hits otherwise. In order to find the state-value function, we can simulate many games following the policy and average the returns for each state.", "As you can tell by the charts above, the value goes up substantially when the player sum is 20 or greater, and with more games played the values of the rest of the states are flattened out. Having a usable ace was pretty rare and so it took more episodes to get a more accurate value function, as seen on the charts on the left.", "Now let\u2019s move on to using Monte Carlo estimation for control, or approximating optimal policies. The main idea is the same as with dynamic programming: generalized policy iteration (GPI). The policy evaluation is done exactly as above, and policy improvement is done by making the policy greedy with respect to the current value function, which is now the action-value function. Action-value functions are needed when a model is not available, since we need to estimate the value of each action to suggest a policy. Therefore, we look to estimate q\u204e. For an action-value function q, the corresponding greedy policy is the one that chooses an action with maximal action-value:", "When performing GPI in gridworld, we used value iteration, iterating through policy evaluation only once between each step of policy improvement. For Monte Carlo policy iteration, the observed returns after each episode are used for policy evaluation, and then the policy is improved at all states that were visited during the episode.", "To ensure exploration, there are two approaches used in Monte Carlo: on-policy methods and off-policy methods. On-policy methods evaluate or improve the policy that is used to make decisions, and off-policy methods evaluate or improve a policy different from the one used to generate the data. In on-policy control methods the policy is generally soft, meaning that \u03c0(a|s) > 0 for all states and actions, but gradually shifts towards a deterministic optimal policy. The on-policy method we\u2019ll look at next will employ an \u03b5-greedy policy, which means that we will randomly select an action with a small probability \u03b5. The beauty of an \u03b5-greedy policy is how simple the concept and implementation is, as well as how effective it is in its guarantee to make progress in being better or as good as the policy we started with. This is called the policy improvement theorem:", "So, without further ado, here is the pseudocode for an on-policy first-visit model-free Monte Carlo control algorithm:", "Upon implementing the algorithm into our blackjack environment and simulating 500,000 episodes, the Monte Carlo algorithm finds us an optimal policy through the iteratively updated action-value function, which also converges to optimality; seen below.", "So now that we\u2019ve covered on-policy learning, let\u2019s move onto off-policy learning. Remember, this means that we evaluate a target policy \u03c0(a|s) while following a separate, behavior policy b(a|s). There are several reasons to employ this type of learning for an agent. One is so that the agent can learn from observing other agents, like humans, for example. The agent can observe how a human performs a certain task and just from that experience can learn from it/improve it. Another very important motivation for off-policy learning is that we want to find an optimal, deterministic policy that doesn\u2019t explore at all, but we need exploratory behavior in order to find it. With off-policy learning we can learn about multiple policies while following one policy.", "The technique we\u2019ll be going over now is importance sampling, a way to estimate expected values under one distribution given samples from another. We apply it to off-policy learning by getting the importance-sampling ratio, which is the weighted returns according to the relative probability of their trajectories occurring under the target and behavior policies. It can be expressed as", "p is the state-transition probability function. Note that although those probabilities are generally unknown (since we\u2019re talking about an unknown environment), they are identical in the numerator and denominator and therefore cancel out. This is important because it means that the importance-sampling ratio only relies on the two policies and the sequence, and not on the MDP.", "So, we are trying to estimate the expected returns under the target policy, but only have the returns G\u209c from the behavior policy, which have the wrong expectation. By bringing in the importance-sampling ratio it transforms the returns to have the right expected value:", "It becomes convenient to number time steps across episode boundaries, meaning that if the first episode ends at time 10, then the next episode begins at t=11. That was we can use time-step numbers as reference for specific steps in specific episodes. We can denote the set of all time steps in which a state s is visited as T(s) for an every-visit method, which we\u2019ll be using below. For a first-visit method, T(s) would only include the time steps that were first visits to s within their episodes. We also denote T(t) as the first time of termination following time t, with G\u209c denoting the return after t up through T(t). Using a simple average, importance sampling works to estimate the value function by scaling the returns by the importance-sampling ratios and averaging the results:", "That is ordinary importance sampling, but the following exercise will be using weighted importance sampling, which uses a weighted average:", "or zero if the denominator is zero. The difference in the two types of importance sampling is seen in their biases and variances. Ordinary importance sampling is unbiased but has generally unbounded variance, and in practice that variance causes the target and behavior policies struggle to find common ground. That is why, usually, the weighted estimator is strongly preferred.", "So, what if we were able to implement Monte Carlo prediction methods on an episode-by-episode basis? This is called incremental implementation. We do this by averaging returns, but how do we do this for off-policy, weighted importance sampling methods? We have to form a weighted average of the returns.", "If we have a sequence of returns G\u2081, G\u2082, \u2026, G\u2099\u208b\u2081, all starting in the same state and each having a corresponding random weight W\u1d62, we wish to estimate", "and update it as we obtain an additional return G\u2099. We also must maintain a cumulative sum C\u2099 of the weights given to the first n returns for each state. The update rule for V\u2099 is", "And so, the final pseudocode of this blog post shows an off-policy Monte Carlo control method based on GPI and weighted importance sampling, estimating \u03c0\u204e and q\u204e. The target policy \u03c0 is the greedy policy with respect to Q, and the behavior policy b is \u03b5-soft to ensure an infinite number of returns for each pair of state and action. The policy \u03c0 converges to optimal at all encountered states even though actions are selected according to policy b, which can change between/within episodes.", "Upon applying this algorithm to the blackjack example, the chart for the optimal value function looks, of course, the same as above, so I won\u2019t post it again. This has been a very exciting look at GPI within Monte Carlo methods, and next I\u2019ll be looking at prediction & control using temporal difference-learning as well as Q-learning!", "Thank you very much for reading, I look forward to the adventure continuing.", "\u201cI only get this one stream of data we call life\u201d \u2014 David Silver", "Reinforcement Learning: An Introduction by Sutton and Barto", "RL Course by David Silver on YouTube", "Data scientist and machine learning engineer with a passion for connecting people through technology and information."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2b14657b7032&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/a-journey-into-r-l", "anchor_text": "A Journey Into Reinforcement Learning"}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----2b14657b7032--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Reuben Kavalov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11db47030451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&user=Reuben+Kavalov&userId=11db47030451&source=post_page-11db47030451----2b14657b7032---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b14657b7032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&user=Reuben+Kavalov&userId=11db47030451&source=-----2b14657b7032---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b14657b7032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&source=-----2b14657b7032---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/analytics-vidhya/my-journey-into-reinforcement-learning-part-1-dijkstras-algorithm-in-python-53408bc1c7c8", "anchor_text": "Part 1"}, {"url": "https://medium.com/@reubena.kavalov/my-journey-into-reinforcement-learning-part-2-markov-decision-processes-55ede33478f2", "anchor_text": "Part 2"}, {"url": "https://medium.com/@reubena.kavalov/my-journey-into-reinforcement-learning-part-3-dynamic-programming-3cb8a8d0815c", "anchor_text": "Part 3"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "http://incompleteideas.net/book/RLbook2018.pdf"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "http://incompleteideas.net/book/RLbook2018.pdf"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "http://incompleteideas.net/book/RLbook2018.pdf"}, {"url": "http://incompleteideas.net/book/RLbook2018.pdf", "anchor_text": "Reinforcement Learning: An Introduction by Sutton and Barto"}, {"url": "https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-", "anchor_text": "RL Course by David Silver"}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "Reinforcement Learning Github"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----2b14657b7032---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/monte-carlo-method?source=post_page-----2b14657b7032---------------monte_carlo_method-----------------", "anchor_text": "Monte Carlo Method"}, {"url": "https://medium.com/tag/python?source=post_page-----2b14657b7032---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----2b14657b7032---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/a-journey-into-r-l?source=post_page-----2b14657b7032---------------a_journey_into_r_l-----------------", "anchor_text": "A Journey Into R L"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b14657b7032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&user=Reuben+Kavalov&userId=11db47030451&source=-----2b14657b7032---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b14657b7032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&user=Reuben+Kavalov&userId=11db47030451&source=-----2b14657b7032---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b14657b7032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----2b14657b7032--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11db47030451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&user=Reuben+Kavalov&userId=11db47030451&source=post_page-11db47030451----2b14657b7032---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce29c814d50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&newsletterV3=11db47030451&newsletterV3Id=ce29c814d50b&user=Reuben+Kavalov&userId=11db47030451&source=-----2b14657b7032---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Written by Reuben Kavalov"}, {"url": "https://medium.com/@reubena.kavalov/followers?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "134 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11db47030451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&user=Reuben+Kavalov&userId=11db47030451&source=post_page-11db47030451----2b14657b7032---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce29c814d50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-journey-into-reinforcement-learning-part-4-monte-carlo-methods-2b14657b7032&newsletterV3=11db47030451&newsletterV3Id=ce29c814d50b&user=Reuben+Kavalov&userId=11db47030451&source=-----2b14657b7032---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b?source=author_recirc-----2b14657b7032----0---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=author_recirc-----2b14657b7032----0---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=author_recirc-----2b14657b7032----0---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Reuben Kavalov"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2b14657b7032----0---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b?source=author_recirc-----2b14657b7032----0---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Value Function Approximation \u2014 Prediction AlgorithmsLearning methods for large, unknown environments"}, {"url": "https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b?source=author_recirc-----2b14657b7032----0---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "\u00b75 min read\u00b7Mar 22, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98722818501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-function-approximation-prediction-algorithms-98722818501b&user=Reuben+Kavalov&userId=11db47030451&source=-----98722818501b----0-----------------clap_footer----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b?source=author_recirc-----2b14657b7032----0---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98722818501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvalue-function-approximation-prediction-algorithms-98722818501b&source=-----2b14657b7032----0-----------------bookmark_preview----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2b14657b7032----1---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2b14657b7032----1---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2b14657b7032----1---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2b14657b7032----1---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2b14657b7032----1---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2b14657b7032----1---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2b14657b7032----1---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----2b14657b7032----1-----------------bookmark_preview----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2b14657b7032----2---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----2b14657b7032----2---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----2b14657b7032----2---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2b14657b7032----2---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2b14657b7032----2---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2b14657b7032----2---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2b14657b7032----2---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----2b14657b7032----2-----------------bookmark_preview----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/analytics-vidhya/my-journey-into-reinforcement-learning-part-1-dijkstras-algorithm-in-python-53408bc1c7c8?source=author_recirc-----2b14657b7032----3---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=author_recirc-----2b14657b7032----3---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=author_recirc-----2b14657b7032----3---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Reuben Kavalov"}, {"url": "https://medium.com/analytics-vidhya?source=author_recirc-----2b14657b7032----3---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "Analytics Vidhya"}, {"url": "https://medium.com/analytics-vidhya/my-journey-into-reinforcement-learning-part-1-dijkstras-algorithm-in-python-53408bc1c7c8?source=author_recirc-----2b14657b7032----3---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "My Journey Into Reinforcement Learning (Part 1) \u2014 Djikstra\u2019s Algorithm in PythonMachine Learning is pushing boundaries of what we are able to do with data, the predictive powers we have, and the different types of AI\u2026"}, {"url": "https://medium.com/analytics-vidhya/my-journey-into-reinforcement-learning-part-1-dijkstras-algorithm-in-python-53408bc1c7c8?source=author_recirc-----2b14657b7032----3---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": "\u00b76 min read\u00b7Jan 13, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-vidhya%2F53408bc1c7c8&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fmy-journey-into-reinforcement-learning-part-1-dijkstras-algorithm-in-python-53408bc1c7c8&user=Reuben+Kavalov&userId=11db47030451&source=-----53408bc1c7c8----3-----------------clap_footer----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/analytics-vidhya/my-journey-into-reinforcement-learning-part-1-dijkstras-algorithm-in-python-53408bc1c7c8?source=author_recirc-----2b14657b7032----3---------------------1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53408bc1c7c8&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fmy-journey-into-reinforcement-learning-part-1-dijkstras-algorithm-in-python-53408bc1c7c8&source=-----2b14657b7032----3-----------------bookmark_preview----1b7f0387_1d11_4cc5_b054_5f3f1e170cbb-------", "anchor_text": ""}, {"url": "https://medium.com/@reubena.kavalov?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "See all from Reuben Kavalov"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----2b14657b7032----0-----------------bookmark_preview----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----1-----------------clap_footer----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----2b14657b7032----1-----------------bookmark_preview----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----0-----------------clap_footer----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----2b14657b7032----0---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----2b14657b7032----0-----------------bookmark_preview----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----1-----------------clap_footer----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2b14657b7032----1---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----2b14657b7032----1-----------------bookmark_preview----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81?source=read_next_recirc-----2b14657b7032----2---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://eligijus-bujokas.medium.com/?source=read_next_recirc-----2b14657b7032----2---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://eligijus-bujokas.medium.com/?source=read_next_recirc-----2b14657b7032----2---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Eligijus Bujokas"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2b14657b7032----2---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81?source=read_next_recirc-----2b14657b7032----2---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "The Values of Actions in Reinforcement Learning using Q-learningThe Q-learning algorithm implemented from scratch in Python"}, {"url": "https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81?source=read_next_recirc-----2b14657b7032----2---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "\u00b710 min read\u00b7Feb 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb4b03be5c81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81&user=Eligijus+Bujokas&userId=d61597e07b4d&source=-----cb4b03be5c81----2-----------------clap_footer----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81?source=read_next_recirc-----2b14657b7032----2---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb4b03be5c81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81&source=-----2b14657b7032----2-----------------bookmark_preview----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e?source=read_next_recirc-----2b14657b7032----3---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----2b14657b7032----3---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----2b14657b7032----3---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Kay Jan Wong"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2b14657b7032----3---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e?source=read_next_recirc-----2b14657b7032----3---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "6 Reinforcement Learning Algorithms ExplainedIntroduction to reinforcement learning terminologies, basics, and concepts (model-free, model-based, online, offline RL)"}, {"url": "https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e?source=read_next_recirc-----2b14657b7032----3---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": "\u00b714 min read\u00b7Nov 25, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F237a79dbd8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-reinforcement-learning-algorithms-explained-237a79dbd8e&user=Kay+Jan+Wong&userId=fee8693930fb&source=-----237a79dbd8e----3-----------------clap_footer----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e?source=read_next_recirc-----2b14657b7032----3---------------------dfe32ab5_b055_4823_a5ca_133457f69571-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F237a79dbd8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-reinforcement-learning-algorithms-explained-237a79dbd8e&source=-----2b14657b7032----3-----------------bookmark_preview----dfe32ab5_b055_4823_a5ca_133457f69571-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2b14657b7032--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----2b14657b7032--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}