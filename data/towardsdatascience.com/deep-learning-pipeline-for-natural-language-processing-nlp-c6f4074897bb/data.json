{"url": "https://towardsdatascience.com/deep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb", "time": 1683016298.801383, "path": "towardsdatascience.com/deep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb/", "webpage": {"metadata": {"title": "Deep learning pipeline for Natural Language Processing (NLP) | by Bauyrjan Jyenis | Towards Data Science", "h1": "Deep learning pipeline for Natural Language Processing (NLP)", "description": "In this article, I will explore the basics of the Natural Language Processing (NLP) and demonstrate how to implement a pipeline that combines a traditional unsupervised learning algorithm with a deep\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/bauyrjanj/NLP-TwitterData", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://medium.com/@divyeshardeshana/create-twitter-developer-account-app-4ac55e945bf4", "anchor_text": "post", "paragraph_index": 6}, {"url": "https://www.kaggle.com/bauyrjanj/2020-us-election-tweets-unlabeled", "anchor_text": "Kaggle", "paragraph_index": 15}, {"url": "https://github.com/bauyrjanj/NLP-TwitterData", "anchor_text": "here", "paragraph_index": 58}, {"url": "https://www.linkedin.com/in/bjenis/", "anchor_text": "LinkedIn", "paragraph_index": 58}], "all_paragraphs": ["In this article, I will explore the basics of the Natural Language Processing (NLP) and demonstrate how to implement a pipeline that combines a traditional unsupervised learning algorithm with a deep learning algorithm to train unlabeled large text data. Hence, the main objective is going to be to demonstrate how to set up that pipeline that facilitates collection and creation of raw text data, preprocessing and categorizing an unlabeled text data to finally training and evaluating deep learning models in Keras.", "After reading this tutorial, you will be able to perform the following:", "Find my Jupyter notebooks with Python code in my GitHub here.", "Roll up your sleeves, we have a lot of work to do, let\u2019s get started\u2026..", "At the time of doing this project, the US 2020 election was just around the corner and it made sense to do sentiment analysis of tweets related to the upcoming election to learn about the kind of opinions and topics being discussed in Twitter world just about 2 weeks prior to the election day. Twitter is a great source for unfiltered opinions as opposed to the typical filtered news we see from the major media outlets. As such, we are going to build our own dataset by collecting tweets from Twitter using Twitter API and the python package Tweepy.", "Before getting started with streaming data from Twitter, you must have the following:", "Setting up a Twitter account and retrieving your Twitter API consumer keys is out of scope of this article. Should you need help with those, check out this post.", "Tweepy could be installed via pip install in Jupyter notebook, the following one line code will do the trick.", "Once installed, go ahead and import the package into your notebook.", "In this section, I will show you how to set your data streaming pipeline with the use of Twitter API, Tweepy and a custom function. We can achieve this in 3 steps:", "Now that the environment is set up, you are ready to start streaming live tweets from Twitter. Before doing that, identify some keywords that you would like to use to collect the relevant tweets of interest for you. Since I will be streaming tweets related to the US election, I have picked some relevant keywords such as \u201cUS election\u201d, \u201cTrump\u201d, \u201cBiden\u201d etc.", "Our goal is going to be to collect at least 400,000 tweets to make it a large enough text data and it is computationally taxing to collect all of that at one go. Thus, I will be setting up a pipeline in a way to efficiently stream data in chunks. Notice from the above custom function, it will listen and stream up to 20,000 tweets at most in each chunk. As such, in order to collect over 400,000 tweets, we will need to run at least 20 chunks.", "Here is how the code looks for the chunk that listens and streams live tweets into a pandas DataFrame:", "As mentioned, in order to collect 400,000 tweets, you will have to run the above code at least 20 times and save the collected tweets in separate pandas dataframe that will be concatenated later to consolidate all of the tweets into a single dataset.", "Now that you have exported your combined dataset into a CSV file, you can use it for the next steps of cleaning and visualizing the data.", "I have donated the dataset I have created and made it publicly available in Kaggle.", "In this section, we will be cleaning the data we just collected. Before the data could be visualized, the dataset must be cleaned and transformed into a format that can be efficiently visualized. Given the dataset with 440,000 rows, one has to find an efficient way of reading and cleaning it. To do that, pandas chunksize attribute could be used to read data from CSV file into a pandas dataframe in chunks. Also, we can specify the names of columns we are interested in reading as opposed to reading the dataset with all of its columns. With the chunksize and smaller amount of columns of interest, the large dataset could be read into a dataframe rather efficiently and quickly without having to need for other alternatives such as distributed computing with PySpark on a cluster.", "To transform the dataset into a shape required for visualization, the following basic NLP techniques will be applied:", "The approach I will follow to implement above steps is as follows:", "Let\u2019s see all of these in action\u2026", "The chunksize in this case is 50,000 and that is how pandas will read 50,000 tweets in each chunk and apply the cleaning steps on them before reading the next batch and so on and so forth.", "After this process, the dataset is going to be clean and be ready for visualization. To avoid performing the data wrangling steps each time you open up your notebook, you could simply export the tidy data to an external file and use that in the future. For the large dataset, it is more efficient to export them into JSON file as opposed to CSV.", "Here is how the tidy data looks like:", "Now that the data is clean, let\u2019s visualize and understand the nature of our data. A few obvious things we can look at are as follows:", "It appears that the number of words in each tweet range from 1 to 19 words and on average falls between 10 to 12 words.", "The average number of characters in a word in a tweet appear to range from 3 to 14 characters and on average occurring between 5 to 7 characters. People probably choose short words to express their opinions in the best way they can within the 280 character limit set by Twitter.", "As expected, the words \u201ctrump\u201d and \u201cbiden\u201d dominate the 2020 US election related tweets that were pulled between Oct 15 and Oct 16.", "From visualizing the data, notice that the words are not lemmatized. Lemmatization is a process of turning the words into their base or dictionary form. It is a common technique used in NLP and in machine learning in general. So in the next step, we are going to lemmatize the tokens with the following code.", "Now, let\u2019s save the lemmatized tokens into another JSON file to make it easy to use in the next step in the pipeline.", "Prior to undertaking the steps in the preprocessing and modeling, let\u2019s review and be clear on our approach for the next steps in the pipeline. Before we can make predictions as to which category a tweet belongs to, we must first tag the raw tweets with categories. Remember, we streamed our data as raw tweets from Twitter hence the data didn\u2019t come as labeled. Therefore, it is appropriate to implement the following approach:", "In this section, the objective is going to be to tag the tweets with 2 labels corresponding to positive or negative sentiments. Then further preprocess and transform the labeled text data into a format that can be further used to train deep learning models.", "There are many different ways to categorize unlabeled text data and such methods include, but not limited to, using SVM, hierarchical clustering, cosine similarity and even Amazon Mechanical Turk. In this example, I will show you another simpler, perhaps not the most accurate, way of doing a quick and dirty way of categorizing the text data. To do that, I will first conduct a sentiment analysis with VADER to determine whether the tweets are positive, negative or neutral. Next, I will use a simple k-means clustering algorithm to cluster the tweets based on the calculated compound value drawn from the values of how positive, negative and neutral the tweets are.", "Let\u2019s look at the dataset first", "The column \u201ctokens_back_to_text\u201d is the lemmatized tokens transformed back to the text format and I am going to be using this column from the tidy dataset for the creation of the sentiments with SenitmentIntensityAnalyzer from VADER package.", "Here is how the first 5 rows of the sentiments look like", "Now, I will use the column \u201ccompound\u201d from the above dataframe and will feed it into a k-means clustering algorithm to categorize the tweets with 0 or 1 representing \u201cnegative\u201d or \u201cpositive\u201d sentiment respectively. That is, I will tag the tweets with the corresponding compound value of greater and equal to 0.05 as a positive sentiment while the value less than 0.05 will be tagged as a negative sentiment. There is no hard rule here, it is just how I set up my experiment.", "Here is how you can implement a text labeling job with k-means clustering algorithm from scikit-learn in python. Remember to give the same index to both labels and the original dataframe where you have your tweets/texts.", "Looking at the counts of labels for 0 and 1, notice that the dataset is imbalanced where more than twice the tweets are labeled as 1. This is going to impact the performance of the model, so we have to balance our dataset prior to training our models.", "In addition, we could also possibly identify topics of the tweets from each of the categories with the help of a powerful NLP algorithm called \u201cLatent Dirichlet Allocation\u201d which could provide an intuition of topics in the negative and positive tweets. I will show this in a separate article at a later time. For now, let\u2019s use the categories 0s and 1s for the sake of this exercise. So now, we have successfully converted our problem to a supervised learning problem and next we will proceed onto training deep learning models using our now labeled text data.", "We have a pretty large dataset with over 400,000 tweets with more than 60,000 unique words. Training RNNs with multiple hidden layers on such a large dataset is computationally taxing and may take days (if not weeks) if you attempted to train them on a CPU. One common approach for training deep learning models is to use GPU optimized machines for higher training performance. In this exercise, we are going to use Amazon SageMaker p2.xlarge instance that comes pre-loaded with TensorFlow backend and CUDA. We will be using Keras interface to the TensorFlow.", "Let\u2019s get started, we will apply the following steps.", "The dataset must be transformed into a numerical format as machine learning algorithms do not understand natural language. Before vectorizing the data, let\u2019s look at the text format of the data.", "As seen from the distribution of data between 0 and 1 from above, the data now looks to be pretty balanced compared to what it was before.", "Now that the data is balanced, we are ready to split the data into training and test sets. I am going to put away 30% of the dataset for testing.", "In this section, I will show you how to implement a couple of variants of RNN deep learning architecture, 3 layer SimpleRNN and 3 layer LSTM architectures. The activation function is set to \u201ctanh\u201d for both SimpleRNN and LSTM layers by default, so let\u2019s just leave it at its default setting. I will use all the 65,125 unique words as the size of the vocabulary, will limit the maximum length of each input to 14 words as it is consistent with the maximum length of word in a tweet and will set the output dimension of the embedding matrix to 32.", "The dropout layers will be used to enforce regularization terms to control overfitting. As my dataset is labeled in binary class, I will use binary crossentropy as the loss function. In terms of an optimizer, Adam optimizer is a good choice and I will include accuracy as the metric. I will run 10 epochs on the training set in which 70% of the training set will be used to train the model while the remaining 30% will be used for validation. This is not to be mixed up with the test set we kept away.", "The SimpleRNN model results are shown as follows \u2014 10 epochs:", "3 layer LSTM model will be trained with dropout layers. I will run 10 epochs on the training set in which 70% of the training set will be used to train the model while the remaining 30% will be used for validation. This is not to be mixed up with the test set we kept away.", "Model summary is shown as follows:", "LSTM model results are shown as follows \u2014 10 epochs:", "Now, let\u2019s plot the models\u2019 performances over time and look at their accuracies and losses across 10 epochs.", "Notice the training accuracy, the SimpleRNN model quickly starts overfitting and the validation accuracy has high variance for the same reason.", "As seen from the accuracy and loss plot of LSTM, the model is overfitting and the validation accuracy not only has high variance but also dropping quickly for the same reason.", "In this project, I attempted to demonstrate how to set up a deep learning pipeline that predicts the sentiments of the tweets related to the 2020 US election. To do that, I first created my own dataset by scraping raw tweets via Twitter API and Tweepy package.", "Over 440,000 tweets were streamed via Twitter API and stored into a CSV file. After wrangling and visualizing the data, a traditional clustering algorithm, k-means clustering in this case, was used to tag the tweets with two different labels, representing positive or negative sentiments. That is, the problem was converted into a supervised learning problem before training the deep learning models with the data. Then the dataset was split into training and test sets.", "Later, the training set was used to train SimpleRNN and LSTM models respectively and were evaluated using the loss and accuracy curves from the model performances in each epoch. Overall, both models appear to be performing as they should and are likely to be overfitting the data according to what is seen from accuracy plots and as such I suggest the following recommendations for the next step.", "3. How to build deep learning architectures, compile and fit in Keras", "4. How to apply basic NLP concepts and techniques to a text data", "Again, find my Jupyter notebooks with Python code in my GitHub here and let\u2019s connect on LinkedIn.", "Enjoy deep learning :) I\u2019d love to hear your feedback and suggestions, so please either use the clap button or comment in the section below.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc6f4074897bb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://b-jyenis.medium.com/?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": ""}, {"url": "https://b-jyenis.medium.com/?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "Bauyrjan Jyenis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fd8605cf6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&user=Bauyrjan+Jyenis&userId=8fd8605cf6ca&source=post_page-8fd8605cf6ca----c6f4074897bb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6f4074897bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6f4074897bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@heyerlein?utm_source=medium&utm_medium=referral", "anchor_text": "h heyerlein"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/bauyrjanj/NLP-TwitterData", "anchor_text": "here"}, {"url": "https://medium.com/@divyeshardeshana/create-twitter-developer-account-app-4ac55e945bf4", "anchor_text": "post"}, {"url": "https://www.kaggle.com/bauyrjanj/2020-us-election-tweets-unlabeled", "anchor_text": "Kaggle"}, {"url": "https://github.com/bauyrjanj/NLP-TwitterData", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/bjenis/", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66", "anchor_text": "NLP to Using RNN and LSTM"}, {"url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "anchor_text": "Sequence Classification with LSTM"}, {"url": "https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47", "anchor_text": "Understanding LSTM"}, {"url": "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/", "anchor_text": "Word Embeddings in Gensim"}, {"url": "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/", "anchor_text": "Lemmatization Approaches"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c6f4074897bb---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c6f4074897bb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c6f4074897bb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/text-mining?source=post_page-----c6f4074897bb---------------text_mining-----------------", "anchor_text": "Text Mining"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c6f4074897bb---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6f4074897bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&user=Bauyrjan+Jyenis&userId=8fd8605cf6ca&source=-----c6f4074897bb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6f4074897bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&user=Bauyrjan+Jyenis&userId=8fd8605cf6ca&source=-----c6f4074897bb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6f4074897bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc6f4074897bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c6f4074897bb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c6f4074897bb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c6f4074897bb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c6f4074897bb--------------------------------", "anchor_text": ""}, {"url": "https://b-jyenis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://b-jyenis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bauyrjan Jyenis"}, {"url": "https://b-jyenis.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "107 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fd8605cf6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&user=Bauyrjan+Jyenis&userId=8fd8605cf6ca&source=post_page-8fd8605cf6ca--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F57108ab6c446&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb&newsletterV3=8fd8605cf6ca&newsletterV3Id=57108ab6c446&user=Bauyrjan+Jyenis&userId=8fd8605cf6ca&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}