{"url": "https://towardsdatascience.com/ml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181", "time": 1683013581.5816362, "path": "towardsdatascience.com/ml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181/", "webpage": {"metadata": {"title": "Must know ML techniques for digital analysts \u2014 Part 1: Association Analysis | by Abhinav Sharma | Towards Data Science", "h1": "Must know ML techniques for digital analysts \u2014 Part 1: Association Analysis", "description": "Business users are increasingly becoming self-service on basic digital analysis and reporting. If you are a digital analyst, it will be a smart move to start to expand/shift towards data science now\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/abhinav-sharma15/market-basket-analysis-instacart", "anchor_text": "github", "paragraph_index": 23}], "all_paragraphs": ["Business users are increasingly becoming self-service on basic digital analysis and reporting. If you are a digital analyst, it will be a smart move to start to expand/shift towards data science now and be equipped to offer more than basic analysis and dash-boarding.", "This was my motivation to learn, apply, and eventually share knowledge around these data science techniques that have been part of incremental data analysis long before \u201cdata science\u201d was a buzz word.", "My approach would be to explain the concept and the use cases it can cover. I will highlight the prerequisites and follow it up with a decent example. My example implementations will be programmed in R.", "In this part 1, I talk about Association Analysis more popularly referred to as Market Basket Analysis. This analysis is relevant for retail and other setups, where a user can add to and eventually purchase multiple products, from a shopping cart. The objective is to better understand what sort of products or items go together well and use that information for better cross-selling, merchandising, or targeted offers.", "Association analysis is a statistical technique that helps you identify top association rules between your products.", "What is an association rule? An example from a grocery transaction would be that the association rule is a recommendation of the form {peanut butter, jelly} => { bread }. It says that, based on the transactions, it\u2019s expected that bread will most likely be present in a transaction that contains peanut butter and jelly. It\u2019s a recommendation to the retailer that there is enough evidence in the database to say that customers who buy peanut butter and jelly will most likely buy bread.", "Association Analysis is simply a search through the data for combinations of items whose statistics are interesting. It helps us establish rules dictating something like \u201cIf A occurs then B is likely to occur as well.\u201d", "But, what are these interesting stats that we have to look for and how should we set their values/thresholds?", "First, we need to consider complexity control: there are likely to be a tremendous number of co-occurrences, many of which might simply be due to chance, rather than to a generalizable pattern. A simple way to control complexity is to place a constraint that such rules must apply to some minimum percentage of the data \u2014 let\u2019s say that we require rules to apply to at least 0.01% of all transactions. This is called the support of the association.", "We also have the notion of \u201clikely\u201d in the association. If a customer buys the jelly then she is likely to buy the bread. Again, we may want to require a certain minimum degree of likelihood for the associations we find. The probability that B occurs when A; it is p(B|A), which in association mining is called the confidence of the rule (not to confuse it with statistical confidence). So we might say we require the confidence to be above some threshold, such as 5% (so that 5% or more of the time, a buyer of A also buys B).", "Just Support and Confidence as a parameter might be misleading for items that are too common/ popular in the basket. It is more likely that popular items are part of the same basket just because they are popular rather than anything else. We need some measure of \u201csurprise\u201d for association analysis. Lift and Leverage are two parameters providing that. The lift of the co-occurrence of A and B is the probability that we actually see the two together, compared to the probability that we would see the two together if they were unrelated to (independent of) each other. As with other uses of the lift we\u2019ve seen, a lift greater than one is the factor by which seeing A \u201cboosts\u201d the likelihood of seeing B as well. An alternative is to look at the difference between these quantities rather than their ratio. This measure is called leverage.", "So, for any items A and B in a transaction -", "As a Market basket analyst, your job is to search for rules with a lift that are greater than 1 backed with high confidence values and often, high support.", "Since we\u2019re using the market basket as an analogy at this point, we should consider broadening our thinking of what might be an item. Why can\u2019t we put just about anything we might be interested in finding associations with into our \u201cbasket\u201d? For example, we might put a user\u2019s location into the basket, and then we could see associations between purchase behavior and locations. For actual market basket data, these sometimes are called virtual items, to distinguish from the actual items that people put into their basket in the store. Association analysis finds and tells us statistically significant observations like \u201cIf A occurs then B is likely to occur as well.\u201d Now, we can replace anything for A and B provided they happened together (can be basketed).", "With the above logic, we have several other applications of association analysis beyond cross-sell opportunities in online e-commerce. It can help us answer questions like:", "There are several algorithmic implementations for association rule mining. Key among them is the apriori algorithm by Rakesh Agrawal and Ramakrishnan Srikanth, introduced in their paper, Fast Algorithms for Mining Association Rules.", "The Apriori algorithm is a commonly-applied technique in computational statistics that identifies itemsets that occur with a support greater than a pre-defined value (frequency) and calculates the confidence of all possible rules based on those itemsets.", "The Apriori algorithm is implemented in the arules package, which can be installed and run in R.", "The algorithm takes as input, transactional data. Transactions are purchases made by a customer on a single visit to a retail store. Typically, transaction data can include the products purchased, quantity purchased, the price, discount, if applied, and a timestamp. A single transaction can include multiple products. It may register information about the user who made the transaction in some cases, where the customer allows the retailer to store his information by joining a rewards program. For mining, the transaction data is first transformed into a binary purchase incidence matrix with columns corresponding to the different items and rows corresponding to transactions. The matrix entries represent the presence (1) or absence (0) of an item in a particular transaction.", "Association mining is based on probability measures hence generating reliable insights from analysis typically requires large volumes of transactional data. Large data sets are difficult to process without highly-scalable storage and compute resources. Usually, you will be doing this exercise sourcing data from your data lake using cloud-based architecture however the inherent principles will remain the same and your objective will be to get data in a transactional format to apply this rule. R has packages to connect to most of the systems and you can even use SQL for data wrangling.", "Let's consider the following scenario -", "A retailer is planning a marketing campaign on a large scale to promote sales. One aspect of his campaign is the cross-selling strategy. Cross-selling is the practice of selling additional products to customers. In order to do that, he wants to know what items/products tend to go together. Equipped with this information, he can now design his cross-selling strategy. He expects us to provide him with a recommendation of top N product associations so that he can pick and choose among them for inclusion in his campaign.", "We will implement a project where we will apply association rule mining to a retail dataset with the final objective of recommending cross-sell items. This project is based on a dataset released by Instacart in 2017. They released over 3 million anonymized orders for the machine learning community to try hands-on. I will be using a subset training dataset for association rule mining (assuming our cross-sell use case). We will have to do some initial data wrangling to get the desired transaction format. Please refer citation below for dataset related information.", "Refer github for the complete code of this project.", "Here is the break down of the code -", "We are provided with 2 files. An orders csv with around 131k orders with order ID and product ID observations and a product file with product ID and product name mapping. First, we will create a transaction dataset containing order ID and associated product name.", "Combining both of them and forming a single transaction dataset -", "Let\u2019s quickly explore our data. We can count the number of unique transactions and the number of unique products:", "We have 131209 transactions and 39123 individual products. There is no information about the number of products purchased in a transaction. We have used the dplyr library to perform these aggregate calculations, which is a library used to perform efficient data wrangling on data frames.", "We begin with reading our transactions stored in the data frame and create an arules data structure called transactions.", "Looking at the parameters of read.transactions, the function used to create the transactions object. For the first parameter, file, we pass our file where we have the transactions from the retailer. The second parameter, format, can take any of two values, single or basket, depending on how the input data is organized. In our case, we have a tabular format with two columns\u2013one column representing the unique identifier for our transaction and the other column for a unique identifier representing the product present in our transaction. This format is named single by arules. Refer to the arules documentation for a detailed description of all the parameters.", "On inspecting the newly created transactions object transaction.obj:", "We can see that there are 131209 transactions and 39121 products. They match the previous count values from the dplyr output.", "We can explore the most frequent items, that is, the items that are present in most of the transactions and vice versa \u2014 the least frequent items and the items present in many fewer transactions?", "The itemFrequency function in the arules package comes to our rescue. This function takes a transaction object as input and produces the frequency count (the number of transactions containing this product) of the individual products:", "In the preceding code, we print the most and the least frequent items in our database using the itemFrequency function. The itemFrequency function produces all the items with their corresponding frequency and the number of transactions in which they appear. We wrap the sort function over itemFrequency to sort this output; the sorting order is decided by the decreasing parameter. When set to TRUE, it sorts the items in descending order based on their transaction frequency. We finally wrap the sort function using the head function to get the top 10 most/least frequent items.", "The Banana product is the most frequently occurring across 18726 transactions. The itemFrequency method can also return the percentage of transactions rather than an absolute number if we set the type parameter to relative instead of absolute.", "The purpose of this project is to focus on the method rather than the output. If you will refer to the dataset source \u2014 the dataset includes orders from many different retailers and is a heavily biased subset of Instacart\u2019s production data, and so is not a representative sample of their products, users, or their purchasing behavior.", "Another convenient way to inspect the frequency distribution of the items is to plot them visually as a histogram. The arules package provides the itemFrequencyPlot function to visualize the item frequency:", "The item frequency plot should give us some idea about the threshold that we should maintain for support. Usually, we should select a support threshold where the long tail starts.", "Now that we have successfully created the transaction object, let\u2019s proceed to apply the apriori algorithm to this transaction object.", "The apriori algorithm works in two phases. Finding frequent itemsets is the first phase of the association rule mining algorithm. A group of product IDs is called an itemset. The algorithm makes multiple passes into the database; in the first pass, it finds out the transaction frequency of all the individual items. These are itemsets of order 1. We will introduce the first interest measure, Support, here.", "Now, in the first pass, the algorithm calculates the transaction frequency for each product. At this stage, we have order 1 itemsets. We will discard all those itemsets that fall below our support threshold. The assumption here is that items with a high transaction frequency are more interesting than the ones with a very low frequency. Items with very low support are not going to make for interesting rules further down the pipeline. Using the most frequent items, we can construct the itemsets as having two products and find their transaction frequency, that is, the number of transactions in which both the items are present. Once again, we discard all the two product itemsets (itemsets of order 2) that are below the given support threshold. We continue this way until we have exhausted them.", "The apriori method is used in arules to get the most frequent items. This method takes two parameters, the transaction.obj and the second parameter, which is a named list. We create a named list called parameters. Inside the named list, we have an entry for our support threshold. We have set our support threshold to 0.005, namely, one percent of the transaction. We settled at this value by looking at the histogram we plotted earlier. By setting the value of the target parameter to frequent itemsets, we specify that we expect the method to return the final frequent itemsets. Minlen and maxlen set lower and upper cut off on how many items we expect in our itemsets. By setting our minlen to 2, we say we don\u2019t want itemsets of order 1. While explaining the apriori in phase 1, we said that the algorithm can do many passes into the database, and each subsequent pass creates itemsets that are of order 1, greater than the previous pass. We also said apriori ends when no higher-order itemsets can be found. We don\u2019t want our method to run till the end, hence by using maxlen, we say that if we reach itemsets of order 10, we stop. The apriori function returns an object of type itemsets.", "It\u2019s good practice to examine the created object, itemset in this case. A closer look at the itemset object should shed light on how we ended up using its properties to create our data frame of itemsets:", "By calling the function label and passing the freq.items object, we retrieve the item names:", "Let\u2019s move on to phase two, where we will induce rules from these itemsets. It\u2019s time to introduce our second interest measure, confidence. Let\u2019s take an itemset from the list given to us from phase one of the algorithm, {Banana, Blueberries}.", "We have two possible rules here:", "Banana => Blueberries: The presence of Banana in a transaction strongly suggests that Blueberries will also be there in the same transaction. Blueberries => Banana: The presence of Blueberries in a transaction strongly suggests that Banana will also be there in the same transaction. How often are these two rules found to be true in our database? The confidence score, our next interest measure, will help us measure this:", "Once again, we use the apriori method; however, we set the target parameter in our parameters named list to rules. Additionally, we also provide a confidence threshold. After calling the method apriori using the returned object rules, we finally build our data frame, rules.df, to explore/view our rules conveniently. Let\u2019s look at our output data frame, rules.df. For the given confidence threshold, we can see the set of rules thrown out by the algorithm:", "Lift is also reflected as another interest measure in the dataframe.", "Alright, we have successfully implemented our association rule mining algorithm; we went under the hood to understand how the algorithm works in two phases to generate rules. We have examined three interest measures: support, confidence, and lift. Finally, we know that the lift can be leveraged to make cross-selling recommendations to our retail customers.", "Given the rule A => B, we explained that lift calculates how many times A and B occur together more often than expected. There are other ways of testing this independence, like a chi-square test or a Fisher\u2019s test. The arules package provides the is.significant method to do a Fisher or a chi-square test of independence. The parameter method can either take the value of fisher or chisq depending on the test we wish to perform.", "We have written a function called find.rules. This function returns the list of top N rules given the transaction and support/confidence thresholds. We are interested in the top 10 rules. We are going to use leverage values for our recommendation.", "The first four entries have a lift value of 2 to 4, indicating that the products are not independent. These rules have support of around 2 percent and the system has 30 percent confidence for these rules. But wait, what about leverage? These items have a leverage of about 1 percentage points. Whatever is driving the co-occurrence results in a one-percentage-point increase in the probability of buying both together over what we would expect simply because they are popular items. Is that sufficient for cross-selling decisions? Maybe yes or no... that\u2019s a business dependent decision.", "For the sake of this example, we recommend that the retailer uses these top products in his cross-selling campaign as, given the lift value, there is a high probability of the customer picking up a {Bag of Organic Bananas} if he picks up an {Organic Hass Avocado}.", "We have also included one other interest measure \u2014 conviction.", "Convicton: Conviction is a measure to ascertain the direction of the rule. Unlike lift, conviction is sensitive to the rule direction. Conviction (A => B) is not the same as conviction (B => A). Conviction, with the sense of its direction, gives us a hint that targeting the customers of Organic Hass Avocado to cross-sell will yield more sales of Bag of Organic Bananas rather than the other way round.", "The plot.graph function is used to visualize the rules that we have shortlisted based on their leverage values. It internally uses a package called igraph to create a graph representation of the rules:", "In the pure vanilla use, arules package uses the frequency of the items in the itemset to measure support. We can replace this by explicitly providing weights for different transactions which can then replace support measures. Doing this can allow us to hardcode certain products in our association rules even though they may not be frequent (by assigning more weight to transactions that contain them).", "In the arules package, the weclat method allows us to use weighted transactions to generate frequent itemsets based on these weights. We introduce the weights through the itemsetinfo data frame in the str(transactions.obj) transactions object.", "If explicit weights are not available, we can use an algorithm called Hyperlink-induced topic search (HITS) to generate one for us. The basic idea of HITS algorithm is to assign weights such that a transaction with a lot of items is considered more important than a transaction with a single item.", "The arules package provides the method (HITS). So for example here, we can use hits to generate weights and then use weclat method to do weighted association ruling\u2026", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2ab198d56181&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab198d56181--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab198d56181--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@abhinavsharma_64776?source=post_page-----2ab198d56181--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhinavsharma_64776?source=post_page-----2ab198d56181--------------------------------", "anchor_text": "Abhinav Sharma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9e994f9ef9cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&user=Abhinav+Sharma&userId=9e994f9ef9cf&source=post_page-9e994f9ef9cf----2ab198d56181---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ab198d56181&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ab198d56181&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@davidveksler", "anchor_text": "David Veksler"}, {"url": "https://unsplash.com/", "anchor_text": "Unsplash"}, {"url": "https://github.com/abhinav-sharma15/market-basket-analysis-instacart", "anchor_text": "github"}, {"url": "https://www.instacart.com/datasets/grocery-shopping-2017", "anchor_text": "https://www.instacart.com/datasets/grocery-shopping-2017"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2ab198d56181---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/digital-analytics?source=post_page-----2ab198d56181---------------digital_analytics-----------------", "anchor_text": "Digital Analytics"}, {"url": "https://medium.com/tag/web-analytics?source=post_page-----2ab198d56181---------------web_analytics-----------------", "anchor_text": "Web Analytics"}, {"url": "https://medium.com/tag/association?source=post_page-----2ab198d56181---------------association-----------------", "anchor_text": "Association"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ab198d56181&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&user=Abhinav+Sharma&userId=9e994f9ef9cf&source=-----2ab198d56181---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ab198d56181&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&user=Abhinav+Sharma&userId=9e994f9ef9cf&source=-----2ab198d56181---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ab198d56181&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab198d56181--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2ab198d56181&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2ab198d56181---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab198d56181--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2ab198d56181--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2ab198d56181--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2ab198d56181--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2ab198d56181--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2ab198d56181--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2ab198d56181--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2ab198d56181--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhinavsharma_64776?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhinavsharma_64776?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhinav Sharma"}, {"url": "https://medium.com/@abhinavsharma_64776/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "27 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9e994f9ef9cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&user=Abhinav+Sharma&userId=9e994f9ef9cf&source=post_page-9e994f9ef9cf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd3f8287a58a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-techniques-to-optimise-digital-analytics-part-1-association-analysis-2ab198d56181&newsletterV3=9e994f9ef9cf&newsletterV3Id=d3f8287a58a&user=Abhinav+Sharma&userId=9e994f9ef9cf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}