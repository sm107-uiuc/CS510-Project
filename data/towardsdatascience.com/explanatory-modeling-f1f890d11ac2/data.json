{"url": "https://towardsdatascience.com/explanatory-modeling-f1f890d11ac2", "time": 1683006440.592863, "path": "towardsdatascience.com/explanatory-modeling-f1f890d11ac2/", "webpage": {"metadata": {"title": "A tutorial to explanatory modeling and statistical causal inference | Towards Data Science", "h1": "A tutorial to explanatory modeling and statistical causal inference", "description": "A hands-on tutorial to explanatory modeling and statistical causal inference with a case study on COVID-19 mortality risk factors and generalized linear models."}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset", "anchor_text": "Kaggle dataset", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/r-tutorial-analyzing-covid-19-data-12670cd664d6", "anchor_text": "This TDS article", "paragraph_index": 0}, {"url": "https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf", "anchor_text": "distinction between the two", "paragraph_index": 3}, {"url": "https://www.datascienceblog.net/post/commentary/inference-vs-prediction/", "anchor_text": "modeling approach", "paragraph_index": 3}, {"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/", "anchor_text": "interpretable model", "paragraph_index": 3}, {"url": "https://www.cambridge.org/core/books/computer-age-statistical-inference/inference-after-model-selection/B6BEB2EA184FAC92CEAC529FE75B7691", "anchor_text": "inference after model selection", "paragraph_index": 4}, {"url": "https://github.com/dimitrics/kaggle-covid19", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://www.cambridge.org/core/books/computer-age-statistical-inference/fisherian-inference-and-maximum-likelihood-estimation/3962E1383087C1A15734030793C66C32", "anchor_text": "well known", "paragraph_index": 14}, {"url": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118856406.app5", "anchor_text": "AIC, BIC", "paragraph_index": 35}, {"url": "https://link.springer.com/article/10.1186/s40537-018-0143-6", "anchor_text": "statistical sin", "paragraph_index": 58}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/", "anchor_text": "MICE algorithm", "paragraph_index": 60}, {"url": "https://www.jstor.org/stable/2984877?seq=1", "anchor_text": "has been characterized", "paragraph_index": 64}, {"url": "https://www.jstor.org/stable/41058949", "anchor_text": "previously cited", "paragraph_index": 64}], "all_paragraphs": ["Perhaps a positive side-effect of the recent pandemic and the associated lockdown is that we get to materialize projects that had been set aside. For a while I\u2019ve wanted to write an article about statistical model interpretation and all I needed was time, motivation, and data. This Kaggle dataset consists of 1085 COVID-19 cases sampled in the city of Wuhan between December 2019 and March 2020, and it was one of the first published datasets to contain records at patient-level. This TDS article provides a nice overview of the data and its fields. Having records at patient-level means that there is enough granularity to derive medical insights about how a person\u2019s individual attributes relate to the risk of dying from exposure to COVID-19.", "This guide to explanatory modeling requires an intermediate understanding of the following topics:", "A mortality risk analysis requires a response variable with records on the event of death. We will consequently try to express this response as a function of predictor variables, which are directly or indirectly related to the effect of COVID-19 on man\u2019s health. The purpose of an explanatory model is to explain rather than predict the outcome of death, where the objective of explanation is the application of statistical inference in order to:", "In other words, we\u2019re interested in a model that will be used for inference rather than prediction or \u2014 to state it more accurately \u2014 one that favors causal inference over predictive inference. It\u2019s crucial to understand the distinction between the two as it determines our rationale behind our choice of modeling approach. Explanatory analysis requires a less flexible (high bias) but more interpretable model, which uses probabilistic reasoning to approximate a hypothesized data generation process. After a model selection procedure is performed, the resulting \u201cbest\u201d model may include predictors that have a significant effect on the response albeit without necessarily improving predictive accuracy.", "Seemingly, a good choice of model would be the binomial logit model, commonly known as binary logistic regression. Given the lack of tools for inference after model selection as well as our intention to validate causal hypotheses, we will use domain knowledge to handpick the independent variables which are assumed to have a direct or indirect effect on the event of death; then we will quantify their effect by fitting our data into the data generation process that our model assumes. The purpose of this article is to implement a pipeline for explanatory modeling, which shows how to use a small and sparse dataset to derive medical insights on COVID-19. For this purpose, I\u2019ve selectively combined theoretical concepts with code snippets in the R programming language. In order to reach the desirable result, the following steps are taken:", "The codebase used in this work can be found here.", "Let\u2019s start off by loading the necessary libraries and data.", "In a predictive modeling scenario we could, at this point, pick any amount of variables we want and then let a model selection algorithm decide for us which ones to consider. In an explanatory modeling scenario, however, inference after algorithmic model selection is not a viable option. An intense variable selection process is expected to bias coefficient p-values and deprive us from using the model\u2019s asymptotic properties. There are various ways to overcome this phenomenon but, in order to explain causal effects, we must rely on domain knowledge to isolate the variables that we consider impactful. For example, an obvious selection of variables would be those related to a patient\u2019s age, gender, country of origin, date on which symptoms started, date on which hospitalization took place, and whether he/she is native to Wuhan. Variables such as \u2018symptom\u2019, \u2018recovered\u2019, \u2018hosp_visit_date\u2019 and \u2018exposure_start\u2019 seem relevant but are simply too sparse to be of any use.", "After isolating the most relevant predictors, our new dataset should look like this:", "We have a binary response variable that represents the event of death and we\u2019re ready to dig deeper into the modeling process.", "Without intending to go into too much detail, I think it\u2019s important to go through some basic steps in order to understand how model interpretability is obtained.", "Recall that in a binomial logit model, the response variable is a random vector that follows a Binomial or Bernoulli distribution:", "This is an important difference from linear regression where the stochastic element comes from the covariates and not the response. Our model makes several other assumptions and the first step in the analysis should be to make sure that our scenario meets them.", "The first assumption on the distribution of the response has been already laid down, but it can be expanded with the assumption that its expected value E[Y=1]=np, for all i, can be linked to the linear combination of the covariates:", "for observations i=1,\u2026,N, explanatory variables j=1,\u2026,M, and link function g. Secondly, explanatory variables X are assumed to be independent of each other and uncorrelated. Thirdly, observations too are assumed to be independent of each other and uncorrelated. Some further assumptions have to do with the limitations of Maximum Likelihood Estimation on the size of data. It is well known that MLE tends to overfit when the number of parameters increases beyond a certain limit but, luckily, our data is nowhere near that danger zone.", "A more generic assumption is that our data is the result of random sampling. This is particularly important in the case of prediction, as the distribution of the response variable (sample class balance) has a direct effect on the estimate of the intercept term and, therefore, on model predictions. On the bright side, slope coefficients remain unaffected. In other words, non-representative class balance in logistic regression has a direct effect on prediction but not interpretation. Since our goal is the latter, we can proceed without making the balanced sample assumption.", "We can now define our model as a Binomial GLM. Recall that any GLM is defined by three components:", "Suppose that random variable Y maps the binary outcome of a COVID-19 case as a Bernoulli trial, such that:", "where p is the probability of observing the outcome of death P(Y=1) in a single patient, N is the number of patients with observed values for Y, and n is the number of trials per patient (in our case, n=1, for all n). This is our stochastic component.", "Let \u03b7 be the expectation of Y, E[Y]=\u03b7, and suppose that there exists a function g(.) such that g(\u03b7) be a linear combination of the covariates:", "For X=x, for all j, the RHS of the above equation is our systematic component.", "We choose g(.)=logit(.) as the link function whose inverse, logistic(.), can squeeze the output of our model inside the [0,1] interval, which also happens to be the desired range for E[Y]=\u03b7=p, for n=1. Therefore, by plugging \u03b7=np=p into our link function g(.), we get:", "from which we derive the final structure of our model:", "Leaving out index i for simplicity\u2019s sake, we have:", "The stochastic component can now be rewritten as:", "NB: This last expression explains why logistic regression is literally a regression and not classification.", "This step involves inspecting variance and sparsity in our dataset, as well as generating new predictors. After having defined our GLM and checked its assumptions, we can move on to inspect class balance in our data:", "The dataset agrees with our prior knowledge on the mortality rate of COVID-19, which places our analysis into the realm of rare event detection. Using domain knowledge on COVID-19 we can instantly consider \u2018age\u2019 and \u2018gender\u2019 as relevant predictors but we can also try to create new predictors. Something that could be informative is the time between the day the first symptoms appeared and the day the individual got hospitalized. We can easily create this predictor by taking the distance in days between hospitalization date and symptom onset date.", "So the variable that counts the days between symptom onset and hospitalization can be added to our model later on in the model selection process. The current state of the data seems to include quite a few missing values (appearing as \u2018NA\u2019) so it would be insightful to visualize the sparsity landscape.", "The above table shows the proportion of missing values in every variable. The same insight is visualized on the barplot below (left image).", "The grid on the right illustrates the proportion of missing values of individual predictions over all values in the dataset. For example, the last row of the matrix, which is full green, tells us that 39% of our datasets cases have no missing values throughout all variables. The row before that tells us that 20% of cases are missing in the first three variables (order of appearance), the row before that shows 15% missing values in the first 5 variables, and so on.", "Clearly, there\u2019s a lot of missing data and it\u2019s scattered all over, so the choice between removal and imputation is not that obvious. Before trying to answer this question, we should try fitting a baseline model using few but substantial predictors. We want to have a model fitted on a dataset that is close to the raw dataset, but instead of imputation we will perform missing value removal. Variables with sparsity or low variance (such as \u2018country\u2019, \u2018hosp_visit_date\u2019, \u2018symptom_onset\u2019) should be left out and, to minimize data removal, variables included into the baseline model should have as little missing values as possible.", "The model selection process will involve fitting several candidate models until we run into the one that\u2019s closest to the \u201ctrue\u201d model. At every comparison, candidate models will be evaluated with respect to specific asymptotic criteria (to be explained).", "We\u2019ll start off with a very simple model that regresses mortality rate against variables \u2018age\u2019 and \u2018gender\u2019.", "Expressing mortality risk as a function of age and gender allows us to rewrite our GLM from equation [1] as:", "We fit the model using R\u2019s standard \u2018glm\u2019 function and inspect goodness-of-fit with the help of the \u2018jtools\u2019 package and some custom functions. We use the \u2018summ\u2019 function from package \u2018jtools\u2019 to get a nicely presented summary of model diagnostics. It should be noted that R\u2019s built-in \u2018glm\u2019 function removes by default all rows with \u2018NA\u2019 values, so we\u2019re being informed in the summary that only 825 out of our total 1082 observations were used in training. We also see reports on goodness-of-fit statistics and criteria such as AIC, BIC, Deviance, which can be used for model selection under certain conditions.", "Let\u2019s provide some context for this output.", "Recall that the deviance is a generalization of the sum of squared residuals in ordinary least squares regression, so it expresses the distance of the current model from the saturated model as a function of the difference of their respective log-likelihoods l(.). Therefore, the (scaled) deviance of the current model is obtained by:", "Naturally, the deviance is always positive and inversely proportional to goodness-of-fit, with D = 0 indicating the perfect fit. The null deviance D0 is the deviance of intercept-only (null) model \u2014 the worst possible model:", "into the above formula. Ds is the deviance of the saturated model \u2014 the best possible model \u2014 a model that has n parameters for n sample size, whose likelihood is equal to 1 and deviance is equal to 0.", "The deviance can be used to assess whether any two models are equal. This can be done by testing the null hypothesis that all the extra coefficients of the larger model are equal to zero. For any given pair of nested models m1 and m2, the comparison can be done by the statistic:", "In other words, for two nested models to be equal, the difference of their corresponding deviances should follow a \u03c7\u00b2 distribution with k degrees of freedom being equal to the difference of their corresponding degrees of freedom. Consequently, this means that we can also test whether an arbitrary model is statistically equal to either the null model (for a very bad fit) or the saturated model (for a very good fit). The code for these comparisons is provided below:", "The above comparison tests the null hypothesis:", "i.e. whether the current model is equal to the intercept-only model. Naturally, we want this null to be rejected in order for the current model not be deemed useless, so we expect a p-value below the accepted decision threshold (conventionally 0.05).", "i.e. whether the current model is statistically equal to the saturated model. Failing to reject the null is the desired outcome, albeit it can\u2019t serve as evidence for the null\u2019s acceptance.", "We can assess the statistical significance of model coefficients by looking at their p-values. There is a lot of discussion these days about the reliability of p-values so I believe some explanation is due about how those p-values are obtained what they really mean. Recall that inference in GLM is asymptotic and the true distributions of fitted coefficients are unknown. The asymptotic properties of MLE give us a Gaussian distribution for model coefficients:", "where where I(\u03b2) denotes the Fisher information matrix. Solving for Normal(0,1) in the RHS, we derive the Wald statistic for j=0,\u2026,M:", "which we use to compute confidence intervals and apply hypothesis tests (as we do with the t-statistic in linear regression). We can, therefore, compute the 100(1\u2212\u03b1)% confidence intervals for the coefficients using:", "Naturally, we can also use the asymptotic distribution of the coefficients to conduct formal hypothesis tests regarding their significance by testing the null hypothesis:", "which uses the Wald statistic and is commonly known as the Wald test.", "McFadden\u2019s pseudo-R\u00b2 is a measure of explained variation defined as:", "where we have the log-likelihood of the current model over 1 minus the log-likelihood of the intercept-only model. This equation is an adaptation of the R\u00b2 formula in linear regression 1\u2212SSE/SST, however, the interpretation of the pseudo-R\u00b2 is the model\u2019s proximity to the best fit, not the percentage of variance explained by the model. A rule of the thumb here is that a value between 0.2\u20130.4 generally indicates a \u201cgood\u201d fit (ranging from good to very good).", "Having gone through some aspects of goodness-of-fit, we realize that our baseline model is objectively not that bad. We\u2019ve got significant p-values for the overall fit as well as one of our coefficients (using the Fisherian 95% confidence level), and we\u2019ve got a decent pseudo-R\u00b2 of 0.20. Furthermore, we can use function \u2018plot_summs\u2019 from package \u2018jtools\u2019 to plot the asymptotic distributions of model coefficients and visualize the results of the Wald test by noticing how the tail of \u03b22 (coefficient of Gender) includes the value of zero and does not reject the null :", "The above density plots can help us understand how asymptotic inference works. Asymptotic confidence intervals and p-values are computed by estimating integrals on this theoretical density plot and \u2014 under the right conditions \u2014 the output of asymptotic inference should match that of any objective non-asymptotic method such as (frequentist) Monte Carlo, bootstrap, or flat-prior Bayesian estimation.", "It would be interesting to use the model\u2019s predictive ability to visualize the effect of age on mortality risk, for both men and women. A reconstitution of the functional relation between mortality risk and age can be seen below:", "This model is fine for a baseline, so we can start building up from here. The obvious next steps are to add more predictors into the model and see whether the fit improves. As previously seen, the bottleneck with this dataset is that we have too many missing values scattered throughout our predictors. If we continue adding variables and removing rows with missing values, our dataset is bound to be dangerously diminished. For example, variables \u2018age\u2019 and \u2018gender\u2019, that we used in our baseline, have already cost us 260 observations. Variable \u2018days_before_hosp\u2019 is intuitively an interesting predictor but if we add it into the baseline model and apply row-wise removal, the overall number of observations in our dataset will be reduced to half its original size. The same goes for variable \u2018from.Wuhan\u2019, which denotes whether the sampled individual was a native to Wuhan (more on the meaning of that predictor later). So if we were to exclude all rows in the dataset that have at least one missing value, we\u2019re going to end up with a much smaller albeit complete dataset. On the other hand, if trust an imputation algorithm to \u201cguess\u201d all of our missing values, we\u2019ll end up with a large portion of our data being the result of simulated observations. Given the current state of affairs, if we\u2019re going to enrich the baseline model with more explanatory variables we\u2019re facing two options:", "Neither of these approaches is necessarily wrong and there is no way for us to know a priori which one fits best our scenario, unless we have a clear understanding of why the data is missing. As we do not have this prior understanding in our case, it would be wise to use both approaches and generate both a reduced and an imputed dataset; then compare a model that was fitted on the former with one that was fitted on the latter. The comparison will not be straight-forward since the use of different datasets (in terms of row inputs) will result to models which will not be comparable by means of likelihood-based criteria such as AIC/BIC, Deviance, and McFadden\u2019s pseudo-R\u00b2. The choice of the better approach will depend on the validation of the \u201cmissing at random\u201d hypothesis. If the results of data removal are similar to those of data imputation we can safely assume that the missing data was in fact \u201cmissing at random\u201d, in which case we will favor the imputed data over the reduced one.", "As previously stated, two interesting variables that could be added the baseline mode are:", "It should be noted that we intentionally refrain from automating this selection process, in an effort of keeping model comparisons down to a minimum. The purpose of this practice is to avoid dwelling into the realms of stepwise selection, a widespread technique which is nowadays wisely regarded as a statistical sin.", "Our next candidate models will be of the following nested form:", "At this point, we must create the imputed dataset. There are plenty of imputation methods to choose from depending on the nature, amount, and distribution of missing values in our data. Under a mild \u201cmissing at random\u201d assumption, we decide that the best method for our scenario would be multiple imputation by chained equation, commonly known as the MICE algorithm. Using the \u2018mice\u2019 library, we can simulate values for all our missing data as shown below:", "Now we have our imputed dataset in \u2018dat1_imp\u2019. Next we\u2019re going to refit the baseline model plus the two new candidate models on the newly imputed dataset and summarize their diagnostics. We\u2019re refitting our baseline model because the multiple imputation has also filled in the missing data on Gender and Age that were previously automatically removed by the \u2018glm\u2019 function. We\u2019re not expecting this to cause a dramatic change on the diagnostics of baseline model yet it\u2019s essential to refit it to the exact same dataset as the new candidate models in order to make all models comparable to each other.", "So now we have three nested models fitted on the same input cases that we can directly compare using likelihood-based criteria. As expected, there is a clear improvement in all aspects as we see both AIC and BIC values drop significantly, while pseudo-R\u00b2 has almost doubled in the third model. It\u2019s safe to conclude that the model with both new variables is the one we should be focusing on.", "The 95% confidence interval plot can be thought of as a \u201cpanoramic\u201d view of the asymptotic distributions of coefficients which allows us to assess the consistency of estimation. Whether two confidence intervals overlap gives us an idea of whether the estimates of the same coefficients differ significantly between two models. This helps us detect the presence of multicollinearity, that may have been caused by the addition of new variables into the model.", "One might wonder, at this point, what is the best way to compare models using likelihood-based measures. As a rule of the thumb, the Akaike information criterion (AIC) has been characterized as the asymptotic equivalent of leave-one-out cross validation, therefore it\u2019s the better metric for the purpose of prediction. The Bayesian information criterion (BIC), on the other hand, has been previously cited to be the better criterion for finding the true model among a set of candidate models, which is what we need for the purpose of explanation. Note that the \u201ctrue model\u201d is the model with the correct functional form and the correct set of regressors regarding what best explain the response, but it is not necessarily the best model for making predictions (some variables required to interpret the response might compromise the precision of estimates without adding predictive value). With that in mind, we are going to be looking at BIC as the criterion for model selection.", "We move onto fitting the same candidate models to a dataset that has undergone data removal.", "The reduced dataset will be created by listwise deletion, i.e. the removal of an input case (observation) with one or more missing values. We create the following using the following code:", "We can see the total number of observations being reduced by 40%. Now let\u2019s see what the diagnostics of the fitted models will look like.", "Given how we reduced our dataset to about half its original size, the fit of this model is surprisingly good. The comparison of asymptotic distributions shows that the diagnostics of the three models fitted on the reduced dataset are as stable as the ones fitted on the imputed dataset. This is a positive result, indicating that the missing data is in fact missing at random as hypothesized, and that the imputation algorithm has done a good job at filling in missing values. This assumption is further strengthened in the next section, where we compare the best candidate models (in terms of BIC) from both datasets.", "As already mentioned, likelihood-based scores are not directly comparable between models fitted on different datasets, still, given our rule of the thumb it\u2019s safe to say a McFadden\u2019s score of 0.56 indicates a very good fit on its own. Plotting the asymptotic distributions of the imputed dataset model\u2019s coefficients next to those of the reduced model is a good way to compare standard errors and confirm that there is no radical change between the two approaches. The estimated coefficients of all predictors were found significant in both approaches, so we\u2019re glad to observe that their inference remains consistent. The point estimate of the coefficient of Gender is higher in the model fitted to the reduced data, but its confidence interval overlaps with the one of the one fitted to the imputed model, therefore the inference between the two estimates is not significantly different.", "By now we have enough evidence to believe that our missing data is, in fact, \u201cmissing at random\u201d (MAR), i.e. the probability that a value is missing depends only on observed values and not on unobserved ones. If the MAR assumption holds, then MICE has done a good job at producing simulated observations that correctly represent the \u201creal\u201d missing data. Consequently, the model on the MICE-imputed dataset is consistent with the model on the reduced (listwise-deleted) dataset and, therefore, it\u2019s safe to assume that both datasets are representative random subsamples of the population. We could, in theory, use any of the two datasets for the next steps of our analysis, but for the reasons of information gain we\u2019ll go with the imputed dataset.", "Having settled to the imputed dataset, we can proceed to the final stage of model selection that requires the creation and addition of a new predictor. We noticed that the addition of variable \u2018from.Wuhan\u2019 to the model improves the fit significantly by reducing BIC and increases pseudo-R\u00b2 by about .20. At this point, it isn\u2019t obvious to us why a variable indicating whether a person is native to Wuhan should be of any relevance to mortality rate, let alone be so influential. Let\u2019s have a closer look at this factor by testing the number of deaths per Wuhan natives versus non-natives.", "There seems to be a striking difference between the mortality rates of those who are from Wuhan and those who are not. If we trust our data, such a counter-intuitive insight should not be ignored, but what could be the reason behind this difference? An unfortunate yet realistic assumption would be that people who are visitors in Wuhan either for business or tourism are, in average, of a higher income than the locals. We can challenge this assumption by comparing the death rates between those who are natives to Wuhan and those who are not. This can be achieved via a simple two-sample \ud835\udc67-test for proportions and visualized as a barplot.", "Clearly, there\u2019s a significant difference in the average mortality rate between Wuhan natives and non-natives. Let\u2019s explore the same test for different covariates, such as the average number of days gone by before hospitalization between the two groups:", "As suspected, there is also a significant difference in this test. The distance between their means may be small (3.70 versus 2.68), but their difference is statistically significant, which further strengthens our hypothesis about a latent connection between the factor \u2018from.Wuhan\u2019 and the individual\u2019s socioeconomic status. At this point we can speculate that variable \u2018from.Wuhan\u2019 acts as confounding factor for another latent variable. In other words, there might be another predictor carrying all socioeconomic information, to which \u2018from.Wuhan\u2019 correlates. A logical assumption would be that this latent predictor is \u2018country\u2019, on the grounds that someone\u2019s ethnicity is directly correlated to both his/her socioeconomic status as well as the event of being a native to Wuhan. We\u2019ve already established that can\u2019t add \u2018country\u2019 into the model because of its low variance, but we can \u2018from.Wuhan\u2019 under the assumption that it acts as its proxy. There has to be another latent variable that is more granular than \u2018from.Wuhan\u2019 and, at the time, is correlated to \u2018country\u2019.", "We will introduce a country-level macroeconomic variable such as GDP (PPP), which expresses a country\u2019s gross domestic product (at purchasing power parity) per capita. If there is a significant difference in the average GDP value per country between groups (from Wuhan, not from Wuhan), then our initial assumption about the patient\u2019s wealth being associated to mortality rate will be further strengthened. Ultimately, this variable should also improve the fit of our model.", "Let\u2019s create a data vector for GDP, where the order of elements corresponds to the order of country names (as listed by \u2018levels(dat$country)\u2019):", "A new column called \u2018gdp\u2019 is now part of the imputed data. Let\u2019s see what happens when we introduce that variable into our model.", "The model with GDP will, therefore, be of the following form:", "As speculated, adding GDP into our model has further improved the fit as shown by a significant reduction in BIC and an increase in pseudo-R\u00b2. Its coefficient may be very small (possibly due to the presence of \u2018from.Wuhan\u2019 in our model) but its impact is nonetheless significant. We can further compare the average GDP per group (Wuhan natives versus non-natives) to assess whether their difference is significant:", "Our hypothesis test confirms that the average GDP (PPP) in deceased patients from Wuhan is significantly different to the average GDP (PPP) from deceased patients who are not from Wuhan. This is a further confirmation in favor of our model\u2019s coefficient p-values.", "So far we\u2019ve confirmed that the imputed dataset is the \u201cbest\u201d dataset, and that the model with predictors \u2018age\u2019, \u2018gender\u2019, \u2018days_before_hosp\u2019, \u2018from.Wuhan\u2019, \u2018gdp\u2019, is the one closest to the \u201ctrue\u201d model. As a last step in our modeling process, we will use computational inference in order to assess coefficient stability and evaluate the homogeneity of our sample.", "So far we\u2019ve been through a conservative model selection process which has led to a model that we believe to be close to the true model. A final step in the validation process would be to test the stability of model coefficients through a bootstrap procedure. If our coefficients are found to be \u201cstable\u201d then our sample can be considered homogeneous and our estimates reliable. The stability of coefficients here is evaluated using two criteria:", "Note that the purpose of bootstrap aggregation (bagging) and resampling, in this context, is not exactly identical to that of Random Forest. We will use random subsampling with replacement to train multiple models, but our goal is to create bagged estimates and bootstrap confidence intervals for model coefficients, not for model predictions. Bagging will be used to produce point estimates for model parameters (bagged coefficients), while the empirical bootstrap distributions and associated bootstrap confidence intervals of those bagged estimates will allow us to evaluate model stability of estimation. In its simplest form, this evaluation consists of testing the hypothesis that the bagged estimates are equal to the asymptotic estimates, which comes down to comparing the asymptotic distribution and the bootstrap distribution of the same parameter. One way to achieve this is by testing the null that the difference between bagged and asymptotic estimates is equal to zero. If we can show that the asymptotic estimate is likely not different from the computational (bootstrap) estimate, we can safely assume that we have converged to the \u201ctrue\u201d coefficients.", "We use our custom function \u2018bagged_glm\u2019 to obtain bootstrap distributions for model parameters. The \u2018alpha\u2019 argument is used in model summary statistics, the \u2018class_balancing\u2019 flag resamples the undersampled class at 50% (not used), the \u2018bag_pct\u2019 argument determines the size of the training bag, and flag \u2018ocv\u2019 activates out-of-bad cross validation (equivalent to Random Forest\u2019s out-of-bag error).", "Let\u2019s have a look at the results of computational inference:", "As we see in the above histograms, the distribution of the difference d between bagged and asymptotic estimates \u03bc2 - \u03bc1 does include zero (blue line) within its bootstrap confidence interval (red lines). Therefore, the null hypothesis:", "fails to be rejected at the 95% significance level. Even though the failure to reject a null hypothesis does not imply its acceptance, this finding combined to all our previous findings should suffice to assume the reliability of our estimation and allow us to move onto model interpretation.", "As the long-awaited step in the analysis, we can finally move onto to interpreting the coefficients of our model. Recall that any binomial logit model (equation [1]) can be expressed as:", "The value of \u03b20 in expression [2] can be translated as the odds of the response being 1, when all continuous regressors are 0 and categorical regressors are at their base level. In our scenario, we can say that \u201cthe average probability of death when days before hospitalization, age of patient, and GDP, are at zero, and the patient is not a native to Wuhan\u201d. Obviously, patient age and GDP being equal to 0 is a nonsensical interpretation but, as previously stated, we\u2019re not interested in the direct interpretation of \u03b20 in this analysis (otherwise we could have used an intercept correction technique). The interpretation of \u03b2j in expression [3] is the one that interests us, and it can be generically interpreted as the percentage change in the odds of the response (being equal to 1) for every unit-increase in regressor j, if x continuous, or for the transition from the base level to the current level if x is categorical; ceteris paribus. For this interpretation to make any sense, we must adapt it to our scenario and look at \u03b2j simply as:", "\u201cThe percentage change in the probability of mortality\u201d,", "for whatever change corresponds to regressor j, all else being equal. We can apply this interpretation to the coefficients of our final model:", "So let\u2019s summarize the coefficients of our model in both raw and transformed form:", "We can keep the transformed coefficient as a factor and interpret the odds of Y to be a number of times higher to those of X, or we can use the percentage change form and say that the probability of Y changes by a certain proportion for every change in X, as explained. Personally, I find the latter interpretation to be more intuitive and below we can see its application on each of our model\u2019s coefficients at the 90% confidence level:", "Data Science & Modeling consultant. Enjoying anything that has to do with data \u2014 my opinions are my own.", "Road to Full Stack Data Science"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff1f890d11ac2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@dimitrics?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dimitrics?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Dimitrios Tziotis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe084b973e038&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=post_page-e084b973e038----f1f890d11ac2---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1f890d11ac2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=-----f1f890d11ac2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1f890d11ac2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&source=-----f1f890d11ac2---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@markusspiske", "anchor_text": "Markus Spiske"}, {"url": "https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset", "anchor_text": "Kaggle dataset"}, {"url": "https://towardsdatascience.com/r-tutorial-analyzing-covid-19-data-12670cd664d6", "anchor_text": "This TDS article"}, {"url": "https://towardsdatascience.com/probability-concepts-explained-probability-distributions-introduction-part-3-4a5db81858dc", "anchor_text": "Probability theory"}, {"url": "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1", "anchor_text": "Statistical estimation"}, {"url": "https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148", "anchor_text": "logistic regression"}, {"url": "https://www.stat.berkeley.edu/~breiman/bagging.pdf", "anchor_text": "bagging"}, {"url": "https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60", "anchor_text": "bootstrap resampling"}, {"url": "https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf", "anchor_text": "distinction between the two"}, {"url": "https://www.datascienceblog.net/post/commentary/inference-vs-prediction/", "anchor_text": "modeling approach"}, {"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/", "anchor_text": "interpretable model"}, {"url": "https://www.cambridge.org/core/books/computer-age-statistical-inference/inference-after-model-selection/B6BEB2EA184FAC92CEAC529FE75B7691", "anchor_text": "inference after model selection"}, {"url": "https://github.com/dimitrics/kaggle-covid19", "anchor_text": "here"}, {"url": "https://www.cambridge.org/core/books/computer-age-statistical-inference/fisherian-inference-and-maximum-likelihood-estimation/3962E1383087C1A15734030793C66C32", "anchor_text": "well known"}, {"url": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118856406.app5", "anchor_text": "AIC, BIC"}, {"url": "https://link.springer.com/article/10.1186/s40537-018-0143-6", "anchor_text": "statistical sin"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/", "anchor_text": "MICE algorithm"}, {"url": "https://www.jstor.org/stable/2984877?seq=1", "anchor_text": "has been characterized"}, {"url": "https://www.jstor.org/stable/41058949", "anchor_text": "previously cited"}, {"url": "https://medium.com/tag/covid-19?source=post_page-----f1f890d11ac2---------------covid_19-----------------", "anchor_text": "Covid-19"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f1f890d11ac2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----f1f890d11ac2---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f1f890d11ac2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/causal-inference?source=post_page-----f1f890d11ac2---------------causal_inference-----------------", "anchor_text": "Causal Inference"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1f890d11ac2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=-----f1f890d11ac2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1f890d11ac2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=-----f1f890d11ac2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1f890d11ac2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@dimitrics?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe084b973e038&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=post_page-e084b973e038----f1f890d11ac2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe084b973e038%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=-----f1f890d11ac2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@dimitrics?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Written by Dimitrios Tziotis"}, {"url": "https://medium.com/@dimitrics/followers?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "10 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe084b973e038&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=post_page-e084b973e038----f1f890d11ac2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe084b973e038%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplanatory-modeling-f1f890d11ac2&user=Dimitrios+Tziotis&userId=e084b973e038&source=-----f1f890d11ac2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f1f890d11ac2----0---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----f1f890d11ac2----0---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----f1f890d11ac2----0---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f1f890d11ac2----0---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f1f890d11ac2----0---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f1f890d11ac2----0---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----0-----------------clap_footer----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f1f890d11ac2----0---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----f1f890d11ac2----0-----------------bookmark_preview----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f1f890d11ac2----1---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----f1f890d11ac2----1---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----f1f890d11ac2----1---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f1f890d11ac2----1---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f1f890d11ac2----1---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f1f890d11ac2----1---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f1f890d11ac2----1---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----f1f890d11ac2----1-----------------bookmark_preview----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f1f890d11ac2----2---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----f1f890d11ac2----2---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----f1f890d11ac2----2---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f1f890d11ac2----2---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f1f890d11ac2----2---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f1f890d11ac2----2---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f1f890d11ac2----2---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----f1f890d11ac2----2-----------------bookmark_preview----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----f1f890d11ac2----3---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----f1f890d11ac2----3---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----f1f890d11ac2----3---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f1f890d11ac2----3---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----f1f890d11ac2----3---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "Time-Series Forecasting: Deep Learning vs Statistics \u2014 Who Wins?A comprehensive guide on the ultimate dilemma"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----f1f890d11ac2----3---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": "\u00b714 min read\u00b7Apr 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----c568389d02df----3-----------------clap_footer----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----f1f890d11ac2----3---------------------fab6ac95_6fcf_4254_a287_c221b03e3946-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&source=-----f1f890d11ac2----3-----------------bookmark_preview----fab6ac95_6fcf_4254_a287_c221b03e3946-------", "anchor_text": ""}, {"url": "https://medium.com/@dimitrics?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "See all from Dimitrios Tziotis"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Arthur Mello"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Exploratory Data Analysis: The Ultimate WorkflowExplore the true potential of your data with Python"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "\u00b716 min read\u00b7Apr 20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fa82b1d21f747&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fexploratory-data-analysis-the-ultimate-workflow-a82b1d21f747&user=Arthur+Mello&userId=9d32d5e0ac40&source=-----a82b1d21f747----0-----------------clap_footer----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82b1d21f747&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fexploratory-data-analysis-the-ultimate-workflow-a82b1d21f747&source=-----f1f890d11ac2----0-----------------bookmark_preview----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----f1f890d11ac2----1-----------------bookmark_preview----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Erdogan Taskesen"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "From Data to Clusters; When is Your Clustering Good Enough?Sensible clusters and hidden gems can be found using clustering approaches but you need the right cluster evaluation method!"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "\u00b717 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&user=Erdogan+Taskesen&userId=4e636e2ef813&source=-----5895440a978a----0-----------------clap_footer----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----f1f890d11ac2----0---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&source=-----f1f890d11ac2----0-----------------bookmark_preview----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f1f890d11ac2----1---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----f1f890d11ac2----1-----------------bookmark_preview----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----f1f890d11ac2----2---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----f1f890d11ac2----2---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----f1f890d11ac2----2---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----f1f890d11ac2----2---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----f1f890d11ac2----2---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----f1f890d11ac2----2---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----f1f890d11ac2----2---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "90"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----f1f890d11ac2----2-----------------bookmark_preview----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/road-to-full-stack-data-science/customer-behaviour-analysis-with-python-38eb90e98771?source=read_next_recirc-----f1f890d11ac2----3---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/@tdalis?source=read_next_recirc-----f1f890d11ac2----3---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/@tdalis?source=read_next_recirc-----f1f890d11ac2----3---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Tasos Pardalis"}, {"url": "https://medium.com/road-to-full-stack-data-science?source=read_next_recirc-----f1f890d11ac2----3---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Road to Full Stack Data Science"}, {"url": "https://medium.com/road-to-full-stack-data-science/customer-behaviour-analysis-with-python-38eb90e98771?source=read_next_recirc-----f1f890d11ac2----3---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "Customer Behaviour Analysis with PythonUse data science and Python to better understand your customer behaviour."}, {"url": "https://medium.com/road-to-full-stack-data-science/customer-behaviour-analysis-with-python-38eb90e98771?source=read_next_recirc-----f1f890d11ac2----3---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": "\u00b714 min read\u00b7Feb 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Froad-to-full-stack-data-science%2F38eb90e98771&operation=register&redirect=https%3A%2F%2Fmedium.com%2Froad-to-full-stack-data-science%2Fcustomer-behaviour-analysis-with-python-38eb90e98771&user=Tasos+Pardalis&userId=ec5020cbf4f7&source=-----38eb90e98771----3-----------------clap_footer----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/road-to-full-stack-data-science/customer-behaviour-analysis-with-python-38eb90e98771?source=read_next_recirc-----f1f890d11ac2----3---------------------ef549901_7341_412e_baeb_c6c8175c8a04-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38eb90e98771&operation=register&redirect=https%3A%2F%2Fmedium.com%2Froad-to-full-stack-data-science%2Fcustomer-behaviour-analysis-with-python-38eb90e98771&source=-----f1f890d11ac2----3-----------------bookmark_preview----ef549901_7341_412e_baeb_c6c8175c8a04-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----f1f890d11ac2--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}