{"url": "https://towardsdatascience.com/variational-autoencoders-as-generative-models-with-keras-e0c79415a7eb", "time": 1683016234.810238, "path": "towardsdatascience.com/variational-autoencoders-as-generative-models-with-keras-e0c79415a7eb/", "webpage": {"metadata": {"title": "Variational Autoencoders as Generative Models with Keras | by Kartik Chaudhary | Towards Data Science", "h1": "Variational Autoencoders as Generative Models with Keras", "description": "This article focuses on giving the readers some basic understanding of the Variational Autoencoders and explaining how they are different from the ordinary autoencoders in Machine Learning and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Generative_adversarial_network", "anchor_text": "Generative Adversarial Networks", "paragraph_index": 0}, {"url": "https://dropsofai.com/autoencoders-in-keras-and-deep-learning/", "anchor_text": "Autoencoders in Keras and Deep Learning", "paragraph_index": 3}, {"url": "https://dropsofai.com/autoencoders-in-keras-and-deep-learning/", "anchor_text": "Autoencoders in Keras and Deep Learning", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL-divergence", "paragraph_index": 10}, {"url": "https://dropsofai.com/convolutional-denoising-autoencoders-for-image-noise-reduction/", "anchor_text": "Convolutional Denoising Autoencoders for image noise reduction", "paragraph_index": 51}, {"url": "https://dropsofai.com/variational-autoencoders-and-image-generation-with-keras/", "anchor_text": "Drops of AI", "paragraph_index": 52}, {"url": "https://dropsofai.com", "anchor_text": "https://dropsofai.com", "paragraph_index": 55}], "all_paragraphs": ["This article focuses on giving the readers some basic understanding of the Variational Autoencoders and explaining how they are different from the ordinary autoencoders in Machine Learning and Artificial Intelligence. Unlike vanilla autoencoders(like-sparse autoencoders, de-noising autoencoders .etc), Variational Autoencoders (VAEs) are generative models like GANs (Generative Adversarial Networks). This article is primarily focused on the Variational Autoencoders and I will be writing soon about the Generative Adversarial Networks in my upcoming posts.", "In this tutorial, we will be discussing how to train a variational autoencoder(VAE) with Keras(TensorFlow, Python) from scratch. We will be concluding our study with the demonstration of the generative capabilities of a simple VAE.", "The rest of the content in this tutorial can be classified as the following-", "An autoencoder is basically a neural network that takes a high dimensional data point as input, converts it into a lower-dimensional feature vector(ie., latent vector), and later reconstructs the original input sample just utilizing the latent vector representation without losing valuable information. Any given autoencoder is consists of the following two parts-an Encoder and a Decoder. The Encoder part of the model takes an input data sample and compresses it into a latent vector. While the decoder part is responsible for recreating the original input sample from the learned(learned by the encoder during training) latent representation. To learn more about the basics, do check out my article on Autoencoders in Keras and Deep Learning.", "Let\u2019s continue considering that we all are on the same page until now.", "One issue with the ordinary autoencoders is that they encode each input sample independently. This means that the samples belonging to the same class (or the samples belonging to the same distribution) might learn very different(distant encodings in the latent space) latent embeddings. Ideally, the latent features of the same class should be somewhat similar (or closer in latent space). This happens because we are not explicitly forcing the neural network to learn the distributions of the input dataset. Due to this issue, our network might not very good at reconstructing related unseen data samples (or less generalizable).", "In the past tutorial on Autoencoders in Keras and Deep Learning, we trained a vanilla autoencoder and learned the latent features for the MNIST handwritten digit images. When we plotted these embeddings in the latent space with the corresponding labels, we found the learned embeddings of the same classes coming out quite random sometimes and there were no clearly visible boundaries between the embedding clusters of the different classes. The following figure shows the distribution-", "Variational Autoencoder is slightly different in nature. Instead of directly learning the latent features from the input samples, it actually learns the distribution of latent features. The latent features of the input data are assumed to be following a standard normal distribution. This means that the learned latent vectors are supposed to be zero centric and they can be represented with two statistics-mean and variance (as standard normal distribution can be attributed with only these two statistics).", "Thus the Variational AutoEncoders(VAEs) calculate the mean and variance of the latent vectors(instead of directly learning latent features) for each sample and forces them to follow a standard normal distribution. Thus the bottleneck part of the network is used to learn mean and variance for each sample, we will define two different fully connected(FC) layers to calculate both. VAEs ensure that the points that are very close to each other in the latent space, are representing very similar data samples(similar classes of data). We are going to prove this fact in this tutorial.", "Before jumping into the implementation details let\u2019s first get a little understanding of the KL-divergence which is going to be used as one of the two optimization measures in our model.", "In the last section, we were talking about enforcing a standard normal distribution on the latent features of the input dataset. This can be accomplished using KL-divergence statistics. KL-divergence is a statistical measure of the difference between two probabilistic distributions. Thus, we will utilize KL-divergence value as an objective function(along with the reconstruction loss) in order to ensure that the learned distribution is very similar to the true distribution, which we have already assumed to be a standard normal distribution.", "In this case, the final objective can be written as-", "Here, the reconstruction loss term would encourage the model to learn the important latent features, needed to correctly reconstruct the original image (if not exactly the same, an image of the same class). While the KL-divergence-loss term would ensure that the learned distribution is similar to the true distribution(a standard normal distribution). This further means that the distribution is centered at zero and is well-spread in the space. We will prove this one also in the latter part of the tutorial.", "The last section has explained the basic idea behind the Variational Autoencoders(VAEs) in machine learning(ML) and artificial intelligence(AI). In this section, we will build a convolutional variational autoencoder with Keras in Python. This network will be trained on the MNIST handwritten digits dataset that is available in Keras datasets.", "This section can be broken into the following parts for step-wise understanding and simplicity-", "In this section, we are going to download and load the MNIST handwritten digits dataset into our Python notebook to get started with the data preparation.", "Here are the dependencies, loaded in advance-", "The following python code can be used to download the MNIST handwritten digits dataset. Few sample images are also displayed below-", "Dataset is already divided into the training and test set. The training dataset has 60K handwritten digit images with a resolution of 28*28. While the Test dataset consists of 10K handwritten digit images with similar dimensions-", "Each image in the dataset is a 2D matrix representing pixel intensities ranging from 0 to 255. We will first normalize the pixel values(To bring them between 0 and 1) and then add an extra dimension for image channels (as supported by Conv2D layers from Keras). Here is the preprocessing code in python-", "In this section, we will define the encoder part of our VAE model. The encoder part of the autoencoder usually consists of multiple repeating convolutional layers followed by pooling layers when the input data type is images. The encoder part of a variational autoencoder is also quite similar, it\u2019s just the bottleneck part that is slightly different as discussed above.", "Here is the python implementation of the encoder part with Keras-", "The above snippet compresses the image input and brings down it to a 16 valued feature vector, but these are not the final latent features. The next section will complete the encoder part by adding the latent features computational logic into it.", "This section is responsible for taking the convoluted features from the last section and calculating the mean and log-variance of the latent features (As we have assumed that the latent features follow a standard normal distribution, and the distribution can be represented with mean and variance statistical values). Two separate fully connected(FC layers) layers are used for calculating the mean and log-variance for the input samples of a given dataset.", "These attributes(mean and log-variance) of the standard normal distribution(SND) are then used to estimate the latent encodings for the corresponding input data points. The function sample_latent_features defined below takes these two statistical values and returns back a latent encoding vector. This latent encoding is passed to the decoder as input for the image reconstruction purpose.", "These latent features(calculated from the learned distribution) actually complete the Encoder part of the model. Now the Encoder model can be defined as follow-", "The encoder is quite simple with just around 57K trainable parameters.", "The Encoder part of the model takes an image as input and gives the latent encoding vector for it as output which is sampled from the learned distribution of the input dataset. The job of the decoder is to take this embedding vector as input and recreate the original image(or an image belonging to a similar class as the original image). As the latent vector is a quite compressed representation of the features, the decoder part is made up of multiple pairs of the Deconvolutional layers and upsampling layers. A deconvolutional layer basically reverses what a convolutional layer does. The upsampling layers are used to bring the original resolution of the image back. In this way, it reconstructs the image with original dimensions.", "Here is the python implementation of the decoder part with Keras API from TensorFlow-", "The decoder model object can be defined as below-", "The decoder is again simple with 112K trainable parameters.", "Finally, the Variational Autoencoder(VAE) can be defined by combining the encoder and the decoder parts. Here is how you can create the VAE model object by sticking decoder after the encoder.", "The overall setup is quite simple with just 170K trainable model parameters. Time to write the objective(or optimization function) function.", "As discussed earlier, the final objective(or loss) function of a variational autoencoder(VAE) is a combination of the data reconstruction loss and KL-loss. In this section, we will define our custom loss by combining these two statistics.", "The following implementation of the get_loss function returns a total_loss function that is a combination of reconstruction loss and KL-loss as defined below-", "Finally, let\u2019s compile the model to make it ready for the training-", "Just like the ordinary autoencoders, we will train it by giving exactly the same images for input as well as the output. The model is trained for 20 epochs with a batch size of 64.", "I hope it can be trained a little more, but this is where the validation loss was not changing much and I went ahead with it.", "In this section, we will see the reconstruction capabilities of our model on the test images. The following python script will pick 9 images from the test dataset and we will be plotting the corresponding reconstructed images for them.", "The above results confirm that the model is able to reconstruct the digit images with decent efficiency. However, one important thing to notice here is that some of the reconstructed images are very different in appearance from the original images while the class(or digit) is always the same. This happens because, the reconstruction is not just dependent upon the input image, it is the distribution that has been learned. And this learned distribution is the reason for the introduced variations in the model output. This is interesting, isn\u2019t it!", "The second thing to notice here is that the output images are a little blurry. This is a common case with variational autoencoders, they often produce noisy(or poor quality) outputs as the latent vectors(bottleneck) is very small and there is a separate process of learning the latent features as discussed before. Variational Autoencoders(VAEs) are not actually designed to reconstruct the images, the real purpose is learning the distribution (and it gives them the superpower to generate fake data, we will see it later in the post).", "As we have quoted earlier, the variational autoencoders(VAEs) learn the underlying distribution of the latent features, it basically means that the latent encodings of the samples belonging to the same class should not be very far from each other in the latent space. Secondly, the overall distribution should be standard normal, which is supposed to be centered at zero.", "Let\u2019s generate the latent embeddings for all of our test images and plot them(the same color represents the digits belonging to the same class, taken from the ground truth labels). Here is the python code-", "The above plot shows that the distribution is centered at zero. Embeddings of the same class digits are closer in the latent space. Digit separation boundaries can also be drawn easily. This is pretty much we wanted to achieve from the variational autoencoder. Let\u2019s jump to the final part where we test the generative capabilities of our model.", "Variational Autoencoders can be used as generative models. The previous section shows that latent encodings of the input data are following a standard normal distribution and there are clear boundaries visible for different classes of the digits.", "Just think for a second-If we already know, which part of the space is dedicated to what class, we don\u2019t even need input images to reconstruct the image. This means that we can actually generate digit images having similar characteristics as the training dataset by just passing the random points from the space (latent distribution space). In this fashion, the variational autoencoders can be used as generative models in order to generate fake data.", "As we can see, the spread of latent encodings is in between [-3 to 3 on the x-axis, and also -3 to 3 on the y-axis]. Let\u2019s generate a bunch of digits with random latent encodings belonging to this range only.", "You can find all the digits(from 0 to 9) in the above image matrix as we have tried to generate images from all the portions of the latent space. The capability of generating handwriting with variations isn\u2019t it awesome!", "This tutorial explains the variational autoencoders in Deep Learning and AI. With a basic introduction, it shows how to implement a VAE with Keras and TensorFlow in python. It further trains the model on MNIST handwritten digit dataset and shows the reconstructed results.", "We have seen that the latent encodings are following a standard normal distribution (all thanks to KL-divergence) and how the trained decoder part of the model can be utilized as a generative model. We have proved the claims by generating fake digits using only the decoder part of the model.", "In case you are interested in reading my article on the Denoising Autoencoders", "Convolutional Denoising Autoencoders for image noise reduction", "Originally published on Drops of AI.", "Thanks for reading! Hope this was helpful. Kindly let me know your feedback by commenting below. See you in the next article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI/ML @ Google | personal blog: https://dropsofai.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe0c79415a7eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kartikgill96?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kartikgill96?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "Kartik Chaudhary"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3fd5a49d1e91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&user=Kartik+Chaudhary&userId=3fd5a49d1e91&source=post_page-3fd5a49d1e91----e0c79415a7eb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0c79415a7eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0c79415a7eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@bamagal", "anchor_text": "\u201cMy Life Through A Lens\u201d"}, {"url": "https://unsplash.com/photos/bq31L0jQAjU", "anchor_text": "Image Source"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://en.wikipedia.org/wiki/Generative_adversarial_network", "anchor_text": "Generative Adversarial Networks"}, {"url": "https://dropsofai.com/autoencoders-in-keras-and-deep-learning/", "anchor_text": "Autoencoders in Keras and Deep Learning"}, {"url": "https://dropsofai.com/autoencoders-in-keras-and-deep-learning/", "anchor_text": "Autoencoders in Keras and Deep Learning"}, {"url": "https://dropsofai.com/autoencoders-in-keras-and-deep-learning/", "anchor_text": "Image Source"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL-divergence"}, {"url": "https://github.com/kartikgill/Autoencoders", "anchor_text": "Github Link"}, {"url": "https://dropsofai.com/convolutional-denoising-autoencoders-for-image-noise-reduction/", "anchor_text": "Convolutional Denoising Autoencoders for image noise reduction"}, {"url": "https://github.com/kartikgill/Autoencoders", "anchor_text": "https://github.com/kartikgill/Autoencoders"}, {"url": "https://dropsofai.com/variational-autoencoders-and-image-generation-with-keras/", "anchor_text": "Drops of AI"}, {"url": "https://dropsofai.com/autoencoders-in-keras-and-deep-learning/", "anchor_text": "Autoencoders in Keras and Deep Learning"}, {"url": "https://dropsofai.com/optimizers-explained-for-training-neural-networks/", "anchor_text": "Optimizers explained for training Neural Networks"}, {"url": "https://dropsofai.com/optimizing-tensorflow-models-with-quantization-techniques/", "anchor_text": "Optimizing TensorFlow models with Quantization Techniques"}, {"url": "https://dropsofai.com/deep-learning-with-pytorch-introduction/", "anchor_text": "Deep Learning with PyTorch: Introduction"}, {"url": "https://dropsofai.com/deep-learning-with-pytorch-first-neural-network/", "anchor_text": "Deep Learning with PyTorch: First Neural Network"}, {"url": "https://blog.paperspace.com/how-to-build-variational-autoencoder-keras/", "anchor_text": "How to Build a Variational Autoencoder in Keras"}, {"url": "https://www.jeremyjordan.me/variational-autoencoders/", "anchor_text": "Variational autoencoders"}, {"url": "https://keras.io/examples/generative/vae/", "anchor_text": "https://keras.io/examples/generative/vae/"}, {"url": "https://arxiv.org/abs/1703.01925", "anchor_text": "Grammar Variational Autoencoder"}, {"url": "https://arxiv.org/abs/1802.04364", "anchor_text": "Junction Tree Variational Autoencoder for Molecular Graph Generation"}, {"url": "http://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions", "anchor_text": "Variational Autoencoder for Deep Learning of Images, Labels, and Captions"}, {"url": "http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf", "anchor_text": "Variational Autoencoder based Anomaly Detection using Reconstruction Probability"}, {"url": "https://arxiv.org/abs/1702.02390", "anchor_text": "A Hybrid Convolutional Variational Autoencoder for Text Generation"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e0c79415a7eb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e0c79415a7eb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/variational-autoencoder?source=post_page-----e0c79415a7eb---------------variational_autoencoder-----------------", "anchor_text": "Variational Autoencoder"}, {"url": "https://medium.com/tag/generative-model?source=post_page-----e0c79415a7eb---------------generative_model-----------------", "anchor_text": "Generative Model"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----e0c79415a7eb---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0c79415a7eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&user=Kartik+Chaudhary&userId=3fd5a49d1e91&source=-----e0c79415a7eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0c79415a7eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&user=Kartik+Chaudhary&userId=3fd5a49d1e91&source=-----e0c79415a7eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0c79415a7eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe0c79415a7eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e0c79415a7eb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e0c79415a7eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kartikgill96?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kartikgill96?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kartik Chaudhary"}, {"url": "https://medium.com/@kartikgill96/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "196 Followers"}, {"url": "https://dropsofai.com", "anchor_text": "https://dropsofai.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3fd5a49d1e91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&user=Kartik+Chaudhary&userId=3fd5a49d1e91&source=post_page-3fd5a49d1e91--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fca74cfd61cd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-autoencoders-as-generative-models-with-keras-e0c79415a7eb&newsletterV3=3fd5a49d1e91&newsletterV3Id=ca74cfd61cd8&user=Kartik+Chaudhary&userId=3fd5a49d1e91&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}