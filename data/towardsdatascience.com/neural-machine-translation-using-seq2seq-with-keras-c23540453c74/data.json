{"url": "https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74", "time": 1682993197.84336, "path": "towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74/", "webpage": {"metadata": {"title": "Neural Machine Translation \u2014 Using seq2seq with Keras | by Ravindra Kompella | Towards Data Science", "h1": "Neural Machine Translation \u2014 Using seq2seq with Keras", "description": "This article is motivated by this keras example and this paper on encoder-decoder network. The idea is to gain intuitive and detailed understanding from this example. My own implementation of this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py", "anchor_text": "keras example", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "this paper", "paragraph_index": 0}, {"url": "https://goo.gl/bEBhGQ", "anchor_text": "github link", "paragraph_index": 0}, {"url": "https://medium.com/@kmsravindra123/lstm-nuggets-for-practical-applications-5beef5252092", "anchor_text": "my other post on LSTM", "paragraph_index": 1}, {"url": "https://medium.com/@kmsravindra123/lstm-nuggets-for-practical-applications-5beef5252092", "anchor_text": "my other post on LSTM", "paragraph_index": 8}], "all_paragraphs": ["This article is motivated by this keras example and this paper on encoder-decoder network. The idea is to gain intuitive and detailed understanding from this example. My own implementation of this example referenced in this story is provided at my github link.", "Before we start, it may help to go through my other post on LSTM that helps in understanding the fundamentals of LSTMs specifically in this context.", "Below is the detailed network architecture used for training the seq2seq Encoder \u2014 Decoder network. We will refer this figure through out.", "Firstly we will go about training the network. Then we will look at the inference models on how to translate a given English sentence to French. Inference model (used for predicting on the input sequence) has a slightly different decoder architecture and we will discuss that in detail when we come there.", "So how does the training data look?", "Detailed flow for training the network with code \u2014", "Refer to snippet 1 \u2014 Note that we have appended \u2018\\t\u2019 for start of the french sentence and \u2018\\n\u2019 to signify end of the french sentence. These appended french sentences will be used as inputs to decoder. All the english characters and french characters are collected in separate sets. These sets are converted to character level dictionaries (useful for retrieving the index and character values later).", "Refer to snippet 2\u2014 Prepare the embeds for encoder input, decoder input and the target data embeds. We will create one-hot encoding for each character in English and French separately. These are called as tokenized_eng_sentences and tokenized_fra_sentences in the code snippet. These will be the inputs to encoder and decoder respectively. Note that the target_data french character embeds that we compare at the softmax layer output are offset by (t+1) compared to the decoder input embeds (because there is no start tag in target data \u2014 refer to the above architecture diagram for more clarity). Hence the target_data in the below code snippet is accordingly offset ( note k-1 in second dimension of the target_data array below)", "Refer to snippet 2 \u2014 As we noted in my other post on LSTM, the embeds (tokenized_eng_sentences and tokenized_fra_sentences and target_data) are 3D arrays. The first dimension corresponds to nb_samples ( =10,000 in this case). The second dimension corresponds to the maximum length of english / french sentence and the third dimension corresponds to total number of english / french characters.", "Refer to snippet 3: We will input character by character (of-course, their corresponding one hot embeds) into the encoder network. For the encoder_LSTM, we had set return_state = True . We did not do return_sequences = True (and by default this is set to False). This would mean that we obtain only the final encoded cell state and the encoded hidden state at the end of the input sequence and not the intermediate states at every time step. These will be the final encoded states that are used to initialize the state of the decoder.", "Refer to snippet 3 \u2014 Also note that the input shape has been specified as (None, len(eng_chars)). This means the encoder LSTM can dynamically unroll that many timesteps as the number of characters till it reaches the end of sequence for that sentence.", "Refer to snippet 4 \u2014 Inputs to the decoder will be the french character embeds (contained in tokenized_fra_sentences array) one by one at each time step along with the previous state values. The previous states for the first step of the decoder will be initialized with the final encoder states that we collected earlier in snippet 3. For this reason, note that the initial_state=encoder_states has been set in the below code snippet. From the subsequent step on wards the state inputs to decoder will be its cell state and its hidden state.", "Also from the above code snippet, notice that the decoder is setup with return_sequences = True along with return_state = True. So we obtain decoder output and the two decoder states at every timestep. While return_state = True has been declared here, we are not going to use the decoder states while training the model. The reason for its presence is that they will be used while building the decoder inference model (that we will see later). The decoder output is passed through the softmax layer that will learn to classify the correct french character.", "Refer to snippet 5 \u2014 The loss function is categorical cross entropy that is obtained by comparing the predicted values from softmax layer with the target_data (one-hot french character embeds).", "Now the model is ready for training. Train the entire network for the specified number of epochs.", "Below is the architecture used for inference models \u2014The inference model will leverage all the network parameters learnt during training but we define them separately because the inputs and outputs during inference are different from what they were during training the network.", "From the below figure, observe that there are no changes on the encoder side of the network. So we feed the new English sentence (one hot character embedded) vector as input sequence to the encoder model and obtain the final encoding states.", "Contrast this figure B with figure A on the decoder side. The major changes can be seen are as below \u2014", "Given the above understanding, now lets look at the code \u2014", "Refer to snippet 6 \u2014 The Encoder inference model is quite straightforward. This is going to output only the encoder_states.", "Refer to snippet 7 \u2014 The decoder model is more elaborate. Note that we create separate \u2018Input\u2019 for decoder hidden state and decoder cell state. This is because we are going to feed these states at every time step (other than the first time step \u2014 Recall that at the first time step we feed only the encoder states) into the decoder and the decoder inference model is a separate standalone model. Both the encoder and decoder will be called recursively for each character that is to be generated in the translated sequence.", "Refer to snippet 8 \u2014We get the encoder states into states_val variable. On the first call inside the while loop, these hidden and cell states from the encoder will be used to initialize the decoder_model_inf that are provided as input to the model directly. Once we predict the character using softmax, we now input this predicted character ( using the target_seq 3D array for one-hot embed of the predicted character) along with the updated states_val (updated from the previous decoder states) for the next iteration of the while loop. Note that we reset our target_seq before we create a one-hot embed of the predicted character every time in the while loop.", "That\u2019s it! Now we have a trained model that can translate English sentences to French! Below are the results obtained after training the network for 25 epochs.", "If you plan to use any of the above architecture diagram figures, please feel free to do so and request you to mention my name in the image credit.", "Please show your applause by holding the clapping icon, if you find any useful takeaway from this article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc23540453c74&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c23540453c74--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kmsravindra123?source=post_page-----c23540453c74--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kmsravindra123?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Ravindra Kompella"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc3f8c66f5451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&user=Ravindra+Kompella&userId=c3f8c66f5451&source=post_page-c3f8c66f5451----c23540453c74---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc23540453c74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc23540453c74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@kiarash_mansouri?utm_source=medium&utm_medium=referral", "anchor_text": "Kiarash Mansouri"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py", "anchor_text": "keras example"}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "this paper"}, {"url": "https://goo.gl/bEBhGQ", "anchor_text": "github link"}, {"url": "https://medium.com/@kmsravindra123/lstm-nuggets-for-practical-applications-5beef5252092", "anchor_text": "my other post on LSTM"}, {"url": "https://medium.com/u/c3f8c66f5451?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Ravindra Kompella"}, {"url": "https://medium.com/@kmsravindra123/lstm-nuggets-for-practical-applications-5beef5252092", "anchor_text": "my other post on LSTM"}, {"url": "https://medium.com/u/c3f8c66f5451?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Ravindra Kompella"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c23540453c74---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c23540453c74---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c23540453c74---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/language-learning?source=post_page-----c23540453c74---------------language_learning-----------------", "anchor_text": "Language Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc23540453c74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&user=Ravindra+Kompella&userId=c3f8c66f5451&source=-----c23540453c74---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc23540453c74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&user=Ravindra+Kompella&userId=c3f8c66f5451&source=-----c23540453c74---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc23540453c74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c23540453c74--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc23540453c74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c23540453c74---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c23540453c74--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c23540453c74--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c23540453c74--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c23540453c74--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c23540453c74--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kmsravindra123?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kmsravindra123?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravindra Kompella"}, {"url": "https://medium.com/@kmsravindra123/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://github.com/kmsravindra", "anchor_text": "https://github.com/kmsravindra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc3f8c66f5451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&user=Ravindra+Kompella&userId=c3f8c66f5451&source=post_page-c3f8c66f5451--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa7d4d6987137&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-machine-translation-using-seq2seq-with-keras-c23540453c74&newsletterV3=c3f8c66f5451&newsletterV3Id=a7d4d6987137&user=Ravindra+Kompella&userId=c3f8c66f5451&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}