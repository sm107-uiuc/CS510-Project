{"url": "https://towardsdatascience.com/a-detailed-explanation-of-the-attention-u-net-b371a5590831", "time": 1683006604.197149, "path": "towardsdatascience.com/a-detailed-explanation-of-the-attention-u-net-b371a5590831/", "webpage": {"metadata": {"title": "A detailed explanation of the Attention U-Net | by Robin Vinod | Towards Data Science", "h1": "A detailed explanation of the Attention U-Net", "description": "How attention gates in the Attention U-Net work based on additive soft attention."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1804.03999", "anchor_text": "Attention U-Net:Learning Where to Look for the Pancreas", "paragraph_index": 0}, {"url": "https://github.com/robinvvinod/unet/", "anchor_text": "https://github.com/robinvvinod/unet/", "paragraph_index": 14}, {"url": "https://www.linkedin.com/in/robin-vinod/", "anchor_text": "https://www.linkedin.com/in/robin-vinod/", "paragraph_index": 16}], "all_paragraphs": ["In this story, I explain the Attention U-Net from Attention U-Net:Learning Where to Look for the Pancreas written by Oktay et. al. The paper was written in 2018 and proposed a novel attention gate (AG) mechanism that allows the U-Net to focus on target structures of varying size and shape.", "Attention, in the context of image segmentation, is a way to highlight only the relevant activations during training. This reduces the computational resources wasted on irrelevant activations, providing the network with better generalisation power. Essentially, the network can pay \u201cattention\u201d to certain parts of the image.", "Attention comes in two forms, hard and soft. Hard attention works on the basis of highlighting relevant regions by cropping the image or iterative region proposal. Since hard attention can only choose one region of an image at a time, it has two implications, it is non-differentiable and requires reinforcement learning to train.", "Since it is non-differentiable, it means that for a given region in an image, the network can either pay \u201cattention\u201d or not, with no in between. As a result, standard backpropagation cannot be done, and Monte Carlo sampling is needed to calculate the accuracy across various stages of backpropagation. Considering the accuracy is subject to how well the sampling is done, there is a need for other techniques such as reinforcement learning to make the model effective.", "Soft attention works by weighting different parts of the image. Areas of high relevance is multiplied with a larger weight and areas of low relevance is tagged with smaller weights. As the model is trained, more focus is given to the regions with higher weights. Unlike hard attention, these weights can be applied to many patches in the image.", "Due to the deterministic nature of soft attention, it remains differentiable and can be trained with standard backpropagation. As the model is trained, the weighting is also trained such that the model gets better at deciding which parts to pay attention to.", "To understand why attention is beneficial in the U-Net, we need to look at the skip connections used.", "During upsampling in the expanding path, spatial information recreated is imprecise. To counteract this problem, the U-Net uses skip connections that combine spatial information from the downsampling path with the upsampling path. However, this brings across many redundant low-level feature extractions, as feature representation is poor in the initial layers.", "Soft attention implemented at the skip connections will actively suppress activations in irrelevant regions, reducing the number of redundant features brought across.", "The attention gates introduced by Oktay et al. uses additive soft attention.", "Oktay et al. also proposed a grid-based gating mechanism, which takes the g vector from the upsampling path rather than the downsampling path (except for the lowest layer), as the vector would have been conditioned to spatial information from multiple scales by previous attention gates.", "As seen in the figure above, the network learns to focus on the desired region as training proceeds. The differentiable nature of the attention gate allows it to be trained during backpropagation, which means the attention coefficients get better at highlighting relevant regions.", "Results obtained by Oktay et al. show that the Attention U-Net has outperformed a plain U-Net in the overall Dice Coefficient Score by a sizeable margin. While the Attention U-Net has more parameters, it is not significantly more and the inference time is only marginally longer.", "In conclusion, attention gates are a simple way to improve the U-Net consistently in a large variety of datasets without a significant overhead in terms of computational cost.", "To get the full implementation of a U-Net with attention, recurrence and inception layers pre-implemented, please check https://github.com/robinvvinod/unet/.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "17 year old student interested in ML research and application development. I use Medium to share what I\u2019ve learnt. https://www.linkedin.com/in/robin-vinod/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb371a5590831&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b371a5590831--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b371a5590831--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@robinvvinod?source=post_page-----b371a5590831--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@robinvvinod?source=post_page-----b371a5590831--------------------------------", "anchor_text": "Robin Vinod"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5e2c08907591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&user=Robin+Vinod&userId=5e2c08907591&source=post_page-5e2c08907591----b371a5590831---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb371a5590831&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb371a5590831&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1804.03999", "anchor_text": "Attention U-Net:Learning Where to Look for the Pancreas"}, {"url": "http://proceedings.mlr.press/v37/xuc15.pdf", "anchor_text": "(Xu et al., 2015)"}, {"url": "https://arxiv.org/abs/1505.04597", "anchor_text": "(Ronneberger et al., 2015)"}, {"url": "https://github.com/robinvvinod/unet/", "anchor_text": "https://github.com/robinvvinod/unet/"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b371a5590831---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b371a5590831---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/keras?source=post_page-----b371a5590831---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/unet?source=post_page-----b371a5590831---------------unet-----------------", "anchor_text": "Unet"}, {"url": "https://medium.com/tag/attentiongates?source=post_page-----b371a5590831---------------attentiongates-----------------", "anchor_text": "Attentiongates"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb371a5590831&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&user=Robin+Vinod&userId=5e2c08907591&source=-----b371a5590831---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb371a5590831&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&user=Robin+Vinod&userId=5e2c08907591&source=-----b371a5590831---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb371a5590831&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b371a5590831--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb371a5590831&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b371a5590831---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b371a5590831--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b371a5590831--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b371a5590831--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b371a5590831--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b371a5590831--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b371a5590831--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b371a5590831--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b371a5590831--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@robinvvinod?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@robinvvinod?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Robin Vinod"}, {"url": "https://medium.com/@robinvvinod/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "60 Followers"}, {"url": "https://www.linkedin.com/in/robin-vinod/", "anchor_text": "https://www.linkedin.com/in/robin-vinod/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5e2c08907591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&user=Robin+Vinod&userId=5e2c08907591&source=post_page-5e2c08907591--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc9834c921339&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-detailed-explanation-of-the-attention-u-net-b371a5590831&newsletterV3=5e2c08907591&newsletterV3Id=c9834c921339&user=Robin+Vinod&userId=5e2c08907591&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}