{"url": "https://towardsdatascience.com/how-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64", "time": 1683017356.4360719, "path": "towardsdatascience.com/how-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64/", "webpage": {"metadata": {"title": "How to Build a Real-Time Twitter Analysis Application Using Big Data Tools | by Chuangxin Lin | Towards Data Science", "h1": "How to Build a Real-Time Twitter Analysis Application Using Big Data Tools", "description": "Data Science/Machine Learning applications have been everywhere now and radically changing our lives and business. With tons of awesome out-of-the-box ML packages today, everyone is able to build\u2026"}, "outgoing_paragraph_urls": [{"url": "https://developer.twitter.com/en/apply-for-access", "anchor_text": "Twitter Developer account", "paragraph_index": 11}, {"url": "http://docs.tweepy.org/en/v3.5.0/streaming_how_to.html", "anchor_text": "Tweepy streaming API", "paragraph_index": 11}, {"url": "https://github.com/Chancylin/twitter_data_analysis/tree/master/spark_streaming/aws_kinesis", "anchor_text": "GitHub", "paragraph_index": 12}, {"url": "https://jayendrapatil.com/aws-kinesis-data-streams-vs-kinesis-firehose/#:~:text=Kinesis%20data%20streams%20%E2%80%93%20Kinesis%20data,streaming%20data%20for%20specialized%20needs.&text=Firehose%20also%20allows%20for%20streaming,for%20processing%20through%20additional%20services.", "anchor_text": "Kinesis Data Stream and Kinesis Data Firehose", "paragraph_index": 13}, {"url": "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html", "anchor_text": "AWS documentation", "paragraph_index": 15}, {"url": "https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html", "anchor_text": "resharding", "paragraph_index": 18}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark", "paragraph_index": 20}, {"url": "https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/", "anchor_text": "Learning Spark", "paragraph_index": 20}, {"url": "https://databricks.com/product/faq/community-edition", "anchor_text": "community edition", "paragraph_index": 24}, {"url": "https://aws.amazon.com/blogs/big-data/submitting-user-applications-with-spark-submit/", "anchor_text": "leave some cores (usually one) for the OS", "paragraph_index": 27}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html", "anchor_text": "maximizeResourceAllocation", "paragraph_index": 29}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cache", "anchor_text": "cache", "paragraph_index": 29}, {"url": "https://spark.apache.org/docs/latest/tuning.html", "anchor_text": "configuration", "paragraph_index": 29}, {"url": "https://nlp.johnsnowlabs.com/", "anchor_text": "Spark-NLK", "paragraph_index": 34}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "TF-IDF", "paragraph_index": 34}, {"url": "https://planspace.org/20150607-textblob_sentiment/", "anchor_text": "by the average of the pre-defined score of each word in the text", "paragraph_index": 37}, {"url": "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html", "anchor_text": "Spark Structured Streaming", "paragraph_index": 38}, {"url": "http://Structured Streaming queries", "anchor_text": "Spark documentation", "paragraph_index": 39}, {"url": "https://spark.apache.org/docs/latest/streaming-programming-guide.html", "anchor_text": "Spark Streaming", "paragraph_index": 40}, {"url": "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html", "anchor_text": "Spark documentation", "paragraph_index": 40}, {"url": "https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html", "anchor_text": "DataBricks documentation", "paragraph_index": 44}, {"url": "https://github.com/Chancylin/twitter_data_analysis/blob/master/notebook/spark_streaming_write_to_S3_clean.ipynb", "anchor_text": "notebook", "paragraph_index": 46}, {"url": "https://aws.amazon.com/athena/?nc=sn&loc=1&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc", "anchor_text": "AWS documentation", "paragraph_index": 51}, {"url": "https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html", "anchor_text": "AWS RedShift", "paragraph_index": 51}, {"url": "https://aws.amazon.com/athena/?nc=sn&loc=1&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc", "anchor_text": "Amazon documentation", "paragraph_index": 53}, {"url": "https://docs.aws.amazon.com/quicksight/latest/user/welcome.html", "anchor_text": "QuickSight", "paragraph_index": 59}, {"url": "https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html", "anchor_text": "connect to Athena", "paragraph_index": 59}, {"url": "https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html", "anchor_text": "documentation", "paragraph_index": 60}, {"url": "https://spark.apache.org/docs/latest/streaming-programming-guide.html#mllib-operations", "anchor_text": "Spark supports Streaming Linear Regression and Streaming KMeans", "paragraph_index": 64}, {"url": "https://www.tutorialspoint.com/kubernetes/index.htm", "anchor_text": "Kubernetes", "paragraph_index": 64}, {"url": "https://mlinproduction.com/model-retraining/", "anchor_text": "post", "paragraph_index": 64}, {"url": "https://sujithjay.com/spark/with-yarn#:~:text=YARN%20is%20a%20generic%20resource,Spark)%20in%20addition%20to%20MapReduce.", "anchor_text": "Understanding Apache Spark on YARN", "paragraph_index": 71}, {"url": "https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html", "anchor_text": "Distribution of Executors, Cores and Memory for a Spark Application running in Yarn", "paragraph_index": 72}], "all_paragraphs": ["Data Science/Machine Learning applications have been everywhere now and radically changing our lives and business. With tons of awesome out-of-the-box ML packages today, everyone is able to build models in a fast and cheap fashion. This is particularly true when working on your own laptop as a local playground. However, to build a more exciting and practical machine learning application, big data tools and cloud platforms are what you should take into account in your toolbox. In this article, I will give you a quick tour of building a (near) real-time system using big data tools and cloud platform. Hopefully, you can grasp a big picture of all these \u201cLEGO pieces\u201d after the read.", "Note: (1) To avoid flooding ourselves with the tweets on the Internet, I will collect the live tweets filtered by the hashtags #AI, #MachineLearning. In other words, we will explore the latent topics for tweets that are talking about AI and Machine Learning. With the sentiment analysis, we can also catch a glimpse of the public attitudes towards this field. (2) In the end, we will also build a dashboard for visualization.", "Note: we will use Spark\u2019s machine learning library to train an LDA (Latent Dirichlet Allocation) model for topic modelling, which will give you a quick taste of ML model training in a distributed system. We also use Spark to process the Streaming, for model scoring/inference.", "Note: the Tweepy streaming will be the producer that will put data into the Kinesis Data Stream.", "Note: We use a Kinesis Firehose deliver stream as a consumer of the Kinesis Data Stream, and save the raw tweets into the S3 directly.", "Note: we use EMR to run Spark for data processing and model training, in a distributed fashion.", "Note: we will use Athena to access the processed tweets that have been saved in S3.", "Note: We will use QuickSight to connect to Athena to build a dashboard for visualization.", "To collect tweets in real time is the very first step for two purposes:", "(1) Create the dataset for the ML model training purpose.", "(2) The streaming will be used to demonstrate the real-time analysis.", "You will need to apply for a Twitter Developer account in order to set up the Twitter streaming. Otherwise, using the Tweepy streaming API is straightforward. An important step is to overload the \u201con_data\u201d method in the \u201cStreamListener\u201d class so we can put the tweets into Kinesis Data Stream. In this way, we have the producer in our streaming pipeline.", "The complete code can be found on GitHub. Here are some highlights:", "If you are an observant reader, you may find out that we have not created the Kinesis Data Streaming yet when introducing the Tweepy Streaming in the previous section. And yes, you are right! Indeed we need to have the Kinesis Data Stream ready beforehand, so we can ingest the streaming twitter data. Simply sign in to your AWS console, go to Amazon Kinesis and create a Data Stream. It is worth noting the difference between Kinesis Data Stream and Kinesis Data Firehose. Firehose loads data streaming directly into the destination (e.g., S3 as data lake). But in this use case, Kinesis Data Stream fits our need better, since aside from saving the raw data into S3 directly, we are interested in analyzing/processing the tweets in real time. The step of creating a Kinesis Data Stream will be like this:", "There are two important points to consider when creating a data stream:", "Now with the Tweepy Streaming as the producer to our Kinesis Stream, we will have the consumers on the other side to consume the data in the Stream. The consumers are essentially the applications you will build to process or analyze the streaming data. Refer to the diagram in AWS documentation for Kinesis Data Streams High-Level Architecture.", "The Kinesis Firehose is one consumer in our data pipeline because we would like to deliver the raw data into S3 (for the purpose of backups as well as creating our training dataset). Setting up a Kinesis Firehose delivery stream as the consumer for Kinesis Stream is straightforward. You can create it directly from the page of the active Kinesis Data Stream.", "Simply follow the steps and configure the setting wisely in Step 4: how much data to buffer before delivering them into the destination. We will use the S3 bucket as the destination in Step 3. The rationale here is we want to save the raw data from future use, for example, to train our topic model.", "Great. When all is set, you can go to the \u201cMonitoring\u201d Tab on the dashboards to inspect the streaming for both Kinesis Data Streaming and Kinesis Firehose. For example, the stream metrics will give you some idea about your data input/output rate for the Kinesis Data Stream. You can dynamically adjust the number of shards within your data stream via resharding.", "Our ultimate goal is to perform real-time analysis on the live tweets. Thus, we will create a Spark streaming as one consumer for the Kinesis Data Stream. In other words, the Kinesis Data Stream will be the source of the Spark streaming that we will discuss later. But before we step into the details on that part, let\u2019s make a detour and see how to use Spark for data processing and machine learning model training.", "Python + scikit-learn is the must-have tool for Data Scientist\u2019s daily work. But it becomes awkward when you try to handle tens or hundreds of GB of data in a single machine. Spark comes to the rescue. Apache Spark is a unified engine designed for large-scale distributed data processing. I really like the description of Spark from the great book \u201cLearning Spark\u201d:", "The central thrust of the Spark project was to bring in ideas borrowed from Hadoop Map\u2010 Reduce, but to enhance the system: make it highly fault tolerant and embarrassingly parallel, support in-memory storage for intermediate results between iterative and interactive map and reduce computations, offer easy and composable APIs in multiple languages as a programming model, and support other workloads in a unified manner.", "The Spark DataFrame (Python) API is very easy to work with if you come from a Pandas background. But watch out for the pitfalls. You need to tune the configuration knobs in order to optimize the performance for the Spark job, given the fact that the data resides in a distributed system. I will give you some simple examples shortly to illustrate this point.", "You can have Spark installed on your laptop and run it in standalone mode (a single machine). But in practice, you will run your Spark job in cluster mode in order to leverage the computing power with the distributed machines (i.e., executors). In cluster mode, you will submit a pre-compile Jar file (Java/Scala) or a Python script. In your PySpark application, the boilerplate code to create a SparkSession is as follows.", "How to configure the resource for your Spark Application depends on the computational demand and the available resources in your cluster. DataBricks provides an easy solution to working with Spark. The community edition allows you to access a micro-cluster and play with Spark. Alternatively, you can use Amazon EMR which utilizes a Hadoop framework running on Amazon EC2 for your computing-intensive Spark job. Here let\u2019s stick to Amazon EMR. You can easily find the tutorial to set up an Amazon EMR cluster, but there is a super useful step worth highlighting here. You can create a bootstrap action when spinning up the EMR cluster, and this can be used to install additional packages across all cluster nodes. For example, the following shell script will install the NLTK Python package in all cluster nodes; just put the script in a location in AWS S3.", "Here I set up an EMR cluster with one master node and two core nodes (worker nodes), both of which are m5.xlarge EC2 instances (4 CPU and 16 GB Memory).", "Note that we will submit our Spark Job in the cluster mode with YARN as the Cluster manager. For the configuration of the Spark job submission, there are some important concepts to understand how Spark and YARN interact. I will put some useful resources at the end for your reference. Here are some important notes you may find useful:", "The following figure can give you a better idea of how Spark and Yarn interact in the use case I created. The Spark Application only has one executor. And I configured 2 cores for the driver and the executor. The rationale here is to leave some cores (usually one) for the OS, given that there are 4 cores on each worker node.", "Another important note on the configuration is about the number of executors (spark.executor.instances) and the number of executor cores (spark.executor.cores), and how these ultimately relate to the number of partitions to achieve better performance (i.e., parallelism) for a Spark job. Use my Spark application as an example here:", "You may find maximizeResourceAllocation and dynamicAllocation useful when running the Spark application in EMR. Overall, performance tuning is a big deal for Spark usage and we only mentioned some important aspects. Another common practice is to use cache wisely to avoid repeated computation. There are many posts and tutorials out there discussing more details on the configuration and they are definitely worth your time.", "Since we are interested in discovering the underlying topics for the tweets, we need to go through some common steps for the NLP task and extract the features that can be fed to the LDA model. The details of the LDA model are out of the scope here. I owe you a clear explanation and will write a post on this topic in the future. But the core idea is: we treat each tweet as a document and a document is a mixture of multiple topics, where each topic is characterized by a distribution over words. Given the fact that a document is a collection of words, we can learn the underlying topic mixture of each document.", "Let\u2019s move on to the Spark part. First, we need to load the raw tweets we have collected. Here I used the Tweets in November as the training dataset and Tweets on December 1-2 as the test dataset.", "Here is the code snippet for loading data:", "The following code snippet for text processing should give you some idea of how the Transformer transforms one DataFrame to another DataFrame (you can use chaining for concise syntax).", "Note that I used stemming instead of lemmatization for efficiency. Spark\u2019s native library doesn\u2019t provide Stemming and Lemmatization functionalities. A quick solution is to create your own user-defined functions (UDF). An alternative is to adopt Spark-NLK. I also found that further removing trivial stemmed words helps for the LDA model. Then we build the ML pipeline to fit the LDA model. We use TF-IDF to build the features for LDA model training. It\u2019s worth noting that the IDF step is very important, otherwise our model will be overwhelmed by the common words that occur too often in every tweet.", "Choosing the numbers of topics as 5, we got the underlying topics with the associated top words as follows:", "The choice of numbers of topics (k) involves many nuances such as the prior information you have, or simple trial-and-error. The output topics should achieve your desired level of interpretability, while this is usually subjective and challenging. We can see Topic 1 associated with [neural, network, google], so it could be tweets about the latest research in machine learning and AI. For Topic 3, it could be marketing promotion on data science training. And Topic 4 seems like some big topic on human society with the development of machine learning and AI (it is also interesting to see the word \u201chealth\u201d).", "We saved the trained LDA model in S3, and use it to discover the underlying topics for the live tweets later. To enrich the data analysis, I also introduce the text sentiment as another dimension. This is achieved by simply including TextBlob in Spark Streaming which I will show you in the next section. Note that TextBlob sentiment is not considered as a machine learning approach since it simply determines the sentiment score by the average of the pre-defined score of each word in the text.", "We will use Spark Structured Streaming to process real-time data.", "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. (from Spark documentation)", "Spark Structured Streaming provides a simple API interface for both batch and streaming workload, compared to Spark Streaming which is powered by RDDs (strictly speaking, we are not dealing with real-time processing here since Spark processes streaming data on a micro-batch basis). Coding with Spark Structured Streaming is very easy; you just define the DataFrame from a streaming data source and the DataFrame operations are the same. The basic concept is to treat a stream as an unbounded, continuously appended table (check Spark documentation for more details).", "We will work in a Notebook environment (DataBricks Community Edition) to illustrate how this works, but eventually, a Spark Streaming will run in your machine (AWS EC2 or EMR) 24/7 in production for the (near) real-time processing purpose.", "First, create a cluster on DataBricks. Two additional steps may require:", "The syntax is almost the same as we read the DataFrame, but now we use \u201creadStream\u201d. Note that:", "Refer to the DataBricks documentation for more details on Kiensis for Structured Streaming.", "The KinesisDF will be a streaming DataFrame, with the schema as follows:", "The data of the live tweets are in the \u201cdata\u201d field, so we need to first parse it to the schema we defined before (i.e., the way you collect the tweet data in the Tweepy Streaming), and then we can process the data and use the Spark LDA model to find the underlying topics for every tweet. The code snippet is as follows and you can find the notebook here.", "Let\u2019s check the schema of the result DataFrame:", "We will write the result DataFrame to an output sink. It could be a DataBase in AWS RedShift, or an AWS ElasticSearch. Here we choose S3 as the output sink.", "This code snippet will start the query and write the result DataFrame to the desire S3 bucket. Note that:", "After a while, we will see the output data show up in the S3 bucket, cool! Now with the data (which keeps increasing in real-time), let\u2019s move to the analysis step.", "Definition from AWS documentation for Athena is as follows. Although it makes more sense to have a DataBase in production, we use Athena which is serverless to access data in S3, so we can save the cost by avoiding spinning up a cloud database service such as AWS RedShift.", "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.", "Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there\u2019s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets. (from Amazon documentation)", "Log in to the AWS console and go to the Athena service. We first need to create a Table which connects to the data in S3. The procedure is quite straightforward. Note that you need to provide the schema for the table because the raw data stored in the S3 bucket is schemaless.", "After creating the table, now you can write the query in the Query editor to do the analysis. For example. show some samples of the tweets and their associated topics and sentiment results:", "Check how the topics distributed on the live tweets:", "Run some more complicated query to get the aggregation information: count the number of tweets in different topics grouped by sentiment:", "Note that Spark Streaming writes new data into the S3 bucket every 15 minutes, so when you run the query again later, you will get the latest result.", "Being able to run the query on the data streaming is nice but not enough, because you will find data visualization is always necessary to provide more insight, particularly in the business. Here we will use QuickSight as a handy tool that can connect to Athena directly. And the in-memory engine, SPICE can make the real-time analysis very efficient.", "Go to the Datasets Tab and create your \u201cdataset\u201d choosing the existing data source in Anetha (see documentation). Here I had one extra step since I wanted to create a new field \u201csentiment_type\u201d from the sentiment score.", "With the dataset, now you can play with Visual Types to create the desired analysis. Some examples for our tweets analysis are as follows.", "Another nice feature is you can schedule the refresh, so the analysis results can be updated automatically.", "I glad you reach this point of reading. The post is much longer than I expected when I started writing. We began with creating our Tweepy Streaming, and used the big data tools for data processing, machine learning model training and streaming processing, then build a real-time dashboard. We had a quick dive into some important concepts in Spark, Streaming. We got a sense of how to build the data architecture for a streaming application. But there are many other important aspects that we can further explore.", "There will be data drift or model performance degradation in the real world, so we need to update our model as time goes by. In most of the use cases, we don\u2019t need the ML model to be updated very frequently. Here, for example, retraining our LDA model monthly will be reasonable. The other extreme case is we want to update the model in real time, which is a very challenging task in practice (Spark supports Streaming Linear Regression and Streaming KMeans). And the latency will be a problem. A more common scenario is batch retraining, and the model retraining can be scheduled periodically or be triggered when drift is detected. This falls into the topic of AutoML and you will find Kubernetes very useful. See a great post on this topic.", "In this post, we use Spark Streaming to get a balance between throughput and latency. It\u2019s barely near real-time. The Spark framework cannot meet the low latency requirement for real-time scoring because of the mini-batch processing.", "However, the question here is: do we really need such low latency? The answer is no. The tweet analysis application we built is not latency-critical, so we can roll it back to the Batch mode and score the new data, let say every 3 hours. Then we need to reflect on this and change our pipeline architecture accordingly. For example,", "Airflow will be the magic to orchestrate the big data pipeline. I will write a post on it following our example here. So please stay tuned!", "There are more to discuss regarding different components in our data architecture. At the end of the day, you have to choose what fits your best depending on the use case. For example, you may find a database is necessary in production instead of using Athena to run ad-hoc queries. Therefore, we can use AWS RedShift as a data warehouse, then leverage Tableau/SuperSet as more powerful data visualization tools rather than QucikSight. Another possible scenario is loading the streaming data into AWS ElasticSearch if your application requires heavy text analysis.", "Big Data tools and Cloud Computing are exciting but can also be intimidating for beginners. I hope you have found this post shed some light on this topic. Thanks for your time! I am interested in discussing best practices in Machine Learning. Feel free to leave your comments and thoughts.", "P.S., I have cited many definitions for AWS and Spark from online documentation. Please kindly let me know if you think any of your work is not properly cited.", "[2] Post: Understanding Apache Spark on YARN.", "[3]Post: Distribution of Executors, Cores and Memory for a Spark Application running in Yarn."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd2240946e64&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/making-sense-of-big-data", "anchor_text": "Making Sense of Big Data"}, {"url": "https://lcxustc.medium.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Chuangxin Lin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890a7b9564d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&user=Chuangxin+Lin&userId=890a7b9564d7&source=post_page-890a7b9564d7----dd2240946e64---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd2240946e64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&user=Chuangxin+Lin&userId=890a7b9564d7&source=-----dd2240946e64---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd2240946e64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&source=-----dd2240946e64---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.tweepy.org/", "anchor_text": "Tweepy"}, {"url": "https://textblob.readthedocs.io/en/dev/", "anchor_text": "TextBlob"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://aws.amazon.com/s3/", "anchor_text": "S3"}, {"url": "https://aws.amazon.com/kinesis/data-streams/", "anchor_text": "Kinesis Data Streams"}, {"url": "https://aws.amazon.com/kinesis/data-firehose/?kinesis-blogs.sort-by=item.additionalFields.createdDate&kinesis-blogs.sort-order=desc", "anchor_text": "Kinesis Firehose"}, {"url": "https://aws.amazon.com/emr/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc", "anchor_text": "EMR"}, {"url": "https://aws.amazon.com/ec2/?ec2-whats-new.sort-by=item.additionalFields.postDateTime&ec2-whats-new.sort-order=desc", "anchor_text": "EC2"}, {"url": "https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc", "anchor_text": "Athena"}, {"url": "https://aws.amazon.com/quicksight/", "anchor_text": "QuickSight"}, {"url": "https://developer.twitter.com/en/apply-for-access", "anchor_text": "Twitter Developer account"}, {"url": "http://docs.tweepy.org/en/v3.5.0/streaming_how_to.html", "anchor_text": "Tweepy streaming API"}, {"url": "https://github.com/Chancylin/twitter_data_analysis/tree/master/spark_streaming/aws_kinesis", "anchor_text": "GitHub"}, {"url": "https://jayendrapatil.com/aws-kinesis-data-streams-vs-kinesis-firehose/#:~:text=Kinesis%20data%20streams%20%E2%80%93%20Kinesis%20data,streaming%20data%20for%20specialized%20needs.&text=Firehose%20also%20allows%20for%20streaming,for%20processing%20through%20additional%20services.", "anchor_text": "Kinesis Data Stream and Kinesis Data Firehose"}, {"url": "https://aws.amazon.com/kinesis/data-streams/faqs/#:~:text=Shard%20is%20the%20base%20throughput,you%20create%20a%20data%20stream.", "anchor_text": "Number of Shards"}, {"url": "https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html", "anchor_text": "Retention Period"}, {"url": "https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html", "anchor_text": "AWS documentation"}, {"url": "https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html", "anchor_text": "AWS documentation"}, {"url": "https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html", "anchor_text": "resharding"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/", "anchor_text": "Learning Spark"}, {"url": "https://databricks.com/product/faq/community-edition", "anchor_text": "community edition"}, {"url": "https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html", "anchor_text": "Note that ResourceManager has two main components: Scheduler and ApplicationsManager."}, {"url": "https://stackoverflow.com/questions/34930108/spark-driver-worker-configuration-does-driver-run-on-master-node", "anchor_text": "For the AWS EMR, if you connect to the master node via ssh and run Spark in client mode, then the Spark drive sits in the master node."}, {"url": "https://aws.amazon.com/blogs/big-data/submitting-user-applications-with-spark-submit/", "anchor_text": "leave some cores (usually one) for the OS"}, {"url": "https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/", "anchor_text": "AWS blog"}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html", "anchor_text": "maximizeResourceAllocation"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cache", "anchor_text": "cache"}, {"url": "https://spark.apache.org/docs/latest/tuning.html", "anchor_text": "configuration"}, {"url": "https://nlp.johnsnowlabs.com/", "anchor_text": "Spark-NLK"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "TF-IDF"}, {"url": "https://planspace.org/20150607-textblob_sentiment/", "anchor_text": "by the average of the pre-defined score of each word in the text"}, {"url": "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html", "anchor_text": "Spark Structured Streaming"}, {"url": "http://Structured Streaming queries", "anchor_text": "Spark documentation"}, {"url": "https://spark.apache.org/docs/latest/streaming-programming-guide.html", "anchor_text": "Spark Streaming"}, {"url": "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html", "anchor_text": "Spark documentation"}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK"}, {"url": "https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html", "anchor_text": "the most recently added record"}, {"url": "https://docs.databricks.com/spark/latest/structured-streaming/kinesis.html", "anchor_text": "DataBricks documentation"}, {"url": "https://github.com/Chancylin/twitter_data_analysis/blob/master/notebook/spark_streaming_write_to_S3_clean.ipynb", "anchor_text": "notebook"}, {"url": "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers", "anchor_text": "other options"}, {"url": "https://aws.amazon.com/athena/?nc=sn&loc=1&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc", "anchor_text": "AWS documentation"}, {"url": "https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html", "anchor_text": "AWS RedShift"}, {"url": "https://aws.amazon.com/athena/?nc=sn&loc=1&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc", "anchor_text": "Amazon documentation"}, {"url": "https://docs.aws.amazon.com/quicksight/latest/user/welcome.html", "anchor_text": "QuickSight"}, {"url": "https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html", "anchor_text": "connect to Athena"}, {"url": "https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html", "anchor_text": "documentation"}, {"url": "https://spark.apache.org/docs/latest/streaming-programming-guide.html#mllib-operations", "anchor_text": "Spark supports Streaming Linear Regression and Streaming KMeans"}, {"url": "https://www.tutorialspoint.com/kubernetes/index.htm", "anchor_text": "Kubernetes"}, {"url": "https://mlinproduction.com/model-retraining/", "anchor_text": "post"}, {"url": "https://airflow.apache.org/docs/apache-airflow/stable/index.html", "anchor_text": "Airflow"}, {"url": "https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/", "anchor_text": "Learning Spark, 2nd Edition"}, {"url": "https://sujithjay.com/spark/with-yarn#:~:text=YARN%20is%20a%20generic%20resource,Spark)%20in%20addition%20to%20MapReduce.", "anchor_text": "Understanding Apache Spark on YARN"}, {"url": "https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html", "anchor_text": "Distribution of Executors, Cores and Memory for a Spark Application running in Yarn"}, {"url": "https://www.jmlr.org/papers/v3/blei03a", "anchor_text": "Latent Dirichlet Allocation"}, {"url": "https://github.com/Chancylin/twitter_data_analysis", "anchor_text": "GitHub Code Repository"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dd2240946e64---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/cloud-computing?source=post_page-----dd2240946e64---------------cloud_computing-----------------", "anchor_text": "Cloud Computing"}, {"url": "https://medium.com/tag/spark?source=post_page-----dd2240946e64---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/aws?source=post_page-----dd2240946e64---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/making-sense-of-big-data?source=post_page-----dd2240946e64---------------making_sense_of_big_data-----------------", "anchor_text": "Making Sense Of Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd2240946e64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&user=Chuangxin+Lin&userId=890a7b9564d7&source=-----dd2240946e64---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd2240946e64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&user=Chuangxin+Lin&userId=890a7b9564d7&source=-----dd2240946e64---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd2240946e64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890a7b9564d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&user=Chuangxin+Lin&userId=890a7b9564d7&source=post_page-890a7b9564d7----dd2240946e64---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F95ae0568d57e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&newsletterV3=890a7b9564d7&newsletterV3Id=95ae0568d57e&user=Chuangxin+Lin&userId=890a7b9564d7&source=-----dd2240946e64---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Written by Chuangxin Lin"}, {"url": "https://lcxustc.medium.com/followers?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "39 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890a7b9564d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&user=Chuangxin+Lin&userId=890a7b9564d7&source=post_page-890a7b9564d7----dd2240946e64---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F95ae0568d57e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-real-time-twitter-analysis-using-big-data-tools-dd2240946e64&newsletterV3=890a7b9564d7&newsletterV3Id=95ae0568d57e&user=Chuangxin+Lin&userId=890a7b9564d7&source=-----dd2240946e64---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/use-shap-loss-values-to-debug-monitor-your-model-83f7808af40f?source=author_recirc-----dd2240946e64----0---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=author_recirc-----dd2240946e64----0---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=author_recirc-----dd2240946e64----0---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Chuangxin Lin"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----dd2240946e64----0---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/use-shap-loss-values-to-debug-monitor-your-model-83f7808af40f?source=author_recirc-----dd2240946e64----0---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Use SHAP loss values to debug/monitor your modelResponsible AI has been a very hot topic in recent years. Accountability and explainability now become the necessary components of your\u2026"}, {"url": "https://towardsdatascience.com/use-shap-loss-values-to-debug-monitor-your-model-83f7808af40f?source=author_recirc-----dd2240946e64----0---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "11 min read\u00b7Jun 22, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83f7808af40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fuse-shap-loss-values-to-debug-monitor-your-model-83f7808af40f&user=Chuangxin+Lin&userId=890a7b9564d7&source=-----83f7808af40f----0-----------------clap_footer----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/use-shap-loss-values-to-debug-monitor-your-model-83f7808af40f?source=author_recirc-----dd2240946e64----0---------------------196483de_c3b2_40be_a919_b49b48d092c7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83f7808af40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fuse-shap-loss-values-to-debug-monitor-your-model-83f7808af40f&source=-----dd2240946e64----0-----------------bookmark_preview----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dd2240946e64----1---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----dd2240946e64----1---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----dd2240946e64----1---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----dd2240946e64----1---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dd2240946e64----1---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dd2240946e64----1---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dd2240946e64----1---------------------196483de_c3b2_40be_a919_b49b48d092c7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----dd2240946e64----1-----------------bookmark_preview----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----dd2240946e64----2---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----dd2240946e64----2---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----dd2240946e64----2---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----dd2240946e64----2---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----dd2240946e64----2---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----dd2240946e64----2---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----dd2240946e64----2---------------------196483de_c3b2_40be_a919_b49b48d092c7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----dd2240946e64----2-----------------bookmark_preview----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/salary-satisfaction-trend-of-data-jobs-f47bdf72afa3?source=author_recirc-----dd2240946e64----3---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=author_recirc-----dd2240946e64----3---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=author_recirc-----dd2240946e64----3---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Chuangxin Lin"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----dd2240946e64----3---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/salary-satisfaction-trend-of-data-jobs-f47bdf72afa3?source=author_recirc-----dd2240946e64----3---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "Salary, Satisfaction, Trend of Data JobsWhat Can We Learn From Stack Overflow Survey Data"}, {"url": "https://towardsdatascience.com/salary-satisfaction-trend-of-data-jobs-f47bdf72afa3?source=author_recirc-----dd2240946e64----3---------------------196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": "12 min read\u00b7Jun 6, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff47bdf72afa3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsalary-satisfaction-trend-of-data-jobs-f47bdf72afa3&user=Chuangxin+Lin&userId=890a7b9564d7&source=-----f47bdf72afa3----3-----------------clap_footer----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/salary-satisfaction-trend-of-data-jobs-f47bdf72afa3?source=author_recirc-----dd2240946e64----3---------------------196483de_c3b2_40be_a919_b49b48d092c7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff47bdf72afa3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsalary-satisfaction-trend-of-data-jobs-f47bdf72afa3&source=-----dd2240946e64----3-----------------bookmark_preview----196483de_c3b2_40be_a919_b49b48d092c7-------", "anchor_text": ""}, {"url": "https://lcxustc.medium.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "See all from Chuangxin Lin"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----dd2240946e64----0-----------------bookmark_preview----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/codex/amazon-redshift-vs-athena-vs-glue-comparison-6ecfb8e92349?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://romanceresnak.medium.com/?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://romanceresnak.medium.com/?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Roman Ceresnak, PhD"}, {"url": "https://medium.com/codex?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/amazon-redshift-vs-athena-vs-glue-comparison-6ecfb8e92349?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Amazon Redshift vs Athena vs Glue. ComparisonAWS provides hundreds of services and sometimes it is very difficult to choose among them those that are ideal for us in terms of speed and\u2026"}, {"url": "https://medium.com/codex/amazon-redshift-vs-athena-vs-glue-comparison-6ecfb8e92349?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "\u00b713 min read\u00b7Jan 10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F6ecfb8e92349&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Famazon-redshift-vs-athena-vs-glue-comparison-6ecfb8e92349&user=Roman+Ceresnak%2C+PhD&userId=2ae5dcefe5f9&source=-----6ecfb8e92349----1-----------------clap_footer----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/codex/amazon-redshift-vs-athena-vs-glue-comparison-6ecfb8e92349?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ecfb8e92349&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Famazon-redshift-vs-athena-vs-glue-comparison-6ecfb8e92349&source=-----dd2240946e64----1-----------------bookmark_preview----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Josue Luzardo Gebrim"}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Data Quality in Python Pipelines!Discover What It Is And How To Achieve Data Quality In Your Data Streams!"}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "\u00b714 min read\u00b7Mar 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4ad1e8eb6603&operation=register&redirect=https%3A%2F%2Fjlgjosue.medium.com%2Fdata-quality-in-python-pipelines-4ad1e8eb6603&user=Josue+Luzardo+Gebrim&userId=9f59dfc0edf7&source=-----4ad1e8eb6603----0-----------------clap_footer----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----dd2240946e64----0---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4ad1e8eb6603&operation=register&redirect=https%3A%2F%2Fjlgjosue.medium.com%2Fdata-quality-in-python-pipelines-4ad1e8eb6603&source=-----dd2240946e64----0-----------------bookmark_preview----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/@marietruong?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/@marietruong?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Marie Truong"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Can ChatGPT Write Better SQL than a Data Analyst?A LeetCode SQL Competition Between ChatGPT and Me"}, {"url": "https://towardsdatascience.com/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "\u00b76 min read\u00b7Jan 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff079518efab2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2&user=Marie+Truong&userId=4cfa1d0b321f&source=-----f079518efab2----1-----------------clap_footer----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2?source=read_next_recirc-----dd2240946e64----1---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff079518efab2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2&source=-----dd2240946e64----1-----------------bookmark_preview----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dd2240946e64----2---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----dd2240946e64----2---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----dd2240946e64----2---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----dd2240946e64----2---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dd2240946e64----2---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dd2240946e64----2---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dd2240946e64----2---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----dd2240946e64----2-----------------bookmark_preview----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----dd2240946e64----3---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----dd2240946e64----3---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----dd2240946e64----3---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----dd2240946e64----3---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----dd2240946e64----3---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----dd2240946e64----3---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----3-----------------clap_footer----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----dd2240946e64----3---------------------0388ac02_4740_45ac_a80f_3c0a20646775-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----dd2240946e64----3-----------------bookmark_preview----0388ac02_4740_45ac_a80f_3c0a20646775-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dd2240946e64--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----dd2240946e64--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}