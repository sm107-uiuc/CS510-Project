{"url": "https://towardsdatascience.com/expectation-maximization-for-gmms-explained-5636161577ca", "time": 1683008546.585168, "path": "towardsdatascience.com/expectation-maximization-for-gmms-explained-5636161577ca/", "webpage": {"metadata": {"title": "Expectation-Maximization for GMMs explained | by Ma\u00ebl Fabien | Towards Data Science", "h1": "Expectation-Maximization for GMMs explained", "description": "In this article, we will review, in the clearest way that I could come up with, the process of training a Gaussian Mixture Model with EM. By the end of the article, you should have a broader\u2026"}, "outgoing_paragraph_urls": [{"url": "https://maelfabien.github.io/machinelearning/GMM/#", "anchor_text": "https://maelfabien.github.io/machinelearning/GMM/#", "paragraph_index": 64}], "all_paragraphs": ["In this article, we will review, in the clearest way that I could come up with, the process of training a Gaussian Mixture Model with EM. By the end of the article, you should have a broader understanding of GMMs, what EM does, and applications of all of this.", "We will cover the following points:", "As a side note, all the code to generate these graphs and put them into an interactive web application is on a dedicated Github repository:", "If you are interested in EM, you probably already know Gaussian Mixture Models. In this section, I provide some reflections on why we need GMMs and how it compares to other algorithms for certain tasks, so it might still be interesting to read it.", "A GMM is a weighted sum of M components Gaussian densities.", "It is a probabilistic model that assumes that the data points are generated by a mixture of Gaussian Components. The probability distribution function of a GMM can be written as:", "Where the parameters are written as lambda, X is the observations, and we assign a weight to each of the Gaussian densities such that the weights sum up to 1. The parameters of a GMM are:", "We can generate data from Gaussian Mixture Models in Python with random parameters (you will find the code attached in the Github repository), and change the number of components.", "Suppose that you have a dataset with 2 features. When you plot your dataset, it might look like this:", "Quite obviously here, there are 2 clusters. There are two main tasks that we usually want to solve:", "Let us now discuss two common approaches that solve these tasks, and compare them with Gaussian Mixture Models:", "k-Means are a popular choice for clustering. We will see in this article that k-Means is in fact a special case of GMMs when solved with a special type of EM algorithm. But we\u2019ll come back to this.", "Alright, you probably know that k-Means iteratively identifies the coordinates of the centro\u00efds of each cluster. Therefore, it relies only on 1 component, which is the mean of each cluster. It might be an issue when the 2 clusters have overlapping means, but different covariance matrices. We can illustrate how well GMMs work compared to k-Means for this task.", "Below is a summary of the key differences between K-Means and GMMs. You might start to see that if we consider an identity covariance matrix, and under one additional assumption, k-Means and GMMs will eventually end up being the same solution.", "Note that for both k-Means and GMMs, you need to specify the number of clusters that you need, and this can be a tough task. We\u2019ll come back to this.", "If we now talk about the second task, i.e. modeling the distribution, Gaussians are often a natural choice. However, in the case of our data with 2 clusters, choosing a single Gaussian to model this data is a problem. Indeed, if we compute the mean of the data, we might end up in a region where there is absolutely no data point, which is the exact contrary of what Maximum Likelihood Estimates of the mean is supposed to give us.", "Instead, using 2 Gaussians can be much more interesting, and would allow us to have:", "Ultimately, you will need to train a GMM, which means to identify the correct set of parameters that characterize your GMM.", "But how do we solve the GMM and estimate these parameters? Let\u2019s start by solving a single Gaussian as a useful reminder.", "To identify the parameters of a single Gaussian, we apply a Maximum Likelihood Estimation (MLE):", "L is the likelihood of the sequence of observations given the parameters. Our goal is to maximize this likelihood to identify the best set of parameters, which make our observations the most \u201clikely\u201d. For convenience, to work with sums, we usually maximize the log-likelihood, since:", "We just need to set the partial derivative to 0, and we obtain the maximum likelihood estimates of our parameters:", "When we consider a GMM, we have several Gaussian components and a weight factor associated with each component. Similarly, we can re-write the likelihood as:", "We can take the log-likelihood again:", "It looks harder to solve. What if we set the derivative with respect to the k-th mean to 0?", "This is where the single Gaussian approach reaches its limits. This expression is analytically unsolvable! And this is why we need Expectation-Maximization (EM), to overcome this unsolvable expression. This is what we will cover in the next section, which focuses on training GMMs with EM.", "The main idea behind EM is the following:", "Visually, we first suppose that we know to which component Zi = k each observation Xi belongs:", "When we suppose that we know to which component each observation belongs, we say that there is a latent variable Z.", "The EM algorithm is made of a few steps:", "At the final step, we have obtained the optimal parameters of the GMM (\u00b1 the fact the convergence also depends on the initialization).", "In the E-step, as mentioned above, we need to estimate the probability that a given observation Xi belongs to a given component Zk. This is actually what we call pseudo-posteriors, and is denoted:", "Visually, it can be represented as simply estimating the following quantities:", "This value is then plugged into what is called an auxiliary function. The auxiliary function is defined as:", "Where \u0398t is the old parameter value, and \u0398 is the new parameter value.", "The expression of this auxiliary function might seem quite weird, but it can actually be proven to be the lower-bound of the gain of likelihood that we get by updating the parameter values. You don\u2019t need to read this if you are not interested, but this is a way to show that the gain of likelihood by parameter update L(\u0398)-L(\u0398t) has the auxiliary function as a lower bound.", "Recall that we don\u2019t use any other approach because it\u2019s analytically unsolvable. Alright, but where do we plug in the estimates of the pseudo-posteriors? Let\u2019s expand this auxiliary function:", "This is where we plug this value. And this is the end of the E-step. We can now jump to the M-step, where after estimating the value of the auxiliary function, we maximize it to identify the optimal value of the parameters!", "In the \u00ab Maximum \u00bb step (M-step), we maximize the value of Q to find the optimal parameter value:", "And this expression can be solved analytically! We can set the derivatives to 0 with respect to the weights, means, and covariances, and identify the optimal values:", "Alright, there was a bit of maths until there, but if you are still reading, this is where it gets more intuitive. The optimal values of the mean parameters are:", "The mean is defined as a weighted average of the data with a weight showing how likely each observation is to belong to a component of the GMM. Visually, you can represent it this way:", "Intuitively, the points that are closer to the first component are more likely to belong to this first component. But it does not mean that the points that are far from this component have no probability to belong to it. Therefore, we compute this weighted average. The values of the covariances can be seen in a similar way, and are expressed as:", "Finally, the weights are the sum of the probabilities that all points belong to the first cluster, divided by the total number of points:", "We use these new parameter values, and inject them in the E-step again:", "The way the parameter values impact the auxiliary function is through the pseudo-posterior estimation:", "We compute the auxiliary function and maximize it in the M-step again. The overall EM algorithm can be represented schematically as such:", "We can plot the visual training cycle of a GMM this way:", "Again, you\u2019ll find the code of this animation that uses Plotly in the Github repository.", "EM is guaranteed to increase the likelihood over the number of iterations:", "Let\u2019s say it, EM is not trivial, but it\u2019s beautiful. However, it has several limitations that I wanted to talk about:", "Let\u2019s get back to this last point. To select the right number of components, we must define a criterion to optimize, iterative over the number of components, and see which one optimizes the criterion. There are two criteria to do that:", "I won\u2019t cover the details of this, but simply remember that the BIC tends to penalize more for model complexity (i.e. number of components) that the AIC. This approach typically requires a large computation power, but you would end up with something like this:", "The AIC and BIC need to be minimized. In the example above, 5 is the optimal number of components of the GMM.", "There is one special case that I wanted to mention also. This is the case of Hard or Viterbi EM. There is one major difference with what we saw previously, which by the way is called full EM or soft EM.", "In Hard EM, we make hard choices. We do not consider a likelihood weighted over all possible Z with their probabilities, but we simply select the most probable Z and move forward.", "There are several reasons for which we would use Hard/Viterbi EM:", "The only thing that would change in the expressions we saw above, is that we must replace the summation over all components by a maximum. In this article, we have seen several expressions of the mean. It can be interesting to compare them and see that there are all linked by this pseudo-posterior definition:", "I thought that it would be nice to conclude this (long) article by a practical overview of where GMMs are used. This is only an overview, and I do not claim by far to have an understanding of how widely used GMMs are.", "GMMs are widely used in speech, for example in gender detection, where one GMM for each gender can be fitted on MFCCs, and we attribute the sample to the GMM with the highest likelihood.", "This is one of the so many applications in speech processing.", "GMMs are also used for background subtraction in computer vision for example, where the background is a given cluster, and the moving objects to keep are another cluster.", "Looking at k-Means as a special case of GMMs, k-Means are used in vector quantization. It is a compression method for images for example, which prevents from storing the value of each pixel, but simply the clusters and the values identified by k-Means. To be more specific, k-Means is one of the methods that can be used to perform VQ.", "I hope you found this article useful. Please leave a comment with your feedback on what was clear and what would need a better explanation/illustration. It took me some time to put all of this together, and I would like to refer to some excellent references that I combined.", "This article is published as a bunch of slides on my personal blog (with close to 200 other articles) right here: https://maelfabien.github.io/machinelearning/GMM/#", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5636161577ca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5636161577ca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5636161577ca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mael4impact?source=post_page-----5636161577ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mael4impact?source=post_page-----5636161577ca--------------------------------", "anchor_text": "Ma\u00ebl Fabien"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddcee06de4c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=post_page-ddcee06de4c8----5636161577ca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5636161577ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5636161577ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/towards-data-science/inside-ai/home", "anchor_text": "Inside AI"}, {"url": "https://github.com/maelfabien/EM_GMM_HMM", "anchor_text": "maelfabien/EM_GMM_HMMIllustrating EM for GMMs and HMMs. Contribute to maelfabien/EM_GMM_HMM development by creating an account on GitHub.github.com"}, {"url": "https://math.stackexchange.com/questions/3122532/gaussian-mixture-model-what-is-a-universal-approximator-of-densities", "anchor_text": "https://math.stackexchange.com/questions/3122532/gaussian-mixture-model-what-is-a-universal-approximator-of-densities"}, {"url": "https://maelfabien.github.io/machinelearning/GMM/#", "anchor_text": "https://maelfabien.github.io/machinelearning/GMM/#"}, {"url": "http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html", "anchor_text": "http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html"}, {"url": "https://stephens999.github.io/fiveMinuteStats/intro_to_em.html", "anchor_text": "https://stephens999.github.io/fiveMinuteStats/intro_to_em.html"}, {"url": "https://ttic.uchicago.edu/~dmcallester/ttic101-07/lectures/em/em.pdf", "anchor_text": "https://ttic.uchicago.edu/~dmcallester/ttic101-07/lectures/em/em.pdf"}, {"url": "https://www.cs.cmu.edu/~tom/10601_fall2012/recitations/em.pdf", "anchor_text": "https://www.cs.cmu.edu/~tom/10601_fall2012/recitations/em.pdf"}, {"url": "http://luthuli.cs.uiuc.edu/~daf/courses/CS-498-DAF-PS/Lecture%2012%20-%20K-means,%20GMMs,%20EM.pdf", "anchor_text": "http://luthuli.cs.uiuc.edu/~daf/courses/CS-498-DAF-PS/Lecture%2012%20-%20K-means,%20GMMs,%20EM.pdf"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.649.8642&rep=rep1&type=pdf", "anchor_text": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.649.8642&rep=rep1&type=pdf"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5636161577ca---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----5636161577ca---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/statistics?source=post_page-----5636161577ca---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----5636161577ca---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/programming?source=post_page-----5636161577ca---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5636161577ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=-----5636161577ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5636161577ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=-----5636161577ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5636161577ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5636161577ca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5636161577ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5636161577ca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5636161577ca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5636161577ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5636161577ca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5636161577ca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5636161577ca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5636161577ca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5636161577ca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5636161577ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mael4impact?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mael4impact?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ma\u00ebl Fabien"}, {"url": "https://medium.com/@mael4impact/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "749 Followers"}, {"url": "http://biped.ai", "anchor_text": "biped.ai"}, {"url": "https://bento.me/mael4impact", "anchor_text": "https://bento.me/mael4impact"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddcee06de4c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=post_page-ddcee06de4c8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc8a99819d6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-for-gmms-explained-5636161577ca&newsletterV3=ddcee06de4c8&newsletterV3Id=c8a99819d6f0&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}