{"url": "https://towardsdatascience.com/what-are-transformers-and-how-can-you-use-them-f7ccd546071a", "time": 1683017140.5357409, "path": "towardsdatascience.com/what-are-transformers-and-how-can-you-use-them-f7ccd546071a/", "webpage": {"metadata": {"title": "What are transformers and how can you use them? | Towards Data Science", "h1": "What are transformers and how can you use them?", "description": "One innovation that has taken natural language processing to new heights in the last three years was the development of transformers."}, "outgoing_paragraph_urls": [{"url": "https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf", "anchor_text": "lecture notes", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "layer normalization", "paragraph_index": 18}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace", "paragraph_index": 26}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "GitHub", "paragraph_index": 26}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "GitHub repository", "paragraph_index": 28}, {"url": "http://www.eng.uwaterloo.ca/~jcslee/poetry/churchill_nevergivein.html", "anchor_text": "Never Give In", "paragraph_index": 36}, {"url": "https://github.com/AmolMavuduru/TransformersExamples", "anchor_text": "GitHub", "paragraph_index": 44}], "all_paragraphs": ["One innovation that has taken natural language processing to new heights in the last three years was the development of transformers. And no, I\u2019m not talking about the giant robots that turn into cars in the famous science-fiction film series directed by Michael Bay.", "Transformers are semi-supervised machine learning models that are primarily used with text data and have replaced recurrent neural networks in natural language processing tasks. The goal of this article is to explain how transformers work and to show you how you can use them in your own machine learning projects.", "Transformers were originally introduced by researchers at Google in the 2017 NIPS paper Attention is All You Need. Transformers are designed to work on sequence data and will take an input sequence and use it to generate an output sequence one element at a time.", "For example, a transformer could be used to translate a sentence in English into a sentence in French. In this case, a sentence is basically treated as a sequence of words. A transformer has two main segments, the first is an encoder that operates primarily on the input sequence and the second is a decoder that operates on the target output sequence during training and predicts the next item in the sequence. In a machine translation problem, for example, the transformer may take a sequence of words in English and iteratively predict the next French word in the proper translation until the sentence has been completely translated. The diagram below demonstrates how a transformer is put together, with the encoder on the left and the decoder on the right.", "It looks like there\u2019s a lot going on in the diagram above, so let\u2019s take a look at each component separately. The parts of a transformer that are particularly important are the embeddings, the positional encoding block, and the multi-head attention blocks.", "If you have ever worked with word embeddings using the Word2Vec algorithm, the input and output embeddings are basically just embedding layers. The embedding layer takes a sequence of words and learns a vector representation for each word.", "In the image above, a word embedding has been created for the sentence \u201cthe quick brown fox jumped over the lazy dog\u201d. Notice how the sentence with nine words has been transformed into a 9 x 5 embedding matrix.", "The Word2Vec algorithm uses a large sample of text as training data and learns word embeddings using one of two algorithms:", "Word2Vec uses a shallow neural network with just one hidden layer to make these predictions. The word vectors come from the weights learned in the hidden layer and are used to represent the semantic meaning of each word in relation to other words. The idea behind Word2Vec is that words with similar meanings will have similar embedding vectors. For a more comprehensive explanation of this algorithm, please see these lecture notes from Stanford\u2019s NLP class.", "What\u2019s important to understand from this description is that the input and output embeddings both take a text document and produce an embedding matrix with an embedding vector for each word.", "The positional encoding block applies a function to the embedding matrix that allows a neural network to understand the relative position of each word vector even if the matrix was shuffled. This might seem insignificant, but you will see why it\u2019s important when I describe the attention blocks in detail.", "The positional encoding blocks inject information about the position of each word vector by concatenating sine and cosine functions of different wavelengths/frequencies to these vectors as demonstrated in the equations below.", "Given the equations below, if we consider an input with 10,000 possible positions, the positional encoding block will add sine and cosine values with wavelengths that increase geometrically from 2\ud835\udf45 to 10000*2\ud835\udf45. This allows us to mathematically represent the relative position of word vectors such that a neural network can learn to recognize differences in position.", "The multi-head attention block is the main innovation behind transformers. The question that the attention block aims to answer is what parts of the text should the model focus on? This is exactly why it is called an attention block. Each attention block takes three input matrices:", "This concept is best explained through a practical example. Let\u2019s say the query matrix has values that represent a sentence in English such as \u201cthe quick brown fox jumped\u201d. Let\u2019s say that our goal is to translate this sentence into French. In this case, the transformer will have learned weights for individual English words in a key matrix and the query matrix will represent the actual input sentence. Computing the dot product of the query and key matrix is known as self-attention and will produce an output that looks something like this.", "Note that the key matrix contains representations of each word and the dot product is essentially a matrix of similarity scores between the query matrix and the key matrix. These scores are later scaled by dividing the dot product matrix by the square root of the number of dimensions in the key and query matrices. A softmax activation function is applied to the scaled scores to convert them into probabilities. These probabilities are referred to as the attention weights, which are then multiplied by the value matrix to produce the final output of the attention block. The final output of the attention block is defined using the equation below:", "Note that n was previously defined as the number of dimensions in the query matrix (Q) and the key matrix (K). The key and value matrices are learned parameters while the query matrix is defined by the input word vectors. It is also important to note that the words of a sentence are passed into the transformer at the same time and the concept of a sequential order present in LSTMs is not that apparent with transformers. This is why the positional encoding blocks mentioned earlier are important. They allow attention block to understand the relative position of words in sentences.", "A single attention block can tell a model to pay attention to something specific such as the tense in a sentence. Adding multiple attention blocks allows the model to pay attention to different linguistic elements such as part of speech, tense, nouns, verbs, and so on.", "This layer simply takes the outputs from the multi-head attention block, adds them together, and normalizes the result with layer normalization. If you have heard of batch normalization, layer normalization is similar but instead of normalizing the input features across the batch dimensions, it normalizes the inputs to a layer across all features.", "This layer needs very little explanation. It is simply a single fully-connected layer of a feed-forward neural network. The feed-forward layer operates on the output attention vectors and learns to recognize patterns within them.", "Now that we have covered each of the building blocks of a transformer, we can see how they fit together in the encoder and decoder segments.", "The encoder is the part of the transformer that chooses what parts of the input to focus on. The encoder can take a sentence such as \u201cthe quick brown fox jumped\u201d, computes the embedding matrix, and then converts it into a series of attention vectors. The multi-head attention block initially produces these attention vectors, which are then added and normalized, passed into a fully-connected layer (Feed Forward in the diagram above), and normalized again before being passed over to the decoder.", "During training, the decoder operates directly on the target output sequence. As per our example, let\u2019s assume the target output is the French translation of the English sentence \u201cthe quick brown fox jumped\u201d, which translates to \u201cle renard brun rapide a saut\u00e9\u201d in French. In the decoder, separate embedding vectors are computed for each French word in the sentence, and the positional encoding is also applied in the form of sine and cosine functions.", "However, a masked attention block is used, meaning that only the previous word in the French sentence is used and the other words are masked. This allows the transformer to learn to predict the next French word. The outputs of this masked attention block are added and normalized before being passed to another attention block that also receives the attention vectors produced by the encoder.", "A feed-forward network receives the final attention vectors and uses them to produce a single vector with a dimension equal to the number of unique words in the model\u2019s vocabulary. Applying the softmax activation function to this vector produces a set of probabilities corresponding to each word. In the context of our example, these probabilities predict the likelihood of each French word appearing next in the translation. This is how a transformer performs tasks such as machine translation and text generation. Just as demonstrated in the figure below, a transformer iteratively predicts the next word in a translated sentence when performing translation tasks.", "In the last few years, several architectures based on the basic transformer introduced in the 2017 paper have been developed and trained for complex natural language processing tasks. Some of the most common transformer models that were created recently are listed below:", "Transformers are definitely useful and as of 2020, are considered state-of-the-art NLP models. But implementing them seems quite difficult for the average machine learning practitioner. Luckily, HuggingFace has implemented a Python package for transformers that is really easy to use. It is open-source and you can find it on GitHub.", "To install the transformers package run the following pip command:", "Make sure to install the library in a virtual environment as per the instructions provided in the GitHub repository. This package allows you to not only use pre-trained state-of-the-art transformers such as BERT and GPT for standard tasks but also lets you finetune them for your own tasks. Consider some of the examples below.", "The transformers package from HuggingFace has a really simple interface provided through the pipeline module that makes it easy to use pre-trained transformers for standard tasks such as sentiment analysis. Consider the example below.", "Running this code produces a dictionary indicating the sentiment of the text.", "We can also use the pipeline module for answering questions given some context information as demonstrated in the example below.", "Running the code produces the output shown below.", "Interestingly, the transformer not only gives us the answer to the question about the name of the dog but also tells us where we can find the answer in the context string.", "In this article, I gave the example of translating English sentences to French in order to demonstrate how transformers work. The pipeline module, as expected, allows us to use transformer models to translate text from one language to another as demonstrated below.", "Running the code above produces the French translation shown below.", "We can also use transformers for text summarization. In the example below, I used the T5 transformer to summarize Winston Churchill\u2019s famous \u201cNever Give In\u201d speech in 1941 during one of the darkest times in World War II.", "Running the code above produces this concise and beautifully worded summary below.", "We can also fine-tune pre-trained transformers for text classification tasks using transfer learning. In one of my previous articles, I used recurrent convolutional neural networks for classifying fake news articles.", "In the example below, I used a preprocessed version of the same fake news dataset to train a BERT transformer model to detect fake news. Fine-tuning models requires a few extra steps so the sample code I provided is understandable but a bit more complicated than the previous examples. We not only have to import the transformer model, but also a tokenizer that can transform a text document into a series of integer tokens corresponding to different words as demonstrated in the image below.", "Please note that I ran the code below on a GPU instance in AWS SageMaker because the training process is computationally expensive. If you plan on running this code yourself, I would recommend using a GPU.", "There\u2019s a lot going on in the code above so here\u2019s an overview of the steps that I performed in the process of fine-tuning the BERT transformer:", "The code produced the following output after the training process was complete:", "The finetuned BERT model achieved a validation accuracy of 95.81 percent after just one training epoch, which is quite impressive. With more training epochs, it may achieve an even higher validation accuracy.", "As usual, I have made the full code for this article available on GitHub.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff7ccd546071a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://amolmavuduru.medium.com/?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": ""}, {"url": "https://amolmavuduru.medium.com/?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "Amol Mavuduru"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F511816e5976d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&user=Amol+Mavuduru&userId=511816e5976d&source=post_page-511816e5976d----f7ccd546071a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7ccd546071a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7ccd546071a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com/s/photos/transformers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is All You Need"}, {"url": "https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf", "anchor_text": "lecture notes"}, {"url": "https://arxiv.org/abs/1607.06450", "anchor_text": "layer normalization"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "T5"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT-2"}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "GitHub"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "GitHub repository"}, {"url": "http://www.eng.uwaterloo.ca/~jcslee/poetry/churchill_nevergivein.html", "anchor_text": "Never Give In"}, {"url": "https://towardsdatascience.com/fake-news-classification-with-recurrent-convolutional-neural-networks-4a081ff69f1a", "anchor_text": "Fake News Classification with Recurrent Convolutional Neural NetworksIntroductiontowardsdatascience.com"}, {"url": "https://github.com/AmolMavuduru/TransformersExamples", "anchor_text": "GitHub"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need"}, {"url": "https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf", "anchor_text": "Word Vectors I: Introduction, SVD and Word2Vec"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f7ccd546071a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f7ccd546071a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f7ccd546071a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----f7ccd546071a---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f7ccd546071a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff7ccd546071a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&user=Amol+Mavuduru&userId=511816e5976d&source=-----f7ccd546071a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff7ccd546071a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&user=Amol+Mavuduru&userId=511816e5976d&source=-----f7ccd546071a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7ccd546071a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff7ccd546071a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f7ccd546071a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f7ccd546071a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f7ccd546071a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f7ccd546071a--------------------------------", "anchor_text": ""}, {"url": "https://amolmavuduru.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://amolmavuduru.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Amol Mavuduru"}, {"url": "https://amolmavuduru.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "565 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F511816e5976d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&user=Amol+Mavuduru&userId=511816e5976d&source=post_page-511816e5976d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1f3b67de8f9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-transformers-and-how-can-you-use-them-f7ccd546071a&newsletterV3=511816e5976d&newsletterV3Id=1f3b67de8f9c&user=Amol+Mavuduru&userId=511816e5976d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}