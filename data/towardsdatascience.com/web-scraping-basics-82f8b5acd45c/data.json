{"url": "https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c", "time": 1683010947.342614, "path": "towardsdatascience.com/web-scraping-basics-82f8b5acd45c/", "webpage": {"metadata": {"title": "Web Scraping Basics. How to scrape data from a website in\u2026 | by Songhao Wu | Towards Data Science", "h1": "Web Scraping Basics", "description": "How to scrape data from website in Python, BeautifulSoup"}, "outgoing_paragraph_urls": [{"url": "http://linkedin.com/in/songhaowu/", "anchor_text": "linkedin.com/in/songhaowu/", "paragraph_index": 45}], "all_paragraphs": ["We always say \u201cGarbage in Garbage out\u201d in data science. If you do not have good quality and quantity of data, most likely you would not get many insights out of it. Web Scraping is one of the important methods to retrieve third-party data automatically. In this article, I will be covering the basics of web scraping and use two examples to illustrate the 2 different ways to do it in Python.", "Web Scraping is an automatic way to retrieve unstructured data from a website and store them in a structured format. For example, if you want to analyze what kind of face mask can sell better in Singapore, you may want to scrape all the face mask information on an E-Commerce website like Lazada.", "Scraping makes the website traffic spike and may cause the breakdown of the website server. Thus, not all websites allow people to scrape. How do you know which websites are allowed or not? You can look at the \u2018robots.txt\u2019 file of the website. You just simply put robots.txt after the URL that you want to scrape and you will see information on whether the website host allows you to scrape the website.", "You can see that Google does not allow web scraping for many of its sub-websites. However, it allows certain paths like \u2018/m/finance\u2019 and thus if you want to collect information on finance then this is a completely legal place to scrape.", "Another note is that you can see from the first row on User-agent. Here Google specifies the rules for all of the user-agents but the website may give certain user-agent special permission so you may want to refer to information there.", "Web scraping just works like a bot person browsing different pages website and copy pastedown all the contents. When you run the code, it will send a request to the server and the data is contained in the response you get. What you then do is parse the response data and extract out the parts you want.", "Alright, finally we are here. There are 2 different approaches for web scraping depending on how does website structure their contents.", "Approach 1: If website stores all their information on the HTML front end, you can directly use code to download the HTML contents and extract out useful information.", "There are roughly 5 steps as below:", "Pros and Cons for this approach: It is simple and direct. However, if the website's front-end structure changes then you need to adjust your code accordingly.", "Approach 2: If website stores data in API and the website queries the API each time when user visit the website, you can simulate the request and directly query data from the API", "Pros and Cons for this approach: It is definitely a preferred approach if you can find the API request. The data you receive will be more structured and stable. This is because compared to the website front end, it is less likely for the company to change its backend API. However, it is a bit more complicated than the first approach especially if authentication or token is required.", "There are many different scraping tools available that do not require any coding. However, most people still use the Python library to do web scraping because it is easy to use and also you can find an answer in its big community.", "The most commonly used library for web scraping in Python is Beautiful Soup, Requests, and Selenium.", "Beautiful Soup: It helps you parse the HTML or XML documents into a readable format. It allows you to search different elements within the documents and help you retrieve required information faster.", "Requests: It is a Python module in which you can send HTTP requests to retrieve contents. It helps you to access website HTML contents or API by sending Get or Post requests.", "Selenium: It is widely used for website testing and it allows you to automate different events(clicking, scrolling, etc) on the website to get the results you want.", "You can either use Requests + Beautiful Soup or Selenium to do web scraping. Selenium is preferred if you need to interact with the website(JavaScript events) and if not I will prefer Requests + Beautiful Soup because it's faster and easier.", "Problem statement: I want to find out about the local market for face mask. I am interested on online face mask price, discount, ratings, sold quantity etc.", "Approach 1 Example(Download HTML for all pages) \u2014 Lazada:", "Step 1: Inspect the website(if using Chrome you can right-click and select inspect)", "I can see that data I need are all wrap in the HTML element with the unique class name.", "Step 2: Access URL of the website using code and download all the HTML contents on the page", "I used the requests library to get data from a website. You can see that so far what we have is unstructured text.", "Step 3: Format the downloaded content into a readable format", "This step is very straightforward and what we do is just parse unstructured text into Beautiful Soup and what you get is as below.", "The output is a much more readable format and you can search different HTML elements or classes in it.", "Step 4: Extract out useful information and save it into a structured format", "This step requires some time to understand website structure and find out where the data is stored exactly. For the Lazada case, it is stored in a Script section in JSON format.", "I created 5 different lists to store the different fields of data that I need. I used the for loop here to loop through the list of items in the JSON documents inside. After that, I combine the 5 columns into the output file.", "Step 5: For information displayed on multiple pages of the website, you may need to repeat steps 2\u20134 to have the complete information.", "If you want to scrape all the data. Firstly you should find out about the total count of sellers. Then you should loop through pages by passing in incremental page numbers using payload to URL. Below is the full code that I used to scrape and I loop through the first 50 pages to get content on those pages.", "Approach 2 example(Query data directly from API) \u2014 Ezbuy:", "Step 1: Inspect the XHR network section of the URL that you want to crawl and find out the request-response that gives you the data that you want", "I can see from the Network that all product information is listed in this API called \u2018List Product by Condition\u2019. The response gives me all the data I need and it is a POST request.", "Step 2: Depending on the type of request(post or get) and also the request header & payload, simulate the request in your code and retrieve the data from API. Usually, the data got from API is in a pretty neat format.", "Here I create the HTTP POST request using the requests library. For post requests, you need to define the request header(setting of the request) and payload(data you are sending with this post request). Sometimes token or authentication is required here and you will need to request for token first before sending your POST request. Here there is no need to retrieve the token and usually just follow what\u2019s in the request payload in Network and define \u2018user-agent\u2019 for the header.", "Another thing to note here is that inside the payload, I specified limit as 100 and offset as 0 because I found out it only allows me to query 100 data rows at one time. Thus, what we can do later is to use for loop to change offset and query more data points.", "Step 3: Extract out useful information that you need", "Data from API is usually quite neat and structured and thus what I did was just to read it in JSON format. After that, I extract the useful data into different columns and combine them together as output. You can see the data output below.", "Step 4: For API with a limit on query size, you will need to use \u2018for loop\u2019 to repeatedly retrieve all the data", "Here is the complete code to scrape all rows of face mask data in Ezbuy. I found that the total number of rows is 14k and thus I write a for loop to loop through incremental offset number to query all the results. Another important thing to note here is that I put a random timeout at the start of each loop. This is because I do not want very frequent HTTP requests to harm the traffic of the website and get spotted out by the website.", "If you want to scrape a website, I would suggest checking the existence of API first in the network section using inspect. If you can find the response to a request that gives you all the data you need, you can build a stable and neat solution. If you cannot find the data in-network, you should try using requests or Selenium to download HTML content and use Beautiful Soup to format the data. Lastly, please use a timeout to avoid a too frequent visits to the website or API. This may prevent you from being blocked by the website and it helps to alleviate the traffic for the good of the website.", "If you are interested to know more about web scraping using Scrapy in Python can refer to my latest article below", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Enthusiast | Let's have this data journey together! | linkedin.com/in/songhaowu/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F82f8b5acd45c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://songhaowu.medium.com/?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": ""}, {"url": "https://songhaowu.medium.com/?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "Songhao Wu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a26b3749a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&user=Songhao+Wu&userId=33a26b3749a7&source=post_page-33a26b3749a7----82f8b5acd45c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F82f8b5acd45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F82f8b5acd45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/U3sOwViXhkY", "anchor_text": "Unsplash"}, {"url": "https://sg-en-web-api.ezbuy.sg/api/EzCategory/ListProductsByCondition'", "anchor_text": "https://sg-en-web-api.ezbuy.sg/api/EzCategory/ListProductsByCondition'"}, {"url": "https://sg-en-web-api.ezbuy.sg/api/EzCategory/ListProductsByCondition'", "anchor_text": "https://sg-en-web-api.ezbuy.sg/api/EzCategory/ListProductsByCondition'"}, {"url": "https://songhaowu.medium.com/web-scraping-in-scrapy-c2d87796f677", "anchor_text": "Web Scraping using Scrapy in PythonHow to retrieve second-hand cars information in Singaporesonghaowu.medium.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----82f8b5acd45c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----82f8b5acd45c---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----82f8b5acd45c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----82f8b5acd45c---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/python?source=post_page-----82f8b5acd45c---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F82f8b5acd45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&user=Songhao+Wu&userId=33a26b3749a7&source=-----82f8b5acd45c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F82f8b5acd45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&user=Songhao+Wu&userId=33a26b3749a7&source=-----82f8b5acd45c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F82f8b5acd45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F82f8b5acd45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----82f8b5acd45c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----82f8b5acd45c--------------------------------", "anchor_text": ""}, {"url": "https://songhaowu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://songhaowu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Songhao Wu"}, {"url": "https://songhaowu.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "433 Followers"}, {"url": "http://linkedin.com/in/songhaowu/", "anchor_text": "linkedin.com/in/songhaowu/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a26b3749a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&user=Songhao+Wu&userId=33a26b3749a7&source=post_page-33a26b3749a7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7a191443ce50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-basics-82f8b5acd45c&newsletterV3=33a26b3749a7&newsletterV3Id=7a191443ce50&user=Songhao+Wu&userId=33a26b3749a7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}