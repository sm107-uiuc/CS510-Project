{"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6", "time": 1683000326.211197, "path": "towardsdatascience.com/fine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6/", "webpage": {"metadata": {"title": "Fine-grained Sentiment Analysis (Part 3): Fine-tuning Transformers | by Prashanth Rao | Towards Data Science", "h1": "Fine-grained Sentiment Analysis (Part 3): Fine-tuning Transformers", "description": "An NLP case study for fine-grained sentiment analysis: Applying transfer learning to a pre-trained PyTorch causal transformer and using a LIME explainer for classification on the SST-5 dataset"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "Parts 1", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d", "anchor_text": "2", "paragraph_index": 0}, {"url": "https://hackingsemantics.xyz/2019/leaderboards/", "anchor_text": "dominating NLP task leaderboards", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "first article", "paragraph_index": 1}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "GPT", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet", "paragraph_index": 4}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "OpenAI\u2019s GPT-2", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/1609.08144.pdf", "anchor_text": "see section 4.1 in this paper", "paragraph_index": 9}, {"url": "https://huggingface.co/pytorch-transformers/model_doc/bert.html?highlight=berttokenize#pytorch_transformers.BertTokenizer", "anchor_text": "as implemented in HuggingFace\u2019s", "paragraph_index": 9}, {"url": "https://huggingface.co/pytorch-transformers/model_doc/bert.html?highlight=berttokenize#pytorch_transformers.BertTokenizer", "anchor_text": "pytorch_transformers", "paragraph_index": 9}, {"url": "https://huggingface.co/pytorch-transformers/model_doc/bert.html?highlight=berttokenize#pytorch_transformers.BertTokenizer", "anchor_text": "library", "paragraph_index": 9}, {"url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader", "anchor_text": "PyTorch", "paragraph_index": 9}, {"url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader", "anchor_text": "DataLoader", "paragraph_index": 9}, {"url": "https://medium.com/swlh/transformer-fine-tuning-for-sentiment-analysis-c000da034bb5", "anchor_text": "In his Medium post", "paragraph_index": 10}, {"url": "https://github.com/google-research/bert/blob/master/README.md", "anchor_text": "maximum sequence length that can be handled by BERT", "paragraph_index": 11}, {"url": "https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/utils.py", "anchor_text": "pretrained model provided by HuggingFace,", "paragraph_index": 19}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/training/train_transformer.py", "anchor_text": "training/train_transformer.py", "paragraph_index": 19}, {"url": "https://pytorch.org/ignite/contrib/handlers.html#ignite.contrib.handlers.param_scheduler.PiecewiseLinear", "anchor_text": "piecewise linear scheduler from", "paragraph_index": 21}, {"url": "https://pytorch.org/ignite/contrib/handlers.html#ignite.contrib.handlers.param_scheduler.PiecewiseLinear", "anchor_text": "pytorch-ignite", "paragraph_index": 21}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "ULMFiT, Howard and Ruder, 2018", "paragraph_index": 21}, {"url": "https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255", "anchor_text": "Practical tips for training neural nets", "paragraph_index": 22}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/classifiers.py", "anchor_text": "classifiers.py", "paragraph_index": 24}, {"url": "https://drive.google.com/open?id=1_6PoXN5THRB9Px-r7GwVx-8H7fNwNAIH", "anchor_text": "available in this Google drive link", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d", "anchor_text": "Part 2", "paragraph_index": 29}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py", "anchor_text": "explainer.py", "paragraph_index": 29}, {"url": "https://drive.google.com/open?id=1_6PoXN5THRB9Px-r7GwVx-8H7fNwNAIH", "anchor_text": "available in this Google drive link", "paragraph_index": 29}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d", "anchor_text": "see Part 2", "paragraph_index": 34}, {"url": "https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/preview?pru=AAABaz2o8Jk*rdDLH7fXP7h4HQFLtzvHNQ#slide=id.g5888218f39_177_4", "anchor_text": "NAACL 2019 tutorial slides", "paragraph_index": 41}], "all_paragraphs": ["This is Part 3 of a series on fine-grained sentiment analysis in Python. Parts 1 and 2 covered the analysis and explanation of six different classification methods on the Stanford Sentiment Treebank fine-grained (SST-5) dataset. In this post, we\u2019ll look at how to improve on past results by building a transformer-based model and applying transfer learning, a powerful method that has been dominating NLP task leaderboards lately.", "From the first article in this series, the below classification accuracies and F1-scores were obtained on the SST-5 dataset:", "In the below sections, we\u2019ll discuss the key training, evaluation and explanation steps that show why transformers are inherently better than the above listed approaches for this task.", "At the heart of the transformer architecture are the following key ideas that make it highly suitable for interpreting complex patterns in natural language:", "In essence, all of the top transformer-based methods of late (GPT, BERT, XLNet) use such a sequential transfer learning approach. Internally, they train a language model during the pretraining stage using unlabelled data from a large corpus, following which an additional task-specific module (attached downstream of the language model) is fine-tuned on custom data. Below is a visualization of this approach for BERT on multiple benchmark datasets.", "The pretraining step involves training a language model in an unsupervised manner\u200a\u2014\u200athis determines how the model learns syntax, semantics and contextual information from the training corpus it is given. Recent evidence (from models such as OpenAI\u2019s GPT-2) indicates that really large language models, given sufficient data and compute, will learn a great deal about language syntax! For the purposes of the transformer model described later in this article, the below pretraining objectives are useful to note.", "For the rest of this post, we will work with a transformer model trained using a causal language modelling objective (similar to, but much smaller than GPT/GPT-2).", "With that bit of background behind us, we can move on to some coding!", "Some basic preprocessing is done on the SST-5 dataset using Pandas. Note that the class labels are decremented by 1 (to be in the range [0, 1, 2, 3, 4]) because PyTorch expects the labels to be zero-indexed.", "The first step in processing the data is to perform tokenization using the WordPiece tokenizer\u2014 [see section 4.1 in this paper for more details]. We use the BertTokenizer as implemented in HuggingFace\u2019s pytorch_transformers library. Next, the tokenized text is encoded to sequences of integers to be processed by our transformer model. Following this, a PyTorch DataLoader is created to load the samples onto batches for training.", "In his Medium post on transformer fine-tuning for sentiment analysis, Oliver Atanaszov wrote a very nice TextProcessor class that encapsulates the tokenization, encoding and data preparation steps (for PyTorch). This class made use of multiple CPU cores to speed up the encoding process, so it is adapted and reused here.", "Note that in this example, we fix the maximum length of the sequence to be 256 \u2014 in theory the maximum sequence length that can be handled by BERT and similar model encoders is 512, but since SST-5 is a relatively small benchmark dataset with relatively short samples, we truncate the maximum token sequence length to 256 to reduce memory usage and model size.", "A special classification token \u2018[CLS]\u2019 is added at the end of each sequence \u2014 this token is used in classification tasks as an aggregate of each sequence\u2019s representation, to learn which class the sequence belongs to. For sequences that are shorter than 256, a padding token \u2018[PAD]\u2019 is added to ensure all batches remain the same size to be loaded into GPU memory during training.", "The general structure of the stack of transformer blocks used for classification tasks is shown below. This is a modified form of the original version used for machine translation by Vaswani et al.", "In PyTorch code, the above structure looks as follows.", "A base Transformer class that inherits from PyTorch\u2019s nn.module is defined. The input sequence (in our case, the text for sentiment classification) is fed to the transformer blocks by summing up the sequence\u2019s token and position embeddings. Each successive transformer block is composed of the following modules:", "Two \u201cmasks\u201d are defined in the feed-forward module of our network.", "The downstream portion of the model uses a linear classification layer on top of the existing transformer. The TransformerWithClfHead class inherits from the base Transformer class, and specifies CrossEntropyLoss as the loss function to optimize for. The size of the linear layer is [embedding_dimensions, num_classes] \u2014 in this case for the existing pretrained model and the SST-5 dataset, 410\u00d75.", "The raw outputs, i.e. the logits, are extracted from the classification layer and fed to a softmax function to generate a class probability vector (1\u00d75) as output.", "The model\u2019s weights are initialized from the pretrained model provided by HuggingFace, and the training script [training/train_transformer.py] is run on the SST-5 dataset.", "The following hyperparameters are used for training the model\u2014 note that the number of embedding dimensions, number of attention heads, etc. are not set explicitly \u2014 these are inherited from the pretrained model. After 3 training epochs the model is checkpointed and its config parameters are saved.", "\ud83d\udca1 Linear warmup schedule: Rather than setting a constant learning rate, a piecewise linear scheduler from pytorch-ignite is defined to ramp up the learning rate for the early portion of training, followed by ramping down linearly to zero. This is typically a good way to ensure good transfer of knowledge during transfer learning (similar to the \u201cslanted-triangular learning rate\u201d in ULMFiT, Howard and Ruder, 2018).", "\ud83d\udca1 Gradient accumulation: As Thomas Wolf so nicely describes in his post \u201cPractical tips for training neural nets\u201d, a good way to simulate larger batch sizes without running into GPU memory issues is to accumulate gradients. This is done by summing the gradient tensors from multiple backpropagation steps and then calling the optimizer to minimize the loss\u200a\u2014\u200anote that the loss would then also need to be divided by the number of accumulation steps. Doing this can allow us to train with batch sizes that are larger than what may actually fit in GPU memory, improving the model\u2019s learning.", "In this section, we\u2019ll go line-by-line and dig into how a trained model can be used to make inferences on our SST-5 sentiment dataset, with an example. The input text is tokenized, converted to integer IDs and fed to the model as an appropriately shaped tensor, shown in the below notebook.", "This process is encapsulated using the TransformerSentiment class, in the file classifiers.py, to read in the SST-5 dataset into a Pandas DataFrame and evaluate the transformer\u2019s performance.", "Running the trained transformer model (available in this Google drive link) on the SST-5 test, we can see that the causal transformer improves the classification accuracy to nearly 50%!", "The macro-F1 scores for the transformer are also improved from the other embedding-based methods (FastText and Flair).", "The confusion matrix of the next-best model, Flair, is placed next to the transformer\u2019s for comparison.", "The transformer does make a lot of false predictions belonging to class labels 2 and 4 \u2014 however, it gets significantly more labels belonging to the minority classes (1 and 3) correct compared to all other methods. The fact that it is able to classify these minority classes reasonably well with such a limited number of training samples is testament to the power of using pretrained language models and transfer learning for sentiment classification tasks.", "Following the model explanation method shown in Part 2 of this series, we make LIME explainer visualizations of specific text examples from the SST-5 test set using our trained transformer model. The code used to generate the visualizations below is available in the file explainer.py \u2014 the trained model file is also made available in this Google drive link.", "Example 1: \u201cIt\u2019s not horrible, just horribly mediocre.\u201d", "The transformer appears to correctly identify the words \u201chorrible\u201d and \u201cmediocre\u201d as the two most important features that contribute to this example having the class label 1 (i.e. strongly negative). The word \u201cnot\u201d pushes the prediction probability away from 1 (presumably to label 2 or 3), but its effect is not as strong as the negative words in the sentence. The adverb \u201chorribly\u201d also plays a small role in classifying this sentence as strongly negative, meaning that the model has a notion of how modifiers such as adverbs alter the degree of sentiment in a sentence.", "Example 2: \u201cThe cast is uniformly excellent \u2026 but the film itself is merely mildly charming.\u201d", "In this example, it is clear that the word \u201cbut\u201d has the biggest weight in the transformer\u2019s prediction of a class label 3 (neutral). This is interesting, because while there are a number of words that show varying degrees of polarity in this sentence (\u201cexcellent\u201d, \u201cmerely\u201d, \u201cmildly\u201d and \u201ccharming\u201d), the word \u201cbut\u201d acts as a critical modifier in this sentence \u2014 it marks the transition of the sentence from being strongly positive to being slightly negative in the latter half. Once again, the transformer seems to have a notion of how modifiers (a conjunction in this case) alter the overall degree of sentiment in the sentence.", "Another interesting observation is that the word \u201ccast\u201d plays no role in the sentiment being predicted as neutral (class 3). All the previously used methods (see Part 2), including Flair, erroneously learned the feature importance of the word \u201ccast\u201d as contributing to sentiment \u2014 because this word occurred many times in the training data. The underlying power of the transformer\u2019s causal language model helps it make a more meaningful association between words and the predicted sentiment label, regardless of how many times the word appeared in the training set.", "Through this series, we explored various NLP methods in Python for fine-grained classification on the Stanford Sentiment Treebank (SST-5) dataset. While this dataset is extremely challenging and poses numerous problems for existing text classification methods, it is clear that the current industry standard in NLP \u2014 transformers combined with transfer learning \u2014 show inherently superior performance on classification tasks compared to other methods. This is mainly due to the transformer\u2019s underlying language model representation, which gives it more contextual awareness and a better syntactic understanding of the training vocabulary.", "The below chart summarizes the sequential improvement (using more and more complex models) made in accuracy and F1 scores on the SST-5 dataset.", "To squeeze more performance out of the transformer, a few additional experiments (aside from increasing the number of parameters in the model) could be tried:", "The purpose of building a sentiment analysis framework from the ground up, as described in this series, was to build confidence in our ability to evaluate and interpret different machine learning techniques, as relevant to our problem statement and available data. Making use of large pretrained transformer-based models such as BERT-large, XLNet or RoBERTa would, of course, yield significantly improved performance on real-world datasets\u200a\u2014\u200ahowever, it is important to balance the immense computational cost of using these models versus using smaller, simpler models with a good pretraining objective and clean, well-annotated, domain-specific training data. As more and more NLP techniques expand on the huge success of transformers in the months to come, there can only be interesting times ahead!", "Feel free to reproduce the results and make your own findings using the transformer!", "All the code used for this post was adapted from the excellent example code in the below links:", "For a much more detailed deep-dive into transfer learning in NLP, it is highly recommended to go through the NAACL 2019 tutorial slides by Sebastian Ruder, Matthew Peters, Swabha Swayamdipta and Thomas Wolf.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Engineer by training. Machine Learning practitioner. I like writing about science, technology and computing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1ae6574f25a6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tech_optimist?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "Prashanth Rao"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefdb4f4a0d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=post_page-efdb4f4a0d1b----1ae6574f25a6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ae6574f25a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ae6574f25a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/illustrations/light-bulbs-light-bulb-light-energy-1125016/", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "Parts 1"}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d", "anchor_text": "2"}, {"url": "https://hackingsemantics.xyz/2019/leaderboards/", "anchor_text": "dominating NLP task leaderboards"}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "first article"}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html#training-loop", "anchor_text": "jointly attending to information from different positions in the sequence"}, {"url": "http://ruder.io/state-of-transfer-learning-in-nlp/", "anchor_text": "\u201cThe State of Transfer Learning in NLP\u201d by Sebastian Ruder"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "GPT"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "Devlin et al. 2019"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "OpenAI\u2019s GPT-2"}, {"url": "https://arxiv.org/pdf/1609.08144.pdf", "anchor_text": "see section 4.1 in this paper"}, {"url": "https://huggingface.co/pytorch-transformers/model_doc/bert.html?highlight=berttokenize#pytorch_transformers.BertTokenizer", "anchor_text": "as implemented in HuggingFace\u2019s"}, {"url": "https://huggingface.co/pytorch-transformers/model_doc/bert.html?highlight=berttokenize#pytorch_transformers.BertTokenizer", "anchor_text": "pytorch_transformers"}, {"url": "https://huggingface.co/pytorch-transformers/model_doc/bert.html?highlight=berttokenize#pytorch_transformers.BertTokenizer", "anchor_text": "library"}, {"url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader", "anchor_text": "PyTorch"}, {"url": "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader", "anchor_text": "DataLoader"}, {"url": "https://medium.com/swlh/transformer-fine-tuning-for-sentiment-analysis-c000da034bb5", "anchor_text": "In his Medium post"}, {"url": "https://github.com/google-research/bert/blob/master/README.md", "anchor_text": "maximum sequence length that can be handled by BERT"}, {"url": "https://arxiv.org/pdf/1904.10509.pdf", "anchor_text": "Child et al."}, {"url": "https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/", "anchor_text": "this blog post by Keita Kurita"}, {"url": "https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/", "anchor_text": "introducing random noise during the training process"}, {"url": "https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/", "anchor_text": "ReLU"}, {"url": "https://datascience.stackexchange.com/questions/49522/what-is-gelu-activation", "anchor_text": "GELU"}, {"url": "https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/preview?pru=AAABaz2o8Jk*rdDLH7fXP7h4HQFLtzvHNQ#slide=id.g5888218f39_177_4", "anchor_text": "NAACL 2019 transferlearning tutorial slides"}, {"url": "https://pytorch.org/docs/stable/torch.html#torch.triu", "anchor_text": "torch.triu"}, {"url": "https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/preview?pru=AAABaz2o8Jk*rdDLH7fXP7h4HQFLtzvHNQ#slide=id.g5888218f39_177_4", "anchor_text": "NAACL 2019 transferlearning tutorial slides"}, {"url": "https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/utils.py", "anchor_text": "pretrained model provided by HuggingFace,"}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/training/train_transformer.py", "anchor_text": "training/train_transformer.py"}, {"url": "https://pytorch.org/ignite/contrib/handlers.html#ignite.contrib.handlers.param_scheduler.PiecewiseLinear", "anchor_text": "piecewise linear scheduler from"}, {"url": "https://pytorch.org/ignite/contrib/handlers.html#ignite.contrib.handlers.param_scheduler.PiecewiseLinear", "anchor_text": "pytorch-ignite"}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "ULMFiT, Howard and Ruder, 2018"}, {"url": "https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255", "anchor_text": "Practical tips for training neural nets"}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/classifiers.py", "anchor_text": "classifiers.py"}, {"url": "https://drive.google.com/open?id=1_6PoXN5THRB9Px-r7GwVx-8H7fNwNAIH", "anchor_text": "available in this Google drive link"}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d", "anchor_text": "Part 2"}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py", "anchor_text": "explainer.py"}, {"url": "https://drive.google.com/open?id=1_6PoXN5THRB9Px-r7GwVx-8H7fNwNAIH", "anchor_text": "available in this Google drive link"}, {"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d", "anchor_text": "see Part 2"}, {"url": "https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/preview?pru=AAABaz2o8Jk*rdDLH7fXP7h4HQFLtzvHNQ#slide=id.g5888218f39_177_4", "anchor_text": "NAACL tutorial"}, {"url": "https://github.com/huggingface/naacl_transfer_learning_tutorial", "anchor_text": "which can be done using the"}, {"url": "https://github.com/huggingface/naacl_transfer_learning_tutorial", "anchor_text": "pretraining_train.py"}, {"url": "https://github.com/huggingface/naacl_transfer_learning_tutorial", "anchor_text": "file in HuggingFace\u2019s repository"}, {"url": "https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/preview?pru=AAABaz2o8Jk*rdDLH7fXP7h4HQFLtzvHNQ#slide=id.g5888218f39_177_4", "anchor_text": "NAACL tutorial"}, {"url": "https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3", "anchor_text": "perplexity"}, {"url": "https://github.com/prrao87/fine-grained-sentiment", "anchor_text": "this project\u2019s GitHub repo"}, {"url": "https://drive.google.com/open?id=1_6PoXN5THRB9Px-r7GwVx-8H7fNwNAIH", "anchor_text": "available in this Google drive link"}, {"url": "https://github.com/huggingface/naacl_transfer_learning_tutorial", "anchor_text": "GitHub repository"}, {"url": "https://colab.research.google.com/drive/1iDHCYIrWswIKp-n-pOg69xLoZO09MEgf", "anchor_text": "Google Colab notebook"}, {"url": "https://medium.com/swlh/transformer-fine-tuning-for-sentiment-analysis-c000da034bb5", "anchor_text": "This Medium post by Oliver Atanaszov"}, {"url": "https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/preview?pru=AAABaz2o8Jk*rdDLH7fXP7h4HQFLtzvHNQ#slide=id.g5888218f39_177_4", "anchor_text": "NAACL 2019 tutorial slides"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ae6574f25a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=-----1ae6574f25a6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ae6574f25a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=-----1ae6574f25a6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ae6574f25a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1ae6574f25a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1ae6574f25a6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1ae6574f25a6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prashanth Rao"}, {"url": "https://medium.com/@tech_optimist/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "473 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefdb4f4a0d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=post_page-efdb4f4a0d1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5aedbeec5972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6&newsletterV3=efdb4f4a0d1b&newsletterV3Id=5aedbeec5972&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}