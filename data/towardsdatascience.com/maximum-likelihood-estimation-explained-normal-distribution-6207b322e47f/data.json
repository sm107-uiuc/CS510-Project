{"url": "https://towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f", "time": 1682999780.554098, "path": "towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f/", "webpage": {"metadata": {"title": "Maximum Likelihood Estimation Explained - Normal Distribution | by Marissa Eppes | Towards Data Science", "h1": "Maximum Likelihood Estimation Explained - Normal Distribution", "description": "To get a handle on this definition, let\u2019s look at a simple example. Let\u2019s say we have some continuous data and we assume that it is normally distributed. By assuming normality, we simply assume the\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["\u201cA method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.\u201d", "To get a handle on this definition, let\u2019s look at a simple example. Let\u2019s say we have some continuous data and we assume that it is normally distributed. By assuming normality, we simply assume the shape of our data distribution to conform to the popular Gaussian bell curve. What we don\u2019t know is how \u201cfat\u201d or \u201cskinny\u201d the curve is, or where along the x-axis the peak occurs.", "This is where estimating, or inferring, parameter comes in. As we know from statistics, the specific shape and location of our Gaussian distribution come from \u03c3 and \u03bc respectively. In other words, \u03bc and \u03c3 are our parameters of interest. These two parameters are what define our curve, as we can see when we look at the Normal Distribution Probability Density Function (PDF):", "Still bearing in mind our Normal Distribution example, the goal is to determine \u03bc and \u03c3 for our data so that we can match our data to its most likely Gaussian bell curve. To be technically correct with our language, we can say we are looking for a curve that maximizes the probability of our data given a set of curve parameters. In other words, we maximize probability of data while we maximize likelihood of a curve. Perhaps the latter interpretation is the more intuitive way of thinking about the problem, but both are correct, and we will approach the problem using the first perspective.", "In order to use MLE, we have to make two important assumptions, which are typically referred to together as the i.i.d. assumption. These assumptions state that:", "In other words, the i.i.d. assumption requires that the observation of any given data point does not depend on the observation of any other data point (each gathered data point is an independent experiment) and that each data point is generated from same distribution family with the same parameters.", "Often times, the parameters \u03bc and \u03c3 are represented together as a set of parameters \u03b8, such that:", "We can set up the problem as a conditional probability problem, of which the goal is to maximize the probability of observing our data given \u03b8. For a dataset of size n, mathematically this looks something like:", "Because we are dealing with a continuous probability distribution, however, the above notation is technically incorrect, since the probability of observing any set of continuous variables is equal to zero. Conceptually, this makes sense because we can come up with an infinite number of possible variables in the continuous domain, and dividing any given observation by infinity will always lead to a zero probability, regardless of what the observation is.", "We need to think in terms of probability density rather than probability. Without going into the technicalities of the difference between the two, we will just state that probability density in the continuous domain is analogous to probability in the discrete domain. Therefore, probability density can be used in this maximization problem. To correct our notation, we will say:", "We want to maximize the probability density of observing our data as a function of \u03b8. In other words, we want to find \u03bc and \u03c3 values such that this probability density term is as high as it can possibly be. We are used to x being the independent variable by convention. But in this case, we are actually treating \u03b8 as the independent variable, and we can consider x_1, x_2, \u2026 x_n to be a constant, since this is our observed data, which cannot change.", "From probability theory, we know that the probability of multiple independent events all happening is termed joint probability. We can treat each data point observation as one single event; therefore we can treat the observation of our exact dataset as a series of events, and we can apply joint probability density as follows:", "Remember, the goal is to maximize this probability density term by finding the optimal \u03b8. To denote this mathematically, we can say we seek the \u201cargmax\u201d of this term with respect to \u03b8:", "Since we are looking for a maximum value, our calculus intuition should tell us it\u2019s time to take a derivative with respect to \u03b8 and set this derivative term equal to zero to find the location of our peak along the \u03b8-axis. This way, we can equate the argmax of the joint probability density term to the scenario when the derivative of the joint probability density term with respect to \u03b8 equals zero as shown below:", "Now, the only problem is that this isn\u2019t a very easy derivative to calculate or approximate. Luckily, we can apply a simple math trick in this scenario to ease our derivation. We can actually change our derivative term using a monotonic function, which would ease the derivative calculation without changing the end result. A monotonic function is any relationship between two variables that preserves the original order. A monotonic function is either always increasing or always decreasing, and therefore, the derivative of a monotonic function can never change signs. The monotonic function we\u2019ll use here is the natural logarithm, which has the following property (proof not included):", "So we can now write our problem as follows. Note that the equality between the third term and fourth term below is a property whose proof is not explicitly shown.", "Why can we use this natural log trick? Due to the monotonically increasing nature of the natural logarithm, taking the natural log of our original probability density term is not going to affect the argmax, which is the only metric we are interested in here. Of course it changes the values of our probability density term, but it does not change the location of the global maximum with respect to \u03b8. Mathematically, we can write this logic as follows:", "To further demonstrate this concept, here are a few functions plotted alongside their natural logs (dashed lines) to show that the location along the x-axis of the maxima are the same for the function and the natural log of the function, despite the maximum values themselves differing significantly.", "The vertical dotted black lines demonstrate alignment of the maxima between functions and their natural logs. These lines are drawn on the argmax values. As we have stated, these values are the same for the function and the natural log of the function. And this is why we can use our natural log trick in this problem.", "Back to the problem now, we have:", "We want to solve for \u03b8 to obtain our optimal parameters which best fit our observed data to a Gaussian curve. Now let\u2019s think about the two parameters we want to infer, \u03bc and \u03c3, rather than the symbolic representation \u03b8. We will switch to gradient notation:", "Let\u2019s start by taking the gradient with respect to \u03bc. We\u2019ll substitute the PDF of the Normal Distribution for f(x_i|\u03bc, \u03c3) here to do this:", "Using properties of natural logs not proven here, we can simplify this as:", "Setting this last term equal to zero, we get the solution for \u03bc as follows:", "We can see that our optimal \u03bc is independent of our optimal \u03c3. And now we will solve for \u03c3 by taking the gradient with respect to \u03c3 in a similar matter:", "Setting this last term equal to zero, we get the solution for \u03c3 as follows:", "And there we have it. Our optimal \u03bc and \u03c3 derivations should look pretty familiar if we\u2019ve done any statistics recently. These parameters work out to the exact same formulas we use for mean and standard deviation calculations. This isn\u2019t just a coincidence. This is a property of the normal distribution that holds true provided we can make the i.i.d. assumption.", "But the key to understanding MLE here is to think of \u03bc and \u03c3 not as the mean and standard deviation of our dataset, but rather as the parameters of the Gaussian curve which has the highest likelihood of fitting our dataset. This line of thinking will come in handy when we apply MLE to Bayesian models and distributions where calculating central tendency and dispersion estimators isn\u2019t so intuitive.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6207b322e47f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6207b322e47f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6207b322e47f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@meppes?source=post_page-----6207b322e47f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@meppes?source=post_page-----6207b322e47f--------------------------------", "anchor_text": "Marissa Eppes"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe6e6ee3b02eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&user=Marissa+Eppes&userId=e6e6ee3b02eb&source=post_page-e6e6ee3b02eb----6207b322e47f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6207b322e47f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6207b322e47f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6207b322e47f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/maximum-likelihood?source=post_page-----6207b322e47f---------------maximum_likelihood-----------------", "anchor_text": "Maximum Likelihood"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6207b322e47f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6207b322e47f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&user=Marissa+Eppes&userId=e6e6ee3b02eb&source=-----6207b322e47f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6207b322e47f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&user=Marissa+Eppes&userId=e6e6ee3b02eb&source=-----6207b322e47f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6207b322e47f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6207b322e47f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6207b322e47f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6207b322e47f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6207b322e47f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6207b322e47f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6207b322e47f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6207b322e47f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6207b322e47f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6207b322e47f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6207b322e47f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6207b322e47f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@meppes?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@meppes?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marissa Eppes"}, {"url": "https://medium.com/@meppes/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "141 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe6e6ee3b02eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&user=Marissa+Eppes&userId=e6e6ee3b02eb&source=post_page-e6e6ee3b02eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F57cbf1490b06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-explained-normal-distribution-6207b322e47f&newsletterV3=e6e6ee3b02eb&newsletterV3Id=57cbf1490b06&user=Marissa+Eppes&userId=e6e6ee3b02eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}