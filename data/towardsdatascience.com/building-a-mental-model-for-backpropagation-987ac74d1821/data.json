{"url": "https://towardsdatascience.com/building-a-mental-model-for-backpropagation-987ac74d1821", "time": 1683010051.125462, "path": "towardsdatascience.com/building-a-mental-model-for-backpropagation-987ac74d1821/", "webpage": {"metadata": {"title": "Building A Mental Model for Backpropagation | by Logan Yang | Towards Data Science", "h1": "Building A Mental Model for Backpropagation", "description": "As the beating heart of deep learning, a solid understanding of backpropagation is required for any deep learning practitioner. Although there are a lot of good resources that explain backpropagation\u2026"}, "outgoing_paragraph_urls": [{"url": "https://cs231n.github.io/", "anchor_text": "CS231n lectures", "paragraph_index": 23}, {"url": "https://github.com/karpathy/micrograd", "anchor_text": "micrograd: tiniest autograd engine", "paragraph_index": 23}, {"url": "https://livebook.manning.com/book/grokking-deep-learning/chapter-13/1", "anchor_text": "Grokking Deep Learning by Andrew Trask", "paragraph_index": 23}, {"url": "https://youtu.be/MswxJw-8PvE", "anchor_text": "great video", "paragraph_index": 23}, {"url": "https://medium.com/@loganyang", "anchor_text": "Medium", "paragraph_index": 24}, {"url": "https://twitter.com/logancyang", "anchor_text": "Twitter", "paragraph_index": 24}, {"url": "https://twitter.com/logancyang", "anchor_text": "https://twitter.com/logancyang", "paragraph_index": 26}], "all_paragraphs": ["As the beating heart of deep learning, a solid understanding of backpropagation is required for any deep learning practitioner. Although there are a lot of good resources that explain backpropagation on the internet already, most of them explain from very different angles and each is good for a certain type of audience. In this post, I\u2019m going to combine intuition, animated graphs and code together for beginners and intermediate level students of deep learning for easier consumption. A good assessment of the understanding of any algorithm is whether you can code it out yourself from scratch. After reading this post, you should have an idea of how to implement your own version of backpropagation in Python.", "Mathematically, backpropagation is the process of computing gradients for the components of a function by applying the chain rule. In the case of neural networks, the function of interest is the loss function. I like the interpretation by Andrej Karpathy in CS231n: take the compute graph as real-valued circuits with logic gates. The gates are the operations in the function, e.g. addition, multiplication, exponentiation, matrix multiplication, etc.", "This is a great mental model because it means backpropagation is a local process. Every gate in the circuit can compute its output and the local gradient without knowing the big picture.", "During the backward pass (backpropagation), the gate applies the chain rule, i.e. taking its output\u2019s gradient on the final output of the circuit and multiply it with the local gradients with respect to all of its inputs. The backward pass can be implemented using a recursive approach to traverse back from the output of the circuit back to all the inputs.", "Intuitively, the final effect of backprop and its associated weight updates is that the circuit \u201cwants\u201d to output a value that\u2019s closer to whatever target value we have. Take the gradient of the add gate (-4) in the graph above as an example, it means that changing q by +1\u03b5 will result in a change of -4\u03b5 in f. If we\u2019d like a higher f, we could make q lower. This is essentially what a gradient is. People sometimes call it \u201csensitivity\u201d. Another great analogy is a force of correction. The sign of the gradient indicates the direction of correction, and the magnitude indicates the strength.", "One of the best ways to visualize backprop is to draw the compute graph of the function. Let\u2019s look at this weird function below to demonstrate how to draw its compute graph and then do backprop on it manually. (\u03c3() is the sigmoid function)", "To calculate its gradients, we can decompose it into add, sigmoid, square gates as shown in the animated steps below:", "Concretely, the process consists of 3 high-level steps", "You can follow along and manually calculate the values. I\u2019ll show how to implement it in the last section, but now let\u2019s look at a trick that will help us simplify the process.", "Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate whenever it is convenient.", "Since we should never explicitly solve for the gradients analytically, the selection of these function components becomes a problem to consider. Take the sigmoid function for example:", "We can decompose it into add, multiply, negate, exponentiate, reciprocal gates individually as shown in the following compute graph:", "A simple sigmoid already has so many operations and gradients, it seems unnecessarily complex. What we could do alternatively is just to have one sigmoid gate applying on the output of the red box along with the function that calculates its gradient. The gradient of the sigmoid is very simple:", "This way we avoid a lot of unnecessary computation. It saves us time, space, energy, makes the code more modular and easier to read, and avoids numerical problems.", "The code for the sigmoid gate could look something like:", "It has both the forward pass output and the function to compute the local gradient for the backward pass in just a few lines of code. In the next section, I will fit this into the bigger picture and show how to write a mini autograd library with such components.", "To let a computer calculate the gradients for any function expressed in Directed Acyclic Graphs (DAG) using the chain rule, we need to write the code for those 3 high-level steps mentioned before. Such a program is often called auto-differentiation or autograd. As you\u2019ll see next, we can structure our code into a Tensor class that defines the data and operations, so that it not only can support building dynamic compute graphs on the fly, but also backpropagating through it recursively.", "A Tensor object has data, grad, a _backward() method, a _prev set of Tensor nodes, and a _op operation. When we execute an expression, it builds the compute graph on the fly since we have overridden the Python operators such as + , * and ** with customized dunder methods. The current Tensor's _backward(), _prev and _op are defined by its parent(s), i.e. the Tensor(s) that produced it. For example, the c in c = a + b has the_backward() defined in __add__, and _prev = {a, b}, _op = '+'. This way we can define any operation we want and let Python construct the graph.", "Here we are talking about neural networks, so the expression we care about is the loss function. Take the MSE loss for example (using 1 scalar data point for simplicity), MSELoss = (w*x - y)**2 where w, x and y are Tensor objects and are initialized to be 3, -4, and 2 respectively. The graph is then automatically constructed as:", "Notice that subtraction is actually negation and addition. I named the intermediate nodes for illustration purposes only. With the graph ready, we can implement backprop!", "backward should compute gradients one by one starting from the current Tensor node and move to its ancestors. The order of the traversal needs to be topologically sorted to make sure the dependencies are calculated at every step. One simple way to implement this topological sort is a depth-first search. Here's an animation to show how it's executed for the MSELoss example.", "This is graph traversal 101: a plain old DFS. If you have a big complex neural net you just replace the upper left corner that is wx with your big graph, and the DFS will go there and compute the gradients, provided that all the operations and their _backward are defined in the Tensor class. Some of the obvious operations we need here include max, sum, matrix multiplication, transpose, etc. To use the gradients to update your weights, do:", "There you have it, the autograd algorithm in a few lines of code. It is the backbone of any deep learning framework. Now that the hard part is behind us, to complete the implementation of your mini deep learning framework, you just need to implement a model interface with a collection of layer types, loss functions, and some optimizers.", "This post is heavily inspired by Andrej Karpathy\u2019s awesome CS231n lectures and beautifully written micrograd: tiniest autograd engine. If you would like to take a look at a different and more extended version of a DIY deep learning framework that closely resembles PyTorch, check out the one implemented in Grokking Deep Learning by Andrew Trask. If you prefer diving into PyTorch\u2019s autograd directly instead, Elliot Waite has a great video on Youtube which will save you a lot of time digging into the source code. Keep on learning!", "See my other posts on Medium, or follow me on Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Engineer, builder, writer. Follow me here and on Twitter for future content https://twitter.com/logancyang"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F987ac74d1821&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----987ac74d1821--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----987ac74d1821--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://loganyang.medium.com/?source=post_page-----987ac74d1821--------------------------------", "anchor_text": ""}, {"url": "https://loganyang.medium.com/?source=post_page-----987ac74d1821--------------------------------", "anchor_text": "Logan Yang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59aa671cf125&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&user=Logan+Yang&userId=59aa671cf125&source=post_page-59aa671cf125----987ac74d1821---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F987ac74d1821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F987ac74d1821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ledoc?utm_source=medium&utm_medium=referral", "anchor_text": "Andre Ouellet"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://cs231n.github.io/optimization-2/", "anchor_text": "https://cs231n.github.io/optimization-2/"}, {"url": "https://cs231n.github.io/optimization-2/", "anchor_text": "https://cs231n.github.io/optimization-2/"}, {"url": "https://cs231n.github.io/", "anchor_text": "CS231n lectures"}, {"url": "https://github.com/karpathy/micrograd", "anchor_text": "micrograd: tiniest autograd engine"}, {"url": "https://livebook.manning.com/book/grokking-deep-learning/chapter-13/1", "anchor_text": "Grokking Deep Learning by Andrew Trask"}, {"url": "https://youtu.be/MswxJw-8PvE", "anchor_text": "great video"}, {"url": "https://cs231n.github.io/", "anchor_text": "CS231n lectures"}, {"url": "https://github.com/karpathy/micrograd", "anchor_text": "micrograd: tiniest autograd engine"}, {"url": "https://medium.com/@loganyang", "anchor_text": "Medium"}, {"url": "https://twitter.com/logancyang", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----987ac74d1821---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/backpropagation?source=post_page-----987ac74d1821---------------backpropagation-----------------", "anchor_text": "Backpropagation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----987ac74d1821---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----987ac74d1821---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----987ac74d1821---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F987ac74d1821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&user=Logan+Yang&userId=59aa671cf125&source=-----987ac74d1821---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F987ac74d1821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&user=Logan+Yang&userId=59aa671cf125&source=-----987ac74d1821---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F987ac74d1821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----987ac74d1821--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F987ac74d1821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----987ac74d1821---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----987ac74d1821--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----987ac74d1821--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----987ac74d1821--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----987ac74d1821--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----987ac74d1821--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----987ac74d1821--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----987ac74d1821--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----987ac74d1821--------------------------------", "anchor_text": ""}, {"url": "https://loganyang.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://loganyang.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Logan Yang"}, {"url": "https://loganyang.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "307 Followers"}, {"url": "https://twitter.com/logancyang", "anchor_text": "https://twitter.com/logancyang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59aa671cf125&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&user=Logan+Yang&userId=59aa671cf125&source=post_page-59aa671cf125--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff079e6cea9be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-mental-model-for-backpropagation-987ac74d1821&newsletterV3=59aa671cf125&newsletterV3Id=f079e6cea9be&user=Logan+Yang&userId=59aa671cf125&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}