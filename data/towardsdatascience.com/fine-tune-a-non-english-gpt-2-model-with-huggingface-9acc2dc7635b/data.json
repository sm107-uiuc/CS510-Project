{"url": "https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b", "time": 1683013456.095197, "path": "towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b/", "webpage": {"metadata": {"title": "Fine-tune a non-English GPT-2 Model with Huggingface | by Philipp Schmid | Towards Data Science", "h1": "Fine-tune a non-English GPT-2 Model with Huggingface", "description": "Unless you\u2019re living under a rock, you probably have heard about OpenAI\u2019s GPT-3 language model. You might also have seen all the crazy demos, where the model writes JSX, HTML code, or its\u2026"}, "outgoing_paragraph_urls": [{"url": "https://openai.com/", "anchor_text": "OpenAI", "paragraph_index": 0}, {"url": "https://twitter.com/Simon_O_Regan", "anchor_text": "Simon O'Regan", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/gpt-3-demos-use-cases-implications-77f86e540dc1", "anchor_text": "article with excellent demos and projects built on top of GPT-3", "paragraph_index": 0}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface", "paragraph_index": 4}, {"url": "http://chefkoch.de", "anchor_text": "chefkoch.de", "paragraph_index": 4}, {"url": "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb", "anchor_text": "colab notebook.", "paragraph_index": 5}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers library", "paragraph_index": 6}, {"url": "https://huggingface.co/models", "anchor_text": "Huggingface model hub", "paragraph_index": 7}, {"url": "https://www.kaggle.com/sterby/german-recipes-dataset", "anchor_text": "German Recipes Dataset", "paragraph_index": 7}, {"url": "http://chefkoch.de/", "anchor_text": "chefkoch.de", "paragraph_index": 7}, {"url": "https://www.philschmid.de/google-colab-the-free-gpu-tpu-jupyter-notebook-service", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb", "anchor_text": "colab notebook", "paragraph_index": 11}, {"url": "https://www.kaggle.com/sterby/german-recipes-dataset", "anchor_text": "German Recipes Dataset", "paragraph_index": 12}, {"url": "http://chefkoch.de/", "anchor_text": "chefkoch.de", "paragraph_index": 12}, {"url": "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class", "anchor_text": "Pytroch", "paragraph_index": 16}, {"url": "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class", "anchor_text": "Dataset", "paragraph_index": 16}, {"url": "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class", "anchor_text": "class", "paragraph_index": 16}, {"url": "https://www.youtube.com/watch?v=PXOzkkB5eH0&ab_channel=PythonEngineer", "anchor_text": "youtube video", "paragraph_index": 16}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer", "anchor_text": "Trainer", "paragraph_index": 20}, {"url": "https://huggingface.co/transformers/examples.html", "anchor_text": "example scripts", "paragraph_index": 20}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments", "anchor_text": "TrainingArguments", "paragraph_index": 20}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines", "anchor_text": "highlight of the transformers library", "paragraph_index": 23}, {"url": "https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines", "anchor_text": "Pipelines", "paragraph_index": 23}, {"url": "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb", "anchor_text": "colab notebook", "paragraph_index": 27}, {"url": "https://twitter.com/_philschmid", "anchor_text": "Twitter", "paragraph_index": 28}, {"url": "https://www.linkedin.com/in/philipp-schmid-a6a2bb196/", "anchor_text": "LinkedIn", "paragraph_index": 28}], "all_paragraphs": ["Unless you\u2019re living under a rock, you probably have heard about OpenAI\u2019s GPT-3 language model. You might also have seen all the crazy demos, where the model writes JSX, HTML code, or its capabilities in the area of zero-shot / few-shot learning. Simon O'Regan wrote an article with excellent demos and projects built on top of GPT-3.", "A Downside of GPT-3 is its 175 billion parameters, which results in a model size of around 350GB. For comparison, the biggest implementation of the GPT-2 iteration has 1,5 billion parameters. This is less than 1/116 in size.", "In fact, with close to 175B trainable parameters, GPT-3 is much bigger in terms of size in comparison to any other model else out there. Here is a comparison of the number of parameters of recent popular NLP models, GPT-3 clearly stands out.", "This is all magnificent, but you do not need 175 billion parameters to get good results in text-generation.", "There are already tutorials on how to fine-tune GPT-2. But a lot of them are obsolete or outdated. In this tutorial, we are going to use the transformers library by Huggingface in their newest version (3.1.0). We will use the new Trainer class and fine-tune our GPT-2 Model with German recipes from chefkoch.de.", "You can find everything we are doing in this colab notebook.", "The Transformers library provides state-of-the-art machine learning architectures like BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5 for Natural Language Understanding (NLU), and Natural Language Generation (NLG). It also provides thousands of pre-trained models in 100+ different languages and is deeply interoperable between PyTorch & TensorFlow 2.0. It enables developers to fine-tune machine learning models for different NLP-tasks like text classification, sentiment analysis, question-answering, or text generation.", "In the tutorial, we fine-tune a German GPT-2 from the Huggingface model hub. As data, we use the German Recipes Dataset, which consists of 12190 german recipes with metadata crawled from chefkoch.de.", "We will use the recipe Instructions to fine-tune our GPT-2 model and let us write recipes afterwards that we can cook.", "We use a Google Colab with a GPU runtime for this tutorial. If you are not sure how to use a GPU Runtime take a look here.", "What are we going to do:", "You can find everything we do in this colab notebook.", "As already mentioned in the introduction of the tutorial we use the \u201c German Recipes Dataset\u201d dataset from Kaggle. The dataset consists of 12190 german recipes with metadata crawled from chefkoch.de. In this example, we only use the Instructions of the recipes. We download the dataset by using the \u201cDownload\u201d button and upload it to our colab notebook since it only has a zipped size of 4,7MB.", "After we uploaded the file we use unzip to extract the recipes.json.", "You also could use the kaggle CLI to download the dataset, but be aware you need your Kaggle credentials in the colab notebook.", "Here is an example of a recipe.", "The next step is to extract the instructions from all recipes and build a TextDataset. The TextDataset is a custom implementation of the Pytroch Dataset class implemented by the transformers library. If you want to know more about Dataset in Pytorch you can check out this youtube video.", "First, we split the recipes.json into a train and test section. Then we extract Instructions from the recipes and write them into a train_dataset.txt and test_dataset.txt", "The next step is to download the tokenizer. We use the tokenizer from the german-gpt2 model.", "Now we can build our TextDataset. Therefore we create a TextDataset instance with the tokenizer and the path to our datasets. We also create our data_collator, which is used in training to form a batch from our dataset.", "The Trainer class provides an API for feature-complete training. It is used in most of the example scripts from Huggingface. Before we can instantiate our Trainer we need to download our GPT-2 model and create TrainingArguments. The TrainingArguments are used to define the Hyperparameters, which we use in the training process like the learning_rate, num_train_epochs, or per_device_train_batch_size. You can find a complete list here.", "To train the model we can simply run trainer.train().", "After training is done you can save the model by calling save_model(). This will save the trained model to our output_dir from our TrainingArguments.", "To test the model we use another highlight of the transformers library called pipeline. Pipelines are objects that offer a simple API dedicated to several tasks, text-generation amongst others.", "\u201c Zuerst Tomaten dazu geben und 2 Minuten kochen lassen. Die Linsen ebenfalls in der Br\u00fche anbr\u00fchen.Die Tomaten auspressen. Mit der Butter verr\u00fchren. Den Kohl sowie die Kartoffeln and\u00fcnsten, bis sie weich sind. \u201c", "Well, that's it\ud83d\udcab. We\u2019ve done it\ud83d\udc68\ud83c\udffb\u200d\ud83c\udf73. We have successfully fine-tuned our gpt-2 model to write us recipes.", "To improve our results we could train it longer and adjust our TrainingArguments or enlarge the dataset.", "You can find everything in this colab notebook.", "Thanks for reading. If you have any questions, feel free to contact me or comment on this article. You can also connect with me on Twitter or LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9acc2dc7635b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@schmidphilipp1995?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@schmidphilipp1995?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "Philipp Schmid"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb5f02a773f45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&user=Philipp+Schmid&userId=b5f02a773f45&source=post_page-b5f02a773f45----9acc2dc7635b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9acc2dc7635b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9acc2dc7635b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@fifthlane?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Peter Dawn"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface", "anchor_text": "https://www.philschmid.de"}, {"url": "https://openai.com/", "anchor_text": "OpenAI"}, {"url": "https://twitter.com/Simon_O_Regan", "anchor_text": "Simon O'Regan"}, {"url": "https://towardsdatascience.com/gpt-3-demos-use-cases-implications-77f86e540dc1", "anchor_text": "article with excellent demos and projects built on top of GPT-3"}, {"url": "https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface", "anchor_text": "created by author"}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface"}, {"url": "http://chefkoch.de", "anchor_text": "chefkoch.de"}, {"url": "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb", "anchor_text": "colab notebook."}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers logo"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers library"}, {"url": "https://huggingface.co/models", "anchor_text": "Huggingface model hub"}, {"url": "https://www.kaggle.com/sterby/german-recipes-dataset", "anchor_text": "German Recipes Dataset"}, {"url": "http://chefkoch.de/", "anchor_text": "chefkoch.de"}, {"url": "https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface", "anchor_text": "created by author"}, {"url": "https://www.philschmid.de/google-colab-the-free-gpu-tpu-jupyter-notebook-service", "anchor_text": "here"}, {"url": "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb", "anchor_text": "colab notebook"}, {"url": "https://www.kaggle.com/sterby/german-recipes-dataset", "anchor_text": "German Recipes Dataset"}, {"url": "http://chefkoch.de/", "anchor_text": "chefkoch.de"}, {"url": "https://www.kaggle.com/sterby/german-recipes-dataset", "anchor_text": "Kaggle competition"}, {"url": "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class", "anchor_text": "Pytroch"}, {"url": "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class", "anchor_text": "Dataset"}, {"url": "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class", "anchor_text": "class"}, {"url": "https://www.youtube.com/watch?v=PXOzkkB5eH0&ab_channel=PythonEngineer", "anchor_text": "youtube video"}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer", "anchor_text": "Trainer"}, {"url": "https://huggingface.co/transformers/examples.html", "anchor_text": "example scripts"}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments", "anchor_text": "TrainingArguments"}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines", "anchor_text": "highlight of the transformers library"}, {"url": "https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines", "anchor_text": "Pipelines"}, {"url": "https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb", "anchor_text": "colab notebook"}, {"url": "https://twitter.com/_philschmid", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/philipp-schmid-a6a2bb196/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/nlp?source=post_page-----9acc2dc7635b---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9acc2dc7635b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----9acc2dc7635b---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9acc2dc7635b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9acc2dc7635b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&user=Philipp+Schmid&userId=b5f02a773f45&source=-----9acc2dc7635b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9acc2dc7635b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&user=Philipp+Schmid&userId=b5f02a773f45&source=-----9acc2dc7635b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9acc2dc7635b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9acc2dc7635b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9acc2dc7635b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9acc2dc7635b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@schmidphilipp1995?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@schmidphilipp1995?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Philipp Schmid"}, {"url": "https://medium.com/@schmidphilipp1995/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "229 Followers"}, {"url": "https://twitter.com/_philschmid", "anchor_text": "https://twitter.com/_philschmid"}, {"url": "https://www.philschmid.de", "anchor_text": "https://www.philschmid.de"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb5f02a773f45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&user=Philipp+Schmid&userId=b5f02a773f45&source=post_page-b5f02a773f45--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9c9055b110ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b&newsletterV3=b5f02a773f45&newsletterV3Id=9c9055b110ca&user=Philipp+Schmid&userId=b5f02a773f45&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}