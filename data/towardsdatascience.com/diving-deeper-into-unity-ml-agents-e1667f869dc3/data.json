{"url": "https://towardsdatascience.com/diving-deeper-into-unity-ml-agents-e1667f869dc3", "time": 1683003515.4508429, "path": "towardsdatascience.com/diving-deeper-into-unity-ml-agents-e1667f869dc3/", "webpage": {"metadata": {"title": "Diving deeper into Unity-ML Agents | by Thomas Simonini | Towards Data Science", "h1": "Diving deeper into Unity-ML Agents", "description": "We'll learn about the theory behind the powerful idea of curiosity in deep reinforcement learning and we'll train this curious agent to destroy Pyramids."}, "outgoing_paragraph_urls": [{"url": "https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt", "anchor_text": "Deep Reinforcement Learning Course from beginner to expert, with Hugging Face \ud83e\udd17", "paragraph_index": 0}, {"url": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "anchor_text": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "paragraph_index": 1}, {"url": "https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt", "anchor_text": "Deep Reinforcement Learning Course from beginner to expert, with Hugging Face \ud83e\udd17", "paragraph_index": 2}, {"url": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "anchor_text": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/an-introduction-to-unity-ml-agents-6238452fcf4c", "anchor_text": "Last time", "paragraph_index": 4}, {"url": "https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419", "anchor_text": "reward hypothesis", "paragraph_index": 11}, {"url": "https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://pathak22.github.io/noreward-rl/resources/icml17.pdf", "anchor_text": "called Intrinsic Curiosity module.", "paragraph_index": 26}, {"url": "https://github.com/simoninithomas/unity_ml_agents_course", "anchor_text": "here.", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e", "anchor_text": "check my article", "paragraph_index": 42}, {"url": "https://www.immersivelimit.com/", "anchor_text": "Immersive Limit", "paragraph_index": 56}, {"url": "https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-PPO.md", "anchor_text": "documentation", "paragraph_index": 57}, {"url": "https://github.com/simoninithomas/unity_ml_agents_course", "anchor_text": "here.", "paragraph_index": 60}, {"url": "https://twitter.com/ThomasSimonini", "anchor_text": "@ThomasSimonini", "paragraph_index": 64}, {"url": "https://bit.ly/3QADz2Q", "anchor_text": "https://bit.ly/3QADz2Q", "paragraph_index": 66}], "all_paragraphs": ["We launched a new free, updated, Deep Reinforcement Learning Course from beginner to expert, with Hugging Face \ud83e\udd17", "The chapter below is the former version, the new version is here \ud83d\udc49 https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "We launched a new free, updated, Deep Reinforcement Learning Course from beginner to expert, with Hugging Face \ud83e\udd17", "The chapter below is the former version, the new version is here \ud83d\udc49 https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "Last time, we learned about how Unity ML-Agents works and trained an agent that learned to jump over walls.", "This was a nice experience, but we want to create agents that can solve more complex tasks. So today we\u2019ll train a smarter one that needs to press a button to spawn a pyramid, then navigate to the pyramid, knock it over, and move to the gold brick at the top.", "To train this new agent, that seek for that button and then the pyramid to destroy, we\u2019ll use a combination of two types of rewards, the extrinsic one given by the environment. But also an intrinsic one called curiosity. This second will push our agent to be curious, or in other terms, to better explore its environment.", "So today we\u2019ll learn about the theory behind this powerful idea of curiosity in deep reinforcement learning and we\u2019ll train this curious agent.", "I already cover curiosity in detail in 2 other articles here and here if you want to dive into the mathematical and implementation details.", "To understand what is curiosity, we need first to understand the two major problems with RL:", "First, the sparse rewards problem: that is, most rewards do not contain information, and hence are set to zero.", "Remember that RL is based on the reward hypothesis, which is the idea that each goal can be described as the maximization of the rewards. Therefore, rewards act as feedback for RL agents, if they don\u2019t receive any, their knowledge of which action is appropriate (or not) cannot change.", "For instance, in Vizdoom \u201cDoomMyWayHome,\u201d your agent is only rewarded if it finds the vest. However, the vest is far away from your starting point, so most of your rewards will be zero. Therefore, if our agent does not receive useful feedback (dense rewards), it will take much longer to learn an optimal policy and it can spend time turning around without finding the goal.", "The second big problem is that the extrinsic reward function is handmade, that is in each environment, a human has to implement a reward function. But how we can scale that in big and complex environments?", "Therefore, a solution to these problems is to develop a reward function that is intrinsic to the agent, i.e., generated by the agent itself. The agent will act as a self-learner since it will be the student, but also its own feedback master.", "This intrinsic reward mechanism is known as curiosity because this reward push to explore states that are novel/unfamiliar. In order to achieve that, our agent will receive a high reward when exploring new trajectories.", "This reward is in fact designed on how human acts, we have naturally an intrinsic desire to explore environments and discover new things.", "There are different ways to calculate this intrinsic reward, and Unity ML-Agents use curiosity through the next-state prediction method.", "I already cover this method here if you want to dive into the mathematical details.", "So we just said that curiosity was high when we were in unfamiliar/novel states. But how we can calculate this \u201cunfamiliarity\u201d?", "We can calculate curiosity as the error of our agent of predicting the next state, given the current state and action taken. More formally, we can define this as:", "Why? Because the idea of curiosity is to encourage our agent to perform actions that reduce the uncertainty in the agent\u2019s ability to predict the consequences of its own actions (uncertainty will be higher in areas where the agent has spent less time, or in areas with complex dynamics).", "If the agent spend a lot of times on these states, it will be good to predict the next state (low curiosity), on the other hand, if it\u2019s a new state unexplored, it will be bad to predict the next state (high curiosity).", "Let\u2019s break it down further. Say you play Super Mario Bros:", "Using curiosity will push our agent to favor transitions with high prediction error (which will be higher in areas where the agent has spent less time, or in areas with complex dynamics) and consequently better explore our environment.", "But because we can\u2019t predict the next state by predicting the next frame (too complicated to predict pixels directly), we use a better feature representation that will keep only elements that can be controlled by our agent or affect our agent.", "And to calculate curiosity, we will use a module introduced in the paper, Curiosity-driven Exploration by Self-supervised Prediction called Intrinsic Curiosity module.", "So now that we understand what is curiosity through the next state prediction and how it works, let\u2019s train this new agent.", "We published our trained models on github, you can download them here.", "The goal in this environment is to train our agent to get the gold brick on the top of the pyramid. In order to do that he needs to press a button to spawn a pyramid, then navigate to the pyramid, knock it over, and move to the gold brick at the top.", "In terms of observation, we use the raycast version. With 148 raycasts, but detecting switch, bricks, golden brick, and walls.", "We also use a boolean variable indicating the switch state.", "The action space is discrete with 4 possible actions:", "Our goal is to hit the benchmark with a mean reward of 1.75.", "First of all, let\u2019s open the UnitySDK project.", "In the examples search for Pyramids and open the scene.", "Like WallJump, you see in the scene, a lot of Agents, each of them comes from the same Prefab and they all share the same Brain (policy).", "In fact, as we do in classical Deep Reinforcement Learning when we launch multiple instances of a game (for instance 128 parallel environments) we do the same hereby copy and paste the agents, in order to have more various states.", "So, first, because we want to train our agent from scratch, we need to remove the brain from the agent prefab. We need to go to the prefabs folder and open the Prefab.", "Now in the Prefab hierarchy, select the Agent and go into the inspector.", "In Behavior Parameters, we need to remove the Model. If you have some GPU you can change Inference Device from CPU to GPU.", "For this first training, we\u2019ll just modify the total training steps because it\u2019s too high and we can hit the benchmark in only 500k training steps. To do that we go to config/trainer_config.yaml and you modify these to max_steps to 5.0e5 for Pyramids situation:", "To train this agent, we will use PPO (Proximal Policy Optimization) if you don\u2019t know about it or you need to refresh your knowledge, check my article.", "We saw that to train this agent, we need to call our External Communicator using the Python API. This External Communicator will then ask the Academy to start the agents.", "So, you need to open your terminal, go where ml-agents-master is and type this.", "It will ask you to run the Unity scene,", "Press the \u25b6\ufe0f button at the top of the Editor.", "You can monitor your training by launching Tensorboard using this command:", "You can watch your agent during the training by looking at the game window.", "When the training is finished you need to move the saved model files contained in ml-agents-master/models to UnitySDK/Assets/ML-Agents/Examples/Pyramids/TFModels.", "And again, open the Unity Editor, and select Pyramids scene.", "Select the Pyramids prefab object and open it.", "In Agent Behavior Parameters, drag the Pyramids.nn file to Model Placeholder.", "Then, press the \u25b6\ufe0f button at the top of the Editor.", "We\u2019ve just trained our agents to learn to jump over walls. Now that we have good results we can try some experiments.", "Remember that the best way to learn is to be active by experimenting. So you should try to make some hypotheses and verify them.", "By the way, there is an amazing video about how to hyperparameter tuning Pyramid environment by Immersive Limit that you should definitely watch.", "The time horizon, as explained in the documentation, is the number of steps of experience to collect per-agent before putting it into the experience buffer. This trades off between a long time horizon (less biased, but higher variance estimate), and a short time horizon (more biased, but less varied estimate).", "In this experience, we doubled the time horizon from 128 to 256. Increasing it allows our agent to capture more important behaviors in his sequence of actions than before.", "However, this didn\u2019t have an impact on the training of our new agent. Indeed, they share quite the same results.", "We published our trained models on github, you can download them here.", "You\u2019ve just trained a smarter agent than last time. And you\u2019ve also learned about Curiosity in Deep Reinforcement Learning. That\u2019s awesome!", "Now that we\u2019ve done that, you might want to go deeper with Unity ML-Agents. Don\u2019t worry, next time we\u2019ll create our own environments and the article next we\u2019ll create our own reinforcement learning implementations.", "So in the next article, we\u2019ll create our first environment from scratch. What this environment will be? I don\u2019t want to spoil everything now, but I give you a hint:", "If you have any thoughts, comments, questions, feel free to comment below or send me an email: hello@simoninithomas.com, or tweet me @ThomasSimonini.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Developer Advocate \ud83e\udd51 at Hugging Face \ud83e\udd17| Founder Deep Reinforcement Learning class \ud83d\udcda https://bit.ly/3QADz2Q |"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe1667f869dc3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://thomassimonini.medium.com/?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": ""}, {"url": "https://thomassimonini.medium.com/?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "Thomas Simonini"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5178b198735a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&user=Thomas+Simonini&userId=5178b198735a&source=post_page-5178b198735a----e1667f869dc3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1667f869dc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1667f869dc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.simoninithomas.com/unitymlagentscourse/", "anchor_text": "Unity-ML Agents Course"}, {"url": "https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt", "anchor_text": "Deep Reinforcement Learning Course from beginner to expert, with Hugging Face \ud83e\udd17"}, {"url": "https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt", "anchor_text": "https://huggingface.co/deep-rl-course/unit0/introduction"}, {"url": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "anchor_text": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt"}, {"url": "https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt", "anchor_text": "Deep Reinforcement Learning Course from beginner to expert, with Hugging Face \ud83e\udd17"}, {"url": "https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt", "anchor_text": "https://huggingface.co/deep-rl-course/unit0/introduction"}, {"url": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt", "anchor_text": "https://huggingface.co/deep-rl-course/unit5/introduction?fw=pt"}, {"url": "https://towardsdatascience.com/an-introduction-to-unity-ml-agents-6238452fcf4c", "anchor_text": "Last time"}, {"url": "https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa", "anchor_text": "here"}, {"url": "https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938", "anchor_text": "here"}, {"url": "https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419", "anchor_text": "reward hypothesis"}, {"url": "https://vimeo.com/felixsteger", "anchor_text": "A big thanks to Felix Steger for this illustration"}, {"url": "https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa", "anchor_text": "here"}, {"url": "https://pathak22.github.io/noreward-rl/resources/icml17.pdf", "anchor_text": "called Intrinsic Curiosity module."}, {"url": "https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa", "anchor_text": "check our detailled article"}, {"url": "https://github.com/simoninithomas/unity_ml_agents_course", "anchor_text": "here."}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e", "anchor_text": "check my article"}, {"url": "https://www.immersivelimit.com/", "anchor_text": "Immersive Limit"}, {"url": "https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-PPO.md", "anchor_text": "documentation"}, {"url": "https://github.com/simoninithomas/unity_ml_agents_course", "anchor_text": "here."}, {"url": "https://twitter.com/ThomasSimonini", "anchor_text": "@ThomasSimonini"}, {"url": "https://towardsdatascience.com/unity-ml-agents-the-mayan-adventure-2e15510d653b", "anchor_text": "The Mayan Adventure"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e1667f869dc3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e1667f869dc3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e1667f869dc3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----e1667f869dc3---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----e1667f869dc3---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe1667f869dc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&user=Thomas+Simonini&userId=5178b198735a&source=-----e1667f869dc3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe1667f869dc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&user=Thomas+Simonini&userId=5178b198735a&source=-----e1667f869dc3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1667f869dc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe1667f869dc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e1667f869dc3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e1667f869dc3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e1667f869dc3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e1667f869dc3--------------------------------", "anchor_text": ""}, {"url": "https://thomassimonini.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://thomassimonini.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thomas Simonini"}, {"url": "https://thomassimonini.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.1K Followers"}, {"url": "https://bit.ly/3QADz2Q", "anchor_text": "https://bit.ly/3QADz2Q"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5178b198735a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&user=Thomas+Simonini&userId=5178b198735a&source=post_page-5178b198735a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9509e095d63c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiving-deeper-into-unity-ml-agents-e1667f869dc3&newsletterV3=5178b198735a&newsletterV3Id=9509e095d63c&user=Thomas+Simonini&userId=5178b198735a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}