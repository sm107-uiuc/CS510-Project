{"url": "https://towardsdatascience.com/the-complete-guide-to-clustering-analysis-10fe13712787", "time": 1683003612.431561, "path": "towardsdatascience.com/the-complete-guide-to-clustering-analysis-10fe13712787/", "webpage": {"metadata": {"title": "The complete guide to clustering analysis | by Antoine Soetewey | Towards Data Science", "h1": "The complete guide to clustering analysis", "description": "Clustering analysis is a form of exploratory data analysis in which observations are divided into different groups that share common characteristics. The purpose of cluster analysis (also known as\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.statsandr.com/blog/data-manipulation-in-r/#scale", "anchor_text": "scale()", "paragraph_index": 11}, {"url": "https://www.statsandr.com/blog/data/Eurojobs.csv", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://www.statsandr.com/blog/how-to-import-an-excel-file-in-rstudio", "anchor_text": "how to import data into R", "paragraph_index": 17}, {"url": "https://www.statsandr.com/blog/variable-types-and-examples/#continuous", "anchor_text": "quantitative continuous variables", "paragraph_index": 19}, {"url": "https://www.statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#optimal-number-of-clusters", "anchor_text": "section", "paragraph_index": 21}, {"url": "https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://www.statsandr.com/blog/graphics-in-r-with-ggplot2/", "anchor_text": "Graphics in R with ggplot2", "paragraph_index": 43}, {"url": "https://www.statsandr.com/blog/data-manipulation-in-r", "anchor_text": "how to subset a dataframe", "paragraph_index": 74}, {"url": "https://www.statsandr.com/blog/files/Hierarchical-clustering-cheatsheet.pdf", "anchor_text": "hierarchical clustering cheatsheet", "paragraph_index": 122}, {"url": "http://statsandr.com", "anchor_text": "statsandr.com", "paragraph_index": 141}], "all_paragraphs": ["Clustering analysis is a form of exploratory data analysis in which observations are divided into different groups that share common characteristics.", "The purpose of cluster analysis (also known as classification) is to construct groups (or classes or clusters) while ensuring the following property: within a group the observations must be as similar as possible, while observations belonging to different groups must be as different as possible.", "There are two main types of classification:", "The first is generally used when the number of classes is fixed in advance, while the second is generally used for an unknown number of classes and helps to determine this optimal number. Both methods are illustrated below through applications by hand and in R. Note that for hierarchical clustering, only the ascending classification is presented in this article.", "Clustering algorithms use the distance in order to separate observations into different groups. Therefore, before diving into the presentation of the two classification methods, a reminder exercise on how to compute distances between points is presented.", "Let a data set containing the points a = (0,0)\u2032, b = (1,0)\u2032 and c = (5,5)\u2032. Compute the matrix of Euclidean distances between the points by hand and in R.", "By the Pythagorean theorem, we will remember that the distance between 2 points", "So for instance, for the distance between the points b = (1,0)\u2032 and c = (5,5)\u2032 presented in the statement above, we have:", "We can proceed similarly for all pairs of points to find the distance matrix by hand. In R, the dist() function allows you to find the distance of points in a matrix or dataframe in a very simple way:", "Note that the argument method = \"euclidean\" is not mandatory because the Euclidean method is the default one.", "The distance matrix resulting from the dist() function gives the distance between the different points. The Euclidean distance between the points b and c is 6.403124, which corresponds to what we found above via the Pythagorean formula.", "Note: If two variables do not have the same units, one may have more weight in the calculation of the Euclidean distance than the other. In that case, it is preferable to scale the data. Scaling data allows to obtain variables independent of their unit, and this can be done with the scale() function.", "Now that the distance has been presented, let\u2019s see how to perform clustering analysis with the k-means algorithm.", "The first form of classification is the method called k-means clustering or the mobile center algorithm. As a reminder, this method aims at partitioning n observations into k clusters in which each observation belongs to the cluster with the closest average, serving as a prototype of the cluster.", "It is presented below via an application in R and by hand.", "For this exercise, the Eurojobs.csv database available here is used.", "This database contains the percentage of the population employed in different industries in 26 European countries in 1979. It contains 10 variables:", "We first import the dataset. See how to import data into R if you need a reminder.", "Note that there is a numbering before the first variable Country. For more clarity, we will replace this numbering by the country. To do this, we add the argument row.names = 1 in the import function read.csv() to specify that the first column corresponds to the row names:", "We now have a \u201cclean\u201d dataset of 26 observations and 9 quantitative continuous variables on which we can base the classification. Note that in this case it is not necessary to standardize the data because they are all expressed in the same unit (in percentage). If this was not the case, we would have had to standardize the data via the scale() function (do not forget it otherwise your results may be completely different!).", "The so-called k-means clustering is done via the kmeans() function, with the argument centers that corresponds to the number of desired clusters. In the following we apply the classification with 2 classes and then 3 classes as examples.", "Note that the argument centers = 2 is used to set the number of clusters, determined in advance. In this exercise the number of clusters has been determined arbitrarily. This number of clusters should be determined according to the context and goal of your analysis, or based on methods explained in this section. Calling print(model$cluster) or model$cluster is the same. This output specifies the group (i.e., 1 or 2) to which each country belongs to.", "The cluster for each observation can be stored directly in the dataset as a column:", "The quality of a k-means partition is found by calculating the percentage of the TSS \u201cexplained\u201d by the partition using the following formula:", "where BSS and TSS stand for Between Sum of Squares and Total Sum of Squares, respectively. The higher the percentage, the better the score (and thus the quality) because it means that BSS is large and/or WSS is small.", "Here is how you can check the quality of the partition in R:", "The quality of the partition is 51.87%. This value has no real interpretation in absolute terms except that a higher quality means a higher explained percentage. However, it is more insightful when it is compared to the quality of other partitions (with the same number of clusters!) in order to determine the best partition among the ones considered.", "The k-means algorithm uses a random set of initial points to arrive at the final classification. Due to the fact that the initial centers are randomly chosen, the same command kmeans(Eurojobs, centers = 2) may give different results every time it is run, and thus slight differences in the quality of the partitions. The nstart argument in the kmeans() function allows to run the algorithm several times with different initial centers, in order to obtain a potentially better partition:", "Depending on the initial random choices, this new partition will be better or not compared to the first one. In our example, the partition is better as the quality increased to 54.25%.", "One of the main limitation often cited regarding k-means is the stability of the results. As the initial centers are randomly chosen, running the same command may yield different results. Adding the nstart argument in the kmeans() function limits this issue as it will generate several different initializations and take the most optimal one, leading to a better stability of the classification.", "We now perform the k-means classification with 3 clusters and compute its quality:", "It can be seen that the classification into three groups allows for a higher explained percentage and a higher quality. This will always be the case: with more classes, the partition will be finer, and the BSS contribution will be higher. On the other hand, the \u201cmodel\u201d will be more complex, requiring more classes. In the extreme case where k = n (each observation is a singleton class), we have BSS = TSS, but the partition has lost all interest.", "In order to find the optimal number of clusters for a k-means, it is recommended to choose it based on:", "We show the R code for these 4 methods below, more theoretical information can be found here.", "The Elbow method looks at the total within-cluster sum of square (WSS) as a function of the number of clusters.", "The location of a knee in the plot is usually considered as an indicator of the appropriate number of clusters because it means that adding another cluster does not improve much better the partition. This method seems to suggest 4 clusters.", "The Elbow method is sometimes ambiguous and an alternative is the average silhouette method.", "The Silhouette method measures the quality of a clustering and determines how well each point lies within its cluster.", "The Silhouette method suggests 2 clusters.", "The optimal number of clusters is the one that maximizes the gap statistic. This method suggests only 1 cluster (which is therefore a useless clustering).", "As you can see these three methods do not necessarily lead to the same result. Here, all 3 approaches suggest a different number of clusters.", "A fourth alternative is to use the NbClust() function, which provides 30 indices for choosing the best number of clusters.", "Based on all 30 indices, the best number of clusters is 3 clusters.", "(See the article \u201cGraphics in R with ggplot2\u201d to learn how to create this kind of plot in {ggplot2}).", "To confirm that your number of classes is indeed optimal, there is a way to evaluate the quality of your clustering via the silhouette plot (which shows the silhouette coefficient on the y axis).", "We draw the silhouette plot for 2 clusters, as suggested by the average silhouette method:", "As a reminder, the interpretation of the silhouette coefficient is as follows:", "The silhouette plot above and the average silhouette coefficient help to determine whether your clustering is good or not. If a large majority of the silhouette coefficients are positive, it indicates that the observations are placed in the correct group. This silhouette plot can therefore be used in the choice of the optimal number of classes.", "It is also possible to plot clusters by using the fviz_cluster() function. Note that a principal component analysis is performed to represent the variables in a 2 dimensions plane.", "Now that the k-means clustering has been detailed in R, see how to do the algorithm by hand in the following sections.", "Perform by hand the k-means algorithm for the points shown in the graph below, with k = 2 and with the points i = 5 and i = 6 as initial centers. Compute the quality of the partition you just found and then check your answers in R.", "Assume that the variables have the same units so there is no need to scale the data.", "Step 1. Here are the coordinates of the 6 points:", "Step 2. Compute the distance matrix point by point with the Pythagorean theorem. Remind that the distance between point a and point b is found with:", "We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to two decimals):", "Step 3. Based on the distance matrix computed in step 2, we can put each point to its closest group and compute the coordinates of the center.", "We first put each point in its closest group:", "Note that computing the distances between each point and the points 5 and 6 is sufficient. There is no need to compute the distance between the points 1 and 2 for example, as we compare each point to the initial centers (which are points 5 and 6).", "We then compute the coordinates of the centers of the two groups by taking the mean of the coordinates x and y:", "Step 4. We make sure that the allocation is optimal by checking that each point is in the nearest cluster. The distance between a point and the center of a cluster is again computed thanks to the Pythagorean theorem. Thus, we have:", "The minimum distance between the points and the two clusters is colored in green.", "We check that each point is in the correct group (i.e., the closest cluster). According to the distance in the table above, point 6 seems to be closer to the cluster 1 than to the cluster 2. Therefore, the allocation is not optimal and point 6 should be reallocated to cluster 1.", "Step 5. We compute again the centers of the clusters after this reallocation. The centers are found by taking the mean of the coordinates x and y of the points belonging to the cluster. We thus have:", "Step 6. Repeat step 4 until the allocation is optimal. If the allocation is optimal, the algorithm stops. In our example we have:", "All points are correctly allocated to its nearest cluster, so the allocation is optimal and the algorithm stops.", "Step 7. State the final partition and the centers. In our example:", "Now that we have the clusters and the final centers, we compute the quality of the partition we just found. Remember that we need to compute the BSS and TSS to find the quality. Below the steps to compute the quality of this partition by k-means, based on this summary table:", "Step 1. Compute the overall mean of the x and y coordinates:", "Step 2. Compute TSS and WSS:", "Regarding WSS, it is splitted between cluster 1 and cluster 2. For cluster 1:", "Finally, the quality of the partition is:", "We are now going to verify all these solutions (the partition, the final centers and the quality) in R.", "As you can imagine, the solution in R us much shorter and requires much less computation on the user side. We first need to enter the data as a matrix or dataframe:", "We now perform the k-means via the kmeans() function with the point 5 and 6 as initial centers:", "Unlike in the previous application with the dataset Eurojobs.csv where the initial centers are randomly chosen by R, in this second application we want to specify which points are going to be the two initial centers. For this, we need to set centers = X[c(5,6), ] to indicate that that there are 2 centers, and that they are going to be the points 5 and 6 (see a reminder on how to subset a dataframe if needed).", "The reason for adding the argument algorithm = \"Lloyd\" can be found in the usage of the R function kmeans(). In fact, there are several variants of the k-means algorithm. The default choice is the Hartigan and Wong (1979) version, which is more sophisticated than the basic version detailed in the solution by hand. By using the original version of Lloyd (1982), we find the same solution in R and by hand. For more information, you can consult the documentation of the kmeans() function (via ?kmeans or help(kmeans)) and read the articles mentioned.", "The solution in R is then found by extracting", "Points 1, 5 and 6 belong to cluster 1, points 2, 3 and 4 belong to cluster 2.", "The 3 results are equal to what we found by hand (except the quality which is slightly different due to rounding).", "Remind that the difference with the partition by k-means is that for hierarchical clustering, the number of classes is not specified in advance. Hierarchical clustering will help to determine the optimal number of clusters.", "Before applying hierarchical clustering by hand and in R, let\u2019s see how it works step by step:", "There are 5 main methods to measure the distance between clusters, referred as linkage methods:", "In the following sections, only the three first linkage methods are presented (first by hand and then the results are verified in R).", "Using the data from the graph and the table below, perform by hand the 3 algorithms (single, complete and average linkage) and draw the dendrograms. Then check your answers in R.", "Step 1. For all 3 algorithms, we first need to compute the distance matrix between the 5 points thanks to the Pythagorean theorem. Remind that the distance between point a and point b is found with:", "We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to three decimals):", "Step 2. From the distance matrix computed in step 1, we see that the smallest distance = 0.328 between points 2 and 4. 0.328 corresponds to the first height (more on this later when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 & 4, 3 and 5. The new distances between the group 2 & 4 and all other points are now:", "To construct this new distance matrix, proceed point by point:", "Step 3. Based on the distance matrix in step 2, the smallest distance is 0.483 between points 3 and 2 & 4 (the second height for the dendrogram). Since points 3 and 2 & 4 are the closest to each other, they are combined to form a new group, the group 2 & 3 & 4. The groups are thus: 1, 2 & 3 & 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:", "Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 & 5. The groups are thus: 1 & 5 and 2 & 3 & 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:", "Step 5. The final combination of points is the combination of points 1 & 5 and 2 & 3 & 4, with a final height of 1.530. Heights are used to draw the dendrogram in the sixth and final step.", "Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:", "In hierarchical clustering, dendrograms are used to show the sequence of combinations of the clusters. The distances of merge between clusters, called heights, are illustrated on the y-axis.", "Complete linkage is quite similar to single linkage, except that instead of taking the smallest distance when computing the new distance between points that have been grouped, the maximum distance is taken.", "The steps to perform the hierarchical clustering with the complete linkage (maximum) are detailed below.", "Step 1. Step 1 is exactly the same than for single linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:", "Step 2. From the distance matrix computed in step 1, we see that the smallest distance = 0.328 between points 2 and 4. It is important to note that even if we apply the complete linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 & 4, 3 and 5. The new distances between the group 2 & 4 and all other points are now:", "To construct this new distance matrix, proceed point by point as we did for single linkage:", "Step 3. Based on the distance matrix in step 2, the smallest distance is 0.603 between points 3 and 2 & 4 (the second height for the dendrogram). Since points 3 and 2 & 4 are the closest to each other, they are combined to form a new group, the group 2 & 3 & 4. The groups are thus: 1, 2 & 3 & 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:", "Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 & 5. The groups are thus: 1 & 5 and 2 & 3 & 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:", "Step 5. The final combination of points is the combination of points 1 & 5 and 2 & 3 & 4, with a final height of 2.675. Heights are used to draw the dendrogram in the sixth and final step.", "Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:", "With the average linkage criterion, it is not the minimum nor the maximum distance that is taken when computing the new distance between points that have been grouped, but it is, as you guessed by now, the average distance between the points.", "The steps to perform the hierarchical clustering with the average linkage are detailed below.", "Step 1. Step 1 is exactly the same than for single and complete linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:", "Step 2. From the distance matrix computed in step 1, we see that the smallest distance = 0.328 between points 2 and 4. It is important to note that even if we apply the average linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 & 4, 3 and 5. The new distances between the groups 2 & 4 and all other points are now:", "To construct this new distance matrix, proceed point by point as we did for the two previous criteria:", "Step 3. Based on the distance matrix in step 2, the smallest distance is 0.543 between points 3 and 2 & 4 (the second height for the dendrogram). Since points 3 and 2 & 4 are the closest to each other, they are combined to form a new group, the group 2 & 3 & 4. The groups are thus: 1, 2 & 3 & 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:", "Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 & 5. The groups are thus: 1 & 5 and 2 & 3 & 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:", "Step 5. The final combination of points is the combination of points 1 & 5 and 2 & 3 & 4, with a final height of 2.126167. Heights are used to draw the dendrogram in the sixth and final step.", "Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:", "To perform the hierarchical clustering with any of the 3 criterion in R, we first need to enter the data (in this case as a matrix format, but it can also be entered as a dataframe):", "We can apply the hierarchical clustering with the single linkage criterion thanks to the hclust() function with the argument method = \"single\":", "Note that the hclust() function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix X corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the dist() function.", "We can now extract the heights and plot the dendrogram to check our results by hand found above:", "As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.", "Remember that hierarchical clustering is used to determine the optimal number of clusters. This optimal number of clusters can be determined thanks to the dendrogram. For this, we usually look at the largest difference of heights:", "How to determine the number of clusters from a dendrogram? Take the largest difference of heights and count how many vertical lines you see", "The largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 & 3 & 4 with the group 1 & 5. To determine the optimal number of clusters, simply count how many vertical lines you see within this largest difference. In our case, the optimal number of clusters is thus 2.", "In R, we can even highlight these two clusters directly in the dendrogram with the rect.hclust() function:", "Note that determining the optimal number of clusters via the dendrogram is not specific to the single linkage, it can be applied to other linkage methods too!", "Below another figure explaining how to determine the optimal number of clusters:", "(See this hierarchical clustering cheatsheet for more visualizations like this.)", "Finally, we could also determine the optimal number of cluster thanks to a barplot of the heights (stored in $height of the clustering output):", "Again, look for the largest jump of heights. In our case, the largest jump is from 1 to 2 classes. Therefore, the optimal number of classes is 2.", "Note that determining the number of clusters using the dendrogram or barplot is not a strict rule. You can also consider other methods such as the silhouette plot, elbow plot or some numerical measures like Dunn\u2019s index, Hubert\u2019s gamma, etc., which show the variation of the error with the number of clusters (k), and you choose the value of k where the error is smallest. Furthermore, measuring the goodness of clusters can be done thanks to the Dunn\u2019s Index (the higher the index, the better). However, these methods are beyond the scope of this course and the method presented with the dendrogram is generally sufficient.", "We can apply the hierarchical clustering with the complete linkage criterion thanks to the hclust() function with the argument method = \"complete\":", "Note that the hclust() function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix X corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the dist() function.", "We can now extract the heights and plot the dendrogram to check our results by hand found above:", "As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.", "Similar to the single linkage, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 & 3 & 4 with the group 1 & 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the rect.hclust() function:", "We can apply the hierarchical clustering with the average linkage criterion thanks to the hclust() function with the argument method = \"average\":", "Note that the hclust() function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix X corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the dist() function.", "We can now extract the heights and plot the dendrogram to check our results by hand found above:", "As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.", "Like the single and complete linkages, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 & 3 & 4 with the group 1 & 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the rect.hclust() function:", "Clustering is rather a subjective statistical analysis and there can be more than one appropriate algorithm, depending on the dataset at hand or the type of problem to be solved. So choosing between k-means and hierarchical clustering is not always easy. If you have a good reason to think that there is a specific number of clusters in your dataset (for example if you would like to distinguish diseased and healthy patients depending on some characteristics but you do not know in which group patients belong to), you should probably opt for the k-means clustering as this technique is used when the number of groups is specified in advance. If you do not have any reason to believe there is a certain number of groups in your dataset (for instance in marketing when trying to distinguish clients without any prior belief on the number of different types of customers), then you should probably opt for the hierarchical clustering to determine in how many clusters your data should be divided.", "In addition to this, if you are still undecided note that, on the one hand, with a large number of variables, k-means may be computationally faster than hierarchical clustering if the number of clusters is small. On the other hand, the result of a hierarchical clustering is a structure that is more informative and interpretable than the unstructured set of flat clusters returned by k-means. Therefore, it is easier to determine the optimal number of clusters by looking at the dendrogram of a hierarchical clustering than trying to predict this optimal number in advance in case of k-means.", "Thanks for reading. I hope this article helped you understand the different clustering methods and how to compute them by hand and in R.", "As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD candidate and teaching assistant in statistics at UCLouvain. Interested in statistics, R, and making them accessible to everyone. Author of statsandr.com."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F10fe13712787&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----10fe13712787--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----10fe13712787--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://antoinesoetewey.medium.com/?source=post_page-----10fe13712787--------------------------------", "anchor_text": ""}, {"url": "https://antoinesoetewey.medium.com/?source=post_page-----10fe13712787--------------------------------", "anchor_text": "Antoine Soetewey"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca32a96e6dc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&user=Antoine+Soetewey&userId=ca32a96e6dc7&source=post_page-ca32a96e6dc7----10fe13712787---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10fe13712787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10fe13712787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@thejohnnyme?utm_source=medium&utm_medium=referral", "anchor_text": "Nikola Johnny Mirkovic"}, {"url": "https://www.statsandr.com/blog/data-manipulation-in-r/#scale", "anchor_text": "scale()"}, {"url": "https://www.statsandr.com/blog/data/Eurojobs.csv", "anchor_text": "here"}, {"url": "https://www.statsandr.com/blog/how-to-import-an-excel-file-in-rstudio", "anchor_text": "how to import data into R"}, {"url": "https://www.statsandr.com/blog/variable-types-and-examples/#continuous", "anchor_text": "quantitative continuous variables"}, {"url": "https://www.statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#optimal-number-of-clusters", "anchor_text": "section"}, {"url": "https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/", "anchor_text": "here"}, {"url": "https://www.statsandr.com/blog/graphics-in-r-with-ggplot2/", "anchor_text": "Graphics in R with ggplot2"}, {"url": "https://www.statsandr.com/blog/data-manipulation-in-r", "anchor_text": "how to subset a dataframe"}, {"url": "https://www.statsandr.com/blog/files/Hierarchical-clustering-cheatsheet.pdf", "anchor_text": "hierarchical clustering cheatsheet"}, {"url": "https://www.statsandr.com/blog/an-efficient-way-to-install-and-load-r-packages/", "anchor_text": "An efficient way to install and load R packages"}, {"url": "https://www.statsandr.com/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/", "anchor_text": "Do my data follow a normal distribution? A note on the most widely used distribution and how to test for normality in R"}, {"url": "https://www.statsandr.com/blog/fisher-s-exact-test-in-r-independence-test-for-a-small-sample/", "anchor_text": "Fisher\u2019s exact test in R: independence test for a small sample"}, {"url": "https://www.statsandr.com/blog/chi-square-test-of-independence-in-r/", "anchor_text": "Chi-square test of independence in R"}, {"url": "https://www.statsandr.com/blog/how-to-create-a-timeline-of-your-cv-in-r/", "anchor_text": "How to create a timeline of your CV in R"}, {"url": "https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/", "anchor_text": "https://statsandr.com"}, {"url": "https://medium.com/tag/statistics?source=post_page-----10fe13712787---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/education?source=post_page-----10fe13712787---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/technology?source=post_page-----10fe13712787---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/science?source=post_page-----10fe13712787---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----10fe13712787---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10fe13712787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&user=Antoine+Soetewey&userId=ca32a96e6dc7&source=-----10fe13712787---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10fe13712787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&user=Antoine+Soetewey&userId=ca32a96e6dc7&source=-----10fe13712787---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10fe13712787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----10fe13712787--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F10fe13712787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----10fe13712787---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----10fe13712787--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----10fe13712787--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----10fe13712787--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----10fe13712787--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----10fe13712787--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----10fe13712787--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----10fe13712787--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----10fe13712787--------------------------------", "anchor_text": ""}, {"url": "https://antoinesoetewey.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://antoinesoetewey.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Antoine Soetewey"}, {"url": "https://antoinesoetewey.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "759 Followers"}, {"url": "http://statsandr.com", "anchor_text": "statsandr.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca32a96e6dc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&user=Antoine+Soetewey&userId=ca32a96e6dc7&source=post_page-ca32a96e6dc7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4d170b501b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-guide-to-clustering-analysis-10fe13712787&newsletterV3=ca32a96e6dc7&newsletterV3Id=4d170b501b4d&user=Antoine+Soetewey&userId=ca32a96e6dc7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}