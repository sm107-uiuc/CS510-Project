{"url": "https://towardsdatascience.com/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a", "time": 1683005580.310109, "path": "towardsdatascience.com/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a/", "webpage": {"metadata": {"title": "Fundamental Iterative Methods of Reinforcement Learning | by Nathan Lambert | Towards Data Science", "h1": "Fundamental Iterative Methods of Reinforcement Learning", "description": "Value iteration, policy iteration, and Q learning. How a markov decision process (MDP) formulates a reinforcement learning problem."}, "outgoing_paragraph_urls": [{"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com", "paragraph_index": 27}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com", "paragraph_index": 27}], "all_paragraphs": ["This article focuses on taking an understanding of basic MDPs (reviewed here briefly) and applying it to how it relates to a fundamental reinforcement learning method. The methods I will focus on are Value Iteration and Policy Iteration. These two methods underpin Q-value Iteration, which directly leads to Q-Learning.", "Some of my related articles that can be read before, or after this (intentionally independent):", "Q-Learning kick-started the deep reinforcement learning wave we are on, so it is a crucial peg in the reinforcement learning student\u2019s playbook.", "Markov Decision Processes (MDPs) are the stochastic model underpinning reinforcement learning (RL). If you\u2019re familiar, you can skip this section, but I added explanations for why each element matters in a reinforcement learning context.", "There are two important characteristic utilities of a MDP \u2014 values of a state, and q-values of a chance node. The * in any MDP or RL value denotes an optimal quantity.", "Learn the values for all states, then we can act according to the gradient. Value iteration learns the value of the states from the Bellman Update directly. The Bellman Update is guaranteed to converge to optimal values, under some non-restrictive conditions.", "Learning a policy may be more direct than learning a value. Learning a value may take an infinite amount of time to converge to numerical precision of a 64bit float (think about a moving average averaging in a constant at every iteration, after starting with an estimate of 0, it will add a smaller and smaller nonzero number forever).", "Learn a policy in tandem to the values. Policy learning incrementally looks at the current values and extracts a policy. Because the action space is finite, the hope is that it can converge faster than Value Iteration. Conceptually, the last change to the actions will happen well before the small rolling-average updates end. There are two steps to Policy Iteration.", "The first is called Policy Extraction, which is how you go from a value to a policy \u2014 by taking the policy that maximizes over expected values.", "The second step is Policy Evaluation. Policy evaluation takes a policy and runs value iteration conditioned on a policy. The samples are forever tied to the policy, but we know we have to run the iterative algorithms for way fewer steps to extract the relevant action information.", "Like value iteration, policy iteration is guaranteed to converge for most reasonable MDPs because of the underlying Bellman Update.", "The problem with knowing optimal values is that it can be hard to distill a policy from it. The argmax operator is distinctly nonlinear and difficult to optimize over, so Q-value Iteration takes a step towards direct policy extraction. The optimal policy at each state is simply the max q-value at that state.", "The reason most instruction starts with Value Iteration is that it slots into the Bellman updates a little more naturally. Q-value Iteration requires the substitution of two of the key MDP value relations together. After doing so, it is one step removed from Q-learning, which we will get to know.", "Let\u2019s make sure you understand all the terms. Essentially, each update is made up of two terms after the summation (and potentially a max term that selects an action). Let\u2019s factor out the brackets and discuss how they relate to an MDP.", "The first term is a summation over the product T(s,a,s\u2019)R(s,a,s\u2019). This term represents the latent value and likelihood of a given state and transition. The T term, or transition, governs how likely it is to get a given reward from a transition (recall, a tuple s,a,s\u2019 determines a tuple where an action a takes an agent from state s to state s\u2019). This will do things like weight low probability states with high rewards against frequent states with lower rewards.", "The next term governs the \u201cBellman-ness\u201d of these algorithms. It is a weighting of the data at the last step of the iterative algorithm \u2014 V, with the term above. This pulls information from neighboring states about value so that we can understand longer-term transitions. Think of this term as to where most of the recursive update happens, and the first term is a weighing prior determined by the environment.", "All of the iterative algorithms are told to \u201cconverge to the optimal value or policy, under some conditions.\u201d What are those conditions you ask?", "Thankfully, in practice, these conditions are easy to meet. Most exploration has an epsilon-greediness that includes a chance at a random action, always (so any action is feasible) and a non-one discount factor results in more favorable performance. Ultimately, these algorithms work in plenty of settings, so they\u2019re definitely worth giving a shot.", "How do we make what we have seen into a reinforcement learning problem? We need to use samples rather than the true T(s,a,s\u2019) and R(s,a,s\u2019) functions.", "The only difference between iterative methods in MDPs and the basic methods of solving a reinforcement learning problem is that RL samples from the underlying transition and reward functions of an MDP, rather than having it in the update rule. There are two things we need to update to get going, a replacement for T(s,a,s\u2019) and a replacement for R(s,a,s\u2019).", "First, let us approximate the transition function as the average action conditioned transition for each observed tuple. All the values we have not seen are initialized with random values. This is the simplest form of model-based reinforcement learning (my research area).", "Now, all that is left is remembering what to do with the reward, right? But, we actually have a reward with each step, so we can get away with it (methods average out to the correct value with many samples). Consider approximating the Q-value iteration equation with a sampled reward, as below.", "The above equation is Q-learning. We start with some vector Q(s,a) that is filled with random values, and then we collect interactions with the world and tune alpha. Alpha is a learning rate, so we will lower it when we think our algorithm is converging.", "It works out that Q-learning converges really similarly to Q-value Iteration, but we are just running the algorithm with an incomplete view of the world.", "The Q-learning used in robotics and games is in more complex feature spaces with neural networks approximating a large table of all the state-action pairs. For a summary of how Deep Q-Learning shocked the world, here is a great video until I write my own piece about it!", "More? Subscribe to my newsletter on robotics, artificial intelligence, and society!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trying to think freely and create equitable & impactful automation @ UCBerkeley EECS. Subscribe directly at robotic.substack.com. More at natolambert.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdf8ff078652a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----df8ff078652a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df8ff078652a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://natolambert.medium.com/?source=post_page-----df8ff078652a--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----df8ff078652a--------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----df8ff078652a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf8ff078652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf8ff078652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/what-is-a-markov-decision-process-anyways-bdab65fd310c", "anchor_text": "What is a Markov Decision Process Anyways?"}, {"url": "https://towardsdatascience.com/the-hidden-linear-algebra-of-reinforcement-learning-406efdf066a", "anchor_text": "The hidden linear algebra of reinforcement learning."}, {"url": "https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec10.pdf", "anchor_text": "lecture"}, {"url": "https://inst.eecs.berkeley.edu/~cs188/sp20/", "anchor_text": "cs188 at UC Berkeley"}, {"url": "https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7", "anchor_text": "Convergence of Reinforcement Learning AlgorithmsAre there any simple bounds one can put on convergence?towardsdatascience.com"}, {"url": "https://robotic.substack.com/", "anchor_text": "Democratizing AutomationA blog about robots & artificial intelligence, making them beneficial for everyone, and the coming automation wave\u2026robotic.substack.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----df8ff078652a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----df8ff078652a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/math?source=post_page-----df8ff078652a---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----df8ff078652a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----df8ff078652a---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf8ff078652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&user=Nathan+Lambert&userId=890b1765e6d&source=-----df8ff078652a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf8ff078652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&user=Nathan+Lambert&userId=890b1765e6d&source=-----df8ff078652a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf8ff078652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df8ff078652a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdf8ff078652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----df8ff078652a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----df8ff078652a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----df8ff078652a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----df8ff078652a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----df8ff078652a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----df8ff078652a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----df8ff078652a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----df8ff078652a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----df8ff078652a--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://natolambert.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "653 Followers"}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com"}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-iterative-methods-of-reinforcement-learning-df8ff078652a&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}