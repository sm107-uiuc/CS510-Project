{"url": "https://towardsdatascience.com/in-depth-review-of-soft-actor-critic-91448aba63d4", "time": 1683001552.861344, "path": "towardsdatascience.com/in-depth-review-of-soft-actor-critic-91448aba63d4/", "webpage": {"metadata": {"title": "In-depth review of Soft Actor-Critic | by Chris Yoon | Towards Data Science", "h1": "In-depth review of Soft Actor-Critic", "description": "In this post, we review Soft Actor-Critic (Haarnoja et al., 2018 & 2019), a very successful reinforcement learning algorithm that attains state-of-the-art performance in continuous control tasks\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1702.08165.pdf", "anchor_text": "this paper by Haarnoja et al., (2017)", "paragraph_index": 14}], "all_paragraphs": ["In this post, we review Soft Actor-Critic (Haarnoja et al., 2018 & 2019), a very successful reinforcement learning algorithm that attains state-of-the-art performance in continuous control tasks (like robotic locomotion and manipulation). Soft Actor-Critic uses the concept of maximum entropy learning, which brings some neat conceptual and practical advantages we will discuss in this post.", "Here is how this post is structured:", "First, we will have a brief review of general policy iteration (I will assume that the reader is familiar with Markov Decision Processes) \u2014 a concept essential to understanding any reinforcement learning algorithm. Then, we will discuss the intuition behind Soft Actor-Critic, and the conceptual benefits it brings. Next, we will discuss Soft Policy Iteration, the theoretical framework Soft Actor-Critic (SAC) approximates, and then go on to SAC. I will also provide code snippets explaining the algorithm throughout the post.", "In essence, SAC seeks to maximize the \u201centropy\u201d in policy, in addition to the expected reward from the environment. The entropy in policy can be interpreted as randomness in the policy.", "From the figure above, we see that probability distributions with low entropy have a tendency to \u201cgreedily\u201d sample certain values, as the probability mass is distributed relatively unevenly. That said, rewarding the policy for high entropy brings several conceptual advantages: first, it explicitly encourages exploration of the state space, improving the transition data collected; second, it allows the policy to capture multiple modes of good policies, preventing premature convergence to bad local optima.", "This motivation and formulation are empirically validated in the Soft Actor-Critic papers; in many of the MuJoCo control tasks, Soft Actor-Critic outperforms other state-of-the-art algorithms:", "In classical MDP theory, a standard approach to finding an optimal policy that maximizes the expected cumulative discounted reward for every state is policy iteration. Policy iteration is a two-step iteration scheme alternating between policy evaluation and policy improvement.", "In the policy evaluation step, we wish to find the accurate value function for our current policy. To do so, we repeatedly apply the Bellman operator defined as below:", "Policy improvement step is carried out by repeatedly applying the Bellman optimality operator:", "which, given the initial value function V, is guaranteed to converge the true (optimal) value function V*. In particular, the convergence properties of the Bellman operator and optimality operator are due to the fact that they are both contraction mappings. Furthermore, in theory, the optimal policy pi* can be constructed from the optimal value function; given an initial policy pi, we minimize some metric between the current policy and the derived update policy.", "As such, by alternating policy evaluation and policy improvement, we can find an exact solution to Markov Decision Processes in the tabular case (i.e. finite state-action space and no function approximation).", "In the paper, Haarnoja introduced Soft Policy Iteration, an extension of general policy iteration with the entropy of the policy as an additional reward term. In particular, the agent seeks to maximize both the environment\u2019s expected reward and the policy\u2019s entropy.", "To that end, the original Bellman operator is augmented with an entropy regularizer term:", "As in the un-regularized case, repeated application of the entropy regularized Bellman operator to any initial Q function is guaranteed to converge to the optimal \u201cSoft\u201d Q function.", "For the policy improvement step, we update the policy distribution towards the softmax distribution for the current Q function (to see why that is, check out this paper by Haarnoja et al., (2017)) In particular, we want to minimize the distance (\u201cdivergence\u201d) between the two distributions. This is accomplished by minimizing the Kullback-Leibler (KL) divergence between the two distributions:", "This update scheme guarantees monotonic improvement of the policy in the tabular case (i.e. when the state and action spaces are finite and no function approximation is used), as proven in Haarnoja et al. (2018). Later, Geist et al. (2019) generalize these properties to any formulation of the regularizer.", "For complex learning domains with high-dimensional and/or continuous state-action spaces, it is mostly impossible to find exact solutions for the MDP. Thus, we must leverage function approximation (i.e. neural networks) to find a practical approximation to soft policy iteration.", "To that end, Haarnoja et al. models the soft Q-function as an expressive neural network, and the policy as a Gaussian distribution over the action space with the mean and covariance given as neural network outputs with the current state as input. Here\u2019s how these look like in code, for the implementation process:", "In addition, the Soft Actor-Critic is an off-policy learning algorithm, meaning that we can update the Q-network and policy parameters with experience data collected from a policy different than the current one; for every actor roll-out, we save all the transition data in a replay buffer (denoted as D in the equations below).", "The Q-function parameters can be optimized every update step by using the gradient of the mean squared loss between the predicted action-value and the target action-value q_t:", "Here, the alpha term represents the \"entropy temperature,\" i.e. how much we weight the \"randomness\" of our policy versus the environment reward.", "In code, we can implement this as:", "Now onto the policy improvement step: in practice, Soft Actor-Critic uses a few modifications from soft policy iteration. Using the fact that the Q parameters are differentiable, Haarnoja et al. uses the \u201cre-parameterization trick\u201d on the policy output to get a low variance estimator; in particular, we represent the actions as the hyperbolic tangent (tanh) applied to z-values sampled from the mean and covariance outputted by the policy neural network.", "Furthermore, to address action bounds on our unbounded Gaussian distribution, we have to modify the log(pi) computation as:", "Here, log(mu) represents the cumulative distribution function (CDF) computed from the mean and covariance from the policy neural network. Perhaps this is clearer in code:", "Then, the policy parameter can be optimized directly by minimizing a simplified form of the KL divergence from soft policy iteration; we take the stochastic gradient of the objective function:", "This simplified form comes from ignoring the denominator of the softmax distribution of Q, as it does not contribute to the gradient of the objective function.", "In code, we can implement this as:", "In the first version of SAC (2018) uses a fixed entropy temperature alpha. Though the performance of the original SAC was quite impressive, alpha turned out to be a very sensitive hyperparameter. To remedy this, the second version of SAC (2019) casts alpha as an update-able parameter. In particular, we update by taking the gradient of the objective function below:", "where H_bar represents the desired minimum entropy, usually set to a zero vector. It is recommended to use the SAC (2019) with this adjustable alpha, as it improves on the performance and stability of the algorithm.", "This concludes the review of the Soft Actor-Critic algorithm!", "To keep my post as concise as possible, I only included the relevant, specific snippets of the implementation; to see the full implementation, check out my GitHub repository:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F91448aba63d4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----91448aba63d4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----91448aba63d4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----91448aba63d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----91448aba63d4--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863----91448aba63d4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91448aba63d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91448aba63d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@awjuliani/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d", "anchor_text": "https://medium.com/@awjuliani/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d"}, {"url": "https://arxiv.org/pdf/1702.08165.pdf", "anchor_text": "this paper by Haarnoja et al., (2017)"}, {"url": "https://github.com/cyoon1729/Policy-Gradient-Methods", "anchor_text": "cyoon1729/Policy-Gradient-MethodsImplementation of Algorithms from the Policy Gradient Family. Currently includes: A2C, A3C, DDPG, TD3, SAC \u2026github.com"}, {"url": "https://arxiv.org/abs/1801.01290", "anchor_text": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. Haarnoja et al. (2018)"}, {"url": "https://arxiv.org/abs/1812.05905", "anchor_text": "Soft Actor-Critic Algorithms and Applications. Haarnoja et al. (2019)"}, {"url": "https://arxiv.org/abs/1901.11275", "anchor_text": "A Theory of Regularized Markov Decision Processes. Geist et al. (2019)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----91448aba63d4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----91448aba63d4---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----91448aba63d4---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/actor-critic?source=post_page-----91448aba63d4---------------actor_critic-----------------", "anchor_text": "Actor Critic"}, {"url": "https://medium.com/tag/markov-decision-process?source=post_page-----91448aba63d4---------------markov_decision_process-----------------", "anchor_text": "Markov Decision Process"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F91448aba63d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&user=Chris+Yoon&userId=b24112d01863&source=-----91448aba63d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F91448aba63d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&user=Chris+Yoon&userId=b24112d01863&source=-----91448aba63d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91448aba63d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----91448aba63d4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F91448aba63d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----91448aba63d4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----91448aba63d4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----91448aba63d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----91448aba63d4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----91448aba63d4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----91448aba63d4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----91448aba63d4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----91448aba63d4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----91448aba63d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/@thechrisyoon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "631 Followers"}, {"url": "https://www.linkedin.com/in/chris-yoon-75847418b/", "anchor_text": "https://www.linkedin.com/in/chris-yoon-75847418b/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6d3234499fec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-depth-review-of-soft-actor-critic-91448aba63d4&newsletterV3=b24112d01863&newsletterV3Id=6d3234499fec&user=Chris+Yoon&userId=b24112d01863&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}