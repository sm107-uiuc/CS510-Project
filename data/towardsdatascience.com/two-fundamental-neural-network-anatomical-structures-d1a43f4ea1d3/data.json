{"url": "https://towardsdatascience.com/two-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3", "time": 1683017451.743444, "path": "towardsdatascience.com/two-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3/", "webpage": {"metadata": {"title": "Two Fundamental Neural Network Anatomical Structures | by Arun Jagota | Towards Data Science", "h1": "Two Fundamental Neural Network Anatomical Structures", "description": "Like other organisms, artificial neural networks have evolved through the ages. In this post, we cover two key anatomies that have emerged: fully-connected versus convolutional. The second one is\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Like other organisms, artificial neural networks have evolved through the ages. In this post, we cover two key anatomies that have emerged: fully-connected versus convolutional. The second one is better suited to problems in image processing in which there are local features in a space with geometry. The first one is generally appropriate on problems in which there isn\u2019t a geometry and spatial locality of features is not paramount.", "Let\u2019s start with models of single artificial neurons, the \u201cLeggo bricks\u201d of neural networks. A neuron takes a vector x as input and derives a scalar output y from it. Most neuron models conform to y = f(wx + b). Here w is the vector of weights of the same dimensionality as x, and b is a scalar called the neuron\u2019s bias. This is graphically depicted below.", "The configurable part of this is the neuron\u2019s activation function f. Different choices of f lead to neurons with quite different capabilities.", "When the target output y is expected to be (approximately) a linear function of x, a linear activation function is called for. This is the setting of linear regression.", "When the target output y is binary, we have a choice: a sigmoidal activation function or a step activation function. Both theory and practice favor the sigmoid.", "There are several reasons for this.", "One is that the sigmoid is differentiable whereas the step function is not. This allows the neuron\u2019s parameters (the weights w and the bias b) to be trained via gradient descent on a data set of input-output pairs.", "A second one is that using a sigmoid captures more information which can often be used profitably. For example, consider two situations, one in which the neuron\u2019s output is 0.9 and one in which it is 0.7. We might be inclined to classify both as 1s. (Remember we seek a binary output.) Should we do so, it makes sense to attach higher confidence to the first one since the neuron\u2019s output was higher.", "Another way we can use the additional information in the neuron\u2019s output is by adjusting the binary classification threshold. This lets us become more (or less) conservative in our decision-making.", "The key point here is that the classification threshold can be adjusted post-training. In fact, any time we wish to. (This is especially useful after the neuron starts making decisions in the \u201cfield\u201d and we realize we\u2019d like to tweak its behavior.) This threshold is not a parameter during training. Only the weight vector w and bias b are.", "Simply put, if we observe the neuron is overly sensitive (i.e. its outputs are sometimes towards 1 when the target is 0) we can increase the classification threshold. Similarly, if the neuron is not sensitive enough, we can decrease the classification threshold.", "Well, actually a single neuron is already an example. A useful one at that. It can be used to map a vector of inputs to a numeric or binary output. That is, to solve regression and binary classification problems.", "The first breakthrough is an intermediate layer of neurons between the input and the output. This is graphically depicted below.", "The intermediate layer is called a hidden layer. In the schematic, we have used sigmoidal neurons in the internal layer. Linear hidden neurons are less useful and step neurons have issues we discussed earlier.", "A sigmoidal hidden neuron may be viewed as representing some binary feature of the input. The neuron\u2019s value is derived from the input vector x. This value may be viewed as the probability that the associated feature is present in the input.", "A neural network with a hidden layer maps an input vector to a vector in a space of features. This mapping is nonlinear. The feature vector is then mapped to the output. This indirect approach results in an architecture that is in principle more powerful than one without the hidden layer.", "In practice, there are some issues. How many neurons should go into the hidden layer? This depends on the complexity of the input-to-output mapping problem. This complexity may not be known. For linear problems, we may not need any hidden neurons. In fact, having them might hurt.", "The short answer is we don\u2019t. That said, we could try a different number of hidden neurons and pick the one that works best or at least adequately.", "On to the next question. For a fixed number of hidden neurons, how do the features they represent get learned? The short answer is via the learning process, typically a form of gradient descent called back-propagation. We won\u2019t go into the details here.", "That said, we will depict the roles the various weights touching a hidden neuron play.", "Finally, let\u2019s mention that, as before, the output neuron is sigmoidal for a binary classification problem and linear for a regression problem.", "Say we have a large set of images, some containing cats, others not. We\u2019d like to learn a classifier that can tell whether or not an image has a cat in it.", "In principle, we could map this problem to a neural network with one hidden layer, as depicted below.", "In practice this approach has difficulties. We have 10,000 input neurons. With a hidden layer of m neurons, this means we have 10000*m input-to-hidden weights. It's hard to imagine that we could use less than 20 hidden neurons to adequately learn a cat-or-not classifier.", "20 hidden neurons means 200,000 input-to-hidden weights. That's a lot of weights to learn! Even with a rich training set, overfitting is a significant risk.", "Let\u2019s think differently. Is there structure in this domain (images) that we might be able to exploit? It turns out the answer is yes. First some observations.", "Let\u2019s expand on 2 and 3. Consider the picture below. It shows horizontal edges at various locations. Each edge is the same feature but at a different location. Each of these feature occurrences is also local. Local just means that to detect the edge at any one location, one only needs to look at pixel values in the proximity of the edge.", "Okay, we now see that each feature is also on the same spatial grid as the input pixels. The implication of this is that for any one feature, there isn\u2019t a single value, rather a grid of values. For each location (i,j), feature f has a value that indicates whether f is present or absent at that location.", "Great. Seems like we have gone in the opposite direction. Seems like we have made the problem more complex. Instead of a feature having a single value, it now has a spatial grid of values, one per location.", "Not really. How would we try to represent a local feature in an MLNN? Separating out the actual feature function from the location where it applies? Plus, leverage its locality. We can\u2019t.", "The inability of the MLNN to (i) exploit feature locality and (ii) the ability to evaluate the feature at many different locations means that we need a lot of hidden neurons to model the combination of the functional and spatial aspects of a feature. (Functional meaning what the feature is, spatial meaning where it is evaluated.) On top of that, because we are unable to exploit locality, the number of input-to-hidden weights explodes.", "The picture below depicts the alternative that leverages the input geometry, the locality of features, and the need to evaluate a feature at many different locations.", "Let\u2019s zoom into the feature\u2019s neuron at location (i,j).", "Okay, so any single (local) feature is represented by the same small set of weights. We just slide this feature\u2019s function (called a Kernel) over the various locations (i, j) to get a reading on the feature\u2019s values over the entire grid. This sliding process is called convolution.", "By contrast, the MLP has no explicit mechanisms for exploiting either locality or sharing of weights.", "We have covered the anatomical structures of the two most important (feedforward) neural network architectures: fully-connected multi-layer neural networks and convolutional neural networks. We have discussed why convolutional neural networks are better suited to image processing than multi-layer neural networks. On the MLNNs are well-suited to problems in which locality and convolutions don\u2019t come into play in obvious ways.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD, Computer Science, neural nets. 14+ years in industry: data science algos developer. 24+ patents issued. 50 academic pubs. Blogs on ML/data science topics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd1a43f4ea1d3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jagota-arun.medium.com/?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----d1a43f4ea1d3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd1a43f4ea1d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd1a43f4ea1d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/", "anchor_text": "http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/"}, {"url": "http://deeplearning.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/", "anchor_text": "http://deeplearning.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/"}, {"url": "https://medium.com/tag/multilayer-perceptron?source=post_page-----d1a43f4ea1d3---------------multilayer_perceptron-----------------", "anchor_text": "Multilayer Perceptron"}, {"url": "https://medium.com/tag/sigmoid-function?source=post_page-----d1a43f4ea1d3---------------sigmoid_function-----------------", "anchor_text": "Sigmoid Function"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----d1a43f4ea1d3---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd1a43f4ea1d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&user=Arun+Jagota&userId=ef9ed921edad&source=-----d1a43f4ea1d3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd1a43f4ea1d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&user=Arun+Jagota&userId=ef9ed921edad&source=-----d1a43f4ea1d3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd1a43f4ea1d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd1a43f4ea1d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d1a43f4ea1d3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d1a43f4ea1d3--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://jagota-arun.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "685 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwo-fundamental-neural-network-anatomical-structures-d1a43f4ea1d3&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}