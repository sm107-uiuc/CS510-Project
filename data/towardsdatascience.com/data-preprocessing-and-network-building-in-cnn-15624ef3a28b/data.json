{"url": "https://towardsdatascience.com/data-preprocessing-and-network-building-in-cnn-15624ef3a28b", "time": 1683012940.755845, "path": "towardsdatascience.com/data-preprocessing-and-network-building-in-cnn-15624ef3a28b/", "webpage": {"metadata": {"title": "Data Preprocessing and Network Building in CNN | by Tanya | Towards Data Science", "h1": "Data Preprocessing and Network Building in CNN", "description": "In this article, we will go through the end-to-end pipeline of training convolution neural networks, i.e. organizing the data into directories, preprocessing, data augmentation, model building, etc\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/alxmamaev/flowers-recognition", "anchor_text": "flowers dataset", "paragraph_index": 1}, {"url": "https://www.kaggle.com/tanyadayanand/end-to-end-pipeline-for-training-cnns-resnet", "anchor_text": "notebook", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "proposed here", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1603.05027.pdf", "anchor_text": "here", "paragraph_index": 16}, {"url": "https://github.com/KaimingHe/resnet-1k-layers", "anchor_text": "ResNet-1001", "paragraph_index": 16}, {"url": "https://www.kaggle.com/tanyadayanand/resnet", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://keras.io/preprocessing/image/", "anchor_text": "the one that comes with the keras API", "paragraph_index": 20}, {"url": "https://keras.io/callbacks/", "anchor_text": "listed here", "paragraph_index": 38}, {"url": "https://keras.io/optimizers/", "anchor_text": "keras optimisers", "paragraph_index": 44}, {"url": "http://www.linkedin.com/in/tanyadayanand", "anchor_text": "LinkedIn", "paragraph_index": 51}], "all_paragraphs": ["In this article, we will go through the end-to-end pipeline of training convolution neural networks, i.e. organizing the data into directories, preprocessing, data augmentation, model building, etc.", "We will spend a good amount of time on data preprocessing techniques commonly used with image processing. This is because preprocessing takes about 50\u201380% of your time in most deep learning projects, and knowing some useful tricks will help you a lot in your projects. We will be using the flowers dataset from Kaggle to demonstrate the key concepts. To get into the codes directly, an accompanying notebook is published on Kaggle(Please use a CPU for running the initial parts of the code and GPU for model training).", "Let\u2019s begin with importing the necessary libraries and loading the dataset. This is a requisite step in every data analysis process.", "Images come in different shapes and sizes. They also come through different sources. For example, some images are what we call \u201cnatural images\u201d, which means they are taken in color, in the real world. For example:", "Taking all these variations into consideration, we need to perform some pre-processing on any image data. RGB is the most popular encoding format, and most \u201cnatural images\u201d we encounter are in RGB. Also, among the first step of data pre-processing is to make the images of the same size. Let\u2019s move on to how we can change the shape and form of images.", "The term morphological transformation refers to any modification involving the shape and form of the images. These are very often used in image analysis tasks. Although they are used with all types of images, they are especially powerful for images that are not natural (come from a source other than a picture of the real world). The typical transformations are erosion, dilation, opening, and closing. Let\u2019s now look at some code to implement these morphological transformations.", "One of the simpler operations where we take all the pixels whose intensities are above a certain threshold and convert them to ones; the pixels having value less than the threshold are converted to zero. This results in a binary image.", "Erosion shrinks bright regions and enlarges dark regions. Dilation on the other hand is exact opposite side \u2014 it shrinks dark regions and enlarges the bright regions.", "Opening is erosion followed by dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.", "Closing is dilation followed by erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.", "All these can be done using the skimage.morphology module. The basic idea is to have a circular disk of a certain size (3 below) move around the image and apply these transformations using it.", "Normalisation is the most crucial step in the pre-processing part. This refers to rescaling the pixel values so that they lie within a confined range. One of the reasons to do this is to help with the issue of propagating gradients. There are multiple ways to normalize images that we will be talking about.", "This brings us to the next aspect of data pre-processing \u2014 data augmentation. Many times, the quantity of data that we have is not sufficient to perform the task of classification well enough. In such cases, we perform data augmentation. As an example, if we are working with a dataset of classifying gemstones into their different types, we may not have enough number of images (since high-quality images are difficult to obtain). In this case, we can perform augmentation to increase the size of your dataset. Augmentation is often used in image-based deep learning tasks to increase the amount and variance of training data. Augmentation should only be done on the training set, never on the validation set.", "As you know that pooling increases the invariance. If a picture of a dog is in the top left corner of an image, with pooling, you would be able to recognize if the dog is in little bit left/right/up/down around the top left corner. But with training data consisting of data augmentation like flipping, rotation, cropping, translation, illumination, scaling, adding noise, etc., the model learns all these variations. This significantly boosts the accuracy of the model. So, even if the dog is there at any corner of the image, the model will be able to recognize it with high accuracy.", "There are multiple types of augmentations possible. The basic ones transform the original image using one of the following types of transformations:", "Let\u2019s now build and train the model.", "We will use \u2018ResNet\u2019 architecture in this section. Since ResNets have become quite prevalent in the industry, it is worth spending some time to understand the important elements of their architecture. Let\u2019s start with the original architecture proposed here. Also, in 2016, the ResNet team had proposed some improvements in the original architecture here. Using these modifications, they had trained nets of more than 1000 layers (e.g. ResNet-1001).", "The \u2018ResNet builder\u2019 module which is used here is basically a Python module containing all the building blocks of ResNet. We will use this module to import the variants of ResNets (ResNet-18, ResNet-34, etc.). The resnet.py module is taken from here. Its biggest upside is that the \u2018skip connections\u2019 mechanism allows very deep networks.", "Data generator supports preprocessing \u2014 it normalizes the images (dividing by 255) and crops the center (100 x 100) portion of the image.", "There was no specific reason to include 100 as the dimension but it has been chosen so that we can process all the images which are of greater than 100*100 dimension. If any dimension (height or width) of an image is less than 100 pixels, then that image is deleted automatically. You can change it to 150 or 200 according to your need.", "Let\u2019s now set up the data generator. The code below sets up a custom data generator which is slightly different than the one that comes with the keras API. The reason to use a custom generator is to be able to modify it according to the problem at hand (customizability).", "To start with, we have the training data stored in nn directories (if there are nn classes). For a given batch size, we want to generate batches of data points and feed them to the model.", "The first for loop 'globs' through each of the classes (directories). For each class, it stores the path of each image in the list paths. In training mode, it subsets paths to contain the first 80% images; in validation mode it subsets the last 20%. In the special case of an ablation experiment, it simply subsets the first ablation images of each class.", "We store the paths of all the images (of all classes) in a combined list self.list_IDs. The dictionary self.labels contains the labels (as key:value pairs of path: class_number (0/1)).", "After the loop, we call the method on_epoch_end(), which creates an array self.indexes of length self.list_IDs and shuffles them (to shuffle all the data points at the end of each epoch).", "The _getitem_ method uses the (shuffled) array self.indexes to select a batch_size number of entries (paths) from the path list self.list_IDs.", "Finally, the method __data_generation returns the batch of images as the pair X, y where X is of shape (batch_size, height, width, channels) and y is of shape (batch size, ). Note that __data_generation also does some preprocessing - it normalises the images (divides by 255) and crops the center 100 x 100 portion of the image. Thus, each image has the shape (100, 100, num_channels). If any dimension (height or width) of an image less than 100 pixels, that image is deleted.", "These refer to taking a small chunk of data and running your model on it \u2014 this helps in figuring out if the model is running at all. This is called an ablation experiment.", "The first part of building a network is to get it to run on your dataset. Let\u2019s try fitting the net on only a few images and just one epoch. Note that since ablation=100 is specified, 100 images of each class are used, so total number of batches is np.floor(200/32) = 6.", "Note that the DataGenerator class 'inherits' from the keras.utils.Sequence class, so it has all the functionalities of the base keras.utils.Sequence class (such as the model.fit_generator method).", "The next step is trying to overfit the model on the training data. Why would we want to intentionally overfit on our data? Simply put, this will tell us whether the network is capable of learning the patterns in the training set. This tells you whether the model is behaving as expected or not.", "We\u2019ll use ablation=100 (i.e. training on 100 images of each class), so it is still a very small dataset, and we will use 20 epochs. In each epoch, 200/32=6 batches will be used.", "The results show that the training accuracy increases consistently with each epoch. The validation accuracy also increases and then plateaus out \u2014 this is a sign of \u2018good fit\u2019, i.e. we know that the model is at least able to learn from a small dataset, so we can hope that it will be able to learn from the entire set as well.", "To summarise, a good test of any model is to check whether it can overfit on the training data (i.e. the training loss consistently reduces along epochs). This technique is especially useful in deep learning because most deep learning models are trained on large datasets, and if they are unable to overfit a small version, then they are unlikely to learn from the larger version.", "we trained the model on a small chunk of the dataset and confirmed that the model can learn from the dataset (indicated by overfitting). After fixing the model and data augmentation, we now need to find the learning rate for the optimizer (here SGD). First, let\u2019s make a list of the hyper-parameters we want to tune:", "The basic idea is to track the validation loss with increasing epochs for various values of a hyperparameter.", "Before you move ahead, let\u2019s discuss a bit about callbacks. Callbacks are basically actions that you want to perform at specific instances of training. For example, we want to perform the action of storing the loss at the end of every epoch (the instance here is the end of an epoch).", "Formally, a callback is simply a function (if you want to perform a single action), or a list of functions (if you want to perform multiple actions), which are to be executed at specific events (end of an epoch, start of every batch, when the accuracy plateaus out, etc.). Keras provides some very useful callback functionalities through the class keras.callbacks.Callback.", "Keras has many builtin callbacks (listed here). The generic way to create a custom callback in keras is:", "In the code above, we have created a custom callback to append the loss to a list at the end of every epoch. Note that logs is an attribute (a dictionary) of keras.callbacks.Callback, and we are using it to get the value of the key 'loss'. Some other keys of this dict are acc, val_loss etc.", "To tell the model that we want to use a callback, we create an object of LossHistory called history and pass it to model.fit_generator using callbacks=[history]. In this case, we only have one callback history, though you can pass multiple callback objects through this list (an example of multiple callbacks is in the section below - see the code block of DecayLR()).", "Here, we tuned the learning rate hyperparameter and observed that a rate of 0.1 is the optimal learning rate when compared to 0.01 and 0.001. However, using such a high learning rate for the entire training process is not a good idea since the loss may start to oscillate around the minima later. So, at the start of the training, we use a high learning rate for the model to learn fast, but as we train further and proceed towards the minima, we decrease the learning rate gradually.", "The results above show that a learning rate of 0.1 is the best, though using such a high learning rate for the entire training is usually not a good idea. Thus, we should use learning rate decay \u2014 starting from a high learning rate and decaying it with every epoch.", "We use another custom callback (DecayLR) to decay the learning rate at the end of every epoch. The decay rate is specified as 0.5 ^ epoch. Also, note that this time we are telling the model to use two callbacks (passed as a list callbacks=[history, decay] to model.fit_generator).", "Although we have used our own custom decay implementation here, you can use the ones built into keras optimisers (using the decay argument).", "Let\u2019s now write some code to implement data augmentation. Augmentation is usually done with data generators, i.e. the augmented data is generated batch-wise, on the fly. You can either use the built-in keras ImageDataGenerator or write your own data generator (for some custom features etc if you want). The below code show how to implement these.", "Depending on the situation, we choose the appropriate metrics. For binary classification problems, AUC is usually the best metric.", "AUC is often a better metric than accuracy. So instead of optimising for accuracy, let\u2019s monitor AUC and choose the best model based on AUC on validation data. We\u2019ll use the callbacks on_train_begin and on_epoch_end to initialize (at the start of each epoch) and store the AUC (at the end of epoch).", "Let\u2019s now train the final model. Note that we will keep saving the best model\u2019s weights at models/best_models.hdf5, so you will need to create a directory models. Note that model weights are usually saved in hdf5 files.", "Saving the best model is done using the callback functionality that comes with ModelCheckpoint. We basically specify the filepath where the model weights are to be saved, monitor='val_auc' specifies that you are choosing the best model based on validation accuracy, save_best_only=True saves only the best weights, and mode='max' specifies that the validation accuracy is to be maximized.", "Hey, we have got a very high probability for class 1 i.e. rose. If you remember, class 0 was daisy and class 1 was rose(on top of the blog). So, the model has learnt perfectly. We have built a model with a good AUC at the end of 3 epochs. If you train this using more epochs, you should be able to reach a better AUC value.", "If you have any questions, recommendations or critiques, I can be reached via LinkedIn or the comment section.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI & ML Enthusiast | Reshaping with technology"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F15624ef3a28b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tanyadayanand?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tanyadayanand?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "Tanya"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff678f7de95fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&user=Tanya&userId=f678f7de95fd&source=post_page-f678f7de95fd----15624ef3a28b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F15624ef3a28b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F15624ef3a28b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral", "anchor_text": "Alina Grubnyak"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/alxmamaev/flowers-recognition", "anchor_text": "flowers dataset"}, {"url": "https://www.kaggle.com/tanyadayanand/end-to-end-pipeline-for-training-cnns-resnet", "anchor_text": "notebook"}, {"url": "https://www.kaggle.com/tanyadayanand/end-to-end-pipeline-for-training-cnns-resnet", "anchor_text": "End-to-end pipeline for training CNNs, ResNetExplore and run machine learning code with Kaggle Notebooks | Using data from multiple data sourceswww.kaggle.com"}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "proposed here"}, {"url": "https://arxiv.org/pdf/1603.05027.pdf", "anchor_text": "here"}, {"url": "https://github.com/KaimingHe/resnet-1k-layers", "anchor_text": "ResNet-1001"}, {"url": "https://www.kaggle.com/tanyadayanand/resnet", "anchor_text": "here"}, {"url": "https://keras.io/preprocessing/image/", "anchor_text": "the one that comes with the keras API"}, {"url": "https://keras.io/callbacks/", "anchor_text": "listed here"}, {"url": "https://keras.io/optimizers/", "anchor_text": "keras optimisers"}, {"url": "http://www.linkedin.com/in/tanyadayanand", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----15624ef3a28b---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/tag/keras?source=post_page-----15624ef3a28b---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/resnet?source=post_page-----15624ef3a28b---------------resnet-----------------", "anchor_text": "Resnet"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----15624ef3a28b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----15624ef3a28b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F15624ef3a28b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&user=Tanya&userId=f678f7de95fd&source=-----15624ef3a28b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F15624ef3a28b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&user=Tanya&userId=f678f7de95fd&source=-----15624ef3a28b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F15624ef3a28b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F15624ef3a28b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----15624ef3a28b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----15624ef3a28b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----15624ef3a28b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----15624ef3a28b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tanyadayanand?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tanyadayanand?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tanya"}, {"url": "https://medium.com/@tanyadayanand/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "68 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff678f7de95fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&user=Tanya&userId=f678f7de95fd&source=post_page-f678f7de95fd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc91183103b2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-network-building-in-cnn-15624ef3a28b&newsletterV3=f678f7de95fd&newsletterV3Id=c91183103b2b&user=Tanya&userId=f678f7de95fd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}