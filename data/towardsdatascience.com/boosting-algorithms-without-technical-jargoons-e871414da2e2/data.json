{"url": "https://towardsdatascience.com/boosting-algorithms-without-technical-jargoons-e871414da2e2", "time": 1683008200.922731, "path": "towardsdatascience.com/boosting-algorithms-without-technical-jargoons-e871414da2e2/", "webpage": {"metadata": {"title": "Boosting Algorithms without technical jargon | Medium | Towards Data Science", "h1": "Boosting Algorithms without technical jargon", "description": "In my previous post (see below), I used an analogy of people voting to show the difference between a weighted Random Forest and boosting algorithms. To recap, the rule of a weighted Random Forest is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=LsK-xG1cLYA&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=45&t=0s", "anchor_text": "this video", "paragraph_index": 8}], "all_paragraphs": ["In my previous post (see below), I used an analogy of people voting to show the difference between a weighted Random Forest and boosting algorithms. To recap, the rule of a weighted Random Forest is, everyone who voted result A will be counted more. The rule of boosting algorithms is, whoever that\u2019s more qualified can cast more votes. And if result A won more votes then A it is.", "This piece will introduce how each boosting algorithm decides the amounts of votes each tree has. By understanding how the algorithms work, we will understand the difference in performance shown in my last post. And know better when to use which tool.", "There are three family members in the boosting family. They are Adaptive Boosting, Gradient Boosting, and XG Boosting. It will be discussed sequentially.", "There is only one ingredient that Adaboost uses to decide the say of a tree \u2014 its accuracy. However, there is a sneaky twist. Each tree was being trained by a slightly different set of samples. How so?", "Imagine you are preparing for your SAT exam and there are four sections: Reading, Writing, Math 1 (w/ a calculator), Math 2 (no calculator). For simplicity of this example, let\u2019s say each section has 15 questions to answer, 60 questions in total. You did a practice exam and you realized you score the lowest in Math 1. So the next time, you practice more Math 1 questions and fewer questions in other sections. Thus, you still practice 60 questions every time but with a slightly different combination.", "The same in Adaboost. One special thing about Adaboost is that it builds a forest with \u201cstump\u201d, a tree with one-level depth. I will discuss more on this later. For now, treat it as a constrain. Let\u2019s solve how it decides the say of a tree/stump first.", "Assume we want to build a model to predict if customers will buy our new product. And we have 6 sample customers. We have the data from Amy, Tom, John, Tim, Lucy, and Mike (See image 1 below). Here are the steps to build a forest:", "2. Stump 1 has 83% accuracy and Tim was being classified incorrectly. So, I will raise the weight of sample Tim to 0.3 and lower the weights of other samples to 0.14 so that they add up to 1.", "Wait\u2026 but what does it mean by \u201craising the weight of a sample\u201d? Great question but is out of the scope of my post. You can check out this video to see how it is done. In short, there are two ways. You can create a new sample set based on the weights or you can use a weighted GINI Index. However, intuition is the same as the SAT analogy I gave above.", "3. I build stump 2 using the new weighted sample and the accuracy is 87%. John was misclassified. Because the weights of correctly classify each sample are now slightly different, stump 2 has 87% accuracy (note: It\u2019s not the precise calculation).", "4. Next time, I will raise the weight of John and lower the weight of other samples. If we do not set the maximum trees that we want, then this process will repeat until we have 100% accuracy.", "Assume I set the limit to 3 stumps. Like I have mentioned before, how many votes each stump can have solely depended on their model accuracy. And it uses the following formula to decide its final say.", "Everything inside the bracket reflects how likely the model will give the right prediction. For example, for stump 1, it is 5 times more likely to give the right predictions than wrong predictions.", "The magic is the log part. Because of the property of logarithm, we give infinite positive power if our model has 0 errors (denominator goes to 0). If we have many errors, we will give a large negative number by logging it (try to put 99% as the error rate).", "5. After we know how much say each stump will have, we simply add up their votes. And the class with more votes will win.", "Let\u2019s take a step back and look at the whole picture. The key idea here is to highlight the samples that are difficult to predict, so that we can build a model which tailored toward those samples.", "When we build a tree with more than 1-level, the prediction also depends on how we build the tree. We can arrive at a different prediction by using predictors in different orders. This dilutes our purpose. You can read more about how to build a tree from here:", "This is also why boosting algorithms give a more robust analysis than Random Forest and Decision Tree for imbalanced datasets. Boosting Algorithms incorporates models that are better at predicting the minority class.", "The problem with Ada boost is that every time you build a stump, you need to rebuild the whole sample set again! Imagine you have thousands of samples, no even the computer wants to do the work. That\u2019s why Gradientboost was introduced!", "Gradientboost solved this problem by NOT using the samples to build trees. It uses the difference between predictions and the actual results, aka the residuals to build trees. Image 6 shows you what\u2019s a residual tree if you are confused like I was too.", "Image 5 shows the process in a nutshell. Based on the sample set, we will first give a log(odds) prediction, meaning, how likely will our model predict one class over another. Then we translate log(odds) into probability, and we calculate the residual. From here, we will start to build a forest so that each tree minimizes the residuals. Then we add everything with the initial prediction all at once. At last, we have a new probability!", "If you have no idea what is log(odds) and why we use it, study Logistic Regression first. You can learn more about Logistic Regression by reading the article below.", "Here is an example to visualize the process so that we can better understand it. We have a classification problem and thus, our prediction can either be 0 or 1. In this example, we have four 1 and two 0. Thus, our log(odds) is 0.69. After we translate it to probability, it would be 0.67. The residual for Amy would be 1\u20130.67 and the residual for Tom will be 0\u20130.67. On the right side, I compared a regular tree and a residual tree.", "In a regular tree, the leaves give us a final class prediction, for example, red or green. In a residual tree, the leaves give us the residuals. For example, from left to right, the first leaf (-0.7) is -0.7 (duh\u2026). The second leaf (0.3, -0.7) is -0.4 and the last leaf is 0.9.", "The intuition here is: if the leaf sums up to a large negative or large positive, it grouped the right samples together. On the other hand, if they are not the same thing, values cancel each other out and thus, close to 0.", "A tree\u2019s value is the sum of all its leaves. After we built all the trees, we sum up all the tree values and add them to the initial log prediction. Therefore, if a tree has a bigger value (better at splitting the groups), it will have more influence on how the initial prediction should change.", "Each tree\u2019s value will times 0.1. This value is called the learning rate, you can arbitrarily pick any value but 0.1 is the conversion. It is small so that it makes sure each tree is only changing the initial value slightly.", "One last thing I want to say about Gradientboost is that it actually uses a tree instead of a stump. But normally we limit the max_depth from 6 to 8 to avoid overfitting. Gradientboost doesn\u2019t use a stump because it is not using a tree to detect the difficult samples. It is building trees to minimize the residuals.", "When we calculate the value of a leaf, we actually use the following formula instead of simply summing the residuals up. And I want to share some intuition on how we can understand this formula. The actual math of this formula is super nasty. It involves 2nd order derivative. So let\u2019s not go there.", "In the above formula, the P in the denominator is the probability that our last tree gave based on its total log(odds). I listed four scenarios below so that we can see what this formula is implying.", "From the denominator, if the previous tree is very confident about its prediction, the next tree will have a larger value (a bigger say). From the numerator, if the current tree\u2019s residual is large, then it will also result in a larger value. This is just like what we have already discussed. This also how the last tree\u2019s accuracy impacts the next tree\u2019s say in the forest.", "XGboost is specifically designed for huge datasets because it is super fast. It used a lot of optimization and regularization techniques that are beyond the scope of what I want to talk about.", "I do want to highlight one key difference between XGboost and Gradientboost. In Gradientboost, after we calculate the residual for each sample, we picked a node to split and kept building the tree using the traditional method. This process can be very costly when facing large datasets.", "Thus, XGboost pushes one step further. It didn\u2019t use predicators as tree nodes. It builds trees to group the residuals together. Like I mentioned before, similar samples will have a similar residual value. So the tree nodes are values that can separate the residuals. So the leaves in XGboost are residuals and the tree nodes in XGboost are the values that can group the residuals!", "The speed of XGboost made it really feasible for huge datasets. However, when we have a reasonable amount of samples, like a couple of thousands, Gradientboost is actually more robust.", "I hope this shows why Gradientboost was the most robust algorithm when I was doing the churn analysis. There were only 3333 samples. You can see the full analysis by clicking the article here:", "Videos and articles that I linked in the post:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Share what I learned, and learn from what I shared. All about machines, humans, and the links between them. Take everything with a grain of salt."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe871414da2e2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e871414da2e2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e871414da2e2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://linnndachen.medium.com/?source=post_page-----e871414da2e2--------------------------------", "anchor_text": ""}, {"url": "https://linnndachen.medium.com/?source=post_page-----e871414da2e2--------------------------------", "anchor_text": "Linda Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0d085971d50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&user=Linda+Chen&userId=b0d085971d50&source=post_page-b0d085971d50----e871414da2e2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe871414da2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe871414da2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.wasserstrom.com/blog/2019/06/12/what-is-the-impossible-burger/", "anchor_text": "Wasserstrom"}, {"url": "https://towardsdatascience.com/from-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc", "anchor_text": "From imbalanced datasets to boosting algorithmsImbalanced dataset full tool kittowardsdatascience.com"}, {"url": "https://medium.com/swlh/programming-journal-2-what-is-and-why-random-forest-without-technical-jargon-c5a9a69d8739", "anchor_text": "What is and why Random Forest without technical jargonUnderstand Random Forest using business language and in the context of businessmedium.com"}, {"url": "https://www.youtube.com/watch?v=LsK-xG1cLYA&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=45&t=0s", "anchor_text": "this video"}, {"url": "http://GIPHY", "anchor_text": "GIPHY"}, {"url": "https://medium.com/swlh/programming-journal-2-what-is-and-why-random-forest-without-technical-jargon-c5a9a69d8739", "anchor_text": "What is and why Random Forest without technical jargonUnderstand Random Forest using business language and in the context of businessmedium.com"}, {"url": "http://GIPHY", "anchor_text": "GIPHY"}, {"url": "https://towardsdatascience.com/programming-journal-3-logistic-regression-cbf68f01bf7d", "anchor_text": "Logistic Regression without technical jargon + comparison to tree modelsIf you like pizza, you will understand the difference between logistic regression and random forest!towardsdatascience.com"}, {"url": "https://media.giphy.com/media/e3zlkCqYQBgzK/giphy.gif", "anchor_text": "GIPHY"}, {"url": "https://towardsdatascience.com/from-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc", "anchor_text": "From imbalanced datasets to boosting algorithmsImbalanced dataset full tool kittowardsdatascience.com"}, {"url": "https://media.giphy.com/media/14iPmDWiwIkyk0/giphy.gif", "anchor_text": "GIPHY"}, {"url": "https://towardsdatascience.com/programming-journal-3-logistic-regression-cbf68f01bf7d", "anchor_text": "Logistic Regression without technical jargon + comparison to tree models"}, {"url": "https://towardsdatascience.com/from-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc", "anchor_text": "From imbalanced datasets to boosting algorithms"}, {"url": "https://medium.com/swlh/programming-journal-2-what-is-and-why-random-forest-without-technical-jargon-c5a9a69d8739?source=friends_link&sk=f569648b9b48c1b93bcc9a6eb0dc6140", "anchor_text": "What is and why Random Forest without technical jargons"}, {"url": "https://www.youtube.com/watch?v=jxuNLH5dXCs&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=47", "anchor_text": "Gradient Boost Part 3: Classification"}, {"url": "https://www.youtube.com/watch?v=StWY5QWMXCw&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=48", "anchor_text": "Gradient Boost Part 4: Classification Details"}, {"url": "https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725", "anchor_text": "Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)- Step by Step Explained"}, {"url": "https://www.youtube.com/watch?v=8b1JEDvenQU&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=54", "anchor_text": "XGBoost Part 2: XGBoost Trees For Classification"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----e871414da2e2---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----e871414da2e2---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e871414da2e2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----e871414da2e2---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/programming?source=post_page-----e871414da2e2---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe871414da2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&user=Linda+Chen&userId=b0d085971d50&source=-----e871414da2e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe871414da2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&user=Linda+Chen&userId=b0d085971d50&source=-----e871414da2e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe871414da2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e871414da2e2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe871414da2e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e871414da2e2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e871414da2e2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e871414da2e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e871414da2e2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e871414da2e2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e871414da2e2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e871414da2e2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e871414da2e2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e871414da2e2--------------------------------", "anchor_text": ""}, {"url": "https://linnndachen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://linnndachen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Linda Chen"}, {"url": "https://linnndachen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "477 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0d085971d50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&user=Linda+Chen&userId=b0d085971d50&source=post_page-b0d085971d50--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe27496a5a656&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-algorithms-without-technical-jargoons-e871414da2e2&newsletterV3=b0d085971d50&newsletterV3Id=e27496a5a656&user=Linda+Chen&userId=b0d085971d50&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}