{"url": "https://towardsdatascience.com/the-ethics-of-ai-a480544c7c7a", "time": 1683016020.3524742, "path": "towardsdatascience.com/the-ethics-of-ai-a480544c7c7a/", "webpage": {"metadata": {"title": "The ethics of AI. Annette Zimmermann on the TDS podcast | by Jeremie Harris | Towards Data Science", "h1": "The ethics of AI", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "https://carrcenter.hks.harvard.edu/", "anchor_text": "Harvard University", "paragraph_index": 4}, {"url": "https://www.york.ac.uk/philosophy/staff/annette-zimmermann/#profile-content", "anchor_text": "University of York", "paragraph_index": 4}, {"url": "https://twitter.com/DrZimmermann", "anchor_text": "follow Annette on Twitter here", "paragraph_index": 6}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 6}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 115}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:", "As AI systems have become more powerful, they\u2019ve been deployed to tackle an increasing number of problems.", "Take computer vision. Less than a decade ago, one of the most advanced applications of computer vision algorithms was to classify hand-written digits on mail. And yet today, computer vision is being applied to everything from self-driving cars to facial recognition and cancer diagnostics.", "Practically useful AI systems have now firmly moved from \u201cwhat if?\u201d territory to \u201cwhat now?\u201d territory. And as more and more of our lives are run by algorithms, an increasing number of researchers from domains outside computer science and engineering are starting to take notice. Most notably among these are philosophers, many of whom are concerned about the ethical implications of outsourcing our decision-making to machines whose reasoning we often can\u2019t understand or even interpret.", "One of the most important voices in the world of AI ethics has been that of Dr Annette Zimmermann, a Technology & Human Rights Fellow at the Carr Center for Human Rights Policy at Harvard University, and a Lecturer in Philosophy at the University of York. Annette is has focused a lot of her work on exploring the overlap between algorithms, society and governance, and I had the chance to sit down with her to discuss her views on bias in machine learning, algorithmic fairness, and the big picture of AI ethics.", "Here were some of my favourite take-homes from our conversation:", "You can follow Annette on Twitter here, and you can follow me on Twitter here.", "Jeremie Harris (00:00):Hey, everyone. Jeremie here. As you know, I am the host of the podcast and a member of the team at the SharpestMinds Data Science Mentorship program. I\u2019m really excited about today\u2019s episode of the podcast because it\u2019s our first chance really to dive deep into the issue of AI ethics. AI ethics obviously is something that is in the air today. A lot of people are talking about it, certainly a lot more people than used to. It\u2019s increasingly important as machines start to take over really more and more of our collective cognition as a species. We\u2019re outsourcing more and more of our thinking to machines. And as we do that, it\u2019s becoming important for us to start reflecting on how we do that and whether or not we should be doing that in certain areas.", "Jeremie Harris (00:38):The field of AI ethics emerged around questions like that. And one of the dominant voices in that field has been that of Dr. Annette Zimmermann, who is, I\u2019m very excited to say, our guest for today\u2019s episode of the podcast. Now technically, Annette calls herself a political philosopher, but I actually think that title sort of hides some of the intricacies of her thinking. She\u2019s, in reality, one part an ethicist, one part a philosopher and then one part a technologist. I mean, she actually has to know an awful lot about engineering, data science and artificial intelligence in terms of how systems are deployed and how they\u2019re built in order to be able to offer up her opinion and her insights on the field at large.", "Jeremie Harris (01:16):AI ethics really is this marriage of different subspecialties and she\u2019s really got them all on lock. You\u2019ll see that on full display during the conversation. I should also mention that Annette has an awful lot of experience talking about and thinking about AI ethics and related issues. She\u2019s currently a Technology and Human Rights Fellow at the Carr Center for Human Rights at Harvard University. In addition to that, she\u2019s also a lecturer at York University. She\u2019s really seen the stuff at the forefront of academe. She\u2019s also done applied research and written a whole bunch about this topic in a whole bunch of different popular publications. You can definitely check those out. We\u2019ll be linking those in the description of the video as well as in the blog post that will come with the podcast.", "Jeremie Harris (01:53):I hope you enjoy listening to the episode as much as I did recording it. This is definitely one of those episodes that I\u2019d love to do more of in the future. We\u2019ll be having more people on to discuss things like AI bias and AI ethics for sure going forward. In the meantime, enjoy the show.", "Jeremie Harris (02:06):Hi, Annette. Thanks so much for joining us for the podcast.", "Dr. Annette Zimmermann (02:10):Hi, Jeremie. Thanks so much for having me on.", "Jeremie Harris (02:12):I\u2019m really excited to have you on. I think it\u2019s fair to say you\u2019re one of the guests I\u2019ve been most excited to talk to because of the breadth of your interest and just how focused you are on this idea of AI ethics. I think AI ethics is obviously getting more and more important. I think it\u2019s already incredibly important, but I also don\u2019t think that many people, myself included, have a complete sense of what AI ethics really is. So maybe that\u2019s a good place to start. What is AI ethics?", "Dr. Annette Zimmermann (02:36):I think you\u2019re absolutely right to diagnose a kind of state of confusion and public debate around this topic at the moment. I think a lot of people disagree about what AI ethics really is and whether it\u2019s useful in any way. From where I stand, that\u2019s the kind of perspective of a trained philosopher, I would view ethics as the kind of discipline that tries to identify how society should work. So how should we distribute power? How should we arrange our social and political institutions? How ought we to act when we engage with other people? What do we owe to other people as a matter of morality?", "Dr. Annette Zimmermann (03:18):Now of course, that\u2019s a very particularly way of framing ethics. There are a lot of other people who do AI ethics but who basically view ethics as something like a legal compliance framework or something like a corporate aspiration, a corporate articulation of values. So a lot of big tech companies are putting out AI ethics principles at the moment and very often they also take the form of a statement that says, you ought to do X, Y, Z. So for instance, a do no harm. That\u2019s relevantly similar to what a philosopher might say, but it\u2019s often much less focused on a kind of noncommercial motive. Obviously, big tech has a commercial incentive to articulate these principles.", "Dr. Annette Zimmermann (04:09):I do think that very often they are articulated in good faith, but I think we need to think critically about them because very often they\u2019re nonbinding principles. So it\u2019s very easy to put out a kind of voluntary statement of what we would like to do ideally. But if there\u2019s no accountability mechanism in place and if you\u2019re the one who can set the agenda because you have a lot of computational power behind you and a lot of resources, then the question is well, what does AI ethics really amount to? I think that really explains why a lot of people are quite skeptical of AI ethics because it just seems really toothless and kind of voluntaristic.", "Dr. Annette Zimmermann (04:48):Just to add to that, I think that skepticism is right, but I don\u2019t think we should therefore conclude that we should throw AI ethics out of the window. I do think that AI ethics properly construed can be a good tool to reason our way through what we ought to do not only as members of big tech corporations but also as members of society at large. So how should I as an ordinary citizen think about the ways in which we use AI and ML in really high stakes decisions in many different domains given that AI is now replacing a lot of human decision makers in our public institutions, for instance. So thinking about those questions morally and politically, which is what AI ethics is good for and is supposed to do, that I think is really valuable. So we shouldn\u2019t ditch that effort.", "Jeremie Harris (05:39):I find that fascinating partly because of what it revealed about my priors. When I think about AI ethics, I think I\u2019m joined here by probably the majority of people. I do think of these, like you said, these big ethics statements put out by Google and Facebook and so on. And I guess to some degree that also reflects the asymmetry in terms of both knowledge and resources that these companies have to even experiment with these technologies. I mean it feels like there\u2019s a sense in which governments are playing catch up on a regular basis and-", "Jeremie Harris (06:08):\u2026 all the cutting edge technologies being developed in house at big tech firms, which therefore can lay claim to being at the edge of not just the technology but also the ethics. Is there something in your research focus or some thinking you\u2019ve done around the question of how do we bring social decision making at a broader level, a higher level of abstraction maybe, that isn\u2019t confined to just companies and enterprises? How do we get that into the game if the resources are a constraint and the knowledge is too?", "Dr. Annette Zimmermann (06:39):Yeah. I think you\u2019re absolutely right to diagnose the core problem that we\u2019re facing at the moment. Right now there aren\u2019t really very much incentives for really talented ML programmers to work in the public sector or to stay in research. Obviously, there are a lot of incentives in favor of going into the corporate domain, and so for that reason it is true that a lot of talent is in private industry at the moment. The question is should that mean that big tech corporations get to set the ethical agenda on top of setting the technological agenda?", "Dr. Annette Zimmermann (07:13):Agenda setting, of course, is a kind of key democratic problem, right? So democratic decision making isn\u2019t only about making actual choices. It\u2019s also about deciding together what we should be thinking about in the first place. So which kinds of solutions are we even considering in our decision making? That\u2019s where I think decision making on AI should and could feasibly trickle down into normal democratic discourse.", "Dr. Annette Zimmermann (07:42):Of course, a lot of people in government don\u2019t understand much about emerging technologies, and I think that explains a kind of widely shared reluctance to put these kinds of AI-related issues on democratic agendas. But at the end of the day if you think about the ethical and political space of using many of these kinds of tools, ultimately they\u2019re not purely technical and you can explain them to ordinary citizens in ways that aren\u2019t reliant on having done an applied mathematics or applied statistics degree.", "Dr. Annette Zimmermann (08:16):For instance, if we think about a system like COMPAS, a criminal recidivism risk scoring tool, that obviously attracted a lot of controversy because investigative journalists at ProPublica found out it had massive racial disparities, which failed worse for Black defendants. That seems like a clear ethical and political problem. If you explain that to people, you might say, \u201cWell, one policy goal we might have in this area is to just improve this algorithm. So we could try to optimize it to get rid of this kind of racial disparity.\u201d So the hope is that will make that system more fair.", "Dr. Annette Zimmermann (08:53):That\u2019s something that you can explain to an ordinary citizen, who might then follow that up, of course, with the statement, \u201cWell, computer scientists have actually shown us that optimizing that kind of algorithm perfectly is very, very hard/impossible.\u201d A lot of computer scientists have articulated impossibility theorems that relate to optimizing for many different kinds of fairness that are all individually plausible at the same time. So we can\u2019t be perfect when we mitigate and optimize algorithmic systems.", "Jeremie Harris (09:27):Right. This has always struck me to be a feature of machine learning that starts to force us to really confront our philosophical lacuna, I mean the areas where we lack in philosophy. Machine learning in some sense really forces us to codify in equations, in really concrete terms what our moral philosophy is in a context where we really don\u2019t know. You just alluded to these no-go theorems, impossibility theorems. You can\u2019t have privacy at the same time as you have a certain level of performance or whatever, and these things are in conflict. I mean there seems to be similar issues even just with the concept of, for example, democracy where we have not just things like tyranny of the masses or whatever but other more fundamental constraints like Arrow\u2019s impossibility theorem.", "Jeremie Harris (10:13):There are no-go theorems in terms of how well you can generalize knowledge from individuals into collectives. Do you see these as being along the same continuum? Is this really one big problem, or is there a sense in which we can actually divide meaningfully between the machine learning part of the problem and then the social part of the problem?", "Dr. Annette Zimmermann (10:31):I think a lot of these questions are part of a structurally similar problem. So I think when people think about AI, they\u2019re really hyper-focused on identifying really unique features of AI because obviously people are really excited about AI and ML at the moment. But as you just suggested, very many collective decision making scenarios are actually structured in a similar way where it becomes very hard to optimize fully. So then the question for us as democratic citizens is well, if we know that we\u2019ll fail to some extent, how do we want to fail?", "Dr. Annette Zimmermann (11:08):Do we want that failure to disproportionately burden people who are already disadvantaged, or can we find a different way of dealing with imperfection and harm and uncertainty? I think the worst possible case, which unfortunately has been realized in many domains recently, is that in fact imperfections and harms and injustices compound for exactly those people who have already been at the receiving end of those injustices and harms. That seems fundamentally antithetical to democratic values. So if we really care about equal freedom for all, then that\u2019s the one case that we should absolutely avoid.", "Dr. Annette Zimmermann (11:49):As you say, we find that in many other domains of democratic decision making that actually don\u2019t have anything to do with AI. As soon as we\u2019re dealing with social complexity, we\u2019re going to find ourselves in that kind of problem space.", "Jeremie Harris (12:00):I guess to some degree this almost belies the fact that human beings themselves are machine learning agents in some meaningful sense. I mean we have a neural network. We have some kind of reinforcement learning circuitry. Yeah. What are some of the more, I don\u2019t want to say the more concrete side, but what are some of the strategies that you see as most promising in terms of dealing with some of these problems where we start to really entrench current norms, whatever they may be, in our algorithms?", "Dr. Annette Zimmermann (12:27):Well, there is one very concrete but very controversial strategy, which is non-deployment in targeted domains. A lot of people recently have been focused on non-deployment of facial recognition technology in particular, again because of the racial disparate impact of that technology. Actually, a lot of big tech corporations have now said, \u201cOkay. We\u2019re going to have a moratorium here. We\u2019re not going to deploy these tools for a set amount of time.\u201d So IBM, Amazon, but also many others have responded to pressure by researchers like Joy Buolamwini and others. So clearly, there\u2019s a kind of public awareness that actually limited and targeted non-deployment may be the best way to deal with these kinds of problems until we have a better technological solution to attack these problems.", "Dr. Annette Zimmermann (13:21):Now, whether there always will be a feasible technological solution, that I think really depends on the domain of deployment and really depends on our goals. Think back to that other case that I mentioned before, that COMPAS case. If you look at that case, you might think, \u201cWell, why are we predicting recidivism risk rates for people in the first place? Is that really the best way of ensuring that our criminal justice system works optimally and what would that mean?\u201d I guess another alternative we could put on our agenda or democratic decision making agenda would be to say, \u201cWell, couldn\u2019t we transform prisons? Couldn\u2019t we have institutional and social and structural and legal changes, changing sentencing guidelines, changing mandatory minimums?\u201d", "Dr. Annette Zimmermann (14:06):All of these are possible solutions. Not all of them will be algorithmic. Some of them might be, but I think we need to get a really, really clear sense of the solution space in its entirety and then contextualize AI approaches within that. That I think will help us identify where we should hold off on deploying something maybe until we\u2019ve tested technology and made it more reliable, or maybe as soon as we\u2019ve deployed other social and institutional mechanisms, then it might be safer to deploy in that domain. But I think we really need to be attuned to this interplay between the social world and the technological world because those will necessarily interact. So that should guide our decision making about things like non-deployment.", "Jeremie Harris (14:55):Yeah. And you really see that temptation to default to deploying when you have a problem, especially one that feels concrete, right? As an engineer, you can look at this problem, say, \u201cOh, there\u2019s a loss function I can define. Why don\u2019t I just train and treat this as a competition and go ahead and deploy?\u201d Whereas yeah, this idea that actually you\u2019re dealing with a very complex system. The different parts interact. You can\u2019t really solve a subproblem without invoking other problems. Yeah, it\u2019s really interesting, the non-deployment as its own option really in this whole situation.", "Dr. Annette Zimmermann (15:26):Right. I think it\u2019s really important not to fall into an optimization trap. It\u2019s often funny for philosophers to engage with tech practitioners on this issue because in philosophy there is this well-known slogan called ought implies can. So it just basically means that I can\u2019t impose a moral duty on you if it\u2019s just impossible for you to meet that duty. That would be overly demanding, and so that wouldn\u2019t be ethical. But in the tech practitioner space, a lot of people turn that on its head. So they can say, \u201cCan implies ought.\u201d So if I can do something, I should innovate. I should put it out there.", "Dr. Annette Zimmermann (16:05):As you rightly say, there is a presumption in favor of putting things out there and seeing if they work rather than thinking about, do we want this? What\u2019s our purpose with using this tool in the first place?", "Jeremie Harris (16:17):You\u2019ve spoken as well about this idea of optimization as not being intrinsically necessarily a desirable, not goal, but practice. Could you speak to that a little bit more? What\u2019s your thinking on that?", "Dr. Annette Zimmermann (16:31):Right. I think very often we view optimization as making incremental improvements, and so that\u2019s a really reasonable ad hoc view, right? So if you think about how can I improve my daily habits, well, I\u2019ll do five minutes of this and maybe I\u2019ll establish a routine of exercising daily or something. Over time, I\u2019ll make these incremental improvements. In life in general, I think that\u2019s a good principle. The problem is that very often in a machine learning space when we make incremental improvements without asking, should we be optimizing in that direction in the first place, it can actually drive us further away from justice.", "Dr. Annette Zimmermann (17:13):Just to give you a very concrete example, think back to that facial recognition case that we just discussed. Before companies put out these non-deployment moratoriums, they actually had a different approach. They said, \u201cWell, we\u2019re just going to improve this technology.\u201d How do you do that? Well, you need more data. And in particular, in order to tackle the racial injustice problem that came to the forefront with these technologies, people in these corporations said, \u201cWell, we need more data about black and brown faces.\u201d Unfortunately, very often that process of gaining more data is very exploitative.", "Dr. Annette Zimmermann (17:51):So a Google subcontractor, for instance, went to the streets in LA and said to homeless people, \u201cYou know, let\u2019s play a game. We have the selfie game. We\u2019ll just record your face and you get a five dollar voucher.\u201d They didn\u2019t tell these people that their faces were being used as biometric data for facial recognition tools. So the community that is already quite vulnerable and probably will be at the receiving end of further injustices once this tool gets scaled up, that\u2019s exactly the community that was being used to optimize.", "Dr. Annette Zimmermann (18:26):I think that\u2019s a really good indicator that our effort of optimization puts us on a path that isn\u2019t really oriented towards greater justice. Because if the people who have to pay for that optimization or who have to provide some sort of services for optimization, if they are not actually taking ownership of that process and if they\u2019re not being informed what is happening, then I think we\u2019re really on the wrong track. That\u2019s a way in which incremental improvements actually end up burdening people further.", "Jeremie Harris (18:59):I think this is one of the most fascinating aspects of this whole conversation is this debate versus incrementalism versus some sort of almost first principles rethinking of the entire approach that we\u2019re taking social structure and so on. To some degree, I mean, the political philosopher in me, this makes me think of the distinction between the two famous American political philosophers. There\u2019s Edmund Burke and there\u2019s Thomas Paine. Burke is this classical conservative who says, \u201cIncrementalism is really the solution. We have decent structures that have evolved through time through a combination of evolution and economics. We\u2019re now in a good place and we should respect that and not risk it.\u201d", "Jeremie Harris (19:37):Whereas Thomas Paine says, \u201cWell, we\u2026 \u201c I think his words were like, \u201cWe have it in our power to begin the world anew,\u201d or something like that, really rethinking the entire structure from first principles. Maybe Burke thinking a little bit more like an optimization engine. Thomas Paine thinking more like a physicist, come up with the equation from scratch. Let\u2019s redo this and make a beautiful system. Is there the possibility to take that Burkeian position, take that gradualist position one step further and say, look, so if they go out to the homeless people and they do this, obviously that\u2019s a huge mistake.", "Jeremie Harris (20:08):What if they then take that information, iterate, try again? Is it possible that we just haven\u2019t tried that long enough? Is that a plausible sort of counter-position to this idea?", "Dr. Annette Zimmermann (20:19):I think it again really depends on the domains. I\u2019m a big fan of iterative decision making and critique approaches. I think very often that is really, really helpful and needed mainly because a lot of technological problems actually only surface over time. So in many different domains I can start out with a machine learning system that actually exhibits no bias, that is just completely unobjectionable on the bias front, but tiny incremental changes compound over time and end up still giving us massive disparities.", "Dr. Annette Zimmermann (20:55):We know a similar phenomenon from human decision making, actually. Think about the sociological phenomenon of belief polarization. You can give two people exactly the same amount of data, same amount of evidence, and depending on what their priors are, they\u2019re going to end up with really dramatically different views. It\u2019s kind of similar with machine learning systems. Contingencies in the social world, just random happenstances may affect a path dependency in a system that isn\u2019t entirely foreseeable and that we have to mitigate sequentially. That\u2019s why sequential intervention can be a really, really helpful tool for improving systems in a dynamic way. That would then again bolster the case against a pure first principles approach.", "Dr. Annette Zimmermann (21:45):That being said, I think we do need some sort of reflection on what our overarching goal should be. That can be a flexible articulation, but I think if we just say, \u201cWe\u2019re going to be gradualists all the way down,\u201d then we might lose track of why we started trying to optimize something in the first place.", "Dr. Annette Zimmermann (22:09):That kind of path dependency is something that I think we should be worried about. There needs to be an approach that reconciles both of these ends of a spectrum with each other, and I think the way to do that is to have these first principle-based conversations about what the goal definition should be but then to iterate sequentially and be ready to change our goalposts if that is actually proving necessary.", "Jeremie Harris (22:34):That\u2019s a good point. That\u2019s a conversation that\u2019s not even really happening at this point, or at least it doesn\u2019t feel like it\u2019s particularly happening outside of, I guess, pretty narrow areas like OpenAI and GovAI policy, policy think tanks that are thinking about what are these optimization processes. What are some of the big discussion points in that ecosystem as people step back and ask themselves, what should we be building? Is that something that you\u2019ve looked at?", "Dr. Annette Zimmermann (23:00):Yeah. I mean, I think a lot of people who are tech practitioners are really interested in questions of personal responsibility. One issue that I focused on recently was this Facebook employee who released the 6,000-word memo. I\u2019m not sure if you followed the story, but essentially there was a person employed by Facebook who became increasingly concerned that she had a huge amount of professional responsibility. And according to her, she would see geopolitical responses to minute decisions that she would make as part of her function at Facebook. This became so disconcerting to her that she tried to raise this within the company but was met with backlash.", "Dr. Annette Zimmermann (23:47):The problem that she seemed to face was one where she wasn\u2019t able to adapt her behavior in a way to prevent these bad outcomes, so she as an individual was unable to prevent harm, right, just because there was a surplus of power. And even if she had rejected that power, she would\u2019ve still been part of a really, really bad process, which is why she was then critiquing the company and ultimately got fired for that. That was a less encouraging example of ethical deliberation happening among practitioners, but I did find it very interesting to see somebody articulate this very specific worry that one person might have too much power, right?", "Dr. Annette Zimmermann (24:32):Because we often think about an increase in responsibility as an honor or as an opportunity to do something really good, but I think the really necessary and really useful perspective that this person was bringing into the debate was, well, is there maybe a kind of sufficient degree of responsibility that one person shouldn\u2019t exceed? Again, I think that might ultimately be a democratic concern, right?", "Dr. Annette Zimmermann (24:58):Obviously, if we elect somebody into a position of massive power, then that person has to report back to us, technically at least, if things go well and why they do certain things. So they have to put themselves through the motion of justifying to us and explaining and rationalizing and arguing, which is something that that Facebook engineer will never do with society at large. I mean even if they try to get that process going within the company, it\u2019s not going to happen on the same kind of democratic scale. So I think intuitively that worries people who want to do well with their work.", "Jeremie Harris (25:34):Right, yeah. I mean to some degree, this harks back to the conversation we were having earlier about democratic systems and their interaction with some of this technology. I guess to some degree one of the problems is that the democratic process is just really, really slow. And as the rate of technological development increases, there may not actually be enough time for meaningful feedback, democratically speaking, to circulate back to the decision makers. That tends to pile on and get away from us.", "Dr. Annette Zimmermann (26:03):Right, yeah. I do think that it\u2019s a huge obstacle for AI policy in particular that democratic institutions are for good reason designed to be robust, which slows them down. So there\u2019s a clear trade off there. On the one hand, we want solid checks and balances, but on the other hand we want responsiveness in our democratic institutions. Those can come apart exactly in these domains where we have a rapidly changing decision landscape.", "Dr. Annette Zimmermann (26:34):I would add here that democratic decision making doesn\u2019t always have to mean electoral politics that involves the entire democratic constituency. I think that\u2019s maybe the most obvious form of democratic decision making but maybe not even the most important one. A lot of people are focused on domain experts in AI, for instance. And I think the choice about what constitutes expertise in this area might be really contestable.", "Dr. Annette Zimmermann (27:05):Very often when we think about medically our innovations, we might be tempted to ask doctors, but we could also ask chronically ill people, right?", "Dr. Annette Zimmermann (27:14):It\u2019s not necessarily obvious which one of these choices might be better or if a combination might actually yield more insightful results. Similarly, when we talk about AI deployed in criminal justice and policing contexts or immigration enforcement contexts and we only have people in the room who serve on a police force, I think we\u2019re going to miss out on a really important perspective there. I think we need to rethink who can be an expert in these discussions and whose voice we are valuing when we\u2019re trying to make really fast decisions. It doesn\u2019t have to be comprehensive, but it does have to be a kind of reasoned and egalitarian thinking that helps us select these people.", "Jeremie Harris (27:59):Just in the interest of playing devil\u2019s advocate on the democracy side of things, so one thing that at least has come to mind, especially as we start to see some of these developments in terms of increasingly sophisticated language models, GPT-3 and so on, when you look at the people who are currently in the AI ethics, AI alignment especially but broadly sort of AI safety, AI policy ecosystem, I\u2019ve always been struck in my conversations with just the almost absurd level of quality of these people and their integrity and the depth of thought. In which case, to the extent that that\u2019s true, it sparks the thought in me that it may not be immediately desirable to actually open this up to the full array, let\u2019s say, to put it nicely, of human behavior and pathology that we see on full display on things like Twitter, where people get into it.", "Jeremie Harris (28:53):I mean, I can\u2019t imagine what those arguments might look like if they had to do with the performance of an algorithm that determined whether or not a person got a bank loan or something. I mean it kind of freaks me out to think about that. I don\u2019t know. Is that something that you\u2019ve been thinking about, the extent to which closed systems that are sort of high trust and high specialization might have trade offs with the democratic open systems?", "Dr. Annette Zimmermann (29:16):Yeah, absolutely. That\u2019s a really, really central problem in political philosophy and also just in moral philosophy. I think the problem that you\u2019re getting at here is justice and democracy could be orthogonal to each other, right?", "Dr. Annette Zimmermann (29:33):I could have a democratic constituency that is just really anti-egalitarian or really divided, really hostile, or just doesn\u2019t really operate in good faith with each other anymore. So that could taint democratic decision making in a way that actually doesn\u2019t care about justice and equality at all anymore and that might also not care about decision quality at all. It could just end up with really terrible decision making that isn\u2019t grounded in any sort of facts anymore, and so that could be massively unjust and massively harmful for a lot of people.", "Dr. Annette Zimmermann (30:11):I don\u2019t think we should conclude that democratizing AI will necessarily get rid of problems like algorithmic injustice. I certainly don\u2019t think that\u2019s true. I think a whole lot more is required to establish just forms of AI and machine learning or otherwise ethically defensible forms of AI and machine learning.", "Jeremie Harris (30:35):I was, sorry, just going to ask your thoughts on mutability of these moral frameworks. There\u2019s a sense in which we design algorithms, and by designing them and certainly by deploying them, we do start, as you\u2019ve said, to enshrine some of these social norms, whether we do it with the awareness that that\u2019s what\u2019s happening or just implicitly. But then over time, obviously, we want our morality to shift. I mean 50 years ago interracial marriage was controversial. Certainly, the way we treated homosexuality and so on has just been completely revolutionized.", "Jeremie Harris (31:06):The idea that today we\u2019ve landed on a set of satisfactory moral norms that\u2019s going to be static for all time is probably pretty shaky. Any ideas about how we might design systems that would have that kind of time domain flexibility?", "Dr. Annette Zimmermann (31:21):Yeah. I mean, I think the first step to doing that would be to acknowledge that any choice we make is ethically and politically shaped in some way. A lot of the time I think we want to think of ourselves as making a totally ethically neutral decision, a purely technological decision. So for instance, when we decide exactly which features to select and measure when we design a system, and so at the face of it that looks really neutral and objective because we\u2019re just trying to represent facts and we\u2019re trying to abstract from those facts in order to come up with a generalizable decision rule.", "Dr. Annette Zimmermann (31:57):At the end of the day, I can make many different choices about exactly what to look at. So depending on what sort of story I want my model to tell, I\u2019m going to get a kind of reinforcement effect from that. If I decide to measure things like arrest rates but actually I\u2019m interested in coming up with a model that tells me a story about crime, I\u2019m going to have a really imperfect story. And that story might start spinning out from reality more and more the less I acknowledge that actually I am working with an imperfect approximation in the data.", "Dr. Annette Zimmermann (32:32):The antidote I think to that is to always realize the degree of uncertainty that we\u2019re working with and the degree of choice dependency that we have in our design processes. I think we shouldn\u2019t say, \u201cHere\u2019s our static principle. Let\u2019s try to get as close to that as possible. One time we\u2019re going to measure something once, and then we\u2019re just going to see what happens.\u201d No, we have to return to that and we have to ask, hang on, did we actually measure the right thing? Did we actually come up with the right decision rule? Why did we adopt that value in the first place?", "Dr. Annette Zimmermann (33:06):It\u2019s not good to have a kind of shopping list in mind of values that we\u2019re trying to reach by making these ostensibly objective design decisions. I think we have to always be flexible, and that flexibility includes questioning our prior assumptions and our prior choices. As defensible as they might have seemed at that point, we need to be ready to completely change them if it turns out that they were deeply wrong.", "Jeremie Harris (33:32):Yeah. One of the things that looking at some of your work, your writing and your YouTube videos as well is made clear, at least to me, that I hadn\u2019t fully appreciated yet was the extent to which feature selection and feature engineering really amounts to choosing the lens through which your algorithm will see the world. We know the effects of getting somebody to read, for example, the New York Times over the course of the year versus getting somebody to read just Fox News over the course of the year. You\u2019re going to end up with two very different models of the world because the dimensionality reduction, essentially the feature selection, they have done on the world to present you with nominally the same events but from different perspectives very often just, I mean, completely changes the way you interpret things.", "Jeremie Harris (34:12):It\u2019s kind of interesting to map that onto these algorithms and say hey, if you actually just choose to feed this algorithm the equivalent of MSNBC versus CNN, you will get a different algorithm with different conclusions.", "Dr. Annette Zimmermann (34:24):Yeah. I think that\u2019s absolutely right, and I think that\u2019s a really underappreciated dimension of untrustworthiness in AI. I think when you ask somebody, what would make AI trustworthy, their gut reaction is going to be something like, \u201cWell, we want to ensure that AI doesn\u2019t make any egregious mistakes,\u201d so the kinds of mistakes that we\u2019ve already discussed. So no egregious harm, no egregious injustice. But think about the kind of counterfactual to that. Think about an AI system that actually hasn\u2019t really failed yet. It hasn\u2019t really betrayed our trust yet.", "Dr. Annette Zimmermann (35:04):We\u2019re kind of comfortable delegating to that system either because we have uncertainty whether it\u2019ll perform well or because we have some form of information about it that makes us reasonably confident that that system won\u2019t fail in certain ways. But then we still haven\u2019t excluded the possibility that the ontology of that system might just not be a faithful representation of the world as we want to represent it, right? That system might be idealizing in ways that we find alienating and weird and somehow not apt. That could be a totally different facet of untrustworthy AI that isn\u2019t actually that outcome dependent. It\u2019s a more process-based concern that we might have about AI.", "Dr. Annette Zimmermann (35:48):The reason why I say it\u2019s process based is I think we don\u2019t only care about doing the right thing. We also care about doing the right thing for the right reason. That\u2019s something that we would say about humans too. If you had a friend who routinely just does good things and hasn\u2019t given you any reason to doubt their loyalty to you, but then let\u2019s say you find out at some point that they just buy into a really, really weird reasoning structure. The whole reason why they were so nice to you as a friend was because they took pity on you. And they thought you really needed help and you can\u2019t hack it on your own, but they feel no friendship towards you. If you find that out, you would say, \u201cOh god, all of the reasons why you did all of these nice things for me were terrible reasons and they\u2019re not in the spirit of friendship.\u201d So you really missed what was going on here.", "Dr. Annette Zimmermann (36:39):I think a similar problem can arise with AI when AI is built on a weird and alienating ontology that just doesn\u2019t connect the right dots in the right way. That in my mind would make AI untrustworthy because you wouldn\u2019t have guarantees that it would continue to do right things because it doesn\u2019t do them for the right reason.", "Jeremie Harris (37:03):Because I guess there\u2019s a countervailing philosophical position that says that the purpose of a system is what it does, so if a system does nice things to you, then it\u2019s a nice system independent of what\u2019s going on between its ears. I guess what you\u2019re really getting at though as well here is that ultimately there might be emergent behaviors that might not be obvious in the short term, but then all of a sudden you realize, \u201cOh crap, this thing was really just trying to look nice to gain my trust so that it could do something absolutely devastating.\u201d Is that a fair kind of assessment?", "Dr. Annette Zimmermann (37:33):Yes. It\u2019s true that the main worry that I\u2019m getting at here is a long-term worry. I don\u2019t think we need to be necessarily assuming that AI in a really sophomore form would be malicious, so I think it\u2019s very dangerous to anthropomorphize AI. I\u2019m not even imagining a kind of malicious AGI case here. I think these kinds of long-term problems can crop up even with narrow AI applications that themselves have weird ontologies but that obviously don\u2019t pursue their own agendas because they\u2019re not conscious agents.", "Jeremie Harris (38:11):Okay, yeah. Very interesting. All right. Let me make the best steel man argument I can for some kind of objectivist framing of the AI problem. Let\u2019s say I want to get rid of the feature selection problem because I get that selecting features and engineering my own features is biased. It\u2019s going to reflect what I think is important and not what really is.", "Jeremie Harris (38:33):Can I push this to the limit of what really is? Could I develop a system that is aware of the positions and momenta of every atom in the very least planet earth or the solar system or something, and would that be more satisfactory? Would that at least get me past the hurdle of feature selection and feature engineering, or am I still missing something even by doing something like that?", "Dr. Annette Zimmermann (38:59):Well, it might get you around the problem of feature selection, but I think you would be dealing with a different problem, which philosophers would call the benevolent dictator problem. Again, this is a kind of thought experiment from political philosophy where you imagine what if you had a dictator that routinely does really amazing things, like a dictator who rules well. He makes sure that everybody\u2019s treated equally, just less equal than them. So the question is, well, what\u2019s your complaint if things go well in that way for everybody?", "Dr. Annette Zimmermann (39:35):Let\u2019s say we have no reason to believe that that dictator will ever change their ways. We don\u2019t need to worry that they\u2019ll ever turn bad. But intuitively, we\u2019re still very worried about our total lack of control. And very often I think as humans we do have that need to be able to say, \u201cWell, I have agency in this sort of interaction.\u201d So I think if we had this really, really, really, really sophisticated fine-grained AI, we might have more construct validity. So we are better approximating this problem of faithful abstraction. We\u2019re doing better on that dimension, but we\u2019re then dealing with the benevolent dictator issue. Then that might be undermining our agency in the process.", "Jeremie Harris (40:24):Right. I guess back to that idea of moral mutability as well, we want to be able to change our moral thinking and this locks us in in a certain sense.", "Dr. Annette Zimmermann (40:33):Exactly. So then there is in fact a trade off between these two different kind of meta goals that we, I think, should be pursuing in AI ethics. On the one hand, we want high construct validity, but that might again be irreconcilable with a lot of agency and sequential mitigation, so that\u2019s a major problem.", "Jeremie Harris (40:54):Actually, can you expand on that idea of construct validity? Because it was new to me before I dove into your stuff, so\u2026", "Dr. Annette Zimmermann (41:00):Right. Construct validity is more the computer science term. Philosophers would probably say faithful abstraction or faithful representation. The idea is just that when we make models and when we articulate fundamental laws and theories, we always have to abstract in some way from the social world. So a lot of people get worried about that because I think intuitively we all know as soon as you build a general theory, you\u2019re going to lose some of its applicability to individual instances of real life. But that might be okay because we want theories and models to be general. Statisticians have a neat slogan for this. Most statisticians buy into the claim all models are wrong, and that\u2019s fine. They can still be useful.", "Dr. Annette Zimmermann (41:51):My question would be, well, how do we sort out when a model isn\u2019t as useful anymore? Where\u2019s that line where we diverse so much from social reality that the model actually loses explanatory power? I would also add here that very often when people think about things like construct validity, so like getting a really fine-grained picture of the real world, people forget that looking at the real world might just reproduce injustices as well. This is another important trade off.", "Dr. Annette Zimmermann (42:25):So computer scientists have talked a lot about this. If I have a really well-calibrated algorithmic system, it\u2019ll just regurgitate exactly the kinds of social stratifications that we already have. Again, this is why our intervention is so important because we can choose which realities we want to represent, right? Which constructs do we want to abstract and idealize and thereby reproduce? Maintaining an awareness of the fact that there\u2019s often multiple stories that I can tell about a person or a group or a society and selecting the right story to tell there, I think that\u2019s really crucial.", "Dr. Annette Zimmermann (43:07):There\u2019s a kind of standard philosophical example of this from Plato. Plato was thinking a lot about idealizations and whether idealizations correspond to reality. He gives the example of a Greek statue. In ancient Greece, statues were built kind of top heavy, so with a bigger head and bigger shoulders and so on because you look at them from below. So your ordinary perspective as a spectator requires that we actually distort the statue in order for it to give us a realistic image. It\u2019s an idealization, but it\u2019s not the only idealization we could come up with because, of course, we could\u2019ve reproduced statues in exactly human proportions. But then that would\u2019ve seemed really alienating and distorting to spectators. I think that\u2019s a really good example of thinking about different ways of representing reality and the implications of making those decisions.", "Jeremie Harris (44:07):That\u2019s really interesting, and it also invites, I guess, some thoughts about the idea of free will versus determinism too. You mentioned earlier, you said, \u201cWe have the choice to decide what our abstractions are, to decide what the world looks like to our model.\u201d If these models get really good, at a certain point they\u2019ll be able, in fact, in many cases they do predict our behavior better than we could. I know I\u2019ve had Duolingo will ping me every once in a while and tell me, \u201cIt\u2019s about time for a lesson.\u201d I go, \u201cYeah, you know what? It\u2019s about time for a lesson.\u201d To some degree, I guess there\u2019s this play off\u2026 It definitely is uncomfortable when your behavior is predicted better than you might be able to.", "Jeremie Harris (44:47):Is there a way of sort of splicing reality at its joints there, sort of deciding where it makes sense to say, \u201cOkay, let\u2019s treat this system as deterministic, but let\u2019s make room for free will explicitly\u201d? Maybe you want to speak to as well, there\u2019s an example you drew on from an exam I think in the UK that was contested recently. Maybe I\u2019ll park the thought there and let you take it from there.", "Dr. Annette Zimmermann (45:11):Right. The UK case that you mention was really, really interesting. Some UK government decided because of COVID-19 that they were just going to cancel final exams for high school students. And instead, what they were going to do was just make predictions about which final grades high school students would get. In the UK, that really determines your entire life, including which university you get to go to. Their system is you apply way before you finish high school, and then you get an initial prediction about your grades from are teacher. If you meet those predictions with the final exam, then you actually get to take up your place in uni.", "Dr. Annette Zimmermann (45:52):In this case, unfortunately, the UK decided to use a really crude statistical model that predicted grades well for people who went to private schools that had a really strong historical track record of high educational achievement. We have reliable long-term data about these private schools, and so the model worked well there. But it really didn\u2019t work well for people who had a large range of different grades who were kind of middle-ranked students. It also worked terribly for people who are in schools that used to be quite low achievement schools but that have rapidly improved in the recent past. So de facto, the model ends up disadvantaging working class students, students of color.", "Dr. Annette Zimmermann (46:43):So it had these massively unjust outcomes where somebody who would\u2019ve gotten a B suddenly failed their A-levels because of the different idiosyncrasies of the model. The model didn\u2019t really take into account uncertainty enough and so really made blunt predictions where allowing for flexibility would\u2019ve been much more apt and would\u2019ve been a better representation of educational attainment. In this case it seems really misguided to use these kinds of predictions.", "Dr. Annette Zimmermann (47:13):Even if that model actually had been very accurate, which it wasn\u2019t, but assume for a moment that we could have a really good model. You might think, \u201cWell, maybe you want to have the opportunity to put yourself through the final exam process.\u201d Yes, it\u2019s going to be extremely stressful, but many students articulated this feeling of you know, I really worked hard for this, and this was my chance to actually prove to people that I can do it. I\u2019ve invested all this time and energy into it, and the whole point of it is to go through that stress and to really try my best. I mean you had kids on the street during the COVID-19 outbreak with signs saying, \u201cLet\u2019s ditch this algorithm. I want to actually do this exam.\u201d", "Dr. Annette Zimmermann (47:59):I think that really shows us that very often the mere prediction isn\u2019t what we\u2019re trying to get at. We want the experience. And whenever we want the experience and the hard human process, that\u2019s where I think we can\u2019t really replace these kinds of processes. The trouble is I think it\u2019s going to be really difficult to predict which areas of life that\u2019s going to apply to. And I also think people are going to dramatically disagree on that. So I don\u2019t think there\u2019s a really neat principled solution to this problem.", "Dr. Annette Zimmermann (48:31):I think we need a process of ongoing political arguing about that because otherwise we\u2019re going to end up imposing a policy on people that basically says, well, in this area of your life, you\u2019re allowed to have agency. And in this area of your life, we don\u2019t really care about your free will. We\u2019re just going to give you that prediction and guess what, it\u2019s pretty accurate. That would be unsatisfactory.", "Jeremie Harris (48:57):Yeah. And you can definitely understand the resentment too of even if, as you say, even if the algorithm were basically 100% accurate or had a wicked good F1 score or whatever value, if somebody tells you, \u201cHey, you know what? We ran the numbers. Based on all this data, much of which has nothing to do with features you would normally associate with yourself personally. It\u2019s features about your community, features about your family life, your upbringing and this and that.\u201d", "Jeremie Harris (49:25):Then we tell you, \u201cOkay. With 99% confidence, we can tell you, you\u2019re going to score between a B and a B+ here.\u201d That\u2019s a tough pill to swallow, in some ways even worse the more accurate the system gets. I mean you can see people getting more resentful as they\u2019re being told what their worth is to society in some sense by these systems.", "Dr. Annette Zimmermann (49:45):Yeah. Sometimes the process of quantification itself can be offensive in some ways, right? Sometimes getting ever more fine grained in our assessment of what it means to be somebody isn\u2019t actually what we want. Sometimes I think we want to take a step back and not care about certain granular details. I mean you can have marginal differences between people. Again, obviously philosophers have a concept for that because I guess we label each tiny idea that we have. In this context the concept that comes to mind for me is this notion of opacity respect.", "Dr. Annette Zimmermann (50:25):Opacity respect basically means that with some kinds of questions, we don\u2019t want to look too closely at differences between people because we just decide these differences might not be relevant for our decision about what we owe to that kind of person or how we should distribute certain benefits and burdens in society. Sometimes we don\u2019t actually need to know everything about somebody in order to treat them respectfully. So it might undermine respect if we ask too many questions. How good are you actually at this, or have you actually thought about this? These kinds of questions, especially when it comes to allocating responsibilities and rights to people can often be undermining.", "Jeremie Harris (51:08):Yeah. It really makes me think of there\u2019s a whole body, obviously, of moral philosophy around free will and this question that if you have somebody who\u2019s just an absolute psychopath and they go around murdering people, you feel entitled to be very angry with them. But then later you discover that they have a brain tumor, and that brain tumor completely explains everything they did. All of a sudden they\u2019ve done a 180 from being this just absolute monster to being someone who\u2019s really worthy of a lot of sympathy and you just feel bad for the person.", "Jeremie Harris (51:39):Sometimes uncovering more and more\u2026 I guess uncovering a cancer or something like that, a tumor, is something we\u2019d want to know about, but then there\u2019s a continuum where you start pushing that much further and you start to go, \u201cOh, okay. Well, Jeremie just said that horribly obnoxious thing because neuron number 25 is connected to neuron number 47 this way, so it\u2019s really purely deterministic. It\u2019s not his fault. It\u2019s just the way his brain is.\u201d But at a certain point these things start to get very uncomfortable.", "Dr. Annette Zimmermann (52:10):Right. I think that\u2019s right. I mean one concern is intrusiveness in this area, and intrusiveness can be disrespectful. But I also think that very often it\u2019s just unhelpful to find more information about somebody because very often we\u2019re just interested in a question of blameworthiness and responsibility. We can answer questions about responsibility and blameworthiness without having a really hyper-detailed explanation of just why this person just said this terribly obnoxious thing.", "Dr. Annette Zimmermann (52:45):So in many domains we just want to say, \u201cLook, you did this thing. We don\u2019t really care why, but can you just apologize? Did some harm and we\u2019re going to hold you accountable for that.\u201d I think that\u2019s a kind of common view that we would have about people, and I think we\u2019re going to have it in an AI context as well. Information as such doesn\u2019t really tell us much. It\u2019s more about what type of information we\u2019re working with and what we do with that.", "Jeremie Harris (53:16):As you\u2019ve pointed out too in the past, I guess doing this algorithmically also causes us to have a little too much sometimes, too much confidence in the outputs of these systems. We start to go, \u201cOh, if the algorithm says that this person\u2019s morally responsible, then no question. We don\u2019t need trial by jury,\u201d right?", "Dr. Annette Zimmermann (53:32):Yeah. I mean there\u2019s a well-studied psychological phenomenon called automation bias, and that is definitely something that we have to grapple with. We as humans just have this bias that when we\u2019re dealing with a quantified story about something, we\u2019re more likely to trust that quantified story because we think, well, as soon as something is measurable, it makes it more tangible and more objective, perhaps even impartial. Because we kind of think, \u201cWell, if it\u2019s just numbers, there can\u2019t be some agenda behind it. There can\u2019t be a bias behind it.\u201d", "Dr. Annette Zimmermann (54:07):That is dangerous thinking. I mean, obviously, we got to be open to the idea that some modes of quantifying something are indeed impartial, but it doesn\u2019t go without saying that all forms of quantification are. That\u2019s something to always have at the forefront of our minds and I think especially important for tech practitioners who have to make these daily judgment calls about how much trust they should place in a system.", "Jeremie Harris (54:33):Yeah. Yeah. It\u2019s almost like the fact that the machine learning engineer gets to\u2026 They kind of make these meta decisions, as you\u2019ve alluded to, by choosing architecture, by choosing levels of abstraction in engineering features. They make those choices. Then they move away, and that creates the illusion that no choices were ever made because the system is just sort of running on its own. Whereas if you had humans doing it, it\u2019s like no, every instance the person is re-engineering features, redeploying their judgment. Yeah, it\u2019s an interesting world we\u2019re headed to.", "Dr. Annette Zimmermann (55:03):Yes. And I think the main thing is there isn\u2019t one easy solution that applies to all forms of technology and all domains, so I think we should be extremely skeptical of blanket tech optimism. But we should be equally skeptical of blanket tech pessimism or tech hostility because I don\u2019t think that any ethical and political argument will just apply across the board to all sorts of different applications.", "Jeremie Harris (55:35):Well, unfortunately, as somebody who\u2019s incapable of holding more than one thought in my head at the same time, I\u2019m going to have to figure one out or the other but really appreciate-", "Jeremie Harris (55:44):That\u2019s it, as with all things nowadays, it seems. Thanks so much for your time, Annette. This was a great conversation. I do want to make sure if people want to follow you on various social media, what would be the best way to follow you?", "Dr. Annette Zimmermann (55:56):Best way is probably on Twitter, so that\u2019s @DrZimmermann with two N\u2019s at the end. Yeah. I\u2019m on Twitter all the time.", "Jeremie Harris (56:06):Perfect, yeah. Me too, unfortunately. A topic for another day. Great. We\u2019ll make sure we link to that. We\u2019re going to write up a blog post to come with the podcast. People can read that as well, and we\u2019ll provide a bunch of links to your work, which is very fascinating as well. Thanks so much for making the time. Really appreciate it.", "Dr. Annette Zimmermann (56:21):Thanks so much. Thanks for having me on. It was a fascinating conversation.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa480544c7c7a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/tagged/tds-podcast", "anchor_text": "TDS podcast"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----a480544c7c7a---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa480544c7c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&user=Jeremie+Harris&userId=59564831d1eb&source=-----a480544c7c7a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa480544c7c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&source=-----a480544c7c7a---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.youtube.com/watch?v=xFhycoHHTfU&feature=youtu.be", "anchor_text": "here"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify"}, {"url": "https://carrcenter.hks.harvard.edu/", "anchor_text": "Harvard University"}, {"url": "https://www.york.ac.uk/philosophy/staff/annette-zimmermann/#profile-content", "anchor_text": "University of York"}, {"url": "https://twitter.com/DrZimmermann", "anchor_text": "follow Annette on Twitter here"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://medium.com/tag/ai-ethics?source=post_page-----a480544c7c7a---------------ai_ethics-----------------", "anchor_text": "Ai Ethics"}, {"url": "https://medium.com/tag/ethics?source=post_page-----a480544c7c7a---------------ethics-----------------", "anchor_text": "Ethics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a480544c7c7a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----a480544c7c7a---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/tag/ai-alignment-and-safety?source=post_page-----a480544c7c7a---------------ai_alignment_and_safety-----------------", "anchor_text": "Ai Alignment And Safety"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa480544c7c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&user=Jeremie+Harris&userId=59564831d1eb&source=-----a480544c7c7a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa480544c7c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&user=Jeremie+Harris&userId=59564831d1eb&source=-----a480544c7c7a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa480544c7c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----a480544c7c7a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=-----a480544c7c7a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Written by Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "122K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----a480544c7c7a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ethics-of-ai-a480544c7c7a&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=-----a480544c7c7a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----a480544c7c7a----0---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----a480544c7c7a----0---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----a480544c7c7a----0---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a480544c7c7a----0---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----a480544c7c7a----0---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "The 4 fastest ways not to get hired as a data scientistAvoiding these common mistakes won\u2019t get you hired. But not avoiding them guarantees your application a one-way ticket to the \u201cno\u201d pile."}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----a480544c7c7a----0---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "6 min read\u00b7Jun 12, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F565b42bd011e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e&user=Jeremie+Harris&userId=59564831d1eb&source=-----565b42bd011e----0-----------------clap_footer----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----a480544c7c7a----0---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F565b42bd011e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e&source=-----a480544c7c7a----0-----------------bookmark_preview----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a480544c7c7a----1---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a480544c7c7a----1---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a480544c7c7a----1---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a480544c7c7a----1---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a480544c7c7a----1---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a480544c7c7a----1---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a480544c7c7a----1---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----a480544c7c7a----1-----------------bookmark_preview----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a480544c7c7a----2---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a480544c7c7a----2---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----a480544c7c7a----2---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a480544c7c7a----2---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a480544c7c7a----2---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a480544c7c7a----2---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----a480544c7c7a----2---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----a480544c7c7a----2-----------------bookmark_preview----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----a480544c7c7a----3---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----a480544c7c7a----3---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----a480544c7c7a----3---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a480544c7c7a----3---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----a480544c7c7a----3---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "Why you\u2019re not a job-ready data scientist (yet)You\u2019re getting rejected for a reason, but it\u2019s almost always something you can fix."}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----a480544c7c7a----3---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": "6 min read\u00b7Jun 16, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a0d73f15012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012&user=Jeremie+Harris&userId=59564831d1eb&source=-----1a0d73f15012----3-----------------clap_footer----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----a480544c7c7a----3---------------------85e7a445_9042_46a1_a48b_e24d4ee90a3f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "38"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a0d73f15012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012&source=-----a480544c7c7a----3-----------------bookmark_preview----85e7a445_9042_46a1_a48b_e24d4ee90a3f-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "See all from Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----a480544c7c7a----0-----------------bookmark_preview----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Babar M Bhatti"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Essential Guide to Foundation Models and Large Language ModelsThe term Foundation Model (FM) was coined by Stanford researchers to introduce a new category of ML models. They defined FMs as models\u2026"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "\u00b714 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&user=Babar+M+Bhatti&userId=10dee34829b&source=-----27dab58f7404----1-----------------clap_footer----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&source=-----a480544c7c7a----1-----------------bookmark_preview----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----a480544c7c7a----0---------------------da696c70_153e_4158_b799_88ab6dd9a712-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----a480544c7c7a----0-----------------bookmark_preview----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Aleid ter Weel"}, {"url": "https://medium.com/better-advice?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Better Advice"}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "10 Things To Do In The Evening Instead Of Watching NetflixDevice-free habits to increase your productivity and happiness."}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "\u00b75 min read\u00b7Feb 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-advice%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&user=Aleid+ter+Weel&userId=6ffe087f07e5&source=-----4e270e9dd6b9----1-----------------clap_footer----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----a480544c7c7a----1---------------------da696c70_153e_4158_b799_88ab6dd9a712-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "204"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&source=-----a480544c7c7a----1-----------------bookmark_preview----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://medium.com/the-generator/how-i-built-writinggpt-a-fully-automated-ai-writing-team-a8fdf0255586?source=read_next_recirc-----a480544c7c7a----2---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://tomsmith585.medium.com/?source=read_next_recirc-----a480544c7c7a----2---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://tomsmith585.medium.com/?source=read_next_recirc-----a480544c7c7a----2---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Thomas Smith"}, {"url": "https://medium.com/the-generator?source=read_next_recirc-----a480544c7c7a----2---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "The Generator"}, {"url": "https://medium.com/the-generator/how-i-built-writinggpt-a-fully-automated-ai-writing-team-a8fdf0255586?source=read_next_recirc-----a480544c7c7a----2---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "How I Built WritingGPT, a Fully Automated AI Writing TeamIt writes articles that rank on Google for about $1 each"}, {"url": "https://medium.com/the-generator/how-i-built-writinggpt-a-fully-automated-ai-writing-team-a8fdf0255586?source=read_next_recirc-----a480544c7c7a----2---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "\u00b714 min read\u00b7Apr 18"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fthe-generator%2Fa8fdf0255586&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fthe-generator%2Fhow-i-built-writinggpt-a-fully-automated-ai-writing-team-a8fdf0255586&user=Thomas+Smith&userId=d00bc5bb7954&source=-----a8fdf0255586----2-----------------clap_footer----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://medium.com/the-generator/how-i-built-writinggpt-a-fully-automated-ai-writing-team-a8fdf0255586?source=read_next_recirc-----a480544c7c7a----2---------------------da696c70_153e_4158_b799_88ab6dd9a712-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "87"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8fdf0255586&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fthe-generator%2Fhow-i-built-writinggpt-a-fully-automated-ai-writing-team-a8fdf0255586&source=-----a480544c7c7a----2-----------------bookmark_preview----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://kozyrkov.medium.com/how-ai-is-evolving-9638e7eae9f1?source=read_next_recirc-----a480544c7c7a----3---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://kozyrkov.medium.com/?source=read_next_recirc-----a480544c7c7a----3---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://kozyrkov.medium.com/?source=read_next_recirc-----a480544c7c7a----3---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "Cassie Kozyrkov"}, {"url": "https://kozyrkov.medium.com/how-ai-is-evolving-9638e7eae9f1?source=read_next_recirc-----a480544c7c7a----3---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "How AI is EvolvingA prediction for the next decade"}, {"url": "https://kozyrkov.medium.com/how-ai-is-evolving-9638e7eae9f1?source=read_next_recirc-----a480544c7c7a----3---------------------da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": "\u00b73 min read\u00b75 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F9638e7eae9f1&operation=register&redirect=https%3A%2F%2Fkozyrkov.medium.com%2Fhow-ai-is-evolving-9638e7eae9f1&user=Cassie+Kozyrkov&userId=2fccb851bb5e&source=-----9638e7eae9f1----3-----------------clap_footer----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://kozyrkov.medium.com/how-ai-is-evolving-9638e7eae9f1?source=read_next_recirc-----a480544c7c7a----3---------------------da696c70_153e_4158_b799_88ab6dd9a712-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "26"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9638e7eae9f1&operation=register&redirect=https%3A%2F%2Fkozyrkov.medium.com%2Fhow-ai-is-evolving-9638e7eae9f1&source=-----a480544c7c7a----3-----------------bookmark_preview----da696c70_153e_4158_b799_88ab6dd9a712-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----a480544c7c7a--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}