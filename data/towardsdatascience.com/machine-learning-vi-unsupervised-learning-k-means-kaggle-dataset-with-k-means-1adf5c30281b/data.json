{"url": "https://towardsdatascience.com/machine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b", "time": 1683011336.650878, "path": "towardsdatascience.com/machine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b/", "webpage": {"metadata": {"title": "Unsupervised Learning and Deep Dive Into K-Means | by Vardaan Bajaj | Towards Data Science", "h1": "Unsupervised Learning and Deep Dive Into K-Means", "description": "So far in the series of posts on Machine Learning, we have had a look at the most popular supervised algorithms up to this point. In the previous post, we discussed Decision Trees and Random Forest\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/analytics-vidhya/machine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be", "anchor_text": "previous", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/K-means%2B%2B", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://www.kaggle.com/arjunbhasin2013/ccdata", "anchor_text": "here", "paragraph_index": 34}, {"url": "https://www.kaggle.com/vardaanbajaj/k-means-to-recognize-credit-card-usage-patterns?scriptVersionId=39202855", "anchor_text": "here", "paragraph_index": 40}, {"url": "https://www.kaggle.com/vardaanbajaj/k-means-to-recognize-credit-card-usage-patterns?scriptVersionId=39202855", "anchor_text": "here", "paragraph_index": 46}, {"url": "https://towardsdatascience.com/deep-dive-into-principal-component-analysis-fc64347c4d20", "anchor_text": "next", "paragraph_index": 47}, {"url": "http://linkedin.com/in/vardaan-bajaj-23a279124/", "anchor_text": "linkedin.com/in/vardaan-bajaj-23a279124/", "paragraph_index": 49}], "all_paragraphs": ["In this post, we\u2019ll be going through:", "So far in the series of posts on Machine Learning, we have had a look at the most popular supervised algorithms up to this point. In the previous post, we discussed Decision Trees and Random Forest in great detail. This post and the next few posts will focus on Unsupervised Learning Algorithms, the intuition and mathematics behind them, with a solved Kaggle dataset at the end.", "Learning tasks done without supervision is unsupervised learning. Unlike supervised machine learning algorithms, there are no labels present in the training data for unsupervised learning which supervise the machine learning model\u2019s performance. But, like supervised learning algorithms, unsupervised learning is used for both, discrete and continuous data values. Formally, unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labelled responses.", "There are two types of Unsupervised Learning: discriminative models and generative models. Discriminative models are only capable of telling you, if you give it X then the consequence is Y whereas the generative model can tell you the total probability that you\u2019re going to see X and Y at the same time.", "So the difference is as follows: the discriminative model assigns labels to inputs, and has no predictive capability. If you gave it a different X that it has never seen before it can\u2019t tell what the Y is going to be because it simply hasn\u2019t learnt it yet. With a generative model, once you set it up and find the baseline you can give it any input and ask for a prediction. Thus, it has a predictive ability.", "Two common use cases of unsupervised learning are:", "(i) Cluster Analysis a.k.a. Exploratory Analysis", "Cluster analysis or clustering is the task of grouping data points in such a way that data points in a cluster are alike and are different from data points in the other clusters. Clustering has various applications such as in market segmentation, pattern recognition, word vectors, detecting cyber-attacks and many such areas.", "Principal Component Analysis is used to reduce the dimensionality (the number of features) of a dataset. In addition to reducing the dimensionality, Principal Component Analysis also converts data into a numeric form which is difficult to interpret by humans. Through Principal Component Analysis, the training time of the model can be reduced to a great extent with just a very slight decline in accuracy.", "Some common examples of unsupervised learning are K-means, Principal Component Analysis (PCA), Autoencoders, etc. In this post, we\u2019ll have a detailed look at K-means algorithm.", "Let\u2019s first get the hang of the term \u2018K-Means\u2019. In statistics, \u2018mean\u2019 refers to the average of a given set of data points. This means that the algorithm deals with \u2018K\u2019 averages. But, what does \u2018K\u2019 averages signify? We usually have just a single average representing the entire data. Having \u2018K\u2019 averages signifies that we have performed the \u2018mean\u2019 operation on \u2018K\u2019 data segments, each of which can be treated as a separate independent unit. So, having \u2018K\u2019 averages for the given data is possible only when it is divided into \u2018K\u2019 parts/groups/clusters. The term K-means in itself is sufficient to describe that it\u2019s a clustering algorithm and hence we should avoid using the term \u2018clustering\u2019 with K-means. Later we\u2019ll see that this is exactly how the mathematics behind K-means works.", "Formally, K-Means is an unsupervised learning algorithm that takes an unlabelled dataset with \u2018m\u2019 records and groups it into \u2018K\u2019 subsets/clusters, where each cluster has records having similar attributes and K < m. The algorithm however is not intelligent enough to determine the number of clusters in the data automatically and hence requires a predefined number of clusters (K) to divide the data into \u2018K\u2019 coherent groups. That being said, K-means is still one of the most widely used clustering algorithms due to its simplicity and fast computation time.", "K-means algorithm works in 2 steps.", "Both of these steps are repeated until the algorithm converges.", "To better understand the 2 steps of K-means, let\u2019s look at how K-means works through an example and the optimization objective (cost function) involved. In order to visualize things, we\u2019ll assume that the data we\u2019re using just has 2 features i.e. 2-dimensional data. Let us divide the data into 2 clusters, so K = 2.", "Given the scatter plot for raw data in fig. (a), first of all 2 random points (cluster centroids) are selected on the graph (fig. b). Once these points have been selected, the proximity of all the data points to the chosen cluster points is calculated. There are many ways to calculate proximity and one of the easiest ways is to use Euclidean distance. Once all the Euclidean distances are calculated, the data points are assigned to those cluster centroids which are nearest to them. In fig. c, data points are assigned to red/blue cluster centroids based on their proximity to the cluster centroid. This is the cluster assignment step where each data point is assigned to a cluster. But these cluster assignments are not optimal since the initial values of cluster centroids were randomly chosen. To overcome this, the centre value of all the data points belonging to a cluster is calculated and the cluster centroid is moved to this new position as represented in fig. d. This is the move centroid step. So far, we have performed one iteration of the cluster assignment and the move centroid step and haven\u2019t got a good division of data points into clusters. We need to perform multiple iterations of these 2 steps in order for the K-means algorithm to converge. Fig. e performs the \u2018cluster assignment step\u2019 for the new cluster centroids obtained in fig. d and then fig. f performs the \u2018move centroid step\u2019 to give new and optimal cluster centroids. At this point, after 2 iterations of these steps, we seem to have reached an optimal cluster grouping for the given data points. In reality, it takes a large number of iterations to reach an optimal cluster grouping.", "The entire process of K-means clustering described above can be summarized through the following pseudo-code.", "Now that we know how the steps involved in K-means lead to clustering, let us try to obtain the optimization objective (cost function) involved through the intuition behind these steps. In one line, K-means algorithm determines the cluster centroid for each data point, then shifts the cluster centroid to a central position of that cluster and both of these steps are repeated until the algorithm converges. Hence, the cost function should minimize the overall distance of every cluster centroid with its corresponding data points; subject to the conditions that occur in \u2018cluster assignment step\u2019 and \u2018move centroid step\u2019.", "Before formally defining the cost function for K-means, let\u2019s have a look at the notation used to define the cost function.", "\u00b7 c(i) = cluster number (between 1 and K) to which training example x(i) is currently assigned", "\u00b7 \u00b5(c(i)) = representation of cluster centroid of the cluster to which training example x(i) has been assigned.", "The parameters for the cost function are c(i) and \u00b5(k) (also \u00b5(c(i))), since by varying these parameters, we can get different clusters and the target is to get the most optimal clusters which can be obtained by minimizing the cost function. The minimization objective for K-means is:", "and the values of c(i) and \u00b5(k) for which the cost function yields minimum value are the optimal values of these parameters. During the \u2018cluster assignment step\u2019, the cost function is minimized wrt the parameters c(i) keeping \u00b5(k) fixed (i.e. the partial derivative of the cost function is taken wrt the c(i) variables) and during the \u2018move centroid\u2019 step, the cost function is minimized wrt the parameters \u00b5(k) keeping c(i) fixed (i.e. the partial derivative of the cost function is taken wrt the \u00b5(k) variables).", "The gradient descent algorithm then updates the cost function after computing the aggregate of these 2 partial derivatives and enables us to reach the global minima/a good local minima.", "In the pseudo-code for k-means, we saw how the cluster assignment step and the move centroid step work in tandem to generate k clusters, but we still haven\u2019t discussed how to initialize the cluster centroids randomly, which was the first line of the pseudo-code. Just like we randomly initialized the parameters \u2018W\u2019 and \u2018b\u2019 in linear and logistic regression, here, there are different ways of initializing the parameters c(i) and \u00b5(k) randomly. Let\u2019s discuss them one by one.", "(i) Forgy Initialization: Forgy initialization is one good and fast way of initializing parameters. For K clusters, Forgy method randomly chooses K observations from the training dataset and uses them as the initial means (cluster centroids). Forgy initialization is quite an intuitive technique for initializing cluster centroids since the cluster centroids will lie somewhere near to the training data points, however in practice it tends to spread the initial means out i.e. the algorithm takes more steps to converge.", "(ii) Random Partition: In this method, each data point is randomly assigned to one of the K clusters, data points with same group are combined together to form groups and their mean value is taken to determine the initial cluster centroids. Random Partition method is generally preferred for fuzzy k-means but for the standard k-means, forgy initialization works well.", "(iii) K-means++: The intuition behind this approach is that spreading out the K initial cluster centroids is a good thing. The first cluster centroid is chosen uniformly at random from the data points that are being clustered, after which each subsequent cluster centroid is chosen from the remaining data points with probability proportional to its squared distance from the point\u2019s closest existing cluster center. This method outperforms both, Forgy Initialization and Random Partition methods and is a preferred choice. More about K-means++ can be found here.", "Although the above mentioned random initializations work well for K-means, they can result in K-means converging to a bad local optimum (some local optima are close to global optima and some are not) of the cost function, resulting in poor cluster formation. In fact, in all the algorithms that have random initialization steps, they are at this risk of getting stuck on local optima, even linear and logistic regression algorithms. Look at the image below to see an example of clusters formed by K-means when the algorithm gets stuck in a bad local optimum.", "These bad local optima are detrimental for the model\u2019s performance. We need to avoid such a scenario. The solution is simple though. For values of K between 2\u201310, we can overcome this problem by running 10 to 1000 iterations of K-means, each time with different initial random initializations and pick that one model for which the set of parameters (c(i) and \u00b5(k)) obtained leads to the smallest value for the cost function. However, this method does not work for large values of K since the more clusters we want, greater is the chance of the algorithm converging to a local optima. Hence, in such cases, we don\u2019t need to apply K-means a large number of times since each time there will be high chances of converging to a local minima. A single iteration of K-means gives satisfactory results for large values of K.", "Since we have no way to evaluate the K-means model\u2019s performance due to unlabelled data, the choice of the variable K becomes challenging. The most sensible thing to do in such cases is to use a hit-and-trial method. In the hit-and-trial method used for K-means, we choose a range of values of K (let\u2019s say 1 to 10) and compute the overall cost by running K-means with every possible value of K in the range selected. The value of K up to which the cost function decreases steeply and after which the cost function decreases quite slow can be considered an optimum value of K. This method is called the Elbow Method but I simply like calling it the hit-and-trial method. However, it is not a compulsion to choose this elbow point as the optimum value of K. The value of cost function decreases with increasing values of K and reaches 0 when K = m (total number of training data points) which happens when each data point is a part of its own unique cluster. Although we don\u2019t want K = m, but we can go beyond the elbow point depending on the computation power, the amount of data and the complexity of data we have at hand.", "So far we\u2019ve discussed about K-means in great detail and I\u2019d like to draw your attention to the point when we just started discussing K-means. I mentioned that one of the limitations of K-means is that it doesn\u2019t capture all the clusters automatically and needs an explicit value for the number of clusters. This however, can also be put to an advantage. Consider a dataset where we need to find clusters for a task such as market segmentation but the data points are uniformly scattered. An algorithm that captures clusters automatically has a higher chance of failing in such a scenario. But through the explicit number of clusters supplied to K-means, we can expect the algorithm to come up with good groupings which even humans can\u2019t find. This is the beauty of K-means.", "Another important thing to note about K-means is that K-means results in linear separation of clusters. This may come as a surprise but there is nothing to worry about. The clusters (the squiggly shapes drawn by hand around a set of data points to represent clusters) are still very much the same as we\u2019ve portrayed them so far. The image below will make things clearer.", "With this, we have come to an end on our discussion of K-means. Now, let\u2019s use an unlabelled Credit Card Dataset from Kaggle and use it to determine the usage pattern of credit cards for different credit card holders.", "We\u2019ll be using Credit Card Dataset on Kaggle to determine map spending activity. The dataset can be found here. As is the norm, we need to pre-process the data in order to feed it to the K-means algorithm. First of all, let\u2019s read the data into a pandas dataframe.", "There are 18 features in the dataset which are difficult to fit inside a single image. But, from the output of df.describe(), we can see that for all the columns/features, the standard deviation is quite high and the min and max values are too far apart with the distribution being skewed towards lower values as can be seen from the 75% mark, mean and max. This means that the given dataset consists of outliers and these outliers need to be dealt with. Simply ignoring the outliers will result in quite a lot of data loss.", "Before we deal with outliers, let\u2019s check for missing values in the data and impute them.", "Only 2 columns have null values. The missing values are small fraction of the entire dataset (1/8950) and (313/8950) and hence can be easily imputed. We\u2019ll impute CREDIT_LIMIT with mean value and since MINIMUM_PAYMENTS is a continuous variable skewed towards the lower side, we can impute it with either the mean or median. It shouldn\u2019t make much of a difference since this the fraction of missing values is quite small. We\u2019ll go with imputing with mean values.", "Now that we\u2019ve taken care of missing values, let\u2019s focus our attention back towards the outliers. Let\u2019s convert the entire dataset\u2019s values to categorical values. As we are interested in finding similarities through clusters, it is a good idea to group values in a particular range and assigning them a category. Later we\u2019ll normalize these category values as well to make sure that no large value in any column dominates/skews the clustering result.", "Since we have modified all the exisitng feature names, we can delete the existing feature names.", "We can plot the frequency distribution for all the features. You can check them out in the notebook here.The above graphs show that the frequencies for lower values are high since most values in the data are small. This is evident from the minimum, 1st quartile, median 3rd quartile and maximum values from the data distribution we obtained by df.describe(). This process however took a certain number of trials but didn\u2019t consume much time. Now we normalize all the values to adjust them in the range of 0\u20131.", "Now that we have converted data from continuous to discrete values and brought it down to a particular range, we\u2019ve made sure that we give equal importance to all the features. Now, we can go ahead and apply K-means. Let\u2019s use Elbow Method to choose an optimal value of K.", "We seem to reach an inflection point when K = 6. After this value of K, the cost decreases very slowly.", "The output of this step is a \u2018cluster\u2019 variable, which contains the cluster number for each record/row of the dataset. Let us add this variable at the end of the dataframe.", "In order to visualize the clusters created and see if they\u2019re well-defined, we need to reduce the dimensionality of the data since it\u2019s difficult to visualize n-dimensional data in 2 dimensional space. However, while reducing the dimensionality of the data, we want to make sure that we capture as many features of the original dataset as possible. For this, we use Principal Component Analysis (PCA), which helps us to achieve the objective mentioned above. Follow the next post for an in-depth understanding of PCA.", "Finally, we plot all the clusters as various subplots inside a single plot.", "The results are decent. We have 6 separate clusters which are well distinguished. We see that the clusters in green can be a part of either yellow and purple clusters if we chose the number of clusters to be 5. Additionally, we can also determine what each cluster means by plotting frequency distributions for each feature in a cluster and then comparing and contrasting these plots. The entire code for this post can be found here.", "That\u2019s it for this post. We had a detailed look at K-means algorithm in this post. In the next post, we\u2019ll have an in-depth look at Principal Component Analysis, which is one of the most popular and extensively used unsupervised learning algorithms.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Development Engineer at American Express with a keen interest in the field of Data Science and Web3. linkedin.com/in/vardaan-bajaj-23a279124/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1adf5c30281b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vardaanbajaj?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vardaanbajaj?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "Vardaan Bajaj"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F168771086803&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&user=Vardaan+Bajaj&userId=168771086803&source=post_page-168771086803----1adf5c30281b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1adf5c30281b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1adf5c30281b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/analytics-vidhya/machine-learning-v-decision-trees-random-forest-kaggle-dataset-with-random-forest-3ebfe6d584be", "anchor_text": "previous"}, {"url": "https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html", "anchor_text": "Source"}, {"url": "https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html", "anchor_text": "Source"}, {"url": "https://stanford.edu/~cpiech/cs221/handouts/kmeans.html", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/K-means%2B%2B", "anchor_text": "here"}, {"url": "https://www.researchgate.net/figure/Result-of-the-elbow-method-to-determine-optimum-number-of-clusters_fig8_320986519", "anchor_text": "Source"}, {"url": "https://aws.amazon.com/blogs/machine-learning/k-means-clustering-with-amazon-sagemaker/", "anchor_text": "Source"}, {"url": "https://www.kaggle.com/arjunbhasin2013/ccdata", "anchor_text": "here"}, {"url": "https://www.kaggle.com/vardaanbajaj/k-means-to-recognize-credit-card-usage-patterns?scriptVersionId=39202855", "anchor_text": "here"}, {"url": "https://www.kaggle.com/vardaanbajaj/k-means-to-recognize-credit-card-usage-patterns?scriptVersionId=39202855", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/deep-dive-into-principal-component-analysis-fc64347c4d20", "anchor_text": "next"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1adf5c30281b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/unsupervised-learning?source=post_page-----1adf5c30281b---------------unsupervised_learning-----------------", "anchor_text": "Unsupervised Learning"}, {"url": "https://medium.com/tag/k-means?source=post_page-----1adf5c30281b---------------k_means-----------------", "anchor_text": "K Means"}, {"url": "https://medium.com/tag/python?source=post_page-----1adf5c30281b---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----1adf5c30281b---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1adf5c30281b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&user=Vardaan+Bajaj&userId=168771086803&source=-----1adf5c30281b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1adf5c30281b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&user=Vardaan+Bajaj&userId=168771086803&source=-----1adf5c30281b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1adf5c30281b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1adf5c30281b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1adf5c30281b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1adf5c30281b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1adf5c30281b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1adf5c30281b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vardaanbajaj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vardaanbajaj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vardaan Bajaj"}, {"url": "https://medium.com/@vardaanbajaj/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "84 Followers"}, {"url": "http://linkedin.com/in/vardaan-bajaj-23a279124/", "anchor_text": "linkedin.com/in/vardaan-bajaj-23a279124/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F168771086803&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&user=Vardaan+Bajaj&userId=168771086803&source=post_page-168771086803--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F342b25109362&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-vi-unsupervised-learning-k-means-kaggle-dataset-with-k-means-1adf5c30281b&newsletterV3=168771086803&newsletterV3Id=342b25109362&user=Vardaan+Bajaj&userId=168771086803&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}