{"url": "https://towardsdatascience.com/infer-your-reward-by-observing-an-expert-140b685fd5b5", "time": 1683015449.791949, "path": "towardsdatascience.com/infer-your-reward-by-observing-an-expert-140b685fd5b5/", "webpage": {"metadata": {"title": "Inverse RL in reward design | Towards Data Science", "h1": "Infer your Reward by Observing an Expert", "description": "Designing a reward function in Reinforcement Learning(RL) can be troublesome. We approach this problem using inverse Reinforcement Learning to learn an optimal reward function."}, "outgoing_paragraph_urls": [{"url": "https://developers.google.com/machine-learning/gan/gan_structure", "anchor_text": "how GANs work", "paragraph_index": 2}, {"url": "https://stats.stackexchange.com/a/487726/296297", "anchor_text": "uses the concept of the discriminator", "paragraph_index": 6}, {"url": "https://arxiv.org/pdf/1710.11248", "anchor_text": "Adversarial Inverse RL", "paragraph_index": 19}, {"url": "https://github.com/mugoh/rl-base/tree/master/rlbase/aiRL", "anchor_text": "AiRL on Github", "paragraph_index": 34}, {"url": "https://arxiv.org/pdf/1611.03852", "anchor_text": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models", "paragraph_index": 36}, {"url": "https://arxiv.org/pdf/1710.11248", "anchor_text": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning", "paragraph_index": 37}, {"url": "https://arxiv.org/pdf/1810.00821", "anchor_text": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow", "paragraph_index": 38}], "all_paragraphs": ["Designing a reward function in Reinforcement Learning(RL) can be troublesome. It\u2019s simple enough that the aim is to take actions that increase the future cumulative reward and avoid those that hurt it. Choosing how to reward actions on real-world tasks, and in a way that is both learnable and expressive of the objective desired of the agent, however, is not straightforward. For example, how do you specify \u201cacceptable\u201d social behaviour as a function?", "Inverse Reinforcement Learning (iRL) is an approach to alleviate this design problem. Instead of attempting to engineer a reward function r(s, a) ourselves, we let the RL agent observe an expert demonstrate what we wish it to learn and infer the intention of the expert from their actions. By doing so, the agent fits a reward function to the expert\u2019s intent.", "Our iRL implementation will connect to Generative Adversarial Networks(GANs). For this, you\u2019ll find a basic understanding of how GANs work pretty useful, though I do brief on it below.", "A GAN comprises of a discriminator D and a generator G. The generator aims to generate fake images that look as close as possible to the real images used in training. The discriminator, on the other hand, classifies a given image as either genuine or fake. So both D and G play a game where D attempts to maximize the probability that it correctly classifies an input x as real or fake, and G minimizes the likelihood that D labels its output as a fake 1 \u2014 D(G(x)).", "The loss function that achieves this is a log of the outputs of D and G.", "We can represent this training process as follows:", "Inverse RL uses the concept of the discriminator from GANs. The discriminator in iRL is a ratio of the policy and the reward function. We see the full picture of this in a moment.", "Similar to having fake images and real images in a GAN, iRL has two sets of data \u2014 the expert demonstrations and the transition data generated by the policy interacting with the environment. Both transition sets comprise state-action pairs up to a finite time step T (s\u2080, a\u2080, s\u2081, a\u2081,\u2026, sT, aT). Relating iRL to GANs, the expert demonstrations can be said to be the real data while the policy-collected samples are the fake data. Which means the policy now acts as the generator.", "The objective of the discriminator is represented in the same way as it is in a GAN.", "The first part of this objective function tries to increase the likelihood that the samples seen are expert demonstrations. The second part decreases the likelihood of the samples being collected by the running policy.", "As seen, the objective in training the discriminator is to maximize the probability of correct classification of the input as real or fake. To achieve this, we maximize the loss function:", "In the implementation, this is achievable in two simple steps:", "1. Sample batch of expert trajectories \u03c4E, forward pass through D and calculate the loss log(D(\u03c4E)).", "2. Sample a batch of collected policy trajectories \u03c4F, forward-pass through D and calculate the loss log(D(\u03c4F)). Here, we avoid doing (minimization of) log(1 \u2014 D(\u03c4F)) because this fails to provide sufficient gradients in the learning process. So we maximize log(D(\u03c4F)) instead.", "The discriminator D is a function of the reward function r:", "\u03a8 represents the reward function\u2019s learnable parameters. The discriminator, being a function of the learned reward, also uses the parameters \u03a8.", "Updating the discriminator D updates the learned reward function r(\u03c4). When the discriminator is optimal, we arrive at an optimal reward function. However, the reward function above r(\u03c4) uses an entire trajectory \u03c4 in the estimation of the reward. That gives high variance estimates compared to using a single state, action pair r(s, a), resulting in poor learning.", "Using single state-action pairs will solve the high variance estimation problem but also has a drawback \u2014 it makes the optimal reward function heavily entangled with the supervised actions proposed by the optimal policy. In other words, the learned reward will encourage mimicking the expert policy and fail to produce sensible behaviour when changes occur in the environment.", "That brings us to our final improvement on the discriminator.", "To extract rewards disentangled from the environment, Adversarial Inverse RL(AIRL) proposed to modify the discriminator with this form:", "We can further simplify the reward function r(s, a, s\u2019) to:", "Here are the discriminator and reward function in code.", "g(s) recovers the optimal reward; h(s) resembles a value function.", "That means r(s, a, s\u2019) recovers the advantage. Here is where it gets interesting.", "A policy update involves finding the gradient of the log of the policy, multiplied by the advantage.", "In iRL, since we aren\u2019t observing environment rewards, which are used when estimating advantages, the change during the update will be using the reward function to approximate these advantages. More simply, the advantage recovered by the reward function finds use in the policy update.", "For an intuitive overview of iRL, here\u2019s a side-by-side comparison of the entire pseudo-code between vanilla policy gradient(VPG), and inverse RL applied to VPG.", "The policy \u03c0 is trained to maximize this estimated reward r(s, a, s\u2019), and when updated, learns to collect trajectories that are more indistinguishable from the expert demonstrations.", "The first step for training on inverse RL is running a policy-based RL algorithm to collect expert demonstrations. I approached this in two ways:", "The above two options for collecting expert data did not seem to have a notable difference in the resultant inverse RL average return (at least within 250 epoch runs). My interpretation for this being that the reward threshold in (b) collected trajectories that mostly occur in the final training episodes seen in (a).", "Here\u2019s a smoothed performance of iRL on HalfCheetah-v2 over 100 steps, using expert data from the five final epochs.", "Below is a single run example of the policy used to collect these demonstrations.", "Inverse Reinforcement Learning allows us to demonstrate desired behaviour to an agent and attempts to enable the agent to infer our goal from the demonstrations. Aligning this goal to the demos recovers a reward function. The recovered reward, then, encourages the agent to take actions that have a similar intent to what the expert was trying to achieve.", "The benefit of recovering the intent is that the agent learns the most optimal way of reaching the goal \u2014 it does not blindly copy the expert\u2019s sub-optimal behaviour or mistakes. As a result, iRL promises more desirable performance compared to the expert policy.", "Inverse RL Repository: AiRL on Github", "For more approaches on inverse RL, have a look at these works:", "[1] C. Finn, P. Christiano, P. Abbeel, and S. Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models, NIPS, 2016.", "[2] J. Fu, K. Luo, S. Levine, Learning Robust Rewards with Adversarial Inverse Reinforcement Learning, ICLR 2018.", "[3] X. Peng, A. Kanazawa, S. Toyer, P. Abbeel, S. Levine, Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow, ICLR, 2019.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Fascinated by bread | Learning to smile | RL & Decision control"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F140b685fd5b5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mugoh?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "mugoh mwaura"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----140b685fd5b5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F140b685fd5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F140b685fd5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/p/140b685fd5b5", "anchor_text": ""}, {"url": "https://unsplash.com/photos/6U5AEmQIajg", "anchor_text": "Unsplash"}, {"url": "https://developers.google.com/machine-learning/gan/gan_structure", "anchor_text": "how GANs work"}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://mugoh.github.io/mug-log/2020/10/20/adversarial-inverse-rl.html", "anchor_text": ""}, {"url": "https://stats.stackexchange.com/a/487726/296297", "anchor_text": "uses the concept of the discriminator"}, {"url": "https://mugoh.github.io/mug-log/2020/10/20/adversarial-inverse-rl.html", "anchor_text": ""}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "source"}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://arxiv.org/pdf/1710.11248", "anchor_text": "Adversarial Inverse RL"}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://arxiv.org/pdf/1710.11248", "anchor_text": "AIRL"}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://github.com/mugoh/rl-base/tree/master/rlbase/aiRL", "anchor_text": "https://github.com/mugoh/rl-base/tree/master/rlbase/aiRL"}, {"url": "https://medium.com/@mugoh/140b685fd5b5", "anchor_text": ""}, {"url": "https://github.com/mugoh/rl-base/tree/master/rlbase/aiRL", "anchor_text": "AiRL on Github"}, {"url": "https://arxiv.org/pdf/1611.03852", "anchor_text": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models"}, {"url": "https://arxiv.org/pdf/1710.11248", "anchor_text": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning"}, {"url": "https://arxiv.org/pdf/1810.00821", "anchor_text": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----140b685fd5b5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----140b685fd5b5---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----140b685fd5b5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F140b685fd5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----140b685fd5b5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F140b685fd5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----140b685fd5b5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F140b685fd5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F140b685fd5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----140b685fd5b5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----140b685fd5b5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----140b685fd5b5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----140b685fd5b5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "mugoh mwaura"}, {"url": "https://medium.com/@mugoh/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1bcb39f5cedf%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finfer-your-reward-by-observing-an-expert-140b685fd5b5&user=mugoh+mwaura&userId=1bcb39f5cedf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}