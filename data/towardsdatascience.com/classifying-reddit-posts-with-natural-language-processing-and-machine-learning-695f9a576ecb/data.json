{"url": "https://towardsdatascience.com/classifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb", "time": 1682994564.638128, "path": "towardsdatascience.com/classifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb/", "webpage": {"metadata": {"title": "Classifying Reddit Posts With Natural Language Processing and Machine Learning | by Britt | Towards Data Science", "h1": "Classifying Reddit Posts With Natural Language Processing and Machine Learning", "description": "In my last post I walked you through my data science process for using machine learning to predict home prices (below). In this post I will walk you through the same process, but for using Natural\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.reddit.com/r/BabyBumps", "anchor_text": "r/BabyBumps", "paragraph_index": 1}, {"url": "https://www.reddit.com/r/menstruation", "anchor_text": "r/menstruation", "paragraph_index": 1}, {"url": "https://github.com/traintestbritt/classifying_reddit_posts/tree/master/notebooks", "anchor_text": "my code", "paragraph_index": 24}, {"url": "https://docs.google.com/presentation/d/1S3R8KSKmX8M_uOtiXYiAQbVpRUHqMC8QfxfBW7Zx18k/edit?usp=sharing", "anchor_text": "my presentation", "paragraph_index": 24}], "all_paragraphs": ["In my last post I walked you through my data science process for using machine learning to predict home prices (below).", "In this post I will walk you through the same process, but for using Natural Language Processing (NLP) and classification modeling to classify Reddit posts from r/BabyBumps and r/menstruation. Before I begin, let\u2019s recall the data science process (outlined below) followed by a fun ice breaker!", "Can you guess which subreddit the posts (pictured below) came from? Your options are r/BabyBumps and r/menstruation. Share your guess in the comments!", "As you may have guessed I was tasked with using machine learning to do what you just tried to do above! In other words, creating a classification model that can distinguish which of two subreddits a post belongs to.", "The assumption for this problem is that a disgruntled, Reddit back-end developer went into every post and replaced the subreddit field with \u201c(\u00b7\u033f\u033f\u0139\u032f\u033f\u033f\u00b7\u033f \u033f)\u201d. As a result, none of the subreddit links will populate with posts until the subreddit fields of each post are re-assigned.", "As you may have gathered, posts in r/BabyBumps and r/menstruation will definitely have a lot of crossover. For example, think about women talking about food cravings, cramps, or mood swings in either channel. I like a challenge so I purposely picked two closely-related subreddits as I wanted to see how well I could leverage natural language processing and machine learning to accurately re-classify the posts to their respective subreddit.", "*The answer to the ice breaker can be found in the last sentence of the following section, but keep reading to see if you\u2019re smarter than the algorithm I built!", "My data acquisition process involved using the requests library to loop through requests to pull data using Reddit\u2019s API which is pretty straightforward. To get posts from /r/menstruation, all I had to do was add .json to the end of the url. Reddit only provides 25 posts per request and I wanted 1000 so I iterated through the process 40 times. I also used the time.sleep()function at the end of my loop to allow for a one second break in between requests.", "My for loop outputted a list of nested json dictionaries of which I indexed to pull out my desired features, Post Text and Title, while simultaneously adding them to two Pandas DataFrames one for r/BabyBumps-related posts and the other for r/menstruation-related posts.", "After getting my posts in their respective DataFrames I checked for duplicate and null values, both of which occurred. For duplicate values I got rid of them by utilizing the drop_duplicates() function. Null values only occurred in my Post Text column, this happens when a Reddit user decides to use only the title field. I decided not to drop null values as I did not want to lose valuable information in the accompanying rows of my Title feat so I filled the nulls with unique and arbitrary text instead.", "After cleaning and concatenating my data, my final DataFrame contained 1818 documents (rows) and 3 features (columns). The third feature was my target, which had a balance of classes of 54% for class 1 (r/BabyBumps) and 46% for class 0 (r/menstruation) \u2014 ice breaker answer!", "I created a word cloud because they\u2019re fun and we\u2019re working with text data!", "This word cloud contained 100 words from both subreddits. I generated it to get a visual understanding of the frequencies (bigger/bolder words have higher frequencies) of words and how their commonality across subreddits might throw my model off; or how it may work in my model\u2019s favor if it\u2019s a word/phrase like \u201cmenstrual cup\u201d which has a medium frequency and likely only appears or mostly appears in r/menstruation posts.", "I began my modeling process by creating my X and my y and splitting my data into training and test sets. I then moved on to my feature engineering process by instantiating two CountVectorizers for my Post Text and Title features. CountVectorizer converts a collection of text documents (rows of text data) to a matrix of token counts. The hyperparameters (arguments) I passed through them were:", "Stop words removes words that commonly appear in the English language. Strip accents removes accents and performs other character normalization. Min_df ignores terms that have a document frequency strictly lower than the given threshold.", "An n-gram is just a string of n words in a row. For example, if you have a text document containing the words \u201cI love my cat.\u201d \u2014 setting the n-gram range to (1, 2) would produce: \u201cI love | love my | my cat\u201d. Having n-gram ranges can be helpful in providing the models with more context around the text I\u2019m feeding it.", "I assumed that setting the Title feature with an n-gram range of (1, 4) would clean up noise and be more helpful to my model by adding more context than if just left alone. I still set a gentle min_df to help clean up any additional noise. I made similar assumptions for my Post Text feature, although I gave it a higher n-gram range since post texts tends to be lengthier.", "This resulted in 393 features which I fed into two variations of the models listed below. I built four functions to run each pair of models and Gridsearched over several hyperparameters to find the best ones to fit my final model with.", "The difference in variations were the penalty and solver parameters. The \u2018newton-cg\u2019, \u2018lbfgs\u2019 and \u2018sag\u2019 solvers only handle L2 penalty (ridge regularization), whereas \u2018liblinear\u2019 and \u2018saga\u2019 handle L1 (lasso regularization).", "The difference in variations for these two models was the criterion parameter. One was set to \u2018gini\u2019 (Gini impurity) while the other was set to \u2018entropy\u2019 (information gain).", "The difference in variations was the fit_prior argument which decides whether to learn class prior probabilities or not. If false, a uniform prior will be used. One was set to True, while the other was set to False.", "My second Multinomial Naive Bayes model performed the best. With the best parameters being \u2014 alpha=0 and fit_prior=False. The accuracy score was 92.4% on training data and 92.2% on unseen data. This means our model is slightly and probably inconsequentially overfit. This also means that 92.2% of our posts will be accurately classified by our model.", "Considering the small amount of data gathered and minimal amount of features used, the Multinomial Naive Bayes model was the most outstanding. It handled unseen data well and balanced the tradeoff between bias and variance the best among the eight models so I would use it to re-classify reddit posts.", "However if given more time and data to answer the problem I would recommend two things: 1) spending more time with current features (e.g. engineering a word length feature) and 2) exploring new features (e.g. upvotes or post comments).", "Check out my code (which I split into 3 Jupyter notebooks \u2014 collection, cleaning/exploration, and modeling \u2014 for readability and organization purposes) and my presentation. As always, please comment feedback and questions. Thanks for reading!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "data analytics & engineering \u2022 data & python educator \u2022 speaker \u2022 writer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F695f9a576ecb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@britt-allen?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@britt-allen?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "Britt"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F92aee99c8bed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&user=Britt&userId=92aee99c8bed&source=post_page-92aee99c8bed----695f9a576ecb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F695f9a576ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F695f9a576ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ikukevk?utm_source=medium&utm_medium=referral", "anchor_text": "Kevin Ku"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/using-machine-learning-to-predict-home-prices-d5d534e42d38", "anchor_text": "Using Machine Learning to Predict Home PricesIn this post I will walk you through my data science process for using machine learning to predict home prices. Before\u2026towardsdatascience.com"}, {"url": "https://www.reddit.com/r/BabyBumps", "anchor_text": "r/BabyBumps"}, {"url": "https://www.reddit.com/r/menstruation", "anchor_text": "r/menstruation"}, {"url": "https://github.com/traintestbritt/classifying_reddit_posts/tree/master/notebooks", "anchor_text": "my code"}, {"url": "https://docs.google.com/presentation/d/1S3R8KSKmX8M_uOtiXYiAQbVpRUHqMC8QfxfBW7Zx18k/edit?usp=sharing", "anchor_text": "my presentation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----695f9a576ecb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----695f9a576ecb---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----695f9a576ecb---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/reddit?source=post_page-----695f9a576ecb---------------reddit-----------------", "anchor_text": "Reddit"}, {"url": "https://medium.com/tag/data-science?source=post_page-----695f9a576ecb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F695f9a576ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&user=Britt&userId=92aee99c8bed&source=-----695f9a576ecb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F695f9a576ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&user=Britt&userId=92aee99c8bed&source=-----695f9a576ecb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F695f9a576ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F695f9a576ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----695f9a576ecb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----695f9a576ecb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----695f9a576ecb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----695f9a576ecb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@britt-allen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@britt-allen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Britt"}, {"url": "https://medium.com/@britt-allen/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "281 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F92aee99c8bed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&user=Britt&userId=92aee99c8bed&source=post_page-92aee99c8bed--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4226abe46954&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&newsletterV3=92aee99c8bed&newsletterV3Id=4226abe46954&user=Britt&userId=92aee99c8bed&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}