{"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "time": 1682994918.3890822, "path": "towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2/", "webpage": {"metadata": {"title": "Coding a 2 layer neural network from scratch in Python | by Javier Ideami | Towards Data Science", "h1": "Coding a 2 layer neural network from scratch in Python", "description": "Code your own 2 layer neural network from scratch in Python. Understand in depth back-propagation and the gradient descent optimization algorithm"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "In part 1 of this article", "paragraph_index": 0}, {"url": "https://colab.research.google.com", "anchor_text": "colab.research.google.com", "paragraph_index": 1}, {"url": "https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab", "anchor_text": "Essence of Linear Algebra Series", "paragraph_index": 6}, {"url": "https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr", "anchor_text": "ssence of Calculus", "paragraph_index": 39}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "In the final part of this article, part 3", "paragraph_index": 107}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3", "paragraph_index": 107}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "Part 1", "paragraph_index": 108}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "Part 2", "paragraph_index": 108}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3", "paragraph_index": 108}], "all_paragraphs": ["In part 1 of this article, we understood the architecture of our 2 layer neural network. Now it\u2019s time to build it! In parallel, we will explore and understand in depth the foundations of deep learning, back-propagation and the gradient descent optimization algorithm.", "First things first, if you want to try your own coding alongside this article, one option is to use Jupyter notebooks. They facilitate enormously working and experimenting with Python code in an environment that is very friendly to data explorers and researchers. You can use Jupyter notebooks for free, for example at Google colab: colab.research.google.com.", "And so, we begin! First we import some standard Python libraries. Numpy will help us with linear algebra and array functionality. Pandas dataframes will be really handy when we import and prepare our data. And Matplotlib will help us do some cool charts. Finally, sklearn helps us normalize our data and display useful graphs, such as confusion matrices.", "Next, we create a Python class that setups and initializes our network.", "We first name our class (dlnet), and define its init method. The init method is executed the first time we instantiate the class. It is responsible for creating the structure of the network and the methods that will control it.", "We then create a series of class variables that will hold key data of the network.", "A lot of these variables will hold matrices. Remember that we will accelerate our computations by combining multiple calculations through the use of matrices. As I pointed out before, if you wish to refresh your linear algebra, check the amazing YouTube videos of 3Blue1Brown and specifically his Essence of Linear Algebra Series.", "Finally, we declare three more parameters.", "Notice that last element, the loss, because it\u2019s crucial. What is that loss value? It all has to do with the process of training our network to learn that mystery function.", "We will go onto that very soon, but first, let\u2019s define the function nInit, which will initialize with random values the parameters of our network.", "When we multiply matrices, as in the product W1 X and W2 A1 , the dimensions of those matrices have to be correct in order for the product to be possible. That\u2019s why it\u2019s essential to set the dimensions of our weights and biases matrices right.", "Now, let\u2019s define within the class a function that will perform the computation at each unit of each layer in our network. We will call it forward because it will take the input of the network and pass it forwards through its different layers until it produces an output.", "We also need to declare the Relu and Sigmoid functions that will compute the non-linear activation functions at the output of each layer.", "The Relu and Sigmoid functions declare the activation computations. And the forward function performs the computations we have described earlier.", "We multiply the weights of the first layer by the input data and add the first bias matrix , b1, to produce Z1. We then apply the Relu function to Z1 to produce A1.", "Next, we multiply the weight matrix of the second layer by its input, A1 (the soutput of the first layer, which is the input of the second layer), and we add the second bias matrix, b2, in order to produce Z2. We then apply the Sigmoid function to Z2 to produce A2, which is in fact Yh, the output of the network.", "That was it! We just ran our input data through the network and produced Yh, an output.", "The logical next step is to find out how good our result was.", "For that, we can compare the output we have produced to the output we should have obtained: Yh and Y. To compute that, we will add a final function to the network, the loss function.", "There are many kinds of loss functions. The objective of the loss function is to express how far from the intended target our result was, and to average that difference across all the samples we have used to train the network.", "One of the simplest loss functions used in deep learning is MSE, or mean square error.", "The MSE loss function calculates the difference, the distance between our predicted and target outputs across all the samples we have used, and then squares that difference.", "Finally, it adds up all those operations. Squaring the distances ensures that we produce an absolute distance value that is always positive.", "MSE is a simple way to find out how far we are from our objective, how precise is so far the function computed by our network in terms of connecting our input data with our target outputs.", "MSE is often used in regression challenges, when the output of the network is a continuous value, for example: a temperature value or the cost of a house.", "However, in this article we will work on a different kind of challenge, a binary classification challenge, where our output will be either 0 or 1 (0 meaning benign, 1 meaning malignant).", "When working with classification challenges, there is a different loss function that works better at expressing the difference between our predicted output and our correct one. It is called the Cross-Entropy Loss Function and we will use it in our network.", "We pick loss functions based on how well they express the quality of our network\u2019s performance in relation to the specific kind of challenge we are working on. Cross-entropy is a great loss function for classification problems (like the one we will work on) because it strongly penalizes predictions that are confident and yet wrong (like predicting with high confidence that a tumor is malign when in fact it is benign).", "As you can see above, at the end of the forward function we call this nloss method (which computes the loss), and then store the resulting loss value in the loss array. This will later allow us to plot and visually understand how the loss value changes during the training of the network.", "Believe or not, we have already created almost half of all the code we will need. But now we arrive to the crucial point.", "We have computed an output Yh, and calculated the loss: how far we are from the intended output, Y. The question now is: how can we improve that result Yh, and what does it mean to improve it?", "To improve the result, we need to get that loss value to decrease. The lower our loss value, the lower the distance between our target and predicted outputs (Y and Yh), and the better our network will perform.", "Let\u2019s recap. We are producing two outputs at the network:", "And it is from that objective, from the objective of minimizing the loss, of minimizing the distance between our predicted and correct outputs, that the training process of the network is born.", "At this stage, we have performed a forward pass, obtained our output Yh, and then calculated our loss, our error, the distance between our predicted and correct output (Yh and Y).", "The next logical step is to change slightly the values of the parameters of our network, of our weights and biases, and perform the forward pass again to see if our loss hopefully decreases. Therefore, the training process would look like this:", "You got it, that\u2019s not efficient at all.", "Now, think about this, to modify our weights and biases by a small amount, we can do one of two things with each one of them:", "There must be a way, in which, taking our loss as a starting point, we can calculate if we should increase them or decrease them in order to minimize such loss.", "Enter Calculus, and enter the mighty derivative, the gradient. Let\u2019s explore in very simple ways how the derivative works. If you want to go deeper I have you covered again with 3Blue1Brown Essence of Calculus series.", "In order to understand in what direction we should change our weights and biases, it would be great to understand what impact a small change in each of those weights and biases has in our final loss.", "And we can use derivatives for this, partial derivatives to be precise. Because a partial derivative is going to tell us what impact a small change on a specific parameter, say, W1, has on our final loss.", "And with that info, we will be able to decide in what direction we want to modify W1 in order to decrease the loss.", "Let\u2019s first refresh the intuition of the derivative.", "Think of the function x to the power of 2: x**2", "At x=3, y=9. Let\u2019s focus on that point and find the derivative, the rate of change at x=3.", "To do that, we will study what happens to y when we increase x by a tiny amount, which we call h. That tiny amount eventually converges to 0 (the limit), but for our purposes we will consider it to be a really small value, say 0.001.", "6 is telling us that in this function x**2, at x=3, the rate of change is positive and has a strength of 6. It is telling us that at that point, if we increase x a bit, y will change in a positive way and with a strength of \u201c6 times more\u201d. Basically, that the 0.001 increment at the input will become a 0.006 increment at the output.", "So we see that we can calculate a derivative by hand easily following this method:", "At x=-2, the rate of change is negative, the function is moving down and with a strength of 4.", "Calculating derivatives in this way takes a long time but thanks to the geniuses of math, it\u2019s easy to calculate them really fast by using differential equations, special equations that express the derivative of the original function. So, instead of having to calculate the derivative at each point, a single equation can calculate it for us everywhere in that function automatically!", "Most, but not all equations, have a derivative that can be expressed with another equation.", "The derivative of x**2 is the function 2x. And we will express the derivative with the letter d followed by the variable whose rate of change we are studying.", "All right, so let\u2019s recap. Thanks to the derivative we can understand in what direction the output of a function is changing at a certain point when we modify a certain input variable, x in this case.", "It would be great then if we could use the derivative to understand how small changes to our weights and biases impact the loss of the network.", "However, there is a little problem, a final obstacle.", "Our network is made of layers. It could have 2 or 200 layers. And we need to understand how changes to all of our weights and biases impact the loss at the end of the network.", "Remember that our network is a series of functions chained together. If we want to calculate in a multi layer network how, for example, a change in W1 impacts the loss at the final output, we need to somehow find a way to connect, to relate to each other, the different derivatives that exist between W1 and the loss of the network at its end.", "Could we maybe, somehow, chain them?", "Yes, indeed. Calculus gives us something called the chain rule of derivatives, which really it\u2019s a pretty simple concept when we look at it in detail.", "First of all, a partial derivative is a derivative that studies the change that occurs in a variable when we modify another variable.", "To connect it all with the code that is coming:", "The chain rule tells us that to understand the impact of the change of a variable on another, when they are distant from each other, we can chain the partial derivatives in between by multiplying them.", "It\u2019s time to talk about the Back-Propagation algorithm within a neural network, and in this case, specifically, in our 2 layer network.", "Back-propagation makes use of the chain rule to find out to what degree changes to the different parameters of our network influence its final loss value.", "Let\u2019s pick one of our parameters and understand the chain rule in action.", "Say that we want to understand how small changes to W1 will impact the Loss. All right, let\u2019s begin with the equation of the Loss:", "Well, W1 is not present in this equation, but Yh is. Let\u2019s proceed to calculate how a change in Yh, our result, influences the loss. And let\u2019s see if, after we do that, we can continue chaining derivatives until we arrive to W1.", "To calculate this derivative, we look for the derivative equation of the Loss function. You can learn to quickly find the derivatives of all kinds of equations by refreshing your calculus a bit or looking them up online. In this case we find that:", "All right, one down. Now, remember, we want to continue chaining derivatives until we arrive to W1.", "So let\u2019s see, what\u2019s the next step backwards in our network, how did we produce Yh?", "All right, great. W1 is still not there, but we got Z2. So let\u2019s find out what impact a change in Z2 has on Yh. For that we need to know the derivative of the sigmoid function, which happens to be:", "To simplify the writing, we will represent that differential equation as dSigmoid. Therefore::", "At this stage, we can already chain (multiply) these 2 derivatives to find the derivative of the Loss in relation to Z2.", "Excellent, let\u2019s proceed. How did we calculate z2?", "Again, W1 is still not there, but we got A1. Let\u2019s find out what impact a change on A1 has on Z2. Therefore:", "And we can chain that derivative to the previous 2 in order to get the total derivative between A1 and the loss of the network:", "As you can see, we are chaining derivatives, one after the other, until we arrive to W1, our target. So far we have moved from the Loss to Yh, from Yh to Z2 and from Z2 to A1.", "Excellent. Let\u2019s continue. How did we produce A1?", "And W1 is still not there, but we got Z1. We need the derivative of Relu. The derivative of the Relu function is 0 when the input is 0 or less than 0, and 1 otherwise.", "Again, to simplify the writing, we will express it as dRelu.", "Great, let\u2019s chain again this latest derivative with all the previous ones to get the full derivative of the Loss in relation to Z1:", "Superb, we are approaching! How did we calculate Z1?", "Yeah! W1 is there! We had missed you W1! It\u2019s so great to see you! :)", "So exciting! This will therefore be the final derivative:", "And let\u2019s chain this latest derivative to all the previous ones:", "And that\u2019s it. We have calculated the derivative of the Loss in relation to our parameter W1. That is, how much and in what direction the Loss changes when we modify slightly W1.", "In Python code, ordering things correctly to account for the way we multiply matrices, the code of this chaining process is:", "Notice that at the last step, we divide the result by the number of units of the layer, so that the derivative in relation to each weight W is scaled correctly at each unit.", "And that, what you have just seen, is back-propagation, or the key ingredient of pretty much all deep learning processes.", "Let\u2019s breathe! That was the hardest bit of the entire article, from now on things get easier.", "In this way we produce the backwards pass, which becomes the back-propagation function of our python class. We also declare dRelu and dSigmoid, the derivatives of the Relu and Sigmoid functions, which are needed when we compute the back-propagation algorithm.", "Within the backward function, after calculating all the derivatives we need for W1, b1, W2 and b2, we proceed, in the final lines, to update our weights and biases by subtracting the derivatives, multiplied by our learning rate.", "Remember that the learning rate is a parameter that allows us to set how fast the network learns. We, therefore, modify our weights and biases by a quantity proportional to that learning rate.", "All right, so far we have:", "That is, in fact, the Gradient Descent optimization algorithm, the other piece of this fascinating puzzle that is training our neural network.", "Time to return to the very first animation in this article.", "Let\u2019s look again at the first animation of the article.", "This is the gradient descent optimization algorithm, the cornerstone and most often used method to gradually optimize the weights of our network, so that eventually they will allow us to compute a function that accurately and efficiently connects our input data with our desired output.", "Let\u2019s analyze what\u2019s happening at the animation, which represents key aspects of the gradient descent algorithm.", "Let\u2019s take a look at the code:", "We first instantiate our neural network. And then run a number of iterations, performing forward and backward passes and updating our weights. Every x iterations we print the loss value.", "After less than 100 lines of Python code, we have a fully functional 2 layer neural network that performs back-propagation and gradient descent.", "This is a basic network that can now be optimized in many ways. Because as we will soon discuss, the performance of neural networks is strongly influenced by a number of key issues. Two very important ones are:", "Regarding the optimization algorithm, in this article we are using the simplest and purest version of the gradient descent algorithm. Our purpose is to understand back-propagation and the basic optimization and training process.", "There are many variations of gradient descent, and later I will name a few of them. Two very simple ones are:", "It is now time to test and try our network. Only by using it we will fully understand its potential and its limitations.", "In the final part of this article, part 3, we will work with the Wisconsin cancer data-set, learn to prepare our data, run it through out network and analyze the results. We will also discuss some more advanced topics. Let\u2019s go to Part 3.", "Links to the 3 parts of this article:Part 1 | Part 2 | Part 3", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A multidisciplinary engineer, researcher, creative director, artist and entrepreneur, from augmented reality to deep learning, filmmaking, 3D and beyond."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4dd022d19fd2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ideami?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84----4dd022d19fd2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dd022d19fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dd022d19fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "In part 1 of this article"}, {"url": "https://colab.research.google.com", "anchor_text": "colab.research.google.com"}, {"url": "https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab", "anchor_text": "Essence of Linear Algebra Series"}, {"url": "https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr", "anchor_text": "ssence of Calculus"}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "In the final part of this article, part 3"}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4dd022d19fd2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4dd022d19fd2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4dd022d19fd2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4dd022d19fd2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4dd022d19fd2---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4dd022d19fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&user=Javier+Ideami&userId=7f7b5d730c84&source=-----4dd022d19fd2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4dd022d19fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&user=Javier+Ideami&userId=7f7b5d730c84&source=-----4dd022d19fd2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dd022d19fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4dd022d19fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4dd022d19fd2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4dd022d19fd2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/@ideami/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4c3e5da2cfc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2&newsletterV3=7f7b5d730c84&newsletterV3Id=4c3e5da2cfc1&user=Javier+Ideami&userId=7f7b5d730c84&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}