{"url": "https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a", "time": 1682993677.804716, "path": "towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a/", "webpage": {"metadata": {"title": "Preventing Deep Neural Network from Overfitting | by Piotr Skalski | Towards Data Science", "h1": "Preventing Deep Neural Network from Overfitting", "description": "Thanks to a huge number of parameters (thousands and sometimes even millions) neural networks have a lot of freedom and can fit a variety of complex datasets. This unique ability has allowed them to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "opening article", "paragraph_index": 1}, {"url": "https://github.com/SkalskiP/ILearnDeepLearning.py", "anchor_text": "GitHub", "paragraph_index": 1}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": "Twitter", "paragraph_index": 16}, {"url": "https://medium.com/@piotr.skalski92", "anchor_text": "Medium", "paragraph_index": 16}, {"url": "https://github.com/SkalskiP", "anchor_text": "GitHub", "paragraph_index": 16}, {"url": "https://www.kaggle.com/skalskip", "anchor_text": "Kaggle", "paragraph_index": 16}, {"url": "http://makesense.ai", "anchor_text": "makesense.ai", "paragraph_index": 17}], "all_paragraphs": ["Thanks to a huge number of parameters (thousands and sometimes even millions) neural networks have a lot of freedom and can fit a variety of complex datasets. This unique ability has allowed them to take over many areas in which it has been difficult to make any progress in the \u2018traditional\u2019 machine learning era - such as image recognition, object detection or natural language processing. Sometimes, however, their greatest advantage becomes a potential weakness. Lack of control over the learning process of our model may lead to overfitting - situation when our neural network is so closely fitted to the training set that it is difficult to generalize and make predictions for new data. Understanding the origins of this problem and ways of preventing it from happening, is essential for a successful design of NN.", "Note: This article is the second part of the \u201cMysteries of Neural Networks\u201d series, if you haven\u2019t had the opportunity yet, read the opening article. The source code used to create visualizations that were used in this article is available on my GitHub.", "In practice, detecting that our model is overfitting is difficult. It\u2019s not uncommon that our trained model is already in production and then we start to realize that something is wrong. In fact, it is only by confronting new data that you can make sure that everything is working properly. However, during the training we should try to reproduce the real conditions as much as possible. For this reason, it is good practice to divide our dataset into three parts - training set, dev set (also known as cross-validation or hold-out) and test set. Our model learns by seeing only the first of these parts. Hold-out is used to track our progress and draw conclusions to optimise the model. While, we use a test set at the end of the training process to evaluate the performance of our model. Using completely new data allows us to get an unbiased opinion on how well our algorithm works.", "It is very important to make sure that your cross-validation and test set come from the same distribution as well as that they accurately reflect data that we expect to receive in the future. Only then we can be sure that the decisions we make during the learning process bring us closer to a better solution. I know what you are thinking about\u2026 \u201cHow should I divide my dataset?\u201d Until recently, one of the most frequently recommended splits was 60/20/20, but in the era of big data, when our dataset can count millions of entries, those fixed proportions are no longer appropriate. In short, everything depends on the size of the dataset we work with. If we have millions of entries at our disposal, perhaps it would be better idea to divide them in 98/1/1 ratio. Our dev and test sets should be simply large enough to give us high confidence in the performance of our model. Recommended methods of dividing the dataset according to its size are illustrated in Figure 1.", "Thanks to the proper preparation of our data, we gave ourself the tools to assess the performance of model. However, before we start to draw any conclusions, we should familiarize ourselves with two new concepts - bias and variance. To give us a better understanding of this some how complex issue, we will use a simple example, that hopefully allow us to develop a valuable intuition. Our dataset consisting of two classes of points, located in a two-dimensional space, is visualised in Figure 2.", "Due to the fact that this is a simple presentation, this time we will skip test set and use only the training and cross-validation sets. Next, we will prepare three models: the first one is simple linear regression, the other two are neural networks built of several densely connected layers. Below in Figure 3. we can see the classification boundaries that were defined using those models. The first model in the top right corner is very simple and therefore has a high bias, i.e. it is not able to find all significant links between features and result. This is understandable - our dataset has a lot of noise in it, and therefore simple linear regression, is not able to deal with it effectively. Neural networks performed much better, but the first one (shown in the lower left corner) fitted into the data too closely, which made it work significantly worse on the hold-out set. This means that it has a high variance - it fits into the noise and not into the intended output. This undesirable effect was mitigated, in the last model, by the use of regularisation.", "I know, I know\u2026 the example I presented is trivial - we have only two features and at any time we can create a graph and visually examine the behavior of our model. What to do when we operate in a multidimensional space, with a dataset containing, say, dozens of features? Then we compare the error values calculated using the training and cross-validation sets. The optimal situation we should aim for is, of course, a low error rate on both those sets. The main problem is to define what a low error rate is - in some cases it can be 1% and in others it can be as much as 10% or more. When training NN, it is helpful to establish a benchmark against which to compare the performance of our model. Very often this will be the level of human effectiveness in performing this task. Then we try to make sure that our algorithm has an error close to our reference level during training. If we have achieved this goal, but the error rate increases significantly when we verify it on a hold-out set, it may mean that we overfitted (high variance). On the other hand, if our model performs poorly on the training set as well as cross-validation, it is probably too weak and has a high bias. This issue is, of course, much more complicated and so extensive that it could be the subject of a separate article, but this basic information should be sufficient to understand the following analysis.", "There are many methods that can help when our neural network has a high variance. Some of them, such as obtaining more data, are quite universal and work well every time. Others, such as regularization, require a lot of finesse and experience. Imposing too many restrictions on our NN may compromise its ability to learn effectively. Let\u2019s now try to look inside some of the most popular methods of reducing overfitting and discuss the reasons they work.", "One of the first methods we should try when we need to reduce overfitting is regularisation. It involves adding an extra element to the loss function, which punishes our model for being too complex or, in simple words, for using too high values in the weight matrix. This way we try to limit its flexibility, but also encourage it to build solutions based on multiple features. Two popular versions of this method are L1 - Least Absolute Deviations (LAD) and L2 - Least Square Errors (LS). Equations describing these regularisations are given below.", "In most cases the use of L1 is preferable, because it reduces the weight values of less important features to zero, very often eliminating them completely from the calculations. In a way, it is a built-in mechanism for automatic featur selection. Moreover, L2 does not perform very well on datasets with a large number of outliers. The use of value squares results in the model minimizing the impact of outliers at the expense of more popular examples.", "Now let\u2019s look inside the two neural networks we used in our example on bias and variance. In one of them, as I mentioned earlier, I used regularisation to eliminate overfitting. I found it an interesting idea to visualize the matrix of weights in the three-dimensional space and compare the results obtained between the models with and without regularization. I also performed a simulation on many models using regularization, changing the \u03bb value to verify its impact on the values contained in the weight matrix. (Honestly, I did it also because I think it looks super cool and I don\u2019t want you to fall asleep.) Matrix row and column indexes correspond to horizontal axis values and weights are interpreted as vertical coordinates.", "In the previously mentioned formulas for regularisation in both versions of L1 and L2, I introduced hyperparameter \u03bb \u2014 also called regularization rate. When choosing its value we try to hit the sweet spot between simplicity of our model and fitting it to the training data. Increasing the \u03bb value also increases the regularisation effect. In Figure 4. we can immediately notice that the planes obtained for the model without regulation, and models with a very low \u03bb coefficient value are very \u201cturbulent\u201d. There are many peaks with significant values. After applying L2 regularization with higher hyperparameter value, the graph is flattening. Finally, we can see that setting the lambda value around 0.1 or 1 causes a drastic decrease in the value of weights in our model. I encourage you to check the source code used to create these visualizations.", "Another very popular method of regularization of neural networks is dropout. This idea is actually very simple - every unit of our neural network (except those belonging to the output layer) is given the probability p of being temporarily ignored in calculations. Hyper parameter p is called dropout rate and very often its default value is set to 0.5. Then, in each iteration, we randomly select the neurons that we drop according to the assigned probability. As a result, each time we work with a smaller neural network. The visualization below shows an example of a neural network subjected to a dropout. We can see how in each iteration random neurons from second and fourth layer are deactivated.", "The effectiveness of this method is quite surprising and counterintuitive. After all, in the real world, the productivity of the factory will not increase if its manager, every day, randomly selects employees and sends them home. Let us look at this problem from the perspective of a single neuron. Since in each iteration, any input value can be randomly eliminated, the neuron tries to balance the risk and not to favour any of the features. As a result, the values in the weight matrix become more evenly distributed. The model wants to avoid a situation in which the solution it proposes, will no longer make sense, because it no longer has information flowing from an inactive feature.", "The graph below shows the change in accuracy values calculated on the test and cross-validation sets during subsequent iterations of learning process. We see right away that the model we get at the end is not the best we could have possibly create. To be honest, it is much worse than what we have had after 150 epochs. Why not interrupt the learning process before the model starts overfitting? This observation inspired one of the popular overfitting reduction method, namely early stopping.", "In practice, it is very convenient to sample our model every few iterations and check how well it works with our validation set. Every model that performs better than all the previous models is saved. We also set a limit, i.e. the maximum number of iterations during which no progress will be recorded. When this value is exceeded, the learning is stopped. Although early stopping allows for a significant improvement in the performance of our model, in practice, its application greatly complicates the process of optimization of our model. It is simply difficult to combine with other regular techniques.", "The ability to recognize that our neural network is overfitting and the knowledge of solutions that we can apply to prevent it from happening are fundamental. However, these are very broad topics and it is impossible to describe them in sufficient detail in one article. My goal, therefore, was to provide basic intuitions as to how tricks such as regularisation or dropout actually work. These topics were difficult for me to understand and I hope I helped you work them out. If you like this article follow me on Twitter and Medium and see other projects I\u2019m working on, on GitHub and Kaggle. Stay curious!", "ML Growth Engineer @ Roboflow / Founder @ makesense.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F953458db800a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://skalskip.medium.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "Piotr Skalski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0----953458db800a---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F953458db800a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&user=Piotr+Skalski&userId=11b65705ec0&source=-----953458db800a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F953458db800a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&source=-----953458db800a---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.publicdomainpictures.net/en/view-image.php?image=234572&picture=brain-cell", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba", "anchor_text": "opening article"}, {"url": "https://github.com/SkalskiP/ILearnDeepLearning.py", "anchor_text": "GitHub"}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": "Twitter"}, {"url": "https://medium.com/@piotr.skalski92", "anchor_text": "Medium"}, {"url": "https://github.com/SkalskiP", "anchor_text": "GitHub"}, {"url": "https://www.kaggle.com/skalskip", "anchor_text": "Kaggle"}, {"url": "https://github.com/SkalskiP", "anchor_text": ""}, {"url": "https://medium.com/@piotr.skalski92", "anchor_text": ""}, {"url": "https://twitter.com/PiotrSkalski92", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----953458db800a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----953458db800a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----953458db800a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----953458db800a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----953458db800a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F953458db800a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&user=Piotr+Skalski&userId=11b65705ec0&source=-----953458db800a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F953458db800a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&user=Piotr+Skalski&userId=11b65705ec0&source=-----953458db800a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F953458db800a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0----953458db800a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd74b5c17627&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&newsletterV3=11b65705ec0&newsletterV3Id=fd74b5c17627&user=Piotr+Skalski&userId=11b65705ec0&source=-----953458db800a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "Written by Piotr Skalski"}, {"url": "https://skalskip.medium.com/followers?source=post_page-----953458db800a--------------------------------", "anchor_text": "4.7K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://makesense.ai", "anchor_text": "makesense.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11b65705ec0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&user=Piotr+Skalski&userId=11b65705ec0&source=post_page-11b65705ec0----953458db800a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd74b5c17627&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreventing-deep-neural-network-from-overfitting-953458db800a&newsletterV3=11b65705ec0&newsletterV3Id=fd74b5c17627&user=Piotr+Skalski&userId=11b65705ec0&source=-----953458db800a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----953458db800a----0---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----953458db800a----0---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----953458db800a----0---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Piotr Skalski"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----953458db800a----0---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----953458db800a----0---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Gentle Dive into Math Behind Convolutional Neural NetworksMysteries of Neural Networks Part V"}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----953458db800a----0---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "12 min read\u00b7Apr 12, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&user=Piotr+Skalski&userId=11b65705ec0&source=-----79a07dd44cf9----0-----------------clap_footer----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9?source=author_recirc-----953458db800a----0---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "22"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79a07dd44cf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&source=-----953458db800a----0-----------------bookmark_preview----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----953458db800a----1---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----953458db800a----1---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----953458db800a----1---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----953458db800a----1---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----953458db800a----1---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----953458db800a----1---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----953458db800a----1---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----953458db800a----1-----------------bookmark_preview----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----953458db800a----2---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----953458db800a----2---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----953458db800a----2---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----953458db800a----2---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----953458db800a----2---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----953458db800a----2---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----953458db800a----2---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----953458db800a----2-----------------bookmark_preview----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----953458db800a----3---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----953458db800a----3---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=author_recirc-----953458db800a----3---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Piotr Skalski"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----953458db800a----3---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----953458db800a----3---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "Let\u2019s code a Neural Network in plain NumPyMysteries of Neural Networks Part III"}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----953458db800a----3---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": "10 min read\u00b7Oct 12, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae7e74410795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-code-a-neural-network-in-plain-numpy-ae7e74410795&user=Piotr+Skalski&userId=11b65705ec0&source=-----ae7e74410795----3-----------------clap_footer----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795?source=author_recirc-----953458db800a----3---------------------20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "47"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae7e74410795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-code-a-neural-network-in-plain-numpy-ae7e74410795&source=-----953458db800a----3-----------------bookmark_preview----20ab0146_8d2f_4e1f_a318_ce0981e89cd3-------", "anchor_text": ""}, {"url": "https://skalskip.medium.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "See all from Piotr Skalski"}, {"url": "https://towardsdatascience.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----0-----------------clap_footer----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----953458db800a----0-----------------bookmark_preview----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----953458db800a----1-----------------bookmark_preview----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----953458db800a----0---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----953458db800a----0-----------------bookmark_preview----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----1-----------------clap_footer----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----953458db800a----1---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----953458db800a----1-----------------bookmark_preview----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----953458db800a----2---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----953458db800a----2---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----953458db800a----2---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----953458db800a----2---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----953458db800a----2---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----953458db800a----2---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----953458db800a----2---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----953458db800a----2-----------------bookmark_preview----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----953458db800a----3---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----953458db800a----3---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----953458db800a----3---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----953458db800a----3---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----953458db800a----3---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----953458db800a----3---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----3-----------------clap_footer----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----953458db800a----3---------------------79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----953458db800a----3-----------------bookmark_preview----79102207_74c3_4ca7_90de_d7ac1d8aaaa7-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----953458db800a--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----953458db800a--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----953458db800a--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----953458db800a--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----953458db800a--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----953458db800a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----953458db800a--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----953458db800a--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----953458db800a--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}