{"url": "https://towardsdatascience.com/how-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6", "time": 1683003389.863876, "path": "towardsdatascience.com/how-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6/", "webpage": {"metadata": {"title": "How to extract facial expressions, head pose, and gaze from any Youtube video | by Jin Cheong, PhD | Towards Data Science", "h1": "How to extract facial expressions, head pose, and gaze from any Youtube video", "description": "A Python tutorial using Google Colab & OpenFace to extract facial expressions, head pose, and gaze from videos without installing a single program on your laptop."}, "outgoing_paragraph_urls": [{"url": "https://www.crunchbase.com/organization/emotient", "anchor_text": "Emotient", "paragraph_index": 0}, {"url": "https://medium.com/@jinhyuncheong/face-analysis-software-comparison-affectiva-affdex-vs-openface-vs-emotient-facet-5f91a4f12cbb?source=friends_link&sk=fa98a9ac38c56b3f15b837718b2aea05", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://link.springer.com/article/10.3758/s13428-017-0996-1", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://github.com/TadasBaltrusaitis/OpenFace", "anchor_text": "OpenFace", "paragraph_index": 1}, {"url": "https://colab.research.google.com/gist/jcheong0428/c16146b386ea60fab888b56e8e5ee747/openface_shared.ipynb", "anchor_text": "Google Colab Jupyter notebook", "paragraph_index": 1}, {"url": "http://www.youtube.com", "anchor_text": "Youtube", "paragraph_index": 1}, {"url": "https://gist.github.com/jcheong0428/c16146b386ea60fab888b56e8e5ee747", "anchor_text": "Here is the link to the notebook", "paragraph_index": 2}, {"url": "https://medium.com/tastespace", "anchor_text": "TasteSpace", "paragraph_index": 4}, {"url": "https://pioneer.app/", "anchor_text": "Pioneer.app", "paragraph_index": 4}, {"url": "https://github.com/TadasBaltrusaitis/OpenFace/wiki/Command-line-arguments", "anchor_text": "Click here to see the full list of command line functions and their arguments and parameters", "paragraph_index": 7}, {"url": "https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format", "anchor_text": "OpenFace wiki", "paragraph_index": 20}, {"url": "https://jincheong.medium.com/membership", "anchor_text": "https://jincheong.medium.com/membership", "paragraph_index": 22}], "all_paragraphs": ["How people feel, their thoughts, and their interests can be studied and analyzed by investigating people\u2019s facial expressions, head pose, and gaze information. There are numerous companies and computer vision algorithms that help extract these facial features from face videos including Emotient and Affectiva (comparison of their algorithms here, here), but very few companies provide these services for free and most companies require users to purchase a subscription or pay per minutes of the video.", "In this post, I share a free, easy-to-use, and robust alternative to paid services for facial feature extraction using OpenFace, a state of the art tool for facial action unit recognition, gaze estimation, facial landmark detection, and head pose estimation. Here I share instructions on how to use a Google Colab Jupyter notebook that allows you to setup OpenFace and extract facial features from any Youtube video without having to install a single package on your laptop.", "Here is the link to the notebook!", "You don\u2019t need to install anything on your laptop, but you still need to install the OpenFace package on your Colab instance. Unfortunately, this part can take a while (~40 minutes) which is the perfect amount of time for you to binge watch some Youtube videos to figure out which one you want to extract facial features from. Oh, and you\u2019ll probably need to have a Google account.", "Find a Youtube video you\u2019d like to analyze. It could be a video with the face of one person or it could be faces of multiple individuals. Just be careful to avoid videos that have too small of a face which gives most algorithms a hard time to finding the face. In this tutorial, we will extract facial features from a short skit I did with my colleagues to pitch an app idea, TasteSpace, for Pioneer.app. The following code will show you the video of interest.", "The next few lines of code download the video and trim the first few seconds (10 seconds) of the video. This is just to save processing time so feel free remove the -t 10 flag on the fifth line if you want to process the whole video.", "Now we will use FaceLandmarkVidMulti which can extract facial features from multiple faces simultaneously.", "If you only have 1 face at a time in your video, then you could use FeatureExtraction instead or FaceLandmarkImg if you\u2019d like to extract features from an image. Click here to see the full list of command line functions and their arguments and parameters.", "You can visualize the results with the following code where you can inspect right from the notebook if the outputs make sense.", "You can now download the extracted facial features by opening the Files tab on the left menu of your Colab notebook and Download the file in the folder processed/videos.csv.", "Hopefully, this was an interesting exercise on how you can use Google Colab and OpenFace to extract facial features from any Youtube video in a few minutes (after installation). If you are interested in learning more about how to analyze the synchrony between individuals in this kind of facial expression data, feel free to check out my previous post on how to do just that.", "If you are still reading this, you might be interested in getting more info on how to handle the outputs of OpenFace, here are some additional code to get you started.", "First, we\u2019ll load the data into a Pandas dataframe, rename the columns to get rid of empty spaces, asses the shape of the dataframe, highest frame number of the data, and plot the head of the data.", "You might notice column face_id which attempts to differentiate between individuals in the video.", "Looks like we get 4 unique faces instead of 3!", "We can further assess what degree of confidence the algorithm had for detecting each face with the following function.", "We see that face_id==3 has the lowest confidence which is likely to be spurious faces that was detected. Let\u2019s further check this by plotting the location of faces throughout the clip.", "We can see that in the plot to the left that indeed the face with face_id==3 was somewhere where a face did not exist. We can threshold the outputs based on an arbitrary confidence level (here we used 80%) and we can see in the lefthand plot that we got rid of the spurious face.", "Now let\u2019s plot what the trajectory of each action unit predictions look like over time for each face. Then we\u2019ll print how similarly people were smiling (action unit 12) over time in the video.", "Lastly we can plot where each individual is looking at. This is probably not the best way to plot radians angles but you can still get a sense of where each face is looking. This passes the sanity check that the face on the left (face_id==2) looks mostly towards the right from the origin (0,0), face on the right (face_id==0) looks towards the left, and the face in the middle looks both ways.", "If you\u2019d like to learn more about the different outputs, I highly recommend reading more about what each output represents in the OpenFace wiki.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Human behavior & machine learning enthusiast || Cognitive Neuroscience PhD turned data nerd || https://jincheong.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2aa6590c2bb6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jincheong.medium.com/?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": ""}, {"url": "https://jincheong.medium.com/?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "Jin Cheong, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed25f2e73793&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&user=Jin+Cheong%2C+PhD&userId=ed25f2e73793&source=post_page-ed25f2e73793----2aa6590c2bb6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2aa6590c2bb6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2aa6590c2bb6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ferventjan?utm_source=medium&utm_medium=referral", "anchor_text": "Fervent Jan"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.crunchbase.com/organization/emotient", "anchor_text": "Emotient"}, {"url": "https://medium.com/@jinhyuncheong/face-analysis-software-comparison-affectiva-affdex-vs-openface-vs-emotient-facet-5f91a4f12cbb?source=friends_link&sk=fa98a9ac38c56b3f15b837718b2aea05", "anchor_text": "here"}, {"url": "https://link.springer.com/article/10.3758/s13428-017-0996-1", "anchor_text": "here"}, {"url": "https://github.com/TadasBaltrusaitis/OpenFace", "anchor_text": "OpenFace"}, {"url": "https://colab.research.google.com/gist/jcheong0428/c16146b386ea60fab888b56e8e5ee747/openface_shared.ipynb", "anchor_text": "Google Colab Jupyter notebook"}, {"url": "http://www.youtube.com", "anchor_text": "Youtube"}, {"url": "https://gist.github.com/jcheong0428/c16146b386ea60fab888b56e8e5ee747", "anchor_text": "Here is the link to the notebook"}, {"url": "https://medium.com/tastespace", "anchor_text": "TasteSpace"}, {"url": "https://pioneer.app/", "anchor_text": "Pioneer.app"}, {"url": "https://medium.com/tastespace", "anchor_text": "TasteSpace"}, {"url": "https://github.com/TadasBaltrusaitis/OpenFace/wiki/Command-line-arguments", "anchor_text": "Click here to see the full list of command line functions and their arguments and parameters"}, {"url": "https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9", "anchor_text": "Four ways to quantify synchrony between time series dataSample code and data to compute synchrony metrics including Pearson correlation, time-lagged cross correlations\u2026towardsdatascience.com"}, {"url": "https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format", "anchor_text": "OpenFace wiki"}, {"url": "https://medium.com/@jinhyuncheong", "anchor_text": "Jin Hyun Cheong - MediumRead writing from Jin Hyun Cheong on Medium. Human behavior & data science enthusiast || PhD in Cognitive Neuroscience\u2026medium.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2aa6590c2bb6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/synchrony?source=post_page-----2aa6590c2bb6---------------synchrony-----------------", "anchor_text": "Synchrony"}, {"url": "https://medium.com/tag/facial-expressions?source=post_page-----2aa6590c2bb6---------------facial_expressions-----------------", "anchor_text": "Facial Expressions"}, {"url": "https://medium.com/tag/python?source=post_page-----2aa6590c2bb6---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/youtube?source=post_page-----2aa6590c2bb6---------------youtube-----------------", "anchor_text": "YouTube"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2aa6590c2bb6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&user=Jin+Cheong%2C+PhD&userId=ed25f2e73793&source=-----2aa6590c2bb6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2aa6590c2bb6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&user=Jin+Cheong%2C+PhD&userId=ed25f2e73793&source=-----2aa6590c2bb6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2aa6590c2bb6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2aa6590c2bb6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2aa6590c2bb6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2aa6590c2bb6--------------------------------", "anchor_text": ""}, {"url": "https://jincheong.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jincheong.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jin Cheong, PhD"}, {"url": "https://jincheong.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "714 Followers"}, {"url": "https://jincheong.medium.com/membership", "anchor_text": "https://jincheong.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed25f2e73793&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&user=Jin+Cheong%2C+PhD&userId=ed25f2e73793&source=post_page-ed25f2e73793--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fffeafbcaaa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-extract-facial-expressions-head-pose-and-gaze-from-any-youtube-video-2aa6590c2bb6&newsletterV3=ed25f2e73793&newsletterV3Id=ffeafbcaaa7f&user=Jin+Cheong%2C+PhD&userId=ed25f2e73793&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}