{"url": "https://towardsdatascience.com/the-mechanics-of-attention-mechanism-f6e9805cca66", "time": 1683004191.249543, "path": "towardsdatascience.com/the-mechanics-of-attention-mechanism-f6e9805cca66/", "webpage": {"metadata": {"title": "The Mechanics of Attention Mechanism in Flowcharts | by Arian Prabowo | Towards Data Science", "h1": "The Mechanics of Attention Mechanism in Flowcharts", "description": "Epistemic status: I am trying to understand attention mechanism properly, at a level where I know how to implement it to any kind of data/problem or any modality, and how to tweak it to improve it\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f", "anchor_text": "Most explanations", "paragraph_index": 1}, {"url": "https://medium.com/heuritech/attention-mechanism-5aba9a2d4727", "anchor_text": "on the internet", "paragraph_index": 1}, {"url": "https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512", "anchor_text": "do a good job", "paragraph_index": 1}, {"url": "https://pathmind.com/wiki/attention-mechanism-memory-network", "anchor_text": "in explaining", "paragraph_index": 1}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/", "anchor_text": "what attention mechanic", "paragraph_index": 1}, {"url": "https://www.kdnuggets.com/2019/08/deep-learning-transformers-attention-mechanism.html", "anchor_text": "is about", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "Natural Language Processing (NLP)", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Machine_translation", "anchor_text": "machine language translation task", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Seq2seq", "anchor_text": "encoder-decoder sequence-to-sequence (seq2seq)", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "RNN", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "LSTM", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Gated_recurrent_unit", "anchor_text": "GRU", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Deep_learning", "anchor_text": "deep learning", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Artificial Neural Network (ANN)", "paragraph_index": 2}, {"url": "https://medium.com/datadriveninvestor/how-is-machine-learning-different-and-why-it-is-better-d5671b52dd65", "anchor_text": "data driven / Machine Learning paradigm of Artificial Intelligence (AI), instead of the model driven paradigm", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word represented as a vector", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks", "anchor_text": "Bidirectional RNN", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Count_noun", "anchor_text": "many", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Mass_noun", "anchor_text": "much", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Grammatical_gender", "anchor_text": "gender", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Grammatical_number", "anchor_text": "number", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Synthetic_language", "anchor_text": "are contained in one word", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Analytic_language", "anchor_text": "another language where this information must be split into multiple words", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Copula_(linguistics)", "anchor_text": "the verb \u201cto be\u201d", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Zero_copula", "anchor_text": "not universal", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Word_order", "anchor_text": "word order", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Analytic_language", "anchor_text": "analytic language", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Grammatical_aspect", "anchor_text": "grammatical aspect", "paragraph_index": 11}, {"url": "https://en.wiktionary.org/wiki/dia#Indonesian", "anchor_text": "Indonesian \u201cdia\u201d", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Neural machine translation by jointly learning to align and translate", "paragraph_index": 20}, {"url": "https://www.sichenzhao.com/", "anchor_text": "Sichen", "paragraph_index": 21}, {"url": "http://www.arianprabowo.com", "anchor_text": "www.arianprabowo.com", "paragraph_index": 24}], "all_paragraphs": ["Epistemic status: I am trying to understand attention mechanism properly, at a level where I know how to implement it to any kind of data/problem or any modality, and how to tweak it to improve it. This article is like my own notes to teach myself. Finally, since I have not implemented attention mechanism myself, I am under-qualified and over-opinionated in writing this.", "Motivation: Most explanations on the internet do a good job in explaining what attention mechanic is about, but not the exact mechanical details of how it works, like which vector goes with which matrix. Although the original paper does that really well, it is like a very condensed version, typical of academic writing, which means that I need a lot of effort to unpack it. This is that.", "The original paper is tackling a specific Natural Language Processing (NLP) problem, which is a machine language translation task, translating a sentence from a source language to a target language. It is formulated as an encoder-decoder sequence-to-sequence (seq2seq) architecture, which are usually done using variations of RNN like LSTM and GRU, which are a class of deep learning algorithms, a part of Artificial Neural Network (ANN), which is the data driven / Machine Learning paradigm of Artificial Intelligence (AI), instead of the model driven paradigm. The rest of this article is assuming that you are familiar with the these concepts.", "Bahdanau et. al. described a typical RNN encoder unit through the following equation. I unpacked that equation with the following diagram. x_t is a word represented as a vector. Function f(.) are usually some type of RNN, like LSTM and GRU. In the paper, they are using Bidirectional RNN (BiRNN). Finally, h_t are the RNN hidden states.", "The full encoder architecture is as below, where c is the vector that encodes the entire sentence. Usually, c = h_T, which is basically just the last RNN hidden state.", "Below is the full structure of a typical encoder-decoder seq2seq RNN with exaggerated mistakes in translation. Imagine a language where information about distance (here vs there), countability (many vs much), and gender/age/number (men vs girl) are contained in one word, being translated to another language where this information must be split into multiple words that might not even be adjacent to one another, or vice versa. Moreover, there is also the issue of the verb \u201cto be\u201d, a feature that is not universal across all languages. Then, we might be able to see these kind of mistakes.", "(I\u2019m pretending that the function g(.) produces the next word (technically a word vector, because embedding, but I\u2019m going to just call it \u201cwords\u201d from now on). In the paper, it is actually vector of probability of all words. But that\u2019s not an important distinction in attention mechanism.)", "Since the attention mechanism is only applied to the decoder part, we can ignore the encoder part and focus on H which is the set of all h_j. (There is a notation change in the original paper, so I\u2019m reflecting it here. \u201cj\u201d is the index for the encoder part, \u201ci\u201d is the index for decoder part, which used to be \u201ct\u201d.)", "Firstly, in the attention mechanism, we are going to use H, the set of all h_j (the set of all the hidden states) instead of just the last one, so let\u2019s keep it there. Secondly, to simplify things, we are going to focus on one RNN decoder unit, as shown on the right of the figure above.", "The function g(.) (an RNN unit) has the main responsibility to produce the next word (vector, embedded) y_i. That decision is made by combining information from c (representing the ENTIRE SENTENCE in the original language), y_i (the previous word), s_i (the previous hidden state).", "What the authors realized is that using \u201cc\u201d is a really bad way to do things. First of all, when a human tries to translate the next word (given they already got the word order figured out), they will only focus (read: PAY ATTENTION) on one (at most, a few) words, not the whole sentence. Yet, by using \u201cc\u201d, as an input, the architecture of the network/model is somewhat suggesting that we should try to figure out the next word from some sort of holistic understanding of the ENTIRE SENTENCE, which is most definitely not the case.", "(Why few words instead of one? Depending on the language (especially when translating from an analytic language to synthetic ones), we have to figure out which form to use depending on the context. Many language changes the form depending on many different grammatical aspect such as singular/plural, male/female/neuter, past/present/future etc. For example, imagine translating from a language where the 3rd person pronoun doesn\u2019t distinguish between gender, (like Indonesian \u201cdia\u201d to English he/she). The network has to pay attention to the word \u2018dia\u2019 and any word that might give a clue to the gender of the person in question, in order to translate it properly.)", "The gist of attention is basically to replace the general \u201cc\u201d with a CONTEXT VECTOR c_i. Instead of giving the same \u201cc\u201d to the decoder at every step, we will give a different c_i to the decoder at every step. The hope is that this c_i gives attention to the relevant words in the source sentence.", "In order to do that, the function c(.) that generates context vector c_i needs detailed information about the original sentence (hence why we use the annotation vectors H, instead of just the last h_j) and the current decoder RNN hidden state s_i (hence the red arrow). Ideally, the context vector c_i will contain a lot of information about the relevant words, and little information about other less relevant words.", "There are 3 important outputs at every decoding steps:", "[I\u2019m not sure if this is the true motivation, but\u2026] It make sense to have 3 different modules, where each module is responsible for each output: c(.), f(.), and g(.):", "The final question is, how to calculate the context vectors c_i? That is best explained through the full blown model below (which, given the annotation, I hope is self-explanatory).", "The most important part is the alignment score e_i,j . (For some reason that I don\u2019t understand, it is also referred to as energy). It calculates the relevance of each word in the source language (represented by annotation h_j) based on the current decoder RNN hidden state. Then, we give more weights to more relevant words, and less weight to less relevant words. This is equivalent to giving attention to more relevant words. The next step is simply to \u2018normalize\u2019 the weights using softmax. (For some reason that I don\u2019t understand, it is equivalent to probability). Finally, we calculate the weighted average of all the words in source language sentence, resulting in the context vector c_i.", "To obtain the famous alignment matrix figure, simply plot all of the a_i,j where i is the column and j is the row.", "The attention mechanism that is being described here can be thought of as soft attention, as it is using softmax. If were to use the max(.) function, then c_i would just be the most relevant word vector. This might be less advantageous, as some words are aligned with multiple words in another language. More importantly, we cannot use backpropagation through max(.), while we can do the same with softmax.", "[1] D. Bahdanau, K. Cho and Y. Bengio, Neural machine translation by jointly learning to align and translate (2015), International Conference on Learning Representations (ICLR)", "To Sichen for teaching me how attention works.", "All images belong to me unless otherwise mentioned in the caption.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in Machine Learning, Deep Learning, Graph & Spatiotemporal data. Degree in physics, physiology, teaching. www.arianprabowo.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff6e9805cca66&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://arian-prabowo.medium.com/?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": ""}, {"url": "https://arian-prabowo.medium.com/?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "Arian Prabowo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5910f3063431&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&user=Arian+Prabowo&userId=5910f3063431&source=post_page-5910f3063431----f6e9805cca66---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6e9805cca66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6e9805cca66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "the original attention paper"}, {"url": "https://unsplash.com/@justinchrn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Justin Chrn"}, {"url": "https://unsplash.com/s/photos/attention?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f", "anchor_text": "Most explanations"}, {"url": "https://medium.com/heuritech/attention-mechanism-5aba9a2d4727", "anchor_text": "on the internet"}, {"url": "https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512", "anchor_text": "do a good job"}, {"url": "https://pathmind.com/wiki/attention-mechanism-memory-network", "anchor_text": "in explaining"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/", "anchor_text": "what attention mechanic"}, {"url": "https://www.kdnuggets.com/2019/08/deep-learning-transformers-attention-mechanism.html", "anchor_text": "is about"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "Natural Language Processing (NLP)"}, {"url": "https://en.wikipedia.org/wiki/Machine_translation", "anchor_text": "machine language translation task"}, {"url": "https://en.wikipedia.org/wiki/Seq2seq", "anchor_text": "encoder-decoder sequence-to-sequence (seq2seq)"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "RNN"}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "LSTM"}, {"url": "https://en.wikipedia.org/wiki/Gated_recurrent_unit", "anchor_text": "GRU"}, {"url": "https://en.wikipedia.org/wiki/Deep_learning", "anchor_text": "deep learning"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Artificial Neural Network (ANN)"}, {"url": "https://medium.com/datadriveninvestor/how-is-machine-learning-different-and-why-it-is-better-d5671b52dd65", "anchor_text": "data driven / Machine Learning paradigm of Artificial Intelligence (AI), instead of the model driven paradigm"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word represented as a vector"}, {"url": "https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks", "anchor_text": "Bidirectional RNN"}, {"url": "https://en.wikipedia.org/wiki/Count_noun", "anchor_text": "many"}, {"url": "https://en.wikipedia.org/wiki/Mass_noun", "anchor_text": "much"}, {"url": "https://en.wikipedia.org/wiki/Grammatical_gender", "anchor_text": "gender"}, {"url": "https://en.wikipedia.org/wiki/Grammatical_number", "anchor_text": "number"}, {"url": "https://en.wikipedia.org/wiki/Synthetic_language", "anchor_text": "are contained in one word"}, {"url": "https://en.wikipedia.org/wiki/Analytic_language", "anchor_text": "another language where this information must be split into multiple words"}, {"url": "https://en.wikipedia.org/wiki/Copula_(linguistics)", "anchor_text": "the verb \u201cto be\u201d"}, {"url": "https://en.wikipedia.org/wiki/Zero_copula", "anchor_text": "not universal"}, {"url": "https://en.wikipedia.org/wiki/Word_order", "anchor_text": "word order"}, {"url": "https://en.wikipedia.org/wiki/Analytic_language", "anchor_text": "analytic language"}, {"url": "https://en.wikipedia.org/wiki/Grammatical_aspect", "anchor_text": "grammatical aspect"}, {"url": "https://en.wiktionary.org/wiki/dia#Indonesian", "anchor_text": "Indonesian \u201cdia\u201d"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Neural machine translation by jointly learning to align and translate"}, {"url": "https://www.sichenzhao.com/", "anchor_text": "Sichen"}, {"url": "https://medium.com/tag/attention?source=post_page-----f6e9805cca66---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f6e9805cca66---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f6e9805cca66---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f6e9805cca66---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----f6e9805cca66---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff6e9805cca66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&user=Arian+Prabowo&userId=5910f3063431&source=-----f6e9805cca66---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff6e9805cca66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&user=Arian+Prabowo&userId=5910f3063431&source=-----f6e9805cca66---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6e9805cca66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff6e9805cca66&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f6e9805cca66---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f6e9805cca66--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f6e9805cca66--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f6e9805cca66--------------------------------", "anchor_text": ""}, {"url": "https://arian-prabowo.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://arian-prabowo.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arian Prabowo"}, {"url": "https://arian-prabowo.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "210 Followers"}, {"url": "http://www.arianprabowo.com", "anchor_text": "www.arianprabowo.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5910f3063431&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&user=Arian+Prabowo&userId=5910f3063431&source=post_page-5910f3063431--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbe87cda0c143&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mechanics-of-attention-mechanism-f6e9805cca66&newsletterV3=5910f3063431&newsletterV3Id=be87cda0c143&user=Arian+Prabowo&userId=5910f3063431&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}