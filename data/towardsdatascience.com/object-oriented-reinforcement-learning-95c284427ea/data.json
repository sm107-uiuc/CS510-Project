{"url": "https://towardsdatascience.com/object-oriented-reinforcement-learning-95c284427ea", "time": 1683004127.7176902, "path": "towardsdatascience.com/object-oriented-reinforcement-learning-95c284427ea/", "webpage": {"metadata": {"title": "Object-based Reinforcement Learning | Towards Data Science", "h1": "Object-based Reinforcement Learning", "description": "Reinforcement Learning is a powerful formalism to model behaviour allowing us to solve many types of decision-making problems such as games, robotics, ..."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Reinforcement Learning constitutes a powerful formalism for modelling behaviour that is allowing us to solve many types of complex decision-making problems such as games, autonomous robotics, automated stock trading; which we used to think them to be nearly impossible to solve effectively with classic (even AI-based) algorithmic approaches.", "In this article we\u2019re going to explore a RL formalism named Object-Oriented Reinforcement Learning [1] which main peculiarity is to reason at a higher level of abstraction, that is at the level of world/environment\u2019s objects and relations about them. The formalism promises superior performances as well a considerable increase in sample-efficiency.", "This article is structured as follows:", "Reinforcement learning provides a set of tools to train an agent to take optimal actions within an environment (a real or simulated world) by trial and error (i.e. executing an action and then experiencing its effects), guided only by rewards. A reward is a scalar feedback signal that tells the agent how good the taken actions are. The agent collects rewards in order to learn which actions are the best to execute in a given situation or state.", "In brief, an agent learns how to solve a task that is decision-making problem. Atask is formally defined as a Markov Decision Process (MDP) described in terms of an environment and a reward function. The environment determines the dynamics of the world, i.e. what happens next when an action is executed in a particular situation (named a state), whilst the reward function tells the agent how much it has earned (or lost) by executing an action, given the current situation of the world (its state).", "The solution to a MDP is a policy \ud835\uded1 that determines the agent\u2019s behaviour. The optimal policy is one that \u201cmaximises the (discounted) sum of (expected) rewards\u201d, also known as the RL objective:", "Notice that this is a generic formula because the policy can be represented in many ways: as a table, linear or shallow combination of hand-engineered features, or as a deep neural network. Moreover, the policy can be stochastic or deterministic. A stochastic policy is a probability distribution over actions, in this case the RL objective maximises the expected sum of rewards. Conversely, a deterministic policy is a deterministic function of the state s in which only one action has nonzero probability.", "Deep RL favours deep neural network policies as they are both expressive (we can easily increase the model capacity) and computationally efficient in space and time, because computing the forward pass of a neural network is usually cheap. Expressed in these terms, the RL objective aims to find the policy\u2019s parameters \u03b8 that yields the highest possible (discounted) sum of cumulative rewards:", "Notice that in both objectives there is a discount factor \ud835\udec4, i.e. a real number between 0 and 1 that \u201cscales\u201d the future rewards. The discount factor is mandatory when the time-horizon T is infinite because it prevents the sum of rewards to diverge. In general, we always want to discount the rewards commonly by a factor of 0.9, 0.95, or 0.99.", "In order to compute the optimal policy we need a way to compare policies. The intuition is that a better policy reaches \u201cmore valuable\u201d states as well as picking out \u201cmore rewarding\u201d actions:", "These two functions are the basic ingredients of the Bellman Optimality Equation that finds some optimal policy. For small problems the equation can be easily solved with dynamic programming. Instead, for high-dimensional and continuous MDPs we often use neural networks to approximate V and/or Q used by model-free methods (e.g. Q-Learning, TD-Learning, Policy Gradients, Actor-Critic) and model-based methods (e.g. Dyna) to find optimal policies for the given task.", "Deep RL is achieving several breakthroughs AI, it is very promising but not perfect because it inherits issues from both deep learning and classic reinforcement learning. The main flaws are:", "These five issues are general limitations of modern reinforcement learning algorithms. Overcoming these limitations is fundamental to enable RL in production systems. For further understanding refer to these two papers [2] and [3] which have inspired this section.", "Object-oriented RL relies on objects and their interactions to devise an optimal policy for a given task. Both objects and interactions form a representation of the task\u2019s environment.", "In this context, an object is whatever relevant entity that appears in the task\u2019s environment, which is characterised by some set of attributes. When two objects interact in some way a change in value of one or multiple attributes in either or both interacting objects can occur, this is known as an effect.", "Representing task\u2019s environments as a set of interacting objects may facilitate the reuse of prior knowledge as well as offering important opportunities to:", "So, objects could help mitigate some fundamental issues of RL but, actually, there are some important prior issues in this approach which we need to overcome in order to implement such framework.", "Firstly, we have to detect objects in order to be able to reason on them. This should be done in an unsupervised way because we don\u2019t want to hand-engineering things as well as introducing problem-specific prior knowledge; we would like to use unsupervised object detection/scene understanding methods that are as widely applicable as possible.", "Secondly, we should be able to predict some attributes of the detected objects in order to eventually constrain them for safety purpose. Lastly and more importantly, we need some way to infer interactions among them. This is the biggest issue as it is not completely clear how to estimate such interactions in an unsupervised way, thus without prior knowledge.", "Let\u2019s suppose we are able to detect objects, attributes, and to estimate interactions. How do we represent them? And, how do we update them? Well, interactions can naturally arrange objects into graph-like structures. So, one possible approach would be to model both as graph neural networks!", "A graph neural network (or graph network) is made up of nodes (i.e. objects\u2019 attributes), edges (i.e. interactions between pairs of objects), and an optional global set of attributes. A graph network (GN) takes a graph as input and returns a graph as output. The structure of the resulting output graph can be different from the one of the input graph.", "A GN is a natural fit to crunch object-based representations as they can be structured as graphs. Thus, a GN could ideally substitute dense, convolutional, or recurrent neural networks to represent agents\u2019 policies.", "Two promising works based on OO-RL, (1) Strategic Object Oriented Reinforcement Learning [8], and (2) COBRA [9], demonstrate the effectiveness of some of the ideas discussed in this section. Further research efforts are needed to fully leverage objects within RL methods.", "There are a lot of things to discuss and even more to discover! Object-oriented RL is a pretty unexplored subfield of RL that in my opinion deserves more attention from the research community. Moreover, further improvements of unsupervised computer vision methods such as object detection and scene understanding could be of fundamental importance for making OO-RL successful.", "Ideally, OO-RL has the potential to help mitigate the fundamental limitations of modern deep RL methods. Solving these issues would push RL-based AI applications a step toward large scale adoption.", "Hope the article was interesting enough for you. Please comment for any doubt or clarification. There are a lot of concepts to unpack in these words, so it can be confusing at first. See you next time!", "[2] McAllister, Rowan, et al. \u201cConcrete problems for autonomous vehicle safety: Advantages of bayesian deep learning.\u201d International Joint Conferences on Artificial Intelligence, Inc., 2017.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in Data Science and Computation @Alma Mater Studiorum"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F95c284427ea&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----95c284427ea--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----95c284427ea--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://luca-anzalone.medium.com/?source=post_page-----95c284427ea--------------------------------", "anchor_text": ""}, {"url": "https://luca-anzalone.medium.com/?source=post_page-----95c284427ea--------------------------------", "anchor_text": "Luca Anzalone"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f6569b0de4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&user=Luca+Anzalone&userId=1f6569b0de4a&source=post_page-1f6569b0de4a----95c284427ea---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95c284427ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95c284427ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@franckinjapan?utm_source=medium&utm_medium=referral", "anchor_text": "Franck V."}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf", "anchor_text": "Source"}, {"url": "https://www.math.csi.cuny.edu/~mvj/MTH513/lectures/Lecture22.html#/", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/2001.02407", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1806.01261.pdf?utm_campaign=nathan.ai%20newsletter&utm_medium=email&utm_source=Revue%20newsletter", "anchor_text": "Source"}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "anchor_text": "link"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "link"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----95c284427ea---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----95c284427ea---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/graph-neural-networks?source=post_page-----95c284427ea---------------graph_neural_networks-----------------", "anchor_text": "Graph Neural Networks"}, {"url": "https://medium.com/tag/unsupervised-learning?source=post_page-----95c284427ea---------------unsupervised_learning-----------------", "anchor_text": "Unsupervised Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----95c284427ea---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F95c284427ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&user=Luca+Anzalone&userId=1f6569b0de4a&source=-----95c284427ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F95c284427ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&user=Luca+Anzalone&userId=1f6569b0de4a&source=-----95c284427ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95c284427ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----95c284427ea--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F95c284427ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----95c284427ea---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----95c284427ea--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----95c284427ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----95c284427ea--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----95c284427ea--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----95c284427ea--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----95c284427ea--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----95c284427ea--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----95c284427ea--------------------------------", "anchor_text": ""}, {"url": "https://luca-anzalone.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://luca-anzalone.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Luca Anzalone"}, {"url": "https://luca-anzalone.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "96 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f6569b0de4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&user=Luca+Anzalone&userId=1f6569b0de4a&source=post_page-1f6569b0de4a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F72dec1d68cf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-oriented-reinforcement-learning-95c284427ea&newsletterV3=1f6569b0de4a&newsletterV3Id=72dec1d68cf4&user=Luca+Anzalone&userId=1f6569b0de4a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}