{"url": "https://towardsdatascience.com/sample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f", "time": 1683018017.107297, "path": "towardsdatascience.com/sample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f/", "webpage": {"metadata": {"title": "Autonomous grasping robot with Deep Reinforcement Learning | Towards Data Science", "h1": "How to build self-learning grasping robot?", "description": "Combination of deep neural networks with reinforcement learning to achieve autonomous robotics grasping. Shaped-reward and curriculum learning for faster convergence"}, "outgoing_paragraph_urls": [{"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "http://rail.eecs.berkeley.edu/deeprlcourse/", "paragraph_index": 54}, {"url": "https://youtu.be/2pWv7GOvuf0", "anchor_text": "https://youtu.be/2pWv7GOvuf0", "paragraph_index": 55}, {"url": "http://pybullet.org.", "anchor_text": "http://pybullet.org.", "paragraph_index": 56}], "all_paragraphs": ["In this post, I will explain my 1-year experience of working with RL on autonomous robotics manipulation. It is always hard to start a big project which requires many moving parts. It was undoubtedly the same in this project. I want to pass the knowledge I gathered through this process to help others overcome the initial inertia.", "In the beginning, it was tough for me to judge the difficulty of different components of the problem. I underestimated the effort required for some parts and overestimated others. I will explain how RL can be cumbersome and straightforward to work with at the same time.", "The below video shows how our agent can transfer the learned policy to different domains and different environments.", "And the link to the GitHub repo:", "Humanlike manipulation is a challenging task for today\u2019s robots. Robots are still performing based on a manually designed controller specifically designed for only one problem at hand. They lack the following skills:", "Robots in the future should know how to manipulate a Rubik cube and kitchen appliances at the same time. They should not need supervision to learn new manipulation skills. The Reinforcement Learning (RL) framework promises end-to-end learning of these skills with no hand-coded controller design.", "Reinforcement Learning is a robust framework to learn complex behaviors. It has already shown great success on Atari games and locomotion problems. Significantly, the underactuated motions like tying shoelaces or wearing a shirt are hard to model and control with traditional methods [1]. RL can tackle these problems by sampling in a simulation and optimizing for the maximum reward.", "The biggest challenges for RL are the [2]:", "The following sections will address these problems to achieve an autonomous robotics manipulator in a simulation. The below components helped to tackle the challenges mentioned above.", "4. Raw depth sensor data as input", "5. Off-policy maximum entropy RL framework \u2014 e.g. SAC", "First of all, let\u2019s go through our problem definition. Our training environment features a gripper attempting to grasp randomly drawn objects from the floor. The gripper is deprived of an arm and a base. Accordingly, the computation of inverse kinematics is ignored. All training runs and experiments were done in PyBullet simulation [3].", "The observed state originates from the camera mounted on the gripper. The gripper is position controlled with continuous input between -1 to 1. An action is represented by [x, y, z, yaw angle, gripper open/close]. The inputted action represents the relative movement based on the position of the gripper. Episode terminates based on the termination goal of the training task; either when an agent lifts an object from clutter or when it successfully picks all objects from the ground.", "Observation is the eyes of the RL agent. Humans see and touch objects to map the environment in their brains. Like us, RL agent also needs some input to store the environment\u2019s state.", "There is a couple of go-to perception types. Some examples are: RGB images and autoencoder from [4][5]. Besides, we implemented depth image observation, which performed better than both of the reference works. We compared the performance of these observation types:", "The below diagram shows how we processed the depth observation from the environment to the learning algorithm. Our simulation environment returns observation with two components:", "We separated the depth image observation from the gripper width. We then fed the depth image into the convolutional network with a fully connected layer at the end. Finally, we concatenated the processed depth image with gripper width information, which returned the shape of 513.", "In contrast to supervised learning, RL creates its data to optimize. When the data was created, it may not point to the high reward region; optimizing it will not lead to good grasping behavior. Imagine optimizing the image-net with falsely labeled images; naturally, it won\u2019t perform right [6].", "RL has both advantages and disadvantages when it comes to data creation. In the RL setting, the data is expensive. It takes many simulator or physical robot iterations to create the data. But we do not need to label the data. Therefore, a well-defined agent can explore the environment on its own. In supervised learning, grasping scenarios need to be modeled tediously and labeled, which is quite challenging when the optimal policy is stochastic.", "Policy will render good data and optimizing this data will lead to a better policy [4]. While we strongly rely on the agent\u2019s random actions for good data, it might never explore the environment in a comprehensive way, leading to an incompetent policy due to the bad data.", "So, we aim to incentivize the agent to the good data region as fast as possible. For this purpose, we used the following techniques:", "They both contributed to the sample efficiency by creating more meaningful data early on in training.", "Curriculum learning governs the difficulty of the environment to facilitate learning. Like our school curriculum, first teaching arithmetic and later introducing differential math. Our curriculum strategy gets more challenging with the success rate of the RL agent. Curriculum strategy modifies the following environment features:", "In the beginning, it is simpler for the agent to explore the terminal state and the intermediate goals in a comfortable setting. When we slightly change the terminal state, it can still extrapolate from what it already knew to a harder environment.", "The shape reward function has the same purpose as curriculum learning. It motivates the agent to explore the high reward region. Through intermediate rewards, it steers the agent to the terminal state.", "Agent receives an intermediate reward when it grasps an object. As soon as the agent lifts the object to a terminal state, it gets the terminal reward. We apply a time penalty until it reaches the terminal state. The sum of the intermediate reward and the time penalty must stay smaller than zero until the terminal state. Otherwise, the agent would exploit the intermediate reward and wait until the episode\u2019s end to get to the terminal state.", "As mentioned before, a shaped reward serves to lead the agent to the good data region. Good data provides better policy, and they reinforce each other during the learning to deliver the optimal policy.", "I think of the normalization as the activator of the observation and shaped reward functions. Without the normalization, agent is unlikely to make sense of the input and rewards that are fed to the neural nets. Especially when the input has different components, and the reward isn\u2019t sparse.", "Our environment\u2019s state is composed of depth sensor input and the gripper width information. Unnormalized state representation can lead to a false emphasis on the state components, giving more weight to the gripper width information than the depth-sensor data or vice-versa. Normalization helps to scale the observation components to the same level.", "\u201cRL uses training information that evaluates the actions taken rather than instructs by giving correct actions \u2014 This is what creates the need for active exploration, for an explicit search for good behavior.\u201d \u2013 Introduction to Reinforcement Learning \u2014 R. Sutton", "Exploration is innate in RL. The uncertainty on the estimation of the action values is unavoidable. Especially in our environment, where reward distribution over actions has a huge variance, we need to apply a sophisticated exploration strategy [7].", "SAC masters the exploration-exploitation trade-off in RL. We expect an RL algorithm to find a balance between exploring and exploiting. This optimal balance could mean finding the optimal policy or stuck at sub-optimal policies.", "Exploration states how flexible it is to try new actions, while exploitation is how confident it is to take a specific action. In most cases, those two concepts are firmly connected. If we explore enough, we could find newer, better actions that return more rewards. Still, if we are confident enough about the action-value estimation, we should stop exploring and start exploiting the greedy actions.", "SAC models the RL problem not just for the expected reward maximization but also the expected entropy at the same time. This nature provides the following advantages:", "1) Optimum entropy provides enhanced exploration behavior.", "SAC is the most robust algorithm we used. It required minimal hyperparameter tuning and sampling. The off-policy nature of the SAC algorithm enables us to use the samples from different policies. Therefore, we can store the samples in a replay buffer and use it as many times as possible. Similar to supervised learning, we draw batches of samples to find optimal actions. We stored the size of 1 million samples in the buffer, which allocated around 50GB of RAM. Be careful if you want to replicate our results; check if you have enough ram on your machine.", "Off-policy algorithms proved to be more sample efficient than on-policy RL counterparts, where we throw away the data, we use each episode and create new experiences for new episodes.", "We have two different training scenarios:", "In single object picking from clutter setup, the gripper needs to pick one random object to a predefined threshold to end the episode successfully. And for the table cleaning setup, it needs to pick each object in the environment to the same height threshold.", "Both scenes required different hyperparameters. For example, we needed to decrease the start object count from three to one for clearing the table scene. Also, neural network layers and buffer size are increased to match the increased complexity of the behavior.", "We aim to get the most generalized grasping model. This model should perform well with unseen objects and adapt to new domains. That\u2019s why we designed two test environments.", "With these different test setups, we can assess if the model generalizes and adapts new scenes and domains.", "We tested with both autoencoder, depth, and RGB-D input. Based on our tests, depth input performed the best. We believe the difference between autoencoder and depth perception lies in the interpretation loss of the depth image. Autoencoders compress the observation onto a latent-space. This compression causes the agent to misinterpret the depth of the objects.", "On the other hand, the depth perception layer is an online method; therefore, it corrects its network weights when a wrong interpretation occurs. The online perception layer also complies the end-to-end nature of the RL framework. Our depth perception layer\u2019s weights are updated to deliver a better-grasping policy; autoencoder\u2019s weights are immutable throughout learning.", "Although SAC is robust to different hyperparameter selections, we still updated some of the learning parameters to achieve a more significant result. Such as the buffer size. Buffer size is a critical hyperparameter, which directly affects the performance of the agent. The agent needs large enough samples/experiences in the buffer to learn, similar to supervised learning datasets. Usually, with complex behaviors, where exploration is a big part of the learning, a larger buffer size is meaningful.", "Different manipulation skills demand different hyperparameter tuning. More complex behaviors require a large buffer size and neural network layers. For example, the hyperparameters we used for single object picking from clutter did not work correctly for the table clearing task. We needed to increase the buffer size from 1m to 2m, and the neural network layer size from 64 to 128.", "Aside from the neural network\u2019s hyperparameters, we also changed the curriculum strategy\u2019s object count parameter from three to one. Agent in table clearing task couldn\u2019t explore the terminal state with three objects at the beginning of the training. Therefore, we had to decrease the object count to smoothen the transition from an easy setting to a more challenging environment.", "To sum up, in this article we covered how to approach the robotic bin picking problem with the help of RL. We mentioned the importance of:", "\u00b7 Leading the agent to the good data region as fast as possible", "\u00b7 Leveraging old experiences with off-policy updates", "\u00b7 Normalization of the observation and reward", "\u00b7 Raw depth pixel as observation to ensure end-to-end learning", "In general, RL can be cumbersome to work with because it\u2019s hard to debug. It\u2019s always good to start out simple. Try implementing a simplified version of your custom environment. First check that baseline RL algorithms can learn the simplified version. And then, gradually make the environment harder to see which parameters make the RL agent struggle to learn. This way you can guarantee that the agent\u2019s learning will not be bottlenecked, and you will not be stressed out to see your agent suffering :)", "For readers interested to learn more about RL can check out the below resources:", "\u00b7 Berkeley RL course from Sergey Levine: http://rail.eecs.berkeley.edu/deeprlcourse/", "\u00b7 The legendary course from David Silver course: https://youtu.be/2pWv7GOvuf0", "[3]E. Coumans and Y. Bai. PyBullet, a Python module for physics simulation for games, robotics and machine learning. http://pybullet.org. 2016\u20132020.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software engineer based in Munich. Interested in reinforcement learning in robotics application."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F71d5d1d4587f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://barisyazici.medium.com/?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": ""}, {"url": "https://barisyazici.medium.com/?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "Baris Yazici"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a84b0bf6cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&user=Baris+Yazici&userId=1a84b0bf6cb&source=post_page-1a84b0bf6cb----71d5d1d4587f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71d5d1d4587f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71d5d1d4587f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://github.com/BarisYazici/tum_masters_thesis/blob/master/final_report.pdf", "anchor_text": "https://github.com/BarisYazici/tum_masters_thesis/blob/master/final_report.pdf"}, {"url": "https://github.com/BarisYazici/deep-rl-grasping", "anchor_text": "BarisYazici/deep-rl-graspingTrain robotics model with integrated curriculum learning-based gripper environment. Choose from different perception\u2026github.com"}, {"url": "https://unsplash.com/@alandelacruz4?utm_source=medium&utm_medium=referral", "anchor_text": "ALAN DE LA CRUZ"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@amandadalbjorn?utm_source=medium&utm_medium=referral", "anchor_text": "Amanda Dalbj\u00f6rn"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@thomasbormans?utm_source=medium&utm_medium=referral", "anchor_text": "Thomas Bormans"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@element5digital?utm_source=medium&utm_medium=referral", "anchor_text": "Element5 Digital"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://sites.google.com/view/deep-rl-bootcamp/lectures", "anchor_text": "https://sites.google.com/view/deep-rl-bootcamp/lectures"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "http://rail.eecs.berkeley.edu/deeprlcourse/"}, {"url": "https://youtu.be/OMraS0GRWK0?t=1258", "anchor_text": "https://youtu.be/OMraS0GRWK0?t=1258"}, {"url": "https://youtu.be/2pWv7GOvuf0", "anchor_text": "https://youtu.be/2pWv7GOvuf0"}, {"url": "http://karpathy.github.io/2016/05/31/rl/", "anchor_text": "http://karpathy.github.io/2016/05/31/rl/"}, {"url": "http://underactuated.csail.mit.edu/rl_policy_search.html", "anchor_text": "http://underactuated.csail.mit.edu/rl_policy_search.html"}, {"url": "http://underactuated.mit.edu/", "anchor_text": "http://underactuated.mit.edu/"}, {"url": "http://arxiv.org/abs/1812.05905", "anchor_text": "http://arxiv.org/abs/1812.05905"}, {"url": "http://pybullet.org.", "anchor_text": "http://pybullet.org."}, {"url": "http://arxiv.org/abs/1806.10293", "anchor_text": "http://arxiv.org/abs/1806.10293"}, {"url": "https://doi.org/10.1109/LRA.2019.2896467", "anchor_text": "https://doi.org/10.1109/LRA.2019.2896467"}, {"url": "https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/", "anchor_text": "https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----71d5d1d4587f---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----71d5d1d4587f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----71d5d1d4587f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----71d5d1d4587f---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----71d5d1d4587f---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71d5d1d4587f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&user=Baris+Yazici&userId=1a84b0bf6cb&source=-----71d5d1d4587f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71d5d1d4587f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&user=Baris+Yazici&userId=1a84b0bf6cb&source=-----71d5d1d4587f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71d5d1d4587f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F71d5d1d4587f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----71d5d1d4587f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----71d5d1d4587f--------------------------------", "anchor_text": ""}, {"url": "https://barisyazici.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://barisyazici.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Baris Yazici"}, {"url": "https://barisyazici.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "35 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a84b0bf6cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&user=Baris+Yazici&userId=1a84b0bf6cb&source=post_page-1a84b0bf6cb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F26a7672e8a7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsample-efficient-robot-training-on-pybullet-simulation-with-sac-algorithm-71d5d1d4587f&newsletterV3=1a84b0bf6cb&newsletterV3Id=26a7672e8a7f&user=Baris+Yazici&userId=1a84b0bf6cb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}