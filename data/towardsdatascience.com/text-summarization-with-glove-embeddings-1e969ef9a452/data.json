{"url": "https://towardsdatascience.com/text-summarization-with-glove-embeddings-1e969ef9a452", "time": 1683007106.8312151, "path": "towardsdatascience.com/text-summarization-with-glove-embeddings-1e969ef9a452/", "webpage": {"metadata": {"title": "Text Summarization with GloVe Embeddings.. | by Sayak Misra | Towards Data Science", "h1": "Text Summarization with GloVe Embeddings..", "description": "This story is a continuation to our previous blog post, where we have discussed the basics of text summarization, the various approaches and how we implemented an encoder-decoder model(with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/lets-give-some-attention-to-summarising-texts-d0af2c4061d1", "anchor_text": "blog post", "paragraph_index": 0}, {"url": "https://projector.tensorflow.org/", "anchor_text": "Tensorflow projector", "paragraph_index": 7}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 8}, {"url": "http://nlp.stanford.edu/data/glove.6B.zip", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://gist.github.com/sayakmisra/9f12e5c86c38c69b2678b8b58e719082", "anchor_text": "code", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/lets-give-some-attention-to-summarising-texts-d0af2c4061d1", "anchor_text": "blog-post", "paragraph_index": 11}, {"url": "https://gist.github.com/sayakmisra/9f12e5c86c38c69b2678b8b58e719082", "anchor_text": "colab-notebook", "paragraph_index": 21}, {"url": "http://Salesken.ai", "anchor_text": "Salesken.ai", "paragraph_index": 26}], "all_paragraphs": ["This story is a continuation to our previous blog post, where we have discussed the basics of text summarization, the various approaches and how we implemented an encoder-decoder model(with attention) to solve the problem in hand.", "Just to recap, text summarization is a process of generating a concise and meaningful summary of text from multiple text resources such as books, news articles, blog posts, research papers, emails, and tweets.", "We have seen an encoder-decoder(seqtoseq) model is a perfect choice for summarization tasks, so we will continue with that architecture. Here in addition to that we will be using GloVe pre-trained word embeddings to give our model a head-start, and check if it really performs better in understanding the language semantics and in turn summarizing.", "\u201cWord embeddings\u201d are a family of natural language processing techniques aiming at mapping semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space.", "For instance, \u201ccoconut\u201d and \u201cpolar bear\u201d are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But \u201ckitchen\u201d and \u201cdinner\u201d are related words, so they should be embedded close to each other.", "Ideally, in a good embeddings space, the \u201cpath\u201d (a vector) to go from \u201ckitchen\u201d and \u201cdinner\u201d would capture precisely the semantic relationship between these two concepts. In this case the relationship is \u201cwhere x occurs\u201d, so you would expect the vector kitchen - dinner (difference of the two embedding vectors, i.e. path to go from dinner to kitchen) to capture this \"where x occurs\" relationship. Basically, we should have the vectorial identity: dinner + (where x occurs) = kitchen (at least approximately). If that's indeed the case, then we can use such a relationship vector to answer questions. For instance, starting from a new vector, e.g. \"work\", and applying this relationship vector, we should get sometime meaningful, e.g. work + (where x occurs) = office, answering \"where does work occur?\".", "Word embeddings are computed by applying dimensionality reduction techniques to datasets of co-occurence statistics between words in a corpus of text. This can be done via neural networks (the \u201cword2vec\u201d technique), or via matrix factorization.", "We can play with this beautiful Tensorflow projector, to get a better understanding of word embeddings.", "The two of the most common word embeddings are: Word2Vec and GloVe, and both of them are equally popular. But GloVe(\u201cGlobal Vectors for Word Representation\u201d) as the name suggests is better for preserving the global contexts as it creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words. Here for summarization the global context is a necessity, so we are moving ahead with GloVe but in most of the use-cases there is very little between the two to choose from.", "Specifically, we will use the 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. You can download them here (warning: following this link will start a 822MB download).", "Here is the complete code of the LSTM Encoder-Decoder model with Attention and GloVe embeddings added to it.", "We are not going through the details of the model architecture, as we have discussed it in our previous blog-post and in turn focusing on adding the GloVe embeddings to it and assessing the performance.", "First let\u2019s download and unzip the GloVe embeddings.", "Next, we compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:", "At this point we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:", "We load this embedding matrix into an Embedding layer. Note that we set trainable=False to prevent the weights from being updated during training. Previously we used the default Embedding layer, withtrainable=True , which learned the Embeddings through the training process.", "Here, we are using the 100 dimension GloVe embeddings and the embeddings are saved in glove.6B.100d.txt.", "An Embedding layer should be fed sequences of integers, i.e. a 2D input of shape (samples, indices). These input sequences should be padded so that they all have the same length in a batch of input data (although an Embedding layer is capable of processing sequence of heterogenous length, if you don't pass an explicit input_length argument to the layer).", "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]. This means that the output of the Embedding layer will be a 3D tensor of shape (samples, sequence_length, embedding_dim).", "We get a validation loss of around 2.25 which is similar to what we got without the GloVe embeddings.", "As we can see from the above examples, it performs quite well on most of the use-cases.", "Here is the colab-notebook of the complete implementation.", "Though this model might produce better summaries for some cases but it can not be affirmed that it has better results across all the examples. The main reason behind this is we have enough records here(100000 to be exact) for our Embedding layer in Encoder-Decoder model to learn the semantics of the language, so it performed quite well even without the pre-trained embeddings.", "Pre-trained models (like GloVe here) can be used with great improvements in results if we have a short dataset. If the number of records in the dataset are less, the Embedding layer won\u2019t be able to generate proper embeddings of it\u2019s own and in those cases using pre-trained embeddings will enhance the performance accuracy.", "Here we have seen how we can add pre-trained embeddings to our existing LSTM Encoder-Decoder architecture, though the results didn\u2019t spike up much, but it can perform brilliantly for a smaller dataset.", "We can further improve the summarising process by a huge margin if we can leverage the true essence of Transfer Learning, by using state-of-the pre-trained language models like: BERTSum, BART, T5. In our next story we shall look into the details of those model and how wonderfully they can summarize texts.", "Data Science enthusiast, working towards solving various NLP problems at Salesken.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1e969ef9a452&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@sayakmisra.misra?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Sayak Misra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcb2f933e1d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&user=Sayak+Misra&userId=fcb2f933e1d7&source=post_page-fcb2f933e1d7----1e969ef9a452---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e969ef9a452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&user=Sayak+Misra&userId=fcb2f933e1d7&source=-----1e969ef9a452---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e969ef9a452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&source=-----1e969ef9a452---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@anniespratt?utm_source=medium&utm_medium=referral", "anchor_text": "Annie Spratt"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/lets-give-some-attention-to-summarising-texts-d0af2c4061d1", "anchor_text": "blog post"}, {"url": "https://projector.tensorflow.org/", "anchor_text": "Tensorflow projector"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "http://nlp.stanford.edu/data/glove.6B.zip", "anchor_text": "here"}, {"url": "https://unsplash.com/@arianismmm?utm_source=medium&utm_medium=referral", "anchor_text": "Arian Darvishi"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://gist.github.com/sayakmisra/9f12e5c86c38c69b2678b8b58e719082", "anchor_text": "code"}, {"url": "https://towardsdatascience.com/lets-give-some-attention-to-summarising-texts-d0af2c4061d1", "anchor_text": "blog-post"}, {"url": "https://gist.github.com/sayakmisra/9f12e5c86c38c69b2678b8b58e719082", "anchor_text": "colab-notebook"}, {"url": "https://www.kaggle.com/snap/amazon-fine-food-reviews", "anchor_text": "https://www.kaggle.com/snap/amazon-fine-food-reviews"}, {"url": "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html", "anchor_text": "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"}, {"url": "https://projector.tensorflow.org/", "anchor_text": "https://projector.tensorflow.org/"}, {"url": "https://medium.com/tag/glove?source=post_page-----1e969ef9a452---------------glove-----------------", "anchor_text": "Glove"}, {"url": "https://medium.com/tag/lstm?source=post_page-----1e969ef9a452---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/text-summarization?source=post_page-----1e969ef9a452---------------text_summarization-----------------", "anchor_text": "Text Summarization"}, {"url": "https://medium.com/tag/encoder-decoder?source=post_page-----1e969ef9a452---------------encoder_decoder-----------------", "anchor_text": "Encoder Decoder"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----1e969ef9a452---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e969ef9a452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&user=Sayak+Misra&userId=fcb2f933e1d7&source=-----1e969ef9a452---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e969ef9a452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&user=Sayak+Misra&userId=fcb2f933e1d7&source=-----1e969ef9a452---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e969ef9a452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcb2f933e1d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&user=Sayak+Misra&userId=fcb2f933e1d7&source=post_page-fcb2f933e1d7----1e969ef9a452---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8f7d5c4f2189&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&newsletterV3=fcb2f933e1d7&newsletterV3Id=8f7d5c4f2189&user=Sayak+Misra&userId=fcb2f933e1d7&source=-----1e969ef9a452---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Written by Sayak Misra"}, {"url": "https://medium.com/@sayakmisra.misra/followers?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "49 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://Salesken.ai", "anchor_text": "Salesken.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcb2f933e1d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&user=Sayak+Misra&userId=fcb2f933e1d7&source=post_page-fcb2f933e1d7----1e969ef9a452---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8f7d5c4f2189&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-summarization-with-glove-embeddings-1e969ef9a452&newsletterV3=fcb2f933e1d7&newsletterV3Id=8f7d5c4f2189&user=Sayak+Misra&userId=fcb2f933e1d7&source=-----1e969ef9a452---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/checking-grammar-with-bert-and-ulmfit-1f59c718fe75?source=author_recirc-----1e969ef9a452----0---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=author_recirc-----1e969ef9a452----0---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=author_recirc-----1e969ef9a452----0---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Sayak Misra"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1e969ef9a452----0---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/checking-grammar-with-bert-and-ulmfit-1f59c718fe75?source=author_recirc-----1e969ef9a452----0---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Checking grammar with BERT and ULMFiT..Let\u2019s check how the two heavyweights of transfer learning perform on checking grammar."}, {"url": "https://towardsdatascience.com/checking-grammar-with-bert-and-ulmfit-1f59c718fe75?source=author_recirc-----1e969ef9a452----0---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "\u00b75 min read\u00b7Apr 27, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f59c718fe75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchecking-grammar-with-bert-and-ulmfit-1f59c718fe75&user=Sayak+Misra&userId=fcb2f933e1d7&source=-----1f59c718fe75----0-----------------clap_footer----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/checking-grammar-with-bert-and-ulmfit-1f59c718fe75?source=author_recirc-----1e969ef9a452----0---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f59c718fe75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchecking-grammar-with-bert-and-ulmfit-1f59c718fe75&source=-----1e969ef9a452----0-----------------bookmark_preview----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1e969ef9a452----1---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1e969ef9a452----1---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1e969ef9a452----1---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1e969ef9a452----1---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1e969ef9a452----1---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1e969ef9a452----1---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1e969ef9a452----1---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----1e969ef9a452----1-----------------bookmark_preview----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1e969ef9a452----2---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1e969ef9a452----2---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1e969ef9a452----2---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1e969ef9a452----2---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1e969ef9a452----2---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1e969ef9a452----2---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1e969ef9a452----2---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----1e969ef9a452----2-----------------bookmark_preview----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-to-speech-with-tacotron-2-and-fastspeech-using-espnet-3a711131e0fa?source=author_recirc-----1e969ef9a452----3---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=author_recirc-----1e969ef9a452----3---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=author_recirc-----1e969ef9a452----3---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Sayak Misra"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1e969ef9a452----3---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/text-to-speech-with-tacotron-2-and-fastspeech-using-espnet-3a711131e0fa?source=author_recirc-----1e969ef9a452----3---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "Text To Speech with Tacotron-2 and FastSpeech using ESPnet.Text-to-speech (TTS) as the name suggests, reads aloud text. It takes written words as input and converts them into audio. TTS can help\u2026"}, {"url": "https://towardsdatascience.com/text-to-speech-with-tacotron-2-and-fastspeech-using-espnet-3a711131e0fa?source=author_recirc-----1e969ef9a452----3---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": "\u00b75 min read\u00b7Sep 2, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a711131e0fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-to-speech-with-tacotron-2-and-fastspeech-using-espnet-3a711131e0fa&user=Sayak+Misra&userId=fcb2f933e1d7&source=-----3a711131e0fa----3-----------------clap_footer----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-to-speech-with-tacotron-2-and-fastspeech-using-espnet-3a711131e0fa?source=author_recirc-----1e969ef9a452----3---------------------88b08aa4_c563_4531_b59d_cd034f2f643c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a711131e0fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-to-speech-with-tacotron-2-and-fastspeech-using-espnet-3a711131e0fa&source=-----1e969ef9a452----3-----------------bookmark_preview----88b08aa4_c563_4531_b59d_cd034f2f643c-------", "anchor_text": ""}, {"url": "https://medium.com/@sayakmisra.misra?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "See all from Sayak Misra"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----0-----------------clap_footer----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----1e969ef9a452----0-----------------bookmark_preview----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----1-----------------clap_footer----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----1e969ef9a452----1-----------------bookmark_preview----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1e969ef9a452----0---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----1e969ef9a452----0-----------------bookmark_preview----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Angel Das"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in PythonIntroduction to embeddings in natural language processing using Artificial Neural Network and Gensim"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "\u00b713 min read\u00b7Nov 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&user=Angel+Das&userId=8418ab50405a&source=-----a8873b225ab6----1-----------------clap_footer----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----1e969ef9a452----1---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&source=-----1e969ef9a452----1-----------------bookmark_preview----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1e969ef9a452----2---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----1e969ef9a452----2---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----1e969ef9a452----2---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Jay Peterman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1e969ef9a452----2---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1e969ef9a452----2---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Make a Text Summarizer with GPT-3Quick tutorial using Python, OpenAI\u2019s GPT-3, and Streamlit"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1e969ef9a452----2---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "\u00b711 min read\u00b7Jan 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&user=Jay+Peterman&userId=9731dc608e6c&source=-----f0917a07189e----2-----------------clap_footer----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1e969ef9a452----2---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&source=-----1e969ef9a452----2-----------------bookmark_preview----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1e969ef9a452----3---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----1e969ef9a452----3---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----1e969ef9a452----3---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----1e969ef9a452----3---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1e969ef9a452----3---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1e969ef9a452----3---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----3-----------------clap_footer----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----1e969ef9a452----3---------------------030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----1e969ef9a452----3-----------------bookmark_preview----030dac59_6daa_4cb4_b0ee_5ac8376c15b0-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----1e969ef9a452--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}