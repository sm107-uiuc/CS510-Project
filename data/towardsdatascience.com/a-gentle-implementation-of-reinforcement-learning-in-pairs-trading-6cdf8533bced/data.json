{"url": "https://towardsdatascience.com/a-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced", "time": 1682996571.933232, "path": "towardsdatascience.com/a-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced/", "webpage": {"metadata": {"title": "A Gentle Implementation of Reinforcement Learning in Pairs Trading | by Wai | Towards Data Science", "h1": "A Gentle Implementation of Reinforcement Learning in Pairs Trading", "description": "This covers topics from concepts to implementation of RL in cointegration pair trading based on 1-minute stock market data. For the Reinforcement Learning here we use the N-armed bandit approach. The\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tiingo.com/", "anchor_text": "Tiingo", "paragraph_index": 2}, {"url": "https://api.tiingo.com/documentation/iex", "anchor_text": "REST IEX API", "paragraph_index": 2}, {"url": "https://pandas-datareader.readthedocs.io/en/latest/readers/tiingo.html", "anchor_text": "relevant tools", "paragraph_index": 6}, {"url": "https://api.tiingo.com/account/billing/pricing", "anchor_text": "subscription", "paragraph_index": 7}, {"url": "https://api.tiingo.com/account/api/token", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html", "anchor_text": "read_json", "paragraph_index": 10}, {"url": "https://python.readthedocs.io/fr/latest/library/asyncio-task.html", "anchor_text": "official example", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Geometric_Brownian_motion", "anchor_text": "Geometric Brownian Motion (GBM)", "paragraph_index": 40}, {"url": "https://math.stackexchange.com/questions/163470/generating-correlated-random-numbers-why-does-cholesky-decomposition-work", "anchor_text": "Cholesky Decomposition", "paragraph_index": 40}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "this", "paragraph_index": 58}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149", "anchor_text": "this post", "paragraph_index": 71}, {"url": "https://www.zipline.io/beginner-tutorial.html", "anchor_text": "Zipline", "paragraph_index": 80}, {"url": "https://github.com/quantopian/pyfolio", "anchor_text": "Pyfolio", "paragraph_index": 80}, {"url": "https://docs.python.org/3/library/abc.html", "anchor_text": "abstract base class", "paragraph_index": 89}, {"url": "https://github.com/tensorflow/agents", "anchor_text": "TF-Agents", "paragraph_index": 94}, {"url": "https://www.youtube.com/watch?v=-TTziY7EmUA", "anchor_text": "this", "paragraph_index": 94}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf", "anchor_text": "article", "paragraph_index": 110}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df", "anchor_text": "article", "paragraph_index": 111}], "all_paragraphs": ["This covers topics from concepts to implementation of RL in cointegration pair trading based on 1-minute stock market data. For the Reinforcement Learning here we use the N-armed bandit approach. The code is expandable so you can plug any strategies, data API or machine learning algorithms into the tool if you follow the style.", "Takeaway:1. Expandable infrastructure with data fetching utility2. Combined techniques of python code structuring3. General concept and theories across coding, econometrics, and reinforcement learning topics", "Tiingo is a financial research platform that provides data including news, fundamentals and prices. We can extract the intraday stock market data through its REST IEX API that retrieves TOPS data (top of book, last sale data and top bid and ask quotes) from the IEX Exchange.", "For example, simply paste this link in your browser:", "and you will get a list of historical 5-minute intraday prices for AAPL as of 2019\u201301\u201302 in JSON format:", "To automate the task we will need functions that can get standardised intraday data within a specified historical window for a list of stocks.", "Pandas also provides relevant tools to extract data from not only Tiingo but also other data providers, but it seems that they only extract daily data.", "Note that subject to your subscription and the corresponding limits on requests, the API data is free. However, you should be aware of the usage when you use the code and avoid challenging the limits.", "First of all, you need a Tiingo API token. Simply sign-up for an account and you will find your token in here.", "These are the major functions which are located in Data/API.py inside the code:", "2. Pandas read_json allows us to read the fetched JSON data into a Series or DataFrame. We can wrap this inside the class:", "3. The functions above are defined under the class Tiingo in Data/API.py. The code will repeat the fetching based on the key inputs: start date, end date, the target attributes (i.e. \u2018date\u2019 and \u2018close\u2019) and a list of stocks and output a combined dataframe", "The implementation above is constrained and slow. Every time when we start a fetch, the program will open the API connection, request for the data from the server and wait for it\u2019s response before closing the connection. After all the progress repeats until the final url is fetched.", "Let\u2019s check how big is the difference with AsyncIO. The code below fetches 1-minute prices between 2018\u201301\u201301 and 2018\u201301\u201331 for GOOG and FB. 391 samples for each date, around 15,600 observations in total. All settings are configured in the config dictionary but let\u2019s ignore that for the time being.", "The result below shows that with AsyncIO it is about 17 times faster than the normal fetching.", "asyncio is a library to write concurrent code using the async/await syntax.", "asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.", "Unlike parallel programming, AsyncIO is single-thread. A simple idea is, it pipelines the assigned tasks in a single-event handler which organizes the distribution of tasks so that multiple tasks can start running while the others are idle.", "To illustrate the concept, let\u2019s have a look into an official example:", "The keyword async def above defines the corresponding function as a coroutine which can suspend or resume executions. Whenever a task is working-in-progress inside under the keyword await, the process control is passed back to the event controller (loop) which allocate and start the process for another task. To put it simply, it does not waste the waiting time.", "In our code we also have something similar. The _fetch_hist_async will create an event loop to controls the process of fetch_data_async which is the underlying task that fetches the intraday prices. When await is encountered, the control is returned to the event loop which triggers another fetching request even if the previous one is yet to be finished.", "Ideally we should set up a database to store the prices. For simplicity let\u2019s save the prices in .csv under the directory STATICS/PRICE:", "Let\u2019s skip the Part 2 which covers the boring code and structure and do some analysis. But I still recommend you to go to the end of this article and read that and have a concept about the skeleton first if you are interested.", "Pairs trading is a market neutral strategy. As described by Gatev et al. (2006):", "\u201cThe concept of pairs trading is disarmingly simple. Find two stocks whose prices have moved together historically. When the spread between them widens, short the winner and buy the loser. If history repeats itself, prices will converge and the arbitrageur will profit. \u201c", "It summarises the strategy into two stages:", "To put it simply, it trades on the mean-reverting spreads. The question is, how do we estimate or verify the price dynamics between the stock pair?", "Krauss (2017) summarises the common approaches in pairs trading strategies into five categories: distance approach, cointegration approach, time series approach, stochastic control approach, and other approaches such as machine learning, principal components analysis, and copula. This article will demonstrate the use of the classical Engle and Granger (1987) cointegration approach in a combination of reinforcement learning algorithms for pairs trading.", "The idea here is linked to a concept in time series analysis called stationarity. It is more often referred to the weak-form (or covariance) stationarity in financial time series with the following criteria:", "Usually, x(t) is regarded as the logarithmic price return (or differences), not the price level. If a time series becomes stationary after first differencing, it is so called integrated of order one I(1).", "Although stock prices could also be mean-reverting, they rarely oscillate, i.e. they are trended and non-stationary (random walk) due to mixed effects of continuous economic drivers and market activities. Therefore, some people may profit from directional bets, but this is not our focus.", "What we actually want is to find a pair of stocks which the price differences or spreads are consistently stationary (and cointegrated).", "I have extracted 1-minute prices from 2018\u201301\u201301 to 2018\u201307\u201330 for 21 US stocks. All prices are saved in STATICS/PRICE in .csv format. Let\u2019s take the first 70% of the time series and do some analyses.", "Here we calculate the price (not return) correlations. The highest correlations sit on PEP, PG, JNJ, and KO. From the economical perspective, they should be grouped into two pairs: JNJ-PG and KO-PEP. Note that high correlation does not necessarily imply cointegration. See the detail below.", "If we look into the their marginal distributions, the linear relationship should be somewhat recognized. We can also found some clusters as well which maybe useful but for the time being let\u2019s avoid further data mining which is not our main focus.", "Let\u2019s create a function to plot the prices and spreads for a sample period. Prices are re-based to 1 at the beginning of the sample. Note that the 2nd subplot will depict the trading range specified by a symmetric trade threshold (th) and stop_loss (stop):", "The following codes calculate the p-value for the cointegration test, and the null hypothesis is no cointegration. So if the p-value is small, the probability of observing a cointegrated relationship should be relatively high.", "Note that the test below is for the whole time series. During the training we should test and trade based on selected samples.", "The following results show that, even their correlations are comparable, the probabilities of finding a cointegrated relationship are very different.", "Sometimes we could find a correlated but not cointegrated price relationship. For example, if two stock prices go up together over time, they are positively correlated. However, if these two stocks trend up in different speeds, the price spread will keep growing rather than oscillating at the equilibrium and hence is non-stationary.", "Let me illustrate this point with an experiment. The code below simulates two highly correlated stock prices by Geometric Brownian Motion (GBM) and Cholesky Decomposition, each contains 1,000 samples.", "As you can see, although the correlations are high, the p-value is very large.", "The spread as shown in the bottom sub-plot is trending rather than mean-reverting.", "Engle and Granger (1987) defines a two-step identification:", "In our case, cointegration exists in stochastic trend components when more than one I(1) non-stationary and exogenous variables (so that they are theoretically independent to each other) exactly offset each other, giving a stationary linear combination and a long-run equilibrium. More specifically, two I(1) logarithmic stock prices x(1,t) and x(2,t) are cointegrated if a cointegration coefficient b exists, giving a stationary time series y(t) (i.e. I(0)):", "where a is simply a constant, and y(t) is our target trading spread.", "Apparently, we can simply use the ordinary least squares (OLS) method to estimate the spread and the coefficient b which is the hedge ratio by regressing x(1,t) against x(2,t). The original idea was based on the Granger representation theorem and represented in form of an error-correction model (ECM). However, based on an idea in Stock (1987) called super-consistency, the OLS estimator is easier to implement and expected to have a better performance in estimating cointegrated relationship due to the faster convergence to the true regression coefficient.", "The most common approach to test for cointegration is to check whether the residuals from the above regression are stationary by using Dickey Fuller (DF) or Augmented Dickey Fuller (ADF) test for unit root.", "Unit Root and Dickey-Fuller (DF) Test", "Unit root is a characteristic of random process. Consider a process with autoregression in order 1 (an AR(1) process):", "e(t) is a white noise and 0 < c \u2264 1. If the process is non-stationary nor purely random, then the hypothetical value of c is equal to 1 (i.e. the root of the equation is 1 and thus the process is I(1)). For instance, this could imply that the price today equals the price yesterday plus a random value.", "Dickey and Fuller (1979) shows that the t-statistics in this case does not follow a t-distribution, so the testing is inconsistent. To solve this problem we can change model above to:", "and test the null hypothesis of (c-1)=0. This is the so-called Dickey-Fuller test. We may also add an intercept or trend term and test for the null hypothesis that the their coefficients equal to zero, depending on the assumption.", "If we expand the autoregression process into an order of p (i.e. AR(p)):", "then we can apply the Augmented Dickey-Fuller test with this formulation:", "and test the null hypothesis of:", "In section 2.3 we have already seen the strategy class EGCointegration and the implementation is consistent with the above explanation. Note that the testing here is based on statsmodels.tsa.stattools.coint, and there is another function statsmodels.tsa.stattools.adfuller in the same library that is used to unit root testing. The difference is that:", "In most cases the two tests should yield the same conclusion, but coint is more intuitive for our implementation.", "I benefited a lot from this series and took some ideas during the development of the code. Definitely recommend.", "The fundamental of reinforcement learning consists of two main components: agent and environment. The environment is represented by different states with a predefined state space, while the agent learns a policy determining what actions to perform out of the action space. In a full reinforcement learning problem, the learning cycle of an agent could be summarized into the following phases:", "As an example, imagine a puppy (agent) is learning how to react to his master\u2019s commands (environment). It is a lazy dog that only knows how to perform the following actions:", "In order to train the puppy his master keeps giving him a set of orders (states) regularly, including \u201csit\u201d, \u201cstand\u201d, and \u201cjump\u201d. If he reacts correctly, his master will give him some dog food (reward).", "At the beginning, the puppy does not really understand what his master wants i.e. he has no idea (policy) how to \u201cmap\u201d the orders properly to the desired actions. However, occasionally he could react in the right way and get the reward, and gradually build the connection between them (update the policy).", "After many trials he finally knows that he should sit/stand whenever he hears the word \u201csit\u201d or \u201cstand\u201d. But no matter how many times his master asks him to \u201cjump\u201d, he completely does not know what to do. He tried a few times to sit or stand in this case, but could not get any reward.", "Eventually the puppy chooses c. Do nothing for the \u201cjump\u201d command because compared with other actions, this option is much less exhausting (so less negative reward).", "In supervised learning, the algorithm learns from instructions. Every instance has an estimation target to compare in order to calculate the cost of discrepancy, and the algorithm is updated by minimising the cost through iteration, so the process is somewhat \u201cinstructed\u201d by the target output which tells what is the correct outcome.", "However, in reinforcement learning, the policy is learned by evaluations. There is no such absolute target in the samples to compare with. The agent could only learn by evaluating the feedback continuously, i.e. it keeps picking an action and evaluating the corresponding rewards in order to adjust the policies, retaining the most desirable outcomes. Therefore, the process flow is much more complicated. When we apply reinforcement learning in trading, we need to ask ourselves what exactly the agent is learning to perform, and be careful in defining the elements especially the state and action spaces.", "Question: In the above picture, there is a 2-armed slot machine. Which arm is the best to pull in order to maximize our reward?", "Answer: The Right Arm since the expected reward is greater than pulling the Left Arm\u2019s.", "But how does a machine learn to solve this puzzle?", "From the perspective of RL, this is the simplest setting in RL problem, and the task above could be summarized by the following spaces:", "In the training process, the RL algorithm will repeat the task above (pulling the arm) and evaluate the rewards obtained and update it\u2019s policy recursively. Eventually, by evaluating the policy weights it should be able to give a conclusion of which arm is the best to pull. See a better explanation in this post.", "The contextual bandit problem is an expansion of the n-armed bandit. As shown in the picture above, given that there is not only 1 but 3 slot machines, we need to consider that for a particular machine (state) which arm is the best to pull. Now the setting becomes:", "What exactly we would like the machine to learn to perform? Following the idea of For each pair of time series, it learns to maximize the expected trading profit [reward] by selecting the best combination of historical window, trading window, trade threshold, and stop lost [action].", "In other words, we formulate it as an N-Armed Bandit problem (stateless):", "Now we are good to go. Here are the implementation:", "Pair: JNJ-PGData period: 2018\u201301\u201301 to 2018\u201307\u201330Frequency: 1-minuteStates: None (fixed by setting it to the fixed transaction cost of 0.1%)Actions:  \u2014 i. Historical window: 60 to 600 minutes, 60-minute step \u2014 ii. Trade window: 120 to 1200 minutes, 120-minute step \u2014 iii. Trade threshold: (+/-)1 to 5, price step is 1 \u2014 iv. Stop loss: (+/-)1 to 2 on top of trade threshold, price step is 0.5 \u2014 v. Confidence level: 90% or 95%Profit taking level: 0Reward: mean return (if it is cointegrated, otherwise it is set to the transaction cost)Trade quantity: 1 spread per buy / sell signalCalibration prices: standardizedTrading prices: actualOthers: assume trading at closing price", "After a trial run I found that the probability output for Boltzmann exploration could go up to 1. To mitigate the impact of extraordinarily high returns the mean reward is capped at 10.", "From the training result the mean reward is positive despite it is capped:", "The following test trade across every minute using the optimal action obtained from the training result, excluding the maximum possible historical window and trading window:", "Alternatively, we can also use Zipline and Pyfolio for more sophisticated back-testing.", "Although the result seems promising, in the real world the situation is complicated by numerous factors such as bid-ask spread, delay in execution, margin, interest, fractional shares etc. However, our objective here is to give an example of how to combine various techniques in developing a systematical trading tool with a structured machine learning components. I hope this is an enjoyable page to you.", "The execution is governed by the config (dictionary). This component allows us to encapsulate a lot of executions and tidy up the code. It can also be used as a carrier of additional parameters.", "For instance, in the previous section, the instantiation of API.Tiingo takes the config as an input set it to an attribute. When it calls the underlying functions, the input parameters such as start date, end date, token, no. of sample per day and data frequency will be extracted from the config.", "Currently only a single config is implemented. Ideally, we should implement multiple configs for different components.", "Using the PyYAML package the code can recognize the fields in .yaml /.yml file and convert the format automatically:", "the package can recognize the indentation and load it into a dictionary:", "Check the UTIL/FileIO.py for the reading and writing functions:", "For this we have already covered the main detail so I am gonna skip this. If you would like to add another API I would suggest you to simply make another class, with the same interface as fetch in the class Tiingo.", "In ./STRATEGY each module contains a strategy category, each strategy should be represented by one class. The class is inherited from an abstract base class which requires it to implement the following:", "Inside the package we can find a strategy class EGCointegration which takes price data x and y and other parameters during the instantiation. When the underlying functions need a sample data set, they will call the get_sample function to perform the sampling from its data attributes.", "During the training phase, in each iteration we will need to calibrate the p-value and coefficients to decide whether and how a pair trading should be triggered. These executions are embedded in the same class. when the process is called, the object will automatically perform the sampling from its data attributes and run the calibration. Based on the calibrated result the function will get a reward and record and set them to the corresponding attributes.", "See more about cointegration and its testing in Part 3.", "These components are highly integrated and governed not only by the config but also the tailor-made agent which control the whole ML process which is highly automated. Many ML algorithms were hard-coded. That means if the logic needs to be fine tuned, the code has to be amended which is a bit inconvenient. Here, although the design is a bit complicated, if you can understand the style you will be able to expand it in any way you want.", "Recently, Google has released an open-source library for reinforcement learning (RL) called TF-Agents. Feel free to check this out. Some concepts are similar, but the main focus of our code is on the automation so you may use that as a foundation if you would like to build a new one.", "It is the main body that runs and control the processes in ML. In RL, it has another layer of implication: in general it is the component that receives the states of the environment and makes decision on what action to take accordingly. The Agent class is meant to be inherited by the machine learning class. It should be initiated with a Network object and a config dictionary. Major functions include:", "- docking: attach the Network input and output layers- assign_network: assign new Network to the Agent object and connect - set_session: set TensorFlow - get_counter: extract the parameters from config and get a dictionary of StepCounter objects for looping or increments such as varying probability- save_model / restore_model: save and restore model in / from .ckpt file- process: abstract method to be implemented for training or testing", "A typical way of building a TensorFlow neural network is something like this inside which the layers and the parameters in each of them are hard-coded:", "Alternatively we could also build a function that repeats the above process, forfeiting the flexibility in setting the layer arguments.", "If you want to build a ML system or something with GUI with flexibility in customizing the detail for each layer (i.e. layer type, layer inputs, layer arguments) while preserving the automaticity, here comes a suggestion:", "The two functions on the left are under the class Network.", "The steps to create a network:", "Therefore, the Network object N now should have 6 attributes in total. Each of them is a layer with predefined properties:", "Since the construction of the network is based on the layer dictionary, automation comes into ply if the generation of such dictionary is streamlined, and we no longer need to hard code the network every time when we build something new.", "Basically it refers to a sample space object. It takes a dictionary of list as an input and create the sample space by making full combinations across list elements. For example, for the following sample space:", "S contains all combinations of \u2018dice\u2019 and \u2018coin\u2019, 12 elements in total. It contains the necessary functions that convert the sample from dictionary to a single index, list of indices, or one_hot array and vice versa that could fit the purpose of adapting different kind of input or output carriers in TensorFlow.", "During training, some parameters are incremental such as the current step in for loop, or the learning rate is set to be variable. We may even want to add a buffer before the actual step is triggered (i.e. the learning rate start to drop after 100 loops). Instead of hard coding these in the script, we can have a step counter to perform the above. The counter also incorporates the ability to buffer pre-train steps. For example, the actual counting value starts to change only after 100 buffering steps.", "A Processor class should take an Agent object as an input for initiation. When the process is called it will extract relevant parameters from the Agent object, including the attached config dictionary, and attach any output to the data dictionary which is an attribute of the Agent. We can actually create another object to carry these attributes but for simplicity let\u2019s not overload the structure in here.", "Both of them inherit the parent class Space and are used to generate state samples or action samples. Based on the method specified in config they can output the samples in different forms (i.e. index/one hot/dictionary) or different ways (with/without exploration) serving different purposes such as network training or taken as the input of the process function in the Strategy object.", "It takes an engine object which contain a process methods. In our example it will be an EGCointegration object.", "This article gives a very good introduction to the exploration methods in reinforcement learning. The purpose of this object is to explore possible actions. The selected method will return an action index to the data carrier in the Agent object. The exploration is implemented when the process function in the ActionSpace is called.", "This leverages the Experience Replay implementation in this article. The purpose is to store the samples and results along the training process, and re-sample from the buffer to allow the agent to re-learn from the history.", "Last but not least, I created a Recorder class which can be used to keep track of the records stored in the data dictionary inside the Agent object. We can select the field we would like it to store by specifying the key names in the RecorderDataField field in the config file:", "With the components described above, we can tailor make any class that takes these building blocks and create a running procedure. This is the only part that needs to be customized for different purpose, but still the logic is pretty standardized for similar cases.", "For example, in this project I have created a ContextualBandit class which can actually perform either N-Armed bandit or contextual bandit running, subject to the number of state. If we would like to run it for N-Armed bandit problem we could just specify a state space with a single fixed state (dummy).", "This article and the relevant codes and content are purely informative and none of the information provided constitutes any recommendation regarding any security, transaction or investment strategy for any specific person. The implementation described in this article could be risky and the market condition could be volatile and differ from the period covered above. All trading strategies and tools are implemented at the users\u2019 own risk.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6cdf8533bced&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wai_i?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wai_i?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "Wai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e51a97639e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&user=Wai&userId=1e51a97639e5&source=post_page-1e51a97639e5----6cdf8533bced---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6cdf8533bced&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6cdf8533bced&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/wai-i/Pair-Trading-Reinforcement-Learning", "anchor_text": "wai-i/Pair-Trading-Reinforcement-LearningContribute to wai-i/Pair-Trading-Reinforcement-Learning development by creating an account on GitHub.github.com"}, {"url": "https://www.tiingo.com/", "anchor_text": "Tiingo"}, {"url": "https://api.tiingo.com/documentation/iex", "anchor_text": "REST IEX API"}, {"url": "https://pandas-datareader.readthedocs.io/en/latest/readers/tiingo.html", "anchor_text": "relevant tools"}, {"url": "https://api.tiingo.com/account/billing/pricing", "anchor_text": "subscription"}, {"url": "https://api.tiingo.com/account/api/token", "anchor_text": "here"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html", "anchor_text": "read_json"}, {"url": "https://docs.python.org/3/library/asyncio.html", "anchor_text": "documentation"}, {"url": "https://python.readthedocs.io/fr/latest/library/asyncio-task.html", "anchor_text": "official example"}, {"url": "https://python.readthedocs.io/fr/latest/library/asyncio-task.html", "anchor_text": "https://python.readthedocs.io/fr/latest/library/asyncio-task.html"}, {"url": "https://en.wikipedia.org/wiki/Geometric_Brownian_motion", "anchor_text": "Geometric Brownian Motion (GBM)"}, {"url": "https://math.stackexchange.com/questions/163470/generating-correlated-random-numbers-why-does-cholesky-decomposition-work", "anchor_text": "Cholesky Decomposition"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "this"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149", "anchor_text": "this post"}, {"url": "https://www.zipline.io/beginner-tutorial.html", "anchor_text": "Zipline"}, {"url": "https://github.com/quantopian/pyfolio", "anchor_text": "Pyfolio"}, {"url": "https://docs.python.org/3/library/abc.html", "anchor_text": "abstract base class"}, {"url": "https://github.com/tensorflow/agents", "anchor_text": "TF-Agents"}, {"url": "https://www.youtube.com/watch?v=-TTziY7EmUA", "anchor_text": "this"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf", "anchor_text": "article"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df", "anchor_text": "article"}, {"url": "https://medium.com/tag/trading?source=post_page-----6cdf8533bced---------------trading-----------------", "anchor_text": "Trading"}, {"url": "https://medium.com/tag/python?source=post_page-----6cdf8533bced---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----6cdf8533bced---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----6cdf8533bced---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6cdf8533bced---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6cdf8533bced&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&user=Wai&userId=1e51a97639e5&source=-----6cdf8533bced---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6cdf8533bced&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&user=Wai&userId=1e51a97639e5&source=-----6cdf8533bced---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6cdf8533bced&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6cdf8533bced&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6cdf8533bced---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6cdf8533bced--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6cdf8533bced--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6cdf8533bced--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wai_i?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wai_i?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wai"}, {"url": "https://medium.com/@wai_i/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "236 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e51a97639e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&user=Wai&userId=1e51a97639e5&source=post_page-1e51a97639e5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8f10839bb70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-implementation-of-reinforcement-learning-in-pairs-trading-6cdf8533bced&newsletterV3=1e51a97639e5&newsletterV3Id=b8f10839bb70&user=Wai&userId=1e51a97639e5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}