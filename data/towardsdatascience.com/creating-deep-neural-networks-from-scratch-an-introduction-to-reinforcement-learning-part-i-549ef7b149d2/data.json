{"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2", "time": 1683005537.3921049, "path": "towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2/", "webpage": {"metadata": {"title": "Creating Deep Neural Networks from Scratch, an Introduction to Reinforcement Learning | by Abhav Kedia | Towards Data Science", "h1": "Creating Deep Neural Networks from Scratch, an Introduction to Reinforcement Learning", "description": "When I first started looking at reinforcement learning in the OpenAI gym, I was unable to find any good resources on how to begin building the solution myself. There are very powerful libraries (like\u2026"}, "outgoing_paragraph_urls": [{"url": "https://gym.openai.com/", "anchor_text": "website", "paragraph_index": 2}, {"url": "https://github.com/openai/gym", "anchor_text": "gym", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Action_selection", "anchor_text": "actions", "paragraph_index": 3}, {"url": "https://gym.openai.com/envs/CartPole-v0/", "anchor_text": "Cartpole problem", "paragraph_index": 6}, {"url": "https://gym.openai.com/docs/", "anchor_text": "docs", "paragraph_index": 11}, {"url": "https://github.com/openai/gym/issues/238#issuecomment-231129955", "anchor_text": "https://github.com/openai/gym/issues/238#issuecomment-231129955", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "neural networks", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Artificial_neuron", "anchor_text": "artificial neurons", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Neuron", "anchor_text": "neurons", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Synapse", "anchor_text": "synapses", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "anchor_text": "stochastic gradient descent", "paragraph_index": 22}], "all_paragraphs": ["This post is the first of a three part series that will give a detailed walk-through of a solution to the Cartpole-v1 problem on OpenAI gym \u2014 using only numpy from the python libraries. This solution is far from an optimal solution (you can find those on the gym website), but rather is focused on doing it from first principles. Pre-requisites for running the code in this article are python (3.x), with gym and numpy modules installed.", "When I first started looking at reinforcement learning in the OpenAI gym, I was unable to find any good resources on how to begin building the solution myself. There are very powerful libraries (like Tensorflow and Pytorch) that allow you to build incredibly complex neural networks and solve the cartpole problem easily, but I wanted to create the neural networks from scratch, as I believe there is value in understanding the core building blocks of modern machine learning techniques. I\u2019m writing what I wish I had been able to find when I was trying to work on this. So let\u2019s get started.", "First, what is OpenAI gym? There is a good, short intro on their website, \u201cGym is a toolkit for developing and comparing reinforcement learning algorithms.\u201d \u201cThe gym library is a collection of test problems \u2014 environments \u2014 that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.\u201d What this means is that the engineering around building and rendering models that simulate real world scenarios is already done for us, so we can just focus on teaching an agent to play the game well.", "The description above also mentions reinforcement learning. What is that? Here\u2019s a wikipedia summary\u2014 \u201cReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize some notion of cumulative reward.\u201d", "To give you an analogy, think about how a dog is trained \u2014 favorable actions are positively reinforced (in the form of a treat) and negative actions are negatively reinforced. In a way even we, humans, are complex reinforcement learning agents trying to maximize the chance of achieving our goals by selecting actions that we think will \u2018benefit\u2019 us (in the form of greater reward) in the future. Here\u2019s a figure that illustrates the cycle in reinforcement learning,", "The above figure shows an agent (the program that we will build) taking as inputs the state of the environment and reward from the previous action, selecting a subsequent action and feeding that back to the environment, before observing the environment once again.", "Great, now that we have an understanding of the basic concepts in reinforcement learning, let\u2019s go back the problem we are trying to solve \u2014 Cartpole. To begin with, take a look at the documentation for the Cartpole problem specifically. The documentation gives a good overview of what we are trying to achieve. In a nutshell, we are in control of the base of a slider with a pole balanced vertically on top. Our goal is to prevent the pole from falling off for as long as possible. If the pole falls (in terms of its angle) below a certain point, the environment is reset. Below is a random agent working on the cartpole problem.", "As you can see, it\u2019s not very good! But that is expected, since this agent disregards the current state of the environment and selects a random action at every time step. Let\u2019s see if we can do better.", "Time for some code! We\u2019ll start by importing the libraries that we will be using. We will need gym for the OpenAI environments as discussed above, and numpy for some math and matrix manipulations.", "Next, we need to import the environment that gym provides for the cartpole problem. Here\u2019s how this is done:", "We can also observe some of the features of this particular environment space by printing them:", "There is much more information about environments and their workings in the docs, but the values above capture the basic elements that define this environment \u2014 the actions that can be performed, and the observations at every time step.", "The action space is discrete and contains 2 values: 0 and 1. These correspond with the two actions that the agent is able to perform, i.e. push the slider towards the left or towards the right.", "The observation space on the other hand is continuous and has four components (not to be thrown off with the data structure Box(4,) , for our purposes it just means an array containing four values). What do the four values mean? They are numbers that represent the state of the environment at that time \u2014 namely, the position of cart, the velocity of cart, the angle of pole, and the rotation rate of the pole. [https://github.com/openai/gym/issues/238#issuecomment-231129955]", "A fundamental thing to understand here is that the meaning of the numbers in the observation and action spaces is explained only for completeness and our goal is not to interpret the values (of either the action space or the observation space) but let the agent learn the meaning of these values in context. Let us get back to our program and add code to get a basic loop running.", "The code above declares a main program loop for the episodes and iterates through the time steps within an episode. In the internal loop, the program takes an action, observes the result and then checks if the episode has concluded (either the pole has fallen over or the slider has gone off the edge). If it has, the environment is reset and the internal loop starts over.", "The line that selects the action is randomly sampling it from the available actions; in fact, it behaves exactly like the random agent shown earlier. Let\u2019s change that to define a custom method for selecting the action given the observation.", "Now, we will define an agent that is (hopefully!) going to learn to be smarter in picking its actions given a state. We will model this agent in a class:", "We also need to add an instance of RLAgent to our global variables and change the action selection to call this function from the instantiated class. Note that at present the select_action function does the same thing as before, but we will change that later.", "We will now create the elements of our neural net. A quick primer on neural networks: \u201cAn ANN (Artificial Neural Network) is a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\u201d", "This is what ours will look like,", "The above picture captures a neural network that has one input layer, two \u2018hidden\u2019 layers (layers 2 & 3) and an output layer. The model provides inputs from the observation space to the input layer, these are \u2018fed forward\u2019 to subsequent layers until the output layer, where the values in the output layer are used to select the action.", "For instance, every node in layer 3 is a linear combination (weighted sum) of layer 2 passed through an activation function. The weights used to calculate layer 3 are initialized randomly in a matrix and gradually tuned through a process called stochastic gradient descent to better predict the outputs. The activation function is a simple non-linear function that allows the classifier to learn non-linear rules in the underlying observation space.", "In our CartPole problem, there are 5 inputs (the elements of the observation space + a bias term) and 2 outputs (the two directions in which we can push the cart).", "The neural net layers are going to be encapsulated in an NNLayer class,", "This class captures three major things:", "We will now add the usage of this class to our RLAgent. First, we\u2019ll edit the select action function,", "Instead of randomly selecting a value every time, this function passes the information about the state of the environment to our neural network and calculates the \u2018expected reward\u2019 for each action (values is a function that takes in a (1,n\u1d62\u2099) array and returns a (1,n\u2092\u1d64\u209c) array). It then selects the action that will lead to the greatest expected reward. Note that this function still selects a random value with probability epsilon. Epsilon, also known as the \u2018rate of exploration\u2019 is the implementation of an important concept in reinforcement learning: the tradeoff between exploration and exploitation. Exploration helps the model to not get stuck in a local minimum, by exploring apparently sub-optimal actions from time to time that may reveal greater rewards further down the road. Exploitation on the other hand allows the agent to use its knowledge of the current state to select the most profitable action. In most RL agents, epsilon starts out high (near 1.0) in the beginning and is gradually reduced to 0 over time as the agent becomes more confident in the learnt values of actions in a given state.", "The select_action function also calls self.forward (RLAgent.Forward), and here\u2019s the code for that function,", "The RLAgent.forward function above has a simple loop. It passes the input (the observation for which we are trying to decide on a course of action) to the network and obtains a set of values for each action. It internally calls the NNLayer.forward function, collecting the output from each layer and passing it to the next layer. To complete the implementation of the select action function, here is the last piece \u2014 the NNLayer.forward function. The remember_for_backprop parameter is a boolean that specifies whether or not certain calculated values need to be stored in order to prevent double calculation during the weight updates (this will be explained in more detail in the section on backpropagation).", "Let\u2019s also add the instantiation for these layers in the RLAgent\u2019s init function,", "You can see above that we have a total of 2 hidden layers and 1 output layer. Also, in all but the output layer we are using an activation function called Rectified Linear Unit (ReLU). This function is an extremely simple function that introduces sufficient non-linearity to our neural network. Here is the implementation for it,", "This function takes in a matrix and returns another matrix that has identical values where the original matrix was greater than 0, and 0 in all other values. Finally, let\u2019s add the initialization of our agent and epsilon decay to our main program loop. This is what the new main program looks like:", "What does this model do currently? It initializes the weights for the neural network randomly and calculates values for actions in any given state based on these weights. However, we need a way for the function to improve the values of the weights in the network such that the agent is able to take the best action in any given state. As alluded to earlier, this is achieved through stochastic gradient descent and implemented via a technique called backpropagation. I will go into detail about backpropagation along with relevant theory of reinforcement learning in the next post.", "To summarize, this is what we have accomplished so far:", "In the next post, we will aim to achieve the following:", "Data Science, FinTech and the future of Technology. MA CompSci & Math, University of Oxford."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F549ef7b149d2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://abhavkedia.medium.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Abhav Kedia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F634761ec89d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&user=Abhav+Kedia&userId=634761ec89d3&source=post_page-634761ec89d3----549ef7b149d2---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F549ef7b149d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&user=Abhav+Kedia&userId=634761ec89d3&source=-----549ef7b149d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F549ef7b149d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&source=-----549ef7b149d2---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.akc.org/expert-advice/training/lure-reward-training-dogs/", "anchor_text": "Stephanie Gibeault"}, {"url": "https://gym.openai.com/", "anchor_text": "website"}, {"url": "https://github.com/openai/gym", "anchor_text": "gym"}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning"}, {"url": "https://en.wikipedia.org/wiki/Action_selection", "anchor_text": "actions"}, {"url": "https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8", "anchor_text": "Mohit Mayank"}, {"url": "https://gym.openai.com/envs/CartPole-v0/", "anchor_text": "Cartpole problem"}, {"url": "https://gym.openai.com/docs/", "anchor_text": "docs"}, {"url": "https://github.com/openai/gym/issues/238#issuecomment-231129955", "anchor_text": "https://github.com/openai/gym/issues/238#issuecomment-231129955"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "neural networks"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neuron", "anchor_text": "artificial neurons"}, {"url": "https://en.wikipedia.org/wiki/Neuron", "anchor_text": "neurons"}, {"url": "https://en.wikipedia.org/wiki/Synapse", "anchor_text": "synapses"}, {"url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "anchor_text": "stochastic gradient descent"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----549ef7b149d2---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----549ef7b149d2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----549ef7b149d2---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----549ef7b149d2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----549ef7b149d2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F549ef7b149d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&user=Abhav+Kedia&userId=634761ec89d3&source=-----549ef7b149d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F549ef7b149d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&user=Abhav+Kedia&userId=634761ec89d3&source=-----549ef7b149d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F549ef7b149d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F634761ec89d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&user=Abhav+Kedia&userId=634761ec89d3&source=post_page-634761ec89d3----549ef7b149d2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5fb20815f9f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&newsletterV3=634761ec89d3&newsletterV3Id=5fb20815f9f8&user=Abhav+Kedia&userId=634761ec89d3&source=-----549ef7b149d2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Written by Abhav Kedia"}, {"url": "https://abhavkedia.medium.com/followers?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "116 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F634761ec89d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&user=Abhav+Kedia&userId=634761ec89d3&source=post_page-634761ec89d3----549ef7b149d2---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5fb20815f9f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2&newsletterV3=634761ec89d3&newsletterV3Id=5fb20815f9f8&user=Abhav+Kedia&userId=634761ec89d3&source=-----549ef7b149d2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/dice-india/the-enabling-power-of-upi-payments-4060826c2a16?source=author_recirc-----549ef7b149d2----0---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=author_recirc-----549ef7b149d2----0---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=author_recirc-----549ef7b149d2----0---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Abhav Kedia"}, {"url": "https://medium.com/dice-india?source=author_recirc-----549ef7b149d2----0---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "DICE India"}, {"url": "https://medium.com/dice-india/the-enabling-power-of-upi-payments-4060826c2a16?source=author_recirc-----549ef7b149d2----0---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "The Enabling Power of UPI PaymentsGetting Under the Hood of the Fastest Growing Payments System in the World"}, {"url": "https://medium.com/dice-india/the-enabling-power-of-upi-payments-4060826c2a16?source=author_recirc-----549ef7b149d2----0---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "9 min read\u00b7Aug 5, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdice-india%2F4060826c2a16&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdice-india%2Fthe-enabling-power-of-upi-payments-4060826c2a16&user=Abhav+Kedia&userId=634761ec89d3&source=-----4060826c2a16----0-----------------clap_footer----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://medium.com/dice-india/the-enabling-power-of-upi-payments-4060826c2a16?source=author_recirc-----549ef7b149d2----0---------------------d281d918_9460_46c7_a760_c6583d9831ef-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4060826c2a16&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdice-india%2Fthe-enabling-power-of-upi-payments-4060826c2a16&source=-----549ef7b149d2----0-----------------bookmark_preview----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----549ef7b149d2----1---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----549ef7b149d2----1---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----549ef7b149d2----1---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----549ef7b149d2----1---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----549ef7b149d2----1---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----549ef7b149d2----1---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----549ef7b149d2----1---------------------d281d918_9460_46c7_a760_c6583d9831ef-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----549ef7b149d2----1-----------------bookmark_preview----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----549ef7b149d2----2---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----549ef7b149d2----2---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----549ef7b149d2----2---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----549ef7b149d2----2---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----549ef7b149d2----2---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----549ef7b149d2----2---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----549ef7b149d2----2---------------------d281d918_9460_46c7_a760_c6583d9831ef-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----549ef7b149d2----2-----------------bookmark_preview----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db?source=author_recirc-----549ef7b149d2----3---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=author_recirc-----549ef7b149d2----3---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=author_recirc-----549ef7b149d2----3---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Abhav Kedia"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----549ef7b149d2----3---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db?source=author_recirc-----549ef7b149d2----3---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "Creating Deep Neural Networks from Scratch, an Introduction to Reinforcement LearningValue Functions and Backpropagation"}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db?source=author_recirc-----549ef7b149d2----3---------------------d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": "13 min read\u00b7Apr 17, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&user=Abhav+Kedia&userId=634761ec89d3&source=-----6bba874019db----3-----------------clap_footer----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db?source=author_recirc-----549ef7b149d2----3---------------------d281d918_9460_46c7_a760_c6583d9831ef-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=-----549ef7b149d2----3-----------------bookmark_preview----d281d918_9460_46c7_a760_c6583d9831ef-------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "See all from Abhav Kedia"}, {"url": "https://towardsdatascience.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----549ef7b149d2----0-----------------bookmark_preview----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----1-----------------clap_footer----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----549ef7b149d2----1-----------------bookmark_preview----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----549ef7b149d2----0---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----549ef7b149d2----0-----------------bookmark_preview----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Feature Importance with Deep Neural Network for CryptocurrenciesA FinRL-Meta Tutorial for NeurIPS 2022 Datasets and Benchmarks"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "\u00b710 min read\u00b7Jan 2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----f06191e2d562----1-----------------clap_footer----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----549ef7b149d2----1---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&source=-----549ef7b149d2----1-----------------bookmark_preview----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----549ef7b149d2----2---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----549ef7b149d2----2---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----549ef7b149d2----2---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Aniruddha Mukherjee"}, {"url": "https://medium.com/dsckiit?source=read_next_recirc-----549ef7b149d2----2---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "GDSC KIIT"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----549ef7b149d2----2---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Reinforcement Learning: An Introduction and Guide to its FundamentalsPolicies, Rewards, the Bellman Equation, and the Markov Decision Process (MDP)"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----549ef7b149d2----2---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "5 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdsckiit%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&user=Aniruddha+Mukherjee&userId=68f97387c191&source=-----467c6a2ed25e----2-----------------clap_footer----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----549ef7b149d2----2---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&source=-----549ef7b149d2----2-----------------bookmark_preview----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/simple-interactive-chess-gui-in-python-c6d6569f7b6c?source=read_next_recirc-----549ef7b149d2----3---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/@williamdennis5?source=read_next_recirc-----549ef7b149d2----3---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/@williamdennis5?source=read_next_recirc-----549ef7b149d2----3---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "William Wu Dennis"}, {"url": "https://blog.devgenius.io/?source=read_next_recirc-----549ef7b149d2----3---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Dev Genius"}, {"url": "https://blog.devgenius.io/simple-interactive-chess-gui-in-python-c6d6569f7b6c?source=read_next_recirc-----549ef7b149d2----3---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "Simple Interactive Chess GUI in PythonWelcome to Part 1 of my tutorial series on chess in Python! In this tutorial, we will explore how to set up an interactive GUI in Python\u2026"}, {"url": "https://blog.devgenius.io/simple-interactive-chess-gui-in-python-c6d6569f7b6c?source=read_next_recirc-----549ef7b149d2----3---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": "5 min read\u00b7Dec 25, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdev-genius%2Fc6d6569f7b6c&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fsimple-interactive-chess-gui-in-python-c6d6569f7b6c&user=William+Wu+Dennis&userId=f0fe7197d20c&source=-----c6d6569f7b6c----3-----------------clap_footer----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/simple-interactive-chess-gui-in-python-c6d6569f7b6c?source=read_next_recirc-----549ef7b149d2----3---------------------f128fb33_9ecf_42f1_9868_ba5586281ce4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6d6569f7b6c&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fsimple-interactive-chess-gui-in-python-c6d6569f7b6c&source=-----549ef7b149d2----3-----------------bookmark_preview----f128fb33_9ecf_42f1_9868_ba5586281ce4-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----549ef7b149d2--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}