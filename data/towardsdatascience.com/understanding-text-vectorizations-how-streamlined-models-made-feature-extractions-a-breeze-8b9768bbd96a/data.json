{"url": "https://towardsdatascience.com/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a", "time": 1683011588.555875, "path": "towardsdatascience.com/understanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a/", "webpage": {"metadata": {"title": "Understanding Text Vectorizations I: How Having a Bag of Words Already Shows What People Think About Your Product | Towards Data Science", "h1": "Understanding Text Vectorizations I: How Having a Bag of Words Already Shows What People Think About Your Product", "description": "Do you know that we already (almost) solved sentiment analysis problems before neural networks become ubiquitous? In the current era of natural language processing (NLP) increasingly relying on deep\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.cs.jhu.edu/~mdredze/datasets/sentiment/", "anchor_text": "multi-domain sentiment dataset", "paragraph_index": 13}, {"url": "https://github.com/chen-bowen/Streamlined_Sentiment_Analysis", "anchor_text": "here", "paragraph_index": 44}, {"url": "http://Archera.ai", "anchor_text": "Archera.ai", "paragraph_index": 47}], "all_paragraphs": ["Do you know that we already (almost) solved sentiment analysis problems before neural networks become ubiquitous?", "In the current era of natural language processing (NLP) increasingly relying on deep learning models that generate amazing performances, we have often overlooked the importance of the simplest types of text vectorization techniques \u2014 bag of words (BOW) and term frequency-inverse document frequency (TF-IDF). In fact, with these two techniques, we have already been able to predict the sentiment of a given piece of text with more than 80% accuracy. In other words, all we are doing with state-of-art deep learning models are simply trying to squeeze more performance improvement out these classification problems. Almost all of the more complicated models, to some extent, have relied on concepts from BOW and TF-IDF. In this blog post, we will attempt to understand and implement BOW and TF-IDF, with some help from sklearn pipelines.", "You might think using pipelines are an extreme overkill. Indeed, implementing pipelines would require extensive utilization in object-oriented programming (OOP), which could potentially make debugging rather difficult. Though it took me two weeks to implement both the BOW and TF-IDF (with some mistakes I made in between, more about this in another blog post), it is definitely worth the time and effort. By using sklearn pipelines, we are able to transform and train/predict with raw data in a single function call. For example, if we want to use sklearn\u2019s TF-IDF vectorizer and train logistic regression model, we just have to use the following lines of code.", "TfidfVectorizer and LogisticRegression are module objects from sklearn, while StreamlinedModel is the pipeline object we will implement. As you probably already see, with this structure we will be able to easily swap the above two objects with any transformers and models \u2014 This structure will keep the interface universal and make feature extractions a breeze. There are also some other really important reasons we would want to build models in this fashion, but that will be discussed in another blog post.", "To implement StreamlinedModel class, we should recall how most of our familiar sklearn packages are used. For example,", "Our StreamlinedModel will have the same behavior, therefore we will add the same .fit and .predict methods in the class. Other useful methods include .predict_proba and .score. These methods could simply be wrappers around the same methods in sklearn\u2019s BaseEstimator . For example, the pipeline training model could simply be", "We can then build a class method called make_model_pipeline that uses the 4 parameters upon class instantiation and make a sklearn pipeline object.", "This self.pipeline is the same object that we can call .fit and .predict etc. That\u2019s it! We have created the StreamlinedModel . Full implementation is here below", "With the flexibility of switching the transformer into any transformer of our choice, we are now able to utilize the same interface with our customized BOW and TF-IDF transformers. For the purpose of differentiation, we will call our customized BOW transformer WordFrequencyVectorizer. The same StreamlinedModel could be used in the following way", "In this blog post, we will focus on implementing the bag of words model and leave the TF-IDF for the next one.", "The idea of the BOW model is very simple. We would like to find the fraction of specific words that are strong indicators of positive or negative sentiment. For example, a review contains more positive words like \u201cgreat\u201d, \u201cfantastic\u201d would be considered more positive than a review with words like \u201cso-so\u201d. Since reviews all have different lengths, we will need to pad each review with 0s for the words that are not contained in the review. For example, if we have the following 3 reviews,", "We will let each unique word occupy a specific position. Those words that are not contained in the review will be filled in to be 0, while the words that appeared more than once will be incremented to the number of their occurrences.", "Now we understand the general idea of how the BOW model works, it\u2019s time to see how we could adapt the idea to our sentiment analysis task. We will start by loading in the reviews from our data source.", "The data we will use for this project are the reviews from John Hopkins University\u2019s multi-domain sentiment dataset. Once we download and unzip the data, we will see a series of folders with the following structure.", "We would like to combine all the positive/negative reviews together so that we will not need to reference the individual folders separately. We will create a class called LoadReviews to accomplish this goal. The full implementation of this class is shown below", "Since the reviews are stored in XML format, we will have to use BeautifulSoup to parse out the reviews into text strings. The actual text reviews are stored in the tag review_text . Upon reading in the text reviews, we will also filter out the characters that are not Unicode strings. The static method strip_non_printable accomplishes such task. All the processed reviews will be stored in the class attribute reviews. As you probably noticed, our BOW model will only operate on individual word tokens rather than long strings, we will need to take further actions to preprocess the reviews into the proper vectorized format that can be consumed by our BOW transformer.", "To begin the preprocessing step, we will first convert the long str format of each review into a tokenized version of the review, which could be easily accomplished by splitting the string by white space. For example, the string \u201cI love nlp\u201d will become the list [\u201cI\u201d, \u201clove\u201d, \u201cnlp\u201d].", "Using split will probably result in a common edge case where the word has punctuation attached but no space in between, such as \u201chello,\u201d. To avoid this word be treated differently as \u201chello\u201d, we will strip all the punctuations and replace it with whitespace.", "Many words that don\u2019t have strong indications of sentiment, such as, \u201cI\u201d, \u201cthey\u201d, \u201ca\u201d, but will still occupy spaces. We will proceed to remove them from the list of unique words so that they no longer occupy a position.", "In addition, Same words of different forms, such as \u201cwork\u201d to \u201cworked\u201d, would need to be treated as the same words. This would require us to stem and lemmatize all the words to its original form, which could be accomplished by using NLTK\u2019s word stemmer.", "Lastly, we will also remove the words that are less than 2 characters in length, as they probably provide very little meanings to the sentiment.", "All of these steps could be wrapped into a single utility class named WordTokenizer , implemented as the following.", "With all of these components ready, we are now prepared to implement the BOW model.", "Since the model will be used as a transformer by the StreamlinedModel object, we will need to follow the sklearn pipeline\u2019s guidelines for building custom transformers. That means the objects will need to have a .transform and a.fit method. The .fit method will take care of the transformations that save their specific parameters from training and apply them during predictions. Specifically, in our case, the list of unique words contained in the dataset will be fitted during training, but not re-fitted again during prediction.", "The word_index_mapping will be a property from the object ReviewProcessor that is essentially an enumeration of all the unique words. We will also set the last position of the vector to be \u201cunknown_word\u201d, in case of the word during prediction is not part of the list of unique words.", "The .transform method is used to convert the inputted raw text to the word frequency vector.", "The get_word_frequency_vector method will take in a review and increment the value at the corresponding index in the vector according to the pre-calculated word_to_index mapping. If the inputted review text contains unknown words, the last index of the word_frequency_vector will be incremented.", "The full implementation of the BOW transformer is shown below", "Now we have implemented the BOW as a transformer, we could use the StreamlinedModel to build 4 different versions of models and compare the performances.", "Just as the StreamlinedModel has the ability to use any transformer, we can also use a different machine learning model by swapping the model argument. For example, previously we have used the logistic regression model by calling the LogisticRegression object, we can change it into the lightGBM model like the following", "We will try 4 different models and collect their prediction AUC scores to make a bar plot.", "Using the logistic regression model could already get us an AUC score of 0.82.", "LightGBM model also helped us improve the performance up to 0.888 \u2014 showing the superior predictive power of tree models than linear models.", "The most interesting results come from the two different naive Bayes model that yielded tremendously different outcomes, as Gaussian naive Bayes performed way worse than multinomial naive Bayes. Since Gaussian naive Bayes models are designed to operate on continuous variables, while word frequencies are only discrete, the subpar performance is expected.", "As you probably noticed, the relatively simple multinomial naive Bayes model achieved comparable performance with the rather advanced lightGBM model in this sentiment analysis task. So what how did our model make the decision? Let\u2019s have a closer look.", "Since there are multiple reviews got misclassified, we will only visualize the one that yielded the largest distance from the correct label. To do this, we will sort the predicted probabilities print out the first/last one (depending on the label)", "This is a rather long and detailed positive review that mentioned the strengths and weaknesses of the book. In the weakness section, some negative words like \u201cproblems\u201d, \u201cnot\u201d are mentioned multiple times, which in turn, making classification on this review more difficult.", "Let\u2019s have a look at the most wrong negative review.", "This negative review also contains a positive word \u201cinteresting\u201d and has no obvious negative words (maybe \u201coutdated\u201d is one negative word), making classifying this review as negative less likely.", "So what words exactly does the model think a review is positive/negative? Let\u2019s have a look next.", "At this point, we would often raise the question of how to choose between the two best performing model (LightGBM and Multinomial naive Bayes). The answer lies in interoperability. Since naive Bayes relies on counting and probabilities, we can easily figure out which words contribute to the sentiment. LightGBM model, however, is not that simple. Due to a large number of unique words, tree splits are quite challenging to visualize. Are we really out of luck in interpreting them?", "Not really. With the help of model explanation package SHAP, we can produce a visualization of feature importance on a single review (more about SHAP in a different post). In the below SHAP visualization graph, red represents the predicted sentiment is closer to 1, while blue represents the predicted sentiment to be 0. Below is an example review that the model predicted to be positive", "As the review contains strong positive words like \u201cbest\u201d and \u201cenjoyed\u201d, this review is predicted to be a positive one.", "The negative reviews normally consist of negative words like \u201cdisappointment\u201d, \u201cfails\u201d or \u201cbad\u201d.", "Bag of words would probably be the simplest type of text vectorization for sentiment analysis tasks, but it was already quite effective. The project GitHub could be found here. Below is what we have learned from today.", "In the next post, we will build on what we accomplished with the bag of words model and improve it with a smart weighting scheme. See you next time!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer@ Archera.ai, Basketball Player Training How to Dunk, Life-long Knicks Fan, Living the Dream"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8b9768bbd96a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://bowenchen.medium.com/?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": ""}, {"url": "https://bowenchen.medium.com/?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "Bowen Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4eab6908bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&user=Bowen+Chen&userId=e4eab6908bf4&source=post_page-e4eab6908bf4----8b9768bbd96a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b9768bbd96a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b9768bbd96a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@amadorloureiroblanco?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Amador Loureiro"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.cs.jhu.edu/~mdredze/datasets/sentiment/", "anchor_text": "multi-domain sentiment dataset"}, {"url": "https://github.com/chen-bowen/Streamlined_Sentiment_Analysis", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8b9768bbd96a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/sklearn-pipeline?source=post_page-----8b9768bbd96a---------------sklearn_pipeline-----------------", "anchor_text": "Sklearn Pipeline"}, {"url": "https://medium.com/tag/object-oriented-design?source=post_page-----8b9768bbd96a---------------object_oriented_design-----------------", "anchor_text": "Object Oriented Design"}, {"url": "https://medium.com/tag/sentiment-analysis?source=post_page-----8b9768bbd96a---------------sentiment_analysis-----------------", "anchor_text": "Sentiment Analysis"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8b9768bbd96a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b9768bbd96a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&user=Bowen+Chen&userId=e4eab6908bf4&source=-----8b9768bbd96a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b9768bbd96a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&user=Bowen+Chen&userId=e4eab6908bf4&source=-----8b9768bbd96a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b9768bbd96a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8b9768bbd96a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8b9768bbd96a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8b9768bbd96a--------------------------------", "anchor_text": ""}, {"url": "https://bowenchen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://bowenchen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bowen Chen"}, {"url": "https://bowenchen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "141 Followers"}, {"url": "http://Archera.ai", "anchor_text": "Archera.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4eab6908bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&user=Bowen+Chen&userId=e4eab6908bf4&source=post_page-e4eab6908bf4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7b1d5930fa0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-text-vectorizations-how-streamlined-models-made-feature-extractions-a-breeze-8b9768bbd96a&newsletterV3=e4eab6908bf4&newsletterV3Id=7b1d5930fa0e&user=Bowen+Chen&userId=e4eab6908bf4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}