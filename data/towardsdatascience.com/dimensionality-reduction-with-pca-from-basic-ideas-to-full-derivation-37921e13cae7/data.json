{"url": "https://towardsdatascience.com/dimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7", "time": 1683005771.569895, "path": "towardsdatascience.com/dimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7/", "webpage": {"metadata": {"title": "Dimensionality reduction with PCA: from basic ideas to full derivation. | by Gleb Kumichev | Towards Data Science", "h1": "Dimensionality reduction with PCA: from basic ideas to full derivation.", "description": "Principal component analysis (PCA) is a powerful algorithm which ideas were laid out by Karl Pearson in 1901 [1] for a data fitting problem. Unlike least square regression, it does not depend on\u2026"}, "outgoing_paragraph_urls": [{"url": "https://zenodo.org/record/1430636#.XpGEef2A5hF", "anchor_text": "[1]", "paragraph_index": 0}, {"url": "https://mathworld.wolfram.com/OrthogonalDecomposition.html", "anchor_text": "orthogonal decomposition theorem", "paragraph_index": 15}, {"url": "https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives#lagrange-multipliers-and-constrained-optimization", "anchor_text": "great source", "paragraph_index": 36}], "all_paragraphs": ["Principal component analysis (PCA) is a powerful algorithm which ideas were laid out by Karl Pearson in 1901 [1] for a data fitting problem. Unlike least square regression, it does not depend on which variable is independent and reflects universal relations between variables. PCA has a lot of applications such as noise-filtration, feature extraction or high dimensional data visualization, but the basic one is data dimensionality reduction. In the following post, I\u2019ll describe PCA from this perspective.", "In this article we are going to:", "In a general sense, dimensionality reduction is a representation of original M-dimensional data N-dimension subspace, where N<M. To build some intuition let\u2019s start with a simple example. Let\u2019s imagine that our data is coordinates retrieved from GPS sensor in a car. Suppose it is a real world data, so it contains some noise.", "So we have two dimensional data at hand, reducing dimensionality in this case means finding some one dimensional representation of the data. It can be done by projecting every point on a one dimensional subspace. It doesn\u2019t have to be an orthogonal projection. For example let\u2019s take subspace U spanned by a basis vector b and project our data using some projection matrix P.", "Now in the subspace U we can store only one value for each data point \u2014 a corresponding coordinate \u03bb in the subspace U. These coordinates in lower dimension are called code. Using these coordinates \u03bb and a basis vector b we can reconstruct data back to the original dimension,the result is called reconstruction.", "You can mention here that reconstructed data differs from the original one, we can measure that difference as Euclidean distance between the original and reconstructed points. As we can see dimensionality reduction comes at a certain cost, the price we have to pay is information loss. But still it has some sense: even with such a random projection, reconstructed data reflects some of the original data properties: we still can see that X and Y are positively correlated, and even the slope of the reconstructed data is quite close to the original.", "We used some random unmotivated projection, so the question is \u201cCan we do better than that\u201d? We absolutely can, and that brings us to the PCA algorithm. But first, before deriving it in a general case, let\u2019s build a better understanding of it.", "Before we begin let\u2019s perform a small data preparation, let\u2019s shift our data to the center (subtract mean from every dimension). As we see such a transformation appears to be very helpful. It does not change the relation between variables and it is easy to get back to the original data, we just need to add mean values to corresponding variables.", "So now we want to perform dimensionality reduction and save as much information as possible. We can measure information loss as a reconstruction error defined as average squared distance between the original data and the reconstructed points:", "The thing we need to mention first here is that we should use an orthogonal projection in order to minimize distances between the original and projected points. Consider left graph on fig. 4: the distance of orthogonal projection (gray) is always less than a distance for not orthogonal (orange) projection. That can be proven with triangle inequality and later we will see it again from another perspective.", "So we\u2019ve covered the projection type, now we need to find such a vector b that minimizes L. From fig. 4 it is clear that b2 is better than b1, but how to find the best one of all?", "For now let\u2019s cheat a little bit, skip most of the math and optimize L numerically by just rotating the basis vector b and calculating a corresponding reconstruction error for every vector b. I\u2019ve also added a variance of projected points on the right graph so you can notice one key feature that we are going to discuss in a second.", "As one can see, in such a way we can find a vector that minimizes a reconstruction error and at the same time maximizes a variance of the projection. This is an important feature, so let\u2019s focus on it for a bit:", "There is one more thing left to discuss. Let\u2019s consider subspace (U2) orthogonal for that we just found (U1) on the following figure:", "Projections on those subspaces are principal components of our data. Consider projected variance in fig. 6: corresponding subspaces have an important property, i.e. the first projection contains most of the variance, while the second contains a minimum of variance. Together they describe all the data variance. Considering total data variance as their sum we can calculate that subspace U1 describes 97.1% of data variance and subspace U2 describes 2.9%. So if we reduce dimensionality and keep only projection on U1 information loss will be just 2.9%.", "Let\u2019s look at what we have done so far from a slightly different perspective: we have performed orthogonal decomposition of data with a new basis in such a way that one of the basis vectors spans a subspace with the maximum variance. According to orthogonal decomposition theorem we always can decompose any vector by some subspace and its orthogonal complement, and in PCA we are searching for a subspace which contains most of the data variance.", "For example, in 3d we can find 2d (a plane) or 1d (a line) subspaces with most data variance. Consider two basis vectors which span the plane with most variance and the third basis vector orthogonal to them:", "For higher dimensions, it is tricky to plot or imagine all that, but the idea remains the same: to find a subspace (line, plane, hyperplane) with most of the data variance and its orthogonal complement. Vectors which span these subspaces are principal components.", "Once we get this, we can move on to a general case.", "Suppose we have data X consisting of N data points, and each point is a D dimension vector:", "Like in the examples mentioned before, we want to decompose each data point in another orthonormal basis in such a way that the first set of components contains most of the data variance. We can write this as follows:", "Here b is our new basis vectors and \u03b2 are corresponding coordinates. We want to find such b and \u03b2, that the projection on subspace spanned by first M basis vectors contains most of the data variance. Let\u2019s recap the examples:", "For dimensionality reduction we get rid of the second term with the smallest variance and represent our data with the first M basis vectors:", "Note: x* is still a D dimension vector, but now it lives in a smaller subspace and can be described by M coordinates in new basis b.", "Our goal remains the same: to minimize a reconstruction error which we define as mean squared distance between the original and reconstructed points:", "But now we are going to derive analytical solution. To do that, let\u2019s rewrite our loss function. First, let\u2019s represent projection coordinates by dot product of data on corresponding basis vectors:", "Note: one can prove that orthogonal projection is the optimal solution and get the same result considering partial derivative of L with respect to \u03b2 and set it to zero.", "Using that representation of \u03b2 we can rewrite x* as follows:", "We can rewrite x based on the same reasoning:", "So the difference between data and reconstruction is:", "And the loss can be written as:", "Knowing that b are components of orthonormal basis and get the following:", "Then we can rearrange the sums and get this:", "And here comes a nice thing: provided our data is centered, a term in brackets is covariance matrix S=Cov(X), so we can write it down like this:", "We are going to minimize variance projected on {M+1, D} basis (this is the same as maximized variance on {1, M} basis, recap result in fig. 6).", "There is one last step to take: we have to minimize L with respect to b. Supposing that b is a formed orthonormal basis, it should satisfy the following equation:", "Thus we have a constraint optimization problem at hand. This can be solved with Lagrangian (if you not familiar with that, here is a great source):", "To find b we need to set gradient of Lagrangian to zero, so for derivatives with respect to b we get:", "And that's exactly an eigenvector problem for covariance matrix! That means that eigenvectors of covariance matrix are principal components (note that they are orthogonal because of covariance matrix symmetry) .", "We also can clearly see the meaning of corresponding eigenvalues, if we plug them in the final equation of the loss function:", "That means that in order to minimize L we should take a subspace spanned by D-M eigenvectors with the smallest eigenvalues. In other words, eigenvectors corresponding to the first M biggest eigenvalues span a subspace with maximum data variance. So if we sort eigenvectors by they eigenvalues in a descending order, the first eigenvector will represent the direction of the biggest data variance, the second will represent the direction of the biggest data variance from remaining orthogonal directions, and so on.", "Setting M=0, we can calculate maximal loss. It is equal to sum of all eigenvalues (this is quite obvious as we get rid of all principal components, i.e. we drop out all the information). That means we can calculate ratio of variance (information) contained in each principal component as follows:", "This makes it easy to decide how many components we should leave for dimensionality reduction. We can plot ratio of explained variance (scree plot) and its cumulative ratio for our convenience. Consider the example for data we already saw in fig. 7:", "So for that example if we reduce dimensionality by 1 dropping the third principal component out, we\u2019ll lose about 10% of information (data variance).", "For now we are ready to define a final algorithm for dimensionality reduction with principal component analysis.", "Finally here comes the PCA algorithm for data X consisting of N data points, where each point is a D dimension vector:", "2. Then we compute covariance matrix S for centered data and find it eigenvectors b and eigenvalues \u03bb.", "3. Sort eigenvectors by corresponding eigenvalues in descending order.", "4. Then we form matrix B from first M (M<D) eigenvectors and perform dimensionality reduction:", "5. For inverse transformation we need to get back to original space (find projections) and shift data back from the center adding mean values:", "We\u2019ve done much work, so let\u2019s have some fun. Remember our monstera leaf from the featured image? Let's compress it using PCA. It is [225, 255, 3] RGB image so we can apply PCA for every channel separately and explore explained variance for different number of principal components.", "Notice that a full decomposition consist of 255 components, at the same time the first component explains more than 40% of variance, and the first 40 components explain approximately 99% of variance. So we can compress the image using approximately 6 times smaller representation and being able to reconstruct it with a loss close to zero. Let's see how it looks:", "So far we have figured out a basic PCA algorithm and derived it using one of the possible ways. Besides basic PCA there are other more specific implementations such as sparse PCA, robust PCA, kernel PCA and others. One of such extensions is PCA from a perspective of latent variable model called probabilistic PCA, which I am curious to write a post about. Stay tuned for updates. Thanks for reading!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F37921e13cae7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@iamglebk?source=post_page-----37921e13cae7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Gleb Kumichev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4162ea7f758c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&user=Gleb+Kumichev&userId=4162ea7f758c&source=post_page-4162ea7f758c----37921e13cae7---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37921e13cae7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&user=Gleb+Kumichev&userId=4162ea7f758c&source=-----37921e13cae7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37921e13cae7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&source=-----37921e13cae7---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://zenodo.org/record/1430636#.XpGEef2A5hF", "anchor_text": "[1]"}, {"url": "https://mathworld.wolfram.com/OrthogonalDecomposition.html", "anchor_text": "orthogonal decomposition theorem"}, {"url": "https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives#lagrange-multipliers-and-constrained-optimization", "anchor_text": "great source"}, {"url": "https://medium.com/tag/data-science?source=post_page-----37921e13cae7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----37921e13cae7---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----37921e13cae7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/science?source=post_page-----37921e13cae7---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/tag/linear-algebra?source=post_page-----37921e13cae7---------------linear_algebra-----------------", "anchor_text": "Linear Algebra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37921e13cae7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&user=Gleb+Kumichev&userId=4162ea7f758c&source=-----37921e13cae7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37921e13cae7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&user=Gleb+Kumichev&userId=4162ea7f758c&source=-----37921e13cae7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37921e13cae7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=post_page-----37921e13cae7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4162ea7f758c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&user=Gleb+Kumichev&userId=4162ea7f758c&source=post_page-4162ea7f758c----37921e13cae7---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3cf8f5332a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&newsletterV3=4162ea7f758c&newsletterV3Id=3cf8f5332a3c&user=Gleb+Kumichev&userId=4162ea7f758c&source=-----37921e13cae7---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Written by Gleb Kumichev"}, {"url": "https://medium.com/@iamglebk/followers?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "67 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://www.linkedin.com/in/gleb-kumichev/", "anchor_text": "https://www.linkedin.com/in/gleb-kumichev/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4162ea7f758c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&user=Gleb+Kumichev&userId=4162ea7f758c&source=post_page-4162ea7f758c----37921e13cae7---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3cf8f5332a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7&newsletterV3=4162ea7f758c&newsletterV3Id=3cf8f5332a3c&user=Gleb+Kumichev&userId=4162ea7f758c&source=-----37921e13cae7---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56?source=author_recirc-----37921e13cae7----0---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=author_recirc-----37921e13cae7----0---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=author_recirc-----37921e13cae7----0---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Gleb Kumichev"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----37921e13cae7----0---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56?source=author_recirc-----37921e13cae7----0---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "The Inductive Bias of ML Models, and Why You Should Care About ItWhat inductive bias is, and how it can harm or help your models"}, {"url": "https://towardsdatascience.com/the-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56?source=author_recirc-----37921e13cae7----0---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "\u00b711 min read\u00b7Jun 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F979fe02a1a56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56&user=Gleb+Kumichev&userId=4162ea7f758c&source=-----979fe02a1a56----0-----------------clap_footer----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56?source=author_recirc-----37921e13cae7----0---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F979fe02a1a56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56&source=-----37921e13cae7----0-----------------bookmark_preview----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----37921e13cae7----1---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----37921e13cae7----1---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----37921e13cae7----1---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----37921e13cae7----1---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----37921e13cae7----1---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----37921e13cae7----1---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----37921e13cae7----1---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----37921e13cae7----1-----------------bookmark_preview----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----37921e13cae7----2---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----37921e13cae7----2---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----37921e13cae7----2---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----37921e13cae7----2---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----37921e13cae7----2---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----37921e13cae7----2---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----37921e13cae7----2---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----37921e13cae7----2-----------------bookmark_preview----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deconstructing-a-frequent-misconception-about-confidence-intervals-33a95b24f387?source=author_recirc-----37921e13cae7----3---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=author_recirc-----37921e13cae7----3---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=author_recirc-----37921e13cae7----3---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Gleb Kumichev"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----37921e13cae7----3---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deconstructing-a-frequent-misconception-about-confidence-intervals-33a95b24f387?source=author_recirc-----37921e13cae7----3---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "Deconstructing a frequent misconception about confidence intervals.Confidence intervals are commonly used as a routine approach of interval estimation. If you\u2019ve ever applied this approach, you may have\u2026"}, {"url": "https://towardsdatascience.com/deconstructing-a-frequent-misconception-about-confidence-intervals-33a95b24f387?source=author_recirc-----37921e13cae7----3---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": "\u00b77 min read\u00b7Jan 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33a95b24f387&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-a-frequent-misconception-about-confidence-intervals-33a95b24f387&user=Gleb+Kumichev&userId=4162ea7f758c&source=-----33a95b24f387----3-----------------clap_footer----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deconstructing-a-frequent-misconception-about-confidence-intervals-33a95b24f387?source=author_recirc-----37921e13cae7----3---------------------344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33a95b24f387&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-a-frequent-misconception-about-confidence-intervals-33a95b24f387&source=-----37921e13cae7----3-----------------bookmark_preview----344d96ac_4eaf_4d41_9ec8_a0ae1e7ebd37-------", "anchor_text": ""}, {"url": "https://medium.com/@iamglebk?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "See all from Gleb Kumichev"}, {"url": "https://towardsdatascience.com/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Carla Martins"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "How to Compare and Evaluate Unsupervised Clustering Methods?Using Python, Scikit-Learn, and Google Colab"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "\u00b720 min read\u00b7Feb 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&user=Carla+Martins&userId=a1022761a1b&source=-----84f3617e3769----0-----------------clap_footer----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&source=-----37921e13cae7----0-----------------bookmark_preview----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----37921e13cae7----1-----------------bookmark_preview----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----0-----------------clap_footer----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----37921e13cae7----0---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----37921e13cae7----0-----------------bookmark_preview----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----37921e13cae7----1---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----37921e13cae7----1-----------------bookmark_preview----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----37921e13cae7----2---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----37921e13cae7----2---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----37921e13cae7----2---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----37921e13cae7----2---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----37921e13cae7----2---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----37921e13cae7----2---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----37921e13cae7----2---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----37921e13cae7----2-----------------bookmark_preview----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----37921e13cae7----3---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----37921e13cae7----3---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----37921e13cae7----3---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----37921e13cae7----3---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----37921e13cae7----3---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----37921e13cae7----3---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----3-----------------clap_footer----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----37921e13cae7----3---------------------93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "90"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----37921e13cae7----3-----------------bookmark_preview----93fe09dc_b3f7_4e10_8cff_be92cfd6b68e-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----37921e13cae7--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----37921e13cae7--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}