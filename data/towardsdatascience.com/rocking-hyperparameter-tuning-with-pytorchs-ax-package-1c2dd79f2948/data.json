{"url": "https://towardsdatascience.com/rocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948", "time": 1683003941.277882, "path": "towardsdatascience.com/rocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948/", "webpage": {"metadata": {"title": "Rocking Hyperparameter Tuning with PyTorch\u2019s Ax Package | by Marina Gandlin | Towards Data Science", "h1": "Rocking Hyperparameter Tuning with PyTorch\u2019s Ax Package", "description": "Hyperparameter tuning is a must with many machine learning tasks. We usually work hard on selecting the right algorithm and architecture for our problem, then train rigorously to get a great model\u2026"}, "outgoing_paragraph_urls": [{"url": "https://ax.dev/docs/installation.html", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://ax.dev/api/service.html", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://ax.dev/docs/why-ax.html", "anchor_text": "documentation", "paragraph_index": 15}, {"url": "https://ax.dev/tutorials/", "anchor_text": "tutorials", "paragraph_index": 15}, {"url": "https://ax.dev/tutorials/visualizations.html", "anchor_text": "visualizations", "paragraph_index": 15}], "all_paragraphs": ["Hyperparameter tuning is a must with many machine learning tasks. We usually work hard on selecting the right algorithm and architecture for our problem, then train rigorously to get a great model. Doing hyperparameter tuning (HPT) after these two might seem unnecessary, but it is, in fact, crucial. HPT should be done periodically and might help us achieve great improvements in performance at a small effort.", "What i\u2019m suggesting here is an easy way to implement HPT with a newly released python package. It will work perfectly and take no more than half an hour to set up for anything you train on your own computer. For other models, especially ones that require training on deployments or running in production, this will work with some small adjustments we will further discuss in part B of this post.", "Ax is an open-source package from PyTorch that helps you find a minima for any function over the range of parameters you define. The most common use case in machine learning is finding the best hyperparameters for training a model, the ones that will bring your overall loss to a minimum. The package does this by running multiple runs of training, each with a different set of parameters and returning the ones that gave the lowest loss. The trick is that it does so without grid search or random search over these parameters, but with a more sophisticated algorithm, hence saving a lot of training and run-time.", "Ax can find minimas for both continuous parameters (say, learning rate) and discrete parameters (say, size of a hidden layer). It uses bayesian optimization for the former and bandit optimization for the latter. Even though the package is from pytorch, it will work for any function, as long as it returns a single value you want to minimize.", "Before we start \u2014 Installation instructions can be found here.", "Ax has a couple of operating modes, but we\u2019ll start from the most basic one with a small example. As mentioned before, we\u2019ll usually use Ax to help us find the best hyperparameters, but at its core, this package helps us find a function\u2019s minimum with respect to some parameters. That\u2019s why for this example we\u2019ll run Ax to find the minimum of a complex quadratic function. For that we\u2019ll define a function named booth that receives its parameters {x1,x2} within a dictionary p:", "To find the minimum for \u201cbooth\u201d, we\u2019ll let ax run the function several times. The parameters it chooses for each run are dependent on previous runs \u2014 which parameters it ran on and what was the result of the function on these parameters (in machine learning, \u201cresult\u201d == \u201closs\u201d). Running this next code bit performs 20 consecutive function runs, with 20 different sets of parameters {x1,x2}. Then prints out the \u201cbest_parameters\u201d. For comparison, if we wanted to run this with grid search, with jumps of 0.5 for x1 and x2, we would need 1600 runs instead of 20!", "Pretty close! This probably takes less than 10 seconds on a laptop. You might notice that the \u201cbest parameters\u201d that were returned are not exactly the true minimum, but they bring the function very close to the actual minimum. How forgiving you are to margins in the result is a parameter you can play with later.", "The only thing we needed to do here is to make sure we have a function with a dictionary as input that returns a single value. This is what you\u2019ll need in most cases when running ax.", "Part A was pretty easy and will work wonderfully on anything you run on your computer, but it has a couple of faults:", "For that we need something more sophisticated: preferably, some kind of magic way to just get a couple sets of parameters to run on, so we could deploy trainings and then patiently wait until they get results back. When the first batch of trainings is finished, we can let ax know what were the losses, get the next set of parameters and start the next batch. In ax lingo, this way of running ax this is what\u2019s called \u201cax service\u201d. It is quick simple to phrase and run, almost as simple as what we saw in part A.", "To fit the \u201cax paradigm\u201d we still need some kind of function that runs the training and returns a loss, maybe something that looks like this:", "After defining a proper wrapper, we could run ax service HPT code. Since I want you to be able to run this demo code on your computer, I will stick to using the quadratic function we introduced in part A \u2014 \u201cbooth\u201d. The parameter ranges for {x1,x2} will stay the same. For running deployments I could replace booth in the next sample code with my version of \u201ctraining_wrapper\u201d. The sample code looks like this:", "This is a little different from part A. We no longer just use the \u201coptimize\u201d function. Instead:", "Meaning, we separated getting the next parameters for the next run and actually running. If you want to run concurrently, you get N get_next_trial() at a time and run them async. Make sure you don\u2019t forget setting the \u201cenforce_sequential_optimization\u201d flag to false if you want to do so. If you are wondering how many runs you could do concurrently, you can use the get_recommended_max_parallelism (Read about the output of this function here).", "That's pretty much it. The package is wonderfully customizable and could probably handle whatever you might want to change for your specific environment. The documentation is very readable, especially the tutorials. It also has a wide variety of visualizations to help you figure out what\u2019s going on.", "The days when you had to manually set up multiple runs or use grid search are pretty much over. It is true that on a very large set of hyperparameters ax might result to something resembling random search, but even if it didn\u2019t reduce the number of trainings you ended up running, it still saved you the coding and the handling of the multiple runs. Setting up is super easy \u2014 I highly recommend you try it yourself!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data science team lead at Taboola. Used to design satellites, but thinks recommendation systems are way cooler."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1c2dd79f2948&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marinagandlin?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marinagandlin?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "Marina Gandlin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b56ffc1598f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&user=Marina+Gandlin&userId=2b56ffc1598f&source=post_page-2b56ffc1598f----1c2dd79f2948---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c2dd79f2948&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c2dd79f2948&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@jarcou", "anchor_text": "Jaros\u0142aw Mi\u015b"}, {"url": "https://ax.dev/docs/installation.html", "anchor_text": "here"}, {"url": "https://ax.dev/api/service.html", "anchor_text": "here"}, {"url": "https://ax.dev/docs/why-ax.html", "anchor_text": "documentation"}, {"url": "https://ax.dev/tutorials/", "anchor_text": "tutorials"}, {"url": "https://ax.dev/tutorials/visualizations.html", "anchor_text": "visualizations"}, {"url": "https://ax.dev/docs/why-ax.html", "anchor_text": "https://ax.dev/docs/why-ax.html"}, {"url": "https://ax.dev/tutorials/", "anchor_text": "https://ax.dev/tutorials/"}, {"url": "https://ax.dev/tutorials/visualizations.html", "anchor_text": "https://ax.dev/tutorials/visualizations.html"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----1c2dd79f2948---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/tag/hyperparameter?source=post_page-----1c2dd79f2948---------------hyperparameter-----------------", "anchor_text": "Hyperparameter"}, {"url": "https://medium.com/tag/ax?source=post_page-----1c2dd79f2948---------------ax-----------------", "anchor_text": "Ax"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----1c2dd79f2948---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1c2dd79f2948---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c2dd79f2948&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&user=Marina+Gandlin&userId=2b56ffc1598f&source=-----1c2dd79f2948---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c2dd79f2948&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&user=Marina+Gandlin&userId=2b56ffc1598f&source=-----1c2dd79f2948---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c2dd79f2948&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1c2dd79f2948&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1c2dd79f2948---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1c2dd79f2948--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marinagandlin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marinagandlin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marina Gandlin"}, {"url": "https://medium.com/@marinagandlin/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "39 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b56ffc1598f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&user=Marina+Gandlin&userId=2b56ffc1598f&source=post_page-2b56ffc1598f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff5e5f76f2b30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948&newsletterV3=2b56ffc1598f&newsletterV3Id=f5e5f76f2b30&user=Marina+Gandlin&userId=2b56ffc1598f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}