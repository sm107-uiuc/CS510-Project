{"url": "https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8", "time": 1682994602.602633, "path": "towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8/", "webpage": {"metadata": {"title": "Entropy: How Decision Trees Make Decisions | by Sam T | Towards Data Science", "h1": "Entropy: How Decision Trees Make Decisions", "description": "You\u2019re a Data Scientist in training. You\u2019ve come a long way from writing your first line of Python or R code. You know your way around Scikit-Learn like the back of your hand. You spend more time on\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["You\u2019re a Data Scientist in training. You\u2019ve come a long way from writing your first line of Python or R code. You know your way around Scikit-Learn like the back of your hand. You spend more time on Kaggle than Facebook now. You\u2019re no stranger to building awesome random forests and other tree based ensemble models that get the job done. However , you\u2019re nothing if not thorough. You want to dig deeper and understand some of the intricacies and concepts behind popular machine learning models. Well , so do I.", "In this blog post, I will introduce the concept of Entropy as a general topic in statistics which will allow me to further introduce the concept of Information Gain and subsequently explain why both these fundamental concepts form the basis of how decision trees build themselves on the data we supply them.", "Right. Let\u2019s get on with it then.", "What is Entropy? In the most layman terms, Entropy is nothing but the measure of disorder. (You can think of it as a measure of purity as well. You\u2019ll see. I like disorder because it sounds cooler.)", "The Mathematical formula for Entropy is as follows -", "Where \u2018Pi\u2019 is simply the frequentist probability of an element/class \u2018i\u2019 in our data. For simplicity\u2019s sake let\u2019s say we only have two classes , a positive class and a negative class. Therefore \u2018i\u2019 here could be either + or (-). So if we had a total of 100 data points in our dataset with 30 belonging to the positive class and 70 belonging to the negative class then \u2018P+\u2019 would be 3/10 and \u2018P-\u2019 would be 7/10. Pretty straightforward.", "If I was to calculate the entropy of my classes in this example using the formula above. Here\u2019s what I would get.", "The entropy here is approximately 0.88. This is considered a high entropy , a high level of disorder ( meaning low level of purity). Entropy is measured between 0 and 1.(Depending on the number of classes in your dataset, entropy can be greater than 1 but it means the same thing , a very high level of disorder. For the sake of simplicity, the examples in this blog will have entropy between 0 and 1).", "Take a look at this graph below.", "The x-axis measures the proportion of data points belonging to the positive class in each bubble and the y-axis axis measures their respective entropies. Right away, you can see the inverted \u2018U\u2019 shape of the graph. Entropy is lowest at the extremes, when the bubble either contains no positive instances or only positive instances. That is, when the bubble is pure the disorder is 0. Entropy is highest in the middle when the bubble is evenly split between positive and negative instances. Extreme disorder , because there is no majority.", "Does it matter why entropy is measured using log base 2 or why entropy is measured between 0 and 1 and not some other range? No. It\u2019s just a metric. It\u2019s not important to know how it came to be. It\u2019s important to know how to read it and what it tells us, which we just did above. Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.", "Now we know how to measure disorder. Next we need a metric to measure the reduction of this disorder in our target variable/class given additional information( features/independent variables) about it. This is where Information Gain comes in. Mathematically it can be written as:", "We simply subtract the entropy of Y given X from the entropy of just Y to calculate the reduction of uncertainty about Y given an additional piece of information X about Y. This is called Information Gain. The greater the reduction in this uncertainty, the more information is gained about Y from X.", "Let me illustrate with an example using a simple contingency table and then I\u2019ll bring all of it together with how decisions trees use entropy and information gain to decide what feature to split their nodes on as they are being trained on a data set.", "Here our target variable is Liability which can take on two values \u201cNormal\u201d and \u201cHigh\u201d and we only have one feature called Credit Rating which can take on values \u201cExcellent\u201d, \u201cGood\u201d and \u201cPoor\u201d. There are a total of 14 observations. 7 of them belong to the Normal Liability class and 7 belong to High Liability Class. So it\u2019s an even split by itself. Summing across the top row we can see there are 4 observations that have value Excellent for the feature credit rating. Furthermore , I can also see how my target variable is split for \u201cExcellent\u201d Credit Rating. For observations that take have value \u201cExcellent\u201d for their credit rating there are 3 that belong to the Normal Liability class and only 1 that belongs to the High Liability class. I can similarly figure out such values for other values of Credit Rating from the contingency table.", "For this illustration , I will use this contingency table to calculate the entropy of our target variable by itself and then calculate the entropy of our target variable given additional information about the feature, credit rating. This will allow me to calculate how much additional information does \u201cCredit Rating\u201d provide for my target variable \u201cLiability\u201d.", "The entropy of our target variable is 1, at maximum disorder due to the even split between class label \u201cNormal\u201d and \u201cHigh\u201d. Our next step is to calculate the entropy of our target variable Liability given additional information about credit score. For this we will calculate the entropy for Liability for each value of Credit Score and add them using a weighted average of the proportion of observations that end up in each value. Why we use a weighted average will become clearer when we discuss this in the context of decision trees.", "We got the entropy for our target variable given the feature Credit Rating. Now we can compute the Information Gain on Liability from Credit Rating to see how informative this feature is.", "Knowing the Credit Rating helped us reduce the uncertainty around our target variable, Liability! Isn\u2019t that what a good feature is supposed to do? Provide us information about our target variable? Well that\u2019s exactly how and why decision trees use entropy and information gain to determine which feature to split their nodes on to get closer to predicting the target variable with each split and also to determine when to stop splitting the tree! ( in addition to hyper-parameters like max depth of course). Let\u2019s see this in action with another example using decision trees.", "Consider an example where we are building a decision tree to predict whether a loan given to a person would result in a write-off or not. Our entire population consists of 30 instances. 16 belong to the write-off class and the other 14 belong to the non-write-off class. We have two features, namely \u201cBalance\u201d that can take on two values -> \u201c< 50K\u201d or \u201c>50K\u201d and \u201cResidence\u201d that can take on three values -> \u201cOWN\u201d, \u201cRENT\u201d or \u201cOTHER\u201d. I\u2019m going to show you how a decision tree algorithm would decide what attribute to split on first and what feature provides more information, or reduces more uncertainty about our target variable out of the two using the concepts of Entropy and Information Gain.", "The dots are the data points with class right-off and the stars are the non-write-offs. Splitting the parent node on attribute balance gives us 2 child nodes. The left node gets 13 of the total observations with 12/13 ( 0.92 probability) observations from the write-off class and only 1/13( 0.08 probability) observations from the non-write of class. The right node gets 17 of the total observation with 13/17( 0.76 probability) observations from the non-write-off class and 4/17 ( 0.24 probability) from the write-off class.", "Let\u2019s calculate the entropy for the parent node and see how much uncertainty the tree can reduce by splitting on Balance.", "Splitting on feature ,\u201cBalance\u201d leads to an information gain of 0.37 on our target variable. Let\u2019s do the same thing for feature, \u201cResidence\u201d to see how it compares.", "Splitting the tree on Residence gives us 3 child nodes. The left child node gets 8 of the total observations with 7/8 (0.88 probability) observations from the write-off class and only 1/8 (0.12 probability) observations from the non-write-off class. The middle child nodes gets 10 of the total observations with 4/10 (0.4 probability) observations of the write-off class and 6/10( 0.6 probability) observations from the non-write-off class. The right child node gets 12 of the total observations with 5/12 ( 0.42 probability) observations from the write-off class and 7/12 ( 0.58 ) observations from the non-write-off class. We already know the entropy for the parent node. We simply need to calculate the entropy after the split to compute the information gain from \u201cResidence\u201d", "The information gain from feature, Balance is almost 3 times more than the information gain from Residence! If you go back and take a look at the graphs you can see that the child nodes from splitting on Balance do seem purer than those of Residence. However the left most node for residence is also very pure but this is where the weighted averages come in play. Even though that node is very pure, it has the least amount of the total observations and a result contributes a small portion of it\u2019s purity when we calculate the total entropy from splitting on Residence. This is important because we\u2019re looking for overall informative power of a feature and we don\u2019t want our results to be skewed by a rare value in a feature.", "By itself the feature, Balance provides more information about our target variable than Residence. It reduces more disorder in our target variable. A decision tree algorithm would use this result to make the first split on our data using Balance. From here on, the decision tree algorithm would use this process at every split to decide what feature it is going to split on next. In a real world scenario , with more than two features the first split is made on the most informative feature and then at every split the information gain for each additional feature needs to be recomputed because it would not be the same as the information gain from each feature by itself. The entropy and information gain would have to be calculated after one or more splits have already been made which would change the results. A decision tree would repeat this process as it grows deeper and deeper till either it reaches a pre-defined depth or no additional split can result in a higher information gain beyond a certain threshold which can also usually be specified as a hyper-parameter!", "There you have it! You now know what entropy and information gain are and how they are computed. You understand how a decision tree either by itself or in a tree based ensemble decides on the best order of features to split on and decides when to stop when it trains itself on given data. If you every have to explain the intricacies of how decision trees work to someone, hopefully you won\u2019t do too bad.", "I hope you were able to get some value from this post. If there is anything that I missed or something was inaccurate or if you have absolutely any feedback , please let me know in the comments. I would greatly appreciate it. Thank you.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2946b9c18c8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@satyamtandon?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@satyamtandon?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "Sam T"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F37a762d5c9bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&user=Sam+T&userId=37a762d5c9bc&source=post_page-37a762d5c9bc----2946b9c18c8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2946b9c18c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2946b9c18c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2946b9c18c8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2946b9c18c8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----2946b9c18c8---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----2946b9c18c8---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2946b9c18c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&user=Sam+T&userId=37a762d5c9bc&source=-----2946b9c18c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2946b9c18c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&user=Sam+T&userId=37a762d5c9bc&source=-----2946b9c18c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2946b9c18c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2946b9c18c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2946b9c18c8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2946b9c18c8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2946b9c18c8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2946b9c18c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@satyamtandon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@satyamtandon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sam T"}, {"url": "https://medium.com/@satyamtandon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "610 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F37a762d5c9bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&user=Sam+T&userId=37a762d5c9bc&source=post_page-37a762d5c9bc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fadda52389abf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-how-decision-trees-make-decisions-2946b9c18c8&newsletterV3=37a762d5c9bc&newsletterV3Id=adda52389abf&user=Sam+T&userId=37a762d5c9bc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}