{"url": "https://towardsdatascience.com/python-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd", "time": 1683002609.9883862, "path": "towardsdatascience.com/python-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd/", "webpage": {"metadata": {"title": "Python One-liner Distributed Acceleration with Wordbatch Apply | by Antti Puurula | Towards Data Science", "h1": "Python One-liner Distributed Acceleration with Wordbatch Apply", "description": "Computing hardware is undergoing a period of rapid evolution due to advances in semiconductor manufacturing processes, with 64-core consumer CPUs and 80-core server CPUs arriving at the market this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ray-project/ray", "anchor_text": "Ray", "paragraph_index": 2}, {"url": "https://github.com/dask/dask", "anchor_text": "Dask", "paragraph_index": 2}, {"url": "https://github.com/apache/spark", "anchor_text": "Spark", "paragraph_index": 2}, {"url": "https://github.com/anttttti/Wordbatch", "anchor_text": "Wordbatch", "paragraph_index": 2}, {"url": "https://github.com/tomMoral/loky", "anchor_text": "Loky", "paragraph_index": 2}, {"url": "https://github.com/anttttti/Wordbatch/releases/tag/1.4.4", "anchor_text": "Wordbatch 1.4.4", "paragraph_index": 3}, {"url": "https://github.com/anttttti/Wordbatch/blob/master/scripts/decorator_test.py", "anchor_text": "benchmark script", "paragraph_index": 8}, {"url": "https://github.com/anttttti/Wordbatch/blob/master/scripts/decorator_test.py", "anchor_text": "benchmark script", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/benchmarking-python-distributed-ai-backends-with-wordbatch-9872457b785c", "anchor_text": "blog post", "paragraph_index": 33}], "all_paragraphs": ["Computing hardware is undergoing a period of rapid evolution due to advances in semiconductor manufacturing processes, with 64-core consumer CPUs and 80-core server CPUs arriving at the market this year. This is bringing about a quantitative leap in CPU performance, after a decade of stagnant development.", "Utilising this increased parallel computing power in Python programs is best done using the multiprocessing library, due to the Global Interpreter Lock (GIL) complicating efficient thread-level parallelism. However, multiprocessing is limited to a single computing node, and setting up multiprocessing tasks requires additional boilerplate code for each task to be parallelized.", "Recent distributed Python processing frameworks such as Ray, Dask and Spark enable distribution of Python processes and data much like multiprocessing, but across a network of workers. Wordbatch is a toolkit that enables using these frameworks as swappable distributed backends. It provides the orchestrator class Batcher, which distributes pipelines of processing tasks on a distributed backend, or a local one, such as multiprocessing or Loky. The most basic pipeline is the Apply-class, which takes an input function, and runs it as a single Map-Reduce operation on the backend.", "Apply is a minimal processing pipeline taking a function or method, and executing it as a minibatch Map-Reduce operation to any iterable data, such as a list, Numpy array, Pandas Series or DataFrame. Starting from Wordbatch 1.4.4, you can import a decorator wrapper to Apply, called decorator_apply, or imported simply as apply. Apply can be used as a Python \u201c@\u201d decorator, or calling it on a function to distribute, as is done in this article. Simply:", "This produces a one-liner Map-Reduce convention for list operations, that can be conveniently used throughout compute-intensive Python programs, AI and data science pipelines in particular.", "The example above uses the default Python multiprocessing backend with all available cores on the computing node. We can choose the backend by passing a Batcher object that has been initialized with the backend, and optionally the backend handle for distributed processing. With Ray this would be done as follows:", "By changing the backend and backend_handle arguments for Batcher, one can easily choose the backend. In fact, the backend is swappable, so that simply changing the batcher.backend and batcher.backend_handle attributes between processing runs will change the scheduler.", "Apply was initially added as a basic minimal pipeline template to Wordbatch. However, as known to those familiar with the Map-Reduce framework, entire data processing pipelines can be executed as Map-Reduce operations over a large network of computing nodes. Aside from feature engineering pipelines shown in this blog post, other practical tasks for Apply include distributing:", "The following code snippets are adapted from the benchmark script, which runs benchmarks using different backend schedulers to Apply. These examples can be easily adjusted for real-world use cases.", "As a first example, we can distribute a global function \u201cnormalize_text\u201d across an iterable of text strings, and store the result as a column \u201ctext\u201d in a DataFrame \u201cdf\u201d:", "References to global functions and variables get automatically sent to each new process. This makes serialization of all required objects trivial, but with longer programs unnecessary variables left in the global namespace will add to the parallelization overhead. Local functions are used the same way as global ones, but standard Python multiprocessing won\u2019t serialize a local function. Loky-fork of multiprocessing and the distributed backends will handle local functions.", "Other types of functions are accelerated just as easily, with minor caveats. We can accelerate a lambda function:", "Just like local functions, lambda functions are not serialized by standard Python multiprocessing. There are many workarounds for this, such as using Loky, or just defining the lambda function as a global function instead.", "An object method can be accelerated the same way:", "With object methods no changes are updated to the original object from the method call, since the method is executed on parallelized copies. If changes are required, basic use of Apply wouldn\u2019t be sufficient. A workaround in many cases would be a more complex pipeline, handling both the processed list data, and updates to the parallelized object.", "Finally, Pandas GroupBy aggregations are a very common type of data processing operation, often forming the bottlenecks in AI and data science processing pipelines, such as feature engineering tasks. There are several ways to use Apply here, for example:", "The aggregation results are mapped back to the original DataFrame using the group ids of the GroupBy. For the purpose of clarity, the operation is expanded into three lines. The minibatch size for Batcher is modified to 200 from the default 20000, since there will be a relatively small number of groups to iterate through, compared to the original DataFrame rows.", "A problem with any group-wise parallelized GroupBy operation is that with real data groups are unevenly distributed, often following a power-law distribution, with most groups having few members. This often negates any benefit from parallelization, since simple per-group operations result in a large parallelization overhead compared to the gains. A simple non-optimal solution is to bin the grouping variable, by hashing and binning it to produce a less sparse grouping variable, and then do the original GroupBy-operations within each binned GroupBy, as follows:", "Compared to the unbinned parallel GroupBy, the \u201cfirst_word_stemmed\u201d variable is now the unbinned variable that gets processed within each minibatch, and \u201cfirst_word_stemmed_hashbin\u201d is the binned grouping variable that splits data into the distributed minibatches. Here 500 bins are used for the binning variable, and the minibatch size for batches is reduced to 10, as each minibatch has more data now on average. Optimal values for these depend on the data.", "Apply also has basic options for per-minibatch acceleration with caching and vectorization. Cache uses functools.lru_cache of the chosen size to store results within each processed minibatch, and should be used when there are multiple repeated values and each function to evaluate is costly. Vectorize uses Numba vectorization of the function, and can be used when the input and output variable types are known in advance.", "Taking the stemming example, caching can be simply added:", "Running combinations of the speedup options, backends and tasks reveals some edge cases where workarounds are needed. For instance, caching with lru_cache won\u2019t directly work with DataFrames, as DataFrames are mutable, and lru_cache will only take immutable input data.", "Accelerating the minibatch processing with Numba is supported for basic cases. Following shows an example with two input variable columns:", "Here the Numba variable types for vectorizing the \u201cdiv\u201d function are defined as the argument \u201cvectorize=[float64(int64, int64)\u201d ]. Float64 defines the output variable type, and int64s define the input variable type. The input for Batcher is constructed by selecting the 2-d Numpy array from the DataFrame columns \u201clen_text\u201d and \u201clen_text_normalized\u201d.", "The main caveat with Numba is that it is still not a fully developed framework, and for more complex examples than this one would need a good understanding of Numba internals. For example, vectorizing over array inputs requires Numba guvectorize instead of the vectorize decorator, and several changes required by this. Numeric multidimensional array operations is where Numba could offer considerable acceleration over native Python code, including GPU acceleration with Cuda. But since these are complicated to set up and extend, a more promising path for mini-batch acceleration could be using Python extensions written with a full low-level language such as C, Cython, or Rust.", "The acceleration speedups can be compared to a baseline function without Apply, and with different scheduler backends for Batcher. Following numbers are from running the benchmark script on each backend separately. Tests times are from using a single i9\u20139900k CPU with 8 cores and 16 threads, running at 4.9GHz with all cores. All of the tests were run on over 1.28M rows of text data from Tripadvisor reviews, using minibatch sizes of 5000 for functions aside the GroupBy aggregations.", "The first example used Apply to distribute a global text normalization function. This does relatively complex processing for each processed string, so the speedup is nearly core-linear with both Loky and Ray backends, with 6.9x speedup for Ray, and 6.6x speedup for Loky.", "Moving on to the lambda function splitting the first word of each textline, and the object method stemming the first word, we get smaller speedups, but still multiple times faster than not using Wordbatch, or using a single process as the backend. For splitting we get 1.8x speedup with Loky, and 3.2x speedup with Ray. For stemming these are 3.1x for Loky, 5.14x for Ray. Both operations are still relatively complex, so gains are achieved despite the parallelization overhead.", "Comparing the GroupBy mean aggregation, we get the first case where parallelization doesn\u2019t directly give a speedup. Since most of the groups distributed have only a few members, and the mean operation for each group is very simple, the parallelization overhead dominates the results of a simple parallelized GroupBy. When using a binned GroupBy with Ray backend, we still get a 1.9x speedup over not using Batcher. If a more intensive list of operations was done within each binned GroupBy, the speedups would again be closer to linear.", "Caching seems to have a massive effect when applied to stemming. Since the input is highly repetitive and the per-item cost is high, caching works in this scenario. Best results are achieved without Batcher, where the lru_cache of 1000 items achieves 0.37s compared to 10.72s without caching. Applying caching within each minibatch of a Ray backend, we get 0.42s. If caching is exceptionally useful, it can be better not to use parallelization. Some use cases could benefit from both parallelization and within-minibatch caching.", "Finally, we can look at how well Numba vectorization works, when distributing a simple division function between two columns. The cost of compiling the Numba function within each minibatch far outweight the gain from parallelizing this simple function. The \u201cno Wordbatch\u201d baseline here additionally uses list comprehension instead of Numpy vectorized array division, which would be many times faster still. Overall, distributed Numba vectorization will only be useful for more complex mathematical expressions, but these could be limited by current Numba capabilities.", "This blog post introduced basic use cases of the recently added apply-decorator in the Wordbatch toolkit. This provides a simple one-liner call that takes any Python function or method and distributes it on a Map-Reduce basis across a chosen backend, either locally or distributed on a scheduler such as Ray, Dask or Spark. Correct use can give close to linear speedups in the number of available cores for many processing tasks.", "Taking full advantage of the apply-decorator requires some understanding of the computational costs of parallelization and the functions that are parallelized. If the methods are very computationally simple compared to the parallelization overhead, then there is little gain. If the opposite is true, one approaches linear speedups.", "For real-world AI pipelines parallelization is best done at the highest level, distributing an entire pipeline function over large minibatches. This reduces the parallelization cost to a single data transfer to the processes, and each large mini-batch can be processed with the full pipeline. With the apply-decorator this can be done by distributing a per-DataFrame function over large groups, as shown in the binned GroupBy example. Alternatively, Wordbatch has a related class ApplyBatch and its decorator function decorator_apply_batch. These work like Apply, but apply functions to evenly-split batches directly, and don\u2019t require setting up variable binning as with the binned GroupBy. My first blog post comparing the Python distributed AI backends gave an example of using ApplyBatch.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fefccecec22dd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----efccecec22dd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----efccecec22dd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@antti.puurula?source=post_page-----efccecec22dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@antti.puurula?source=post_page-----efccecec22dd--------------------------------", "anchor_text": "Antti Puurula"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd31c16454dd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&user=Antti+Puurula&userId=d31c16454dd3&source=post_page-d31c16454dd3----efccecec22dd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fefccecec22dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fefccecec22dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/ray-project/ray", "anchor_text": "Ray"}, {"url": "https://github.com/dask/dask", "anchor_text": "Dask"}, {"url": "https://github.com/apache/spark", "anchor_text": "Spark"}, {"url": "https://github.com/anttttti/Wordbatch", "anchor_text": "Wordbatch"}, {"url": "https://github.com/tomMoral/loky", "anchor_text": "Loky"}, {"url": "https://github.com/anttttti/Wordbatch/releases/tag/1.4.4", "anchor_text": "Wordbatch 1.4.4"}, {"url": "https://github.com/anttttti/Wordbatch/blob/master/scripts/decorator_test.py", "anchor_text": "benchmark script"}, {"url": "https://github.com/anttttti/Wordbatch/blob/master/scripts/decorator_test.py", "anchor_text": "benchmark script"}, {"url": "https://towardsdatascience.com/benchmarking-python-distributed-ai-backends-with-wordbatch-9872457b785c", "anchor_text": "blog post"}, {"url": "https://medium.com/tag/python?source=post_page-----efccecec22dd---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----efccecec22dd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----efccecec22dd---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----efccecec22dd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----efccecec22dd---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fefccecec22dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&user=Antti+Puurula&userId=d31c16454dd3&source=-----efccecec22dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fefccecec22dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&user=Antti+Puurula&userId=d31c16454dd3&source=-----efccecec22dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fefccecec22dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----efccecec22dd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fefccecec22dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----efccecec22dd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----efccecec22dd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----efccecec22dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----efccecec22dd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----efccecec22dd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----efccecec22dd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----efccecec22dd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----efccecec22dd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----efccecec22dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@antti.puurula?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@antti.puurula?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Antti Puurula"}, {"url": "https://medium.com/@antti.puurula/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "72 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd31c16454dd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&user=Antti+Puurula&userId=d31c16454dd3&source=post_page-d31c16454dd3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd37b752caa64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-one-liner-distributed-acceleration-with-wordbatch-apply-efccecec22dd&newsletterV3=d31c16454dd3&newsletterV3Id=d37b752caa64&user=Antti+Puurula&userId=d31c16454dd3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}