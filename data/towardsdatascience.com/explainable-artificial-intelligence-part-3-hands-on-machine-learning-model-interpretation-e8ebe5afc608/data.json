{"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608", "time": 1682994193.90303, "path": "towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608/", "webpage": {"metadata": {"title": "Hands-on Machine Learning Model Interpretation | by Dipanjan (DJ) Sarkar | Towards Data Science", "h1": "Hands-on Machine Learning Model Interpretation", "description": "Interpreting Machine Learning models is no longer a luxury but a necessity given the rapid adoption of AI in the industry. This article in a continuation in my series of articles aimed at\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2 of this series", "paragraph_index": 2}, {"url": "https://archive.ics.uci.edu/ml/datasets/adult", "anchor_text": "UCI ML repository", "paragraph_index": 8}, {"url": "https://xgboost.readthedocs.io/en/latest/", "anchor_text": "XGBoost", "paragraph_index": 19}, {"url": "https://github.com/TeamHG-Memex/eli5", "anchor_text": "ELI5", "paragraph_index": 26}, {"url": "https://eli5.readthedocs.io/en/latest/overview.html", "anchor_text": "the documentation", "paragraph_index": 27}, {"url": "http://blog.datadive.net/interpreting-random-forests/", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://oracle.github.io/Skater/install.html", "anchor_text": "installing skater", "paragraph_index": 36}, {"url": "https://oracle.github.io/Skater/gallery.html", "anchor_text": "Gallery of Interactive Notebook", "paragraph_index": 39}, {"url": "https://christophm.github.io/interpretable-ml-book/lime.html", "anchor_text": "LIME chapter", "paragraph_index": 64}, {"url": "http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions", "anchor_text": "SHAP NIPS paper", "paragraph_index": 84}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2", "paragraph_index": 84}, {"url": "https://pypi.org/project/shap", "anchor_text": "PyPI", "paragraph_index": 85}, {"url": "https://arxiv.org/abs/1802.03888", "anchor_text": "Tree SHAP arXiv paper", "paragraph_index": 86}, {"url": "https://arxiv.org/pdf/1802.03888.pdf", "anchor_text": "Consistent Individualized Feature Attribution for Tree Ensembles", "paragraph_index": 89}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2", "paragraph_index": 118}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_model_interpretation_xai", "anchor_text": "my GitHub", "paragraph_index": 122}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_model_interpretation_xai/Human-interpretable%20Machine%20Learning%20-%20DS.ipynb#", "anchor_text": "Jupyter Notebook", "paragraph_index": 122}, {"url": "https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476", "anchor_text": "\u2018Part 1 \u2014 The Importance of Human Interpretable Machine Learning\u2019", "paragraph_index": 123}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2 \u2014 Model Interpretation Strategies\u2019", "paragraph_index": 124}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS", "paragraph_index": 125}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn", "paragraph_index": 125}], "all_paragraphs": ["Interpreting Machine Learning models is no longer a luxury but a necessity given the rapid adoption of AI in the industry. This article in a continuation in my series of articles aimed at \u2018Explainable Artificial Intelligence (XAI)\u2019. The idea here is to cut through the hype and enable you with the tools and techniques needed to start interpreting any black box machine learning model. Following are the previous articles in the series in case you want to give them a quick skim (but are not mandatory for this article).", "In this article we will give you hands-on guides which showcase various ways to explain potential black-box machine learning models in a model-agnostic way. We will be working on a real-world dataset on Census income, also known as the Adult dataset available in the UCI ML Repository where we will be predicting if the potential income of people is more than $50K/yr or not.", "The purpose of this article is manifold. The first main objective is to familiarize ourselves with the major state-of-the-art model interpretation frameworks out there (a lot of them being extensions of LIME \u2014 the original framework and approach proposed for model interpretation which we have covered in detail in Part 2 of this series).", "We cover usage of the following model interpretation frameworks in our tutorial.", "The major model interpretation techniques we will be covering in this tutorial include the following.", "Without further ado let\u2019s get started!", "We will be using a lot of frameworks and tools in this article given it is a hands-on guide to model interpretation. We recommend you to load up the following dependencies to get the maximum out of this guide!", "Rememeber to call the shap.initjs() function since a lot of the plots from shap require JavaScript.", "You can actually get the census income dataset (popularly known as the adult dataset) from the UCI ML repository. Fortunately shap provides us an already cleaned up version of this dataset which we will be using here since the intent of this article is model interpretation.", "Let\u2019s take a look at the major features or attributes of our dataset.", "We will explain these features shortly.", "Let\u2019s view the distribution of people with <= $50K (False) and > $50K (True) income which are our class labels which we want to predict.", "Definitely some class imbalance which is expected given that we should have less people having a higher income.", "Let\u2019s now take a look at our dataset attributes and understand their meaning and significance.", "We have a total of 12 features and our objective is to predict if the income of a person will be more than $50K (True) or less than $50K (False). Hence we will be building and interpreting a classification model.", "Here we convert the categorical columns with string values to numeric representations. Typically the XGBoost model can handle categorical data natively being a tree-based model so we don\u2019t one-hot encode the features here.", "Time to build our train and test datasets before we build our classification model.", "For any machine learning model, we always need train and test datasets. We will be building the model on the train dataset and test the performance on the test dataset. We maintain two datasets (one with the encoded categorical values and one with the original values) so we can train with the encoded dataset but use the original dataset as needed later on for model interpretation.", "We also maintain our base dataset with the actual (not encoded) values also in a separate dataframe (useful for model interpretation later).", "We will now train and build a basic boosting classification model on our training data using the popular XGBoost framework, an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.", "Here we do the usual, use the trained model to make predictions on the test dataset.", "Time to put the model to the test! Let\u2019s evaluate how our model has performed with its predictions on the test data. We use my nifty model_evaluation_utils module for this which leverages scikit-learn internally to give us standard classification model evaluation metrics.", "By default it is difficult to gauge on specific model interpretation methods for machine learning models out of the box. Parametric models like logistic regression are easier to interpret given that the total number of parameters of the model are fixed regardless of the volume of data and one can make some interpretation of the model\u2019s prediction decisions leveraging the parameter coefficients.", "Non-parametric models are harder to interpret given that the total number of parameters remain unbounded and increase with the increase in the data volume. Some non-parametric models like tree-based models do have some out of the box model interpretation methods like feature importance which helps us in understanding which features might be influential in the model making its prediction decisions.", "Here we try out the global feature importance calcuations that come with XGBoost. The model enables us to view feature importances based on the following.", "Note that they all contradict each other, which motivates the use of model interpretation frameworks like SHAP which uses something known as SHAP values, which claim to come with consistency guarantees (meaning they will typically order the features correctly).", "ELI5 is a Python package which helps to debug machine learning classifiers and explain their predictions in an easy to understand an intuitive way. It is perhaps the easiest of the three machine learning frameworks to get started with since it involves minimal reading of documentation! However it doesn\u2019t support true model-agnostic interpretations and support for models are mostly limited to tree-based and other parametric\\linear models. Let\u2019s look at some intuitive ways of model interpretation with ELI5 on our classification model.", "We recommend installing this framework using pip install eli5 since the conda version appears to be a bit out-dated. Also feel free to check out the documentation as needed.", "Typically for tree-based models ELI5 does nothing special but uses the out-of-the-box feature importance computation methods which we discussed in the previous section. By default, \u2018gain\u2019 is used, that is the average gain of the feature when it is used in trees.", "One of the best way to explain model prediction decisions to either a technical or a more business-oriented individual, is to examine individual data-point predictions. Typically, ELI5 does this by showing weights for each feature depicting how influential it might have been in contributing to the final prediction decision across all trees. The idea for weight calculation is described here; ELI5 provides an independent implementation of this algorithm for XGBoost and most scikit-learn tree ensembles which is definitely on the path towards model-agnostic interpretation but not purely model-agnostic like LIME.", "Typically, the prediction can be defined as the sum of the feature contributions + the \u201cbias\u201d (i.e. the mean given by the topmost region that covers the entire training set)", "Here we can see the most influential features being the Age, Hours per week, Marital Status, Occupation & Relationship", "Here we can see the most influential features being the Education, Relationship, Occupation, Hours per week & Marital Status", "It is definitely interesting to see how similar features play an influential role in explaining model prediction decisions for both classes!", "Skater is a unified framework to enable Model Interpretation for all forms of models to help one build an Interpretable machine learning system often needed for real world use-cases using a model-agnostic approach. It is an open source python library designed to demystify the learned structures of a black box model both globally(inference on the basis of a complete data set) and locally(inference about an individual prediction).", "Skater originally started off as a fork of LIME but then broke out as an independent framework of it\u2019s own with a wide variety of feature and capabilities for model-agnostic interpretation for any black-box models. The project was started as a research idea to find ways to enable better interpretability(preferably human interpretability) to predictive \u201cblack boxes\u201d both for researchers and practitioners.", "You can typically install Skater using a simple pip install skater. For detailed information on the dependencies and installation instruction check out installing skater.", "We recommend you to check out the detailed documentation of Skater.", "Skater has a suite of model interpretation techniques some of which are mentioned below.", "Since the project is under active development, the best way to understand usage would be to follow the examples mentioned in the Gallery of Interactive Notebook. But we will be showcasing its major capabilities using the model trained on our census dataset.", "A predictive model is a mapping from an input space to an output space. Interpretation algorithms are divided into those that offer statistics and metrics on regions of the domain, such as the marginal distribution of a feature, or the joint distribution of the entire training set. In an ideal world there would exist some representation that would allow a human to interpret a decision function in any number of dimensions. Given that we generally can only intuit visualizations of a few dimensions at time, global interpretation algorithms either aggregate or subset the feature space.", "Currently, model-agnostic global interpretation algorithms supported by skater include partial dependence and feature importance with a very new release of tree-surrogates also. We will be covering feature importance and partial dependence plots here.", "The general workflow within the skater package is to create an interpretation, create a model, and run interpretation algorithms. Typically, an Interpretation consumes a dataset, and optionally some metadata like feature names and row ids. Internally, the Interpretation will generate a DataManager to handle data requests and sampling.", "We will use the following workflow:", "Feature importance is generic term for the degree to which a predictive model relies on a particular feature. The skater feature importance implementation is based on an information theoretic criteria, measuring the entropy in the change of predictions, given a perturbation of a given feature. The intuition is that the more a model\u2019s decision criteria depend on a feature, the more we\u2019ll see predictions change as a function of perturbing a feature. The default method used is prediction-variance which is the mean absolute value of changes in predictions, given perturbations in the data.", "Partial Dependence describes the marginal impact of a feature on model prediction, holding other features in the model constant. The derivative of partial dependence describes the impact of a feature (analogous to a feature coefficient in a regression model). This has been adapted from T. Hastie, R. Tibshirani and J. Friedman, Elements of Statistical Learning Ed. 2, Springer. 2009.", "The partial dependence plot (PDP or PD plot) shows the marginal effect of a feature on the predicted outcome of a previously fit model. PDPs can show if the relationship between the target and a feature is linear, monotonic or more complex. Skater can show 1-D as well as 2-D PDPs", "Let\u2019s take a look at how the Age feature affects model predictions.", "Looks like the middle-aged people have a slightly higher chance of making more money as compared to younger or older people.", "Let\u2019s take a look at how the Education-Num feature affects model predictions.", "Looks like higher the education level, the better the chance of making more money. Not surprising!", "Let\u2019s take a look at how the Capital Gain feature affects model predictions.", "Unsurprisingly higher the capital gain, the more chance of making money, there is a steep rise in around $5K \u2014 $8K.", "Remember that relationship is coded as a categorical variable with numeric representations. Let\u2019s first look at how it is represented.", "Let\u2019s now take a look at how the Relationship feature affects model predictions.", "Interesting definitely that married folks (husband-wife) have a higher chance of making more money than others!", "We run a deeper model interpretation here over all the data samples, trying to see interactions between Age and Education-Numand also their effect on the probability of the model predicting if the person will make more money, with the help of a two-way partial dependence plot.", "Interesting to see higher the education level and the middle-aged folks (30\u201350) having the highest chance of making more money!", "We run a deeper model interpretation here over all the data samples, trying to see interactions between Education-Num and Capital Gain and also their effect on the probability of the model predicting if the person will make more money, with the help of a two-way partial dependence plot.", "Basically having a better education and more capital gain leads to you making more money!", "Local Interpretation could be possibly be achieved in two ways. Firstly, one could possibly approximate the behavior of a complex predictive model in the vicinity of a single input using a simple interpretable auxiliary or surrogate model (e.g. Linear Regressor). Secondly, one could use the base estimator to understand the behavior of a single prediction using intuitive approximate functions based on inputs and outputs.", "LIME is a novel algorithm designed by Riberio Marco, Singh Sameer, Guestrin Carlos to access the behavior of the any base estimator(model) using interpretable surrogate models (e.g. linear classifier/regressor). Such form of comprehensive evaluation helps in generating explanations which are locally faithful but may not align with the global behavior. Basically, LIME explanations are based on local surrogate models. These, surrogate models are interpretable models (like a linear model or decision tree) that are learned on the predictions of the original black box model. But instead of trying to fit a global surrogate model, LIME focuses on fitting local surrogate models to explain why single predictions were made.", "The idea is very intuitive. To start with, just try and unlearn what you have done so far! Forget about the training data, forget about how your model works! Think that your model is a black box model with some magic happening inside, where you can input data points and get the models predicted outcomes. You can probe this magic black box as often as you want with inputs and get output predictions.", "Now, you main objective is to understand why the machine learning model which you are treating as a magic black box, gave the outcome it produced. LIME tries to do this for you! It tests out what happens to you black box model\u2019s predictions when you feed variations or perturbations of your dataset into the black box model. Typically, LIME generates a new dataset consisting of perturbed samples and the associated black box model\u2019s predictions. On this dataset LIME then trains an interpretable model weighted by the proximity of the sampled instances to the instance of interest. Following is a standard high-level workflow for this.", "We recommend you to read the LIME chapter in Christoph Molnar\u2019s excellent book on Model Interpretation which talks about this in detail.", "Skater can leverage LIME to explain model predictions. Typically, its LimeTabularExplainer class helps in explaining predictions on tabular (i.e. matrix) data. For numerical features, it perturbs them by sampling from a Normal(0,1) and doing the inverse operation of mean-centering and scaling, according to the means and stds in the training data. For categorical features, it perturbs by sampling according to the training distribution, and making a binary feature that is 1 when the value is the same as the instance being explained. The explain_instance() function generates explanations for a prediction. First, we generate neighborhood data by randomly perturbing features from the instance. We then learn locally weighted linear (surrogate) models on this neighborhood data to explain each of the classes in an interpretable way.", "Since XGBoost has some issues with feature name ordering when building models with dataframes, we will build our same model with numpy arrays to make LIME work without additional hassles of feature re-ordering. Remember the model being built is the same ensemble model which we treat as our black box machine learning model.", "Skater gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person\u2019s income as below $50K.", "Skater gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person\u2019s income as above $50K.", "We have see various ways to interpret machine learning models with features, dependence plots and even LIME. But can we build an approximation or a surrogate model which is more interpretable from a really complex black box model like our XGBoost model having hundreds of decision trees?", "Here in, we introduce the novel idea of using TreeSurrogates as means for explaining a model's learned decision policies (for inductive learning tasks), which is inspired by the work of Mark W. Craven described as the TREPAN algorithm.", "We recommend checking out the following excellent papers on the TREPAN algorithm to build surrogate trees.", "Briefly, Trepan constructs a decision tree in a best-first manner. It maintains a queue of leaves which are expanded into subtrees as they are removed from the queue. With each node in the queue, Trepan stores,", "The stored subset of training examples consists simply of those examples that reach the node. The query instances are used, along with the training examples, to select the splitting test if the node is an internal node or to determine the class label if it is a leaf. The constraint set describes the conditions that instances must satisfy in order to reach the node; this information is used when drawing a set of query instances for a newly created node. The process of expanding a node in Trepan is much like it is in conventional decision tree algorithms: a splitting test is selected for the node, and a child is created for each outcome of the test. Each child is either made a leaf of the tree or put into the queue for future expansion.", "For Skater\u2019s implementation, for building explainable surrogate models, the base estimator(Oracle) could be any form of a supervised learning predictive model \u2014 our black box model. The explanations are approximated using Decision Trees(both for Classification/Regression) by learning decision boundaries similar to that learned by the Oracle (predictions from the base model are used for learning the Decision Tree representation). The implementation also generates a fidelity score to quantify tree based surrogate model\u2019s approximation to the Oracle. Ideally, the score should be 0 for truthful explanation both globally and locally. Let\u2019s check this out in action!", "NOTE: The implementation is currently experimental and might change in future.", "We can use the Interpretation object we instantiated earlier to invoke a call to the TreeSurrogate capability.", "We can now fit this surrogate model on our dataset to learn the decision boundaries of our base estimator.", "We do this since the feature names in the surrogate tree are not displayed (but are present in the model)", "We can now visualize our surrogate tree model using the following code.", "Here are some interesting rules you can observe from the above tree.", "Feel free to derive more interesting rules from this and also your own models! Let\u2019s look at how our surrogate model performs on the test dataset now.", "Let\u2019s check the performance of our surrogate model now on the test data.", "Just as expected, the model performance drops a fair bit but still we get an overall F1-score of 83% as compared to our boosted model\u2019s score of 87% which is quite good!", "SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on what they claim! (do check out the SHAP NIPS paper for details). We have also covered this in detail in Part 2 of this series.", "SHAP can be installed from PyPI", "The really awesome aspect about this framework is while SHAP values can explain the output of any machine learning model, for really complex ensemble models it can be slow. But they have developed a high-speed exact algorithm for tree ensemble methods (Tree SHAP arXiv paper). Fast C++ implementations are supported for XGBoost, LightGBM, CatBoost, and scikit-learn tree models!", "SHAP (SHapley Additive exPlanations) assigns each feature an importance value for a particular prediction. Its novel components include: the identification of a new class of additive feature importance measures, and theoretical results showing there is a unique solution in this class with a set of desirable properties. Typically, SHAP values try to explain the output of a model (function) as a sum of the effects of each feature being introduced into a conditional expectation. Importantly, for non-linear functions the order in which features are introduced matters. The SHAP values result from averaging over all possible orderings. Proofs from game theory show this is the only possible consistent approach.", "An intuitive way to understand the Shapley value is the following: The feature values enter a room in random order. All feature values in the room participate in the game (= contribute to the prediction). The Shapley value \u03d5\u1d62\u2c7c is the average marginal contribution of feature value x\u1d62\u2c7c by joining whatever features already entered the room before, i.e.", "The following figure from the KDD 18 paper, Consistent Individualized Feature Attribution for Tree Ensembles summarizes this in a nice way!", "Let\u2019s now dive into SHAP and leverage it for interpreting our model!", "Here we use the Tree SHAP implementation integrated into XGBoost to explain the test dataset! Remember that there are a variety of explainer methods based on the type of models you are building. We estimate the SHAP values for a set of samples (test data)", "This returns a matrix of SHAP values (# samples, # features). Each row sums to the difference between the model output for that sample and the expected value of the model output (which is stored as expected_value attribute of the explainer). Typically this difference helps us in explaining why the model is inclined on predicting a specific class outcome.", "SHAP gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person\u2019s income as below $50K. The below explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the actual model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue.", "Similarly, SHAP gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person\u2019s income as greater than $50K.", "One of the key advantages of SHAP is it can build beautiful interactive plots which can visualize and explain multiple predictions at once. Here we visualize model prediction decisions for the first 1000 test data samples.", "The above visualization can be interacted with in multiple ways. The default visualization shows some interesting model prediction pattern decisions.", "Definitely interesting how we can find out patterns which lead to the model making specific decisions and being able to provide explanations for them.", "This basically takes the average of the SHAP value magnitudes across the dataset and plots it as a simple bar chart.", "Besides a typical feature importance bar chart, SHAP also enables us to use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset. Features are sorted by the sum of the SHAP value magnitudes across all samples. Note that when the scatter points don\u2019t fit on a line they pile up to show density, and the color of each point represents the feature value of that individual.", "It is interesting to note that the age and marital status feature has more total model impact than the capital gain feature, but for those samples where capital gain matters it has more impact than age or marital status. In other words, capital gain affects a few predictions by a large amount, while age or marital status affects all predictions by a smaller amount.", "SHAP dependence plots show the effect of a single (or two) feature across the whole dataset. They plot a feature\u2019s value vs. the SHAP value of that feature across many samples. SHAP dependence plots are similar to partial dependence plots, but account for the interaction effects present in the features, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature can be chosen for coloring to highlight possible interactions.", "You will also notice its similarity with Skater\u2019s Partial Dependence Plots!", "Let\u2019s take a look at how the Age feature affects model predictions.", "Just like we observed before. the middle-aged people have a slightly higher shap value, pushing the model\u2019s prediction decisions to say that these individuals make more money as compared to younger or older people", "Let\u2019s take a look at how the Education-Num feature affects model predictions.", "Higher education levels have higher shap values, pushing the model\u2019s prediction decisions to say that these individuals make more money as compared to people with lower education levels.", "Let\u2019s take a look at how the Relationship feature affects model predictions.", "Just like we observed during the model prediction explanations, married people (husband or wife) have a slightly higher shap value, pushing the model\u2019s prediction decisions to say that these individuals make more money as compared to other folks!", "Let\u2019s take a look at how the Capital Gain feature affects model predictions.", "The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature is chosen for coloring to highlight possible interactions. Here we are trying to see interactions between Age and Capital Gainand also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot.", "Interesting to see higher the higher capital gain and the middle-aged folks (30\u201350) having the highest chance of making more money!", "Here we are trying to see interactions between Education-Num and Relationship and also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot.", "This is interesting because both the features are similar in some context, we can see typically married people with relationship status of either husband or wife having the highest chance of making more money!", "Here we are trying to see interactions between Marital Status and Relationship and also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot.", "Interesting to see higher the higher education level and the husband or wife (married) folks having the highest chance of making more money!", "Here we are trying to see interactions between Age and Hours per week and also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot.", "Nothing extra-ordinary here, middle-aged people working the most make the most money!", "If you are reading this, I would like to really commend your efforts on going through this huge and comprehensive tutorial on machine learning model interpretation. This article should help you leverage the state-of-the-art tools and techniques which should help you in your journey on the road towards Explanable AI (XAI). Based on the concepts and techniques we learnt in Part 2, in this article, we actually implemented them all on a complex machine learning ensemble model trained on a real-world dataset. I encourage you to try out some of these frameworks with your own models and datasets and explore the world of model interpretation!", "In Part 4 of this series, we will be looking at a comprehensive guide to building interpreting models on unstructured data like text and maybe even deep learning models!", "Stay tuned for some interesting content!", "Note: There are a lot of rapid developments in this area including a lot of new tools and frameworks being released over time. In case you want me to cover any other popular frameworks, feel free to reach out to me. I\u2019m definitely interested and will be starting by taking a look into H2O\u2019s model interpretation capabilities some time in the future.", "The code used in this article is available on my GitHub and also as an interactive Jupyter Notebook.", "Check out \u2018Part 1 \u2014 The Importance of Human Interpretable Machine Learning\u2019 which covers the what and why of human interpretable machine learning and the need and importance of model interpretation along with its scope and criteria in case you haven\u2019t!", "Also Part 2 \u2014 Model Interpretation Strategies\u2019 which covers the how of human interpretable machine learning where we look at essential concepts pertaining to major strategies for model interpretation.", "Have feedback for me? Or interested in working with me on research, data science, artificial intelligence or even publishing an article on TDS? You can reach out to me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe8ebe5afc608&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://djsarkar.medium.com/?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682----e8ebe5afc608---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8ebe5afc608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8ebe5afc608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476", "anchor_text": "Part 1 \u2014 The Importance of Human Interpretable Machine Learning\u2019"}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2 \u2014Model Interpretation Strategies\u2019"}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2 of this series"}, {"url": "https://github.com/TeamHG-Memex/eli5", "anchor_text": "ELI5"}, {"url": "https://github.com/datascienceinc/Skater", "anchor_text": "Skater"}, {"url": "https://github.com/slundberg/shap", "anchor_text": "SHAP"}, {"url": "https://archive.ics.uci.edu/ml/datasets/adult", "anchor_text": "UCI ML repository"}, {"url": "https://xgboost.readthedocs.io/en/latest/", "anchor_text": "XGBoost"}, {"url": "https://github.com/TeamHG-Memex/eli5", "anchor_text": "ELI5"}, {"url": "https://eli5.readthedocs.io/en/latest/overview.html", "anchor_text": "the documentation"}, {"url": "http://blog.datadive.net/interpreting-random-forests/", "anchor_text": "here"}, {"url": "https://oracle.github.io/Skater/install.html", "anchor_text": "installing skater"}, {"url": "https://oracle.github.io/Skater/gallery.html", "anchor_text": "Gallery of Interactive Notebook"}, {"url": "https://christophm.github.io/interpretable-ml-book/lime.html", "anchor_text": "LIME chapter"}, {"url": "http://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.thesis.pdf", "anchor_text": "Mark W. Craven(1996) EXTRACTING COMPREHENSIBLE MODELS FROM TRAINED NEURAL NETWORKS"}, {"url": "https://papers.nips.cc/paper/1152-extracting-tree-structured-representations-of-trained-networks.pdf", "anchor_text": "Mark W. Craven and Jude W. Shavlik(NIPS, 96). Extracting Thee-Structured Representations of Thained Networks"}, {"url": "http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions", "anchor_text": "SHAP NIPS paper"}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2"}, {"url": "https://pypi.org/project/shap", "anchor_text": "PyPI"}, {"url": "https://anaconda.org/conda-forge/shap", "anchor_text": "conda-forge"}, {"url": "https://arxiv.org/abs/1802.03888", "anchor_text": "Tree SHAP arXiv paper"}, {"url": "https://arxiv.org/pdf/1802.03888.pdf", "anchor_text": "Consistent Individualized Feature Attribution for Tree Ensembles"}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_model_interpretation_xai", "anchor_text": "my GitHub"}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_model_interpretation_xai/Human-interpretable%20Machine%20Learning%20-%20DS.ipynb#", "anchor_text": "Jupyter Notebook"}, {"url": "https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476", "anchor_text": "\u2018Part 1 \u2014 The Importance of Human Interpretable Machine Learning\u2019"}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739", "anchor_text": "Part 2 \u2014 Model Interpretation Strategies\u2019"}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn"}, {"url": "https://www.linkedin.com/in/dipanzan", "anchor_text": "Dipanjan Sarkar - AI Consultant &amp; Data Science Mentor - Springboard | LinkedInView Dipanjan Sarkar's profile on LinkedIn, the world's largest professional community. Dipanjan has 2 jobs listed on\u2026www.linkedin.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e8ebe5afc608---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e8ebe5afc608---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e8ebe5afc608---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----e8ebe5afc608---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e8ebe5afc608---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8ebe5afc608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----e8ebe5afc608---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8ebe5afc608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----e8ebe5afc608---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8ebe5afc608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe8ebe5afc608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e8ebe5afc608---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e8ebe5afc608--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://djsarkar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "10.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa34c887aa0f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608&newsletterV3=6278d12b0682&newsletterV3Id=a34c887aa0f4&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}