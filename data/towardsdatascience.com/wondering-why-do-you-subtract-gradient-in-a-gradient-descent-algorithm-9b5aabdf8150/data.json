{"url": "https://towardsdatascience.com/wondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150", "time": 1683012528.9614122, "path": "towardsdatascience.com/wondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150/", "webpage": {"metadata": {"title": "Wondering why do you subtract Gradient in a Gradient Descent Algorithm? | by Mayank Mishra | Towards Data Science", "h1": "Wondering why do you subtract Gradient in a Gradient Descent Algorithm?", "description": "While working with Machine Learning, one of the most basic approaches towards a supervised ML problem is defining a cost function, and then minimizing the cost function for the best output. Gradient\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["While working with Machine Learning, one of the most basic approaches towards a supervised ML problem is defining a cost function, and then minimizing the cost function for the best output. Gradient descent is one of the most widely used algorithms to do so. A basic mathematical representation of a gradient descent algorithm looks something like this.", "Here, \u03b8 represents the vector containing the parameters and the J is the cost function. Assignment operator suggests that after every iteration, the values in vector \u03b8 will update. The learning rate can be set manually which defines the size of the steps in the gradient descent algorithm. But why did we subtract the gradient descent to attain minimal loss? To understand that, let\u2019s understand a few basic concepts of vector calculus.", "A gradient of a function f, written as \u2207f, is a vector that contains all the partial derivatives of f. Let\u2019s look at it with an example.", "Consider a function f(x,y) = x\u00b3 sin(y). We first need to find out the partial derivatives of the function f. To find the partial derivative of a function with respect to a variable, treat all the other variable as constants.", "Now a gradient of f, denoted by \u2207f, would simply be a vector with the collection of all the partial derivative, i.e. -", "Therefore, to generalise, we can say that the gradient of a function J can be written as-", "The partial derivative of a multivariable function with respect to a particular variable simply tells us that, while keeping the other variables constant, if we change a particular variable very slightly, how will the output of the function change.", "For example, let\u2019s take a very simple function:", "f (x,y) = x\u00b2y. At any point, say (2,3), the partial derivative of the function with respect to x :", "Let\u2019s see the rate of change in the output when x was changed, \u2202f(x,y)/\u2202x .", "This value is equal to the value of partial derivative of the function with respect to x, thus implying the change in the output of a function when an input variable is slightly changed keeping the other variables constant. This same can be done for y too, keeping the value x constant.", "Let us now visualise the function f(x,y) = x\u00b2+y\u00b2 for better understanding. The graph of the function is given on the left. If the function was our loss function, we can see that the loss would be minimum near the origin and would increase as we go further away. Let us plot a vector field of gradients on the x-y plane. The gradient to the given function is [2x,2y].", "We see that the vectors in the vector field point away from the origin for this function. That is because when we take any point on the graph (a,b), project them on the x-y plane and then draw the vector (a,b) from the origin to that point, the gradient to that is vector [2a,2b] which points even further away from the origin. Also, a point to notice here is that the points that lie close to the origin have a gradient vector with smaller length than the one that are further away. Which is not surprising because the gradient vector is [2x,2y], so any point that is further away from the origin will have a bigger length than the points with small values of x,y.", "To visualise this better, given below are gradients at particular points on the graph.", "Imagine you are at any point on the graph and you want to climb up. You want to find a direction in which you should move from a random point to increase the altitude the fastest. That direction is given by the gradient vector at that particular point. Thus, the gradient gives us a direction of steepest ascent. In our gradient descent algorithm, we aim to minimize the value of loss function, therefore at any given point, we need to move in direction where the value of the function decreases the most, i.e. in the opposite direction of gradient which is direction of steepest descent. Therefore we subtract the gradient in the algorithm.", "But it is still not clear why gradient is the direction of steepest ascent. To understand that, we need to get an overview of directional derivatives.", "Consider a function with two variables x,y. We saw earlier that partial derivative of a function f(x,y) would tell us the change in the output if we would move slightly in the x-direction (keeping y constant) or in the y-direction (keeping x constant). But what if we move in a direction that is not parallel to either of the axes. What if we move in the direction of a vector say [2,3]. i.e. 2 units in the x-direction and 3 units in the y- direction. How will the output of the function change now? This information is given by the help of directional derivative.", "The directional derivative is mathematically represented by the dot product of gradient of the function with \ud835\udc64\u20d7 , which is the vector in the direction we are moving.", "Take the earlier function f(x,y) = x\u00b2+y\u00b2. You are standing at a point say (2,3). Now you move slightly in the direction of vector(3,1). What would be the change in the output?", "Now as we were standing at (2,3), the directional derivative at that point will be:", "This means that if at (2,3) we move slightly in the direction of vector(3,1), the output would change by a factor of 18. The reason why we dot product is also obvious. When we were moving in only x-direction the change in the output is \u2202f/\u2202x and when we were moving only in y-direction the change in the output was \u2202f/\u2202y. And now when we are moving in the direction of vector \ud835\udc64 \u20d7 say [a,b], we are going a units in x-direction and b units in y-direction. Thus, the change in the output can be denoted by a(\u2202f/\u2202x) + b(\u2202f/\u2202y).", "Now at any given point, we can move in a lot of directions. Assume we wish to move in a direction \ud835\udc64 \u20d7 where the directional derivative is the maximum, i.e. the output of the function increases the most. This direction is also the direction of steepest ascent.", "Thus we can say we are looking for:", "A point to note here is that the vectors in the equation are of length 1, i.e. unit vectors. This is to eradicate the possibility that we might choose any other vector with a large magnitude pointing in some completely different direction, just because it gave the highest value of the dot product.", "We know that given any two vectors, the dot product can be written as:", "Here \u03b8 is the angle between the two vectors. For the dot product to be maximum, the angle between the two vectors should be 0.", "In our case, the magnitude of the vectors is 1. Therefore, only the angle between the vectors matters. We can see that we will have the highest value of the directional derivative when our vector \ud835\udc64 \u20d7(the direction in which we are moving) is in the direction of the gradient of the function.", "Thus we can say that the gradient at any point of a function points in the direction of steepest ascent, i.e. if I were to climb the function as quickly as possible, I would choose the direction of the gradient.", "While in gradient descent algorithm, we wish to choose to move in the direction where the function (i.e. the loss function) reduces the maximum, thus we move in the opposite direction of the gradient, i.e. the direction of steepest descent. Therefore, we subtract the gradient in the gradient descent algorithm.", "If you wish to read more on core topics of deep learning, feel free to follow and stay updated.", "All the graphical representations and images in the story are by the author.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9b5aabdf8150&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mayank-mishra.medium.com/?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": ""}, {"url": "https://mayank-mishra.medium.com/?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "Mayank Mishra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3f3faff922cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&user=Mayank+Mishra&userId=3f3faff922cd&source=post_page-3f3faff922cd----9b5aabdf8150---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b5aabdf8150&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b5aabdf8150&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jeswinthomas?utm_source=medium&utm_medium=referral", "anchor_text": "Jeswin Thomas"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@mmayank74567/a-simple-guide-to-deep-learning-topics-1-neuron-macculloch-pits-neuron-perceptron-bef18eded324", "anchor_text": "A Simple Guide to Deep Learning Topics \u2014 1 ( Neuron, MacCulloch Pits Neuron, Perceptron)A beginner\u2019s go to place to get an overview on core Deep Learning topics.medium.com"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----9b5aabdf8150---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9b5aabdf8150---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9b5aabdf8150---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/vector?source=post_page-----9b5aabdf8150---------------vector-----------------", "anchor_text": "Vector"}, {"url": "https://medium.com/tag/calculus?source=post_page-----9b5aabdf8150---------------calculus-----------------", "anchor_text": "Calculus"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b5aabdf8150&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&user=Mayank+Mishra&userId=3f3faff922cd&source=-----9b5aabdf8150---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b5aabdf8150&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&user=Mayank+Mishra&userId=3f3faff922cd&source=-----9b5aabdf8150---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b5aabdf8150&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9b5aabdf8150&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9b5aabdf8150---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9b5aabdf8150--------------------------------", "anchor_text": ""}, {"url": "https://mayank-mishra.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mayank-mishra.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mayank Mishra"}, {"url": "https://mayank-mishra.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "80 Followers"}, {"url": "https://mmayank74567.github.io/", "anchor_text": "https://mmayank74567.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3f3faff922cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&user=Mayank+Mishra&userId=3f3faff922cd&source=post_page-3f3faff922cd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd45ffb5266e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150&newsletterV3=3f3faff922cd&newsletterV3Id=d45ffb5266e6&user=Mayank+Mishra&userId=3f3faff922cd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}