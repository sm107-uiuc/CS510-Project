{"url": "https://towardsdatascience.com/gpt-3-just-another-language-model-but-bigger-1add6e9277fa", "time": 1683013247.709447, "path": "towardsdatascience.com/gpt-3-just-another-language-model-but-bigger-1add6e9277fa/", "webpage": {"metadata": {"title": "GPT-3: Just Another Language Model But Bigger | by Sanket Doshi | Towards Data Science", "h1": "GPT-3: Just Another Language Model But Bigger", "description": "GPT-3 has takeover the NLP world in a very short period of time. It has proved the theory that increasing number of parameters will increase the accuracy of model. Language model tries to predict the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Language_model", "anchor_text": "Language model", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/1606.06031.pdf", "anchor_text": "dataset", "paragraph_index": 10}, {"url": "http://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15791880.pdf", "anchor_text": "fine-tuned", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU", "paragraph_index": 24}, {"url": "https://www.aclweb.org/anthology/P02-1040.pdf", "anchor_text": "BLEU", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/1811.01778.pdf", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://arxiv.org/pdf/2005.00700.pdf", "anchor_text": "UnifiedQA", "paragraph_index": 39}, {"url": "https://adversarialnli.com/#", "anchor_text": "ANLI", "paragraph_index": 54}, {"url": "https://arxiv.org/pdf/1910.14599.pdf", "anchor_text": "ANLI", "paragraph_index": 55}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "here", "paragraph_index": 85}, {"url": "https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/", "anchor_text": "MIT technology review", "paragraph_index": 85}], "all_paragraphs": ["GPT-3 has takeover the NLP world in a very short period of time. It has proved the theory that increasing number of parameters will increase the accuracy of model.", "Language model tries to predict the next word given the m words. It assigns the probability to the next word which makes the sentence more probable.", "It\u2019s the successor of GPT-2 language model which has highest number of parameters to trained. GPT-3 model is created by OpenAI and it proves that the language model size is proportional to the accuracy and hence, more we increase the model size more the accuracy.", "As we can see from the graph that increasing number of language models proportionally increases the accuracy. Therefore, it is possible to obtain higher accuracy by adding more parameters to the model.", "GPT-3 is the largest language model present with 175 billion parameters 10 times bigger than the Turing-NLG model which has 17 billion parameters.", "In this blog, we\u2019ll go through the research paper of GPT-3 and will deduce why it\u2019s just the another language model and why it cannot be called as the model that can imitate human at any level. It\u2019s the best language model present but not the model that can understand and can learn the human language.", "The accuracy of the model was compared to the state-of-the-art task specific model also known as fine-tuning models. The disadvantage of this model is just that they need huge task-specific dataset to be trained on. This type of models are task specific and will perform poorly on other tasks.", "The dataset used by GPT-3 was huge almost containing everything present on the internet. The did the fuzzy search on the training dataset to remove anything related to the test and validation dataset which will help to give more accurate results. They created this dataset from merging various dataset and through crawling.", "They used gradient noise scale to find out the right batch size to be used. The architecture was similar to the GPT-2 with minor modifications.", "The GPT-3 was evaluated on various natural language tasks and on various datasets.", "In this dataset the context is given than sentence is given to the model with the blank which needs to be completed by the model. The word chosen should be based on the context given rather than the past knowledge model has. This dataset proves that model can understand the context and provides the answers based on that.", "As we can see that all settings of GPT-3 surpasses the accuracy of SOTA on this dataset. GPT-3 one-shot accuracy is less than the accuracy of zero-shot. We can imply that GPT-3 predicts more accurately with any example than when given one example. It is possible that GPT-3 had the knowledge of the dataset through training set and in few-shot method the model can know which weights to use more accurately. Because when you do google search for Yes, of course. Why do you even ask?\"This baby wasn't planned for.\u201d you can get various links with answer to the statement.", "These are multiple choice questions in which model needs to select the most suitable option. This dataset is used to test the common sense of the model. The GPT-3 FS achieved the accuracy of 79.3 around 6 percent below the SOTA which is fine-tuned multi-task model ALUM.", "These dataset is also used for common sense test. In this the model is given the context and the model has to predict the right ending from the two options provided. GPT-3 FS has achieved the accuracy of 87.7% well below the SOTA but has shown a great improvement in GPT-3 ZS by 10%.", "Open Book Question Answering is used in search engines which can search the relevant information and can extract the answer from it. In closed book the model is not allowed to search and doesn\u2019t have any context or excerpt provided to know what model is actually asking.", "This dataset is a fact based that is the answer are the facts rather than any context depended. So, we can say that this dataset tests the memory of the model as GPT-3 is trained on wikipedia text too.", "The model is asked question and is expected the answer based on facts. Excerpt is only for our knowledge it is not feed to the model. Multiple answers are also allowed for example, if the answers is name Bill Gates the answer can be Bill, Gates, Bill Gates, Microsoft founder Bill Gates.", "We can see that GPT-3 has surpassed the fine-tuned open book SOTA. So we can say that GPT-3 is more efficient as an information retrieval engine for facts.", "This dataset is similar to TriviaQA but the dataset consist of the real-world questions asked by the actual users in real world. So, it tests the model in real world scenario.", "The GPT-3 FS has similar accuracy to the fine-tuned SOTA models.", "This dataset is provided by the Google for testing the Question Answering model in real world. All the questions are searched by users on the google search. In the model tries to predict the paragraph from the wikipedia page which may contain the answer to the question. And model also predicts the short answer which will the exact answer to the question. But the short answer must be present in the paragraph selected also known as long answer.", "The accuracy fo GPT-3 FS setting is 29.9% well below the fine-tuned SOTA. This dataset tests the limit of model to retain the information of wikipedia.", "Through this tests we can say that the bigger the model more the information can be retained and hence, higher the accuracy.", "The accuracy of GPT-3 in language translation is a surprise. As 93% words of the training set are English. So, the model will be biased toward English language more than the other language. GPT-3 is the upgrade over GPT-2 and is tested for machine translation for languages French, German, and Romanian other than English.", "The model efficiency for the Machine Translation task is measured in BLEU rather than as accuracy. We\u2019ll look into it some other time.", "BLEU is calculated on 6 language pairs:", "The training dataset had around 92.5% of words in English, 1.8% of words in French, 1.5% in German and 0.16% of words in Romanian.", "We can see that the GPT-3 model is almost or better than the SOTA when translating any text to English.", "As we can see, only the description text is provided to the model along with the sentence and the targeted language.", "For Few-Shot around 64 examples were given to the model before asking the question.", "As we can see as the data increased the BLEU score of the model increased. The score of English->French is greater than English->German which is greater than English->Romanian and is proportional to the percentage of words present of each language in the dataset.", "This type of tasks involves determining what the pronoun used in the sentence resemble. The pronoun is ambiguous for the computer but unambiguous for humans. The partial evaluation method was used for evaluation about which you can read here.", "The answer for the sentence must be answer1 but once we switch the sentence the answer should also change to answer0", "GPT-3 performance on this dataset is below SOTA. Some examples of the test set was also found in the training dataset.", "This dataset is similar to winograde dataset but more advanced and difficult.", "This test is conducted to test how the model applies the common sense in real world questions. For example, if we ask the human what\u2019s the time? Human will respond the current time such as 5:30 PM rather than explain the concept of time. We check whether the model can answer such questions using common sense. Previous all tasks where mostly dependent on the data and probability rather than how much the model actually understands the inner concepts. Here, we test how much the model understand the human language.", "For all types of tuning GPT-3 beats the SOTA on this dataset with a small margin. But the confidence on this result was not much due to possibly data contamination.", "Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk.", "This dataset consist of the multiple-choice science questions of 3rd to 9th grade. The dataset is divided into parts: 1. Easy version 2. Challenge version. The challenge set consist of the questions which were wrongly answered by the retrieval-based algorithms.", "GPT-3 performance on this dataset was very much below the SOTA achieved by UnifiedQA.", "This dataset are similar to open book tests and the answer to the question is combination of fact and common sense.", "As GPT-3 is trained on all the text present on internet it\u2019s a good question to answer that whether GPT-3 has applied common sense to answer this questions and not just retrieved the information available somewhere in the training dataset.", "Here, we test the ability of the model on understanding the passage and answering the questions based on the information provided in the passage.", "In this dataset the passage is provided and the questions asked are based on the conversation among humans.", "This dataset is very interesting as in this the student does not see the passage but asks questions which are answered by the teacher. The student asks the basic question and teacher answer with some extra context based on which the student asks another question. The interaction ends if the teacher is not able to answer too many question or if student does not ask any relevant question.", "In this dataset a model must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them such as addition, counting, or sorting.", "As we can see here the model needs to understand the passage and question in order to answer it correctly and it should also perform some operations on it rather than just retrieve the information.", "In this along with the questions for which the answer is present in the passage the questions for which there is no answer present in the passage are also asked. The model is expected not to answer the question for which the passage does not contain the answer.", "This dataset consists the passages from the English exams for middle and high school Chinese students in the age range between 12 to 18 designed to evaluate the students reasoning capability. The dataset is divided into two sections: 1. RACE-m dataset consist of the passages of middle school 2. RACE-h dataset consists the passages of high school.", "It can be considered as the standardized test for all the NLP model which are able to perform multiple language modeling tasks. It gives the single result based on how model performed in different tasks. As the result is ingle value it\u2019s easy to compare with different models on same scale.", "This benchmark includes the following tasks:", "For GPT-3 few-shot tuning 32 examples were used in context.", "As we can see more the examples the accuracy of GPT-3 increased but in very small number. GPT-3 FS surpassed the fine-tuned BERT-Large when 8 examples were given in context.", "In this type of task the model determines the relation between two sentences. It\u2019s a classification type of problem where model classifies the second sentence into three classes: 1. Supports the first sentence 2. Contradicts the first sentence 3. Neutral", "RTE dataset is used for testing the model on NLI task. Along with RTE more difficult Adversarial NLI (ANLI) was used for testing the GPT-3 model. IN ANLI GPT-3 performed slightly better than random chance but well below the SOTA.", "ANLI is divided into three sets based in difficulty: round 1, round 2, round 3.", "In this the GPT-3 is tested to perform various synthetic tasks such as arithmetic operations, rearranging and unscrambling the letters in a word, solve SAT-style analogy problems, using new words in a sentence, correcting English grammar, and news article generation.", "In this the ability of GPT-3 to perform arithmetic operations is tested. Various arithmetic operations such as addition, subtraction and multiplication are tested.", "The arithmetic problems consists of the following types:", "The model performs better in few-shot setting than other type of settings. And, as the number of digits increases the accuracy reduces greatly. GPT-3 has made a huge jump on this set than the smaller models.", "As the digit gets increased the accuracy of the model decreases exponentially. And more the complex operations such as multiplication and composite equations the model performance is degraded.", "The model may have memorized the smaller digit arithmetic operations as they are available more frequently on the internet than the big numbers. The training set was searched for \u201c<NUM1> + <NUM2> = \u201d and \u201c<NUM1> -<NUM2> = \u201d and only 0.8% and 0.1% matched were found for the test cases. But, it is possible that the training set may have \u201cforty-two + twenty-three equals sixty-five\u201d or \u201cadding forty-two to twenty-three gives sixty-five\u201d type of equations and the model may have mapped the integer and operations to this strings.", "The model was inspected on wrong results and was found that it works incorrectly when the operation includes carry.", "In this various operations are done on the word and is given as an input to the model. The model needs to predict the original word.", "This tasks include 5 type of tasks:", "2. Anagrams of all but first and last characters (A1) \u2014 All the letters of the word are scrambled except the first and last letter. The model needs to output the original word.", "3. Anagrams of all but first and last 2 characters (A2) \u2014 All the letters of the word are scrambled except the first 2 and last 2 letters. The model need to output the original word.", "4. Random insertions in word (R1) \u2014 A random punctuation or space is inserted between letters and model needs to predict the original word.", "5. Reversed words (RW) \u2014 The word is spelled backwards and the model needs to predict the original word.", "The model performs poorly in zero-shot setting and shows average performance in one-shot setting. The model was given 100 examples for few-shot setting in which it performs considerably well.", "In this the model needs to answer the questions based on analogy. This questions were asked in SAT the college entrance exam. The random guessing accuracy is 20% while the average score of college students is 57%. GPT-3 few-shot setting achieves 65.2% of accuracy and one-shot setting achieves 59.1% accuracy well above the student average. The accuracy of zero-shot setting is 53.7%. For few-shot setting number of examples given were 20.", "In this the model is provided the title and subtitle using which it needs to write an article of around 200 words. The model was tested on few-shot setting only as model was not able to understand whether it needs to write an article or need to respond as follow up tweet.", "The GPT-3 performance is compared with the control model which deliberately produces bad articles. Humans were used to distinguish the article whether it was human made or made by model. Humans accurately detected the article was generated by control model with 86% accuracy. But as the model size increased the human were able to detect with 52% accuracy that the article was generated by the model.", "Most articles which were detected as made by the model contained wrong factual information or had a repeated sentences and unusual phrasing.", "The model was also tested on articles with 500 words but still the human were able to distinguish with human articles by 52% only.", "When you google any sentence from the above article you\u2019ll get various articles on the some topic with similar sentences. It\u2019s possible that GPT-3 as trained on whole internet may just have summarised all the articles of the relevant topic in less than 500 words. Instead of generating articles it maybe just merging various sentences from different articles hence, making articles similar to humans. We will know whether the GPT-3 can generate the article if we provide the title very unique.", "In this the model is tested if it can use the word in a sentence after defining the word once.", "So to test the model on this task the word definition is given once so one-shot setting but the example is provided for the each word defined hence, few-shot setting with respect to task description.", "The words were human made and hence, model has no prior knowledge of that. We can see the model generated all the sentences logical using the defined word.", "The model is tested if it ca predict the wrong grammar and can modify the sentence with right grammar. The model is tested on few-shot setting.", "As we can see, the model is tested on very basic grammar instead of complicated grammar such as clauses. It\u2019ll be good to see the model performance on such bad grammar as it\u2019s a complicated task even for humans.", "As the training dataset was generated through crawling the internet it is possible that the training dataset may contain various test set examples. If it happens then the confidence on the accuracy reported by the model on various tasks may be unreliable.", "The training dataset was searched for the test set example and the documents are considered match if N words are similar among both the sets. N is equal to the 5th percentile example length in words of each sets. Smaller value of N caused huge number of matches which were found illogical. So minimum value of N was kept 8 and maximum 13. N words are matched or the whole sentence is matched depending on the minimum length. If the match is found the example is considered as dirty otherwise clean example.", "GPT-3 was tested for various tasks on the clean dataset and the accuracy is compared with the original score. If the accuracy almost matches then it means that contamination even if present does not effect the model accuracy. If the accuracy is lower than the original score implies that contamination is inflating the results.", "For dataset QuAC 0% of clean data is present in the dataset and still the accuracy changed positively by 20% which must mean there was something wrong with it. They found around 90% of dataset contaminated. But, after further analysis deduced that the training set included only passages but not question and answers.", "GPT-3 has various limitations and there is still chance of improvements in fields such as text synthesis. GPT-3 is based on the training dataset and hence, can\u2019t think of it\u2019s own. If majority of humans has some bias towards gender, race, religion then the model will also represent those bias. For more limitations and bias you can read here. MIT technology review also published a blog on limitations of GPT-3.", "GPT-3 is model which just predicts which word should come next. It does not understand the context and does not understand the actual meanings of the word. The model lacks logical reasoning and common sense reasoning. The model output can be changed just by tweaking the dataset.", "Here are great resources from where you learn more about GPT-3:", "Thanks for reading and have a great day.", "*text in italics are just my personal views", "Currently working as a Backend Developer. Exploring how to make machines smarter than me."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1add6e9277fa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@sdoshi579?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Sanket Doshi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a8e3136747c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&user=Sanket+Doshi&userId=4a8e3136747c&source=post_page-4a8e3136747c----1add6e9277fa---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1add6e9277fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&user=Sanket+Doshi&userId=4a8e3136747c&source=-----1add6e9277fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1add6e9277fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&source=-----1add6e9277fa---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://twitter.com/sama?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor", "anchor_text": "Sam Altman"}, {"url": "https://www.befunky.com/create/designer/", "anchor_text": "befunky"}, {"url": "https://en.wikipedia.org/wiki/Language_model", "anchor_text": "Language model"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1606.06031.pdf", "anchor_text": "Lambada"}, {"url": "https://arxiv.org/pdf/1606.06031.pdf", "anchor_text": "dataset"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1905.07830.pdf", "anchor_text": "HellaSwag"}, {"url": "https://www.cs.rochester.edu/~nasrinm/files/Papers/lsdsem17-shared-task.pdf", "anchor_text": "Story Cloze"}, {"url": "https://homes.cs.washington.edu/~eunsol/papers/acl17jcwz.pdf", "anchor_text": "TriviaQA"}, {"url": "https://homes.cs.washington.edu/~eunsol/papers/acl17jcwz.pdf", "anchor_text": "TriviaQA research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1607.06275.pdf", "anchor_text": "WebQuestions"}, {"url": "https://arxiv.org/pdf/1607.06275.pdf", "anchor_text": "WebQA research paper"}, {"url": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf", "anchor_text": "Natural Questions"}, {"url": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/1f7b46b5378d757553d3e92ead36bda2e4254244.pdf", "anchor_text": "NQs research paper"}, {"url": "http://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15791880.pdf", "anchor_text": "fine-tuned"}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU"}, {"url": "https://www.aclweb.org/anthology/P02-1040.pdf", "anchor_text": "BLEU"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1811.01778.pdf", "anchor_text": "here"}, {"url": "https://aaai.org/ocs/index.php/KR/KR12/paper/view/4492", "anchor_text": "Winograd"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1907.10641.pdf", "anchor_text": "Winogrande"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1911.11641.pdf", "anchor_text": "PhysicalQA"}, {"url": "https://arxiv.org/pdf/1911.11641.pdf", "anchor_text": "PhysicalQA research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1803.05457.pdf", "anchor_text": "AI2 Reasoning Challenge Dataset (ARC)"}, {"url": "https://arxiv.org/pdf/2005.00700.pdf", "anchor_text": "UnifiedQA"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/1809.02789.pdf", "anchor_text": "OpenBookQA"}, {"url": "https://arxiv.org/pdf/1809.02789.pdf", "anchor_text": "OpenBookQA research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://stanfordnlp.github.io/coqa/", "anchor_text": "CoQA Dataset"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://quac.ai/", "anchor_text": "QuAC Dataset"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://allennlp.org/drop", "anchor_text": "DROP Dataset"}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD 2.0 Dataset"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://www.aclweb.org/anthology/D17-1082/", "anchor_text": "RACE Dataset"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://super.gluebenchmark.com/", "anchor_text": "SuperGLUE"}, {"url": "https://demo.allennlp.org/coreference-resolution", "anchor_text": "Coreference Resolution (coref.)"}, {"url": "http://www.scholarpedia.org/article/Word_sense_disambiguation", "anchor_text": "Word Sense Disambiguation (WSD)"}, {"url": "https://arxiv.org/abs/1905.10044", "anchor_text": "BoolQ"}, {"url": "https://github.com/mcdm/CommitmentBank", "anchor_text": "Commitment Bank"}, {"url": "https://people.ict.usc.edu/gordon/public_html/publications/AAAI-SPRING11A.PDF", "anchor_text": "COPA"}, {"url": "https://github.com/CogComp/multirc", "anchor_text": "MultiRC"}, {"url": "https://sheng-z.github.io/ReCoRD-explorer/", "anchor_text": "ReCoRD"}, {"url": "https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool", "anchor_text": "RTE"}, {"url": "https://pilehvar.github.io/wic/", "anchor_text": "WiC"}, {"url": "https://aaai.org/ocs/index.php/KR/KR12/paper/view/4492", "anchor_text": "WSC"}, {"url": "https://super.gluebenchmark.com/", "anchor_text": "SuperGLUE research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://adversarialnli.com/#", "anchor_text": "ANLI"}, {"url": "https://arxiv.org/pdf/1910.14599.pdf", "anchor_text": "ANLI"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/cs/0309035.pdf", "anchor_text": "SAT Analogies"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT-3 research paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "here"}, {"url": "https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/", "anchor_text": "MIT technology review"}, {"url": "https://twitter.com/an_open_mind", "anchor_text": "Jerome Pesenti"}, {"url": "https://towardsdatascience.com/you-can-understand-gpt-3-with-these-youtube-videos-6a30887c928b", "anchor_text": "You Can Understand GPT-3 With These YouTube VideosGet a surface understanding on GPT-3 in less than 3 minutes through these YouTube videotowardsdatascience.com"}, {"url": "https://analyticsindiamag.com/top-free-resources-to-learn-gpt-3/", "anchor_text": "Top Free Resources To Learn GPT-3 - Analytics India MagazineWith Open AI releasing its avant-garde pre-trained language model - GPT-3 has suddenly become an obsession for the\u2026analyticsindiamag.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----1add6e9277fa---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/language-model?source=post_page-----1add6e9277fa---------------language_model-----------------", "anchor_text": "Language Model"}, {"url": "https://medium.com/tag/gpt-3?source=post_page-----1add6e9277fa---------------gpt_3-----------------", "anchor_text": "Gpt 3"}, {"url": "https://medium.com/tag/openai?source=post_page-----1add6e9277fa---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----1add6e9277fa---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1add6e9277fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&user=Sanket+Doshi&userId=4a8e3136747c&source=-----1add6e9277fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1add6e9277fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&user=Sanket+Doshi&userId=4a8e3136747c&source=-----1add6e9277fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1add6e9277fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a8e3136747c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&user=Sanket+Doshi&userId=4a8e3136747c&source=post_page-4a8e3136747c----1add6e9277fa---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F157262d7e358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&newsletterV3=4a8e3136747c&newsletterV3Id=157262d7e358&user=Sanket+Doshi&userId=4a8e3136747c&source=-----1add6e9277fa---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Written by Sanket Doshi"}, {"url": "https://medium.com/@sdoshi579/followers?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "578 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a8e3136747c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&user=Sanket+Doshi&userId=4a8e3136747c&source=post_page-4a8e3136747c----1add6e9277fa---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F157262d7e358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-3-just-another-language-model-but-bigger-1add6e9277fa&newsletterV3=4a8e3136747c&newsletterV3Id=157262d7e358&user=Sanket+Doshi&userId=4a8e3136747c&source=-----1add6e9277fa---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6?source=author_recirc-----1add6e9277fa----0---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=author_recirc-----1add6e9277fa----0---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=author_recirc-----1add6e9277fa----0---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Sanket Doshi"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1add6e9277fa----0---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6?source=author_recirc-----1add6e9277fa----0---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Various Optimization Algorithms For Training Neural NetworkThe right optimization algorithm can reduce training time exponentially."}, {"url": "https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6?source=author_recirc-----1add6e9277fa----0---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "7 min read\u00b7Jan 13, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&user=Sanket+Doshi&userId=4a8e3136747c&source=-----59450d71caf6----0-----------------clap_footer----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6?source=author_recirc-----1add6e9277fa----0---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59450d71caf6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizers-for-training-neural-network-59450d71caf6&source=-----1add6e9277fa----0-----------------bookmark_preview----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1add6e9277fa----1---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1add6e9277fa----1---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1add6e9277fa----1---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1add6e9277fa----1---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1add6e9277fa----1---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1add6e9277fa----1---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1add6e9277fa----1---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----1add6e9277fa----1-----------------bookmark_preview----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1add6e9277fa----2---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----1add6e9277fa----2---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----1add6e9277fa----2---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1add6e9277fa----2---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1add6e9277fa----2---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1add6e9277fa----2---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1add6e9277fa----2---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----1add6e9277fa----2-----------------bookmark_preview----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c?source=author_recirc-----1add6e9277fa----3---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=author_recirc-----1add6e9277fa----3---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=author_recirc-----1add6e9277fa----3---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Sanket Doshi"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1add6e9277fa----3---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c?source=author_recirc-----1add6e9277fa----3---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "Skip-Gram: NLP context words prediction algorithmSkip-gram is used when we need to predict the context words given the target word."}, {"url": "https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c?source=author_recirc-----1add6e9277fa----3---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": "5 min read\u00b7Mar 16, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5bbf34f84e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fskip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c&user=Sanket+Doshi&userId=4a8e3136747c&source=-----5bbf34f84e0c----3-----------------clap_footer----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c?source=author_recirc-----1add6e9277fa----3---------------------1b8dc020_ff7c_43a8_80cb_fcac4a884970-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5bbf34f84e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fskip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c&source=-----1add6e9277fa----3-----------------bookmark_preview----1b8dc020_ff7c_43a8_80cb_fcac4a884970-------", "anchor_text": ""}, {"url": "https://medium.com/@sdoshi579?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "See all from Sanket Doshi"}, {"url": "https://towardsdatascience.com/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----0-----------------clap_footer----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----1add6e9277fa----0-----------------bookmark_preview----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Arslan Mirza"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "How To Build Your Own Custom ChatGPT BotA step-by-step guide to building and fine-tuning custom ChatGPT models"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "\u00b79 min read\u00b7Mar 29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&user=Arslan+Mirza&userId=35aaa5742af7&source=-----cf4af959adcc----1-----------------clap_footer----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&source=-----1add6e9277fa----1-----------------bookmark_preview----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----0-----------------clap_footer----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----1add6e9277fa----0---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----1add6e9277fa----0-----------------bookmark_preview----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----1-----------------clap_footer----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----1add6e9277fa----1---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----1add6e9277fa----1-----------------bookmark_preview----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1add6e9277fa----2---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----1add6e9277fa----2---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----1add6e9277fa----2---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Jay Peterman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1add6e9277fa----2---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1add6e9277fa----2---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Make a Text Summarizer with GPT-3Quick tutorial using Python, OpenAI\u2019s GPT-3, and Streamlit"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1add6e9277fa----2---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "\u00b711 min read\u00b7Jan 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&user=Jay+Peterman&userId=9731dc608e6c&source=-----f0917a07189e----2-----------------clap_footer----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----1add6e9277fa----2---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&source=-----1add6e9277fa----2-----------------bookmark_preview----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----1add6e9277fa----3---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----1add6e9277fa----3---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----1add6e9277fa----3---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1add6e9277fa----3---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----1add6e9277fa----3---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----1add6e9277fa----3---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----3-----------------clap_footer----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----1add6e9277fa----3---------------------01aed99f_8ac8_41f1_a921_c42117a953e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----1add6e9277fa----3-----------------bookmark_preview----01aed99f_8ac8_41f1_a921_c42117a953e3-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----1add6e9277fa--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}