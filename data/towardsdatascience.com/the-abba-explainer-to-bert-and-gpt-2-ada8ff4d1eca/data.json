{"url": "https://towardsdatascience.com/the-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca", "time": 1683013123.917544, "path": "towardsdatascience.com/the-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca/", "webpage": {"metadata": {"title": "The ABBA explainer to BERT and GPT-2 | by Vered Zimmerman | Towards Data Science", "h1": "The ABBA explainer to BERT and GPT-2", "description": "I read articles; followed diagrams; squinted at equations; watched recorded classes; read code documentation; and still struggled to make sense of it all. This article bridges the gap, explaining in\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["For the life of me, I couldn\u2019t understand how BERT or GPT-2 worked.", "I read articles; followed diagrams; squinted at equations; watched recorded classes; read code documentation; and still struggled to make sense of it all.", "It wasn\u2019t the math that made it hard.", "More like, that the big part you\u2019d expect to precede the nitty-gritty was somehow missing.", "This article bridges the gap, explaining in simple terms how these models are built. It\u2019s the article I wish I could have read first; many of the details would have then slotted right into place. With the generous help of ABBA, we\u2019ll introduce three different ideas. I guarantee there won\u2019t be a single mathematical equation in any of them:  1. Thinking about words and meaning \u2014 Attention 2. Pancake stacking small components \u2014 Deep Learning 3. Thinking about words and signals \u2014 Embeddings  In the fourth and final section, we\u2019ll see how these ideas tie neatly into a bow.", "Let\u2019s look at the following sentence:", "Suppose we asked three friends to read this sentence. While they\u2019d probably agree the sentence\u2019s topic is the rain, they might diverge on which words are most important to \u2018rain\u2019 to convey this sentence\u2019s true meaning.", "Benny might say \u2018tapped\u2019 and \u2018window\u2019 were the most important, because the sentence is about the noise the rain makes.", "Frida might say \u2018softly\u2019 and \u2018summer\u2019 are the most important, because this is a sentence about what summer rain is like.", "Bjorn might take a different approach altogether, and focus on \u2018ed\u2019 and \u2018that\u2019 to suggest this sentence is about the memory of past rain.", "While all three readers are taking in the same sentence, each is paying attention to different parts. Each is attributing different weighing to some words in relation to \u2018rain\u2019, while discounting the importance of others.", "Their different ways of understanding this sentence also set their expectations of what comes next. When asked to continue the following sentence starting with: \u201cIt \u2026\u201d, they might say:", "Each of these three options makes sense.", "This concept of attention, and how words in a text are related to one another in lots of different ways is a central idea. We\u2019d like to take this idea, of paying attention to semantic relationships between words, and teach a machine to do it.", "The task models like BERT and GPT-2 are trained on is guessing correctly the next word in a piece of text. Once trained, such a model can, therefore, be used to generate new text, word by word.", "The main difference between the two models is that, during training, in the BERT model you\u2019re allowed to also look at the text that comes after the missing word, whereas with GPT-2 you\u2019re only looking backwards. Generating text word-by-word sounds iffy; wouldn\u2019t it make sense to generate several words that together make sense?", "Indeed, to have a longer horizon beyond a single next word, you can think of a \u201cbeam\u201d into the future: instead of going straight for your top next-word candidate, you keep the top 5 candidates, and for each, generate their top 5 next-words, and so on.", "After several rounds of this, you\u2019ve quite a few potential extensions to the sentence and you can choose the best ones. If we then ask GPT-2 to complete a sentence, it might offer sensible suggestions like so:", "The premise \u2014 and promise \u2014 of this article is to explain how large language models like GPT-2 work.", "At the very least, then, I need to tell you what they are. The answer is: They\u2019re a pancake stack.", "GPT-2 is a stack of identical components called decoders, and BERT is a stack of slightly different identical components called encoders.", "How big are these stacks? Well, it varies.", "The more layers there are, the more your input gets tumbled, mixed and crunched. That\u2019s where the \u2018Deep\u2019 in \u2018Deep Learning\u2019 comes from. One of the things researchers are finding (and this was a specific research question when building GPT-3) is the more you stack \u2014 the better the results on some natural language processing tasks.", "You\u2019ll notice it\u2019s not just the stack getting taller as the models get bigger. The number at the bottom, Model Dimensionality, is also getting bigger. We\u2019ll get to this number in the next section; for now, please ignore it.", "To understand how a large language model works, all you need to know is what happens to the input as it passes through one of these components. From then on, it\u2019s just rinse-and-repeat.", "Everything boiling down to a pass-through step inside a small box is a core idea in deep learning. Maintaining a firm grip on the bigger picture, I won\u2019t be covering here an exact step-through.", "Instead, let\u2019s first get clear on inputs.", "These models \u201ceat up\u201d chunks of text. (For the extra-large GPT-2 it\u2019s 1024 tokens, which makes for about 600\u2013700 consecutive words. For BERT it\u2019s half this amount.) This chunk of text is called context.", "Every word is processed individually, and the model calculates an impression of how each word relates to the words that came before it.", "Also, even without specifics, you can be sure that a pass-through step will be made out of a combination of five types of actions:", "Why those? For two very specific reasons:", "Pretty much all of deep learning is founded on the above theoretical premise.", "When it comes to natural language processing, though, I feel not enough pause is given to the odd idea that language tasks are functions too:", "When we translate from English to French, we\u2019re performing a function, that takes in some of words in English and outputs a specific bunch of words in French.", "For text generation, we\u2019re approximating a function that takes in a bunch of written text and spits out a sensible choice for the next word.", "No question, these are intricate functions: English has tens of thousands of words, and an untold number of relationships between them.", "But to get a tingly feel for how something sophisticated can be approximated with a whole lot of simple functions, think of an ever-more intricate 3D mesh:", "2. In practice, approximating something like text generation or translation means repeating such actions MANY MANY MANY TIMES. For the theoretical idea of attention to be computationally feasible, the calculations performed need to have specific mathematical properties.", "Looking at the list, of the five possible actions, one is redundant. For some reason, it also appears in big, shouty letters. That reason is explained in the next part:", "Let\u2019s think about adding things together. On the one hand, we can add NUMBERS: if we take 3 and add 4, we get 7. But, just looking at 7, there\u2019s no trace left of either 3 or 4. It\u2019s just 7.", "On the other hand, you can add MUSIC together: if you take a song and overlay it with another song, you get a new track, but you can still make out each of the two songs. You\u2019ve created something new, but still retained quite a lot of information about each of the original tracks. As of now, I\u2019d like you to start thinking of the words you read as if each and every one were a different 3-minute musical track.", "The reason for this musical detour is because humans have utterly rubbish intuition when it comes to higher-dimension spaces. We extrapolate based on our experience with 2D and 3D. But in many important ways, such as adding stuff together, things behave very differently in higher dimensions.", "A word\u2019s track is called its embedding. Without telling you how it\u2019s done, I\u2019ll state that words with similar meanings end up having similar embeddings.", "Before words go into models like BERT and GPT-2, they\u2019re first turned into long signals \u2014 just not musical ones. But remember the core idea: the sum retains lots of information about each of the original parts.", "Going back to the list of functions each pancake would be made of:", "The reason for the big shouty letters starts becoming clear. Think back to our ABBA attention committee:", "We\u2019re showing Benny, Frida and Bjorn the rain sentence, and asking them to judge how the word \u2018It\u2019 in the following sentence relates to words that came before.", "Then comes Agnetha, and plays the role of the committee chair:", "The numbers I threw (20% or 60%) are not fixed. As the model learns, what changes is both how Benny, Bjorn and Frida decide which past words are important, and also the weighting Agnetha gives to their opinions.", "This all happens within one pancake. Now we stack them: each consecutive box essentially receives as input the sum of the weighted judgement from the previous box, and the previous box\u2019s input. This makes sure the opinions of lower layers don\u2019t get completely lost along the way.", "Remember those big numbers I told you to ignore? As the model grows, it\u2019s not just the number of pancakes in the stack, but how long\u2019s the \u201cmusical signal\u201d.", "Imagine moving from ABBA\u2019s Waterloo, which is just under 3 minutes long, to Queen\u2019s Bohemian Rhapsody (which is nearly 6 minutes long)", "Intuitively, you can sense that when adding together longer musical tracks, information about added components is preserved even better.", "How we grasp meaning is closely tied to how we pay attention to different words. We weigh the influence of words, near and far, in the context of whatever we\u2019re currently reading.", "It allows us, as human readers, to perform tasks like completing gaps in a sentence, or continue the writing in a way that makes sense. So, philosophically speaking, the true meaning of a word in the current context is the collective influence of all the words before it. We can thus imagine a theoretical language model, which would \u2018understand\u2019 text by keeping track of relationships \u2014 however subtle \u2014 between words.", "It would learn this by repeatedly trying to guess the next word in a text. Every time, it\u2019ll check how well it\u2019s done and if it guessed wrongly, it\u2019ll re-balance the different weights it gives relationships between words.", "When making a real-life language model, we use embeddings instead of written words, and these get crunched inside the likes of BERT and GPT-2.", "To get a better feel for what happens to these representations along the way, I suggested we\u2019d better think of it like music, because adding two things together looks more like overlaying music than adding regular numbers.", "Then we looked at the belly of the language machine. It didn\u2019t look very fancy, just a pancake-stack of identical boxes, each one feeding from the one below.", "And, although we didn\u2019t say exactly what goes on inside these boxes, we understand conceptually that they\u2019re a way to implement paying attention to relationships between words, near and far.", "Have I glazed over some of the details? Hell yeah!  The aim was to never lose sight of the bigger picture. While the ABBA committee is not EXACTLY what happens in each decoder, it\u2019s close enough to the truth to make the details comprehensible. If I now told you there aren\u2019t three attention heads in each layer, but twelve of them, it would hardly make much difference, right?", "Generally speaking, whenever this article mentions a specific number (like 12 or 1024), assume researchers have already made it bigger.", "That\u2019s the story of GPT-3 in a nutshell:", "(Given everything we\u2019ve seen, that last line gives room for pause: Do we actually need to go from an ABBA song to a 27-minute musical number for each and every word? Probably not.)", "But the basic premise stays the same. Once in the stratified experimental realm, language models are now fast becoming part of everyday life. They\u2019ve already changed Google Search. They\u2019re already changing translation services. They\u2019re already used to generate text.", "When technology moves away from an academic toy to something with tangible influence, I don\u2019t believe researchers are entitled to stick with obfuscating mathematical language.", "There\u2019s a moral duty to explain these concepts so that anyone can understand. At least, that\u2019s what I told ABBA so they\u2019d agree to come and help.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fada8ff4d1eca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vered.t.zimmerman?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vered.t.zimmerman?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "Vered Zimmerman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35c722c596d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&user=Vered+Zimmerman&userId=35c722c596d3&source=post_page-35c722c596d3----ada8ff4d1eca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fada8ff4d1eca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fada8ff4d1eca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/", "anchor_text": "https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/"}, {"url": "https://jalammar.github.io/illustrated-gpt2/", "anchor_text": "https://jalammar.github.io/illustrated-gpt2/"}, {"url": "https://amaarora.github.io/2020/02/18/annotatedGPT2.html", "anchor_text": "https://amaarora.github.io/2020/02/18/annotatedGPT2.html"}, {"url": "https://arxiv.org/abs/1902.06006", "anchor_text": "https://arxiv.org/abs/1902.06006"}, {"url": "https://lambdalabs.com/blog/demystifying-gpt-3/", "anchor_text": "https://lambdalabs.com/blog/demystifying-gpt-3/"}, {"url": "https://medium.com/dissecting-bert", "anchor_text": "https://medium.com/dissecting-bert"}, {"url": "https://transformer.huggingface.co/doc/gpt2-large", "anchor_text": "https://transformer.huggingface.co/doc/gpt2-large"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ada8ff4d1eca---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/bert?source=post_page-----ada8ff4d1eca---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/gpt-2?source=post_page-----ada8ff4d1eca---------------gpt_2-----------------", "anchor_text": "Gpt 2"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----ada8ff4d1eca---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ada8ff4d1eca---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fada8ff4d1eca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&user=Vered+Zimmerman&userId=35c722c596d3&source=-----ada8ff4d1eca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fada8ff4d1eca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&user=Vered+Zimmerman&userId=35c722c596d3&source=-----ada8ff4d1eca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fada8ff4d1eca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fada8ff4d1eca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ada8ff4d1eca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ada8ff4d1eca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vered.t.zimmerman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vered.t.zimmerman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vered Zimmerman"}, {"url": "https://medium.com/@vered.t.zimmerman/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "243 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35c722c596d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&user=Vered+Zimmerman&userId=35c722c596d3&source=post_page-35c722c596d3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3a38ce4b1cd9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-abba-explainer-to-bert-and-gpt-2-ada8ff4d1eca&newsletterV3=35c722c596d3&newsletterV3Id=3a38ce4b1cd9&user=Vered+Zimmerman&userId=35c722c596d3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}