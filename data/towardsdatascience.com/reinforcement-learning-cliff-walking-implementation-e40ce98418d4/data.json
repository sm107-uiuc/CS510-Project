{"url": "https://towardsdatascience.com/reinforcement-learning-cliff-walking-implementation-e40ce98418d4", "time": 1682996779.055188, "path": "towardsdatascience.com/reinforcement-learning-cliff-walking-implementation-e40ce98418d4/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Cliff Walking Implementation | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Cliff Walking Implementation", "description": "The essence of reinforcement learning is the way the agent iteratively updates its estimation of state, action pairs by trials(if you are not familiar with value iteration, please check my previous\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff", "anchor_text": "example", "paragraph_index": 0}, {"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "reinforcement learning an introduction", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://github.com/MJeremy2017/RL/blob/master/CliffWalking/cliffWalking.py", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://github.com/MJeremy2017/RL/blob/master/CliffWalking/cliffWalking.py", "anchor_text": "here", "paragraph_index": 23}], "all_paragraphs": ["The essence of reinforcement learning is the way the agent iteratively updates its estimation of state, action pairs by trials(if you are not familiar with value iteration, please check my previous example). In previous posts, I have been repetitively talking about Q-learning and how the agent updates its Q-value based on this method. In fact, besides the update method defined in Q-learning, there are more other ways of updating estimations of state, action pairs. In this post, we will together explore another method called SARSA, compare this method with Q-learning and see how the difference in update methods affects an agent\u2019s behaviour.", "Let\u2019s first talk of temporal difference, which is the core of an updating method. We know that at each iteration or episode, an agent explores the environment by taking action following a policy(say \u03f5-greedy), and based on its latest observation, which is summarised as a value of state-action, it updates its current estimates by tweaking the current estimation a little bit towards the latest observation, and the difference between the values of latest observation and last is called temporal difference. And it is from this temporal difference that our agent learns and updates itself.", "The definition of temporal difference distinguishes methods from each other. In order to give you a more concrete sense, let\u2019s directly dive into the algorithm definition and check the difference.", "It is clear that the only difference lies in updating the Q function. In SARSA(by the way, the name SARSA comes explicitly from the process of the agent, which state, action, reward, state, action \u2026), the temporal difference is defined as :", "where the observed Q value of next state, action pair contributes directly to the update of current state. SARSA is also called on-policy, because the update process is consistent with the current policy.", "However the temporal difference defined in Q-learning is:", "where the observed Q value of next state, action pair may not directly contribute to the update of current state. The Q-learning always uses the max value of next state, in which case the state, action being taken to update the Q value may not be consistent with the current policy, thus it is called off-policy method.", "The difference, in fact, could result in different behaviours of an agent. The gut feeling is that Q-learning(off-policy) is more optimistic in value estimation, where it always assume the best action be taken in the process, which, could result in bolder actions of the agent. Whereas SARSA(off-policy) is more conservative in value estimation, which result in saver actions of the agent.", "To clearly demonstrate this point, let\u2019s get into an example, cliff walking, which is drawn from the reinforcement learning an introduction.", "This is a standard un-discounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is -1 on all transitions except those into the region marked Cliff. Stepping into this region incurs a reward of optimal path -100 and sends the agent instantly back to the start.", "This is a typical 2 dimensional board game, so the board settings are mostly same as the example I described here. In the following sections, I will mainly emphasise on the implementation of SARSA and Q-learning, and the comparison of resulted agent\u2019s behaviours between these two methods.", "In a nut shell, we will have a Cliff class which represents the board that is able to:", "These are the major function inside class Cliff. The nxtPosition function takes in an action and returns the agent\u2019s next position on the board, and if the agent bashes its head into the wall(reaches the border), it remains at the same position. The giveReward function gives reward -1 to all states except the cliff area, where results in reward -100.", "Let\u2019s have a look at the board we implemented.", "The agent starts at the left end of the board with a sign S, and the only way to end the game is to reach the right end of the board with a sign G. And * represents the cliff area.", "In terms of game playing, we will have an Agent class representing our agent, and inside the agent class, there is a function decides the agent\u2019s action taking, which is again the \u03f5-greedy policy. (Check out full implementation here)", "The key difference lies in the play function:", "In each episode(each round of the game), we keep track of our agent\u2019s action, state and reward in the list self.states . And at the end of the game, we update the Q function(the self.state_actions) in reversed fashion \u2014 if the method is SARSA(on-policy), the newly updated reward (which essentially is Q(S', A')) will be directly applied to next update, and if the method is Q-learning, there is one more step,", "which takes the maximum value of all actions in that position to the next round of update.The maximum operation shapes the agent behaviour and enables it to take more adventurous actions.", "I ran both methods with exploration rate 0.1 for 500 rounds and took the final (state, action) they learned.", "The result I learnt is slightly different from the optimal result in the book, but it is clear enough to see the difference between the two.", "The conclusion part I will take a reference from Sutton\u2019s book, which perfectly summarises the difference between the two methods:", "Q-learning learns values for the optimal policy, that which travels right along the edge of the cliff. Unfortunately, this results in its occasionally falling off the cliff because of the \u201cepsilon-greedy\u201d action selection. SARSA, on the other hand, takes the action selection into account and learns the longer but safer path through the upper part of the grid. Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of SARSA, which learns the roundabout policy. Of course, if \u03f5 were gradually reduced, then both methods would asymptotically converge to the optimal policy.", "And finally, please check out the full code here. You are welcomed to contribute, and if you have any questions or suggestions, please raise comment below!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe40ce98418d4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----e40ce98418d4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe40ce98418d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe40ce98418d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff", "anchor_text": "example"}, {"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "reinforcement learning an introduction"}, {"url": "https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/RL/blob/master/CliffWalking/cliffWalking.py", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/RL/blob/master/CliffWalking/cliffWalking.py", "anchor_text": "here"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e40ce98418d4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----e40ce98418d4---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/python3?source=post_page-----e40ce98418d4---------------python3-----------------", "anchor_text": "Python3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe40ce98418d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----e40ce98418d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe40ce98418d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----e40ce98418d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe40ce98418d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe40ce98418d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e40ce98418d4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e40ce98418d4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e40ce98418d4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e40ce98418d4--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-cliff-walking-implementation-e40ce98418d4&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}