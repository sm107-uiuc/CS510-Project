{"url": "https://towardsdatascience.com/decision-trees-from-the-root-up-1f169b6e0460", "time": 1683006453.039953, "path": "towardsdatascience.com/decision-trees-from-the-root-up-1f169b6e0460/", "webpage": {"metadata": {"title": "Decision Trees from the Root Up. Build an optimal decision tree by hand\u2026 | by Matt Britton | Towards Data Science", "h1": "Decision Trees from the Root Up", "description": "Decision trees are one of the foundational model types in data science. And luckily, they provide a great example of how computers can automate simple human intuitions to build large, complex models\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0", "anchor_text": "KDE", "paragraph_index": 19}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Kaggle", "paragraph_index": 37}, {"url": "https://github.com/MattJBritton/InteractiveDecisionTrees", "anchor_text": "source code", "paragraph_index": 37}], "all_paragraphs": ["Decision trees are one of the foundational model types in data science. And luckily, they provide a great example of how computers can automate simple human intuitions to build large, complex models. In fact, decision trees optimize a simple criterion, which mirrors how we make decisions in everyday life. Understanding how predictive models mirror our own mental models allows us to think critically about how they might go wrong, and what a good model looks like.", "The goal of this article is to provide an interactive introduction to the theory of decision trees. By the end of this tutorial, you should be able to:", "Accessing the code for this tutorial", "The code and visualizations in this article were all generated in Python using numpy, pandas, and Altair (images by the author except where noted). You can follow along and interact with the charts by using the methods below. Since the purpose is to build your own decision tree by hand, this is highly recommended!", "A decision tree is a logical model that helps you make a prediction based on known data. This prediction consists of whether or not something will happen, or whether an item belongs in a category or not. For example, predict whether a tumor is malignant or benign based on a patient\u2019s medical history.", "This prediction is made using a series of Yes or No questions. \u201cIs the patient a smoker?\u201d is a question that helps a decision tree divide the data into subsets, each with their own behavior. If this behavior is easier to predict than for the whole data set, then this decision tree is effective. Let\u2019s make this concrete with an everyday example.", "You\u2019ve undoubtedly made a decision tree at some point in your life. Let\u2019s take the example of packing for a vacation. Should you bring a bathing suit? Your decision process might look a little like this:", "Decision trees always begin at a single node (the top grey bubble), representing the first question to ask. In this example, we ask if our destination has a place to swim. This question is referred to as a split\u00b9. It must be Yes/No \u2014 note that we handle the temperature quantitative scale by creating a threshold at eighty degrees.", "Eventually, we get to a \u201cleaf\u201d node. This is where we make our decision. Also note that each node is a \u201cparent\u201d of two \u201cchild\u201d nodes unless it is a leaf.", "This model makes intuitive sense to us. Remember, though, that in machine learning we have to evaluate models, and we\u2019ll often be working with complex data sets for which we have limited intuition. So how might we evaluate the model above? More broadly, what characteristics differentiate a good model from a bad one? A couple of examples will help us.", "On the left, we have an example of a good decision tree. One way we can assess the usefulness of this mental model is to ask ourselves how it would have performed on our last 10 trips, represented here by colored squares. Red for a trip where we didn\u2019t need a swim suit, and blue for ones where we did.", "We can see that we initially have a 60/40 split, but that our \u201cNo\u201d branch is all red. In other words, in 3 out of our last 10 trips, we knew beforehand that there was no place to swim, and on all of those trips, we did not regret our lack of bathing suit. So when we answer \u201cNo\u201d to this question, we can be very sure we are making the right decision.", "In 7 out of our last 10 trips, we were aware of a place to swim, and in 6 out of 7 of those occasions, this was the right choice. What happened on the last one? Maybe it was too cold, or the pool was closed unexpectedly. The good thing is that for both of our branches, we are able to make a better guess than we could if we did not ask the question.", "On the right, we have a question that doesn\u2019t seem to have anything to do with swim suits. Moreover, we can see that our question subdivides our past trips into two subsets that each have the same Bring It/Leave it at Home split. If both the Yes/No branches have the same distribution (60/40), then asking the question didn\u2019t give us any new information. So why did we bother asking the question in the first place?", "We can run with this observation and provisionally state that our goal in building decision trees is to produce \u201cpure\u201d child nodes with as few questions as possible. And it follows that if this is the goal of the tree, then each split should aim to \u201cpurify\u201d its child nodes to the extent possible.", "Hold on to this concept of \u201cpurity\u201d, as it will be useful in a second. In the meantime, let\u2019s switch gears to a real data science problem.", "For our real problem, we\u2019re going to use the Titanic data set\u00b2. Each row is a passenger along with demographic and travel information such as their age, sex, and the class of their ticket. The usual purpose is to predict whether a passenger survived or perished.", "How would we go about building a decision tree to predict which passengers survived? What\u2019s a good first split?", "Without any statistics or visual instruments, all we have to go on is any existing knowledge about this data set or the domain. Maybe we\u2019ve seen the movie and guess that more women survived than men. Or we can use knowledge about the world and the design of the ship to guess that first class passengers fared better than third class.", "Of course, as data scientists, we will often be called on to build models on data from new domains, or about which we have little prior intuition. In other words, we need a data-driven way to choose good splits. We can start with the distribution of each feature across each class (Survived and Perished). We\u2019ll use stacked bar charts for categorical features, and KDE (kernel density estimate, basically a smoothed histogram) plots for quantitative features.", "How do we use distributions to identify candidate splits? When looking at categorical variables, we want to select a category that has a markedly different distribution than the others. For example, we can see that men and women survived at different rates. For quantitative variables, we\u2019re hunting for a region where one line is much higher than the other. Age < 10 seems to fit that bill, as do individuals traveling alone (Family Size < 1).", "But now that we have several potential splits, how do we evaluate whether they are any good? How do we compare multiple options?", "Remember the \u201cpure child nodes\u201d objective we discussed earlier? We can use this concept to build a quantitative measure of the \u201cgoodness\u201d of a split.", "We\u2019re going to define entropy, a measure of the purity of a node, and information gain, a measure of how much a split improves the entropy from a parent node to its children\u00b3. Below you\u2019ll find the definitions and formulas. If this is a bit confusing, feel free to skip to the visual below which presents the metrics in context.", "We can get a better intuition for how to choose the best splits by overlaying the calculations on the decision tree structure. The visual below evaluates the information gain of the split Class = Third Class.", "The orange and blues squares show the breakdown of survived/perished at each node. For each node, we also have the entropy, survival rate, and the number of people. Together, these facts tell a story about the data set AND allow us to evaluate how good this split might be:", "Armed with the concept of information gain, we can now calculate the relative value of an individual split. An algorithm for building decision trees can evaluate many potential splits quickly to find the best one. To do this manually, we need a tool to assist us.", "This interactive visualization lets us try out many potential splits. The feature distribution charts from earlier have been augmented with the ability to select a split, by clicking on a category in bar charts or just panning on quantitative features. When a split is selected, the sub-charts below lists the information gain and the entropy of the left and right subsets.", "Note that entropy and information gain figures have been multiplied by 100 for readability. Otherwise, the example above can be recreated exactly.", "There\u2019s a clear winner in information gain here: Male/Female is by far the best split. However, given that we want a more complex, multi-variable model, we can repeat this process on the left and right side to determine the best split for each of these nodes. We now are building a model like the following:", "You can use the interactive visualization for the female and male subsets below. This version adds a line chart showing the information gain for all possible splits. Look for any interesting scenarios where multiple splits have similar information gain values.", "Now we can compare this simple, handmade model with the version produced by an implementation of a decision tree algorithm, such as the one in scikit-learn. The following code performs basic data cleaning and feature engineering, then builds the model and outputs its structure.", "This model has a total information gain (from the first node to the leaves) of 0.312, and an accuracy of 0.796. Did you get the same splits?", "We sometimes think of machine learning models as a fundamentally different type of intelligence from our own. However, in this case, at least, the process of model-building is rooted in the same type of common sense solutions we might bring to bear on a problem. Decision trees are built using simple iterative greedy maximization of entropy, a quantity that we have an intuition for.", "Moreover, gaining familiarity with the tree-construction algorithm helps us as data scientists to understand and appreciate the trade-offs inherent in the models we can make with a few lines of code.", "In my next article on decision trees, we will build on these concepts to analyze the structures of more complex decision trees, and understand how trees can overfit to a training set. Understanding the structural characteristics of good decision trees, and placing the different types of structures into a taxonomy, is a very helpful skill for a data scientist.", "\u00b9Alternate terms for a split include rule or test node. I use split here to draw attention to how the predicate splits the data set into subsets.", "\u00b2The data set consists of the 891 passengers in the \u201ctrain\u201d subset on Kaggle. Missing data for the Age column has been imputed. Details are available in the source code.", "\u00b3You will often see gini impurity as an alternative criterion for splitting nodes. For example, scikit-learn uses this as the default criterion, with the ability to switch to information gain. While this math is slightly different, the conceptual considerations are the same, and in practice it doesn\u2019t matter much which one is used.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I\u2019m a data scientist with experience in the non-profit and education sectors. Interested in interpretable ML and computational social science."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1f169b6e0460&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://blog.mattbritton.info/?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": ""}, {"url": "https://blog.mattbritton.info/?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "Matt Britton"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e9905adba83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&user=Matt+Britton&userId=6e9905adba83&source=post_page-6e9905adba83----1f169b6e0460---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f169b6e0460&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f169b6e0460&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@leliejens?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Jens Lelie"}, {"url": "https://www.unsplash.com", "anchor_text": "Unsplash"}, {"url": "https://github.com/MattJBritton/InteractiveDecisionTrees", "anchor_text": "Github"}, {"url": "https://nbviewer.jupyter.org/github/MattJBritton/InteractiveDecisionTrees/blob/master/InteractiveDecisionTrees.ipynb", "anchor_text": "NBViewer"}, {"url": "https://mybinder.org/v2/gh/MattJBritton/InteractiveDecisionTrees/master?filepath=InteractiveDecisionTrees.ipynb", "anchor_text": "Binder"}, {"url": "https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0", "anchor_text": "KDE"}, {"url": "https://datapane.com/mbritton/reports/2494c592-ea45-402a-b1ab-6f6cb34ea2c7/", "anchor_text": "Datapane"}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Kaggle"}, {"url": "https://github.com/MattJBritton/InteractiveDecisionTrees", "anchor_text": "source code"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1f169b6e0460---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----1f169b6e0460---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1f169b6e0460---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----1f169b6e0460---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----1f169b6e0460---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f169b6e0460&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&user=Matt+Britton&userId=6e9905adba83&source=-----1f169b6e0460---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f169b6e0460&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&user=Matt+Britton&userId=6e9905adba83&source=-----1f169b6e0460---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f169b6e0460&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1f169b6e0460&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1f169b6e0460---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1f169b6e0460--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1f169b6e0460--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1f169b6e0460--------------------------------", "anchor_text": ""}, {"url": "https://blog.mattbritton.info/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://blog.mattbritton.info/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matt Britton"}, {"url": "https://blog.mattbritton.info/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "67 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e9905adba83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&user=Matt+Britton&userId=6e9905adba83&source=post_page-6e9905adba83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa89a8c50e06d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-from-the-root-up-1f169b6e0460&newsletterV3=6e9905adba83&newsletterV3Id=a89a8c50e06d&user=Matt+Britton&userId=6e9905adba83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}