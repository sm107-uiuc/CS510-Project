{"url": "https://towardsdatascience.com/generative-classification-algorithms-from-scratch-d6bf0a81dcf7", "time": 1683013434.975908, "path": "towardsdatascience.com/generative-classification-algorithms-from-scratch-d6bf0a81dcf7/", "webpage": {"metadata": {"title": "Generative Classification Algorithms from Scratch | by Daniel Friedman | Towards Data Science", "h1": "Generative Classification Algorithms from Scratch", "description": "Probabilistic generative algorithms \u2014 such as Naive Bayes, linear discriminant analysis, and quadratic discriminant analysis \u2014 have become popular tools for classification. These methods can be\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "anchor_text": "scikit-learn", "paragraph_index": 0}, {"url": "https://cran.r-project.org/web/packages/e1071/e1071.pdf", "anchor_text": "e1071", "paragraph_index": 0}, {"url": "https://dafriedman97.github.io/mlbook/content/introduction.html", "anchor_text": "Machine Learning from Scratch", "paragraph_index": 1}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html", "anchor_text": "book", "paragraph_index": 14}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html#class-priors", "anchor_text": "original book", "paragraph_index": 21}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html#data-likelihood", "anchor_text": "here", "paragraph_index": 30}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html#data-likelihood", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://dafriedman97.github.io/mlbook", "anchor_text": "book", "paragraph_index": 55}], "all_paragraphs": ["Probabilistic generative algorithms \u2014 such as Naive Bayes, linear discriminant analysis, and quadratic discriminant analysis \u2014 have become popular tools for classification. These methods can be easily implemented in Python through scikit-learn or in R through e1071. But how do the methods actually work? This article derives them from scratch.", "(Note that this article is adapted from a chapter of my book Machine Learning from Scratch, which is available online for free).", "We\u2019ll use the following conventions for this article.", "Most classification algorithms fall into one of two categories: discriminative and generative classifiers. Discriminative classifiers model the target variable, y, as a direct function of the predictor variables, x. For instance, logistic regression uses the following model, where \ud835\udf37 is a length-D vector of coefficients and x is a length-D vector of predictors:", "Generative classifiers instead view the predictors as being generated according to their class \u2014 i.e., they see x as a function of y, rather than the other way around. They then use Bayes\u2019 rule to get from p(x|y = k) to P(y = k|x), as explained below.", "Generative models can be broken down into the three following steps. Suppose we have a classification task with K unordered classes, represented by k = 1, 2, \u2026, K.", "We then classify an observation as belonging to the class k for which the following expression is greatest:", "Note that we do not need p(x), which would be the denominator in the Bayes\u2019 rule formula, since it would be equal across classes.", "A generative classifier models two sources of randomness. First, we assume that out of the \ud835\udc3e possible classes, each observation belongs to class \ud835\udc58 independently with probability given by the kth entry in the vector \ud835\udf45. I.e., \ud835\udf45[k] gives P(y = k).", "Second, we assume some distribution of x conditional on y. We typically assume x comes from the same family of distributions regardless of y, though its parameters depend on the class. For instance, we might assume", "though we wouldn\u2019t assume x is distributed MVN if y = 1 but distributed Multivariate-t otherwise. Note that it is possible, however, for the individual variables within the vector x to follow different distributions. For instance, we might assume the ith and jth variables in x to be distributed as follows", "The machine learning task is then to estimate the parameters of these distributions \u2014 \ud835\udf45 for the target variable y and whatever parameters index the assumed distributions of x|y = k (in the first case above, \ud835\udf41_k and \ud835\udeba_k for k = 1, 2, \u2026, K. Once that\u2019s done, we can calculate P(y = k) and p(x|y = k) for each class. Then through Bayes\u2019 rule, we choose the class k that maximizes P(y = k|x).", "Now let\u2019s get to estimating the model\u2019s parameters. Recall that we calculate P(y = k|x) with", "To calculate this probability, we need to first estimate \ud835\udf45 (which tells us P(y = k)) and to second estimate the parameters in the distribution p(x|y = k). These are referred to as the class priors and the data likelihood.", "Note: Since we\u2019ll talk about the data across observations, let y_n and x_n be the target and predictors for the nth observation, respectively. (The math below is a little neater in the original book.)", "Let\u2019s start by deriving the estimates for \ud835\udf45, the class priors. Let I_nk be an indicator which equals 1 if y_n = k and 0 otherwise. We want to find an expression for the likelihood of \ud835\udf45 given the data. We can write the probability that the first observation has the target value it does as follows:", "This is equivalent to the likelihood of \ud835\udf45 given a single target variable. To find the likelihood across all our variables, we simply use the product:", "This gives us the class prior likelihood. To estimate \ud835\udf45 through maximum likelihood, let\u2019s first take the log. This gives", "where the number of observations in class k is given by", "Now we are ready to find the MLE of \ud835\udf45 by optimizing the log likelihood. To do this, we\u2019ll need to use a Lagrangian since we have the constraint that the sum of the entries in \ud835\udf45 must equal 1. The Lagrangian for this optimization problem looks as follows:", "The Lagrangian optimization. The first expression represents the log likelihood and the second represents the constraint.", "More on the Lagrangian can be found in the original book. Next, we take the derivative of the Lagrangian with respect to \ud835\udf06 and each entry in \ud835\udf45:", "This system of equations gives the intuitive solution below, which says that our estimate of P(y = k) is just the sample fraction of the observations from class k.", "The next step is to model the conditional distribution of x given y so that we can estimate this distribution\u2019s parameters. This of course depends on the family of distributions we choose to model x. Three common approaches are detailed below.", "In LDA, we assume the following distribution for x", "for k = 1, 2, \u2026, K. Note that each class has the same covariance matrix but a unique mean vector.", "Let\u2019s derive the parameter estimates in this case. First, let\u2019s find the likelihood and log likelihood. Note that we can write the joint likelihood of all the observations as", "Then, we plug in the Multivariate Normal PDF (dropping multiplicative constants) and take the log:", "Finally, we have our data likelihood. Now we estimate the parameters by maximizing this expression.", "Let\u2019s start with \ud835\udeba. First, simplify the log-likelihood to make the gradient with respect to \ud835\udeba more apparent.", "Then, we take the derivative. Note that this uses matrix derivatives (2) and (3) introduced in the \u201cmath note\u201d here.", "Then we set this gradient equal to 0 and solve for \ud835\udeba.", "Half way there! Now to estimate \ud835\udf41_k (the kth class\u2019s mean vector), let\u2019s look at each class individually. Let C_k be the set of observations in class k. Looking only at terms involving \ud835\udf41_k, we get", "Using equation (4) from the \u201cmath note\u201d here, we get the gradient to be", "Finally, we set this gradient equal to 0 and find our estimate of the mean vector:", "where the last term gives the sample mean of the x in class k.", "QDA looks very similar to LDA but assumes each class has its own covariance matrix. I.e.,", "The log-likelihood is the same in LDA except we the \ud835\udeba with a \ud835\udeba_k:", "Again, let\u2019s look at the parameters for the kth class individually. The log-likelihood for class k is given by", "We could take the gradient of this log-likelihood with respect to \ud835\udf41_k and set it equal to 0 to solve for our estimate of \ud835\udf41_k. However, we can also note that this estimate from the LDA approach will hold since this expression didn\u2019t depend on the covariance term (which is the only thing we\u2019ve changed). Therefore, we again get", "To estimate the \ud835\udeba_k, we take the gradient of the log-likelihood for class k.", "Then we set this equal to 0 to get our estimate:", "Naive Bayes assumes the random variables within x are independent conditional on the class of the observation. That is, if x is D-dimensional,", "This makes calculating p(x|y = k) very easy \u2014 to estimate the parameters of p(x[j]|y), we can ignore all variables in x other than the jth.", "As an example, suppose x is two-dimensional and we use the following model, where for simplicity \u03c3 is known.", "As before, we estimate the parameters in each class by looking only at the terms in that class. Let \u03b8_k = (\u03bc_k, \u03c3_k, p_k) contain the relevant parameters for class k. The likelihood for class k is given by the following,", "where the two are equal due to the assumed independence between the entries in x. Subbing in the Normal and Bernoulli densities for x_n1 and x_n2, respectively, we get", "Then we can take the log likelihood as follows", "Finally we\u2019re ready to find our estimates. Taking the derivative with respect to p_k, we\u2019re left with", "which will give us the sensible result that", "Notice this is just the average of the x_2\u2019s. The same process will give us the typical results for \u03bc_k and \u03c3_k.", "Regardless of our modeling choices for p(x|y = k), classifying new observations is easy. Consider a test observation x_0. For k = 1, 2, \u2026, K, we use Bayes\u2019 rule to calculate", "where \ud835\udc5d\u0302 gives the estimated density of x_0 conditional on y_0. We then predict y_0 = k for whichever value k maximizes the above expression.", "Generative models like LDA, QDA, and Naive Bayes are among the most common methods for classifications. However, the (albeit arduous) details of their fitting process are often swept under the rug. The aim of this article is to make those details clear.", "While the low-level details of estimating parameters for a generative model can be quite complex, the high-level intuition is quite straightforward. Let\u2019s recap this intuition in a few simple steps.", "And that\u2019s it! To see more derivations from scratch like this one, please check out my free online book! I promise that most of them have less math.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Stats Major at Harvard and Data Scientist in Training"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd6bf0a81dcf7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dafrdman?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dafrdman?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "Daniel Friedman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc908b9a1316f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&user=Daniel+Friedman&userId=c908b9a1316f&source=post_page-c908b9a1316f----d6bf0a81dcf7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6bf0a81dcf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6bf0a81dcf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/ml-from-scratch", "anchor_text": "Machine Learning from Scratch"}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/construction.html", "anchor_text": "Source"}, {"url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "anchor_text": "scikit-learn"}, {"url": "https://cran.r-project.org/web/packages/e1071/e1071.pdf", "anchor_text": "e1071"}, {"url": "https://dafriedman97.github.io/mlbook/content/introduction.html", "anchor_text": "Machine Learning from Scratch"}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html", "anchor_text": "book"}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html#class-priors", "anchor_text": "original book"}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html#data-likelihood", "anchor_text": "here"}, {"url": "https://dafriedman97.github.io/mlbook/content/c4/concept.html#data-likelihood", "anchor_text": "here"}, {"url": "https://dafriedman97.github.io/mlbook", "anchor_text": "book"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d6bf0a81dcf7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d6bf0a81dcf7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----d6bf0a81dcf7---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/naive-bayes?source=post_page-----d6bf0a81dcf7---------------naive_bayes-----------------", "anchor_text": "Naive Bayes"}, {"url": "https://medium.com/tag/ml-from-scratch?source=post_page-----d6bf0a81dcf7---------------ml_from_scratch-----------------", "anchor_text": "Ml From Scratch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6bf0a81dcf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&user=Daniel+Friedman&userId=c908b9a1316f&source=-----d6bf0a81dcf7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6bf0a81dcf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&user=Daniel+Friedman&userId=c908b9a1316f&source=-----d6bf0a81dcf7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6bf0a81dcf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd6bf0a81dcf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d6bf0a81dcf7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d6bf0a81dcf7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dafrdman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dafrdman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daniel Friedman"}, {"url": "https://medium.com/@dafrdman/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "27 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc908b9a1316f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&user=Daniel+Friedman&userId=c908b9a1316f&source=post_page-c908b9a1316f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fc908b9a1316f%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-classification-algorithms-from-scratch-d6bf0a81dcf7&user=Daniel+Friedman&userId=c908b9a1316f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}