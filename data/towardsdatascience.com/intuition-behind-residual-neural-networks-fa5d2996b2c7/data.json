{"url": "https://towardsdatascience.com/intuition-behind-residual-neural-networks-fa5d2996b2c7", "time": 1683011375.991494, "path": "towardsdatascience.com/intuition-behind-residual-neural-networks-fa5d2996b2c7/", "webpage": {"metadata": {"title": "Intuition behind Residual Neural Networks | by Ilango Rajagopal | Towards Data Science", "h1": "Intuition behind Residual Neural Networks", "description": "Deep Neural Networks \u2014 \u201cdeep\u201d because of large number of layers, have come a long way in lot of Machine Learning tasks. But how deep? Let\u2019s see the popular case of Image Classification: AlexNet\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/AlexNet", "anchor_text": "AlexNet", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1409.4842", "anchor_text": "Inception", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet paper", "paragraph_index": 5}, {"url": "https://github.com/KaimingHe/resnet-1k-layers", "anchor_text": "ResNet-1001", "paragraph_index": 26}, {"url": "https://tensorboard.dev/experiment/n2VONYJsRRC1nGLI18aLag", "anchor_text": "tensorboard experiment", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep Residual Learning for Image Recognition", "paragraph_index": 32}], "all_paragraphs": ["Deep Neural Networks \u2014 \u201cdeep\u201d because of large number of layers, have come a long way in lot of Machine Learning tasks. But how deep? Let\u2019s see the popular case of Image Classification: AlexNet popularized stacking CNN layers. It consisted of 5 convolution layers. Soon, it was believed that stacking more convolution layers brings better accuracy. Inception is one of the first architectures to demonstrate better performance by using a very deep architecture. It uses 22 convolution layers. Now, what is the deepest we can go to get better accuracy?", "Let\u2019s build a toy dataset and train on that with neural networks having more and more fully connected layers:", "This creates \u201csin function\u201d dataset, creates neural networks with 1 to 30 hidden layers, trains them on the dataset. One might expect that the loss values should be decreasing, then saturating at a point and staying constant. But the results are different:", "What?! Initially, when having 1 hidden layer, we have high loss, where increasing the number of layers is actually reducing the loss, but when going further than 9 layers, the loss increases. Here we are training for epochs=20*t, meaning more training epochs for bigger model. We get our lowest loss at 9 layers, but above that, loss increases. An important point to note here is this is not overfitting, since this is just training loss that we are considering. Does this mean, more layers result in worser performance?", "Let\u2019s try to understand this problem intuitively. If a \u201cshallow\u201d model is able to achieve an accuracy, then their deeper counterparts should at least have the same accuracy. But, when the model gets deeper, it becomes more and more difficult for the layers to propagate the information from shallow layers and the information is lost. This is called Degradation Problem.", "ResNet paper studies about this problem:", "When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly.", "To solve the problem, the deeper layers have to propagate the information from the shallow layers directly, i.e. identity mapping.", "In our case, model with 9 layers is the best performing. In model with 30 layers, the same 9 layers are also present, if the further 21 layers propagate the same result as 9th layer, then the whole model will have the same loss. So now this problem reduces to getting those layers to learn \u201cidentity function\u201d, f(x) = x.", "Can we modify our network in anyway to avoid this information loss? An intuitive solution is to connect the shallow layers and deep layers directly, so that the information is passed directly to the deep layers, like identity function.", "Here we bypass the intermediate layers, and connect the shallow layer to a deep layer. In our case, we could connect 9th layer neurons to the 30th layer directly, then the deep model would perform as same as shallow model. This way, the information is passed directly as identity function. This is the intuition behind Residual Networks.", "By \u201cshortcuts\u201d or \u201cskip connections\u201d, we mean that the result of a neuron is added directly to the corresponding neuron of a deep layer. When added, the intermediate layers will learn their weights to be zero, thus forming identity function. Now, let\u2019s see formally about Residual Learning.", "Let g(x) be the function learned by the layers. Let\u2019s consider h(x) = g(x)+x, layers with skip connections. Here +x term denotes the skip connection.", "In h(x)=g(x)+x, the +x term will bring the original value, layer g(x) has to learn just the changes in the value, or the residue or delta x. Whatever being learned in g(x) is just the residue, either positive or negative to modify x to required value. Hence the name \u201cResidual Learning\u201d.", "For h(x) to be identity function, the residue g(x) just has to become zero function, which is very easy to learn, i.e. set all weights to zero. Then h(x) = 0+x = x, which is the required identity function. This will help overcome the degradation problem.", "Without skip connections, the weights and bias values have to be modified so that it will correspond to identity function. It is very difficult to learn identity function from the scratch, exacerbated by the non-linearity in the layers and results in the degradation problem.", "With the residual learning re-formulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.", "Let\u2019s see the building blocks of Residual Neural Networks or \u201cResNets\u201d, the Residual Blocks.", "Here, the skip connection helps bring the identity function to deeper layers. In the Residual Block, some may notice two points:", "For 1, if we had performed relu before addition, then the residues will all be positives or zero. Only positive increments to the identity are learnt, which significantly reduces the learning capacity. For example in the sin function, sin(3\u03c0/2) = -1, which would need negative residue. Similarly, using sigmoid will also be disadvantageous, because it produces residues only within 0 to 1. Ideally, we would like unconstrained response from weight layer (spanning any numerical range), to be added to skip layer, then apply activation to provide non-linearity. This helps the model learn any function.", "For 2, if we had used a single weight layer, adding skip connection before relu, gives F(x) = Wx+x, which is a simple linear function. This is equivalent to just a single weight layer and there is no point in adding skip connection. So we need at least one non-linearity before adding skip connection, which is achieved by using two layers.", "The weight layers in these blocks are learning residuals as we saw in previous section. These blocks can be stacked more and more, but there won\u2019t be degradation in the performance.", "Enough theory, let\u2019s see how we can implement residual block:", "This is a simple implementation of residual block. We can call this multiple times to stack more and more blocks. Here we can replace dense layers with convolution layers in the case of images. One constraint to this residual block is that the layer outputs have to be in the same shape as the inputs, but there are workarounds for it.", "Let\u2019s experimentally verify whether the ResNets work the way we describe. We build progressively deeper models and train it on sin function dataset, similar to the example above, this time with Residual blocks. The result:", "The above graph clearly verifies the whole point of this ResNets! We see that ResNet is not degrading in performance when we increase the number of layers. It\u2019s actually improving, which is even better! The code for training the PlainNets and ResNets on sin function dataset is in the following github repo:", "This characteristic of ResNet helped train very deep models, spawning several popular neural networks namely ResNet-50, ResNet-101, etc. Can we go even deeper? Lo and behold, ResNet-1001!", "Working on toy dataset helped understand the ResNet. Now, time for some real world dataset. The above github repo has code to build and train multiple configurations of ResNets and PlainNets on CIFAR-10. You can see all the implementation details there. It is built using Tensorflow (Keras API).", "The results of training on CIFAR-10 are available here in this tensorboard experiment. You can see the comparison between different depths of PlainNet and ResNet:", "The run names are Network x Size. In the \u2018Graphs\u2019 tab, you can visualize the network architectures. We can see the skip connections in ResNet models and absence of them in PlainNets.", "Plotting accuracy values vs network size, we can clearly see, for PlainNet, the accuracy values are decreasing with increase in network size, showcasing the same degradation problem that we saw earlier. As for ResNet, we see increase in accuracy as we increase the network depth.", "Looking at these graphs, you might be wondering, what ResNetV2 is. ResNetV2 is ResNet with some improvements. That will be topic of another article!", "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun, Deep Residual Learning for Image Recognition (2015)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa5d2996b2c7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ilango100?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilango100?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "Ilango Rajagopal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8cd0801dc76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&user=Ilango+Rajagopal&userId=8cd0801dc76f&source=post_page-8cd0801dc76f----fa5d2996b2c7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa5d2996b2c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa5d2996b2c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@craig000?utm_source=medium&utm_medium=referral", "anchor_text": "Riccardo Pelati"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/AlexNet", "anchor_text": "AlexNet"}, {"url": "https://arxiv.org/abs/1409.4842", "anchor_text": "Inception"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet paper"}, {"url": "https://github.com/ilango100/resnet", "anchor_text": "ilango100/resnetThis repository contains tensorflow implementation of Residual Neural Network. For comparison, three network\u2026github.com"}, {"url": "https://github.com/KaimingHe/resnet-1k-layers", "anchor_text": "ResNet-1001"}, {"url": "https://tensorboard.dev/experiment/n2VONYJsRRC1nGLI18aLag", "anchor_text": "tensorboard experiment"}, {"url": "https://tensorboard.dev/experiment/n2VONYJsRRC1nGLI18aLag", "anchor_text": "Tensorboard makes life easier!"}, {"url": "https://tensorboard.dev/experiment/n2VONYJsRRC1nGLI18aLag/#graphs", "anchor_text": "ResNet and its skip connection"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep Residual Learning for Image Recognition"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----fa5d2996b2c7---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/image-recognition?source=post_page-----fa5d2996b2c7---------------image_recognition-----------------", "anchor_text": "Image Recognition"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fa5d2996b2c7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----fa5d2996b2c7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa5d2996b2c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&user=Ilango+Rajagopal&userId=8cd0801dc76f&source=-----fa5d2996b2c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa5d2996b2c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&user=Ilango+Rajagopal&userId=8cd0801dc76f&source=-----fa5d2996b2c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa5d2996b2c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffa5d2996b2c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fa5d2996b2c7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fa5d2996b2c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilango100?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilango100?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ilango Rajagopal"}, {"url": "https://medium.com/@ilango100/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "96 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8cd0801dc76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&user=Ilango+Rajagopal&userId=8cd0801dc76f&source=post_page-8cd0801dc76f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6b2bef1737bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-behind-residual-neural-networks-fa5d2996b2c7&newsletterV3=8cd0801dc76f&newsletterV3Id=6b2bef1737bc&user=Ilango+Rajagopal&userId=8cd0801dc76f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}