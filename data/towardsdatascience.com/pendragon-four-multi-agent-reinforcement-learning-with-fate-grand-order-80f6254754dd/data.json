{"url": "https://towardsdatascience.com/pendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd", "time": 1683002159.848858, "path": "towardsdatascience.com/pendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd/", "webpage": {"metadata": {"title": "Pendragon Four: Multi-Agent Reinforcement Learning with Fate Grand Order | by Michael Sugimura | Towards Data Science", "h1": "Pendragon Four: Multi-Agent Reinforcement Learning with Fate Grand Order", "description": "Multi-agent policy gradient based reinforcement learning in a custom build game environment to train 4 agents simultaneously to play the mobile phone game Fate Grand Order"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/project-pendragon-an-ai-bot-for-fate-grand-order-23f51b6e3268", "anchor_text": "Project Pendragon", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/project-pendragon-part-2-a-reinforcement-learning-bot-for-fate-grand-order-7bc75c87c4f3", "anchor_text": "RL agents", "paragraph_index": 0}, {"url": "https://openai.com/projects/five/", "anchor_text": "Dota 2", "paragraph_index": 6}, {"url": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "anchor_text": "Starcraft 2", "paragraph_index": 6}, {"url": "https://github.com/sugi-chan/multiagent_pendragon/blob/hal_way_to_policy_gradient/fgo_environment/heroic_spirt.py#L72", "anchor_text": "HeroicSpirit", "paragraph_index": 15}, {"url": "https://github.com/sugi-chan/Pendragon_Alter/blob/master/colosseum.py", "anchor_text": "old environment", "paragraph_index": 24}, {"url": "https://github.com/sugi-chan/multiagent_pendragon/blob/hal_way_to_policy_gradient/fgo_environment/colosseum.py", "anchor_text": "new one", "paragraph_index": 24}, {"url": "http://karpathy.github.io/2016/05/31/rl/", "anchor_text": "Pong to Pixel blog", "paragraph_index": 31}, {"url": "https://ai.stackexchange.com/questions/2980/how-to-handle-invalid-moves-in-reinforcement-learning", "anchor_text": "stack overflow post", "paragraph_index": 35}, {"url": "https://towardsdatascience.com/project-pendragon-an-ai-bot-for-fate-grand-order-23f51b6e3268", "anchor_text": "pendragon post", "paragraph_index": 57}, {"url": "https://www.linkedin.com/in/michael-sugimura-b8120940/", "anchor_text": "https://www.linkedin.com/in/michael-sugimura-b8120940/", "paragraph_index": 70}], "all_paragraphs": ["A little over a year ago I did my first series of blog posts with Towards Data Science, I affectionately called it Project Pendragon, about how I built a reinforcement learning (RL) environment, successfully trained RL agents, and made an API for to allow them to extract information and send commands to the mobile phone game Fate Grand Order (FGO). This post will cover how I picked that project back up and trained 4 RL agents to work together to play the game.", "One of my main motivations for building bots for FGO is that the game requires players to clear levels many times in order to gather required materials for events, leveling characters, skills etc. By many times I mean 40\u201350 times for a 5 minute level is not unheard of. So being able to have a bot play these repeated missions for me is quite nice.", "FGO is a game where you pick between 1 and 6 characters and use that team to fight through levels until all enemies or the team is defeated. At any given point in time you can have 3 characters on the field and they each have 3 abilities. These abilities can be attack buffs, healing spells, etc.", "The main mechanics of combat in FGO is that combat is done by picking a set of 3 cards out of the 5 you are dealt from your deck. My previous FGO bot pendragon alter is focused on picking these cards in an effective manner.", "Each type of card has its own properties and playing them in different combinations has different benefits. For example, playing three cards of the same color adds additional bonuses on top of the base cards . The nature of the bonuses depend on the types of card. Arts cards (blue) charge powerful ultimate abilities, Buster cards (red) deal additional damage, and Quick cards (green) create the possibility of critical hits, double damage. I try to explain the game mechanics that are important for this post, but feel free to check out the previous post for a more detailed overview of other card mechanics that make up FGO.", "Since training those initial card picking RL agents I haven\u2019t done much work on adding additional functionality to improve my bots or raise their level of play. However recently I have been pretty inspired to improve them because of all the interesting things that have been happening with RL over the past year.", "For me it has been really exciting to see the progress with research groups like OpenAI and Deepmind and their successes in applying reinforcement learning to games like Dota 2 (OpenAI Five) and Starcraft 2 (Alphastar) both reaching extremely high levels of play. While they did have their faults I still found it impressive nonetheless. Initially the one that I found the most exciting was Alphastar since I was a long time Starcraft 2 player. However reading through the OpenAI Five literature was what inspired me to work on more reinforcement learning with the goal of taking additional steps with my FGO bots.", "When I made my initial RL agents for FGO I focused on just picking cards effectively. This made it so my bots could clear the a good amount of game content but it meant I was not using the individual character abilities that I mentioned above. These abilities can be very powerful and help human players successfully clear levels faster and clear harder content. So my thought was that if I could incorporate the usage of abilities into my bot it would be beneficial. Something I was unsure of was what was the best way to do this\u2026 and I was stuck in a bind until I was reading through the posts that OpenAI released which stated how they train 5 individual bots and have them work together to play.", "Trying to make a single agent to handle all three characters would have made for a confusing action space so the idea of training a single agent per character helped me simplify it mentally and at least let me think about starting to attack the problem. Although I say that this helped me clarify the problem in my head, it also left me at a point where I was VERY seriously considering training 3 maybe 4 RL agents simultaneously in a single environment which I had never done.", "My idea was to have 3 agents (one for each of the three characters) whose action spaces would be the spells they could use plus a node to allow them to pass (action space of size 4). Then have 1 additional agent for picking cards. If I could train these 4 agents to play cooperatively then I would likely be able to make a FGO bot that could clear harder content or at least clear content more quickly than my previous versions of the bots.", "With this end goal in mind\u2026 what is required to get this done? A LOT of upgrades to my previous game environment which was designed for just card picking.", "While I would like to be able to train my bots in a perfect recreation of FGO with all the characters, skills, enemies etc, it just isn\u2019t feasible for me to build all of that. Even for groups like OpenAI and Deepmind it is always interesting to see what aspects of games they limit and how that list changes over time as they build more complex agents.", "For this version of my multi agent FGO bots I have a number of noteworthy constraints.", "Now with that out of the way, let us begin!", "Since one of the main goals of this project is to add in individual agents for individual characters to let them play together as a team to clear levels\u2026 one of the first things I needed to figure out how to do was how to represent the individual characters. This is something that I wanted to do last year when I made my original bots, but I was a much weaker software engineer at the time so I pushed it off to a later date and I guess that time is now.", "What I did was I setup a general class called HeroicSpirit which contains the neural network for that agent and methods for getting actions, training/updating the network, tracking skills, and the three character skills themselves. This general class sets the framework for managing and maintaining my characters in the game and then I create HeroicSpirit subclasses for each of the specific characters that I want to train.", "For the most part each subclass just has to adjust the features specific for that new character. This would be things like character specific skills and abilities while the book keeping features are maintained in the main HeroicSpirit class. This helps minimize the amount of code I need to change/update if I am just adding in generic features.", "For instance when I started prototyping code I didn\u2019t realize how much effort it was going to be to track active abilities, current damage modifiers for each character, and map the effects of those abilities onto the characters they should be targeting so I had to add a lot of functionality onto HeroicSpirit that would have been annoying to update in three places.", "At the moment I have three subclasses of HeroicSpirit for the three characters whose abilities I have actually programmed out at the moment JAlter ,Ishtar ,ArtoriaSaber. For each of these characters as I mentioned before is that their skills perform different in game effects that I had to be able to account for. At the moment I am representing skills with a dictionary that tracks a wide variety of values that have values depending on the skill. For instance Ishtar\u2018s first skill is a combat oriented team wide damage boost represented by the following dictionary:", "So for every skill I track its name, targets, and various effects like hit point boost, charging ultimate ability (np_boost), critical damage boost, critical star boost, damage boost, and duration. Most skills will only have effects for a few of these fields so for this skill 'dmg_boost': ('all', .2) means a 20% damage boost is applied to everyone on the team represented by spots on the target field ['hero1','hero2','hero3']. 'duration': 3 means that the skill lasts for 3 turns.", "On the other hand Ishtar\u2018s second skill is focused on charging her ultimate ability rather than direct combat.", "So while all the same categories are tracked in the skill dictionary this spell only targets Ishtar herself so target is directed at whatever spot on the team Ishtar is placed at and no damage boost is applied. Instead Ishtar receives 50% charge for her ultimate ability. This ability also only lasts for a single turn.", "I started with three characters because that is the bare minimum I wanted to try and field at once. The reason I started with these characters it that they are all damage oriented characters whose abilities are combat oriented. In this initial phase this helps me simplify the number of things I have to build. Future work will go into adding in support characters whose abilities often target other team members and are crucial to clearing harder content in the game.", "These character classes set up the framework for what characters can do and in general how we will make track what they are doing so now we have to build an environment for them to play in.", "There are oh so many things I had to do to upgrade the game environment to handle the additional complexity from having each character active at once, track buffs and their effects, and applying their buffs to modify their damage outputs. The old environment was ~400 lines of code and the new one is ~1200. A lot which is just adding new functions to track various aspects of the game.", "The basic structure of this environment is each game consists of a 3 round battle (mirroring FGO game levels) and each round becomes more difficult such that round 3 is significantly harder than round 1. In this environment I do this by increasing both the health and damage per turn of the opponents who the bots must face off against. At the moment I have not put in the time to build out complex opponents but it is something I may do in the future.", "In my HeroicSpirit class I have the functionality to get actions and train the agents based on rewards calculated by the environment. So in the environment I needed to be able to process those outputs and apply them to the game itself and be ready to feed accumulated rewards back to the bots. This comes down to a few steps per character.", "For each of the three characters:", "Once all the characters have gone I check through all of the buffs in each \u201cbuff dictionary\u201d and apply the relevant modifiers from each character\u2019s buffs to their damage or other combat statistics. once that is done I remove ones buffs are no longer active i.e duration = 0 . After the buffs for that turn are applied then I do the card picking combat section of the game and the effects of the bots are applied if they are relevant. For example if a character has a 20% damage buff active for themselves and their cards are picked, then the damage from that card will be 20% higher and so on.", "In previous RL projects I have done I have used Deep Q Learners and tried to make it so at any given game state all actions were valid. However with this project I switched to make a policy gradient method and allow for invalid moves. Both of these things added complexity in my code since I had never implemented these things before but I think they were worthwhile upgrades.", "Deep Q Learners are agents which are given a reward every round for their actions and they are effectively looking for the highest reward at any given state. This is a fine approach but for me it often requires engineering rewards for different parts of the game and I feel like I have to impose my will/play style onto the bots. I still do this with my card picker bot that I train up alongside the 3 character agents. The other way to attack the problem is to collect all of the actions and game states of the agents and once the game is finished assign a reward to the group based on something like winning or losing. This is essentially what policy gradient methods are (as I understand now, and I could easily be quite wrong).", "At first this seems like a funny idea since even in a loss there could have been moves that were good and reasonable to make, but they will be penalized negatively. This is true! but the hope is that if that is actually a good move then over time it will have a net positive reward and the agent will learn to do it more in certain situations. Andrej Karpathy\u2019s Pong to Pixel blog is excellent on this topic!", "At the moment I am not using a pure +1 or -1 style reward for the policy gradients. I apply modifiers to certain turns if they used skills and whatnot to try and incentivize skill usage. For a large portion of the game the bots just pass which makes sense given they only have 3 skills that I only allow to use once, but the game could last 10\u201315 rounds. So they tended to just get used to passing most of the time without some differentiated reward.", "The next important thing to think through was how to handle invalid moves. In this game example invalid moves are skills which have been used and are now on cooldown and if the bot tries to pick it in the game it won\u2019t actually do anything. In the past I avoided this issue by making game states where all moves were valid at any given time. The trade off here is that the action space becomes much larger than it would be if valid moves were accounted for.", "For instance in my card picker bot, the action space is 60 which represents every way you could pick 3 cards out of 5 where order matters. I did this because while the action space is large at 60, all 60 moves are valid. On the other hand if I accounted for invalid choices then I could have had as few as 3 actions (one for each card type) and had it choose what hand to play by looking at what cards it had not already picked.", "The reason I have avoided doing this until now is because I was really just unsure on the best way to do it/how to do it at all. When digging into how to account for invalid moves I came across a very informative stack overflow post.", "\u201cJust ignore the invalid moves.\u201d -Stack Overflow", "What I ended up doing was that during training, once a skill goes on cooldown I suppress the output of that skill\u2019s node to a low number (the lowest number in the array -1). This suppressed version of the output array is fed back into the network as the target for the network. What skills that have been used are also tracked in the game state for the bots. The hope here is that the bots will learn that it cannot pick a skill that has been previously used.", "Rewards and invalid moves I think of as more mechanical parts of building these bots, so now I just have to figure out some of the more philosophical things. What do I have to do to get the bots to play together as a team? What does teamwork even mean for a bunch of neural networks?", "I mentioned earlier how I was inspired to try and make my next series of FGO bots after reading the work OpenAI has been doing over the past year or so. One of the most interesting parts to me was this idea that they could train up individual bots and have them play together as cooperative teams. OpenAI has a teamwork parameter which they can tune to be between 0 and 1 and higher values show how much bots should value team rewards over maximizing their own.", "So that got me thinking how to fold in teamwork or at least some version of communication. While I considered using another overarching network to manage teamwork, I actually ended up going with a pretty simple idea which was to have the bots go in a set order each turn and after each bot goes I modify the game state to show what actions they took. The hope here is that the bots will be able to make more informed decisions based on the actions of earlier ones.", "For my current bots/environment the game state is represented by two length 9 arrays (3 spots per character) along with the round number and turn number. The two length 9 arrays are the ones which track the actions of the 3 bots. The first array represents which skills are active that turn, the second represents which skills have been cast. On turn 1 of round 1 of a new game these are both all 0. Below is an example game state from round 1 turn 1 of a game.", "This original game state is what the first character will see on turn 1 of the game take their turn. After seeing this array Say they choose to use their first skill. The game state is modified so that the first element of each array is set to 1 since character 1 used their first skill and that first skill currently active.", "Then we feed this new_gamestate1 to the second character as input. So the second character can see that character 1 used a skill and that it is active. Based on that knowledge it can decide if it wants to use a skill as well. Lets say that seeing that character 1 used a skill character 2 follows suit and uses skill 3. The new game state is below.", "The game state from character 1 state is modified and then passed to character 3.", "So character 3 is in the best position to make decisions since it gets to see what character 1 and 2 do before it has to expend any of its skills and it can act in conjunction with the other two bots.", "My hope is that this framework for teamwork will allow bots to work together and one of the ways this will appear in the game is if the bots use multiple skills in conjunction. For now I\u2019m going to call this feature a \u201cskill barrage\u201d.", "In FGO a lot of the power of skills is that you can use them in conjunction with other skills from other characters. Layering different effects on top of one another allows players to amplify effects from different characters to drastically increase their team\u2019s damage output.", "At the moment with the characters I have trained agents for that means they are amplifying damage but the strongest damage buffs in the game usually come from \u201csupport\u201d characters. Support characters deal less damage so they bring less direct firepower than a damage dealer type character. They make up for this by bringing powerful skills that can make or break a difficult game.", "My idea is to place support characters in spots 1 and 2 and have the main damage dealer in spot 3. So when the damage dealer sees that the two support characters have cast their buffs then character 3 would also use a skill to further increase their damage.", "Below is an example of a skill barrage from the three agents.", "I will write up some general notes on training the models here, but will add a more detailed post on the topic soon (this article is getting WAY too long for most people to want to read it).", "I built all 4 agents in Keras with a Tensorflow backend and trained them on a single 2080 TI GPU. For those of you that have used Tensorflow on a GPU, Tensorflow allocates all available GPU memory to whatever model is loaded in for training. In previous version of this pipeline I was not able to run multiple Tensorflow bots at once which made memory allocation difficult.", "I ended up getting around this for this pipeline with a code snippet I found online. Basically what it does is that it limits the amount of GPU space a model can allocate to itself to some percentage. The models I trained here are actually quite small so I limited it to 20% of my GPU memory.", "Since each of the 4 models I was training and at certain points I actually had multiple training runs going on my single 2080 TI.", "After some tinkering I made a game environment that was hard enough to allow for decent training, my first versions were too hard and the bots just lost all the time without getting much positive reward if ever. Once I had tuned it to a decent level I saw the random guess win rate for the 4 bots was around 38% and would peak at around 72% after maybe 20\u201330K games. After doing some additional work and a lot of experimenting I got the win rate up to around 84%.", "I will cover the fun details of that in a seperate post since I am already running quite long on this current one.", "Once that was done I had to update my previously built API to allow the new 4 bot version to click on abilities and get the required information from the game. A lot of that initial framework I lay out in my first pendragon post.", "The main idea is that when a bot chooses to use a skill, I have the location of the skill button recorded and can send the mouse to that location and click using python. Once that was in place I was able to start running the bots against actual FGO missions and I have been pleasantly surprised with the results.", "They clear missions relatively quickly and for a pretty generic mission similar to the way their environment was structured they take around 7\u20138 turns to clear. By comparison I think a human could clear that level with this specific team in 5\u20138 turns.", "Some initial thoughts on bot behavior:", "This is behavior I personally think is sub optimal since round 1 and 2 are usually much easier than round 3. However for the bots the future is uncertain and they want to make sure that they actually live until round 3.", "These bots are trained in a policy gradient way where they are rewarded based on winning or losing. So while they might have a good chance of clearing round 3 quickly if they save more skills. The bots need to make sure they live until that point. If they save all their skills they may not even make it to round 3 or may take too much damage along the way. So using skills early and saving a few for round 3 seems like a more stable strategy for them to use. It lets them get to the late game and at least have a chance of winning.", "At the moment the bots do appear to successfully use skill barrages as I hoped and 2\u20133 of them frequently use skills together. However these skills may not go together super well. For instance one bot might use a skill that increases their critical hit damage and while other bots could pair other skills that increase critical damage they do things like charge their own NP ability or something similar. Those skills do not have any real synergy, but the bots use them together.", "This may mean the bots need more time to explore the game environment or that the environments I have built aren\u2019t hard enough for them to require that level of communication. They never see what their abilities do, they just see the rewards from the environment based on if they win or lose after using them.", "So while I complained that the bots play sub optimally and use skills early, they also tend to save some abilities for the late game. I think this is the bots realizing that clearing the first part of the mission is important and clearing it with high health makes them more likely to win. They still need to save some abilities for that final battle if they want a good chance at winning. This is good longer term planning that I was hoping the bots would be able to learn.", "In order to get to this point of training and deploying these bots to play the game Fate Grand Order has taken me around a month of work and a lot of failure/experimentation. Most of my posts are based off of projects other people have done and can usually find some loose set of documentation or suggestions to follow. This one I was mostly working things through on the fly and frequently annoying my friends about different ideas I had.", "Technically my previous bot could play a lot of FGO content where I would want to use a bot to play. But it is hard for me to resist the temptation of trying to build bots that surpasses it significantly as soon as I figured out a plausible way to do so. I have a bit of a data science addiction which doesn\u2019t feel like a bad thing.", "Going forward I have a number of goals for this project:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "data scientist, gamer, martial artist, photographer, and chef\u2026 also part time house cat. https://www.linkedin.com/in/michael-sugimura-b8120940/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F80f6254754dd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----80f6254754dd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----80f6254754dd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@michaelsugimura?source=post_page-----80f6254754dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michaelsugimura?source=post_page-----80f6254754dd--------------------------------", "anchor_text": "Michael Sugimura"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5d6bed290a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&user=Michael+Sugimura&userId=5d6bed290a7c&source=post_page-5d6bed290a7c----80f6254754dd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80f6254754dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80f6254754dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/sugi-chan/multiagent_pendragon", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/project-pendragon-an-ai-bot-for-fate-grand-order-23f51b6e3268", "anchor_text": "Project Pendragon"}, {"url": "https://towardsdatascience.com/project-pendragon-part-2-a-reinforcement-learning-bot-for-fate-grand-order-7bc75c87c4f3", "anchor_text": "RL agents"}, {"url": "https://openai.com/projects/five/", "anchor_text": "Dota 2"}, {"url": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "anchor_text": "Starcraft 2"}, {"url": "https://github.com/sugi-chan/multiagent_pendragon/blob/hal_way_to_policy_gradient/fgo_environment/heroic_spirt.py#L72", "anchor_text": "HeroicSpirit"}, {"url": "https://github.com/sugi-chan/Pendragon_Alter/blob/master/colosseum.py", "anchor_text": "old environment"}, {"url": "https://github.com/sugi-chan/multiagent_pendragon/blob/hal_way_to_policy_gradient/fgo_environment/colosseum.py", "anchor_text": "new one"}, {"url": "http://karpathy.github.io/2016/05/31/rl/", "anchor_text": "Pong to Pixel blog"}, {"url": "https://ai.stackexchange.com/questions/2980/how-to-handle-invalid-moves-in-reinforcement-learning", "anchor_text": "stack overflow post"}, {"url": "https://towardsdatascience.com/project-pendragon-an-ai-bot-for-fate-grand-order-23f51b6e3268", "anchor_text": "pendragon post"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----80f6254754dd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----80f6254754dd---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----80f6254754dd---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/fate-grand-order?source=post_page-----80f6254754dd---------------fate_grand_order-----------------", "anchor_text": "Fate Grand Order"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F80f6254754dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&user=Michael+Sugimura&userId=5d6bed290a7c&source=-----80f6254754dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F80f6254754dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&user=Michael+Sugimura&userId=5d6bed290a7c&source=-----80f6254754dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80f6254754dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----80f6254754dd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F80f6254754dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----80f6254754dd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----80f6254754dd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----80f6254754dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----80f6254754dd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----80f6254754dd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----80f6254754dd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----80f6254754dd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----80f6254754dd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----80f6254754dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michaelsugimura?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michaelsugimura?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Sugimura"}, {"url": "https://medium.com/@michaelsugimura/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "306 Followers"}, {"url": "https://www.linkedin.com/in/michael-sugimura-b8120940/", "anchor_text": "https://www.linkedin.com/in/michael-sugimura-b8120940/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5d6bed290a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&user=Michael+Sugimura&userId=5d6bed290a7c&source=post_page-5d6bed290a7c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffdc932923852&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpendragon-four-multi-agent-reinforcement-learning-with-fate-grand-order-80f6254754dd&newsletterV3=5d6bed290a7c&newsletterV3Id=fdc932923852&user=Michael+Sugimura&userId=5d6bed290a7c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}