{"url": "https://towardsdatascience.com/building-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5", "time": 1683002623.658239, "path": "towardsdatascience.com/building-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5/", "webpage": {"metadata": {"title": "Building a Dynamic data pipeline with Databricks and Azure Data Factory | by Paul Simpson | Towards Data Science", "h1": "Building a Dynamic data pipeline with Databricks and Azure Data Factory", "description": "TL;DR A few simple useful techniques that can be applied in Data Factory and Databricks to make your data pipelines a bit more dynamic for reusability. Passing parameters, embedding notebooks\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pypi.org/project/azure-storage-blob/", "anchor_text": "azure-storage", "paragraph_index": 9}, {"url": "https://docs.databricks.com/security/secrets/secret-scopes.html", "anchor_text": "The next part will assume that you have created a secret scope for your blob store in databricks CLI", "paragraph_index": 10}, {"url": "https://docs.databricks.com/data/data-sources/azure/azure-storage.html", "anchor_text": "other documented ways of connecting with Scala or pyspark", "paragraph_index": 11}], "all_paragraphs": ["TL;DR A few simple useful techniques that can be applied in Data Factory and Databricks to make your data pipelines a bit more dynamic for reusability. Passing parameters, embedding notebooks, running notebooks on a single job cluster.", "-Passing Data Factory parameters to Databricks notebooks", "-Running multiple ephemeral jobs on one job cluster", "This section will break down at a high level of basic pipeline", "The main idea is to build out a shell pipeline in which we can make any instances of variables parametric. In this instance we look at using a get metadata to return a list of folders, then a foreach to loop over the folders and check for any csv files (*.csv) and then setting a variable to True. Then *if* the condition is true inside the true activities having a Databricks component to execute notebooks.", "This goes without saying, completing a pipeline to make sure as many values are parametric as possible. This is so values can be passed to the pipeline at run time or when triggered. Reducing as many hard coded values will cut the amount of changes needed when utilizing the shell pipeline for related other work.", "For maintainability reasons keeping re-usable functions in a separate notebook and running them embedded where required. A quick example of this; having a function to trim all columns of any additional white space.", "There is the choice of high concurrency cluster in Databricks or for ephemeral jobs just using job cluster allocation. After creating the connection next step is the component in the workflow. Below we look at utilizing a high-concurrency cluster.", "Adjusting base parameter settings here as in fig1 will allow for the Databricks notebook to be able to retrieve these values. This is achieved by using the getArgument(\u201cBlobStore\u201d) function. These parameters can be passed from the parent pipeline. This makes it particularly useful because they can be scheduled to be passed using a trigger.", "A crucial part is to creating this connection to the Blob store is the azure-storage library. This will allow us to create a connection to blob, so this library has to be added to the cluster.", "The next part will assume that you have created a secret scope for your blob store in databricks CLI. Here you can store SAS URIs for blob store. This may be particularly useful if you are required to have data segregation, and fencing off access to individual containers in an account.", "Above is one example of connecting to blob store using a Databricks notebook. Take it with a grain of salt, there are other documented ways of connecting with Scala or pyspark and loading the data into a Spark dataframe rather than a pandas dataframe.", "After creating the code block for connection and loading the data into a dataframe. You can now carry out any data manipulation or cleaning before outputting the data into a container.", "Last step of this is sanitizing the active processing container and shipping the new file into a blob container of its own or with other collated data.", "For efficiency when dealing with jobs smaller in terms of processing work (Not quite big data tasks), dynamically running notebooks on a single job cluster. This option is used if for any particular reason that you would choose not to use a job pool or a high concurrency cluster.", "The idea here is you can pass a variable or pipeline parameter to these values. For the simplicity in demonstrating this example I have them hard coded. Where the name dataStructure_*n* defining the name of 4 different notebooks in Databricks.", "The code below from the Databricks Notebook will run Notebooks from a list nbl if it finds an argument passed from Data Factory called exists. A use case for this may be that you have 4 different data transformations to apply to different datasets and prefer to keep them fenced.", "Hopefully you may pickup something useful from this, or maybe have some tips for me. Please feel free to reach out.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "MSc Data Science, Consultant @ EY"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5460ce423df5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5460ce423df5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5460ce423df5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@DrPaulSimpson?source=post_page-----5460ce423df5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@DrPaulSimpson?source=post_page-----5460ce423df5--------------------------------", "anchor_text": "Paul Simpson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8e98217de665&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&user=Paul+Simpson&userId=8e98217de665&source=post_page-8e98217de665----5460ce423df5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5460ce423df5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5460ce423df5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tannerboriack?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Tanner Boriack"}, {"url": "https://unsplash.com/s/photos/data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://pypi.org/project/azure-storage-blob/", "anchor_text": "azure-storage"}, {"url": "https://docs.databricks.com/security/secrets/secret-scopes.html", "anchor_text": "The next part will assume that you have created a secret scope for your blob store in databricks CLI"}, {"url": "https://docs.databricks.com/data/data-sources/azure/azure-storage.html", "anchor_text": "other documented ways of connecting with Scala or pyspark"}, {"url": "https://medium.com/tag/python?source=post_page-----5460ce423df5---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/databricks?source=post_page-----5460ce423df5---------------databricks-----------------", "anchor_text": "Databricks"}, {"url": "https://medium.com/tag/azure-data-factory?source=post_page-----5460ce423df5---------------azure_data_factory-----------------", "anchor_text": "Azure Data Factory"}, {"url": "https://medium.com/tag/blob-storage?source=post_page-----5460ce423df5---------------blob_storage-----------------", "anchor_text": "Blob Storage"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----5460ce423df5---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5460ce423df5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&user=Paul+Simpson&userId=8e98217de665&source=-----5460ce423df5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5460ce423df5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&user=Paul+Simpson&userId=8e98217de665&source=-----5460ce423df5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5460ce423df5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5460ce423df5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5460ce423df5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5460ce423df5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5460ce423df5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5460ce423df5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5460ce423df5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5460ce423df5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5460ce423df5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5460ce423df5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5460ce423df5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5460ce423df5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@DrPaulSimpson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@DrPaulSimpson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Paul Simpson"}, {"url": "https://medium.com/@DrPaulSimpson/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "133 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8e98217de665&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&user=Paul+Simpson&userId=8e98217de665&source=post_page-8e98217de665--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4d382d94a721&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&newsletterV3=8e98217de665&newsletterV3Id=4d382d94a721&user=Paul+Simpson&userId=8e98217de665&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}