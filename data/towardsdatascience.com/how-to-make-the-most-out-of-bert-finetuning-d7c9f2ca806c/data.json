{"url": "https://towardsdatascience.com/how-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c", "time": 1683006058.7990122, "path": "towardsdatascience.com/how-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c/", "webpage": {"metadata": {"title": "How To Make The Most Out Of BERT Finetuning | by Jonas Vetterle | Towards Data Science", "h1": "How To Make The Most Out Of BERT Finetuning", "description": "Finetuning pretrained language models like BERT on downstream tasks has become ubiquitous in NLP research and applied NLP. That\u2019s in part because one can save a lot of time and money by using\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2002.06305", "anchor_text": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1909.03004", "anchor_text": "Show your work: Improved reporting of experimental results", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1603.06560", "anchor_text": "Hyperband: A novel bandit-based approach to hyperparameter optimization", "paragraph_index": 28}], "all_paragraphs": ["Finetuning pretrained language models like BERT on downstream tasks has become ubiquitous in NLP research and applied NLP. That\u2019s in part because one can save a lot of time and money by using pretrained models. They also often serve as strong baseline models which, when finetuned, significantly outperform training models from scratch.", "While finetuning BERT is relatively straightforward in theory, it can be time-intensive and unrewarding in practice due to seemingly random outcomes of different training runs. In fact, even when finetuning a model with the same hyperparameters over and over again, there can be a great degree of variability in final model performance due to randomness in (1) weight intializations and (2) data orders (how data sets are shuffled). This is especially a problem when finetuning BERT on small data sets.", "Recent research\u00b9 explores these often overlooked sources of randomness. The authors offer 2 practical tips you can use to finetune better models given a certain computational budget, thereby making the most out of BERT finetuning:", "Apart from these practical tips, the paper provides two interesting insights:", "The authors finetune BERT multiple times on 4 different GLUE binary classification datesets. Three of them are quite small (MRPC, RTE, and CoLA), and one is relatively big (SST).", "On each data set, they jointly finetune all BERT parameters and train a classification head in what is called an episode. In each episode, the hyperparameters are the same, but the random seeds which control weight initialization and data order are modified. The authors try 25 different random seeds for both randomizations on the small datasets, and 15 on SST. That is, in total they run 2,100 episodes (3 x 25\u00b2 + 15\u00b2).", "The standard machine learning workflow amounts to training a certain number of models on training data, picking the preferred model on a validation set and evaluating its final performance on a test set.", "Given this workflow, training more models naturally leads to higher expected performance of the best model and smaller variance. But training more models is also more time- and resource-intensive. So practitioners face a trade-off between expected performance and resource spent.", "It is really important to know what this trade-off looks like. We can visualize it by plotting the Expected Validation Performance\u00b2 after running a certain number of finetuning episodes:", "As expected, when training a model with some hyperparameters only once, the variance in validation performance is huge. The variance decreases as more models (with the same hyperparameters, but different seeds for weight initialiazation and data order) are trained. Similarly, the expected value of the performance of the best model is increasing in the number of different initializations we try out.", "We see from the above graphs that evaluating models 10 times per epoch (blue lines) tends to lead to higher expected validation performance than evaluating once per epoch or less. The authors therefore conclude that", "\u201cmore frequently evaluating the model on validation data leads to higher expected validation values\u201d", "We\u2019ve just seen than if we finetune BERT only once, with a certain hyperparameter setting, there is agreat amount of variance in validation performance. This is due to random weight initialization (WI) and data order (DO). How much of an impact do these initializations have on performance?", "To answer this question the authors plot the distribution of validation performance across all episodes of the best/worst WI/DO seeds for each dataset:", "From the graphs above we see that the best seeds tend to have more density in high performance regions than the worst seeds. The authors conduct an ANOVA test which provides evidence that the distributions of the best and worst seeds indeed have different means.", "Different random seeds for weight initialization and data order can affect performance a lot. Compared to previously reported scores with the same hyperparameters and experimental setting, the authors\u2019 best finetuned BERT models perform much better. On some tasks, BERT even becomes competitive with more recent models like ALBERT.", "Interestingly, there appear to be globally good initializations. As all 4 tasks involve training a binary classifier, it is possible to check whether good seeds for weight initialization for one task are also good for other tasks. The authors find that this is the case: there are some initializations that perform consistently well across all tasks! This is really exciting and the authors leave this question open for future research.", "The performance difference between best and worst random seeds is significant. Can we identify bad random seeds early on in training and derive practical insights from this? (the answer is yes! read on)", "In practice, resources are often limited \u2014 both in terms of time and money. We therefore want to try and get the best model given the constraints imposed on us. If we could identify episodes which lead to bad final models early on during the training process, we could stop them and spend our resources on more promising episodes. Luckily the authors demonstrate that we can do exactly that.", "The graphs above demonstrate that it is possible to identify bad initializations which will lead to bad models at the end of training. This is especially true for the smaller data sets. For the larger SST dataset this appears to be less obvious from the graph, but in this case there is still a strong correlation between validation performance after 2 epochs and final validation performance.", "The question then becomes how to decide when to stop training a model, and how many models to train. The authors use an algorithm that was inspired by an early stopping criterion for hyperparameter search\u00b3 for this purpose. The algorithm takes the following 3 parameters:", "Running this algorithm takes (tf +p(1\u2212f))s number of steps to complete where s is the number of total epochs (in this case s=3). The authors obtain the best results with f in the region of 20\u201330%. They also run experiments and show the best parameters for different computational budgets. Common trends are:", "The results are summarised in the graph below. It shows the relative error reduction when using the above algorithm for each of the 4 tasks. Error reduction is relative to not using the above algorithm \u2014 that is, just training a certain number of models fully (x-axis) and selecting the best one. As we can see, for any computational budget, the early stopping algorithm leads to a sizeable performance increase.", "Starting many models, stopping the bad ones early, and proceeding with only a few can lead to better performance overall when finetuning BERT on a constrained budget.", "In a resource constrained setting (there is a fixed computational budget) use these 2 tips to make the most out of BERT finetuning:", "If you found this interesting, you might also want to check out the article below, which discusses ways to improve the pretraining of Transformer models like BERT.", "[1]: Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah Smith, Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping (2020).", "[2]: Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, Noah A. Smith, Show your work: Improved reporting of experimental results. In Proc. of EMNLP (2019).", "[3]: Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar, Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research (2018).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer. Be curious, stay optimistic."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd7c9f2ca806c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jonvet.medium.com/?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": ""}, {"url": "https://jonvet.medium.com/?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "Jonas Vetterle"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff5f495e54b8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&user=Jonas+Vetterle&userId=f5f495e54b8b&source=post_page-f5f495e54b8b----d7c9f2ca806c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7c9f2ca806c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7c9f2ca806c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@drewpatrickmiller?utm_source=medium&utm_medium=referral", "anchor_text": "Drew Patrick Miller"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/2002.06305.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/2002.06305.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/2002.06305.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/2002.06305.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/2002.06305.pdf", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978", "anchor_text": "This is how to train better transformer modelsHow to train faster, higher performant transformerstowardsdatascience.com"}, {"url": "https://arxiv.org/abs/2002.06305", "anchor_text": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"url": "https://arxiv.org/abs/1909.03004", "anchor_text": "Show your work: Improved reporting of experimental results"}, {"url": "https://arxiv.org/abs/1603.06560", "anchor_text": "Hyperband: A novel bandit-based approach to hyperparameter optimization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d7c9f2ca806c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d7c9f2ca806c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d7c9f2ca806c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tech?source=post_page-----d7c9f2ca806c---------------tech-----------------", "anchor_text": "Tech"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d7c9f2ca806c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd7c9f2ca806c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&user=Jonas+Vetterle&userId=f5f495e54b8b&source=-----d7c9f2ca806c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd7c9f2ca806c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&user=Jonas+Vetterle&userId=f5f495e54b8b&source=-----d7c9f2ca806c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7c9f2ca806c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd7c9f2ca806c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d7c9f2ca806c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d7c9f2ca806c--------------------------------", "anchor_text": ""}, {"url": "https://jonvet.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jonvet.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jonas Vetterle"}, {"url": "https://jonvet.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "37 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff5f495e54b8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&user=Jonas+Vetterle&userId=f5f495e54b8b&source=post_page-f5f495e54b8b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff5f495e54b8b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-make-the-most-out-of-bert-finetuning-d7c9f2ca806c&user=Jonas+Vetterle&userId=f5f495e54b8b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}