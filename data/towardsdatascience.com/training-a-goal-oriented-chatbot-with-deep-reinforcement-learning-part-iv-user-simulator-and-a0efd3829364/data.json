{"url": "https://towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364", "time": 1682994042.415978, "path": "towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364/", "webpage": {"metadata": {"title": "Training a Goal-Oriented Chatbot with Deep Reinforcement Learning \u2014 Part IV | by Max Brenner | Towards Data Science", "h1": "Training a Goal-Oriented Chatbot with Deep Reinforcement Learning \u2014 Part IV", "description": "Learn how to code the user simulator to a goal-oriented chatbot DRL training system"}, "outgoing_paragraph_urls": [{"url": "https://github.com/MiuLab/TC-Bot", "anchor_text": "TC-Bot", "paragraph_index": 2}, {"url": "https://github.com/maxbren/GO-Bot-DRL", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/user_simulator.py", "anchor_text": "user_simulator.py", "paragraph_index": 2}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/error_model_controller.py", "anchor_text": "error_model_controller.py", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1612.05688.pdf", "anchor_text": "user sim from TC-Bot", "paragraph_index": 3}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_user_goals.txt", "anchor_text": "file of user goals", "paragraph_index": 4}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/dialogue_config.py", "anchor_text": "dialogue_config.py", "paragraph_index": 10}, {"url": "https://gym.openai.com/docs/", "anchor_text": "openai gym environments", "paragraph_index": 12}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-v-running-the-agent-and-63d8cd27d1d", "anchor_text": "final part", "paragraph_index": 36}, {"url": "https://github.com/maxbren/GO-Bot-DRL", "anchor_text": "repo", "paragraph_index": 36}], "all_paragraphs": ["Check out the previous parts to this series that cover the training loop, DQN agent and state tracker, oh my!", "This part explains the all-important user simulator and error model controller.", "This tutorial series is based off of TC-Bot. The code for this series can be found here. The two files focused on in this part are user_simulator.py and error_model_controller.py.", "User simulators are used to simulate an actual user so that an agent can be trained much faster than if real users were forced to sit down and give it multiple hours of training. User simulator research for chatbot domains is a hot research topic. The one covered in this tutorial is a fairly simple deterministic rule-based simulator based off of the user sim from TC-Bot with a few minor changes. This user simulator, like most so far, is an agenda-based system. This means that a user has a goal for that episode and it takes actions in accordance to this goal while keeping track of an internal state that allows it follow the dialogue to take informed actions. Each round an action is crafted in response to an agent action using mainly deterministic rules along with a few stochastic rules to create diversity in responses.", "User goals are sampled from a corpus of real dialogue or hand-crafted (or both). Each goal is made up of inform slots and request slots just like an action but without the intent. There are a couple standard ways to grab the goals from a corpus. Number one: all of the slots (request and inform) from the initial user action in an episode make a single goal. Number two: all slots from all user actions in an episode are combined to make a single goal. In addition to this automatic collection some hand-crafted rules are used to lower the diversity of the user goals so that they are more achievable for an agent. Luckily, TC-Bot comes with the file of user goals we will be using so we don\u2019t need to generate them ourselves from a corpus.", "The inform slots of a user goal simulate constraints of a user in finding a ticket that works for them. The request slots simulate the user gaining information about what tickets are available. The main difference between this system and a real user is that the user might change their mind as they learn more about what tickets are available. This won\u2019t happen here as the goal is not altered during an episode. Think about ways in which you could beef this system up to act more like a real user in this way!", "Final note, a default slot which in this case is \u201cticket\u201d is added to the request slots of every goal. The agent must successfully inform a value to fill the default slot to fulfill the goal and succeed in an episode.", "A few examples, from part I, of user goals:", "The internal state (not the same as a state from the state tracker) of a user sim keeps track of both the goal slots and history of the current the conversation. It is used to craft the user action each step. Specifically the state is a list of 4 dictionaries of slots and an intent:", "The actions the user can take are more diverse and sometimes more complex than the possible agent actions. A user action can have multiple request slots or multiple inform slots. It can also actually contain both request and inform slots on certain occasions such as the initial action.", "Here are the dialogue config constants for the user sim in dialogue_config.py:", "The user sim reset is important as it selects a new user goal, resets the state and returns an initial action.", "The user responds to an agent action in the step method which takes as input an agent action. Step takes this action and the user sim\u2019s internal state and returns a crafted response (action), scalar reward, done bool and success bool. This is similar to the step function in say openai gym environments and serves the same purpose!", "By the way, these two uses of the intent \u2018done\u2019 are the only times that a user sim action takes that as an intent, i.e. \u2018done\u2019 symbolizes the closing of the conversation", "Note: success can be NO_OUTCOME if the episode is not done, FAIL if the episode is done and a failure or SUCCESS if the episode is done and a success", "This reward function helps the agent learn to succeed by giving it a big reward for succeeding. And allows it to learn to avoid failure by giving it a large penalty for failing but not as large as the reward for success. I believe this helps the agent not be too afraid to take risks for the large bonus of succeeding, otherwise it might prematurely end the episode to lessen the negative reward. In the end reward shaping like this tends to be a balancing act. Try experimenting with this reward function and see if you can get better results.", "Some of the rules for responding are complex or might seem arbitrary. But remember that many of the rules for crafting a response are complex so the user sim can be more human-like which will help the agent deal with real users. However, these are by no-means the best rules.", "4 main cases used to respond to a request:", "Case 1) If agent requests for something that is in the goal inform slots and it hasn\u2019t been informed, then inform it from the goal itself", "Case 2) If the agent requests for something that is in the goal request slots and it has already been informed, then inform it from the history", "Case 3) If the agent requests for something that is in the goal request slots and it hasn\u2019t been informed, then request that same slot with a random inform as well", "Case 4) Otherwise the user sim does not care about the slot being requested; inform the special value of \u2018anything\u2019 as the value of the requested slot", "Case 1) If the agent informs something that is in goal informs and the value it informed does not match, then inform the correct value", "Case 2) Otherwise pick some slot to request or inform", "2.a) If anything in state requests then request it", "2.b) Else if something to say in rest slots, pick something", "2.c) Otherwise respond with \u2018thanks\u2019 which really means \u201cnothing to say\u201d; In fact, this indicates to the agent that its rest slots are empty which is a requirement for success discussed below", "Before moving on to response to match found and response to done, it\u2019s important to understand the success constraints for an episode. Remember that the goal inform slots represent the constraints that the found match MUST include. Here is what an agent must do to succeed:", "These success constraints require a matching ticket as that is the goal of the task and empty rest slots showing the agent has informed all goal request slots. This helps the agent to learn to let the user ask questions (request) before committing a match.", "If successful match then reply with intent \u2018thanks\u2019 and any request slots that are still in state requests. Otherwise reply with intent \u2018reject\u2019 and no inform or request slots. \u2018thanks\u2019 indicates to the agent that the ticket worked. \u2018reject\u2019 indicates that the ticket did not work.", "Constraint 1) self.constraint_check must be set to SUCCESS indicating a match was found that worked", "Constraint 2) Rest slots must be empty", "Note: as shown in the step method, the reply has an intent of \u2018done\u2019 and no slots", "After the the user action is received from step it is sent to the error model controller (EMC) to infuse it with error. In TC-Bot it was found that doing this actually helped the agent deal with real-life natural language component errors or from the user making a mistake in their reply. The EMC can add error to both the inform slots and intent of a user action.", "Types of slot level error for each inform slot in the action:", "In summary, the user simulator we have created here uses simple rules to create human-like responses for the purpose of training the agent to deal with real-life users. The error model controller\u2019s purpose is to add error to the intent and/or inform slots of the user sim action which increases the quality of the agent in real-life testing.", "In the final part we will cover how to actually run the code we have created (from the repo) as well as where to go from here. There are a lot improvements to be made and much research to be done!", "See you in the final part!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interested in all things machine learning, procedural and generative"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa0efd3829364&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a0efd3829364--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a0efd3829364--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@maxbrenner110?source=post_page-----a0efd3829364--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxbrenner110?source=post_page-----a0efd3829364--------------------------------", "anchor_text": "Max Brenner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe83c3988e008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&user=Max+Brenner&userId=e83c3988e008&source=post_page-e83c3988e008----a0efd3829364---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa0efd3829364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa0efd3829364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.themattrix.com/matrix-moving-high-resolution-wallpaper-xjgs-hd-matrix-wallpaper-moving-animated-android-iphone-windows-7-gif-5-4-download/", "anchor_text": ""}, {"url": "http://www.themattrix.com/matrix-moving-high-resolution-wallpaper-xjgs-hd-matrix-wallpaper-moving-animated-android-iphone-windows-7-gif-5-4-download/", "anchor_text": "http://www.themattrix.com/matrix-moving-high-resolution-wallpaper-xjgs-hd-matrix-wallpaper-moving-animated-android-iphone-windows-7-gif-5-4-download/"}, {"url": "https://github.com/MiuLab/TC-Bot", "anchor_text": "TC-Bot"}, {"url": "https://github.com/maxbren/GO-Bot-DRL", "anchor_text": "here"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/user_simulator.py", "anchor_text": "user_simulator.py"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/error_model_controller.py", "anchor_text": "error_model_controller.py"}, {"url": "https://arxiv.org/pdf/1612.05688.pdf", "anchor_text": "user sim from TC-Bot"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/data/movie_user_goals.txt", "anchor_text": "file of user goals"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/dialogue_config.py", "anchor_text": "dialogue_config.py"}, {"url": "https://gym.openai.com/docs/", "anchor_text": "openai gym environments"}, {"url": "https://github.com/maxbren/GO-Bot-DRL/blob/master/utils.py", "anchor_text": "utils.py:"}, {"url": "https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-v-running-the-agent-and-63d8cd27d1d", "anchor_text": "final part"}, {"url": "https://github.com/maxbren/GO-Bot-DRL", "anchor_text": "repo"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a0efd3829364---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a0efd3829364---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/chatbots?source=post_page-----a0efd3829364---------------chatbots-----------------", "anchor_text": "Chatbots"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a0efd3829364---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa0efd3829364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&user=Max+Brenner&userId=e83c3988e008&source=-----a0efd3829364---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa0efd3829364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&user=Max+Brenner&userId=e83c3988e008&source=-----a0efd3829364---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa0efd3829364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a0efd3829364--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa0efd3829364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a0efd3829364---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a0efd3829364--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a0efd3829364--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a0efd3829364--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a0efd3829364--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a0efd3829364--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a0efd3829364--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a0efd3829364--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a0efd3829364--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxbrenner110?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxbrenner110?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Brenner"}, {"url": "https://medium.com/@maxbrenner110/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "238 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe83c3988e008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&user=Max+Brenner&userId=e83c3988e008&source=post_page-e83c3988e008--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffb2cbe1972e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364&newsletterV3=e83c3988e008&newsletterV3Id=fb2cbe1972e0&user=Max+Brenner&userId=e83c3988e008&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}