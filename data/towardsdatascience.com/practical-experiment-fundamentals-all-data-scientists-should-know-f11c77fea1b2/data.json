{"url": "https://towardsdatascience.com/practical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2", "time": 1683000302.539285, "path": "towardsdatascience.com/practical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2/", "webpage": {"metadata": {"title": "A How-to for Non-Parametric Power Analyses, p-values, Confidence Intervals, and Checking for Bias | by Dan VanLunen (DVL) | Towards Data Science", "h1": "A How-to for Non-Parametric Power Analyses, p-values, Confidence Intervals, and Checking for Bias", "description": "Randomized experiments are the gold standard for causal inference: if you want to get an unbiased (the average value of your estimation method is the true value) estimate of an effect of a treatment\u2026"}, "outgoing_paragraph_urls": [{"url": "https://r4ds.had.co.nz/", "anchor_text": "R for Data Science", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1607.00698", "anchor_text": "\u201cstress randomization-based inference as opposed to sampling-based inference.\u201d", "paragraph_index": 59}], "all_paragraphs": ["R for Data Science is a free, amazing read to learn how to use the tidyverse code used in this post.", "Randomized experiments are the gold standard for causal inference: if you want to get an unbiased (the average value of your estimation method is the true value) estimate of an effect of a treatment, a randomized experiment is the best way to do so.", "But experiments are expensive. So if we are going to make the investment to do one, we need to know we\u2019ll be able to collect enough data in the experiment to have a high chance of measuring an effect if it exists. A power analysis will tell us how much data we need.", "We also want to know that the effect we measure isn\u2019t just the result of chance. We declare our effect statistically significant only if the chance that we would measure an effect as extreme as the one we measure in our experiment if the treatment has no effect (the p-value) is sufficiently low.", "Additionally, we would like to be able to give some sense of how precisely the effect is measured by providing a confidence interval \u2014 a range of values of the effect, which if calculated the same way for many experiments repeated the same way, would include the true effect a large percent of the time (e.g. 95%).", "Finally, we\u2019d like to know if our experimental design is flawed such that we don\u2019t have a way to measure an effect in an unbiased way.", "This post will enable you to do a power analysis, calculate p-values, get confidence intervals, and check for bias in your design without making any assumptions (non-parametrically).", "Let\u2019s say I\u2019m a dog football marketer. In previous advertisements, I\u2019ve used the picture on the left below to showcase how fun dog footballs are. I hypothesize that if I add a funny note like \u201cTeddy Approved\u201d, a larger portion of people (p) who see the ad will purchase a football (p is called the \u201cconversion rate\u201d).", "To test my hypothesis, every time a person is about to see one of my ads, I\u2019ll randomly show them one of the two images: a control image (with no label) or a treatment image with \u201cTeddy Approved\u201d. Then, I\u2019ll compare the conversion rates within the groups (p_{control} vs. p_{teddy_approved}) to see if the label improves conversion (conversion = # customers ordered football / # customers saw the ad).", "The null hypothesis is the assumption that there is no effect: that adding \u201cTeddy Approved\u201d to the ad will not increase conversion:", "We test this against our alternative hypothesis that the label increases conversion.", "We do this by calculating the chance that we would see our experimental test statistic (the difference in conversion rates among customers that saw the label and those that didn\u2019t \\hat{p}_{teddy_approved} - \\hat{p}_{control} ) given the null hypothesis is true (i.e., a p-value). If that chance is low enough (e.g., <5%), we reject H_0 and can say that the label significantly (in the statistical sense) increases conversion. In our power analysis, we\u2019ll determine how large the true difference p_{teddy_approved} \u2014 p_{control} will need to be or how much data we will need to collect to have a high chance (e.g. >80%) of observing a test statistic with a significant p-value (i.e., a high power).", "The key here is we need to be able to calculate the probability of seeing different values of our test statistic under the null hypothesis: we need to have a sense of what the distribution of the difference in our sample conversion rates \\hat{p}_{teddy_approved} - \\hat{p}_{control} looks like given the true difference in conversion rate is 0: p_{teddy_approved} - p_{control} = 0.", "We could use some parametric assumptions. For example, we could assume normality and perform a t-test. However, in many settings, data is highly skewed and this assumption won\u2019t hold. It would be better if we could simulate the difference in conversion rates non-parametrically (without any assumptions), and trace out this distribution ourselves. To do that, we\u2019ll need some data.", "Let\u2019s assume we have historical information where the label was not applied: we have a data set of customers that have seen the control group image and whether or not they purchased a football. If the null hypothesis is true, the data generating process for this historical data\u2019s conversion rates should be similar to the process during the experiment because adding the label has no impact on conversion.* That is, we could consider this historical data set to be similar to a realization of a hypothetical data set collected during the experiment under the null hypothesis.", "*note we are assuming time isn\u2019t impacting the variability in the conversion rate. If we aren\u2019t interested in using the simulation to do a power analysis (e.g., we just want to calculate confidence intervals/p-values or confirm our design isn\u2019t biased), we can apply randomization inference on the true experimental data to remove this assumption.", "Let\u2019s create a toy historical data set assuming conversion is independent between customers and that the true probability of purchase after seeing the control ad, p_{control} is 0.01.", "With our historical data set, there are 2 key ways to simulate example differences in observed conversion rates between groups \\hat{p}{teddy_approved} \u2014 \\hat{p}{control} under the null hypothesis:", "We\u2019ll focus on randomization inference, but bootstrapping is also described at the end of this post.", "Randomization Inference is my preferred method for identifying variability in your data. I first came across it in Field Experiments by Gerber and Green. This book is a rigorous, concise, and well-written introduction to notation for causal inference and experimental methods. I highly recommend it.", "The idea behind randomization inference is that uncertainty arises naturally from the random assignment to treatment groups. It\u2019s kind of like answering the question: what would the effect size have been if we just assigned our treatment groups differently?", "The procedure to simulate your effect is as follows", "We can make this a function:", "In our example experiment, every customer who sees an ad will randomly see the picture without the label or the picture with the label with 50% probability. We can make an assignment function that follows this methodology:", "The effect we are measuring is the difference in conversion. So let\u2019s make a function that gets the effect size: it calculates the mean conversion in the treatment and control groups and takes the difference.", "Now, let\u2019s use randomization inference to visualize the distribution of our test statistic (the difference in conversion rate) under the null. The lines at the bottom of the chart show the effect size from each randomization. The height of the curve represents how many simulated effect sizes were found in that area of the x-axis.", "Now that we know how to get simulated effect sizes, we are ready to perform a power analysis. The idea behind a power analysis is that we want to know how much data we need to have a high chance of getting a statistically significant result. We want to do this before the experiment begins so we don\u2019t waste time and money running an experiment that can\u2019t measure anything.", "Let\u2019s say we\u2019ll declare the change in conversion significant if there is a 5% or less chance of seeing that change in conversion or greater under the null hypothesis. We traced out the effect size distribution under the null in the previous section, so it\u2019s easy to find this critical value point (it\u2019s just the 95th percentile of our simulated effect sizes).", "Let\u2019s say we want to have at least an 80% chance of getting a significant result. The trick here is to assume that the true effect size is constant, such that we can just shift the distribution of effect sizes under the null by the true effect size. We shift the distribution until 80% of the area under it is beyond the critical region. The amount we need to shift is called the minimum measurable effect (\u201cMME\u201d) because it is the smallest true difference in conversion that we would be able to get a statistically significant result with at least 80% probability. This minimum measurable effect can be calculated by taking the difference between the critical value and the 20th percentile of the distribution of effect sizes simulated under the null.", "A minimum measurable difference in the conversion rate of 0.019 does not seem plausible given our conversion rate started at 0.01. However, all of this was assuming we would collect 1000 customers during the course of our experiment (just like we had in the historical data). We can get more power if we increase our sample size though. Let's see how the minimum measurable effect changes if we create a historical dataset that is 100x as large.", "With a lot more data the minimum measurable effect is reduced to 0.0016, which is still big, but at least a possible effect size to measure.", "The increased precision is more noticeable if we plot the two simulated distributions together:", "p-values are easy to calculate using the simulated effect sizes. We want to know under the null, what is the chance we would have seen an effect size at least as extreme as the effect size we measured. We can find this by just finding what percentile the measured effect size is in our distribution of simulated effect sizes under the null. We could continue to use the effect sizes we simulated using the historical data, but we can apply randomization inference to the actual experimental data to get a better sense of the distribution of effect sizes under the null during the actual experiment.", "Let\u2019s assume the true effect size is 0.0005: that the label will increase conversion by 0.0005. Then, we\u2019ll make a dataset that represents the result of our experiment.", "We found a 0.0015 difference in conversion in our experiment. Now to get the p-value of our effect size, we'll", "The p-value is 0.01. We have found that the label significantly increases conversion at the 95% level. The p-value is the area under the distribution to the right of the measured effect:", "Confidence intervals with a simulated distribution of effect sizes under the null are easy so long as your experimental design is unbiased (see next section). Just shift the distribution so that it is centered at the experimentally measured effect size. Then, the bounds of a 95% confidence interval are the 2.5th and 97.5th percentiles of that shifted distribution.", "This confidence interval contains the true effect: 0.0005.", "In our example, we have used a very clean experimental design: we compared apples to apples groups (because customers were randomly assigned to them) in the same period and calculated a difference in means. In this situation, the measured difference in conversion will be an unbiased estimate of the true difference in conversion caused by the added label: though the treatment effect we measure is random, on average, it will be the true treatment effect:", "We pass this gut check given the mean is so close to 0. Even though this doesn\u2019t confirm the design is unbiased, it gives us more confidence that it is.", "However, many experiments do not have such a clean design and the measured effect size can be prone to bias. In such a case, you should do all you can to correct the design so that it is unbiased. However, there may be a situation where you run an experiment and cannot correct the bias post-experiment. In such a case, you may still be able to use the same p-value methodology so long as the bias is applied uniformly: randomization inference still indicates the actual assignments measured effect size is in the top 95% of measured effect sizes under other hypothetical assignments even if all the measured effects are biased. However, you\u2019ll need to adjust your reported effect size and thus confidence interval. One quick and dirty way to do so is to just report the measured effect size minus the average of the simulated effect sizes under the null distribution. Then, make a confidence interval the same way as above, but centered at that adjusted effect size instead.", "Let\u2019s do an example. Let\u2019s say that we can only show 1 type of ad to every customer in a geography (e.g., a certain metro-area). Now we can\u2019t randomly assign treatment by customer. We could randomly assign half of the geographies to the treatment (label added) condition and the other half to the control (no label) condition. Then, take the difference in conversion between the groups, which would bring us back to the clean design above. However, this could have less precision given the high level clustered treatment.", "To add more precision, someone proposes a design where we look at geographies in a pre-period as well to get more \u201ccontrol\u201d data. Then, we will take the difference between the average conversion across geographies in treatment in the experimental period and the average conversion across geographies in the pre-period and those randomly assigned to control in the experimental period. To simplify things, let\u2019s assume all geography-periods have the same number of customers (so we can take averages easily).", "This design is flawed because differences between the experimental period and the pre-period will accidentally be attributed to the treatment. For example, let\u2019s assume the conversion rate is naturally going to double from the pre-period to the experimental period without applying treatment and see what happens.", "We\u2019ll have a data set with two rows per geography (one row per period). Let\u2019s say the true effect size is 0.002.", "Then, we observe an experimental result like the following.", "We get a measured difference in conversion that is more than 10x the true effect!", "Now we can perform randomization inference to simulate the effect size under the null.", "The average under the null should be zero if we have an unbiased design. Let\u2019s do our gut check of getting the average of our simulated values.", "We failed the gut check because the mean is not close to 0. In this situation, we would want to see if there were a better, unbiased way to measure the effect (e.g., add period fixed effects or do a difference-in-differences design), but if we couldn\u2019t find a better design we would at least want to adjust the effects we report.", "For example, instead of reporting 0.024183 as the effect size, we could subtract out the estimated bias (0.023929: the average of simulated effects under the null) and report 0.024183 - 0.023929 = 0.000254 as the effect size. Note that this doesn't necessarily make your adjusted estimate unbiased, but it is a more honest result to report than the one you know to be biased.", "We would also want to adjust our confidence interval by centering our simulated distribution of effect sizes under the null at this adjusted effect size (instead of the measured effect size) and getting the 2.5% and 97.5% percentiles.", "In this post we\u2019ve gone over non-parametric ways to", "Learn more in my other posts:", "If you want to know about bootstrapping, feel free to read on.", "Bootstrapping is an additional way to simulate the distribution of the effect size.", "Instead of using natural variation arising from the treatment assignment in your data, bootstrapping explores uncertainty over the specific sample of the population you draw. The idea is that the most natural distribution to assume your data follows is the one that was actually observed in your sample. So just sample from your data with replacement to create a simulation.", "With bootstrapping, we need to have an initialized hypothetical treatment so when we sample with replacement, the treatment comes with it.", "Then, we can apply the bootstrapping procedure (note that we aren\u2019t clustering our data so we are just sampling using each unit).", "Titans of causal inference Susan Athey and Guido Ambens \u201cstress randomization-based inference as opposed to sampling-based inference.\u201d That link is a great one to read for more information. I prefer randomization inference to bootstrapping for a couple of reasons.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science Manager \u2014 Causal Inference, Prediction, Data Engineering"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff11c77fea1b2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dvl-data?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dvl-data?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "Dan VanLunen (DVL)"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F534d9d75f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&user=Dan+VanLunen+%28DVL%29&userId=534d9d75f626&source=post_page-534d9d75f626----f11c77fea1b2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff11c77fea1b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff11c77fea1b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://r4ds.had.co.nz/", "anchor_text": "R for Data Science"}, {"url": "https://towardsdatascience.com/an-experiment-assignment-method-all-data-scientists-should-know-e4d57d96b26b?source=friends_link&sk=e4580b55c82b514cadf41e07cf8eccbc", "anchor_text": "An Experiment Assignment Method All Data Scientists Should Know"}, {"url": "https://towardsdatascience.com/get-a-grip-when-to-add-covariates-in-a-linear-regression-f6a5a47930e5?source=email-534d9d75f626-1584307789520-layerCake.autoLayerCakeWriterNotification-------------------------5b264995_1b97_4e64_a82a_770b76c90329&sk=fa2da69f55faa9641bdc8e19bb3ce4af", "anchor_text": "Get a Grip! When to Add Covariates to a Linear Regression"}, {"url": "https://arxiv.org/abs/1607.00698", "anchor_text": "\u201cstress randomization-based inference as opposed to sampling-based inference.\u201d"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f11c77fea1b2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/experimentation?source=post_page-----f11c77fea1b2---------------experimentation-----------------", "anchor_text": "Experimentation"}, {"url": "https://medium.com/tag/causal-inference?source=post_page-----f11c77fea1b2---------------causal_inference-----------------", "anchor_text": "Causal Inference"}, {"url": "https://medium.com/tag/statistics?source=post_page-----f11c77fea1b2---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/r?source=post_page-----f11c77fea1b2---------------r-----------------", "anchor_text": "R"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff11c77fea1b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&user=Dan+VanLunen+%28DVL%29&userId=534d9d75f626&source=-----f11c77fea1b2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff11c77fea1b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&user=Dan+VanLunen+%28DVL%29&userId=534d9d75f626&source=-----f11c77fea1b2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff11c77fea1b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff11c77fea1b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f11c77fea1b2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f11c77fea1b2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dvl-data?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dvl-data?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dan VanLunen (DVL)"}, {"url": "https://medium.com/@dvl-data/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "81 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F534d9d75f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&user=Dan+VanLunen+%28DVL%29&userId=534d9d75f626&source=post_page-534d9d75f626--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8e09e2cd9254&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-experiment-fundamentals-all-data-scientists-should-know-f11c77fea1b2&newsletterV3=534d9d75f626&newsletterV3Id=8e09e2cd9254&user=Dan+VanLunen+%28DVL%29&userId=534d9d75f626&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}