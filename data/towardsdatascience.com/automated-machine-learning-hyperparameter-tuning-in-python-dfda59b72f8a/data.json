{"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "time": 1682993477.188326, "path": "towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a/", "webpage": {"metadata": {"title": "Automated Machine Learning Hyperparameter Tuning in Python | by Will Koehrsen | Towards Data Science", "h1": "Automated Machine Learning Hyperparameter Tuning in Python", "description": "Tuning machine learning hyperparameters is a tedious yet crucial task, as the performance of an algorithm can be highly dependent on the choice of hyperparameters. Manual tuning takes time away from\u2026"}, "outgoing_paragraph_urls": [{"url": "http://ceur-ws.org/Vol-1998/paper_09.pdf", "anchor_text": "performance of an algorithm can be highly dependent", "paragraph_index": 0}, {"url": "https://www.featurelabs.com/blog/secret-to-data-science-success/", "anchor_text": "feature engineering", "paragraph_index": 0}, {"url": "https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime", "anchor_text": "interpreting results", "paragraph_index": 0}, {"url": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "random search", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Bayesian_optimization", "anchor_text": "Bayesian optimization", "paragraph_index": 1}, {"url": "https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf", "anchor_text": "to machine learning hyperparameter tuning", "paragraph_index": 1}, {"url": "http://proceedings.mlr.press/v28/bergstra13.pdf", "anchor_text": "results", "paragraph_index": 1}, {"url": "https://hyperopt.github.io/hyperopt/", "anchor_text": "Hyperopt library", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "earlier article I outlined the concepts", "paragraph_index": 2}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "Jupyter Notebook on GitHub", "paragraph_index": 3}, {"url": "https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf", "anchor_text": "Bayesian optimization", "paragraph_index": 4}, {"url": "https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf", "anchor_text": "Expected Improvement", "paragraph_index": 4}, {"url": "https://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf", "anchor_text": "Bayesian hyperparameter tuning", "paragraph_index": 5}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "Tree Parzen Estimator (TPE)", "paragraph_index": 6}, {"url": "https://github.com/HIPS/Spearmint", "anchor_text": "Spearmint", "paragraph_index": 6}, {"url": "https://automl.github.io/SMAC3/stable/", "anchor_text": "SMAC", "paragraph_index": 6}, {"url": "https://automl.github.io/HPOlib2/stable/", "anchor_text": "interesting work", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0", "anchor_text": "introduction to Hyperopt, see this article", "paragraph_index": 6}, {"url": "https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf", "anchor_text": "powerful abstraction that lets us solve many problems", "paragraph_index": 8}, {"url": "https://www.kaggle.com/uciml/caravan-insurance-challenge", "anchor_text": "Caravan Insurance dataset", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "anchor_text": "Receiver Operating Characteristic Area Under the Curve", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "gradient boosting machine", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Early_stopping", "anchor_text": "early stopping", "paragraph_index": 11}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-API.html", "anchor_text": "LightGBM", "paragraph_index": 11}, {"url": "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d", "anchor_text": "high level article", "paragraph_index": 12}, {"url": "https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf", "anchor_text": "technical paper", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Black_box", "anchor_text": "black box", "paragraph_index": 14}, {"url": "https://machinelearningmastery.com/k-fold-cross-validation/", "anchor_text": "KFold cross validation", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0", "anchor_text": "kernel density estimate plots", "paragraph_index": 28}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin", "anchor_text": "the documentation", "paragraph_index": 31}, {"url": "https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf", "anchor_text": "selecting only a", "paragraph_index": 33}, {"url": "https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf", "anchor_text": "subsample", "paragraph_index": 33}, {"url": "https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf", "anchor_text": "fraction of the training observations to use on each iteration", "paragraph_index": 33}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "Tree Parzen Estimator", "paragraph_index": 37}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin", "anchor_text": "GitHub page says other methods may be coming", "paragraph_index": 38}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "the notebook", "paragraph_index": 41}, {"url": "http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec21.pdf", "anchor_text": "Bayesian optimization", "paragraph_index": 49}, {"url": "http://www.cs.cmu.edu/~rsalakhu/10703/Lecture_Exploration.pdf", "anchor_text": "exploration \u2014 trying new hyperparameter values \u2014 to exploitation", "paragraph_index": 49}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "the notebook", "paragraph_index": 52}, {"url": "https://github.com/hyperopt/hyperopt/issues/267", "anchor_text": "pass in the same trials object and the algorithm will continue searching", "paragraph_index": 64}, {"url": "https://en.wikipedia.org/wiki/Bayes_error_rate", "anchor_text": "Bayes\u2019 Error", "paragraph_index": 66}, {"url": "https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf", "anchor_text": "Bayesian optimization", "paragraph_index": 67}, {"url": "http://ml4aad.org/automl/", "anchor_text": "number of libraries in Python", "paragraph_index": 69}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "hyperparameter tuning", "paragraph_index": 69}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 70}], "all_paragraphs": ["Tuning machine learning hyperparameters is a tedious yet crucial task, as the performance of an algorithm can be highly dependent on the choice of hyperparameters. Manual tuning takes time away from important steps of the machine learning pipeline like feature engineering and interpreting results. Grid and random search are hands-off, but require long run times because they waste time evaluating unpromising areas of the search space. Increasingly, hyperparameter tuning is done by automated methods that aim to find optimal hyperparameters in less time using an informed search with no manual effort necessary beyond the initial set-up.", "Bayesian optimization, a model-based method for finding the minimum of a function, has recently been applied to machine learning hyperparameter tuning, with results suggesting this approach can achieve better performance on the test set while requiring fewer iterations than random search. Moreover, there are now a number of Python libraries that make implementing Bayesian hyperparameter tuning simple for any machine learning model.", "In this article, we will walk through a complete example of Bayesian hyperparameter tuning of a gradient boosting machine using the Hyperopt library. In an earlier article I outlined the concepts behind this method, so here we will stick to the implementation. Like with most machine learning topics, it\u2019s not necessary to understand all the details, but knowing the basic idea can help you use the technique more effectively!", "All the code for this article is available as a Jupyter Notebook on GitHub.", "As a brief primer, Bayesian optimization finds the value that minimizes an objective function by building a surrogate function (probability model) based on past evaluation results of the objective. The surrogate is cheaper to optimize than the objective, so the next input values to evaluate are selected by applying a criterion to the surrogate (often Expected Improvement). Bayesian methods differ from random or grid search in that they use past evaluation results to choose the next values to evaluate. The concept is: limit expensive evaluations of the objective function by choosing the next input values based on those that have done well in the past.", "In the case of hyperparameter optimization, the objective function is the validation error of a machine learning model using a set of hyperparameters. The aim is to find the hyperparameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyperparameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyperparameter choices. Bayesian hyperparameter tuning uses a continually updated probability model to \u201cconcentrate\u201d on promising hyperparameters by reasoning from past results.", "There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There is a lot of interesting work going on in this area, so if you aren\u2019t happy with one library, check out the alternatives! The general structure of a problem (which we will walk through here) translates between the libraries with only minor differences in syntax. For a basic introduction to Hyperopt, see this article.", "There are four parts to a Bayesian Optimization problem:", "With those four pieces, we can optimize (find the minimum) of any function that returns a real value. This is a powerful abstraction that lets us solve many problems in addition to tuning machine learning hyperparameters.", "For this example, we will use the Caravan Insurance dataset where the objective is to predict whether a customer will purchase an insurance policy. This is a supervised classification problem with 5800 training observations and 4000 testing points. The metric we will use to assess performance is the Receiver Operating Characteristic Area Under the Curve (ROC AUC) because this is an imbalanced classification problem. (A higher ROC AUC is better with a score of 1 indicating a perfect model). The dataset is shown below:", "Because Hyperopt requires a value to minimize, we will return 1-ROC AUC from the objective function, thereby driving up the ROC AUC.", "Detailed knowledge of the gradient boosting machine (GBM) is not necessary for this article and here are the basics we need to understand: The GBM is an ensemble boosting method based on using weak learners (almost always decision trees) trained sequentially to form a strong model. There are many hyperparameters in a GBM controlling both the entire ensemble and individual decision trees. One of the most effective methods for choosing the number of trees (called estimators) is early stopping which we will use. LightGBM provides a fast and simple implementation of the GBM in Python.", "For more details on the GBM, here\u2019s a high level article and a technical paper.", "With the necessary background out of the way, let\u2019s go through writing the four parts of a Bayesian optimization problem for hyperparameter tuning.", "The objective function is what we are trying to minimize. It takes in a set of values \u2014 in this case hyperparameters for the GBM \u2014 and outputs a real value to minimize \u2014 the cross validation loss. Hyperopt treats the objective function as a black box because it only considers what goes in and what comes out. The algorithm does not need to know the internals of the objective function in order to find the input values that minimize the loss! At a very high level (in pseudocode), our objective function should be:", "We need to be careful not to use the loss on the testing set because we can only use the testing set a single time, when we evaluate the final model. Instead, we evaluate the hyperparameters on a validation set. Moreover, rather than separating training data into a distinct validation set, we use KFold cross validation, which, in addition to preserving valuable training data, should give us a less biased estimate of error on the testing set.", "The basic structure of the objective function for hyperparameter tuning will be the same across models: the function takes in the hyperparameters and returns the cross-validation error using those hyperparameters. Although this example is specific to the GBM, the structure can be applied to other methods.", "The complete objective function for the Gradient Boosting Machine using 10 fold cross validation with early stopping is shown below.", "The main line is cv_results = lgb.cv(...) . To implement cross-validation with early stopping , we use the LightGBM function cv which takes in the hyperparameters, a training set, a number of folds to use for cross validation, and several other arguments. We set the number of estimators ( num_boost_round) to 10000, but this number won\u2019t actually be reached because we are using early_stopping_rounds to stop the training when validation scores have not improved for 100 estimators. Early stopping is an effective method for choosing the number of estimators rather than setting this as another hyperparameter that needs to be tuned!", "Once the cross validation is complete, we get the best score (ROC AUC), and then, because we want a value to minimize, we take 1-best score. This value is then returned as the loss key in the return dictionary.", "This is objective function is actually a little more complicated than it needs to be because we return a dictionary of values. For the objective function in Hyperopt, we can either return a single value, the loss, or a dict that has at a minimum keys \"loss\" and \"status\" . Returning the hyperparameters will let us inspect the loss resulting from each set of hyperparameters.", "The domain space represents the range of values we want to evaluate for each hyperparameter. Each iteration of the search, the Bayesian optimization algorithm will choose one value for each hyperparameter from the domain space. When we do random or grid search, the domain space is a grid. In Bayesian optimization the idea is the same except this space has probability distributions for each hyperparameter rather than discrete values.", "Specifying the domain is the trickiest part of a Bayesian optimization problem. If we have experience with a machine learning method, we can use it to inform our choices of hyperparameter distributions by placing greater probability where we think the best values are. However, the optimal model settings will vary between datasets and with a high-dimensionality problem (many hyperparameters) it can be difficult to figure out the interaction between hyperparameters. In cases where we aren\u2019t sure about the best values, we can use wide distributions and let the Bayesian algorithm do the reasoning for us.", "First, we should look at all the hyperparameters in a GBM:", "I\u2019m not sure there\u2019s anyone in the world who knows how all of these interact together! Some of these we don\u2019t have to tune (such as objective and random_state ) and we will use early stopping to find the best n_estimators. However, we still have 10 hyperparameters to optimize! When first tuning a model, I usually create a wide domain space centered around the default values and then refine it in subsequent searches.", "As an example, let\u2019s define a simple domain in Hyperopt, a discrete uniform distribution for the number of leaves in each tree in the GBM:", "This is a discrete uniform distribution because the number of leaves must be an integer (discrete) and each value in the domain is equally likely (uniform).", "Another choice of distribution is the log uniform which distributes values evenly on a logarithmic scale. We will use a log uniform (from 0.005 to 0.2) for the learning rate because it varies across several orders of magnitude:", "Because this is a log-uniform distribution, the values are drawn between exp(low) and exp(high). The plot on the left below shows the discrete uniform distribution and the plot on the right is the log uniform. These are kernel density estimate plots so the y-axis is density and not a count!", "Now, let\u2019s define the entire domain:", "Here we use a number of different domain distribution types:", "(There are other distributions as well listed in the documentation.)", "There is one important point to notice when we define the boosting type:", "Here we are using a conditional domain which means the value of one hyperparameter depends on the value of another. For the boosting type \"goss\", the gbm cannot use subsampling (selecting only a subsample fraction of the training observations to use on each iteration). Therefore, the subsample ratio is set to 1.0 (no subsampling) if the boosting type is \"goss\" but is 0.5\u20131.0 otherwise. This is implemented using a nested domain.", "Conditional nesting can be useful when we are using different machine learning models with completely separate parameters. A conditional lets us use different sets of hyperparameters depending on the value of a choice.", "Now that our domain is defined, we can draw one example from it to see what a typical sample looks like. When we sample, because subsample is initially nested, we need to assign it to a top-level key. This is done using the Python dictionary get method with a default value of 1.0.", "(This reassigning of nested keys is necessary because the gradient boosting machine cannot deal with nested hyperparameter dictionaries).", "Although this is the most conceptually difficult part of Bayesian Optimization, creating the optimization algorithm in Hyperopt is a single line. To use the Tree Parzen Estimator the code is:", "That\u2019s all there is to it! Hyperopt only has the TPE option along with random search, although the GitHub page says other methods may be coming. During optimization, the TPE algorithm constructs the probability model from the past results and decides the next set of hyperparameters to evaluate in the objective function by maximizing the expected improvement.", "Keeping track of the results is not strictly necessary as Hyperopt will do this internally for the algorithm. However, if we want to find out what is going on behind the scenes, we can use a Trials object which will store basic training information and also the dictionary returned from the objective function (which includes the loss andparams ). Making a trials object is one line:", "Another option which will allow us to monitor the progress of a long training run is to write a line to a csv file with each search iteration. This also saves all the results to disk in case something catastrophic happens and we lose the trials object (speaking from experience). We can do this using the csv library. Before training we open a new csv file and write the headers:", "and then within the objective function we can add lines to write to the csv on every iteration (the complete objective function is in the notebook):", "Writing to a csv means we can check the progress by opening the file while training (although not in Excel because this will cause an error in Python. Use tail out_file.csv from bash to view the last rows of the file).", "Once we have the four parts in place, optimization is run with fmin :", "Each iteration, the algorithm chooses new hyperparameter values from the surrogate function which is constructed based on the previous results and evaluates these values in the objective function. This continues for MAX_EVALS evaluations of the objective function with the surrogate function continually updated with each new result.", "The best object that is returned from fmin contains the hyperparameters that yielded the lowest loss on the objective function:", "Once we have these hyperparameters, we can use them to train a model on the full training data and then evaluate on the testing data (remember we can only use the test set once, when we evaluate the final model). For the number of estimators, we can use the number of estimators that returned the lowest loss in cross validation with early stopping. Final results are below:", "As a reference, 500 iterations of random search returned a model that scored 0.7232 ROC AUC on the test set and 0.76850 in cross validation. A default model with no optimization scored 0.7143 ROC AUC on the test set.", "There are a few important notes to keep in mind when we look at the results:", "Bayesian optimization is effective, but it will not solve all our tuning problems. As the search progresses, the algorithm switches from exploration \u2014 trying new hyperparameter values \u2014 to exploitation \u2014 using hyperparameter values that resulted in the lowest objective function loss. If the algorithm finds a local minimum of the objective function, it might concentrate on hyperparameter values around the local minimum rather than trying different values located far away in the domain space. Random search does not suffer from this issue because it does not concentrate on any values!", "Another important point is that the benefits of hyperparameter optimization will differ with the dataset. This is a relatively small dataset (~ 6000 training observations) and there is a small payback to tuning the hyperparameters (getting more data would be a better use of time!). With all of those caveats in mind, in this case, with Bayesian optimization we can get:", "Bayesian methods can (although will not always) yield better tuning results than random search. In the next few sections, we will examine the evolution of the Bayesian hyperparameter search and compare to random search to understand how Bayesian Optimization works.", "Graphing the results is an intuitive way to understand what happens during the hyperparameter search. Moreover, it\u2019s helpful to compare Bayesian Optimization to random search so we can see how the methods differ. To see how the plots are made and random search is implemented, see the notebook, but here we will go through the results. (As a note, the exact results will change across iterations, so if you run the notebook, don\u2019t be surprised if you get different images. All of these plots are made with 500 iterations).", "First we can make a kernel density estimate plot of the learning_rate sampled in random search and Bayes Optimization. As a reference, we can also show the sampling distribution. The vertical dashed lines show the best values (according to cross validation) for the learning rate.", "We defined the learning rate as a log-normal between 0.005 and 0.2, and the Bayesian Optimization results look similar to the sampling distribution. This tells us that the distribution we defined looks to be appropriate for the task, although the optimal value is a little higher than where we placed the greatest probability. This could be used to inform the domain for further searches.", "Another hyperparameter is the boosting type, with the bar plots of each type evaluated during random search and Bayes optimization shown below. Since random search does not pay attention to past results, we would expect each boosting type to be used roughly the same number of times.", "According to the Bayesian algorithm, the gdbt boosting type is more promising than dart or goss. Again, this could help inform further searches, either Bayesian methods or grid search. If we wanted to do a more informed grid search, we could use these results to define a smaller grid concentrated around the most promising values of the hyperparameters.", "Since we have them, let\u2019s look at all of the numeric hyperparameters from the reference distribution, random search, and Bayes Optimization. The vertical lines again indicate the best value of the hyperparameter for each search:", "In most cases (except for the subsample_for_bin ) the Bayesian optimization search tends to concentrate (place more probability) near the hyperparameter values that yield the lowest loss in cross validation. This shows the fundamental idea of hyperparameter tuning using Bayesian methods: spend more time evaluating promising hyperparameter values.", "There are also some interesting results here that might help us in the future when it comes time to define a domain space to search over. As just one example, it looks like reg_alpha and reg_lambda should complement one another: if one is high (close to 1.0), the other should be lower. There\u2019s no guarantee this will hold across problems, but by studying the results, we can gain insights that might be applied to future machine learning problems!", "As the optimization progresses, we expect the Bayes method to focus on the more promising values of the hyperparameters: those that yield the lowest error in cross validation. We can plot the values of the hyperparameters versus the iteration to see if there are noticeable trends.", "The black star indicates the optimal value. The colsample_bytree and learning_rate decrease over time which could guide us in future searches.", "Finally, if Bayes Optimization is working, we would expect the average validation score to increase over time (conversely the loss decreases):", "The validation scores from Bayesian hyperparameter optimization increase over time, indicating the method is trying \u201cbetter\u201d hyperparameter values (it should be noted that these are only better according to the validation score). Random search does not show an improvement over the iterations.", "If we are not satisfied with the performance of our model, we can keep searching using Hyperopt from where we left off. We just need to pass in the same trials object and the algorithm will continue searching.", "As the algorithm progresses, it does more exploitation \u2014 picking values that have done well in the past \u2014 and less exploration \u2014 picking new values. Instead of continuing where the search left off, it might therefore be a good idea to start an entirely different search. If the best hyperparameters from the first search really are \u201coptimal\u201d, we would expect subsequent searches to focus on the same values. Given the high dimensionality of the problem, and the complex interactions between hyperparameters, it\u2019s unlikely that another search would result in a similar set of hyperparameters.", "After another 500 iterations of training, the final model scores 0.72736 ROC AUCon the test set. (We really should not have evaluated the first model on the test set and instead relied only on validation scores. The test set should ideally be used only once to get a measure of algorithm performance when deployed on new data). Again, this problem may have diminishing returns to further hyperparameter optimization because of the small size of the dataset and there will eventually be a plateau in validation error (there is an inherent limit to the performance of any model on a dataset because of hidden variables that are not measured and noisy data, referred to as Bayes\u2019 Error).", "Automated hyperparameter tuning of machine learning models can be accomplished using Bayesian optimization. In contrast to random search, Bayesian optimization chooses the next hyperparameters in an informed method to spend more time evaluating promising values. The end outcome can be fewer evaluations of the objective function and better generalization performance on the test set compared to random or grid search.", "In this article, we walked step-by-step through Bayesian hyperparameter optimization in Python using Hyperopt. We were able to improve the test set performance of a Gradient Boosting Machine beyond both the baseline and random search although we need to be cautious of overfitting to the training data. Furthermore, we saw how random search differs from Bayesian Optimization by examining the resulting graphs which showed that the Bayesian method placed greater probability on the hyperparameter values that resulted in lower cross validation loss.", "Using the four parts of an optimization problem, we can use Hyperopt to solve a wide variety of problems. The basic parts of Bayesian optimization also apply to a number of libraries in Python that implement different algorithms. Making the switch from manual to random or grid search is one small step, but to take your machine learning to the next level requires some automated form of hyperparameter tuning. Bayesian Optimization is one approach that is both easy to use in Python and can return better results than random search. Hopefully you now feel confident to start using this powerful method for your own machine learning problems!", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdfda59b72f8a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----dfda59b72f8a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfda59b72f8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfda59b72f8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://ceur-ws.org/Vol-1998/paper_09.pdf", "anchor_text": "performance of an algorithm can be highly dependent"}, {"url": "https://www.featurelabs.com/blog/secret-to-data-science-success/", "anchor_text": "feature engineering"}, {"url": "https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime", "anchor_text": "interpreting results"}, {"url": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "random search"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_optimization", "anchor_text": "Bayesian optimization"}, {"url": "https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf", "anchor_text": "to machine learning hyperparameter tuning"}, {"url": "http://proceedings.mlr.press/v28/bergstra13.pdf", "anchor_text": "results"}, {"url": "https://hyperopt.github.io/hyperopt/", "anchor_text": "Hyperopt library"}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "earlier article I outlined the concepts"}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "Jupyter Notebook on GitHub"}, {"url": "https://medium.com/p/dfda59b72f8a#8811", "anchor_text": "Bayesian Optimization Methods"}, {"url": "https://medium.com/p/dfda59b72f8a#4d6d", "anchor_text": "Four Parts of Optimization Problem"}, {"url": "https://medium.com/p/dfda59b72f8a#05d4", "anchor_text": "Objective Function"}, {"url": "https://medium.com/p/dfda59b72f8a#951b", "anchor_text": "Domain Space"}, {"url": "https://medium.com/p/dfda59b72f8a#8fcc", "anchor_text": "Optimization Algorithm"}, {"url": "https://medium.com/p/dfda59b72f8a#6a85", "anchor_text": "Result History"}, {"url": "https://medium.com/p/dfda59b72f8a#7aac", "anchor_text": "Optimization"}, {"url": "https://medium.com/p/dfda59b72f8a#80b4", "anchor_text": "Results"}, {"url": "https://medium.com/p/dfda59b72f8a#9169", "anchor_text": "Visualizing Search Results"}, {"url": "https://medium.com/p/dfda59b72f8a#1cbc", "anchor_text": "Evolution of Search"}, {"url": "https://medium.com/p/dfda59b72f8a#ba35", "anchor_text": "Continue Searching"}, {"url": "https://medium.com/p/dfda59b72f8a#f323", "anchor_text": "Conclusions"}, {"url": "https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf", "anchor_text": "Bayesian optimization"}, {"url": "https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf", "anchor_text": "Expected Improvement"}, {"url": "https://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf", "anchor_text": "Bayesian hyperparameter tuning"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "Tree Parzen Estimator (TPE)"}, {"url": "https://github.com/HIPS/Spearmint", "anchor_text": "Spearmint"}, {"url": "https://automl.github.io/SMAC3/stable/", "anchor_text": "SMAC"}, {"url": "https://automl.github.io/HPOlib2/stable/", "anchor_text": "interesting work"}, {"url": "https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0", "anchor_text": "introduction to Hyperopt, see this article"}, {"url": "https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf", "anchor_text": "powerful abstraction that lets us solve many problems"}, {"url": "https://www.kaggle.com/uciml/caravan-insurance-challenge", "anchor_text": "Caravan Insurance dataset"}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "anchor_text": "Receiver Operating Characteristic Area Under the Curve"}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "gradient boosting machine"}, {"url": "https://en.wikipedia.org/wiki/Early_stopping", "anchor_text": "early stopping"}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-API.html", "anchor_text": "LightGBM"}, {"url": "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d", "anchor_text": "high level article"}, {"url": "https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf", "anchor_text": "technical paper"}, {"url": "https://en.wikipedia.org/wiki/Black_box", "anchor_text": "black box"}, {"url": "https://machinelearningmastery.com/k-fold-cross-validation/", "anchor_text": "KFold cross validation"}, {"url": "https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0", "anchor_text": "kernel density estimate plots"}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin", "anchor_text": "the documentation"}, {"url": "https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf", "anchor_text": "selecting only a"}, {"url": "https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf", "anchor_text": "subsample"}, {"url": "https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf", "anchor_text": "fraction of the training observations to use on each iteration"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "Tree Parzen Estimator"}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin", "anchor_text": "GitHub page says other methods may be coming"}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "the notebook"}, {"url": "http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec21.pdf", "anchor_text": "Bayesian optimization"}, {"url": "http://www.cs.cmu.edu/~rsalakhu/10703/Lecture_Exploration.pdf", "anchor_text": "exploration \u2014 trying new hyperparameter values \u2014 to exploitation"}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "the notebook"}, {"url": "https://github.com/hyperopt/hyperopt/issues/267", "anchor_text": "pass in the same trials object and the algorithm will continue searching"}, {"url": "https://en.wikipedia.org/wiki/Bayes_error_rate", "anchor_text": "Bayes\u2019 Error"}, {"url": "https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf", "anchor_text": "Bayesian optimization"}, {"url": "http://ml4aad.org/automl/", "anchor_text": "number of libraries in Python"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "hyperparameter tuning"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dfda59b72f8a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----dfda59b72f8a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----dfda59b72f8a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/education?source=post_page-----dfda59b72f8a---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/automation?source=post_page-----dfda59b72f8a---------------automation-----------------", "anchor_text": "Automation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdfda59b72f8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----dfda59b72f8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdfda59b72f8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----dfda59b72f8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdfda59b72f8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdfda59b72f8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dfda59b72f8a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dfda59b72f8a--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}