{"url": "https://towardsdatascience.com/cord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276", "time": 1683014340.314829, "path": "towardsdatascience.com/cord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276/", "webpage": {"metadata": {"title": "CORD Crusher: Slicing the COVID-19 Data into Summaries | by Rishi Patel, PhD | Towards Data Science", "h1": "CORD Crusher: Slicing the COVID-19 Data into Summaries", "description": "During the early period of the COVID-19 outbreak in December, my wife and I were in a cozy cocoon awaiting the birth of our son. After his birth, it was clear that the outbreak of COVID-19 was taking\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.whitehouse.gov/briefings-statements/call-action-tech-community-new-machine-readable-covid-19-dataset/", "anchor_text": "White House Office of Science and Technology Policy", "paragraph_index": 1}, {"url": "https://medium.com/analytics-vidhya/6-steps-to-build-roberta-a-robustly-optimised-bert-pretraining-approach-e508ebe78b96", "anchor_text": "RoBERTa", "paragraph_index": 3}, {"url": "https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents", "anchor_text": "RAKE", "paragraph_index": 8}, {"url": "https://spacy.io/", "anchor_text": "SpaCy", "paragraph_index": 16}, {"url": "https://allenai.github.io/scispacy/", "anchor_text": "SciSpaCy", "paragraph_index": 16}, {"url": "https://spacy.io/usage/rule-based-matching#phrasematcher", "anchor_text": "are phrase matching", "paragraph_index": 17}, {"url": "https://spacy.io/usage/rule-based-matching#matcher", "anchor_text": "rule based matching", "paragraph_index": 17}, {"url": "https://course.spacy.io/en", "anchor_text": "this tutorial", "paragraph_index": 18}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "sklearn feature extraction tools", "paragraph_index": 20}, {"url": "https://medium.com/nlpgurukool/fuzzy-matching-1baac719aa25", "anchor_text": "fuzzy string matching", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/fuzzywuzzy-how-to-measure-string-distance-on-python-4e8852d7c18f", "anchor_text": "Levenshtein Distance", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558", "anchor_text": "term-frequency inverse document frequency", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Nonnegative_matrix", "anchor_text": "it can be approximately factorized by two non-negative matrices", "paragraph_index": 26}, {"url": "https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c", "anchor_text": "K-Means clustering algorithm", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3", "anchor_text": "Latent-Semantic analysis,", "paragraph_index": 27}, {"url": "https://medium.com/@cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a", "anchor_text": "Kulback-Leibler cost function", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html", "anchor_text": "sklearn", "paragraph_index": 27}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "t-SNE", "paragraph_index": 28}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "PyTextRank", "paragraph_index": 34}, {"url": "https://doi.org/10.1101/2020.02.10.942185", "anchor_text": "Structural modeling of 2019-novel coronavirus (nCoV) spike protein reveals a proteolytically-sensitive activation loop as a distinguishing feature compared to SARS-CoV and related SARS-like coronaviruses", "paragraph_index": 41}, {"url": "https://doi.org/10.1038/s41467-020-17687-3", "anchor_text": "Origin and cross-species transmission of bat coronaviruses in China", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/2004.10706", "anchor_text": "CORD-19: The Covid-19 Open Research Dataset", "paragraph_index": 46}, {"url": "https://www.sciencedirect.com/science/article/pii/S0022283620302874?via%3Dihub", "anchor_text": "Structural modeling of 2019-novel coronavirus (nCoV) spike protein reveals a proteolytically-sensitive activation loop as a distinguishing feature compared to SARS-CoV and related SARS-like coronaviruses", "paragraph_index": 46}, {"url": "https://doi.org/10.1038/s41467-020-17687-3", "anchor_text": "Origin and cross-species transmission of bat coronaviruses in China", "paragraph_index": 46}], "all_paragraphs": ["During the early period of the COVID-19 outbreak in December, my wife and I were in a cozy cocoon awaiting the birth of our son. After his birth, it was clear that the outbreak of COVID-19 was taking hold of the world. I got to thinking more about my own birth at the end of 1985, a few months before the Chernobyl disaster in April of 1986. It seems like in an ever evolving world, new life and new challenges will always go hand in hand. So whenever my son slept (not as much as I would have liked), I quietly picked up my computer and began to wade, then swim, and finally dive into natural language processing (NLP) in python.", "In March of 2020, the White House Office of Science and Technology Policy released the CORD 19 dataset and a call to action:", "\u201ca call to action to the Nation\u2019s artificial intelligence experts to develop new text and data mining techniques that can help the science community answer high-priority scientific questions related to COVID-19\"", "CORD 19 was the perfect opportunity to develop code to find relevant and timely information on the new coronavirus. It was overwhelming the number of NLP packages and techniques available (e.g. RoBERTa, which is also the name of my mother-in-law who heralded the news of the new virus to us), and the list is still expanding. In this article, I will demonstrate how I put some of these NLP packages together to build an extractive summary code, called CORD crusher. I will zoom in on the components of my NLP code, explain their function, and show how they fit together. The five main steps were:", "2. Extract keywords and group papers according to a broad subject", "3. Build topics from keywords for each subject", "4. Refine keywords into more specific topic phrases", "5. Search CORD 19 text and rank by similarity", "I found RAKE (Rapid Automatic Keyword Extraction) to be both a rapid algorithm and an intuitive one. Keywords like \u201cnovel coronavirus\u201d will contain multiple words and each document may use different terminology: \u201cnovel coronavirus\u201d, \u201cCOVID 19\u201d, \u201c2019-nCoV\u201d. RAKE only considers keywords in a single document as opposed to all of the CORD 19 publications. This makes it useful to find keywords based on the terminology of a single publication.", "RAKE keyword scoring is based on a graph of word frequency and co-occurrence with other words. Intuitively this can be understood as a word correlation matrix, where the diagonal is the word frequency and the other entries are the probability of one word to adjoin another. This simple approach makes it less computationally expensive, and is key to making the algorithm fast (runs in less than a millisecond for 120 words).", "Since keywords strongly represent the context of a document, I expect the title and abstract of a publication to be the prime location to look for keywords. These parts of a publication are written to give the reader a synopsis of everything written.", "In the above example, the full text of 180 words is boiled down to 40 words where the keywords contain just enough information to identify that the article is about COVID-19 with the keyword \u201cnovel coronavirus\u201d, and is about the potential intermediate hosts of pangolins in a \u201cphylogenetic analyses\u201d. The algorithm also successfully pairs \u201cMalayan\u201d and \u201c pangolins\u201d. The 40 words do not summarize the abstract, but instead flag the most keywords that can be used to label the subject of the publication and group it with others that mention the same words.", "The above example also shows that many key words are still too general e.g. \u201crecent studies\u201d or \u201claboratory confirmed cases\u201d, but these will be handled with term frequency inverse document frequency weights (TFIDF) at a later stage to find rare keywords that are more specific.", "As a physicist, the dimension of time is always an important feature to gauge the evolution of a system. Also dividing the metadata csv file for CORD-19 into time ranges is a useful technique to manage the memory and CPU usage of the code. Choosing the right time ranges can also roughly group publications based on the features seen in the plot below.", "CORD 19 has publications dating as far back as 1870, but early publications often do not have machine readable full text. By 1970, there are only a handful of publications available to text mine and most of them are about coronaviruses in animals (Zoonotic CoV). Publications about human coronavirus strains (like 229E and OC43) are also few in number because these viruses result in mild symptoms not epidemics. The large increase in publications is seen in 2002 during the SARS epidemic, resulting in the peak of publications seen in the red line from 2002 to 2005. A smaller peak is also seen in the purple line during the MERS epidemic in 2012. Most of the publications in the dataset are from 2019 to 2020 during the COVID-19 epidemic as expected.", "For each time range, we expect certain keywords to be more prevalent, like public health is a more common key word after the SARS 2002 outbreak. Before 2002, we expect most keywords to have to do with animal coronaviruses as opposed to the human strains. After 2019, the bulk of publications have COVID-19 terminology as keywords. This slicing in time takes advantage of the clear separation between the different coronavirus outbreaks of SARS, MERS, and COVID-19 (roughly seperated by 10 years).", "SpaCy provides the backbone of my code. SpaCy provides a pipeline that automatically breaks text into tokens, assigns parts of speech, and assigns labels based on how words depend on each other. It also integrates packages like PyTextRank and RAKE into the processing pipeline for speed and consistency. Choosing a specific SpaCy model can improve results, and for CORD 19 there is SciSpaCy, which is trained on biomedical data.", "SpaCy is also a powerful tool for matching key words to the CORD 19 text. Matched keywords found in the abstract of each publication are used as to label the publication according to a subject (e.g. public health, intensive care, diagnostic techniques). The two main ways for matching are phrase matching (for finding more exact terminology) and rule based matching (finding more varied terminology). The phrase matching tends to run fast, as strings are stored as hash values (to save memory), and a vocabulary dictionary is used to convert between the hash value and string. The pattern below \u201cintensive care unit\u201d is used both as the keyword for the subject \u201cICU\u201d assigned as a match pattern:", "Rule based matching relies on knowing the possible tokens that result in a given term, the order of the tokens, or separations of punctuation. The code below matches a single term \u201ccomputed tomography\u201d (CT). The rule based pattern below makes use of lists with the \u201cIN\u201d attribute as logical OR for any word in the list. A logistic operation \u201c+\u201d requires each word in the lists to matched at least once. This results in a set of terminology for CT as the patterns: \u201cchest scans\u201d, \u201cct scans\u201d, \u201cchest findings\u201d, and \u201cct findings\u201d. Also a pattern like \u201cchest ct scans\u201d are found with the \u201c+\u201d operation. In this case, several keywords are used to match to the subject CT as opposed to the subject ICU where there was a single string \u201cintensive care unit\u201d. (For more info about SpaCy match patterns and code excercies check out this tutorial by Ines Montani)", "In the above plot, I use only the publications where a match pattern for COVID-19 is found, and look at the most common associated patterns. Looking at the year 2020 with the largest number of publications, you can see a breakdown in the subjects of the publications per month:", "N-gram, a sequence of N words, is useful for combining single tokens into more descriptive features. Some terms like \u201ccomputed tomography\u201d are useful bigrams (N=2) for looking at the subject of diagnostics. Trigrams (N=3) are also found very often in the CORD 19 data in terms such as \u201cWorld Health Organization\u201d, \u201cintensive care unit\u201d, or \u201cacute respiratory distress\u201d. To look at the frequency of different N-grams, I used the sklearn feature extraction tools like CountVectorizer, which also to set a min, max range of the sequence:", "The above gives a matrix of bigram and trigram counts (tf), which can be converted to a series for plotting:", "Looking at an example for the subject of COVID-19 and public health, I get the following histogram:", "Though some key words seem like they are useful (e.g. \u201csocial distancing\u201d, \u201cintensive care unit\u201d), some trigrams are split incorrectly like \u201cworld health\u201d and \u201chealth organization\u201d should be pieced together to be \u201cworld health organization\u201d. To make this association I use fuzzy string matching, and give a window for the ratio between how similar the bigram is to a trigram based on the Levenshtein Distance. I store a dictionary of bigrams with a Levenshtein distance with the trigrams between 64 and 100. This shows that \u201cworld health organization\u201d is the most frequently occurring word. Trigrams like \u201cpersonal protective equipment\u201d when pieced back from \u201cpersonal protective\u201d also become more frequent. This merging with fuzzy string matching shows more accurate counts for the features that can be input to the next stage of the code for topic building.", "The subjects described above give only a broad genre of topics to coarsely divide papers into sets of features. The histogram of counts already shows a characteristic trend: less frequent words tend to be about more specific topics like \u201csocial distancing\u201d where as very frequent words are too general and would match huge amounts of text like \u201cworld health organization\u201d.", "A good way to account for this trend is to represent the keywords as term-frequency inverse document frequency (TF-IDF) matrices. The term frequency is the counts of a keyword in a document divided by the total number of words. The inverse document frequency is the total number of documents divided by the counts of the feature across all the documents. The log of this ratio allows the TF-IDF score to be computed for a large number of documents. The TF-IDF score is computed by multiplying the two terms: term frequency and inverse document frequency. The score is low for words that occur very frequently across the selected data like \u201cworld health organization\u201d or \u201cpublic health\u201d. The TF-IDF score for a word that is found frequently in one document compared to all the documents, like \u201csocial distancing\u201d has a larger score. This representation of features appropriately weights them according to their relevance as useful search terms.", "What to do with this matrix of TF-IDF values for the features? The answer is to factorize it into smaller matrices whose product is approximately the original matrix. We can take advantage of the fact that the TF-IDF matrix is a non-negative rectangular matrix (with dimensions of N documents and M keywords) so it can be approximately factorized by two non-negative matrices. The rectangular matrices of the factors (WH) need to have a specified parameter, the number of topics or clusters:", "The factors are found using a cost function, a simple cost function like ||V-WH||(the sum of square errors) results in the K-Means clustering algorithm. For my code, I use the same cost function used for Latent-Semantic analysis, the Kulback-Leibler cost function, which separates a topic model P from topic model Q for the same data X based on the log likelihood ratio P/Q. The data X will give larger values for the log likelihood if P is a better model than Q. This mode of non-negative matrix factorization (NMF) can be done using sklearn:", "The number of topics can be a key parameter, too few may not maximize separation power resulting in incoherent topics. Too many topics can result in redundant topics where each contains little information. This separation power can be difficult to understand without visualizing topic clusters in 2D. t-SNE is a useful way to see the topic clusters (from a higher dimension) projected into a 2D (or even 3D) plane.", "The 2D projection with the right number of topics, should look like the fragments from a burst firework. The clusters should be well seperated from each other, and the points in each cluster should be close to the centroid.", "What is useful about the topics created with NMF?", "To see how things fit together concretely, I build 3 topics using NMF on the subject of COVID-19 and its zoonotic origin. The text box below shows the top 10 topic keywords for each topic and the top 3 document titles:", "Based on the above, Topic 0 likely carries a label for \u201ccross-species transmission\u201d as this term is in both the ranked keywords and the title of the most typical document. Topic 1 likely carries the label of \u201cgenomic sequences\u201d for a similar reason. Topic 2 is less clear based on the titles and topic keywords, so we will take this topic as an example for the next section.", "In the analogy of actual mining, at this point in text mining we have rough cut stones that can be further refined into precious gems ready for appraisal. The process of refining involves building phrases from the topic keywords, so that more specific text searches can be made based on phrases in the abstract. The appraisal depends on how well a given publication, paragraph of matched sentences, or single sentences relate to others in the CORD 19 data.", "For refining the keywords, I use PyTextRank on the publication abstracts to obtain more specific topic phrases. The topic phrases contain the topic words plus additional contextual information to refine the search. PyTextRank is the python implementation of the TextRank algorithm, which ranks topic phrases based on their associations in a graph. It is often used for extractive summaries because it can infer links between topic phrases, which are vertices in the graph, and rank them by relevance according to the importance of a vertex. Compared to RAKE, PyTextRank runs more slowly, but also gives a more accurate indication of the context of the paper.", "Returning to the example in the previous step, topic 2 about the zoonotic origin of COVID-19 is difficult to pin down with just the topic words. Some topic words are still very vague like \u201camino acid\u201d. In the text box below, you can see how the topic phrases make the topic words \u201camino acid\u201d more specific:", "Similarly, other topic words like \u201cwater samples\u201d are made more specific with the topic phrases above. If the topic words do represent relevant search terms, they should show up in topic phrases with high PyTextRank scores compared to the other phrases. The plot below shows the scatter plot of the two cases. High PyText rank scores (>0.02) also result in lower count frequency as they represent more specific contexts compared to low rank scores. A score of 0.0 is assigned for stop words (e.g. we, is, the). The highest scored phrase containing a topic word \u201ccoronavirus spike protein receptor binding domain\u201d makes it clear that the binding domain is on the coronavirus spike protein . Based on the topic phrases, Topic 2 likely has the label \u201cSpike protein amino acid insertions\u201d.", "The plot below shows some of the topic phrases across the three topics for the zoonotic origin of COVID-19. In general, the larger the rank of the phrase, the less frequent it is found across the publication abstracts. This gives much more specific search phrases used to search CORD 19 text for valuable insights.", "The SpaCy phrase matcher is again used to find matched text in the body of the publications. Considering all the publications with either matched text or from the NMF embedding, the full text of publications can be compared using cosine similarity:", "The top 3 most similar documents for topic 2 are:", "Similarly, if the corpus in the code block above is filled with paragraphs of matched text in a document (instead of the full body text), then the paragraphs can be ranked with cosine similarity. The top ranked paragraph is:", "\u201cWithin S1, the N-terminal domain (NTD) was found to be less conserved (51% identity) compared to the receptor binding domain (RBD, 74% identity)\u2026 However, the exposed loop feature has been demonstrated in both modeled and cryo-EM CoV S structures with similar amino acid sequences at the S1/S2 site \u2026 Amino acid sequences of the S protein used in the phylogenetic analysis were obtained from Figure 4 \u2026 Amino acid sequences of the S1/S2 and S2' sites are shown\u2026 domain (NTD) was found to be less conserved (51% identity) compared to the receptor binding domain (RBD, 74% identity)\u2026 However, the exposed loop feature has been demonstrated in both modeled and cryo-EM CoV S structures with similar amino acid sequences at the S1/S2 author/funder\u2026 Amino acid sequences of the S protein used in the phylogenetic analysis were obtained \u2026 Amino acid sequences of the S1/S2 and S2' sites are shown\u201d Structural modeling of 2019-novel coronavirus (nCoV) spike protein reveals a proteolytically-sensitive activation loop as a distinguishing feature compared to SARS-CoV and related SARS-like coronaviruses", "The above paragraph is still too dense to comprehend if there is a useful insight, so each sentence of matched text (seperated by \u201c\u2026\u201d) can be ranked with cosine similarity for this topic. The largest ranked sentence from all the matched text for this topic is :", "\u201cAdditionally, our analysis shows that the virus RmYN02 from R. malayanus, which is characterized by the insertion of multiple amino acids at the junction site of the S1 and S2 subunits of the Spike (S) protein, belongs to the same clade as both RaTG13 and SARS-CoV-2, providing further support for the natural origin of SARS-CoV-2 in Rhinolophus spp bats in the region\u201d Origin and cross-species transmission of bat coronaviruses in China", "The most granular information, sentences in the selected CORD 19 data, ranked with cosine similarity can reach a concise conclusion. The insertion of amino acids in the spike protein of SARS-CoV-2, responsible for COVID-19, supports evidence that the origin of the virus is in horsehoe bats (Rhinolophus malayanus) according to the paper above.", "I think if I were to briefly summarize my mining endeavor, I cut, sifted, and then ranked the text in the steps demonstrated above. This crushes tens of thousands of documents into a short reading list. An expert in public health, medicine, or epidemiology could skim the insights and find interesting conclusions specific to their research or survey important open questions. For data scientists, it may be useful to extend this algorithm to an unsupervised learning approach. The last step could be tied to the first so that the reading list is used to find more keywords to label the subjects. In this way the topics and the subjects will be more robust to look for pertinent information.", "[1] Lu Wang, Lucy et al. CORD-19: The Covid-19 Open Research Dataset (22 Apr. 2020)ArXiv Preprint.(Papers quoted above)[2] Javier A. Jaimes, Nicole M. Andr\u00e9, Jean K. Millet, Gary R. Whittaker Structural modeling of 2019-novel coronavirus (nCoV) spike protein reveals a proteolytically-sensitive activation loop as a distinguishing feature compared to SARS-CoV and related SARS-like coronaviruses (May 2020) Journal of Molecular Biology[3] Latinne, A., Hu, B., Olival, K.J. et al. Origin and cross-species transmission of bat coronaviruses in China. Nat Commun 11, 4235 (2020).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fca5d8f95e276&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rgp230.medium.com/?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": ""}, {"url": "https://rgp230.medium.com/?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "Rishi Patel, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F837c74fe45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&user=Rishi+Patel%2C+PhD&userId=837c74fe45d&source=post_page-837c74fe45d----ca5d8f95e276---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca5d8f95e276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca5d8f95e276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.whitehouse.gov/briefings-statements/call-action-tech-community-new-machine-readable-covid-19-dataset/", "anchor_text": "White House Office of Science and Technology Policy"}, {"url": "https://unsplash.com/@caleblaz?utm_source=medium&utm_medium=referral", "anchor_text": "Caleb Perez"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/analytics-vidhya/6-steps-to-build-roberta-a-robustly-optimised-bert-pretraining-approach-e508ebe78b96", "anchor_text": "RoBERTa"}, {"url": "https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents", "anchor_text": "RAKE"}, {"url": "https://spacy.io/", "anchor_text": "SpaCy"}, {"url": "https://allenai.github.io/scispacy/", "anchor_text": "SciSpaCy"}, {"url": "https://spacy.io/usage/rule-based-matching#phrasematcher", "anchor_text": "are phrase matching"}, {"url": "https://spacy.io/usage/rule-based-matching#matcher", "anchor_text": "rule based matching"}, {"url": "https://course.spacy.io/en", "anchor_text": "this tutorial"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "sklearn feature extraction tools"}, {"url": "https://medium.com/nlpgurukool/fuzzy-matching-1baac719aa25", "anchor_text": "fuzzy string matching"}, {"url": "https://towardsdatascience.com/fuzzywuzzy-how-to-measure-string-distance-on-python-4e8852d7c18f", "anchor_text": "Levenshtein Distance"}, {"url": "https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558", "anchor_text": "term-frequency inverse document frequency"}, {"url": "https://en.wikipedia.org/wiki/Nonnegative_matrix", "anchor_text": "it can be approximately factorized by two non-negative matrices"}, {"url": "https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c", "anchor_text": "K-Means clustering algorithm"}, {"url": "https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3", "anchor_text": "Latent-Semantic analysis,"}, {"url": "https://medium.com/@cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a", "anchor_text": "Kulback-Leibler cost function"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html", "anchor_text": "sklearn"}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "t-SNE"}, {"url": "https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf", "anchor_text": "PyTextRank"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/32320687", "anchor_text": "Phylogenetic Analysis and Structural Modeling of SARS-CoV-2 Spike Protein Reveals an Evolutionary Distinct and Proteolytically Sensitive Activation Loop"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/32344679/", "anchor_text": "The COVID-19 Pandemic: A Comprehensive Review of Taxonomy, Genetics, Epidemiology, Diagnosis, Treatment, and Control"}, {"url": "https://www.sciencedirect.com/science/article/pii/S1567134820302203", "anchor_text": "Exploring the genomic and proteomic variations of SARS-CoV-2 spike glycoprotein: A computational biology approach"}, {"url": "https://doi.org/10.1101/2020.02.10.942185", "anchor_text": "Structural modeling of 2019-novel coronavirus (nCoV) spike protein reveals a proteolytically-sensitive activation loop as a distinguishing feature compared to SARS-CoV and related SARS-like coronaviruses"}, {"url": "https://doi.org/10.1038/s41467-020-17687-3", "anchor_text": "Origin and cross-species transmission of bat coronaviruses in China"}, {"url": "https://medium.com/analytics-vidhya/avalanches-of-data-text-mining-inspired-by-proton-collisions-32bd50f8c2a5", "anchor_text": "Avalanches of Data: Text mining inspired by proton collisions"}, {"url": "https://medium.com/rebel-public-health/insights-from-the-infodemic-fa43307ef42e", "anchor_text": "Insights from the Infodemic"}, {"url": "https://github.com/rpatelCERN/CORD19/", "anchor_text": "CORDCrusher"}, {"url": "https://github.com/rpatelCERN/CORD19/wiki", "anchor_text": "wiki area includes"}, {"url": "https://www.semanticscholar.org/cord19/download", "anchor_text": "Link to the CORD 19 data"}, {"url": "https://arxiv.org/abs/2004.10706", "anchor_text": "CORD-19: The Covid-19 Open Research Dataset"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0022283620302874?via%3Dihub", "anchor_text": "Structural modeling of 2019-novel coronavirus (nCoV) spike protein reveals a proteolytically-sensitive activation loop as a distinguishing feature compared to SARS-CoV and related SARS-like coronaviruses"}, {"url": "https://doi.org/10.1038/s41467-020-17687-3", "anchor_text": "Origin and cross-species transmission of bat coronaviruses in China"}, {"url": "https://medium.com/tag/covid-19?source=post_page-----ca5d8f95e276---------------covid_19-----------------", "anchor_text": "Covid-19"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ca5d8f95e276---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----ca5d8f95e276---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/search-engine-optimizati?source=post_page-----ca5d8f95e276---------------search_engine_optimizati-----------------", "anchor_text": "Search Engine Optimizati"}, {"url": "https://medium.com/tag/text-mining?source=post_page-----ca5d8f95e276---------------text_mining-----------------", "anchor_text": "Text Mining"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca5d8f95e276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&user=Rishi+Patel%2C+PhD&userId=837c74fe45d&source=-----ca5d8f95e276---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca5d8f95e276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&user=Rishi+Patel%2C+PhD&userId=837c74fe45d&source=-----ca5d8f95e276---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca5d8f95e276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fca5d8f95e276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ca5d8f95e276---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ca5d8f95e276--------------------------------", "anchor_text": ""}, {"url": "https://rgp230.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rgp230.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rishi Patel, PhD"}, {"url": "https://rgp230.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F837c74fe45d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&user=Rishi+Patel%2C+PhD&userId=837c74fe45d&source=post_page-837c74fe45d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F837c74fe45d%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcord-crusher-slicing-the-cord-19-data-into-summaries-ca5d8f95e276&user=Rishi+Patel%2C+PhD&userId=837c74fe45d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}