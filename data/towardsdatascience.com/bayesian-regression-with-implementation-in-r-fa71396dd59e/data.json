{"url": "https://towardsdatascience.com/bayesian-regression-with-implementation-in-r-fa71396dd59e", "time": 1683005169.042701, "path": "towardsdatascience.com/bayesian-regression-with-implementation-in-r-fa71396dd59e/", "webpage": {"metadata": {"title": "Bayesian regression with implementation in R | by Liyi Zhang | Towards Data Science", "h1": "Bayesian regression with implementation in R", "description": "Regression from the Bayesian viewpoint, theoretical derivations from scratch, R implementation, and discussion of the Bayesian worldview"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Linear regression can be established and interpreted from a Bayesian perspective. The first parts discuss theory and assumptions pretty much from scratch, and later parts include an R implementation and remarks. Readers can feel free to copy the two blocks of code into an R notebook and play around with it.", "Recall that in linear regression, we are given target values y, data X, and we use the model", "where y is N*1 vector, X is N*D matrix, w is D*1 vector, and the error is N*1 vector. We have N data points. Dimension D is understood in terms of features, so if we use a list of x, a list of x\u00b2 (and a list of 1\u2019s corresponding to w_0), we say D=3. If you don\u2019t like matrix form, think of it as just a condensed form of the following, where everything is a scaler instead of a vector or matrix:", "In classical linear regression, the error term is assumed to have Normal distribution, and so it immediately follows that y is normally distributed with mean Xw, and variance of whatever variance the error term has (denote by \u03c3\u00b2, or diagonal matrix with entries \u03c3\u00b2). The normal assumption turns out well in most cases, and this normal model is also what we use in Bayesian regression.", "We are now faced with two problems: inference of w, and prediction of y for any new X. Using the well-known Bayes rule and the above assumptions, we are only steps away towards not only solving these two problems, but also giving a full probability distribution of y for any new X. Here is the Bayes rule using our notations, which expresses the posterior distribution of parameter w given data:", "\u03c0 and f are probability density functions. Since the result is a function of w, we can ignore the denominator, knowing that the numerator is proportional to lefthand side by a constant. We know from assumptions that the likelihood function f(y|w,x) follows the normal distribution. The other term is prior distribution of w, and this reflects, as the name suggests, prior knowledge of the parameters.", "Prior Distribution. Defining the prior is an interesting part of the Bayesian workflow. For convenience we let w ~ N(m_o, S_o), and the hyperparameters m and S now reflect prior knowledge of w. If you have little knowledge of w, or find any assignment of m and S too subjective, \u2018non-informative\u2019 priors are an amendment. In this case, we set m to 0 and more importantly set S as a diagonal matrix with very large values. We are saying that w has a very high variance, and so we have little knowledge of what w will be.", "With all these probability functions defined, a few lines of simply algebraic manipulations (quite a few lines in fact) will give the posterior after observation of N data points:", "It looks like a bunch of symbols, but they are all defined already, and you can compute this distribution once this theoretical result is implemented in code. (N(m,S) means normal distribution with mean m and covariance matrix S.)", "A full Bayesian approach means not only getting a single prediction (denote new pair of data by y_o, x_o), but also acquiring the distribution of this new point.", "What we have done is the reverse of marginalizing from joint to get marginal distribution on the first line, and using Bayes rule inside the integral on the second line, where we have also removed unnecessary dependences. Notice that we know what the last two probability functions are. The result of full predictive distribution is:", "Implementation in R is quite convenient. Backed up with the above theoretical results, we just input matrix multiplications into our code and get results of both predictions and predictive distributions. To illustrate with an example, we use a toy problem: X is from -1 to 1, evenly spaced, and y is constructed as the following additions of sinusoidal curves with normal noise (see graph below for illustration of y).", "The following code gets this data.", "The following code (under section \u2018Inference\u2019) implements the above theoretical results. We also expand features of x (denoted in code as phi_X, under section Construct basis functions). Just as we would expand x into x\u00b2, etc., we now expand it into 9 radial basis functions, each one looking like the follows. Note that although these look like normal density, they are not interpreted as probabilities.", "One advantage of radial basis functions is that radial basis functions can fit a variety of curves, including polynomial and sinusoidal.", "One detail to note in these computations, is that we use non-informative prior. The commented out section is exactly the theoretical results above, while for non-informative prior we use covariance matrix with diagonal entries approaching infinity, so the inverse of that is directly considered as 0 in this code. If you\u2019d like to use this code, make sure you install ggplot2 package for plotting.", "The following illustration aims at representing a full predictive distribution and giving a sense of how well the data is fit.", "Multiple linear regression result is same as the case of Bayesian regression using improper prior with an infinite covariance matrix. Generally, it is good practice to obtain some empirical knowledge regarding the parameters, and use an informative prior. Bayesian regression can then quantify and show how different prior knowledge impact predictions. In any case, the Bayesian view can conveniently interpret the range of y predictions as a probability, different from the Confidence Interval computed from classical linear regression.", "Data fitting in this perspective also makes it easy for you to \u2018learn as you go\u2019. Say I first observed 10000 data points, and computed a posterior of parameter w. After that, I somehow managed to acquire 1000 more data points, and instead of running the whole regression again, I can use the previously computed posterior as my prior for these 1000 points. This sequential process yields the same result as using the whole data all over again. I like this idea in that it\u2019s very intuitive, in the manner as a learned opinion is proportional to previously learned opinions plus new observations, and the learning goes on. A joke says that a Bayesian who dreams of a horse and observes a donkey, will call it a mule. But if he takes more observations of it, eventually he will say it is indeed a donkey.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa71396dd59e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zhang-liyi.medium.com/?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": ""}, {"url": "https://zhang-liyi.medium.com/?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "Liyi Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9dc347a964a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&user=Liyi+Zhang&userId=9dc347a964a2&source=post_page-9dc347a964a2----fa71396dd59e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa71396dd59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa71396dd59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/statistics?source=post_page-----fa71396dd59e---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/probability?source=post_page-----fa71396dd59e---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fa71396dd59e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bayesian?source=post_page-----fa71396dd59e---------------bayesian-----------------", "anchor_text": "Bayesian"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fa71396dd59e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa71396dd59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&user=Liyi+Zhang&userId=9dc347a964a2&source=-----fa71396dd59e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa71396dd59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&user=Liyi+Zhang&userId=9dc347a964a2&source=-----fa71396dd59e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa71396dd59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffa71396dd59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fa71396dd59e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fa71396dd59e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fa71396dd59e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fa71396dd59e--------------------------------", "anchor_text": ""}, {"url": "https://zhang-liyi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zhang-liyi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Liyi Zhang"}, {"url": "https://zhang-liyi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "28 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9dc347a964a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&user=Liyi+Zhang&userId=9dc347a964a2&source=post_page-9dc347a964a2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F819377f70641&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-regression-with-implementation-in-r-fa71396dd59e&newsletterV3=9dc347a964a2&newsletterV3Id=819377f70641&user=Liyi+Zhang&userId=9dc347a964a2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}