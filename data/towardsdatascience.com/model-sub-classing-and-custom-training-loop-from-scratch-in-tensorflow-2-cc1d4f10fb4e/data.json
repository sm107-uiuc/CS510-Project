{"url": "https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e", "time": 1683016155.2538261, "path": "towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e/", "webpage": {"metadata": {"title": "Model Sub-Classing and Custom Training Loop from Scratch in TensorFlow 2 | by Innat | Towards Data Science", "h1": "Model Sub-Classing and Custom Training Loop from Scratch in TensorFlow 2", "description": "In this article, we will try to understand the Model Sub-Classing API and Custom Training Loop from Scratch in TensorFlow 2. It may not be a beginner or advanced introduction but aim to get rough\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1611.03530.pdf", "anchor_text": "src", "paragraph_index": 9}, {"url": "https://github.com/tensorflow/tensorflow/issues/31647#issuecomment-692586409", "anchor_text": "this discussion", "paragraph_index": 20}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10", "paragraph_index": 23}, {"url": "https://www.tensorflow.org/tensorboard/image_summaries", "anchor_text": "displaying per batches samples + confusion matrix", "paragraph_index": 34}, {"url": "https://www.tensorflow.org/guide/keras/save_and_serialize", "anchor_text": "ways to save", "paragraph_index": 37}, {"url": "https://www.tensorflow.org/guide/saved_model", "anchor_text": "SaveModel", "paragraph_index": 37}, {"url": "https://www.tensorflow.org/tutorials/keras/save_and_load", "anchor_text": "HDF5", "paragraph_index": 37}, {"url": "https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=xXgtNRCSyuIW", "anchor_text": "article", "paragraph_index": 47}, {"url": "https://twitter.com/fchollet?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor", "anchor_text": "Fran\u00e7ois Chollet", "paragraph_index": 47}], "all_paragraphs": ["In this article, we will try to understand the Model Sub-Classing API and Custom Training Loop from Scratch in TensorFlow 2. It may not be a beginner or advanced introduction but aim to get rough intuition of what they are all about. The post is divided into three parts:", "So, at first, we will see how many ways to define model using TensorFlow 2 and how they differ from each other. Next, we will see how feasible it is to build a complex neural architecture using the model subclassing API which is introduced in TF 2. And then we will implement a custom training loop and train these subclassing models end-to-end from scratch. We will also use Tensorboard in our custom training loop to track the model performance for each batch. We will also see how to save and load the model after training. In the end, we will measure the model performance via the confusion matrix and classification report, etc.", "In TF.Keras there are basically three-way we can define a neural network, namely", "Among them, Sequential API is the easiest way to implement but comes with certain limitations. For example, with this API, we can\u2019t create a model that shares feature information with another layer except to its subsequent layer. In addition, multiple input and output are not possible to implement either. At this point, Functional API does solve these issues greatly. A model like Inception or ResNet is feasible to implement in Functional API. But often deep learning researcher wants to have more control over every nuance of the network and on the training pipelines and that\u2019s exactly what Model Subclassing API serves. Model Sub-Classing is a fully customizable way to implement the feed-forward mechanism for the custom-designed deep neural network in an object-oriented fashion.", "Let\u2019s create a very basic neural network using these three APIs. It will be the same neural architecture and will see what are the implementation differences. This of course will not demonstrate the full potential, especially for Functional and Model Sub-Classing API. The architecture will be as follows:", "Simple enough. As mentioned, let\u2019s create the neural nets with Sequential, Functional, and Model Sub-Classing respectively.", "In Model Sub-Classing there are two most important functions __init__ and call. Basically, we will define all the trainable tf.keras layers or custom implemented layers inside the __init__ method and call those layers based on our network design inside the call method which is used to perform a forward propagation. ( It\u2019s quite the same as the forward method that is used to build the model in PyTorch anyway. )", "Let\u2019s run these models on the MNIST data set. We will load from tf.keras.datasets. However, the input image is 28 by 28 and in grayscale shape. We will repeat the axis three times so that we can feasibly experiment with the pretrained weight later on if necessary.", "The core data structures in TF.Keras is layers and model classes. A layer encapsulates both state (weight) and the transformation from inputs to outputs, i.e. the call method that is used to define the forward pass. However, these layers are also recursively composable. It means if we assign a tf.keras.layers.Layer instances as an attribute of another tf.keras.layers.Layer, the outer layer will start tracking the weights matrix of the inner layer. So, each layer will track the weights of its sublayers, both trainable and non-trainable. Such functionality is required when we need to build such a layer of a higher level of abstraction.", "In this part, we will be building a small Inception model by subclassing the layers and model classes. Please see the diagram below. It\u2019s a small Inception network, src. If we give a close look we\u2019ll see that it mainly consists of three special modules, namely:", "From the diagram we can see, that it consists of one convolutional network, one batch normalization, and one relu activation. Also, it produces C times feature maps with K x K filters and S x S strides. Now, it would be very inefficient if we simply go with the sequential modeling approach because we will be re-using this module many times in the complete network. So, defining a functional block would be efficient and simple enough. But this time, we will prefer layer subclassing which is more pythonic and more efficient. To do that, we will create a class object that will inherit the tf.keras.layers.Layer classes.", "Now, We can also initiate the object of this class and see the following properties.", "Next comes the Inception module. According to the above graph, it consists of two convolutional modules that then merge together. Now as we know to merge, here we need to ensure that the output feature maps dimension ( height and width ) needs to be the same.", "Here you may notice that we are now hard-coded the exact kernel size and strides number for both convolutional layers according to the network (diagram). And also in ConvModule, we have already set padding to the \u2018same\u2019, so that the dimension of the feature maps will be the same for both (self.conv1 and self.conv2); which is required in order to concatenate them to the end.", "Again, in this module, two variable performs as the placeholder, kernel_size1x1, and kernel_size3x3. This is for the purpose of course. Because we will need different numbers of feature maps for the different stages of the entire model. If we look into the diagram of the model, we will see that InceptionModule takes a different number of filters at different stages in the model.", "Lastly the downsampling module. The main intuition for downsampling is that we hope to get more relevant feature information that highly represents the inputs to the model. As it tends to remove the unwanted feature so that model can focus on the most relevant. There are many ways we can reduce the dimension of the feature maps (or inputs). For example: using strides 2 or using the conventional pooling operation. There are many types of pooling operations, namely: MaxPooling, AveragePooling, and GlobalAveragePooling.", "From the diagram, we can see that the downsampling module contains one convolutional layer and one max-pooling layer which later merge together. Now, if we look closely at the diagram (top-right), we will see that the convolutional layer takes a 3 x 3 size filter with strides 2 x 2. And the pooling layer (here MaxPooling) takes pooling size 3 x 3 with strides 2 x 2. Fair enough, however, we also need to ensure that the dimension coming from each of them should be the same in order to merge at the end. Now, if we remember when we design the ConvModule, we purposely set the value of the padding argument to `same`. But in this case, we need to set it to `valid`.", "In general, we use the Layer class to define the inner computation blocks and will use the Model class to define the outer model, practically the object that we will train. In our case, in an Inception model, we define three computational blocks: Conv Module, Inception Module, and Downsample Module. These are created by subclassing the Layer class. And so next, we will use the Model class to encompass these computational blocks in order to create the entire Inception network. Typically the Model class has the same API as Layer but with some extra functionality.", "Same as the Layer class, we will initialize the computational block inside the init method of the Model class as follows:", "The amount of filter number for each computational block is set according to the design of the model (also visualized down below in the diagram). After initialing all the blocks, we will connect them according to the design (diagram). Here is the full Inception network using the Model subclass:", "As you may notice, apart from the __init__ and call method additionally we define a custom method build_graph. We\u2019re using this as a helper function to plot the model summary information conveniently. Please, check out this discussion for more details. Anyway, let\u2019s check out the model\u2019s summary.", "Now, it is complete to build the entire Inception model via model subclassing.", "Now we have built a complex network, it\u2019s time to make it busy to learn something. We can now easily train the model simply just by using the compile and fit. But here we will look at a custom training loop from scratch. This functionality is newly introduced in TensorFlow 2. Please note, that this functionality is a little bit complex comparatively and more fit for the deep learning researcher.", "For the demonstration purpose, we will be using the CIFAR-10 data set. Let\u2019s prepare it first.", "Here we will convert the class vector (y_train, y_test) to the multi-class matrix. And also we will use tf.data API for better and more efficient input pipelines.", "Let\u2019s quickly check the data shape after label conversion and input slicing:", "so far so good. We have an input shape of 32 x 32 x 3 and a total of 10 classes to classify. However, it\u2019s not ideal to make a test set as a validation set but for demonstration purposes, we are not considering the train_test_split approach. Now, let\u2019s see what the custom training pipelines consist of in Tensorflow 2.", "In TF.Keras, we have convenient training and evaluating loops, fit, and evaluate. But we also can have leveraged the low-level control over the training and evaluation process. In that case, we need to write our own training and evaluation loops from scratch. Here are the recipes:", "TensorFlow provides the tf.GradientTape() API for automatic differentiation, that is, computing the gradient of computation with respect to some inputs.", "Below is a short demonstration of its operation process. Here we have some input (x) and trainable param (w, b). Inside the tf.GradientTape() scope, output (y, which basically would be the model output), and loss are measured. And outside the scope, we retrieve the gradients of the weight parameter with respect to the loss.", "Now, let\u2019s implement the custom training recipes accordingly.", "Great. However, we are still not talking about how we gonna add metrics to monitor this custom training loop. Obviously, we can use built-in metrics or even custom metrics in the training loop also. To add metrics in the training loop is fairly simple, here is the flow:", "Here is another thing to consider. The default runtime in TensorFlow 2.0 is eager execution. The above training loops are executing eagerly. But if we want graph compilation we can compile any function into a static graph with @tf.function decorator. This also speeds up the training step much faster. Here is the setup for the training and evaluation function with @tf.function decorator.", "Here we\u2019re seeing the usage of metrics.update_state(). These functions return to the training loop where we will set up displaying the log message, metric.result(), and also reset the metrics, metric.reset_states().", "Here is the last thing we like to set up, the TensorBoard. There are some great functionalities in it to utilize such as: displaying per batches samples + confusion matrix, hyper-parameter tuning, embedding projector, model graph, etc. For now, we will only focus on logging the training metrics on it. Simple enough but we will integrate it into the custom training loop. So, we can\u2019t use tf.keras.callbacks.TensorBoard but need to use the TensorFlow Summary API. The tf.summary module provides API for writing summary data on TensorBoard. We want to write the logging state after each batch operation to get more details. Otherwise, we may prefer at the end of each epoch. Let\u2019s create a directory where the message of the event will be saved. In the working directory, create the log/train and log/test. Below is the full training pipelines. We recommend reading the code thoroughly at first in order to get the overall training flow.", "Voila! We run the code in our local system, with RTX 2070. By enabling the mixed-precision we\u2019re able to increase the batch size up to 256. Here is the log output:", "Overfitting! But that\u2019s ok for now. For that, we just need some care to consider such as Image Augmentation, Learning Rate Schedule, etc. In the working directory, run the following command to the live tensorboard. In the below command, logs are the folder name that we created manually to save the event logs.", "There are various ways to save TensorFlow models depending on the API we\u2019re using. Model saving and re-loading in model subclassing are not the same as in Sequential or Functional API. It needs some special attention. Currently, there are two formats to store the model: SaveModel and HDF5. From the official doc:", "The key difference between HDF5 and SavedModel is that HDF5 uses object configs to save the model architecture, while SavedModel saves the execution graph. Thus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the orginal code.", "So, it looks like SavedModels are able to save our custom subclassed models. But what if we want HDF5 format for our custom subclassed models? According to the doc. we can do that either but we need some extra stuff. We must define the get_config method in our object. And also need to pass the object to the custom_object argument when loading the model. This argument must be a dictionary mapping: tf.keras.models.load_model(path, custom_objects={\u2018CustomLayer\u2019: CustomLayer}). However, it seems like we can\u2019t use HDF5 for now as we don\u2019t use the get_config method in our customed object. However, it\u2019s actually a good practice to define this function in the custom object. This will allow us to easily update the computation later if needed.", "But for now, let\u2019s now save the model and reload it again with the SavedModel format.", "After that, it will create a new folder named net in the working directory. It will contain assets, saved_model.pb, and variables. The model architecture and training configuration, including the optimizer, losses, and metrics are stored in saved_model.pb. The weights are saved in the variables directory.", "When saving the model and its layers, the SavedModel format stores the class name, call function, losses, and weights (and the config, if implemented). The call function defines the computation graph of the model/layer. In the absence of the model/layer config, the call function is used to create a model that exists like the original model which can be trained, evaluated, and used for inference. Later to re-load the saved model, we will do:", "Set compile=False is optional, I do this to avoid warning logs. Also as we are doing custom loop training, we don\u2019t need any compilation.", "So far, we have talked about saving the entire model (computation graph and parameters). But what, if we want to save the trained weight only and reload the weight when need it. Yeap, we can do that too. Simply just,", "It will save the weight of our model. Now, when it comes to re-load it again, here is one thing to keep in mind. We need to call the build method before we try to load the weight. It mainly initializes the layers in a subclassed model, so that the computation graph can build. For example, if we try as follows:", "To solve that we can do as follows:", "It will load successfully. Here is an awesome article regarding saving and serializing models in TF.Keras by Fran\u00e7ois Chollet, must-read.", "Though not necessary, let\u2019s end up by measuring the model performance. CIFAR-10 class label maps are as follows: 0:airplane, 1:automobile, 2:bird, 3:cat, 4:deer, 5:dog, 6:frog, 7:horse, 8:ship, 9:truck. Let\u2019s find the classification report first.", "Prediction / Inference on new sample:", "This ends here. Thank you so much for reading the article, hope you guys enjoy it. The article is a bit long, so here is a quick summary.", "We first compare TF.Keras modeling APIs. Next, we use the Model Sub-Classing API to build a small Inception network step by step. Then we look at the process of newly introduced custom loop training in TensorFlow 2 with GradientTape. We\u2019ve also trained the subclassed Inception model end to end. And lastly, we discuss custom model saving and reloading followed by measuring the performance of these trained models.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI Software Engineer @ AI Samurai Japan Ltd."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcc1d4f10fb4e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://innat-2k14.medium.com/?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": ""}, {"url": "https://innat-2k14.medium.com/?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "Innat"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe7d2fd3db33b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&user=Innat&userId=e7d2fd3db33b&source=post_page-e7d2fd3db33b----cc1d4f10fb4e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc1d4f10fb4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc1d4f10fb4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@nima_sarram?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Nima Sarram"}, {"url": "https://unsplash.com/t/nature?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1611.03530.pdf", "anchor_text": "src"}, {"url": "https://arxiv.org/pdf/1611.03530.pdf", "anchor_text": "src"}, {"url": "https://github.com/tensorflow/tensorflow/issues/31647#issuecomment-692586409", "anchor_text": "this discussion"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10"}, {"url": "https://www.tensorflow.org/tensorboard/image_summaries", "anchor_text": "displaying per batches samples + confusion matrix"}, {"url": "https://www.tensorflow.org/guide/keras/save_and_serialize", "anchor_text": "ways to save"}, {"url": "https://www.tensorflow.org/guide/saved_model", "anchor_text": "SaveModel"}, {"url": "https://www.tensorflow.org/tutorials/keras/save_and_load", "anchor_text": "HDF5"}, {"url": "https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=xXgtNRCSyuIW", "anchor_text": "article"}, {"url": "https://twitter.com/fchollet?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor", "anchor_text": "Fran\u00e7ois Chollet"}, {"url": "https://stackoverflow.com/a/67591848/9215780", "anchor_text": "AutoGrad in PyTorch and GradientTape in TensorFlow 2"}, {"url": "https://stackoverflow.com/a/66849164/9215780", "anchor_text": "Multi-Input Multi-Output Modeling in TensorFlow 2 [Keras]"}, {"url": "https://stackoverflow.com/a/67851641/9215780", "anchor_text": "Selecting Loss and Metrics"}, {"url": "https://stackoverflow.com/a/67744905/9215780", "anchor_text": "Reload Best Checkpoint in Keras"}, {"url": "https://stackoverflow.com/a/66627036/9215780", "anchor_text": "Keras Dense Layer vs PyTorch Linear Layer"}, {"url": "https://stackoverflow.com/a/66880334/9215780", "anchor_text": "Keras Modeling API vs Sequential API"}, {"url": "https://stackoverflow.com/a/67238117/9215780", "anchor_text": "Keras Model.Predict Slower than NumPy!"}, {"url": "https://stackoverflow.com/a/66189774/9215780", "anchor_text": "Implement GradCAM in Keras"}, {"url": "https://stackoverflow.com/a/68297796/9215780", "anchor_text": "Keras Attention Layer for LSTM / GRU"}, {"url": "https://stackoverflow.com/a/66238161/9215780", "anchor_text": "TensorFlow Data API vs Numpy Array"}, {"url": "https://stackoverflow.com/a/67467084/9215780", "anchor_text": "Neural Network & Binary Classification Guidance"}, {"url": "https://stackoverflow.com/a/66881336/9215780", "anchor_text": "Quantization Aware Training with tf.GradientTape"}, {"url": "https://stackoverflow.com/a/66486573/9215780", "anchor_text": "Purpose of self.add_loss Function in Keras"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cc1d4f10fb4e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----cc1d4f10fb4e---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cc1d4f10fb4e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/keras?source=post_page-----cc1d4f10fb4e---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/research?source=post_page-----cc1d4f10fb4e---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc1d4f10fb4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&user=Innat&userId=e7d2fd3db33b&source=-----cc1d4f10fb4e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcc1d4f10fb4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&user=Innat&userId=e7d2fd3db33b&source=-----cc1d4f10fb4e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc1d4f10fb4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcc1d4f10fb4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cc1d4f10fb4e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cc1d4f10fb4e--------------------------------", "anchor_text": ""}, {"url": "https://innat-2k14.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://innat-2k14.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Innat"}, {"url": "https://innat-2k14.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe7d2fd3db33b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&user=Innat&userId=e7d2fd3db33b&source=post_page-e7d2fd3db33b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff1dd70601e67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e&newsletterV3=e7d2fd3db33b&newsletterV3Id=f1dd70601e67&user=Innat&userId=e7d2fd3db33b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}