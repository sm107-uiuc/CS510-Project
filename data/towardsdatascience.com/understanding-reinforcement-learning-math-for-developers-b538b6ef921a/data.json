{"url": "https://towardsdatascience.com/understanding-reinforcement-learning-math-for-developers-b538b6ef921a", "time": 1683003530.10391, "path": "towardsdatascience.com/understanding-reinforcement-learning-math-for-developers-b538b6ef921a/", "webpage": {"metadata": {"title": "Understanding Reinforcement Learning Math, for Developers | by Ziad SALLOUM | Towards Data Science", "h1": "Understanding Reinforcement Learning Math, for Developers", "description": "Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com If you are a developer with not enough knowledge in math, you might be having hard time\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4", "anchor_text": "Math Behind Reinforcement Learning, the Easy Way", "paragraph_index": 10}], "all_paragraphs": ["Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "If you are a developer with not enough knowledge in math, you might be having hard time grasping the basic formula of Reinforcement Learning.", "Understanding this equation might be challenging for people with insufficient mathematical background. However, aside from the cryptic symbols, it is not that hard to understand.", "It all boils down to asking a question: what is it worth to be at state S ?To make it more concrete, consider that you are in a TV show, and placed in front of two doors, one has 5000$ behind it, and the other has 1000$ and you are asked to choose one door to open.", "You have no clue which is which, so you have equal probabilities to open any of them. The question is now, what is it worth to be in this TV show ?The answer is rather easy. On the worst case, it is worth 1000$ (because you can win a minimum of 1000$), on the best case it is worth 5000$ (because you can win a maximum of 5000$). To have only one value, we compute the average which will be 3000$.", "So the value of the situation or state, is what you can expect to receive (on average) as future prizes or rewards.", "Let\u2019s modify a little bit the hypothesis. You are told that the door on left contains 5000$ and the one the right 1000$, but to open any of them you have to hit the lock with a ball, and there are 20% chance to hit the lock of the left door and 80% chance to hit the lock on the right door.", "What would be the value of such situation ?", "Intuitively we can think of the value of the state V(S) as the sum of the discounted future rewards, weighted by the probabilities of getting them.", "The need for discount factor (\u03b3), comes from the fact that a reward that you will get in the far future is less valuable than the reward that you will get in the next step, so it will get discounted by the number of steps needed to reach it, example: \u03b3*\u03b3*\u2026.*\u03b3*r, where the number of multiplication of \u03b3 is equal to the number of steps to reach the reward.", "Mathematical details can be found in this article \u201cMath Behind Reinforcement Learning, the Easy Way\u201d", "The following picture shows the relation between a state S, actions, rewards and target states.", "You can see that state S has 3 possible actions, each of them leads to one or more states with different probabilities and different rewards. It is worth noting that state S4 is not reachable from S, but S1 is reachable if action a1 is taken and will lead to 2 possible rewards. State S2 is reachable from S, if actions a2 or a3 were taken. Actually a2 is sure to reach S2, while a3 might reach S2 or S3 with different probabilities.", "The value V(s) of a state s, will be the probability of taking an action a, times the probability of arriving at state s\u2019, times the probability of getting a reward r, times the term (r + \ud835\udec4V(s\u2019)).But since at each stage we might have several options, meaning several actions to choose from, which leads to several possible states s\u2019 and several possible rewards r. We need to sum all these different options.", "The initial formula could be rewritten like the following.", "How this formula translates into code is rather straightforward.Each sum (\u03a3) translates into a loop, as depicted in the following picture.", "What the code shows might be strange for a developer. It loops over all actions in the action set A, and all states in the state set S, as well as all rewards in the reward set R.However, we know that there are times when not all actions are available at all states, and not all states are reachable from the current state, and of course not all types of rewards are available for every action and every state.So looping over all of these sets is simply a waste of effort and time.There are ways to optimize the code in order to improve performance.", "Looking at the figure above, it is easy to see the number of nested loops.Programmatically speaking, there are some optimization that can be done. There is no need to loop over all actions, all states and all rewards. But we will loop only over possible options and ignore unavailable ones.", "For example, not all the set of actions is present on all states, so instead of looping over A, we do it over actions that are available in that state such as state.getAvailableActions().Likewise, instead of the whole set of states S, we loop over the states reachable from the current one, state.getAccessibleStates(), and instead of all rewards R we loop over the possible rewards that are related to the current state and the selected action, getRewards(state, a).", "The following pseudo code creates an interface (IState) that describes the behaviour of the state. This interface will be used in the algorithms below.", "Class R represents a utility class that returns the reward given a state s and an action a. The function getReward(s, a) returns only one reward, whereas getPossibleRewards(s, a) returns a list of rewards.Function getRewardProbability(r, s, a) returns the probability of getting reward r when at state s and performing action a.", "In many cases, the reward system is simple, meaning there is only one reward, per state and action. So getReward(s, a) and getPossibleRewards(s, a) return the same reward and getRewardProbability(r, s, a) returns 1.", "Function computeStateValue(s) computes the value of a state s in the most general case, where we assume the reward system might not be simple and there might be several possible rewards for the same state/action pair.", "Function computeStateValueSinpleReward(s) computes the value of state s with the assumption that the reward system is simple. So no need to loop over possible rewards, but we simply call R.getReward(s, a), and the probability of getting it is 1.", "It is not hard to notice that the above code concerns the computation of one state. In order to compute the values of all states, we loop over all the state set S, as shown in the pseudo code function computeValueForAllStates()", "The implementation of the formula is straightforward. However by itself, this is not enough, because many of the parameters are not known in real-life case. For this reason, there are lots of other techniques that can be used to help estimate each component of this formula.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb538b6ef921a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zsalloum.medium.com/?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----b538b6ef921a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb538b6ef921a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb538b6ef921a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tine999?utm_source=medium&utm_medium=referral", "anchor_text": "Tine Ivani\u010d"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com"}, {"url": "https://unsplash.com/@diesektion?utm_source=medium&utm_medium=referral", "anchor_text": "Robert Anasch"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4", "anchor_text": "Math Behind Reinforcement Learning, the Easy Way"}, {"url": "https://towardsdatascience.com/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4", "anchor_text": "Math Behind Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/@zsalloum/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182", "anchor_text": "Reinforcement Learning Policy for Developers"}, {"url": "https://medium.com/p/9350e1523031", "anchor_text": "Q vs V in Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/p/1b7ed0c030f4", "anchor_text": "Math Behind Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac", "anchor_text": "Dynamic Programming in Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511", "anchor_text": "Monte Carlo in Reinforcement Learning, the Easy Way"}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "TD in Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----b538b6ef921a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b538b6ef921a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b538b6ef921a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/math?source=post_page-----b538b6ef921a---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb538b6ef921a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----b538b6ef921a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb538b6ef921a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----b538b6ef921a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb538b6ef921a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb538b6ef921a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b538b6ef921a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b538b6ef921a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b538b6ef921a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b538b6ef921a--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "845 Followers"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-reinforcement-learning-math-for-developers-b538b6ef921a&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}