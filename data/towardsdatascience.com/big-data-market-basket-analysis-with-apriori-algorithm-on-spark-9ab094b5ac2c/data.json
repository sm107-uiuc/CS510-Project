{"url": "https://towardsdatascience.com/big-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c", "time": 1683015934.270328, "path": "towardsdatascience.com/big-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c/", "webpage": {"metadata": {"title": "Big Data Market Basket Analysis with Apriori Algorithm on Spark | by Sergen Cansiz | Towards Data Science", "h1": "Big Data Market Basket Analysis with Apriori Algorithm on Spark", "description": "The probability of buying a particular product by customers may be based on several factors. Factors such as the customer\u2019s transaction history, demographic characteristics, and interests are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@sergencansiz/what-is-apriori-algorithm-and-how-it-works-c99a9513753d", "anchor_text": "\u201cWhat is \u201cApriori Algorithm\u201d and How It Works\u201d", "paragraph_index": 1}], "all_paragraphs": ["The probability of buying a particular product by customers may be based on several factors. Factors such as the customer\u2019s transaction history, demographic characteristics, and interests are examples of these. So, what if we don\u2019t have those factors. It means that the customer has not been to our store before and we don\u2019t have his/her data (not even gender). In such cases, there are methods that we can find the products which customer might buy. Usually, the data used in these methods belong to the other customers who made purchases before in the store. These methods take all transactions, control each item (product) in the transactions, and find the patterns that show which products are mostly bought in the same basket by customers. The methods that find these patterns are called Frequent Pattern Mining or Association Rule Mining [1].", "Apriori is one of the algorithms that is used for frequent pattern mining. In this article, I am going to explain how to apply the \u201cApriori Algorithm\u201d with Spark in Python. It is important to know how the \u201cApriori Algorithm\u201d works before you start applying the codes below. I am also going to explain the basics of Apriori but if you want to learn it in detail please read my previous article called \u201cWhat is \u201cApriori Algorithm\u201d and How It Works\u201d. In this article, will see the following parts,", "Short-Cut: You can find the source code of implementation of Apriori algorithm on python PySpark in my GitHub repository and you can use the module created for Spark.", "Suppose there is a greengrocer in New York city. This greengrocer is the biggest greengrocer in New York City and it has millions of customers in a month. The owner of the greengrocer buys tons of products just for one day. However, some days she can\u2019t sell all the products and throws them away because the products\u2019 lifetime expires. So, throwing away products causes lots of loss. Therefore, the owner of the greengrocer decided to innovate their marketing strategy. She wanted to hire a Data Scientist for her Greengrocer. This Data Scientist\u2019s duty is to create an algorithm that can find the products that customers might buy after they get the products into their basket. She was going to offer a discount on the products which the algorithm founds for every particular customer. Thanks to this algorithm, she plans to sell her products just in time. Great idea!Now, think that the data scientist that she hired is you, how would have you started?", "Apache Spark [2] is an open-source analytics engine that focuses on speed, ease in use, and distributed system. It has the capability to run machine learning algorithms 100 times faster than undistributed systems [3]. If there is a massive data set Spark would have been the best option to analyze that data set. Okay, let\u2019s go back to our example: You will have to deal with millions of transaction data by considering that the Greengrocer has millions of customers in a month. In order to provide faster data analysis, one of the best options might be Spark.", "First, you should do is preparing a data set over the customers\u2019 transactions. But, you shouldn\u2019t forget that one customer might buy one or more than one item from the greengrocer, it means our data-set can\u2019t have a certain number of columns for each row. Therefore, I decided to write transactions line by line and separate each item in the transaction by comma and save as a \u201ctxt or CSV\u201d file. Each row represents each transaction with comma-separated items. For example, at the first transaction customer had bought Apple, Mango, and Banana and at the second transaction, the customer had bought Banana and Mango. If you have data-set as CSV you can also use that (It is already comma-separated, but do not forget to remove the header line).", "Before starting to implement Apriori on Spark we need to understand the main terms and concepts of it. Apriori algorithm depends on the frequencies of the item set. It creates different tables that include combinations of items. It scans the main dataset that shows all transactions and finds frequencies by considering how many time these combinations occurs in main data-set [4]. These frequencies are called support values in the Apriori algorithm. Also, the number of tables varies according to the maximum item set length. If the maximum item-set length is 5, it means there will be 5 support value tables. However, there is min support value that is used for deciding which item-set should stay in the table. Let\u2019s say min support values have been decided as 2. So, If any item-set occurs in the main data-set less than 2 times, we should remove that item-set from the table.", "Let\u2019s inspect the following figure in order to understand how Apriori works. I took a few transaction records in the following example figure in order to make it simple and understandable.", "\u201cDataset\u201d represents our transaction data and each row in the \u201cDataset\u201d shows each transaction item-set that has been bought at the same time by a customer. There are single item frequencies in Table A. This is the first table that we need to create for the Apriori algorithm. After Table A, there is Table B which includes binary combinations of single items. The most important part is here that the order of items doesn\u2019t make sense. So, \u201cApple, Mango\u201d and \u201cMango, Apple\u201d is the same thing. Therefore, while generating combinations of items, replications of item-sets have to be removed. As you can see from Table B, the item-set of \u201cBanana-Coconut\u201d and \u201cMango-Coconut\u201d whose support values less than 2 has been removed. However, there are triple combinations of single items in Table C and support values that show how many times occur in the main data-set. Item-sets whose support values less than 2 also removed from this table. If you see Table D there is only one item-set whose support value is 0 and it has four single items. This table shouldn\u2019t be taken into account while applying the algorithm. Because there isn\u2019t such a transaction in the main data-set.", "So, how can we calculate the probability of buying Mango if a customer bought apple by using these support tables in the above figure? In order to calculate that probability, we need to take the support value of \u201c Mango \u201d and \u201cMango, Apple\u201d together. The support value of \u201cMango\u201d is 4 and \u201cMango, Apple\u201d is 3. We can calculate the probability; (3 / 4) * 100 = %75. This probability is called Confidence Value in Apriori. The interpretation of this value is Customers who buy Mango might buy Apple with a %75 confidence value.", "Now, we know how to calculate support and confidence value. We are clear to go into the Apriori algorithm with PySpark. In the following section we will see;", "PySpark works on RDDs, it means that we need to convert all transactions records to multiple RDDs. Before doing that we need to be sure SparkContext is created in order to define each record of RDDs and read the CSV file.", "Before you start this section please make sure Spark and PySpark are installed on your system.", "First, create a python file in your working directory called \u201capprioriSpark.py\u201d (or any name you want), and then you can easily create a SparkContext by the command below (You should also import SparkContext from PySpark).", "After SparkContext has been created we can read the data by using the textFile() method that comes in SparkContext. Please make sure that your data file is in the same directory as your python file.", "The default textFile() method reads file line by line, it means every line in our CSV file will be a value in RDD. These RDDs include CSV lines as a single string value (Result at 6th line). That\u2019s why we need to map into every RDD and split these single values by comma in order to obtain every item in lines and split them into an RDD array. In order to do that run;", "Now, you can clearly see that every line in the transaction data-set is separated by a comma. And every array in our RDD represents the item lists those are customers\u2019 transactions. We successfully red the file and converted it to a Spark RDD object by considering each transaction that belongs to customers.", "Apriori algorithm depends on the frequencies of items. Because of that first, we need to obtain frequencies for every single item. These frequencies will be our first support values in table one (as mentioned in the previous section). In order to that, we need to extract every item in RDDs to whole items of the array. We can do that by using the \u201cflatMap\u201d method.", "As you can see from the result; All our transaction items are in a single array. Now we can calculate each unique item\u2019s frequency. Do not forget that these frequencies will be our first supports values. If we would be working on a \u201cNumPy\u201d array, finding frequencies would be easy. But, we work on RDDs and that\u2019s why we need to find a way to obtain frequencies by considering the \u201cMapReduce\u201d approach. Solution; First we can convert every item to a \u201ctuple\u201d object and add \u201c1\u201d as a second item of \u201ctuple\u201d. We can sum these values by using the \u201creduceByKey\u201d (It is like the groupby method in SQL) method. By summing tuple\u2019s second numbers we can get every unique item\u2019s frequency (how many time occurs on customers\u2019 transactions). We will also need to list unique items in the feature sections. So, we can also obtain unique items by using the \u201cdistinct\u201d method.", "If you run \u201csupportRdd.collect()\u201d after the above code snippet, you get the first item -support tuples (assume as Table A). As you can see from support values, \u201cApple\u201d has more frequency than the others. It means the probability of \u201cApple\u201d purchases is greater than the others. These support values are gotten by considering each item separately. We also need to consider how they occur together in transactions. Thanks to that we can calculate confidence values over the Apriori algorithm. We will see these steps in the next sessions.", "In order to decide which item-sets will stay in the support tables, we need to define a min support value. We can choose min support value as the minimum frequency that is in the first support values array (table). If any support values in item set arrays are less than min support value, we should remove that item set from that array. If our data hasn\u2019t got many records, min support might be 1. In such cases, we can define min support as 2 or any value more than 1.", "In this example, we have a minimum support value that has been found as 1. To get a more consistent result we can set min support as 2. It means that if any item or item set occurs in a transaction less than 2 times we won\u2019t take it into account. As you can see from line 8 we filtered our first item set table (which is single item support values) according to min support value. We also created a \u201cbaseRdd\u201d object. \u201cbaseRdd\u201d represents our first support values for every single item. This object will be updated with upcoming support values of combinations. We also need to define \u201csupportRdd\u201d that shows only items without support values. We will use it in the next sections in order to create combinations of items.", "We only found each item\u2019s support value (frequencies) until this section. Now, we will create an algorithm that generates combinations of items in a while loop. There will be different support tables that will be created in every loop. This while loop will end when there isn\u2019t any combination whose support value more than min support value. This algorithm will control how many times these combinations of items occur together in transaction data-set in every loop and save it in an RDD.", "We already created a \u201csupportRdd\u201d object which only includes the first table\u2019s item set without support values (which only have one item in each row). Now, we will use this RDD in a while loop to combine it with unique items in order to create other support tables. However, this RDD will be updated after every loop. For example;", "The algorithm will filter every combination table according to min support value. When there is no item set while loop will end. Besides, we also need to define a function that can find replications inside the combined item set. As mentioned before; sets of (Apple, Mango) and (Mango, Apple) are the same thing for the Apriori algorithm. Because of that, we need to find such patterns and remove one of them. As you can see from the below code snippet, there is a function called \u201cremoveReplica\u201d. This function removes such duplicated items after a combination and returns only one of them.", "Yeap! We are ready to create our while loop. First, we need to define a variable that we can control item-set length in each loop. It is represented as \u201cc\u201d in the above code snippet. This \u201cc\u201d variable starts from 2. Why? Remember that we already created our first support table as \u201csupportRdd\u201d. So, in the while loop support table will start from c=2. It means the first support table that is going to be created in while loop will have item-sets which has 2 items. In order to create combinations of items, we can use the \u201ccartesian\u201d function that comes with PySpark (at line 6). It is created we will remove duplicated items (at line 7). We will use the \u201cc\u201d variable in order to filter combined items more than the \u201cc\u201d variable (at line 9). However, we also use the \u201cdistinct\u201d method to get a unique item-set just in case.", "As you can see there are two objects created in each loop as \u201ccombined\u201d and \u201ccombined_2\u201d. Combined already explained above. \u201cCombined_2\u201d, on the other hand, is a combination of the \u201ccombined\u201d variable and each line of the whole data set (each transaction). As you can see from line 14, there is a filtering process that controls each item-set whether item-sets of \u201ccombined\u201d are in the data-set. If there is not any such item-set in the data-set it removes that item-set. After all, we also get frequencies by using the \u201creduceByKey\u201d method and filter them according to min support value. At line 21, we add the final support table to \u201cbaseRDD\u201d which has our all support values with item-sets (in theory it is called table). However, we also need to update the \u201csupportRdd\u201d variable with \u201ccombined_2\u201d without support values. This process will go on until there isn\u2019t left any item-set in \u201csupportRdd\u201d.", "Finally, we can calculate the confidence values. You can use the below codes for calculating confidence values for each combination of baseRdd (whole item-set combinations). There is a \u201cFilter\u201d class that can filter data according to confidence calculation. It also includes the method as \u201ccalculateConfidence\u201d that calculates confidences.", "If you run baseRddConfidence.collect() you can get all confidence values. You can also filter the result which is greater than certain confidence. A few examples from the result is shown below;", "The first item-set in arrays shows the products that customers bought and the second shows customers might buy if they bought products in the first item-set. The last element of the array shows the confidence value for that pattern. For example, a customer who buys Mango might buy Apple with %58 confidence. Another example; customer who buys Mango and Banana might buy Apple with 66.6% confidence. If you look carefully at the first two arrays [\u201cMango\u201d, \u201cApple\u201d ] and [\u201cApple\u201d, \u201cMango\u201d] have different confidence values. Let\u2019s write the support formula for [\u201cMango\u201d => \u201cApple\u201d] (confidence of buying Apple after Mango)", "Support of [\u201cMango\u201d, \u201cApple\u201d] is 7, support of [\u201cMango\u201d] is 10, and support of [\u201cApple\u201d] is 12. Although the frequency (support) of [\u201cApple\u201d \u201cMango\u201d] is the same in both calculations, they have different confidence values \u200b\u200bbecause the frequency of \u201cApple\u201d and the frequency of \u201cMango\u201d are different.", "You can also convert the result pandas data-frame by using the below codes;", "In this article, we learned what is Frequent Pattern Mining and how to apply it over the Apriori algorithm. We took an example case with a data-set and applied the Apriori algorithm on PySpark from scratch. There are also other FPM methods that you can check out such as FPGrowth, Eclat, and so on. After understanding Apriori you can easily understand how other methods work. If you have any questions please feel free to ask.", "Data Scientist, Statistician, Python and R Developer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9ab094b5ac2c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://sergencansiz.medium.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Sergen Cansiz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a38abffb19e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&user=Sergen+Cansiz&userId=5a38abffb19e&source=post_page-5a38abffb19e----9ab094b5ac2c---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ab094b5ac2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----9ab094b5ac2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9ab094b5ac2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&source=-----9ab094b5ac2c---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@jordanmadrid", "anchor_text": "Jordan Madrid"}, {"url": "https://unsplash.com/?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@sergencansiz/what-is-apriori-algorithm-and-how-it-works-c99a9513753d", "anchor_text": "\u201cWhat is \u201cApriori Algorithm\u201d and How It Works\u201d"}, {"url": "https://github.com/sergencansiz/apriori-pyspark", "anchor_text": "sergencansiz/apriori-pysparkThis module is developed to run apriori algorithm on RDD based pyspark. Until feature update, data should be presented\u2026github.com"}, {"url": "https://en.wikipedia.org/wiki/Frequent_pattern_discovery", "anchor_text": "[1] Frequent Pattern Mining https://en.wikipedia.org/wiki/Frequent_pattern_discovery"}, {"url": "https://spark.apache.org/", "anchor_text": "https://spark.apache.org/"}, {"url": "https://databricks.com/spark/about", "anchor_text": "https://databricks.com/spark/about"}, {"url": "https://en.wikipedia.org/wiki/Apriori_algorithm", "anchor_text": "https://en.wikipedia.org/wiki/Apriori_algorithm"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9ab094b5ac2c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9ab094b5ac2c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/big-data-analytics?source=post_page-----9ab094b5ac2c---------------big_data_analytics-----------------", "anchor_text": "Big Data Analytics"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----9ab094b5ac2c---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/market-basket-analysis?source=post_page-----9ab094b5ac2c---------------market_basket_analysis-----------------", "anchor_text": "Market Basket Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ab094b5ac2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----9ab094b5ac2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ab094b5ac2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----9ab094b5ac2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9ab094b5ac2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a38abffb19e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&user=Sergen+Cansiz&userId=5a38abffb19e&source=post_page-5a38abffb19e----9ab094b5ac2c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6b5ddf93b82a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&newsletterV3=5a38abffb19e&newsletterV3Id=6b5ddf93b82a&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----9ab094b5ac2c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Written by Sergen Cansiz"}, {"url": "https://sergencansiz.medium.com/followers?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "414 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a38abffb19e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&user=Sergen+Cansiz&userId=5a38abffb19e&source=post_page-5a38abffb19e----9ab094b5ac2c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6b5ddf93b82a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbig-data-market-basket-analysis-with-apriori-algorithm-on-spark-9ab094b5ac2c&newsletterV3=5a38abffb19e&newsletterV3Id=6b5ddf93b82a&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----9ab094b5ac2c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1?source=author_recirc-----9ab094b5ac2c----0---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=author_recirc-----9ab094b5ac2c----0---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=author_recirc-----9ab094b5ac2c----0---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Sergen Cansiz"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9ab094b5ac2c----0---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1?source=author_recirc-----9ab094b5ac2c----0---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "5 Things You Should Know About CovarianceWhen dealing with problems on statistics and machine learning, one of the most frequently encountered terms is covariance. While most of\u2026"}, {"url": "https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1?source=author_recirc-----9ab094b5ac2c----0---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "\u00b77 min read\u00b7Mar 25, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F26b12a0516f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-things-you-should-know-about-covariance-26b12a0516f1&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----26b12a0516f1----0-----------------clap_footer----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1?source=author_recirc-----9ab094b5ac2c----0---------------------32736610_e209_4a68_acff_e964881087e2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "15"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F26b12a0516f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-things-you-should-know-about-covariance-26b12a0516f1&source=-----9ab094b5ac2c----0-----------------bookmark_preview----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9ab094b5ac2c----1---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----9ab094b5ac2c----1---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----9ab094b5ac2c----1---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9ab094b5ac2c----1---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9ab094b5ac2c----1---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9ab094b5ac2c----1---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9ab094b5ac2c----1---------------------32736610_e209_4a68_acff_e964881087e2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----9ab094b5ac2c----1-----------------bookmark_preview----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9ab094b5ac2c----2---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----9ab094b5ac2c----2---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----9ab094b5ac2c----2---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9ab094b5ac2c----2---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9ab094b5ac2c----2---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9ab094b5ac2c----2---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9ab094b5ac2c----2---------------------32736610_e209_4a68_acff_e964881087e2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----9ab094b5ac2c----2-----------------bookmark_preview----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/multivariate-outlier-detection-in-python-e946cfc843b3?source=author_recirc-----9ab094b5ac2c----3---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=author_recirc-----9ab094b5ac2c----3---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=author_recirc-----9ab094b5ac2c----3---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Sergen Cansiz"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9ab094b5ac2c----3---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/multivariate-outlier-detection-in-python-e946cfc843b3?source=author_recirc-----9ab094b5ac2c----3---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "Multivariate Outlier Detection in PythonMultivariate Outliers and Mahalanobis Distance in Python"}, {"url": "https://towardsdatascience.com/multivariate-outlier-detection-in-python-e946cfc843b3?source=author_recirc-----9ab094b5ac2c----3---------------------32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": "\u00b75 min read\u00b7Mar 20, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe946cfc843b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-outlier-detection-in-python-e946cfc843b3&user=Sergen+Cansiz&userId=5a38abffb19e&source=-----e946cfc843b3----3-----------------clap_footer----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/multivariate-outlier-detection-in-python-e946cfc843b3?source=author_recirc-----9ab094b5ac2c----3---------------------32736610_e209_4a68_acff_e964881087e2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe946cfc843b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-outlier-detection-in-python-e946cfc843b3&source=-----9ab094b5ac2c----3-----------------bookmark_preview----32736610_e209_4a68_acff_e964881087e2-------", "anchor_text": ""}, {"url": "https://sergencansiz.medium.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "See all from Sergen Cansiz"}, {"url": "https://towardsdatascience.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Arthur Mello"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Exploratory Data Analysis: The Ultimate WorkflowExplore the true potential of your data with Python"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "\u00b716 min read\u00b7Apr 20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fa82b1d21f747&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fexploratory-data-analysis-the-ultimate-workflow-a82b1d21f747&user=Arthur+Mello&userId=9d32d5e0ac40&source=-----a82b1d21f747----0-----------------clap_footer----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82b1d21f747&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fexploratory-data-analysis-the-ultimate-workflow-a82b1d21f747&source=-----9ab094b5ac2c----0-----------------bookmark_preview----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----9ab094b5ac2c----1-----------------bookmark_preview----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Erdogan Taskesen"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "From Data to Clusters; When is Your Clustering Good Enough?Sensible clusters and hidden gems can be found using clustering approaches but you need the right cluster evaluation method!"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "\u00b717 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&user=Erdogan+Taskesen&userId=4e636e2ef813&source=-----5895440a978a----0-----------------clap_footer----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----9ab094b5ac2c----0---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&source=-----9ab094b5ac2c----0-----------------bookmark_preview----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Albers Uzila"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Wanna Break into Data Science in 2023? Think Twice!It won\u2019t be smooth sailing for you"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "\u00b711 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&user=Albers+Uzila&userId=159e5ce51250&source=-----26842e9a87fe----1-----------------clap_footer----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----9ab094b5ac2c----1---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&source=-----9ab094b5ac2c----1-----------------bookmark_preview----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----9ab094b5ac2c----2---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://zachl-quinn.medium.com/?source=read_next_recirc-----9ab094b5ac2c----2---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://zachl-quinn.medium.com/?source=read_next_recirc-----9ab094b5ac2c----2---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Zach Quinn"}, {"url": "https://medium.com/pipeline-a-data-engineering-resource?source=read_next_recirc-----9ab094b5ac2c----2---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Pipeline: A Data Engineering Resource"}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----9ab094b5ac2c----2---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble.3 work samples that got my foot in the door, and 1 that almost got me tossed out."}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----9ab094b5ac2c----2---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "\u00b77 min read\u00b7Aug 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpipeline-a-data-engineering-resource%2Ff376682b4e21&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fpipeline-a-data-engineering-resource%2F3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21&user=Zach+Quinn&userId=4364d136bdda&source=-----f376682b4e21----2-----------------clap_footer----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----9ab094b5ac2c----2---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "46"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff376682b4e21&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fpipeline-a-data-engineering-resource%2F3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21&source=-----9ab094b5ac2c----2-----------------bookmark_preview----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----9ab094b5ac2c----3---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/?source=read_next_recirc-----9ab094b5ac2c----3---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/?source=read_next_recirc-----9ab094b5ac2c----3---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Leihua Ye, PhD"}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----9ab094b5ac2c----3---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "Why Data Scientists Should Learn Causal InferenceClimb up the ladder of causation"}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----9ab094b5ac2c----3---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": "\u00b77 min read\u00b7Jul 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fa70c4ffb4809&operation=register&redirect=https%3A%2F%2Fleihua-ye.medium.com%2Fwhy-data-scientists-should-learn-causal-inference-a70c4ffb4809&user=Leihua+Ye%2C+PhD&userId=4e1d06dd743&source=-----a70c4ffb4809----3-----------------clap_footer----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----9ab094b5ac2c----3---------------------fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa70c4ffb4809&operation=register&redirect=https%3A%2F%2Fleihua-ye.medium.com%2Fwhy-data-scientists-should-learn-causal-inference-a70c4ffb4809&source=-----9ab094b5ac2c----3-----------------bookmark_preview----fa3ce4db_8ba7_4da8_b851_659dd6ed95e3-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----9ab094b5ac2c--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}