{"url": "https://towardsdatascience.com/the-magic-behind-embedding-models-c3af62f71fb", "time": 1683002044.799798, "path": "towardsdatascience.com/the-magic-behind-embedding-models-c3af62f71fb/", "webpage": {"metadata": {"title": "The Magic Behind Embedding Models | by Mohamed Gharibi | Towards Data Science", "h1": "The Magic Behind Embedding Models", "description": "Embeddings are types of knowledge representation where each textual variable is represented with a vector (think about it as a list of numbers for now). A textual variable could be a word, node in a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://kawine.github.io/", "anchor_text": "Kawin Ethayarajh", "paragraph_index": 5}], "all_paragraphs": ["Embeddings are types of knowledge representation where each textual variable is represented with a vector (think about it as a list of numbers for now). A textual variable could be a word, node in a graph or a relation between two nodes in a knowledge graph. These vectors can be called different names such as space vectors, latent vectors, or embedding vectors. These vectors represent multidimensional feature space on which machine learning methods can be applied. Therefore, we need to make a shift in how we think about language from a sequence of words to points that occupy a high-dimensional semantic space where points in space can be close together or far apart.", "The purpose of this representation is to get words with similar meanings (semantically related) to have a similar representation and be closer to each other after plotting them in a space. Why is that important? Well, for many reasons, mainly:", "For the sake of simplicity, let us start with this example, consider the words \u201cking\u201d, \u201cqueen\u201d, \u201cman\u201d, and \u201cwoman\u201d are represented with the vectors [9, 8, 7], [5, 6, 4], [5, 5, 5], and [1, 3, 2] respectively. Figure (1) depicted these vectors representation. Notice that the word \u201cking\u201d and the word \u201cman\u201d are semantically related in a way that both \u201cman\u201d and \u201cking\u201d represent a male human. However, the word \u201cking\u201d has an extra feature which is royalty. Similarly, the word \u201cqueen\u201d is similar to \u201cwoman\u201d but has an extra feature which is royalty as well.", "Since the relation between \u201cking\u201d to \u201cqueen\u201d (male royalty - female royalty) is similar to the relation between \u201cman\u201d and \u201cwoman\u201d (male human - female human)then subtracting them from each other gives us this famous equation: (king - queen = man - woman). By the way, when subtracting two words from each other, we subtract their vectors.", "Suppose we do not know what is the female name for \u201cking\u201d, so how can we get it? Well, since we know that (king - queen = man - woman), we change the formula to be (queen = king - man + woman) which makes sense. The formula states if you remove the male gender from \u201cking\u201d (royalty is the reminder) then add the female gender to royalty to give us what we are looking for which is \u201cqueen\u201d.", "Image is taken from (Kawin Ethayarajh), Why does King - Man + Woman = Queen? Understanding Word Analogies", "Now we know embedding can be helpful in question answering systems. Other examples may be similar (USA - English = France - French), (Germany - Berlin = France - Paris). Moreover, embeddings are also helpful in simple recommendation tasks. For example, if someone likes \u201corange\u201d, then we look at the most similar vectors to the vector that represents \u201corange\u201d and we get the vectors for \u201capple\u201d, \u201ccherry\u201d, and \u201cbanana\u201d. As we can see, the better representation (list of numbers) we get for each word, the better accuracy our recommendation system gets. So the reminding question is how do we come up with this list of numbers for each word? (which is called embedding, latent or space vector).", "There are three main categories and we will discuss them one by one:", "Word2vec is one of the earliest vectors that is mainly to embed words rather than sentences or books. Moreover, the dimension of Word2vec is not related to the number of words in the training data since it uses some algorithms to reduce the dimensions into (50, 100, 300, etc.). Word2vec falls under prediction based embeddings which tend to predict a word in a given context. Word2vec has two flavors: Continuous Bag Of Words (CBOW) and Skip-Gram model. CBOW tend to predict the probability of a word given a context, whereas skip-Gram uses opposite CBOW architecture (predict a context given a single word).", "We start by specifying a context window size which is the beginning and ending for each context. Then we get the One Hot Encoding vectors for each word. Given the corpus \u201cI like driving fast cars\u201d, the window size is 1 (1 word before and one word after the target word), vector dimension is 3 and we want to predict the middle word \u201cdriving\u201d from the context \u201cI \u2026\u2026. driving\u201d. Notice that we have only 1 hidden layer where its size associated with the required vector dimension, that is the reason for calling this technique learning the representations of vectors because we have only 1 hidden layer. Bellow is the architecture, note that the input are the words in the context window size and the output is the learning the representation for the target word. Also note that there are no activation function applied to the hidden layer. However, the output layer utilizes Softmax.", "The output of the previous neural network is the following weight matrix:", "After having the weight matrix we multiply the matrix with the One Hot Encoding vector for the target word to get its representation vector.", "You may as why are we multiplying the weight matrix with a vector filled with zero\u2019s with a single 1. Of course, the output is just its position in the matrix, consider the following example:", "Well, the real purpose of this multiplication is just to lookup the target word vector based on its space in the One Hot Encoding vector.", "Skip-Gram or sometimes called Skip-Ngram model uses a headed flipped architecture of CBOW and the rest are the same. Below is the architecture for skip-gram where we try to predict all the words within a window size given one context word:", "GloVe is a word embedding model that is trained on the co-occurrence matrix counts. It use the corpus statistics by minimizing least square error in order to obtain the word vector space.", "Given a corpus having V words, our co-occurrence matrix X will be of size VxV where each word i in X is a unique word in the corpus and each word j denotes to the number of times occurred in the window size of word i. Given this sentence \u201cthe dog ran after the man\u201d and a window size 1, we get the following matrix:", "Let us start with this simple formula:", "Where P_ij refer to the probability of word j that appear in the context of word i. The formula denotes this probability where X_ij is the number of times j appeared in context of i and X_i is the total number of words appeared in context of i.", "Moreover, we need a function F that takes the input embeddings for the words i, k, j (where k is the index of a context vector) and compute their output embeddings (expressed as w and w~ consequently). The main goal for GloVe was building meaningful embeddings using simple arithmetic operation which is making the input of F the difference between the vectors i and j:", "With that being said, we still have a simple issue in our previous formula and that is the left hand side of the formula is a vector whereas the right hand side is just a scalar. To fix this issue mathematically, we use the dot product between the transpose of (w_i, w_j) and (w_k) to get the following:", "While the distinction between context words and standard words is arbitrary in our co-occurrence matrix, we can replace the probabilities in our formula with their ratios:", "Still one question, which is what is the function F could be? Let us say it the exp() function. Then we solve for:", "Move log(X_i) to the left hand side:", "To add a bias to our equation, we substitute log(X_i) to get:", "Finally, GloVe efficiency relies on minimizing a linear regression function as its loss function.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D. candidate interested in Machine Learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc3af62f71fb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gharibimo?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "Mohamed Gharibi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F532ac9d36827&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&user=Mohamed+Gharibi&userId=532ac9d36827&source=post_page-532ac9d36827----c3af62f71fb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3af62f71fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3af62f71fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@foxfox", "anchor_text": "Natalia Y"}, {"url": "https://kawine.github.io/", "anchor_text": "Kawin Ethayarajh"}, {"url": "https://kawine.github.io/", "anchor_text": "Kawin Ethayarajh"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation"}, {"url": "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795", "anchor_text": "Word embeddings: exploration, explanation, and exploitation (with code in Python)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c3af62f71fb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c3af62f71fb---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----c3af62f71fb---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----c3af62f71fb---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c3af62f71fb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3af62f71fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&user=Mohamed+Gharibi&userId=532ac9d36827&source=-----c3af62f71fb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3af62f71fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&user=Mohamed+Gharibi&userId=532ac9d36827&source=-----c3af62f71fb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3af62f71fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc3af62f71fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c3af62f71fb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c3af62f71fb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c3af62f71fb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c3af62f71fb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mohamed Gharibi"}, {"url": "https://medium.com/@gharibimo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F532ac9d36827&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&user=Mohamed+Gharibi&userId=532ac9d36827&source=post_page-532ac9d36827--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F532ac9d36827%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-c3af62f71fb&user=Mohamed+Gharibi&userId=532ac9d36827&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}