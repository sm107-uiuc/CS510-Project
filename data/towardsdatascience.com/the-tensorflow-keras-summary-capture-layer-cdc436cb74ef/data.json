{"url": "https://towardsdatascience.com/the-tensorflow-keras-summary-capture-layer-cdc436cb74ef", "time": 1683016684.325826, "path": "towardsdatascience.com/the-tensorflow-keras-summary-capture-layer-cdc436cb74ef/", "webpage": {"metadata": {"title": "The TensorFlow Keras Summary Capture Layer | by Chaim Rand | Towards Data Science", "h1": "The TensorFlow Keras Summary Capture Layer", "description": "In previous posts, I have told you about how my team at Mobileye, (officially known as Mobileye, an Intel Company), has tackled some of the challenges that came up, while using TensorFlow to train\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/tensorflow-performance-analysis-314b56dceb59", "anchor_text": "performance profiling", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/debugging-in-tensorflow-392b193d0b8", "anchor_text": "debugging", "paragraph_index": 0}, {"url": "https://www.tensorflow.org/tensorboard", "anchor_text": "TensorBoard", "paragraph_index": 2}, {"url": "https://www.tensorflow.org/api_docs/python/tf/summary", "anchor_text": "tf.summary", "paragraph_index": 3}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer", "anchor_text": "tf.keras.Layer", "paragraph_index": 4}, {"url": "https://www.tensorflow.org/api_docs/python/tf/estimator/SummarySaverHook", "anchor_text": "tf.estimator.SummarySaverHook", "paragraph_index": 7}, {"url": "https://www.tensorflow.org/guide/keras/train_and_evaluate", "anchor_text": "tf.keras.model.fit()", "paragraph_index": 8}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard", "anchor_text": "TensorBoard callback", "paragraph_index": 8}, {"url": "https://www.tensorflow.org/guide/eager", "anchor_text": "eager", "paragraph_index": 9}, {"url": "https://www.tensorflow.org/guide/effective_tf2", "anchor_text": "TensorFlow documentation", "paragraph_index": 10}, {"url": "https://www.tensorflow.org/guide/function", "anchor_text": "improve your runtime performance", "paragraph_index": 11}, {"url": "https://www.tensorflow.org/api_docs/python/tf/function", "anchor_text": "tf.function", "paragraph_index": 11}, {"url": "https://www.tensorflow.org/guide/effective_tf2#functions_not_sessions", "anchor_text": "TensorFlow documentation", "paragraph_index": 12}, {"url": "https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_eager_execution", "anchor_text": "tf.compat.v1.disable_eager_execution", "paragraph_index": 15}, {"url": "https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough", "anchor_text": "customized training loop", "paragraph_index": 18}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/Model#train_step", "anchor_text": "train_step", "paragraph_index": 18}, {"url": "https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://www.tensorflow.org/api_docs/python/tf/function", "anchor_text": "tf.function", "paragraph_index": 19}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/models/model_from_json", "anchor_text": "JSON model configuration", "paragraph_index": 28}, {"url": "http://wiki.mobileye.com/mw/index.php/Debugging_in_TensorFlow", "anchor_text": "previous post", "paragraph_index": 30}], "all_paragraphs": ["In previous posts, I have told you about how my team at Mobileye, (officially known as Mobileye, an Intel Company), has tackled some of the challenges that came up, while using TensorFlow to train deep neural networks. In particular, I have covered topics such as performance profiling, and debugging. This post addresses an additional component of training machine learning models, that of monitoring the learning process.", "Monitoring the learning process is an important, and often time-consuming, part of DNN training, during which we track a variety of tensors, metrics and statistics, in order to understand how our training is progressing, and figure out what improvements to make to model architecture and/or hyperparameters.", "One of the most popular ways of doing this when training in TensorFlow, is using TensorBoard. TensorBoard supports a variety of methods for tracking and debugging the training process. On our team, we rely heavily on TensorBoard. We track losses, generate gradient histograms, and measure activation outputs. We log metrics, display confusion matrices, and generate visual images from the output data. Sometimes we use it debug intermediate operations performed by the loss function, or to measure the distribution of weights on a specific layer in the graph. We use TensorBoard extensively.", "The way to use TensorBoard is by inserting \u201csummary\u201d commands into the training code using the tf.summary module. In version 2, TensorFlow made significant revisions to the training flow, and, in particular, to the tf summary mechanism. While this revision simplified some of the straightforward usages of tf.summary, some of the more advanced usages are not immediately obvious. For example, it is not at all clear (to me, at least), how TensorFlow intended for me to be able to log graph (non-eager) tensors, such as the distribution of the output of a specific activation layer within my graph, or the value of an arbitrary tensor within my loss function. (If you know, please don\u2019t hesitate to share :).)", "My intention, in this post, is to propose a solution for capturing summaries from graph tensors, that relies on the creation of a custom tf.keras.Layer. Along the way, I will provide a bit more insight into the problem, as well as some of the alternative solutions we considered. If you are eager to get straight to the custom summary capture layer, feel free to skip ahead.", "As TensorFlow and TensorBoard continue to undergo changes and improvements, it is important to note that my story is based on trials that I ran using TensorFlow version 2.3.0.", "The following brief history is partial, in the sense that it is incomplete, and that it is based on my own experience.", "In version 1 of TensorFlow, we would freely scatter tf.summary ops on arbitrary tensors across our code, and would use the tf session.run() function to record the associated tensor values to event files, at predetermined intervals. As our use of TensorBoard evolved, we would also use it to record the results of general python calculations that we would make based on the graph output, such as image visualizations of model predictions overlaid on an input frame, or visualizations of the confusion matrix. Since summaries needed to be graph ops, we would do this by creating small auxiliary graphs that would define a summary op on a numpy array (such as an image). Later, we adopted the high level tf.estimator APIs, which included controls for setting the summary interval and handled the session.run calls on the summary ops for us. We also integrated the use of the estimator hooks, such as the tf.estimator.SummarySaverHook and other custom hooks, to support our wide variety of usages. Life was grand.", "Along came tf.keras with its own unique blueprint for the training flow, and its own high level training API tf.keras.model.fit(). The model.fit() function did not include support for managing tf.summarys, requiring a bit more manual labor. But we quickly worked out how to enable all of our TensorBoard usages by creating custom tf.keras.callbacks, and directly configuring the execution function to collect the desired tensors, using all kinds of voodoo involving model._fit_function.fetches and model._fit_function.fetch_callbacks. No, it wasn\u2019t pretty, but it worked. The tf.keras API also introduced a TensorBoard callback, which supported capturing a variety of popular training monitoring metrics.", "Then came TensorFlow 2. Once again, the blueprint for the training flow changed, the use of the model._fit_function became obsolete, and the mechanism for recording tensors was revised. As described by TensorFlow, the new mechanism does have its advantages. Previously, recording tensors was a two step process. Now, a call to tf.summary would result in immediate recording of the tensor to the event file. In particular, this meant that the tf summary ops were no longer built into the TensorFlow computation graph. In addition, the tf.summary call could only be called on eager tensors (or raw numpy arrays). More on this in the next section.", "While a full understanding of the changes that were introduced in TensorFlow 2 is beyond the scope of this post, I would like to touch on a few of the details that pertain to the topic of collecting summaries. To get the full picture, I refer you to the TensorFlow documentation.", "While in tf1, the default execution mode is graph mode, a symbolic execution mode in which users define an abstract syntax tree and execute it using the TensorFlow session.run() call, TensorFlow v2 applications run eagerly. Tensors that are created within the eager execution scope, are called eager tensors, and can be accessed freely. To improve your runtime performance you can configure functions to run in graph mode by applying the tf.function qualifier to them. This is the recommended (most performant) way to configure your training step when performing training, and is what happens automatically if you rely on the high level model.fit() training API. This means that, unless you program your training step to run in (the slower) eager execution mode, all of the tensors that you defined as part of your model, will be graph (non-eager) tensors. This includes intermediate layer inputs and outputs, and intermediate tensors in your loss function. The exceptions are tensors that are defined as outputs of the tf.function, which will be eager tensors. As mentioned above, the tf2 tf.summary ops will not work on graph tensors. The TensorFlow recommendation is to debug in eager execution mode, and to perform training in graph mode. Since the tf.summaries are required for monitoring tensors, specifically during the training process, we need a solution for extracting the values of these graph tensors.", "In tf1, the underlying mechanism for extracting the values of graph tensors was the tf.session object. We would provide the session.run() function with a list of graph operations and input values, and receive as output the values of the corresponding tensors. In tf2, the sessions have been replaced by functions, specifically tf.funcions when running in graph mode. Here is the change as demonstrated in code by the TensorFlow documentation.", "One useful property of the session.run() method was that we had quite a bit of freedom in determining the list of input ops, and thus the list of collecting tensor values. This was particularly useful for extracting summaries. At every, predetermined summary step, we could simply expand the list of input ops to include the tensors of interest. The way to do this when using tf.functions, in tf2, is not as straightforward. Since the output of a tf.function is fixed, we would need to be able to define multiple tf.functions with varying output definitions, and toggle between the tf.functions based on the training step. Or, we could just use a single tf.function that outputs the superset of all tensors of interest. But this might come at a considerable performance cost.", "The main goal of this post is to propose a solution for extracting graph tensors that is based on defining a custom tf.keras layer. But before we get into that, let\u2019s review a couple of alternative solutions.", "One, relatively easy, way to solve this is to simply disable the eager execution by calling tf.compat.v1.disable_eager_execution at the beginning of your script. As stated in the documentation, this function is intended for projects for which the migration from tf1 to tf2 proves difficult. When this function is used, the training loop essentially falls back to the legacy, tf1, training loop. In particular, this means that all the voodoo we described above for extracting graph tensors in tf.keras v1, can be carried over to tf2.", "The feeling on our team was that we did not go to all the trouble of updating our code to tf2, only to fall back to using a legacy, tf1 version, training sequence. Our working assumption is that newer is better when it comes to TensorFlow versions, and that staying up do date with the latest tf version, will ensure that we are benefiting from the most up to date enhancements and optimizations. Not to mention that there are a number of features, such as tf profiling and custom training steps, that won\u2019t work in the legacy mode.", "I think it goes without saying that this option is ill-advised.", "TensorFlow provides support for running a fully customized training loop (instead of model.fit()), as well as APIs for customizing just the train_step of model.fit() as described here. Here we refer to the second mode of customization, although some of the conclusions are relevant to the first as well.", "It is very tempting to try to take advantage of the relative freedom that customization provides in order to support the capturing of graph tensors. The thing is, that even when using a custom training step, the advised way to do so, is such that it is wrapped by a tf.function qualifier. Of course, this makes all of the tensors part of the tf.Graph and thus inaccessible.", "However, using a custom training step, does grant us some freedom in defining the tensors that are output from the graph, and thus accessible. For example, we can extend the list of model outputs to include outputs of layers that we wish to record. Or, we can define our loss function such that in addition to calculating and returning the loss, it also returns tensors that we would like to record, either in the form of images or histograms.", "In the simplified code block below, we define a customized training step, such that it collects the model outputs, and the tensors returned by the loss function, and stores references to them in the extended Model class. These tensors are eager tensors, that can be freely referenced in other locations of the code. For example, we could define a custom keras callback that, at fixed intervals, writes these tensors to an event file, as histograms or images.", "You may have picked up on the built-in inefficiency of this proposal. In most cases, we want to post summaries at specific training step intervals, certainly not at every step. However, we have redefined our model such that the training step outputs the tensors of interest at every step. If we were to define the set of prediction tensors that we are interested in at every step as A, and the superset that includes the summary tensors as B, then we have expanded our tf.function to output B. This can create a significant overhead, in particular, if we have many tensors of interest. Ideally, we would want to output these tensors only at the training steps at which they will be recorded. Ideally, we would want to define two tf.functions, one that outputs A and one that outputs B, and use them interchangeably. However, as of the time of this writing, I have not discovered an elegant way to do this.", "The solution we propose is based on the observations that layer weights, including non-trainable weights, as described here, are eager tensors. Thus, in order to store the value of a given graph tensor, all we need to do is assign it to a \u201cnon-trainable\u201d weight. Let\u2019s start by demonstrating how this can be done in order to record the value of the output of an activation layer. In the block of code below, we have created a custom tf.keras layer that extends the standard tf.keras Activation with a layer that includes a non-trainable weight, called record_tensor. The call function is enhanced, such that on every step the record_tensor field is updated with the value of the current activation result. Since it is an eager tensor, record_tensor can be read outside of the training loop, and, in particular, recorded to TensorBoard.", "We have demonstrated the tensor capturing method on an activation layer, but we could have just as easily extended just about any keras layer and added a non-trainable tensor capturing variable. (One exception appears to be the keras InputLayer. It seems that this layer is handled differently, and that the \u201ccall\u201d function is not called for every instance and every version of TensorFlow. In order to capture graph inputs, consider using a dedicated capturing layer, as we describe below.)", "An alternative solution, is to define a general purpose tensor capturing layer as described in the next section.", "To support the general case of capturing a graph tensor, we can define a general purpose tensor capture layer. This layer is defined as a pass through for the input, where all it does is store the current value to its internal non-trainable weight variable. This can be used to capture tensors at any stage in the graph, including layer inputs and outputs, and tensors in the loss function. Additionally, it can be used to capture tensors in the input pipeline, which was not covered by the previous solutions.", "The complete solution requires storing a reference to the created layer and then accessing the record_tensor field, as needed, as shown in the previous example.", "One detail that you may have noticed, in both examples, is that in the definition of the non-trainable tf.Variable, we used an arbitrary batch_size (of 1) in the initialization value, and set validate_shape to False. This is because, while the initial value requires a well defined shape (it can\u2019t include None in any of the dimensions), at the time of model creation, we don\u2019t always know what the batch_size will be. (For non-input layers, this also enables the use of JSON model configuration without fixing the batch size.)", "In this post we have chosen to call our custom layer a SummaryCaptureLayer since the usage we have demonstrated has been recording tensor summaries to TensorBoard. But it would have been more appropriate to call the custom layer by the name TensorCaptureLayer, since all we are actually doing is capturing the tensor. Capturing tensors can support additional needs such as debugging.", "In a previous post, I expanded on some tools and techniques for debugging a broken training session. The custom layer that we described can be used as an additional debugging tool. Suppose you suspect that the root cause of your bug is a problem with one, or more, particular tensors, either in the input pipeline, the loss function, or one of your layers. By feeding the suspect tensors into the Custom Summary Layer, you can track the value of the tensors at each iteration, or more realistically, dump the tensor values on the iteration that causes the training process to fail. It is clear to see how, compared to other debugging options, this kind of debugging scheme, coupled with the proposal I shared for debugging in the previous post, could save considerable time in solving the types of monster bugs I describe in the post.", "Naturally, adding custom summary layers to the model, however thin they are, will incur a performance penalty. Since the performance penalty will depend directly on the model architecture, the number of summary layers that are inserted, and the frequency at which the tensors are written to the event file, it should be evaluated on a case by case basis. However, to provide some measure of comparison, we will provide the results of a simple case study, in which we capture 49 arbitrary tensors of a deep neural network, using the custom summary capture callback. The measurements below are averaged across epochs of 100 steps during which the summaries were captured, but not written (so as to remove the overhead of the summary writer).", "The relatively high overhead of outputting all of the tensors of interest using a custom train step is apparent. We also see that there is a little overhead to the use of custom summary layers. While collecting tensors using the non-eager execution mode is slightly faster than using custom summary layers, I do not believe that this advantage is great enough to warrant falling back to a legacy training scheme.", "Again, these results provide little information about the performance overhead of other use cases. Each model should be evaluated independently before settling on the number and types of tensors to collect, and the frequency at which they should be collected.", "In this post I have proposed a way to collect summaries of graph tensors in TensorFlow 2. The solution is not perfect, but in the absence of a better one, I think it is simple and elegant. If you know of a better solution, or if TensorFlow provides a built in solution, I would love to hear about it.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Machine Learning Algorithm Developer working on Autonomous Vehicle technologies at Mobileye."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcdc436cb74ef&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaimrand.medium.com/?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": ""}, {"url": "https://chaimrand.medium.com/?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "Chaim Rand"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----cdc436cb74ef---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdc436cb74ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdc436cb74ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://towardsdatascience.com/tensorflow-performance-analysis-314b56dceb59", "anchor_text": "performance profiling"}, {"url": "https://towardsdatascience.com/debugging-in-tensorflow-392b193d0b8", "anchor_text": "debugging"}, {"url": "https://www.tensorflow.org/tensorboard", "anchor_text": "TensorBoard"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/summary", "anchor_text": "tf.summary"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer", "anchor_text": "tf.keras.Layer"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/estimator/SummarySaverHook", "anchor_text": "tf.estimator.SummarySaverHook"}, {"url": "https://www.tensorflow.org/guide/keras/train_and_evaluate", "anchor_text": "tf.keras.model.fit()"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard", "anchor_text": "TensorBoard callback"}, {"url": "https://www.tensorflow.org/guide/eager", "anchor_text": "eager"}, {"url": "https://www.tensorflow.org/guide/effective_tf2", "anchor_text": "TensorFlow documentation"}, {"url": "https://www.tensorflow.org/guide/effective_tf2#eager_execution", "anchor_text": "Eager execution"}, {"url": "https://www.tensorflow.org/guide/function", "anchor_text": "improve your runtime performance"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/function", "anchor_text": "tf.function"}, {"url": "https://www.tensorflow.org/guide/effective_tf2#functions_not_sessions", "anchor_text": "Functions instead of sessions"}, {"url": "https://www.tensorflow.org/guide/effective_tf2#functions_not_sessions", "anchor_text": "TensorFlow documentation"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_eager_execution", "anchor_text": "tf.compat.v1.disable_eager_execution"}, {"url": "https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough", "anchor_text": "customized training loop"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/Model#train_step", "anchor_text": "train_step"}, {"url": "https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/function", "anchor_text": "tf.function"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/models/model_from_json", "anchor_text": "JSON model configuration"}, {"url": "http://wiki.mobileye.com/mw/index.php/Debugging_in_TensorFlow", "anchor_text": "previous post"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----cdc436cb74ef---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cdc436cb74ef---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tensorboard?source=post_page-----cdc436cb74ef---------------tensorboard-----------------", "anchor_text": "Tensorboard"}, {"url": "https://medium.com/tag/keras?source=post_page-----cdc436cb74ef---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdc436cb74ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&user=Chaim+Rand&userId=9440b37e27fe&source=-----cdc436cb74ef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdc436cb74ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&user=Chaim+Rand&userId=9440b37e27fe&source=-----cdc436cb74ef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdc436cb74ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcdc436cb74ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cdc436cb74ef---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cdc436cb74ef--------------------------------", "anchor_text": ""}, {"url": "https://chaimrand.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaimrand.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chaim Rand"}, {"url": "https://chaimrand.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "324 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbdff1fc03bc4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-tensorflow-keras-summary-capture-layer-cdc436cb74ef&newsletterV3=9440b37e27fe&newsletterV3Id=bdff1fc03bc4&user=Chaim+Rand&userId=9440b37e27fe&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}