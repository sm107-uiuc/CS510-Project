{"url": "https://towardsdatascience.com/investigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e", "time": 1683002031.943518, "path": "towardsdatascience.com/investigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e/", "webpage": {"metadata": {"title": "Investigating Differentiable Neural Architecture Search for Scientific Datasets | by Dylan Randle | Towards Data Science", "h1": "Investigating Differentiable Neural Architecture Search for Scientific Datasets", "description": "Disclaimer: The views and opinions expressed in this blog are those of the authors and are not endorsed by Harvard or Google. Deep learning frees us from feature engineering, but creates a new\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.reddit.com/r/MachineLearning/comments/4nwn2e/in_deep_learning_architecture_engineering_is_the/", "anchor_text": "architecture engineering", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/neural-network-architectures-156e5bad51ba", "anchor_text": "neural network architectures", "paragraph_index": 2}, {"url": "https://www.fast.ai/2018/07/16/auto-ml2/", "anchor_text": "Neural Architecture Search (NAS)", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1808.05377", "anchor_text": "survey", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1806.09055", "anchor_text": "DARTS (Differentiable Architecture Search)", "paragraph_index": 5}, {"url": "https://openreview.net/forum?id=S1eYHoC5FX", "anchor_text": "published at ICLR 2019", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1707.07012", "anchor_text": "NASNet", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1802.01548", "anchor_text": "AmoebaNet", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1802.03268", "anchor_text": "ENAS", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet", "paragraph_index": 5}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR", "paragraph_index": 6}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet", "paragraph_index": 6}, {"url": "https://www.nature.com/articles/d41586-018-02174-z", "anchor_text": "biology", "paragraph_index": 6}, {"url": "https://www.nature.com/collections/gcgiejchfa", "anchor_text": "medicine", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1701.04503", "anchor_text": "chemistry", "paragraph_index": 6}, {"url": "https://dl4physicalsciences.github.io/", "anchor_text": "various physical sciences", "paragraph_index": 6}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST", "paragraph_index": 8}, {"url": "https://openreview.net/forum?id=S1eYHoC5FX&noteId=r1ekErZ53Q", "anchor_text": "comments on the original DARTS paper", "paragraph_index": 28}, {"url": "http://capstone.iacs.seas.harvard.edu/", "anchor_text": "capstone.iacs.seas.harvard.edu", "paragraph_index": 39}, {"url": "https://github.com/capstone2019-neuralsearch/darts", "anchor_text": "https://github.com/capstone2019-neuralsearch/darts", "paragraph_index": 40}], "all_paragraphs": ["Team members: Dylan Randle, Julien Laasri, Michael Emanuel, Jiawei Zhuang", "Disclaimer: The views and opinions expressed in this blog are those of the authors and are not endorsed by Harvard or Google.", "Deep learning frees us from feature engineering, but creates a new problem of \u201carchitecture engineering\u201d. Numerous neural network architectures have been invented, but the design of architectures often feels more like an art than science. There is great interest in automating such time-consuming design processes via Neural Architecture Search (NAS), as shown by the rapidly growing number of research papers:", "A typical NAS workflow involves (1) proposing a candidate architecture within a pre-defined search space, (2) estimating the performance of the proposed architecture, and (3) proposing the next candidate based on a search strategy.", "There are many search strategies (reviewed in the survey), including Reinforcement Learning, Bayesian Optimization, Evolutionary Algorithms, Gradient-Based Optimization, or even Random Search. Some strategies can be very expensive, consuming massive amounts of energy and leading to million-dollar cloud compute bills:", "In this project, we investigate an efficient gradient-based search method called DARTS (Differentiable Architecture Search), recently published at ICLR 2019. DARTS is shown to require ~100x fewer GPU hours than previous methods like NASNet and AmoebaNet, and is competitive to the ENAS approach from Google Brain. We will compare DARTS to random search (which is actually quite good, see the table below) and state-of-the-art, hand-designed architectures such as ResNet.", "Most NAS studies, including the original DARTS paper, report experimental results using standard image datasets such as CIFAR and ImageNet. However, we believe that deep learning shows promise for scientific studies including biology, medicine, chemistry, and various physical sciences. In this project we want to see whether DARTS is useful for scientific datasets and how neural architectures may or may not transfer across these domains.", "We use three datasets from materials science, astronomy, and medical imaging, specifically:", "We also run experiments on MNIST (a well-known image dataset for classifying handwritten digits) as a non-scientific baseline.", "There are 3 keys ideas that makes DARTS efficient: (1) only search for small \u201ccells\u201d, (2) weight sharing, and (3) continuous relaxation. Each idea addresses one problem in traditional search methods, summarized in the figure below.", "To avoid searching the massive space of all \u201carbitrary architectures\u201d, DARTS leverages an important observation that popular ConvNet architectures often contain repeating blocks, stacked sequentially. For example, ResNet is a sequence of residual blocks:", "Following a similar idea, DARTS only searches for the optimal \u201cblock\u201d or \u201ccell\u201d, not the whole model. A cell typically consists of ~5-10 operations, represented by a directed acyclic graph (DAG). All cells are stacked sequentially to form the whole model.", "Two types of cells are defined in DARTS: (1) a \u201cNormal Cell\u201d which keeps the output spatial dimension the same as input, and (2) a \u201cReduction Cell\u201d which halves the output spatial dimension while doubling the number of filters/channels. All normal cells share the same architecture (operations and connections) but have independent weights to be trained \u2014 the same goes for all reductions cells.", "The steps to find the optimal cell are summarized in the figure below:", "\u201cFinding the optimal cell\u201d is equivalent to \u201cfinding the optimal placement of operations at the edges of the DAG\u201d. Instead of evaluating all kinds of placements independently (with each needing to be trained from scratch), DARTS superposes all candidate operations (e.g. Conv 3x3, MaxPooling, Identity, etc.) on each edge, so their weights can be trained together in a single process. The actual operation at edge (i, j) is the average of all candidate operations o(x), weighted by \u03b1:", "With a certain choice of architecture weight \u03b1, the corresponding architecture can in principle be trained to convergence, leading to the optimal model weights w*(\u03b1) and the final validation loss L(w*(\u03b1), \u03b1). The gradient of L w.r.t. to \u03b1 gives the direction for gradient descent on architecture parameters.", "Computing the true loss L by training w to convergence each time is too expensive. Thus DARTS just trains w for one step to get a proxy loss:", "where the optimal model weights w*(\u03b1) is approximated by one-step training.", "The training of \u03b1 and w is performed in an alternating manner:", "Below, we present a summary of the results obtained with DARTS, Random Search, and ResNet. We report the results of DARTS with and without architecture discretization (which we call \u201cdiscrete\u201d and \u201ccontinuous\u201d, respectively).", "As mentioned earlier, we first experimented with MNIST. This immediately led us to discover the important role of hyperparameters in ensuring stability during training. Below we show the effect of tuning the learning rates (both for regular and architecture weights) appropriately.", "On MNIST, ResNet performed better than DARTS. This was not too surprising since hand-designed architectures are generally specifically optimized to datasets such as MNIST (and ImageNet, CIFAR, etc.). With this in mind, we continued onward to the main scientific datasets.", "The Graphene Kirigami task is to predict stretch properties from the cut configuration of a graphene sheet. The data come from (computationally expensive) numerical simulations.", "We found that continuous (non-discretized) DARTS and ResNet performed about equally well on this task. However, we also found that this task may be too simple to evaluate DARTS. Below we show the results of DARTS, ResNet, and \u201ctiny\u201d ResNet. Notice that even this \u201ctiny\u201d model achieved the best R\u00b2 of ~0.92.", "This led us to conclude that the Graphene Kirigami task is too simple for DARTS to be useful. We further investigated this notion by looking at a) the distribution of learned architecture weights, as well as b) the performance distribution of the random search architectures, shown below:", "Of particular note is that most of the architecture weights learned by DARTS are hardly changed from their initialized value of 1/8 (we had 8 operations in this experiment). We also see that, apart from a few major outliers, architectures from random search all performed quite similarly. This led us to conclude that the Graphene Kirigami problem admits many performant architectures, and that DARTS similarly does not learn a sparse architecture as a result.", "Inspired by the apparent lack of difficulty found in the Graphene task, we sought a more complex problem of scientific interest. We settled on the Galaxy Zoo challenge. Unlike a typical classification problem, this task is scored on a root mean squared error (RMSE) metric due to the target labels being averaged predictions of human labelers. The task is to regress galaxy images to the mean prediction of human labelers for 37 distinct attributes.", "Excitingly, we found that DARTS (continuous) performed best. If we examine the architecture weights and random search performance (below), we see that DARTS learned a much more sparse cell than on the Graphene task. Additionally, the variability in the random architecture performance (note the log scale) was very large. This indicates that, for the Galaxy Zoo problem, there are indeed some architectures which are much better than others. And it appears that DARTS has induced some sparsity in the learned cell which reflects this.", "While these results show that DARTS is capable of learning a cell that outperforms ResNet, the performance was severely degraded after discretizing the architecture. This highlights the point made by some of the reviewers\u2019 comments on the original DARTS paper, specifically:", "The last step of how to move from the one-shot model to a single model is in a sense the most interesting aspect of this work, but also the one that leaves the most questions open: Why does this work? Are there cases where we lose arbitrarily badly by rounding the solution to the closest discrete value or is the performance loss bounded? How would other ways of moving from the relaxation to a discrete choice work?", "Our finding indicates that the discretization step is heuristic, and we show that it can fail in practice.", "To see if our results on Galaxy Zoo might carry over to another difficult scientific dataset, we looked into the Chest X-Ray imaging dataset. The task is a multi-label (binary) classification for various diseases (multiple diseases may be either present or not present) from chest X-ray images.", "Here we found that continuous DARTS modestly outperforms ResNet. Examining architecture weights and random search performance (shown below), we see a similar story to Galaxy Zoo. From the random search performance plot, there appears to be some architectures that perform much better than others (again note the log scale). We see that DARTS appears to have learned some sparsity in architecture weights that reflects the notion that some architectures in the space of all architectures are better suited to this task than others.", "Once again, we note that the discretized DARTS model performed markedly worse than the continuous one.", "In conclusion, we think that DARTS is a useful tool but is overkill on simple tasks. We showed that ResNet and random search perform quite well, but that DARTS is capable of modestly better performance. We noted that DARTS introduces many additional hyperparameters that must be carefully tuned to ensure stable learning. Most importantly, we showed that the DARTS \u201cdiscretization step\u201d can fail in practice and that better techniques are needed to ensure robustness.", "Our final recommendation to anyone considering DARTS is this: if a small increase in performance is important to you (e.g. 1% increase in accuracy), DARTS could be helpful and worth your time to investigate.", "Our suggestions for future work generally follow from the conclusions. Specifically, we suggest:", "As an alternative to the last point, one could simply eliminate the discretization step (as we did here when reporting the continuous results). But this would no longer really be an \u201carchitecture search\u201d and might instead be viewed as a new type of architecture entirely (not that there\u2019s anything wrong with this, but perhaps some folks are interested in staying true to the goal of architecture search).", "We would like to thank Pavlos Protopapas (Harvard), Javier Zazo (Harvard), and Dogus Cubuk (Google) for their support and guidance throughout this work.", "For more information about the Harvard Data Science Capstone, visit: capstone.iacs.seas.harvard.edu", "Our customized DARTS code to handle various scientific datasets can be found at: https://github.com/capstone2019-neuralsearch/darts", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @ Amazon | Harvard, Berkeley"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F62899be8714e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----62899be8714e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----62899be8714e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dylanrandle?source=post_page-----62899be8714e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dylanrandle?source=post_page-----62899be8714e--------------------------------", "anchor_text": "Dylan Randle"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F79c67bc7f802&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&user=Dylan+Randle&userId=79c67bc7f802&source=post_page-79c67bc7f802----62899be8714e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62899be8714e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62899be8714e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://ai.google/", "anchor_text": "Google AI"}, {"url": "https://www.reddit.com/r/MachineLearning/comments/4nwn2e/in_deep_learning_architecture_engineering_is_the/", "anchor_text": "architecture engineering"}, {"url": "https://towardsdatascience.com/neural-network-architectures-156e5bad51ba", "anchor_text": "neural network architectures"}, {"url": "https://www.fast.ai/2018/07/16/auto-ml2/", "anchor_text": "Neural Architecture Search (NAS)"}, {"url": "https://arxiv.org/abs/1909.02453", "anchor_text": "Best Practices for Scientific Research on Neural Architecture Search"}, {"url": "https://arxiv.org/abs/1808.05377", "anchor_text": "Neural Architecture Search: A Survey"}, {"url": "https://arxiv.org/abs/1808.05377", "anchor_text": "survey"}, {"url": "https://arxiv.org/abs/1906.02243", "anchor_text": "Energy and Policy Considerations for Deep Learning in NLP"}, {"url": "https://arxiv.org/abs/1806.09055", "anchor_text": "DARTS (Differentiable Architecture Search)"}, {"url": "https://openreview.net/forum?id=S1eYHoC5FX", "anchor_text": "published at ICLR 2019"}, {"url": "https://arxiv.org/abs/1707.07012", "anchor_text": "NASNet"}, {"url": "https://arxiv.org/abs/1802.01548", "anchor_text": "AmoebaNet"}, {"url": "https://arxiv.org/abs/1802.03268", "anchor_text": "ENAS"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet"}, {"url": "https://arxiv.org/abs/1806.09055", "anchor_text": "DARTS paper"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR"}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet"}, {"url": "https://www.nature.com/articles/d41586-018-02174-z", "anchor_text": "biology"}, {"url": "https://www.nature.com/collections/gcgiejchfa", "anchor_text": "medicine"}, {"url": "https://arxiv.org/abs/1701.04503", "anchor_text": "chemistry"}, {"url": "https://dl4physicalsciences.github.io/", "anchor_text": "various physical sciences"}, {"url": "https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.123.069901", "anchor_text": "Accelerated Search and Design of Stretchable Graphene Kirigami Using Machine Learning"}, {"url": "https://arxiv.org/abs/1308.3496", "anchor_text": "Galaxy Zoo 2"}, {"url": "https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/overview", "anchor_text": "Kaggle Galaxy Zoo Challenge"}, {"url": "https://arxiv.org/abs/1705.02315", "anchor_text": "ChestX-ray8"}, {"url": "https://www.kaggle.com/nih-chest-xrays/data", "anchor_text": "Kaggle NIH Chest X-ray Dataset"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST"}, {"url": "http://torch.ch/blog/2016/02/04/resnets.html", "anchor_text": "here"}, {"url": "https://www.researchgate.net/figure/The-representation-of-model-architecture-image-for-ResNet-152-VGG-19-and-two-layered_fig2_322621180", "anchor_text": "here"}, {"url": "https://openreview.net/forum?id=S1eYHoC5FX&noteId=r1ekErZ53Q", "anchor_text": "comments on the original DARTS paper"}, {"url": "http://capstone.iacs.seas.harvard.edu/", "anchor_text": "capstone.iacs.seas.harvard.edu"}, {"url": "https://github.com/capstone2019-neuralsearch/darts", "anchor_text": "https://github.com/capstone2019-neuralsearch/darts"}, {"url": "https://github.com/capstone2019-neuralsearch/AC297r_2019_NAS", "anchor_text": "https://github.com/capstone2019-neuralsearch/AC297r_2019_NAS"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----62899be8714e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----62899be8714e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----62899be8714e---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62899be8714e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&user=Dylan+Randle&userId=79c67bc7f802&source=-----62899be8714e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62899be8714e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&user=Dylan+Randle&userId=79c67bc7f802&source=-----62899be8714e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62899be8714e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----62899be8714e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F62899be8714e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----62899be8714e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----62899be8714e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----62899be8714e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----62899be8714e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----62899be8714e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----62899be8714e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----62899be8714e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----62899be8714e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----62899be8714e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dylanrandle?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dylanrandle?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dylan Randle"}, {"url": "https://medium.com/@dylanrandle/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "33 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F79c67bc7f802&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&user=Dylan+Randle&userId=79c67bc7f802&source=post_page-79c67bc7f802--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F79c67bc7f802%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finvestigating-differentiable-neural-architecture-search-for-scientific-datasets-62899be8714e&user=Dylan+Randle&userId=79c67bc7f802&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}