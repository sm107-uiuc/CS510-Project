{"url": "https://towardsdatascience.com/information-planning-and-naive-bayes-380ee1feedc7", "time": 1682993187.5375118, "path": "towardsdatascience.com/information-planning-and-naive-bayes-380ee1feedc7/", "webpage": {"metadata": {"title": "Information Planning and Naive Bayes | by Vadim Smolyakov | Towards Data Science", "h1": "Information Planning and Naive Bayes", "description": "Information planning involves making decisions based on information measures. Information planning is closely related to active learning [1] and optimum experiment design [2] in which labeled data is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/info_planning.ipynb", "anchor_text": "ipython notebook", "paragraph_index": 13}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20newsgroups", "paragraph_index": 14}, {"url": "https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/info_planning.ipynb", "anchor_text": "ipython notebook", "paragraph_index": 34}, {"url": "https://github.com/vsmolyakov", "anchor_text": "https://github.com/vsmolyakov", "paragraph_index": 37}], "all_paragraphs": ["Information planning involves making decisions based on information measures. Information planning is closely related to active learning [1] and optimum experiment design [2] in which labeled data is expensive to obtain.", "The key idea behind active learning is that a model can learn better with fewer labeled examples if the training examples are carefully selected to maximize the information gain for a particular task. In other words, given an abundance of unlabeled data, we would like to rank the unlabeled examples according to their usefulness in learning the parameters of the model. The top-K most informative training examples are labelled by expert annotators and are added to the training set.", "Here, we\u2019ll consider the task of text classification using a Naive Bayes graphical model. We\u2019ll look at two information measures [3]: entropy and mutual information that will be used to rank unlabeled set of documents for information planning.", "Let x_ij be Bernoulli random variables indicating the presence (x_ij = 1) or absence (x_ij=0) of a word j in {1,\u2026,D} for document i in {1,\u2026,n}, parameterized by theta_jc (probability that word j is present in class c) for a given class label y = c in {1,\u2026,C}.", "In addition, let pi be a Dirichlet distribution representing the prior over the class labels. Thus, the total number of learnable parameters is |theta| + |pi| = O(DC) + O(C) = O(DC), where D is the dictionary size and C is the number of classes. Due to the small number of parameters, the Naive Bayes model is immune to over-fitting.", "The choice of Bernoulli Naive Bayes formulation is important because it leads itself to word-based information planning. By associating each word in the dictionary with a binary random variable, we are able to compute the influence of individual words on class label distribution.", "We can write down the class conditional density as follows:", "We can derive the Naive Bayes inference algorithm by maximizing the log-likelihood. Consider words x_i in a single document i:", "Using the Naive Bayes assumption, we can compute the log-likelihood objective:", "By setting the gradient of log p(D|theta) with respect to the model parameters pi_c and theta_jc to zero, we obtain the following MLE updates: pi_c = N_c/N and theta_jc = N_jc / N_c where N_c = sum(1[y_i = c]).", "Note, that it\u2019s straightforward to add a Beta prior for the Bernoulli random variables and a Dirichlet prior for the class density to smooth the MLE counts:", "During test time, we would like to predict the class label y given the training data D and the learned model parameters. Applying the Bayes rule:", "Substituting the distributions for p(y=c|D) and p(x_ij|y=c,D) and taking the log, we get:", "The Baeysian formulation of Bernoulli Naive Bayes is implemented in scikit-learn [5] using the steps outlined above. While it\u2019s a really good practice to derive and implement ML algorithms from scratch (and in faster languages such as C++), we\u2019ll use the scikit-learn implementation and focus on information planning. All code is available in the following ipython notebook.", "We will train our model on a subset of the 20newsgroups dataset. In particular, we\u2019ll restrict ourselves to 4 classes: space, graphics, autos, and hockey. We\u2019ll use a count vectorizer to produce a vector of word counts for each document while filtering stop and low-frequency words.", "Entropy measures the amount of uncertainty in a random variable. In entropy based planning, we want to rank test-documents according to the entropy of their class label. The idea is that by annotating documents that have highest uncertainty about their class label will be able to better train our model with fewer annotated examples. In the case of the class distribution, the entropy can be computed in closed form:", "In the case of mutual information, we need to choose the random variables we want to use for planning. Since we are interested in classifying documents, it makes sense to choose y_i (the class label for document i) as one of the variables. Our choice of the second variable depends on the quantity of interest. We can choose one of the global variables theta_jc (probability that word j is present in class c) or pi_c (probability of class c). In this example, we consider estimating MI(y_i; theta), i.e. we are interested in measuring the information gain about the word distribution theta given the class label y_i for a test document. Since both variables are discrete, the mutual information can be estimated in closed form:", "We can compute p(theta) using the law of total probability:", "In addition, we compute MI(x_j; pi) to measure how informative each word x_j is to the global label distribution pi.", "Let\u2019s look at the learned word probabilities for every class:", "We can display the top 10 (highest probability) words above associated with each class:", "top-10 words for class: 0get please use like one anyone thanks graphics would know", "top-10 words for class: 1think well know also cars get like one would car", "top-10 words for class: 2first time season year play hockey one would game team", "top-10 words for class: 3nasa could much know get also like one would space", "Let\u2019s look at the top 20 (highest MI) informative words:", "top-20 MI words:teams program league moon playoffs orbit players earth games nasa nhl play cars season graphics hockey team game car space", "Not bad! Considering the ground truth labels are:", "We can see that highest MI words reflect the ground truth labels really well.", "Finally, let\u2019s visualize the test document ranking based on entropy and mutual information:", "In particular, we are interested in outliers, i.e. documents that have highest entropy and mutual information. From the violin plot above, we can see a greater density of high MI test documents compared to high entropy documents, which suggests that MI could be better at differentiating which documents are most suitable for information planning.", "In the case of discrete X and Y, to maximize the information gain I(X;Y), we want to maximize H(X) (uncertainty about X) and minimize H(X|Y) (i.e. we want Y to be informative about X).", "To visualize the difference between entropy and MI based planning, we can sort the documents according to entropy and use the same index to sort MI.", "In the above figure, documents that have high entropy but low MI are uncertain but not informative. Therefore, MI provides a better measure for information planning.", "All code is available in the following ipython notebook.", "We looked at two information measures for selecting training examples: entropy and mutual information in a simple Naive Bayes setting. While entropy ranks documents according to their uncertainty, MI also takes into account how informative the planning variables are. By actively choosing what examples to train on, we can achieve higher accuracy with fewer labelled examples. Information planning can play a role in reducing our dependence on large amounts of training data, particularly in cases where labels are expensive to obtain.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "passionate about data science and machine learning https://github.com/vsmolyakov"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F380ee1feedc7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vsmolyakov?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vsmolyakov?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "Vadim Smolyakov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5d7cb5e269f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=post_page-5d7cb5e269f9----380ee1feedc7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F380ee1feedc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F380ee1feedc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/info_planning.ipynb", "anchor_text": "ipython notebook"}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20newsgroups"}, {"url": "https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/info_planning.ipynb", "anchor_text": "ipython notebook"}, {"url": "http://scikit-learn.org/stable/", "anchor_text": "http://scikit-learn.org/stable/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----380ee1feedc7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/active-learning?source=post_page-----380ee1feedc7---------------active_learning-----------------", "anchor_text": "Active Learning"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----380ee1feedc7---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----380ee1feedc7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----380ee1feedc7---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F380ee1feedc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=-----380ee1feedc7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F380ee1feedc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=-----380ee1feedc7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F380ee1feedc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F380ee1feedc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----380ee1feedc7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----380ee1feedc7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----380ee1feedc7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----380ee1feedc7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vsmolyakov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vsmolyakov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vadim Smolyakov"}, {"url": "https://medium.com/@vsmolyakov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://github.com/vsmolyakov", "anchor_text": "https://github.com/vsmolyakov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5d7cb5e269f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=post_page-5d7cb5e269f9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F55bf1aebf554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-planning-and-naive-bayes-380ee1feedc7&newsletterV3=5d7cb5e269f9&newsletterV3Id=55bf1aebf554&user=Vadim+Smolyakov&userId=5d7cb5e269f9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}