{"url": "https://towardsdatascience.com/an-introduction-to-bayesian-inference-in-pystan-c27078e58d53", "time": 1682993836.622266, "path": "towardsdatascience.com/an-introduction-to-bayesian-inference-in-pystan-c27078e58d53/", "webpage": {"metadata": {"title": "An Introduction to Bayesian Inference in PyStan | by Matthew West | Towards Data Science", "h1": "An Introduction to Bayesian Inference in PyStan", "description": "The many virtues of Bayesian approaches in data science are seldom understated. Unlike the comparatively dusty frequentist tradition that defined statistics in the 20th century, Bayesian approaches\u2026"}, "outgoing_paragraph_urls": [{"url": "https://people.eecs.berkeley.edu/~avivt/BRLS_journal.pdf", "anchor_text": "reinforcement learning", "paragraph_index": 0}, {"url": "https://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/", "anchor_text": "current research", "paragraph_index": 0}, {"url": "https://link.springer.com/chapter/10.1007/978-94-010-1436-6_6", "anchor_text": "argued", "paragraph_index": 0}, {"url": "https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/", "anchor_text": "Markov chain Monte Carlo", "paragraph_index": 2}, {"url": "http://mc-stan.org", "anchor_text": "Stan", "paragraph_index": 2}, {"url": "http://mc-stan.org/users/interfaces/pystan", "anchor_text": "PyStan", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 theorem", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Bayes'_theorem#Drug_testing", "anchor_text": "false positives", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/bayes-rule-applied-75965e4482ff", "anchor_text": "here", "paragraph_index": 6}, {"url": "http://setosa.io/ev/markov-chains/", "anchor_text": "Markov chains", "paragraph_index": 12}, {"url": "https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/", "anchor_text": "Metropolis-Hastings", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1111.4246.pdf", "anchor_text": "No-U-Turn Sampler", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Andrew_Gelman", "anchor_text": "Andrew Gelman", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Stanislaw_Ulam", "anchor_text": "Stanislaw Ulam", "paragraph_index": 16}, {"url": "https://github.com/stan-dev/pystan", "anchor_text": "GitHub repository", "paragraph_index": 18}, {"url": "https://pystan.readthedocs.io/en/latest/", "anchor_text": "documentation", "paragraph_index": 18}, {"url": "https://pystan.readthedocs.io/en/latest/windows.html", "anchor_text": "install", "paragraph_index": 21}, {"url": "http://mc-stan.org/users/documentation/", "anchor_text": "reference manual", "paragraph_index": 23}, {"url": "https://github.com/mwestt/An-Introduction-to-Bayesian-Inference-in-PyStan/blob/master/PyStan_plotting.py", "anchor_text": "this script", "paragraph_index": 30}, {"url": "http://mc-stan.org/users/documentation/", "anchor_text": "Stan manual", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Effective_sample_size", "anchor_text": "effective sample size", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Autocorrelation", "anchor_text": "autocorrelation", "paragraph_index": 37}, {"url": "http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/307.pdf", "anchor_text": "Gelman-Rubin", "paragraph_index": 38}, {"url": "https://github.com/mwestt/An-Introduction-to-Bayesian-Inference-in-PyStan/blob/master/PyStan_plotting.py", "anchor_text": "GitHub", "paragraph_index": 41}, {"url": "https://en.wikipedia.org/wiki/Credible_interval", "anchor_text": "credible intervals", "paragraph_index": 45}, {"url": "https://github.com/mwestt/An-Introduction-to-Bayesian-Inference-in-PyStan", "anchor_text": "GitHub repository", "paragraph_index": 47}, {"url": "http://mwestt.github.io", "anchor_text": "mwestt.github.io", "paragraph_index": 49}], "all_paragraphs": ["The many virtues of Bayesian approaches in data science are seldom understated. Unlike the comparatively dusty frequentist tradition that defined statistics in the 20th century, Bayesian approaches match more closely the inference that human brains perform, by combining data-driven likelihoods with prior beliefs about the world. This kind of approach has been fruitfully applied in reinforcement learning, and efforts to incorporate it into deep learning are a hot area of current research. Indeed, it has been argued that Bayesian statistics is the more fundamental of the two statistical schools of thought, and should be the preferred picture of statistics when first introducing students to the subject.", "As the predictions from Bayesian inference are probability distributions rather than point estimates, this allows for the quantification of uncertainty in the inferences that are made, which is often missing from the predictions made by machine learning methods.", "Although there are clear motivations for incorporating Bayesian approaches into machine learning, there are computational challenges present in actually implementing them. Often, it is not practical to analytically compute the required distributions, and stochastic sampling methods such as Markov chain Monte Carlo (MCMC) are used instead. One way of implementing MCMC methods in a transparent and efficient way is via the probabilistic programming language, Stan.", "In this article, I\u2019ll provide a bit of background about Bayesian inference and MCMC, before demonstrating a simple example where Stan is used to perform inference on a generated dataset, through Stan\u2019s Python interface, PyStan.", "The crux of Bayesian inference is in Bayes\u2019 theorem, which was discovered by the Reverend Thomas Bayes in the 18th century. It\u2019s based on a fundamental result from probability theory, which you may have seen before:", "That thing on the left is our posterior, which is the distribution we\u2019re interested in. On the right-hand side, we have the likelihood, which is dependent on our model and data, multiplied by the prior, which represents our pre-existing beliefs, and divided by the marginal likelihood which normalises the distribution.", "This theorem can be used to arrive at many counterintuitive results, that are nonetheless true. Take, for instance, the example of false positives in drug tests being much higher when the test population is heavily skewed. Rather than go into detail on the basics of Bayesian statistics, I\u2019m going to press onward to discussing inference with Stan. William Koehrsen has some great material for understanding the intuition behind Bayes\u2019 theorem here.", "Bayesian inference is hard. The reason for this, according to statistician Don Berry:", "\u201cBayesian inference is hard in the sense that thinking is hard.\u201d \u2014 Don Berry", "Well, OK. What I suppose he means here is that there\u2019s little mathematical overhead that gets in the way of making inferences using Bayesian methods, and so the difficulties come from the problems being conceptually difficult rather than any technical or methodological abstraction. But more concretely, Bayesian inference is hard because solving integrals is hard.", "That P(B) up there involves an integral over all possible values that the model parameters can take. Luckily, we\u2019re not totally at a loss, as it is possible to construct an approximation to the posterior distribution by drawing samples from it, and creating a histogram of those sampled values to serve as the desired posterior.", "In generating those samples, we need a methodological framework to govern how the sampler should move through the parameter space. A popular choice is Markov chain Monte Carlo. MCMC is a class of methods that combines two powerful concepts: Markov chains and Monte Carlo sampling.", "Markov chains are stochastic processes that evolve over time in a \u201cmemoryless\u201d way, known as the Markov property. The Markov property means that the state of a Markov chain transitions to another state with a probability that depends only on the most recent state of the system, and not its entire history.", "Monte Carlo sampling, on other hand, involves solving deterministic problems by repeated random sampling. The canonical way of doing this is with the Metropolis-Hastings algorithm.", "Stan instead generates these samples using a state-of-the-art algorithm known as Hamiltonian Monte Carlo (HMC), which builds upon the Metropolis-Hastings algorithm by incorporating many theoretical ideas from physics. Actually, by default it implements a version of HMC called the No-U-Turn Sampler (NUTS).", "It\u2019s easy to get bogged down in the conceptual complexity of these methods, so don\u2019t worry if you\u2019re not fully on-board at this stage; just internalise that we\u2019re generating samples stochastically, and accepting or rejecting those samples based on some probabilistic rule.", "Stan is a probabilistic programming language developed by Andrew Gelman and co., mainly at Columbia University. If you think the name is an odd choice, it\u2019s named after Stanislaw Ulam, nuclear physicist and father of Monte Carlo methods.", "While it is incredibly useful, Stan has a relatively steep learning curve and there isn\u2019t exactly a wealth of accessible introductory material on the internet that demonstrates this usefulness. The syntax is itself is mostly borrowed from Java/C, but there are libraries for R, Python, Julia and even MATLAB. Yes, that\u2019s right, even MATLAB. So whatever your programmatic cup of tea, there\u2019s probably something in there for you.", "My language of choice is Python, so I\u2019ll be using PyStan. Like the rest of Stan, the code for PyStan is open source and can be found here in this GitHub repository, and the documentation is pretty comprehensive. Popular alternatives to Stan, which some of you may be familiar with, are PyMc and Edward, though I haven\u2019t had much experience with them. The nice thing about Stan is that you can simply specify the distributions in your model and you\u2019re underway, already steaming ahead towards your impending inferential bounty. Where Stan really shines is in very high dimensional problems, where you\u2019ve got large numbers of predictors to infer.", "For this article, however, I am going to keep things simple and restrict our model to a straightforward univariate linear regression, allowing greater focus on the Stan workflow. The model we\u2019ll implement is", "where we have an intercept \u03b1 and a gradient \u03b2, and our data is distributed about this straight line with Gaussian noise of standard deviation \u03c3.", "Now we\u2019ve got some of the background out of the way, let\u2019s implement some of what we\u2019ve discussed. First, you\u2019ll need to install PyStan, which you can do with:", "Let\u2019s start by importing the relevant packages and setting the numpy seed for reproducibility purposes. Next, we\u2019ll begin our Stan script by specifying our model for the linear regression. The model is written in Stan and assigned to a variable of type string called model. This is the only part of the script that needs to by written in Stan, and the inference itself will be done in Python.", "The code for this model comes from the first example model in chapter III of the Stan reference manual, which is a recommended read if you\u2019re doing any sort of Bayesian inference.", "A Stan model requires at least three blocks, for each of data, parameters, and the model. The data block specifies the types and dimensions of the data that will be used for sampling, and the parameter block specifies the relevant parameters. The distribution statement goes in the model block, which in our case is a straight line with additional Gaussian noise. Though not included here, it is also possible to specify transformed data and transformed parameter blocks, as well as blocks for functions and generated quantities.", "Notice that in the parameter block, we\u2019ve specified the lower bound for sigma as 0, as it\u2019s impossible for the magnitude of Gaussian noise to be negative. This is an example of a prior on the parameter sigma, and more detailed priors can be added in the model block.", "Notice also that we\u2019re not adding any priors to our alpha and beta parameters, though feel free to experiment with adding priors in the model block and see how they affect the posterior estimates.", "Here we will specify the \u2018ground truth\u2019 values of our parameters which we\u2019ll aim to reproduce using Stan, and generate data from these parameters using numpy, making sure to add Gaussian noise.", "We can plot this data to get an idea of what we\u2019re dealing with.", "Now we can perform the inference using our PyStan model. The data needs to be in a Python dictionary to run the sampler, and needs a key for every element we specified in the data block of the Stan model.", "The model then needs to compile before it can be sampled from, though it is possible to load a pre-compiled model instead, which you can do with this script hosted in the GitHub repository created to accompany this article. This setup allows us to change the data that we want to generate estimates from, without having to recompile the model.", "Within the sampling method, there are a number of parameters that can be specified. iter refers to the total number of samples that will be generated from each Markov chain, and chains is the number of chains from which samples will be combined to form the posterior distribution. Because the underlying Markov process is stochastic, it's advantageous to have more chains that will initialise at different locations in parameter space, though adding more chains will increase the amount of time it takes to sample.", "warmup, also known as 'burn-in' is the amount of samples that will be discarded from the beginning of each chain, as the early samples will be drawn when the Markov chain hasn't had a chance to reach equilibrium.", "By default this is half of iter, so for each chain we'll get 1000 samples, and chuck away the first 500. With 4 chains, we'll have 2000 samples in total.", "thin specifies an interval in sampling at which samples are retained. So if thin is equal to 3, every third sample is retained and the rest are discarded. This can be necessary to mitigate the effect of correlation between successive samples. It's set to 1 here, and so every sample is retained. Finally, seed is specified to allow for reproducibility.", "Once sampling is completed, the fit object can be printed to examine the results of the inference. Here we can see summary statistics for our two regression parameters, as well as for the Gaussian noise in our model.", "Additionally, we can see those same statistics for a quantity called lp__, which according to the Stan manual is the log posterior density up to a constant. Checking that lp__ has converged allows greater confidence that the whole sampling process has converged, but the value itself isn't particularly important.", "In addition to the mean and quantile information, each parameter has two further columns, n_eff and Rhat. n_eff is the effective sample size, which because of correlation between samples, can be significantly lower than the nominal amount of samples generated. The effect of autocorrelation can be mitigated by thinning the Markov chains, as described above.", "Rhat is the Gelman-Rubin convergence statistic, a measure of Markov chain convergence, and corresponds to the scale factor of variance reduction that could be observed if sampling were allowed to continue forever. So if Rhat is approximately 1, you would expect to see no decrease in sampling variance regardless of how long you continue to iterate, and so the Markov chain is likely (but not guaranteed) to have converged.", "We can cast this fit output to a pandas DataFrame to make our analysis easier, allowing greater access to the relevant summary statistics. The fit object above has a plot method, but this can be a bit messy and appears to be deprecated. We can instead extract the sequence of sampled values for each parameter, known as a \u2018trace\u2019, which we can plot alongside the corresponding posterior distributions for the purposes of diagnostics.", "Now that we\u2019ve extracted the relevant information from our fitted model, we can look at plotting the results of the linear regression. To get an idea of the kind of spread we can expect to see in our regression line from the uncertainty in the inferred parameters, we can also plot potential regression lines with their parameters sampled from our posteriors.", "For more detail on the code used to generate this plot, you can look at the script on GitHub.", "Rather than restricting our analysis to summary statistics, we can also look in more detail at the series of sampled values for each parameter that we extracted previously. This will allow more insight into the sampling process and is an important part of performing fit diagnostics.", "Here I\u2019ve defined a function that plots the trace and posterior distribution for a given parameter. Calling this function for the parameters of interest, we can generate the desired plots.", "From the trace plot on top, we can see that the movement through parameter space resembles a random walk, which is indicative that the underlying Markov chain has reached convergence as we would hope. We can also take a look at the corresponding plots for beta and sigma.", "We can see from the plots of the posterior distributions that the ground truths are on the whole pretty close to the modes of our distributions, and comfortably within the 95% credible intervals. There is some bias between the peaks of the posteriors and the ground truths, which is introduced as a result of the noise in the data. The handy thing about the Bayesian approach is that the uncertainty in estimation is captured in the spread of these posteriors, and can be improved by providing more insightful priors.", "So we\u2019ve seen how to write a model in Stan, perform sampling using generated data in PyStan, and examine the outputs of the sampling process. Bayesian inference can be extremely powerful, and there are many more features of Stan that remain to be explored. I hope this example has been useful and that you can use some of this material in performing your own sampling and inference.", "Thus wraps up my first technical blog post. Feel free to leave any feedback and be sure to check out the GitHub repository to perform your own inference and make your own plots. Thanks for reading!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Arblet Inc. Lapsed physicist. mwestt.github.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc27078e58d53&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c27078e58d53--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c27078e58d53--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@m.west2718?source=post_page-----c27078e58d53--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.west2718?source=post_page-----c27078e58d53--------------------------------", "anchor_text": "Matthew West"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F937b2e244d3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&user=Matthew+West&userId=937b2e244d3a&source=post_page-937b2e244d3a----c27078e58d53---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc27078e58d53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc27078e58d53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://people.eecs.berkeley.edu/~avivt/BRLS_journal.pdf", "anchor_text": "reinforcement learning"}, {"url": "https://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/", "anchor_text": "current research"}, {"url": "https://link.springer.com/chapter/10.1007/978-94-010-1436-6_6", "anchor_text": "argued"}, {"url": "https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/", "anchor_text": "Markov chain Monte Carlo"}, {"url": "http://mc-stan.org", "anchor_text": "Stan"}, {"url": "http://mc-stan.org/users/interfaces/pystan", "anchor_text": "PyStan"}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 theorem"}, {"url": "https://en.wikipedia.org/wiki/Bayes'_theorem#Drug_testing", "anchor_text": "false positives"}, {"url": "https://towardsdatascience.com/bayes-rule-applied-75965e4482ff", "anchor_text": "here"}, {"url": "http://setosa.io/ev/markov-chains/", "anchor_text": "Markov chains"}, {"url": "https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/", "anchor_text": "Metropolis-Hastings"}, {"url": "https://arxiv.org/pdf/1111.4246.pdf", "anchor_text": "No-U-Turn Sampler"}, {"url": "https://en.wikipedia.org/wiki/Andrew_Gelman", "anchor_text": "Andrew Gelman"}, {"url": "https://en.wikipedia.org/wiki/Stanislaw_Ulam", "anchor_text": "Stanislaw Ulam"}, {"url": "https://github.com/stan-dev/pystan", "anchor_text": "GitHub repository"}, {"url": "https://pystan.readthedocs.io/en/latest/", "anchor_text": "documentation"}, {"url": "https://pystan.readthedocs.io/en/latest/windows.html", "anchor_text": "install"}, {"url": "http://mc-stan.org/users/documentation/", "anchor_text": "reference manual"}, {"url": "https://github.com/mwestt/An-Introduction-to-Bayesian-Inference-in-PyStan/blob/master/PyStan_plotting.py", "anchor_text": "this script"}, {"url": "http://mc-stan.org/users/documentation/", "anchor_text": "Stan manual"}, {"url": "https://en.wikipedia.org/wiki/Effective_sample_size", "anchor_text": "effective sample size"}, {"url": "https://en.wikipedia.org/wiki/Autocorrelation", "anchor_text": "autocorrelation"}, {"url": "http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/307.pdf", "anchor_text": "Gelman-Rubin"}, {"url": "https://github.com/mwestt/An-Introduction-to-Bayesian-Inference-in-PyStan/blob/master/PyStan_plotting.py", "anchor_text": "GitHub"}, {"url": "https://en.wikipedia.org/wiki/Credible_interval", "anchor_text": "credible intervals"}, {"url": "https://github.com/mwestt/An-Introduction-to-Bayesian-Inference-in-PyStan", "anchor_text": "GitHub repository"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c27078e58d53---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----c27078e58d53---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/markov-chains?source=post_page-----c27078e58d53---------------markov_chains-----------------", "anchor_text": "Markov Chains"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c27078e58d53---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c27078e58d53---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc27078e58d53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&user=Matthew+West&userId=937b2e244d3a&source=-----c27078e58d53---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc27078e58d53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&user=Matthew+West&userId=937b2e244d3a&source=-----c27078e58d53---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc27078e58d53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c27078e58d53--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc27078e58d53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c27078e58d53---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c27078e58d53--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c27078e58d53--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c27078e58d53--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c27078e58d53--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c27078e58d53--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c27078e58d53--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c27078e58d53--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c27078e58d53--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.west2718?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.west2718?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matthew West"}, {"url": "https://medium.com/@m.west2718/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "100 Followers"}, {"url": "http://mwestt.github.io", "anchor_text": "mwestt.github.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F937b2e244d3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&user=Matthew+West&userId=937b2e244d3a&source=post_page-937b2e244d3a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9d67b63e4211&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-in-pystan-c27078e58d53&newsletterV3=937b2e244d3a&newsletterV3Id=9d67b63e4211&user=Matthew+West&userId=937b2e244d3a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}