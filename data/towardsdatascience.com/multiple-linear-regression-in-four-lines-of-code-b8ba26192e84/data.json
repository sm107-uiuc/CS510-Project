{"url": "https://towardsdatascience.com/multiple-linear-regression-in-four-lines-of-code-b8ba26192e84", "time": 1682995673.073617, "path": "towardsdatascience.com/multiple-linear-regression-in-four-lines-of-code-b8ba26192e84/", "webpage": {"metadata": {"title": "The Complete Beginner\u2019s Guide to Machine Learning: Multiple Linear Regression in 4 Lines of Code! | by Anne Bonner | Towards Data Science", "h1": "The Complete Beginner\u2019s Guide to Machine Learning: Multiple Linear Regression in 4 Lines of Code!", "description": "You might want to predict how well a stock will do based on some other information that you just happen to have. It might help you to know if how often you bathe and how many cats you have relates to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/simple-linear-regression-in-four-lines-of-code-d690fe4dba84", "anchor_text": "this article on building a simple linear regressor", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d", "anchor_text": "this article on data cleaning and preprocessing", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/simple-linear-regression-in-four-lines-of-code-d690fe4dba84", "anchor_text": "that article on simple linear regression", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d", "anchor_text": "hat data cleaning and preparation article", "paragraph_index": 50}, {"url": "https://towardsdatascience.com/simple-linear-regression-in-four-lines-of-code-d690fe4dba84", "anchor_text": "this article on building a simple linear regressor", "paragraph_index": 60}, {"url": "https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html", "anchor_text": "the official documentation here", "paragraph_index": 76}, {"url": "https://www.linkedin.com/in/annebonnerdata/", "anchor_text": "@annebonnerdata", "paragraph_index": 86}, {"url": "http://bit.ly/submissions2023", "anchor_text": "http://bit.ly/submissions2023", "paragraph_index": 88}], "all_paragraphs": ["Being able to predict the future is awesome.", "You might want to predict how well a stock will do based on some other information that you just happen to have.", "It might help you to know if how often you bathe and how many cats you have relates to how long you\u2019ll live.", "You might want to figure out if there\u2019s a relationship between a man who 1.) calls his mom more than three times a day, 2.) refers to another man as \u201cbro,\u201d 3.) has never done his own laundry and above average divorce rates.", "Multiple linear regression might be for you!", "Multiple linear regression is fun because it looks at the relationships within a bunch of information. Instead of just looking at how one thing relates to another thing (simple linear regression), you can look at the relationship between a lot of different things and the thing you want to predict.", "A linear regression model is a statistical model that\u2019s frequently used in data science. It\u2019s also one of the basic building blocks of machine learning! Multiple linear regression (MLR/multiple regression) is a statistical technique. It can use several variables to predict the outcome of a different variable. The goal of multiple regression is to model the linear relationship between your independent variables and your dependent variable. It looks at how multiple independent variables are related to a dependent variable.", "I\u2019m going to assume that you know a little bit about simple linear regression. If you don\u2019t, check out this article on building a simple linear regressor. It will give you a quick (and fun) walk-through of the basics.", "Simple linear regression is what you can use when you have one independent variable and one dependent variable. Multiple linear regression is what you can use when you have a bunch of different independent variables!", "Multiple regression analysis has three main uses.", "We\u2019re going to keep things super simple here so that multiple linear regression as a whole makes sense. I do want you to know that things can get a lot more complex than this in the real world.", "For the purposes of this post, you are now working for a venture capitalist.", "So here\u2019s the thing: you have a dataset in front of you with information on 50 companies. You have five columns that contain information about how much those companies spend on admin, research and development (R&D), and marketing, their location by state, and their profit for the most recent year. This dataset is anonymized, which means we don\u2019t know the names of these companies or any other identifying information.", "You\u2019ve been hired to analyze this information and create a model. You need to inform the guy who hired you what kind of companies will make the most sense in the future to invest in. To keep things simple, let\u2019s say that your employer wants to make this decision based on last year\u2019s profit. This means that the profits column is your dependent variable. The other columns are the independent variables.", "So you want to learn about the dependent variable (profit) based on the other categories of information you have.", "The guy who hired you doesn\u2019t want to invest in these specific companies. He wants to use the information in this dataset as a sample. This sample will help him understand which of the companies he looks at in the future will perform better based on the same information.", "Does he want to invest in companies that spend a lot on R&D? Marketing? Does he want to invest in companies that are based in Illinois? You need to help him create a set of guidelines. You\u2019re going to help him be able to say something along the lines of, \u201cI\u2019m interested in a company that\u2019s based in New York that spends very little on admin expenses but a lot on R&D.\u201d", "You\u2019re going to come up with a model that will allow him to assess where and into which companies he wants to invest to maximize his profit.", "Linear regression is great for correlation, but remember that correlation and causation are not the same things! You are not saying that one thing causes the other, you\u2019re finding which independent variables are strongly correlated to the dependent variable.", "There are some assumptions that absolutely have to be true:", "You need to check that these assumptions are true before you proceed and build your model. We\u2019re totally skipping past that here. Make sure that if you\u2019re doing this in the real world, you aren\u2019t just blindly following this tutorial. Those assumptions need to be correct when you\u2019re building your regression!", "If you aren\u2019t familiar with the concept of dummy variables, check out this article on data cleaning and preprocessing. It has some simple code that we can go ahead and copy and paste here.", "So we\u2019ve already decided that \u201cprofit\u201d is our dependent variable (y) and the others are our independent variables (X). We\u2019ve also decided that what we want is a linear regression model. What about that column of states? \u201cState\u201d is a categorical variable, not a numerical variable. We need our independent variables to be numbers, not words. What do we do?", "If you looked at the information in the locations column, you might see that all of the companies that are being examined are based in two states. For the purposes of this explanation, let\u2019s say all of our companies are located in either New York or Minnesota. That means that we\u2019ll want to turn this one column of information into two columns of 1s and 0s. (If you want to learn more about why we\u2019re doing that, check out that article on simple linear regression. It explains why this would be the best way to arrange our data.)", "So how do we populate those columns? Basically, we\u2019ll turn each state into its own column. If a company is located in New York, it will have a 1 in the \u201cNew York\u201d column and a 0 in the \u201cMinnesota\u201d column. If you were using more states, you\u2019d have a 1 in the New York column, and, for example, a 0 in the \u201cCalifornia\u201d column, a zero in the \u201cIllinois\u201d column, a 0 in the Arkansas column, and so on. We won\u2019t be using the original \u201clocations\u201d column anymore because we won\u2019t need it!", "These 1s and 0s are basically working as a light switch. 1 is \u201con\u201d or \u201cyes\u201d and 0 is \u201coff\u201d or \u201cnope.\u201d", "You never want to include both variables at the same time.", "You\u2019d be duplicating a variable. The first variable (d1) is always equal to 1 minus the second variable (d2). (d1 = 1-d2) When one variable predicts another, it\u2019s called multicollinearity. As a result, the model wouldn\u2019t be able to distinguish the results of d1 from the results of d2. You can\u2019t have the constant and both dummy variables at the same time. If you have nine variables, include eight of them. (If you have two sets of dummy variables, then you have to do this for each set.)", "You\u2019re going to want to be familiar with the concept of a P-value. That\u2019s definitely going to come up.", "The P-value is the probability of getting a sample like ours (or more extreme than ours) if the null hypothesis is true.", "It gives a value to the weirdness of your sample. If you have a large P-value, then you probably won\u2019t change your mind about the null hypothesis. A large value means that it wouldn\u2019t be at all surprising to get a sample like yours if the hypothesis is true. As the P-value gets smaller, you should probably start to ask yourself some questions. You might want to change your mind and maybe even reject the hypothesis.", "The null hypothesis is the official way to refer to the claim (hypothesis) that\u2019s on trial here. It\u2019s the default position where there\u2019s just no association among the groups that are being tested. In every experiment, you\u2019re looking for an effect among the groups that are being tested. Unfortunately, there\u2019s always the possibility that there\u2019s no effect (or no difference) between the groups. That lack of difference is called the null hypothesis.", "It\u2019s like if you were doing a trial of a drug that doesn\u2019t work. In that trial, there just wouldn\u2019t be a difference between the group that took the drug and the rest of the population. The difference would be null.", "You always assume that the null hypothesis is true until you have evidence that it isn\u2019t.", "We need to figure out which columns we want to keep and which we want to toss. If you just chuck a bunch of stuff into your model, it won\u2019t be a good one. It definitely won\u2019t be reliable! (Also, at the end of the day, you need to be able to explain your model to the guy who hired you to create this thing. You\u2019re only going to want to explain the variables that actually predict something!)", "There are essentially five methods of building a multiple linear regression model.", "You\u2019ll almost certainly hear about Stepwise Regression as well. Stepwise regression is most commonly used as another way of saying bidirectional elimination (method 4). Sometimes when people use that phrase they\u2019re referring to a combination of methods 2, 3, and 4. (That\u2019s the idea behind bidirectional elimination as well.)", "Method 1 (Chuck Everything In): Okay. That isn\u2019t the official name for this method (but it should be). Occasionally you\u2019ll need to build a model where you just throw in all your variables. You might have some kind of prior knowledge. You might have a particular framework you need to use. You might have been hired by someone who\u2019s insisting that you do that. You might want to prepare for backward elimination. It\u2019s a real option, so I\u2019m including it here.", "Method 2 (backward elimination): This has a few basic steps.", "(After we go through these concepts, I\u2019ll walk you through an example of backward elimination so you can see it in action! It\u2019s definitely confusing, but if you really look at what\u2019s going on, you\u2019ll get the hang of it.)", "Method 3 (forward selection): This is way more complex than just reversing backward elimination.", "We can stop when P<SL is no longer true, or there are no more P-values that are less than the significance level. It means that the variable is not significant anymore. You won\u2019t keep the current model, though. You\u2019ll keep the previous one because, in the final model, your variable is insignificant.", "Method 3 (bidirectional elimination): This method combines the previous two!", "Method 4 (score comparison): Here, you\u2019re going to be looking at all possible methods. You\u2019ll look at a comparison of the scores for all of the possible methods. This is definitely the most resource-consuming approach!", "Fun fact: if you have 10 columns of data, you\u2019ll wind up with 1,023 models here. You\u2019d better be ready to commit if you\u2019re going to go this route!", "If you\u2019re just getting started with machine learning, statistics, or data science, that all looks like it will be an insane amount of code. It\u2019s not!", "So much of what you need to do with a machine learning model is all ready to go with the amazing libraries out there. You\u2019ll need to do the tough parts where you decide what information is important and what kind of models you\u2019ll want to use. It\u2019s also up to you to interpret the results and be able to communicate what you\u2019ve built. However, the code itself is very doable.", "Backward elimination is the fastest and the best method to start with, so that\u2019s what I\u2019m going to walk you through after we build the quick and easy multiple linear regression model.", "First, let\u2019s prepare our dataset. Let\u2019s say we have a .csv file called \u201cstartups.csv\u201d that contains the information we talked about earlier. We\u2019ll say it has 50 companies and columns for R&D spending, admin spending, marketing spending, what state the company is located in (let\u2019s say, New York, Minnesota, and California), and one column for last year\u2019s profit.", "It\u2019s a good idea to import your libraries right away.", "Now we can go ahead and copy and paste the code from that data cleaning and preparation article! We\u2019re definitely going to want to change the name of our dataset to ours. I\u2019m calling it \u2018startups.csv.\u2019 We\u2019ll adjust a couple of other tiny details as well. Profit (y) is still our last column, so we\u2019ll continue to remove that with [:, :-1]. We\u2019ll make a little adjustment to grab our independent variables with [:, 4]. Now we have a vector of the dependent variable (y) and a matrix of independent variables that contains everything except the profits (X). We want to see if there is a linear dependency between the two!", "Now we need to encode the categorical variable. We can use label encoder and one hot encoder to create dummy variables. (We can copy and paste this from that other article too! Make sure you\u2019re grabbing the right information and you don\u2019t encode the dependent variable.) You\u2019re going to change the index of the column in both spots [:, 3] and [:, 3] again, and replace the index in one hot encoder too [3].", "You\u2019re ready to go! Our one column of information is now three columns, each of which corresponds to one state!", "What about avoiding the dummy variable trap? You don\u2019t actually need to do that with our libraries! It\u2019s all taken care of for you here with the libraries that we\u2019re choosing to use. However, if you ever want or need to run that code, it\u2019s simple! You can do that with one line right after you encode your data.", "What does that do? It removes the first column from X. Putting the 1 there means that we want to take all of the columns starting at index 1 to the end. You won\u2019t take the first column. For some libraries, you\u2019ll need to take one column away manually to be sure your dataset won\u2019t contain redundancies.", "Now let\u2019s split our training and testing data. The most common split is an 80/20 split, which means 80% of our data would go to training our model and 20% would go to testing it. Let\u2019s do that here!", "We don\u2019t need to do feature scaling here! The library will take care of that for us.", "We\u2019ll import linear regression from Scikit-Learn. (That makes a little sense, doesn\u2019t it?)", "Now we\u2019ll introduce our regressor. We\u2019ll create an object of the class LinearRegression and we\u2019ll fit the object to our training set. We want to apply this to both our X_train and y_train.", "Now let\u2019s test the performance of our multiple linear regressor!", "(We won\u2019t plot a graph here because we\u2019d need five dimensions to do that. If you\u2019re interested in plotting a graph with a simple linear regressor, check out this article on building a simple linear regressor.)", "We\u2019ll create the vector of predictions (y_pred). We can use the regressor with the predict method to predict the observations of the test set (X_test).", "That\u2019s it! Four lines of code and you\u2019ve built a multiple linear regressor!", "Now we can see the ten predicted profits! You can print them any time with a simple print(y_pred). We can easily compare them by taking a look at the predictions and then comparing them to the actual results. If you were to take a look, you\u2019d see that some are incredibly accurate and the rest are pretty darn good. Nice work!", "There is definitely some linear dependency between our dependent and independent variables. We can clearly see a strong linear relationship between the two.", "Congratulations!! You now know how to make a multiple linear regressor in Python!", "Things are about to get more challenging!", "What if some of the variables have a lot of impact on our dependent variable and some are statistically insignificant? We can definitely find out which are the variables that have the highest impact on the dependent variable. We\u2019ll want to find a team of variables that all have a definite effect, positive or negative.", "We need to prepare something specific for backward elimination. We want a library stats model, so let\u2019s import statsmodels.formula.api. That\u2019s a little long to have to keep retyping, so we\u2019ll make a shortcut using sm.", "We need to add a column of ones in our matrix of features of independent variables because of the way it works with the constant. (Our model needs to take into account our constant b0. In most libraries it\u2019s included, but not in the stats model that we\u2019re using. We\u2019ll add a column of ones so our stats model will understand the formula correctly.)", "This starts pretty simply. We\u2019ll use .append because we want to append.", "We have our matrix of features X. The values argument is perfect for us because it\u2019s an array. We\u2019ll input a matrix of 50 lines and one column with 1s inside. We can create that with Numpy\u2019s np.ones. We\u2019ll need to specify the numbers of lines and columns we want (50,1). We need to convert the array into the integer type to make this work, so we\u2019ll use .astype(int). Then we need to decide if we\u2019re adding a line or a column (line = 0, column = 1), so we\u2019ll say axis = 1 for a column!", "We want this column to be located at the beginning of our dataset. What do we do? Let\u2019s add matrix X to the column of 50 ones, rather than the other way around. We can do that with values = X.", "We want to create a new matrix of our optimal features (X_opt). These features are the ones that are statistically significant. The ones that have a high impact on the profit. This will be the matrix containing the team of optimal features with high impact on the profit.", "We\u2019ll need to initialize it. We can remove the variables that are not statistically significant one by one. We\u2019ll do this by removing the index at each step. First take all the indexes of the columns in X, separated by commas [0,1,2,3,4,5].", "If you look back at the methods earlier, you\u2019ll see that we first need to select our significance level, which we talked about earlier. Then we need to fit the model!", "We aren\u2019t going to take the regressor we built. We\u2019re using a new library, so now we need a new fit to our future optimal matrix. We\u2019ll create a new regressor (our last one was from the linear regression library). Our new class will be ordinary least squares (OLS). We\u2019ll need to call the class and specify some arguments. (You can check out the official documentation here.) For our arguments, we\u2019ll need an endog (our dependent variable) and an exog (our X_opt, which is just our matrix of features (X) with the intercept, which isn\u2019t included by default). In order to fit it we\u2019ll just use a .fit()!", "Now let\u2019s look at our P-values! How do we look for the predictor with the highest P-values? We\u2019ll take our regressor object and call the function .summary().", "Now we can see a table with some very useful information about our model! We can see the adjusted R-squared values and our P-values. The lower the p-value, the more significant your independent variable will be with respect to your dependent variable. Here, we\u2019re looking for the highest one. That\u2019s easy to see.", "We can copy and paste our code from above and remove index 2. That will look like this:", "Just keep going until you don\u2019t have any P-values that are higher than the SL value you chose. Remember that you always want to look at the original matrix in order to choose the correct index! You\u2019re using the columns in your original matrix (X), not in X_opt.", "You might get to the point where you have a P-value that\u2019s incredibly close to the SL value that you chose. For example, we chose 0.050 and here\u2019s 0.060.", "That\u2019s a tough situation because the value that you chose could have been anything. If you want to thoroughly follow your framework, you\u2019ll need to remove that index. But there are other metrics that can help make more sense of whether or not we want to do that. We could add other metrics, like a criterion, that can help us decide if we really want to make that choice. There\u2019s also a lot of information right in the summary here, like the R-squared value, that can help us make our decision.", "So let\u2019s say we ran backward elimination until the end and we\u2019re left with only the index for the R&D spending column.", "If we\u2019ve been following our model carefully, that means that we now know that R&D spending is a powerful predictor for our dependent variable! The conclusion here is that the data that can predict profits with the highest impact is composed of only one category: R&D spending!", "You did it! You used multiple linear regression and backward elimination! You figured out that looking at R&D spending will give you the best sense of what a company\u2019s profits will be!", "As always, if you\u2019re doing anything cool with this information, let people know about it in the responses below or reach out any time on LinkedIn @annebonnerdata!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building a community space at VentureBeat \ud83d\udcab Submit your articles here: http://bit.ly/submissions2023"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb8ba26192e84&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@annebonner?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@annebonner?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "Anne Bonner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa71060a2ef24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&user=Anne+Bonner&userId=a71060a2ef24&source=post_page-a71060a2ef24----b8ba26192e84---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8ba26192e84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8ba26192e84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://pixabay.com", "anchor_text": "Pixabay"}, {"url": "https://giphy.com/gifs/musicchoice-love-and-hip-hop-cardi-b-3oEjI6SEda9CiPEYXm", "anchor_text": "GIPHY"}, {"url": "https://towardsdatascience.com/simple-linear-regression-in-four-lines-of-code-d690fe4dba84", "anchor_text": "this article on building a simple linear regressor"}, {"url": "https://towardsdatascience.com/simple-linear-regression-in-four-lines-of-code-d690fe4dba84", "anchor_text": "The complete beginner\u2019s guide to machine learning: simple linear regression in four lines of code!A clear and comprehensive blueprint for absolutely anyone who wants to build a simple machine learning modeltowardsdatascience.com"}, {"url": "https://pixabay.com/users/RondellMelling-57942/", "anchor_text": "RondellMelling"}, {"url": "http://pixabay.com", "anchor_text": "Pixabay"}, {"url": "https://giphy.com/gifs/rihanna-gifs-gif-hunt-Ngqas56VMI1uU", "anchor_text": "GIPHY"}, {"url": "https://giphy.com/gifs/nervous-smiling-leonardo-dicaprio-wDmXCmtDHTdLi", "anchor_text": "GIPHY"}, {"url": "https://towardsdatascience.com/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d", "anchor_text": "this article on data cleaning and preprocessing"}, {"url": "https://towardsdatascience.com/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d", "anchor_text": "The complete beginner\u2019s guide to data cleaning and preprocessingHow to successfully prepare your data for a machine learning model in minutestowardsdatascience.com"}, {"url": "https://pixabay.com/users/3dman_eu-1553824/", "anchor_text": "3dman_eu"}, {"url": "http://pixabay.com", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/simple-linear-regression-in-four-lines-of-code-d690fe4dba84", "anchor_text": "that article on simple linear regression"}, {"url": "https://en.wikipedia.org/wiki/Akaike_information_criterion", "anchor_text": "Akaike criterion"}, {"url": "https://giphy.com/gifs/missy-elliott-timbaland-get-your-freak-on-SZtVAR4ZcZisU", "anchor_text": "GIPHY"}, {"url": "https://towardsdatascience.com/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d", "anchor_text": "hat data cleaning and preparation article"}, {"url": "https://unsplash.com/@gift?utm_source=medium&utm_medium=referral", "anchor_text": "Gift Habeshaw"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/simple-linear-regression-in-four-lines-of-code-d690fe4dba84", "anchor_text": "this article on building a simple linear regressor"}, {"url": "https://giphy.com/gifs/outkast-gWtupEAZizyFy", "anchor_text": "GIPHY"}, {"url": "https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html", "anchor_text": "the official documentation here"}, {"url": "https://giphy.com/gifs/television-rupauls-drag-race-glitter-8zcg4BB5NKQ5W", "anchor_text": "GIPHY"}, {"url": "https://www.linkedin.com/in/annebonnerdata/", "anchor_text": "@annebonnerdata"}, {"url": "https://mailchi.mp/00c59b0317a3/content-simplicity", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b8ba26192e84---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b8ba26192e84---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b8ba26192e84---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----b8ba26192e84---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b8ba26192e84---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8ba26192e84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&user=Anne+Bonner&userId=a71060a2ef24&source=-----b8ba26192e84---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8ba26192e84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&user=Anne+Bonner&userId=a71060a2ef24&source=-----b8ba26192e84---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8ba26192e84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb8ba26192e84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b8ba26192e84---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b8ba26192e84--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b8ba26192e84--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b8ba26192e84--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@annebonner?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@annebonner?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anne Bonner"}, {"url": "https://medium.com/@annebonner/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6.4K Followers"}, {"url": "http://bit.ly/submissions2023", "anchor_text": "http://bit.ly/submissions2023"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa71060a2ef24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&user=Anne+Bonner&userId=a71060a2ef24&source=post_page-a71060a2ef24--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F29e21558a591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-in-four-lines-of-code-b8ba26192e84&newsletterV3=a71060a2ef24&newsletterV3Id=29e21558a591&user=Anne+Bonner&userId=a71060a2ef24&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}