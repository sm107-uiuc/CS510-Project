{"url": "https://towardsdatascience.com/recurrence-in-biological-and-artificial-neural-networks-e8a6d5639781", "time": 1682996250.498982, "path": "towardsdatascience.com/recurrence-in-biological-and-artificial-neural-networks-e8a6d5639781/", "webpage": {"metadata": {"title": "Recurrence in biological and artificial neural networks | by Matthew Roos | Towards Data Science", "h1": "Recurrence in biological and artificial neural networks", "description": "Recurrence is an overloaded term in the context of neural networks, with disparate colloquial meanings in the machine learning and the neuroscience communities. The difference is narrowing, however\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/deep-learning-versus-biological-neurons-floating-point-numbers-spikes-and-neurotransmitters-6eebfa3390e9", "anchor_text": "yet still vastly different on the whole", "paragraph_index": 0}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "blog post", "paragraph_index": 8}, {"url": "http://www.scholarpedia.org/article/Attractor_network", "anchor_text": "attractor network", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53", "anchor_text": "this blog post", "paragraph_index": 13}], "all_paragraphs": ["Recurrence is an overloaded term in the context of neural networks, with disparate colloquial meanings in the machine learning and the neuroscience communities. The difference is narrowing, however, as the artificial neural networks (ANNs) used for practical applications are increasingly sophisticated and more like biological neural networks (BNNs) in some ways (yet still vastly different on the whole).", "In this post we\u2019ll discuss the historic differences in the use of term recurrence within these two communities, highlight some fairly recent deep learning ANN models that creep towards the neuroscience, point to some neuroscience studies that shine light on the function of recurrence, and speculate on future advancements.", "As many readers will be aware, deep learning networks are sub-type of ANNs in which the neurons (or nodes) are arranged into layers. The presence of many layers in such a network gives it a subjective depth that is it\u2019s namesake, contrasting with earlier researched networks of this type that had only one or two such layers. In a stereotypical fully-connected feed-forward deep learning network, all neurons in a given layer send their outputs to all neurons in the layer immediately following it (the directional flow of computations are typically schematized as going between layers from bottom-to-top or from left-to-right).", "One might also design networks in which neurons in a given layer send their outputs to the layer immediately preceding it, thus introducing feed-back connections between layers. We\u2019ll come back to that a bit later.", "Finally, a layer of neurons could send its outputs back to itself in a fully-connected (or other) fashion. Information stored in the layer recurs as input to that same layer in the next time/processing step. This is the type of recurrence that is nearly always meant when discussed by a deep learning practitioner \u2014 recurrence that is confined to a layer. (Note that there may be multiple recurrent layers, but the inter-layer connections are only feed-forward.)", "What this recurrent connectivity does is bestow memory in the recurrent neural network (RNN). The outputs of the network are no longer dependent solely on the input at the aligned time step. Rather, the network has a \u201cstate\u201d at any given time, which in combination with the next input provides a new output and also updates that network\u2019s state.", "This allows the RNNs to recognize or produce patterns that vary in their temporal structure, such as speech [1]. The utterances <sleep> and <sleeeep> can both be recognized as the word \u201csleep,\u201d for example. In fact, major advancements in the design and training methods of such sequence-to-sequence networks [2] is a key reason why speech recognition technologies have advanced so greatly in the past 2\u20133 years. Siri and Alexa may still be dumb as rocks, but at least they can translate your spoken words into text with great accuracy (though you may not always know it based on their responses!).", "Language translation for text has been another area of great success. The use of recurrence allows for information to be accumulated during an encoding phase and distributed (output across time) during a decoding phase, whereby a direct word-to-word or phrase-to-phrase alignment is not necessary [2]. For example, allowing modifiers that precede a word in one language to follow it in another, such as when translating red hat to sombrero rojo.", "We\u2019d be remiss not to mention that the \u201cvanilla\u201d RNN architecture described above is rarely used in practice. Advanced applications typically rely on human-devised modifications that accommodate gating mechanisms. In some sense, this allows the state memory of the recurrent layer to be \u201cdumped\u201d when a certain input is received or a certain output is delivered. As an analogy, when you finish a sentence, and the related thought, you may wish to dump that thought so it does not become muddled with your next one. Remarkably, one of the most common and effective gated layers, the long short-term memory (LSTM) layer, was originally created in 1997 [3], well before the recent advances in RNN-based applications. See Christopher Olah\u2019s blog post for an excellent tutorial on LSTMs.", "Among neuroscientists, recurrence has a boarder definition \u2014 based in part on the nearly isotropic connectivity patterns between neurons in biological neural networks (BNNs). Neurons are prolific in their axonal projections to other neurons, sending them both forwards and backwards, short distances and long distances. While there is strong evidence of a coarse hierarchical arrangement both structurally and functionally [4], the cortex of the brain is clearly not arranged into confined layers (groups) of neurons. The brain as a whole has distinct regions, with distinct types of neurons and neurotransmitters, but nothing like the compartmentalized connectivity that is a defining feature of deep learning ANNs. Nonetheless, what a deep learning practitioner calls recurrent connections are more likely to be called lateral connections by a neuroscientist.", "An aspect of recurrent networks that has been heavily research by computational neuroscientists is the pattern-completion property of a so-called attractor network. Consider how in our own minds it may take only a brief glimpse, a short burst of sound, or a scant whiff of an odor to bring about a strong, vibrant memory. Or when trying to recall an actor or actress\u2019s name we visualize their face, think of names of other performers they have worked with, movie titles, etc., until suddenly their name seems to magically pop into our heads. An analogy to this sort of phenomenon has been observed in simulations of recurrent attractor networks (an ANN, but without a deep learning structure, and often with inhibitory as well as excitatory artificial neurons, meant to be a more realistic model of BNNs). For example, a pattern of neural activity driven by the image of a face may also be driven by an obscured or noisy image of that same face, although the dynamics of the network take longer to evolve to a stable state in the latter case.", "What may be more important than the distinction between the confined recurrence (within layers) of deep learning ANNs versus the broad recurrence of BNNs is the lack of feed-back connectivity in most deep learning models. In the neuroscience community, the term recurrence is nearly synonymous with a mix of feed-back and feed-forward connections, and recent studies are providing new evidence of the role of feed-back.", "We\u2019ll briefly highlight the iterative sensory processing role of recurrent/feed-back connections in BNNs and contrast it with the feed-forward convolutional neural networks (CNNs) that dominate image classification tasks in deep learning ANNs.", "Deep learning object (image) recognition models have been a huge success in the field, and since the publication of the first CNN model to win the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), \u201cAlexNet\u201d [6], the field has grown rapidly. For a nice tutorial, see this blog post, or one of many others. Because the visual cortex is the most well-studied area of the mammalian cortex, many subjective and quantitative comparisons have been made between deep learning CNNs and mammalian vision.", "Earlier neuroscientific models of vision, based on recordings of individual neurons by Hubel and Weisel [7], and others, were similar to standard CNNs in that they had convolutions, pooling, and exclusively feed-forward connections [8, 9]. Part of the motivation for functional models that were exclusively feed-forward is the fact that visual perception happens quickly, on the order of 100 ms. This estimate is based on neural firing times in \u201chigher\u201d areas of the brain relative to the moment at which an image is shown to lab animals. Based on anatomy, the visual cortex is often modeled as a loose hierarchy of 4\u20136 levels with heavy feedback connectivity. Despite the feedback connections, the rapidity of neural responses at the higher levels suggested that the feedback connections are not altogether necessary (for a simple object recognition task). If that were not the case, stable responses would be slower to form in those areas because additional time would be needed for the contributions from the feedback loops to propagate.", "Yet CNNs require dozens if not hundreds of layers to achieve good image classification performance on the challenging ILSVRC test set, in contradiction to a model of the visual cortex composed of just a few feed-forward levels. In addition, in some computational studies, relatively shallow RNNs have performed comparably to very deep CNNs [10, 11].", "Recently, a pair of neuroscientific studies out of world-class labs, along with a more nuanced understanding of the time delays of biological recurrent connections, suggests that recurrence is required to capture the dynamic computations of the human visual cortex [12], and that recurrence is critical to the visual cortex\u2019s execution of object recognition behavior [13]. Simply put, the evidence suggests that more \u201cchallenging\u201d instances of object images could not be recognized without being iterated upon multiple times by the recurrent network. Said another way, additional nonlinear transformations were needed to successfully recognize objects in the challenge cases.", "As mentioned, while recurrent deep learning ANN models have within-layer recurrence (\u201clateral\u201d connectivity in neuroscience parlance), very few have the type of feed-back connections generally studied by neuroscientists \u2014 that is, connections from higher layers to lower layers. Notable exceptions include ANNs that model attention, and a handful of image classification models.", "One reason for the near absence of deep learning models with feed-back connectivity is the difficulty in training such models. We may need new learning rules (methods other than back-propagation) to achieve the type of functionality that feed-back provides in BNNs.", "Relatedly, biological neurons operate in parallel such that computations in a massive recurrent network can happen quickly. Indeed, the simultaneous computational update of neuronal states may be critical for success. This degree of parallelism can be difficult or impossible to implement for large, highly recurrent (neuroscience parlance) ANNs running on modern hardware.", "We speculate that the introduction of heavy feed-back recurrence into deep learning models, and the development of training methods for such models, will bring about powerful now AI capabilities. The rate of progress on these advances is difficult to predict, however.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe8a6d5639781&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@matt.roos?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matt.roos?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "Matthew Roos"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd82d4fe47b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&user=Matthew+Roos&userId=1fd82d4fe47b&source=post_page-1fd82d4fe47b----e8a6d5639781---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8a6d5639781&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8a6d5639781&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/deep-learning-versus-biological-neurons-floating-point-numbers-spikes-and-neurotransmitters-6eebfa3390e9", "anchor_text": "yet still vastly different on the whole"}, {"url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "anchor_text": "The Keras Blog"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "blog post"}, {"url": "http://www.scholarpedia.org/article/Attractor_network", "anchor_text": "attractor network"}, {"url": "http://jackterwilliger.com/attractor-networks/", "anchor_text": "blog post"}, {"url": "https://en.wikipedia.org/wiki/Two-streams_hypothesis#Criticisms", "anchor_text": "overly simplistic description"}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53", "anchor_text": "this blog post"}, {"url": "https://arxiv.org/abs/1211.3711", "anchor_text": "https://arxiv.org/abs/1211.3711"}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "https://arxiv.org/abs/1409.3215"}, {"url": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735", "anchor_text": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3532569/", "anchor_text": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3532569/"}, {"url": "https://www.nature.com/articles/nn.4244", "anchor_text": "https://www.nature.com/articles/nn.4244"}, {"url": "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ", "anchor_text": "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ"}, {"url": "https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1959.sp006308", "anchor_text": "https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1959.sp006308"}, {"url": "https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf", "anchor_text": "https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf"}, {"url": "https://www.nature.com/articles/nn1199_1019", "anchor_text": "https://www.nature.com/articles/nn1199_1019"}, {"url": "https://arxiv.org/abs/1604.03640", "anchor_text": "https://arxiv.org/abs/1604.03640"}, {"url": "https://arxiv.org/pdf/1612.09508.pdf", "anchor_text": "https://arxiv.org/pdf/1612.09508.pdf"}, {"url": "https://arxiv.org/abs/1903.05946", "anchor_text": "https://arxiv.org/abs/1903.05946"}, {"url": "https://www.nature.com/articles/s41593-019-0392-5", "anchor_text": "https://www.nature.com/articles/s41593-019-0392-5"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e8a6d5639781---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neuroscience?source=post_page-----e8a6d5639781---------------neuroscience-----------------", "anchor_text": "Neuroscience"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e8a6d5639781---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e8a6d5639781---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/inside-ai?source=post_page-----e8a6d5639781---------------inside_ai-----------------", "anchor_text": "Inside Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8a6d5639781&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&user=Matthew+Roos&userId=1fd82d4fe47b&source=-----e8a6d5639781---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8a6d5639781&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&user=Matthew+Roos&userId=1fd82d4fe47b&source=-----e8a6d5639781---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8a6d5639781&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe8a6d5639781&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e8a6d5639781---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e8a6d5639781--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e8a6d5639781--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e8a6d5639781--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matt.roos?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matt.roos?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matthew Roos"}, {"url": "https://medium.com/@matt.roos/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "164 Followers"}, {"url": "https://binarycognition.com/", "anchor_text": "https://binarycognition.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd82d4fe47b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&user=Matthew+Roos&userId=1fd82d4fe47b&source=post_page-1fd82d4fe47b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F87de77b0bed0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrence-in-biological-and-artificial-neural-networks-e8a6d5639781&newsletterV3=1fd82d4fe47b&newsletterV3Id=87de77b0bed0&user=Matthew+Roos&userId=1fd82d4fe47b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}