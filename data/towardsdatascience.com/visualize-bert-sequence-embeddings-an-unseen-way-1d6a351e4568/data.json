{"url": "https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568", "time": 1683018371.323284, "path": "towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568/", "webpage": {"metadata": {"title": "Visualize BERT sequence embeddings: An unseen way | by Tanmay Garg | Towards Data Science", "h1": "Visualize BERT sequence embeddings: An unseen way", "description": "Transformer-encoder based language models like BERT [1] have taken the NLP community by storm, with both research and development strata utilising these architectures heavily to solve their tasks\u2026"}, "outgoing_paragraph_urls": [{"url": "https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification", "anchor_text": "BertForSequenceClassification", "paragraph_index": 8}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm", "anchor_text": "BertForMaskedLM", "paragraph_index": 8}, {"url": "https://www.kaggle.com/tanmay17061/transformers-bert-hidden-embeddings-visualization", "anchor_text": "the companion Jupyter notebook", "paragraph_index": 10}, {"url": "https://www.kaggle.com/tanmay17061/transformers-bert-hidden-embeddings-visualization", "anchor_text": "this notebook", "paragraph_index": 11}, {"url": "https://www.kaggle.com/tanmay17061/transformers-bert-hidden-embeddings-visualization", "anchor_text": "the notebook", "paragraph_index": 11}, {"url": "https://seaborn.pydata.org/", "anchor_text": "Seaborn library", "paragraph_index": 15}, {"url": "http://Skit.ai", "anchor_text": "Skit.ai", "paragraph_index": 28}], "all_paragraphs": ["Transformer-encoder based language models like BERT [1] have taken the NLP community by storm, with both research and development strata utilising these architectures heavily to solve their tasks. They have become ubiquitous by displaying state-of-the-art results on a wide range of language tasks like text classification, next-sentence prediction, etc.", "The BERT-base architecture stacks 12 encoder-layers, which brings it up to a whopping 100 million tuneable parameters! The BERT-large architecture takes it yet a notch higher with 24 encoder-layers and ~350 million parameters! \ud83e\udd2f", "In the forward pass for an input text sequence, the output from each of these encoder blocks can be seen as a sequence of contextualised embeddings. Each contextualised embedding sequence is then fed as an input to the next layer.", "This repetitive application of encoder layers enables:", "It is a known fact that when using these deep architectures, it is very easy to fall into the pits of overfitting.", "Looking at how each encoder layer offers its own embeddings for the input, an interesting question may arise in one\u2019s head:\u201cAs I train my model, how effectively do each layer\u2019s embeddings generalize on unseen data?\u201dIn simpler words, it is of interest to see to what extent each layer of BERT is able to find patterns in data that hold on unseen data.", "In this tutorial we will be talking about a really cool way to visualize how effective each layer is in finding patterns for a classification task.", "With the motivation set, let\u2019s look at what we will be doing.", "Train a BERT model for multiple epochs, and visualize how well each layer separates out the data over these epochs. We will be training the BERT for a sequence classification task (using the BertForSequenceClassification class). The same exercise can be extended to other tasks with some tweaks in implementation details. For example, language modeling (using the BertForMaskedLM class). Re-train a language model on your own dataset, and inspect the characteristics of each cluster or the distribution of embeddings!", "HatEval [2], a dataset with tweets labeled as Hateful/Neutral.", "However, feel free to load your own dataset in the companion Jupyter notebook.", "I am placing the complete code for data loading, model training and embedding visualization in this notebook. The code present in this tutorial is intended only for explanation purposes. Please refer the notebook for complete working code.", "Since this article focuses only on the visualization of layer embeddings, we will be walking through only relevant parts of the code. Rest of the code lies outside the scope of this tutorial.I assume a prior knowledge of the \ud83e\udd17Transformers BERT basic workflow (data preparation, training/eval loops, etc).", "Extract Hidden States of each BERT encoder layer:", "Next, we define a function that can plot the layers\u2019 embeddings for a split of our dataset (eg- train/val/test) after an epoch:", "These computed values are finally plotted on a new plot using the Seaborn library.", "Finally, putting together what we\u2019ve seen till now inside a training loop:", "Here we call the visualize_layerwise_embeddings function once per epoch for every split of the dataset we want to visualize separately.", "I choose to visualize embeddings from the first 4 and last 4 layers.", "We have our visualizations ready!I took a step further to make things more convenient by stitching the different images into a gif! Once again, code is present in the notebook.Pretty aesthetic, right?\ud83e\udd29", "Spend a moment on each layer\u2019s output. Try to draw some interesting inferences out of them!", "I\u2019ll give some examples:- can you comment on how each layer is performing with each successive epoch?- the train accuracy of the classifier dropped from epoch 4 to epoch 5! Can you verify this fact from the above gif? **", "Finally, we are more interested in knowing if our embeddings are helping us generalize. We can judge that by the validation split visualizations above.", "Some interesting questions from the top of my head are:- Which layer are generalizing better that the others?- How well is the last layer able to separate out the classes?- Do you see any difference in the separability between the embeddings for train and validation splits?- Does taking an average of the embeddings across all non-masked tokens of the sequence produce better results that taking embedding only for \u2018[CLS]\u2019 token? (You might have to tweak the notebook a little to answer this one\ud83d\ude09)", "Don\u2019t stop just here, yet!Go and play around in the notebook provided, and try to mix and match different layers\u2019 output embeddings to see which combination helps you produce the best downstream performance!", "** Answer: the embeddings for the 12th layer are more neatly clustered for epoch 4 as compared to epoch 5! It is a clear indicator of the classifier having hit and then over-shot a minima in the loss-function space.", "[1]: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Delvin et al., 2019[2]: SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter, Basile et al., 2019", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer @ Skit.ai | IIIT Delhi | NLP | ML"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d6a351e4568&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tanmay17061.medium.com/?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": ""}, {"url": "https://tanmay17061.medium.com/?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "Tanmay Garg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73cb2d1309f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&user=Tanmay+Garg&userId=73cb2d1309f8&source=post_page-73cb2d1309f8----1d6a351e4568---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d6a351e4568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d6a351e4568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@filisantillan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Filiberto Santill\u00e1n"}, {"url": "https://unsplash.com/s/photos/data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification", "anchor_text": "BertForSequenceClassification"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm", "anchor_text": "BertForMaskedLM"}, {"url": "https://www.kaggle.com/tanmay17061/transformers-bert-hidden-embeddings-visualization", "anchor_text": "the companion Jupyter notebook"}, {"url": "https://www.kaggle.com/tanmay17061/transformers-bert-hidden-embeddings-visualization", "anchor_text": "this notebook"}, {"url": "https://www.kaggle.com/tanmay17061/transformers-bert-hidden-embeddings-visualization", "anchor_text": "the notebook"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForSequenceClassification.forward", "anchor_text": "forward pass"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html", "anchor_text": "PCA"}, {"url": "https://seaborn.pydata.org/", "anchor_text": "Seaborn library"}, {"url": "https://medium.com/tag/nlp?source=post_page-----1d6a351e4568---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/bert?source=post_page-----1d6a351e4568---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/visualization?source=post_page-----1d6a351e4568---------------visualization-----------------", "anchor_text": "Visualization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1d6a351e4568---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/transformers?source=post_page-----1d6a351e4568---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d6a351e4568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&user=Tanmay+Garg&userId=73cb2d1309f8&source=-----1d6a351e4568---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d6a351e4568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&user=Tanmay+Garg&userId=73cb2d1309f8&source=-----1d6a351e4568---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d6a351e4568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d6a351e4568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d6a351e4568---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d6a351e4568--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d6a351e4568--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d6a351e4568--------------------------------", "anchor_text": ""}, {"url": "https://tanmay17061.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tanmay17061.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tanmay Garg"}, {"url": "https://tanmay17061.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "46 Followers"}, {"url": "http://Skit.ai", "anchor_text": "Skit.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73cb2d1309f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&user=Tanmay+Garg&userId=73cb2d1309f8&source=post_page-73cb2d1309f8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6f621934819d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568&newsletterV3=73cb2d1309f8&newsletterV3Id=6f621934819d&user=Tanmay+Garg&userId=73cb2d1309f8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}