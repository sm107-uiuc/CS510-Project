{"url": "https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48", "time": 1683004407.718604, "path": "towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48/", "webpage": {"metadata": {"title": "What is Gradient Clipping?. A simple yet effective way to tackle\u2026 | by Wanshun Wong | Towards Data Science", "h1": "What is Gradient Clipping?", "description": "Recurrent Neural Networks (RNN) work very well with sequential data by utilizing hidden states that stores information about past inputs: the values of hidden states at time t depend on their values\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Recurrent Neural Networks (RNN) work very well with sequential data by utilizing hidden states that stores information about past inputs: the values of hidden states at time t depend on their values at time t -1 and also the inputs at time t. This architecture, while powerful, causes two problems in training: exploding gradients and vanishing gradients. In this article, we will look into gradient clipping which deals with the exploding gradients problem.", "Exploding gradients refer to the problem that the gradients get too large in training, making the model unstable. Similarly, vanishing gradients refer to gradients getting too small in training. This prevents the network weights from changing their values. Both problems cause the model unable to learn from the training data. The following informal discussion is not that rigorous but is sufficient to give us an intuition about the source of exploding and vanishing gradients.", "When we train a RNN by Backpropagation Through Time, it means we first unroll the RNN in time by creating a copy of the network for each time step, viewing it as a multi-layer feedforward neural network, where the number of layers is equal to the number of time steps. Then we do backpropagation on the unrolled network, taking into account the weight sharing:", "where W is the recurrent weight matrix. It can be shown that the gradient of the loss function consists of a product of n copies of W\u1d40, where n is the number of layers going back in time. This product of matrices is the source of exploding and vanishing gradients.", "For a scaler a \u2260 1, a\u207f shrink or grow exponentially. For example, consider n = 30. Then, for instance, 1.1\u207f \u2248 17.45 and 0.9\u207f \u2248 0.042. We pick 30 because it is quite common for a sentence in a Natural Language Processing task to have 30 words, and it is also quite typical for time series analysis to process 30 days of data. The situation for products of matrices (W\u1d40)\u207f is very similar. The easiest way to see this is to assume W\u1d40 is diagonalizable. Then W\u1d40 = QDQ\u207b\u00b9 for some diagonal matrix D = diag(\u03bb\u2081, \u2026, \u03bb\u2093), and (W\u1d40)\u207f = QD\u207fQ\u207b\u00b9 with D\u207f = diag(\u03bb\u2081\u207f, \u2026, \u03bb\u2093\u207f).", "We refer to [2] for a rigorous treatment of the exploding and vanishing gradients problems.", "Gradient clipping is a technique that tackles exploding gradients. The idea of gradient clipping is very simple: If the gradient gets too large, we rescale it to keep it small. More precisely, if \u2016g\u2016 \u2265 c, then", "where c is a hyperparameter, g is the gradient, and \u2016g\u2016 is the norm of g. Since g/\u2016g\u2016 is a unit vector, after rescaling the new g will have norm c. Note that if \u2016g\u2016 < c, then we don\u2019t need to do anything.", "Gradient clipping ensures the gradient vector g has norm at most c. This helps gradient descent to have a reasonable behaviour even if the loss landscape of the model is irregular. The following figure shows an example with an extremely steep cliff in the loss landscape. Without clipping, the parameters take a huge descent step and leave the \u201cgood\u201d region. With clipping, the descent step size is restricted and the parameters stay in the \u201cgood\u201d region.", "Q: How do we choose the hyperparameter c?", "A: We can train our neural networks for some epochs and look at the statistics of the gradient norms. The average value of gradient norms is a good initial trial.", "Q: Can we use gradient clipping in training neural architectures other than RNN?", "A: Yes. We can use gradient clipping for any neural architectures whenever we have exploding gradients.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb8e815cdfb48&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wanshunwong?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wanshunwong?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "Wanshun Wong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb145fb04b8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&user=Wanshun+Wong&userId=b145fb04b8bd&source=post_page-b145fb04b8bd----b8e815cdfb48---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8e815cdfb48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8e815cdfb48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@ahsan19?utm_source=medium&utm_medium=referral", "anchor_text": "Ahsan S."}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm", "anchor_text": "tf.clip_by_global_norm"}, {"url": "https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_", "anchor_text": "torch.nn.utils.clip_grad_norm_"}, {"url": "http://www.deeplearningbook.org/", "anchor_text": "Deep Learning"}, {"url": "http://proceedings.mlr.press/v28/pascanu13.pdf", "anchor_text": "On the difficulty of training Recurrent Neural Networks"}, {"url": "https://openreview.net/pdf?id=BJgnXpVYwS", "anchor_text": "Why gradient clipping accelerates training: A theoretical justification for adaptivity"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----b8e815cdfb48---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/gradient-clipping?source=post_page-----b8e815cdfb48---------------gradient_clipping-----------------", "anchor_text": "Gradient Clipping"}, {"url": "https://medium.com/tag/exploding-gradient?source=post_page-----b8e815cdfb48---------------exploding_gradient-----------------", "anchor_text": "Exploding Gradient"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8e815cdfb48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&user=Wanshun+Wong&userId=b145fb04b8bd&source=-----b8e815cdfb48---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8e815cdfb48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&user=Wanshun+Wong&userId=b145fb04b8bd&source=-----b8e815cdfb48---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8e815cdfb48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb8e815cdfb48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b8e815cdfb48---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b8e815cdfb48--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wanshunwong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wanshunwong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wanshun Wong"}, {"url": "https://medium.com/@wanshunwong/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "263 Followers"}, {"url": "https://www.linkedin.com/in/wanshunwong/", "anchor_text": "https://www.linkedin.com/in/wanshunwong/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb145fb04b8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&user=Wanshun+Wong&userId=b145fb04b8bd&source=post_page-b145fb04b8bd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa149bfd5dd79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-gradient-clipping-b8e815cdfb48&newsletterV3=b145fb04b8bd&newsletterV3Id=a149bfd5dd79&user=Wanshun+Wong&userId=b145fb04b8bd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}