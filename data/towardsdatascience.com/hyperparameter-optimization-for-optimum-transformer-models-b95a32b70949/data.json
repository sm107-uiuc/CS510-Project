{"url": "https://towardsdatascience.com/hyperparameter-optimization-for-optimum-transformer-models-b95a32b70949", "time": 1683010826.387866, "path": "towardsdatascience.com/hyperparameter-optimization-for-optimum-transformer-models-b95a32b70949/", "webpage": {"metadata": {"title": "Hyperparameter Optimization for Optimum Transformer Models | by Thilina Rajapakse | Towards Data Science", "h1": "Hyperparameter Optimization for Optimum Transformer Models", "description": "The goal of any Deep Learning model is to take in an input and generate the correct output. The nature of these inputs and outputs, which can vary wildly from application to application, depends on\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 8}, {"url": "https://www.wandb.com/", "anchor_text": "W&B", "paragraph_index": 8}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 9}, {"url": "https://aclweb.org/aclwiki/Recognizing_Textual_Entailment", "anchor_text": "Textual Entailment", "paragraph_index": 10}, {"url": "https://super.gluebenchmark.com/", "anchor_text": "SuperGLUE", "paragraph_index": 10}, {"url": "https://aclweb.org/aclwiki/Textual_Entailment", "anchor_text": "Textual Entailment", "paragraph_index": 11}, {"url": "https://aclweb.org/aclwiki/index.php?title=Natural_Language_Processing&action=edit&redlink=1", "anchor_text": "NLP", "paragraph_index": 11}, {"url": "https://aclweb.org/aclwiki/index.php?title=Question_Answering&action=edit&redlink=1", "anchor_text": "Question Answering", "paragraph_index": 11}, {"url": "https://aclweb.org/aclwiki/index.php?title=Information_Retrieval&action=edit&redlink=1", "anchor_text": "Information Retrieval", "paragraph_index": 11}, {"url": "https://aclweb.org/aclwiki/Information_Extraction", "anchor_text": "Information Extraction", "paragraph_index": 11}, {"url": "https://aclweb.org/aclwiki/Text_Summarization", "anchor_text": "Text Summarization", "paragraph_index": 11}, {"url": "https://simpletransformers.ai/docs/classification-data-formats/#sentence-pair-data-format", "anchor_text": "input format", "paragraph_index": 14}, {"url": "https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://simpletransformers.ai/docs/classification-models/#configuring-a-classification-model", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/runs/1bxg7keb?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://simpletransformers.ai/docs/usage/#hyperparameter-optimization", "anchor_text": "docs", "paragraph_index": 31}, {"url": "https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model", "anchor_text": "configuration options", "paragraph_index": 34}, {"url": "https://docs.wandb.com/sweeps/configuration#stopping-criteria", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://simpletransformers.ai/docs/usage/#additional-evaluation-metrics", "anchor_text": "docs", "paragraph_index": 43}, {"url": "https://docs.wandb.com/app/panels/parameter-importance", "anchor_text": "Importance", "paragraph_index": 48}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/sweeps/10e2ucii?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 53}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/runs/1epxu37u?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 60}, {"url": "https://simpletransformers.ai/docs/usage/#custom-parameter-groups-freezing-layers", "anchor_text": "different learning rates", "paragraph_index": 64}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/sweeps/nx4f206f?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 86}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 99}], "all_paragraphs": ["The goal of any Deep Learning model is to take in an input and generate the correct output. The nature of these inputs and outputs, which can vary wildly from application to application, depends on the specific job that the model should perform. For example, a dog breed classification model might take images as its input and generate the name of the dog breed (or a numeric label corresponding to the breed) as the output. Another model might accept a text description of a dog as its input and generate the name of the dog breed as its output. The first model is an example of a computer vision model, whereas the latter is an example of a natural language processing (NLP) model.", "The internals of both models will consist of many fancy parts (convolutional layers, attention mechanisms, etc.), each tailored for their specific task. From a high-level perspective, all of these components constitute a set of parameters (or weights) which determine what the output will be for any given input. Training a deep learning model is the process of finding the set of values for these parameters which yield the best results on the given task.", "By contrast, hyperparameters are the factors which control the training process itself. The learning rate, the number of training epochs/iterations, and the batch size are some examples of common hyperparameters. The values chosen for the hyperparameters has a significant impact on the learned parameters, and by extension, the performance of the model.", "In a nutshell, the parameters are what the model learns, and the hyperparameters determine how well (or how badly) the model learns.", "Just like we have various techniques to train model parameters, we also have methods to find the best hyperparameter values. The process of finding the best hyperparameter values, which enables the model to discover the best set of parameters to perform a given task, is hyperparameter optimization.", "As a loose analogy, think about overclocking a CPU. By optimizing things like voltages, temperatures, clock frequencies, etc. (the hyperparameters), you can get the CPU to perform at higher speeds despite not changing the CPU architecture (the model), or the components of the CPU (the parameters of the model).", "Knowing what hyperparameters optimization is, you might wonder whether it is needed whenever you train a model. After all, many of us don\u2019t bother with overclocking our CPUs considering they typically perform well out-of-the-box. Just like with modern-day CPUs, state-of-the-art deep learning models can generally perform well even without hyperparameter optimization. As long as you stick to sensible defaults, the power of SOTA pre-trained models combined with transfer learning is sufficient to produce a model with satisfactory performance.", "But, when you don\u2019t consider \u201cgood enough\u201d to be good enough, hyperparameter optimization is a vital tool in your toolbox to help your model go the extra mile.", "Simple Transformers is a library designed to make the training and usage of Transformer models as easy as possible. In keeping with this idea, it has native support for hyperparameter optimization through the W&B Sweeps feature.", "Simple Transformers is built upon the incredible Transformers library by Hugging Face whose contribution to making NLP accessible to everyone cannot be overstated!", "This article will focus on using the Simple Transformers library, along with W&B Sweeps, to perform hyperparameter optimization on the Recognizing Textual Entailment [1] task of the SuperGLUE [2] benchmark. The task is a binary classification on a sentence pair as described below.", "Textual Entailment Recognition has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Text Summarization. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text.", "In this guide, we will conduct three main tasks to highlight the value of hyperparameter optimization and to see how you can customize the optimization process.", "For each task, we will be training RoBERTa-Large [3] models on the RTE dataset. Let\u2019s get our development environment set up and the dataset downloaded so we can get down to training!", "The function given below can be used to read these jsonl files and convert the data into the Simple Transformers input format (Pandas DataFrame with the three columns text_a, text_b, labels).", "Since we will be using this function in many places, add it to a file utils.py at the root of the project so that it can be imported as needed.", "The RTE dataset contains three sub-datasets.", "To avoid having to submit predictions to test our final models, we will split the validation set into two random parts and use one for validation (eval_df) and the other for testing (test_df).", "Running the data_prep.py file shown above will create eval_df and test_df which we will use to validate and test our models.", "With experience, most people tend to develop an intuition about important hyperparameters and what values will work for those hyperparameters. In my experience, two of the most critical hyperparameters to consider when training a Transformer model on an NLP task is the learning rate and the number of training epochs.", "Training for too many epochs or using too high a learning rate typically leads to catastrophic forgetting with the model often resorting to generating the same output/label to any given input. On the other hand, an insufficient number of training epochs or too low a learning rate results in a sub-par model.", "My go-to values for these two hyperparameters is usually a learning rate of 5e-5 and 2 or 3 training epochs (I increase the number of training epochs for smaller datasets).", "However, I found that my intuition has failed me when I applied these values to training a model on the RTE dataset. The model ended up predicting the same label for all inputs, suggesting that the learning rate is too high. Lowering the learning rate to 1e-5 was sufficient to avoid this issue.", "The script below shows how you can train a model with sensible defaults.", "You can refer to the Simple Transformers docs (here and here) for details on the model_args attributes if needed.", "As the RTE task uses accuracy as the metric in the SuperGLUE benchmark, we will do the same.", "The model achieves an accuracy of 0.8116 using the sensible default hyperparameter values. The confusion matrix for the model is given below.", "You can also see the full training progress as well as other metrics here.", "Not bad, but let\u2019s see what hyperparameter optimization can do.", "The W&B Sweeps [4] integration in Simple Transformers simplifies the process of conducting hyperparameter optimization.", "The Sweep configuration can be defined through a Python dictionary which specifies the metric to be optimized, the search strategy to use, and the hyperparameters to be optimized.", "I highly recommend looking through the docs for more information regarding how a Sweep can be configured.", "For our basic hyperparameter optimization, we will focus on the same two hyperparameters as in the earlier section, i.e. learning rate and the number of training epochs.", "The Sweep configuration can be defined as shown below.", "Note: You can use any of the configuration options of a Simple Transformer model as a parameter to be optimized during the Sweep.", "Here, we are using the bayes (Bayesian Optimization) search strategy to optimize the hyperparameters.", "Bayesian Optimization uses a gaussian process to model the function and then chooses parameters to optimize the probability of improvement.", "The metric we want to optimize is the accuracy and the goal is obviously to maximize it. Note that the metric to be optimized must be logged to W&B.", "The parameters to be optimized is the learning_rate and the num_train_epochs. Here, the learning rate can take any value from 0 to 4e-4 while the number of training epochs can be any integer from 1 to 40.", "W&B Sweeps can also speed up the hyperparameter optimization by terminating any poorly performing runs (early_terminate). This uses the hyperband algorithm, as explained here.", "The final requirement for the Sweep is a function which can be called to train a model with a given set of hyperparameter values.", "The function will initialize the wandb run, set up the Simple Transformers model, train the model, and finally sync the results.", "The set of hyperparameter values for the current run are contained in wandb.config, which can be passed to a Simple Transformers model. All Simple Transformers models accept a sweep_config keyword argument and will automatically update the model_args based on the wandb.config passed to sweep_config.", "We also make sure that the accuracy metric is calculated on the validation set (eval_df) as the model is trained. All eval_model() and train_model() methods in Simple Transformers accepts keyword-arguments consisting of the name of a metric and a metric function. This is used here to calculate the accuracy on the eval_df. (More info in the docs)", "Putting all of this together into a single Python file gives us the script below.", "The W&B dashboard provides many graphs and visualizations which include a lot of valuable information. The graphic below plots the accuracy obtained by each model trained during the Sweep (~13 hours for 48 runs).", "Breaking down these 48 runs according to the accuracy achieved gives us the following:", "These results might seem a little weird (multiple runs with the exact same accuracy score) until you consider the fact that Transformer models can be prone to breaking entirely when trained with bad hyperparameter values. This leads to the model predicting the same label for any input, explaining the models with identical accuracy scores (there are two possible accuracies due to the imbalanced labels in the test set).", "This hypothesis is confirmed in the Parameter Importance visualization.", "Here, we have the impact of the two hyperparameters on the final accuracy obtained. The learning_rate has a very high negative correlation to the accuracy as high learning rates results in models which predict the same label for all inputs.", "The Parallel Coordinates plot below bears out the same theory.", "Focusing on the runs with accuracy above 0.8 further reinforces this idea.", "Interestingly, the number of training epochs also starts to play a more significant role when using learning rate values in this range.", "Feel free to dig into the Sweep results here!", "Now, let\u2019s see how a model trained with these hyperparameters performs on the test set.", "Reminder: The model is trained on the train set, the Sweep is evaluated on the validation set (eval_df), the final model (using the best hyperparameter values) will be assessed on the test set (test_df).", "This script uses the hyperparameter values which yielded the best accuracy on eval_df during the sweep.", "The model trained with these hyperparameter values obtains an accuracy of 0.8768, a significant improvement over the sensible defaults model (0.8116).", "Looking at the Confusion Matrix for the predictions on the test set:", "Clearly, our hyperparameter optimization has paid off!", "You can also see the full training progress as well as other metrics here.", "In the previous section, we saw that the learning rate plays a vital role in the performance of a trained model. Having too high a learning rate results in broken models, while training a model with a too low learning rate can result in an under fitted model or a model stuck in local minima.", "One potential solution to this conundrum relies on the fact that the learning rate doesn\u2019t seem to affect all parts of the model equally. For instance, the classification layer added on top of the Transformer model layers is far less prone to breaking under higher learning rates. This is likely because the classification layer is initialized randomly when fine-tuning a pre-trained model. I.e. the classification layer is not a pre-trained layer and therefore doesn\u2019t have any acquired knowledge to lose through catastrophic forgetting. On the other hand, the classification layer is potentially more likely to get stuck in local minima (with low learning rates), precisely because it is randomly initialized.", "The same phenomena can be observed, albeit to a lesser extent (and with less consistency), when comparing the impact of the learning rate on the early layers Vs the latter layers. The earlier layers can be more prone to catastrophic forgetting than the final layers.", "With Simple Transformers, we can define different learning rates for each named parameter (weight) in a Transformer model. Conveniently, we can also set learning rates for any given layer in the model (24 layers for RoBERTa-LARGE). Let\u2019s see if we can combine this feature with the knowledge we have regarding the effect of the learning rate to push our model towards even better performance.", "While we can try to optimize the learning rate for each layer separately, I am choosing to bundle the layers into four equal groups, each containing six consecutive layers. This should make it easier for us to visualize the Sweep results and may also make it easier for the Sweep to optimize the hyperparameters as there are fewer variables.", "The classification layer of a RoBERTa model has four named parameters, each of which will be optimized individually. Alternatively, these could also be combined into a single group (this may be the more logical choice), but I am keeping them separate for demonstration purposes.", "Tip: All Simple Transformers models have a get_named_parameters() method which returns a list of all parameter names in the model.", "Again, we will start by setting up a configuration for our Sweep.", "We are using the insights gained from the basic hyperparameter optimization to set a smaller maximum value for the learning rates of the Transformer model layers while giving more leeway to the classification layer parameters.", "A quick side-note on using custom parameter groups with Simple Transformers adapted from the docs:", "The model_args (in this case, a ClassificationArgs object) of a Simple Transformers model has three attributes associated with configuring custom parameter groups.", "custom_layer_parameters makes it more convenient to set the (PyTorch) optimizer options for a given layer or set of layers. This should be a list of Python dicts where each dict contains a layer key and any other optional keys matching the keyword arguments accepted by the optimizer (e.g. lr, weight_decay). The value for the layer key should be an int (must be numeric) which specifies the layer (e.g. 0, 1, 11).", "custom_parameter_groups offers the most granular configuration option. This should be a list of Python dicts where each dict contains a params key and any other optional keys matching the keyword arguments accepted by the optimizer (e.g. lr, weight_decay). The value for the params key should be a list of named parameters (e.g. [\"classifier.weight\", \"bert.encoder.layer.10.output.dense.weight\"]).", "The train_custom_parameters_only option is used to facilitate the training of specific parameters only. If train_custom_parameters_only is set to True, only the parameters specified in either custom_parameter_groups or in custom_layer_parameters will be trained.", "While you can use any of the configuration options available in a Simple Transformers model as a hyperparameter to be optimized, W&B Sweeps do not currently support configurations with nested parameters. This means that configuration options where the expected data type is a collection (dictionary, list, etc.) cannot be configured directly in a Sweep configuration. However, we can easily handle this logic ourselves.", "Looking at the sweep_config we defined earlier, we can observe that none of the parameters is nested, despite the fact that we want multiple layers with the same learning rate.", "The Sweep will give us a single value (per run) for each parameter defined in the config. We will take care of converting this into the format expected by Simple Transformers in the train() function for the Sweep.", "The names of the parameters in the sweep_config were chosen to make the conversion relatively straightforward.", "Based on this naming convention, let\u2019s see how we can parse the sweep_config to extract the proper hyperparameter values.", "First, we grab the hyperparameter values assigned to the current run. These can be accessed through the wandb.config object inside the train() method of a Sweep. Fortunately, we can convert the wandb.config to a Python dictionary which can be parsed easily.", "We also remove the _wandb key and rearrange the other dictionary items into a direct mapping from the parameter name to its current value.", "Next, we iterate through each of the items in the dictionary and build a dictionary in the format expected by Simple Transformers. Finally, we update the model_args (a ClassificationArgs object) with the dictionary values.", "Combine all of this into a Python script and we are off to the races!", "In the advanced hyperparameter optimization Sweep, thirty-eight (38) of forty (40) runs achieve an accuracy higher than 0.8 (compared to 10 of 48 in the basic optimization).", "The Parallel Coordinates plot is a little harder to visualize when we include all the runs, but we can focus on individual groups to get a clearer picture.", "I recommend checking out the Parallel Plot (here) yourself as it has many interactive features which do a much better job of visualizing the Sweep than individual images.", "While this plot is admittedly a little noisy, we can still see that the better models (lighter green) do have lower learning rates for the first half of the 24 layers and higher learning rates for the other half.", "The Parallel Coordinates Plot for the classification layers seems to be even noisier than the plot for the Transformer model layers. This may indicate that the classification layers are comparatively more robust and can learn good weights as long as the learning rate is high enough to avoid getting stuck in local minima.", "Finally, let\u2019s see how a model trained with the best hyperparameters found from the advanced Sweep will perform on the test set.", "To download a CSV file with the hyperparameters for the best run, go to the Sweep table in the W&B dashboard, search for deep-sweep (name of the best run), and click the download button (top-right). Create a directory sweep_results and save the CSV file as sweep_results/deep-sweep.csv.", "The script below will train the model and evaluate it against the test set.", "The hyperparameter values extracted from deep-sweep.csv are shown below.", "The model achieves a final accuracy score of 0.8913 on the test set.", "Looking at the Confusion Matrix for the predictions on the test set:", "The accuracy gain from advanced hyperparameter optimization vs basic hyperparameter optimization (0.8913 vs 0.8768) is far more modest compared to the accuracy gain from basic hyperparameter optimization vs sensible defaults (0.8768 vs 0.8116). However, it is still a noticeable improvement over basic hyperparameter optimization.", "[1] Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing (pp. 1\u20139).", "[2] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv preprint 1905.00537.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb95a32b70949&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b95a32b70949--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b95a32b70949--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----b95a32b70949--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----b95a32b70949--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----b95a32b70949---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb95a32b70949&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb95a32b70949&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@hansenhouse?utm_source=medium&utm_medium=referral", "anchor_text": "Glenn Hansen"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://www.wandb.com/", "anchor_text": "W&B"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://aclweb.org/aclwiki/Recognizing_Textual_Entailment", "anchor_text": "Textual Entailment"}, {"url": "https://super.gluebenchmark.com/", "anchor_text": "SuperGLUE"}, {"url": "https://aclweb.org/aclwiki/Textual_Entailment", "anchor_text": "Textual Entailment"}, {"url": "https://aclweb.org/aclwiki/index.php?title=Natural_Language_Processing&action=edit&redlink=1", "anchor_text": "NLP"}, {"url": "https://aclweb.org/aclwiki/index.php?title=Question_Answering&action=edit&redlink=1", "anchor_text": "Question Answering"}, {"url": "https://aclweb.org/aclwiki/index.php?title=Information_Retrieval&action=edit&redlink=1", "anchor_text": "Information Retrieval"}, {"url": "https://aclweb.org/aclwiki/Information_Extraction", "anchor_text": "Information Extraction"}, {"url": "https://aclweb.org/aclwiki/Text_Summarization", "anchor_text": "Text Summarization"}, {"url": "https://aclweb.org/aclwiki/Recognizing_Textual_Entailment", "anchor_text": "Recognizing Textual Entailment"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "here"}, {"url": "https://github.com/NVIDIA/apex", "anchor_text": "here"}, {"url": "https://dl.fbaipublicfiles.com/glue/superglue/data/v2/RTE.zip", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/classification-data-formats/#sentence-pair-data-format", "anchor_text": "input format"}, {"url": "https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/classification-models/#configuring-a-classification-model", "anchor_text": "here"}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/runs/1bxg7keb?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/usage/#hyperparameter-optimization", "anchor_text": "docs"}, {"url": "https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model", "anchor_text": "configuration options"}, {"url": "https://docs.wandb.com/sweeps/configuration#search-strategy", "anchor_text": "W&B docs"}, {"url": "https://docs.wandb.com/sweeps/configuration#stopping-criteria", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/usage/#additional-evaluation-metrics", "anchor_text": "docs"}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/sweeps/10e2ucii?workspace=user-thilina", "anchor_text": "Accuracy vs Created"}, {"url": "https://docs.wandb.com/app/panels/parameter-importance", "anchor_text": "Importance"}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/sweeps/10e2ucii?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/runs/1epxu37u?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/usage/#custom-parameter-groups-freezing-layers", "anchor_text": "different learning rates"}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/sweeps/nx4f206f?workspace=user-thilina", "anchor_text": "Accuracy vs Created"}, {"url": "https://app.wandb.ai/thilina/RTE%20-%20Hyperparameter%20Optimization/sweeps/nx4f206f?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://neptune.ai/blog/hyperparameter-tuning-in-python-a-complete-guide-2020", "anchor_text": "this"}, {"url": "https://docs.wandb.com/sweeps", "anchor_text": "https://docs.wandb.com/sweeps"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b95a32b70949---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b95a32b70949---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b95a32b70949---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----b95a32b70949---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb95a32b70949&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----b95a32b70949---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb95a32b70949&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----b95a32b70949---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb95a32b70949&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b95a32b70949--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb95a32b70949&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b95a32b70949---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b95a32b70949--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b95a32b70949--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b95a32b70949--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b95a32b70949--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b95a32b70949--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b95a32b70949--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b95a32b70949--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b95a32b70949--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-for-optimum-transformer-models-b95a32b70949&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}