{"url": "https://towardsdatascience.com/how-to-fool-a-neural-network-958ba5d82d8a", "time": 1683012594.839763, "path": "towardsdatascience.com/how-to-fool-a-neural-network-958ba5d82d8a/", "webpage": {"metadata": {"title": "How to fool a Neural Network?. With some adversarial inputs, neural\u2026 | by Aakarsh Yelisetty | Towards Data Science", "h1": "How to fool a Neural Network?", "description": "Imagine you\u2019re in the year 2050 and you\u2019re on your way to work in a self-driving car (probably). Suddenly, you realize your car is cruising at 100KMPH on a busy road after passing through a cross\u2026"}, "outgoing_paragraph_urls": [{"url": "http://karpathy.github.io/2015/03/30/breaking-convnets/", "anchor_text": "here", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1412.6572.pdf", "anchor_text": "Explaining and Harnessing Adversarial Examples", "paragraph_index": 18}, {"url": "https://colab.research.google.com/drive/1trIwmGKQB-62pniDZnCby7y24i-BY217?usp=sharing", "anchor_text": "notebook", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "paper", "paragraph_index": 26}, {"url": "https://www.linkedin.com/in/aakarsh-yelisetty-6b691b171/", "anchor_text": "LinkedIn", "paragraph_index": 36}], "all_paragraphs": ["Imagine you\u2019re in the year 2050 and you\u2019re on your way to work in a self-driving car (probably). Suddenly, you realize your car is cruising at 100KMPH on a busy road after passing through a cross lane and you don\u2019t know why.", "Well, there might be many reasons. But in this article, we are going to focus on one particular reason \u2014 the car was fooled.", "To be precise, the neural network that saw a signboard at the intersection was tricked into thinking a STOP sign as a 100KMPH sign and that resulted in its instantaneous acceleration.", "Yes, it is. But before getting deep into it first, let\u2019s understand what a neural network sees after it gets trained. It is believed that every independent neuron in the network works similarly as of our biological neuron and we assume that the neural network thinks the same as of our brain when it looks at an image. Practically, it isn\u2019t the case. Let\u2019s look at it with an example.", "Guess what the below image is.", "You guessed it right. It\u2019s a temple and the neural network predicts it as a temple with 97% confidence.", "Now, guess what this image is.", "They look identical but they aren\u2019t. The above image is predicted as an ostrich with 98% confidence by the same model we used for the previous one. The network is fooled by this image now. But how?", "This second image didn\u2019t come from a real-world camera but instead, it was hand-engineered specifically to fool the neural network classifier while being the same to our visual system.", "This noisy guy is responsible for the misclassification by the model. The addition of this noise to the first image resulted in the modified second image and this is called an adversarial example. And the external noise added is called a perturbation.", "In the same way, the car might have misclassified the STOP sign with a 100KMPH sign in this manner.", "Let me give you an idea of why this is a very significant threat to a lot of real-world machine learning applications apart from the above self-driving cars case.", "In this way, there are a ton of different attacks neural networks are prone to. There are white-box attacks, black-box attacks, physical attacks, digital attacks, perceptible and imperceptible attacks, and whatnot. While working under any real-world situation, the network must be robust to all such types of attacks.", "There\u2019s a very interesting blog on this written by Andrej Karpathy and you could read it here. Here\u2019s a small sneak peek of it.", "So what do we do in a traditional training process? We get the loss function, we backpropagate, calculate the gradient, take this gradient and use it to perform a parameter update, which wiggles every parameter in the model a tiny amount in the correct direction, to increase the prediction score. These parameter updates are responsible for increasing the confidence scores of the right class of the input image.", "Notice how this worked. We kept the input image fixed, and we regulated the model parameters to increase the score of whatever class we wanted. On the other way round, we can easily flip this process around to create fooling images. That is, we will hold the model parameters fixed, and instead, we\u2019re computing the gradient of all pixels in the input image on any class we wish. For example, we can ask a question that \u2014", "What happens to the score of (whatever class you want) when I tweak the pixels of the image instead?", "We compute the gradient just as before with backpropagation, and then we can perform an image update instead of a parameter update, with the end result being that we increase the score of whatever class we want. For example, we can take a panda image and regulate every pixel according to the gradient of that image on the cat class. This would change the image a tiny amount, but the score of the cat would now increase. Somewhat unintuitively, it turns out that you don\u2019t have to change the image too much to toggle the image from being classified correctly as a panda to being classified as anything else (e.g. cat).", "Now that you have got a basic idea of how this works, there\u2019s one popular technique you should know called the Fast Gradient Sign Method, used to generate adversarial examples, which was discussed by Ian J. Goodfellow in Explaining and Harnessing Adversarial Examples.", "In this method, you take an input image and use the gradients of the loss function with respect to the input image to create a new image that maximizes the existing loss. In this way, we achieve an image with the change that is almost imperceptible to our visual system but the same neural network could see a significant difference. This new image is called the adversarial image. This can be summarised using the following expression:", "You can play around with this method by generating your own adversarial examples for images in this notebook. Here, you\u2019ll find a model trained on the MNIST dataset and you can see how the confidence scores change while tweaking the \u03f5(epsilon) parameter.", "For any x \u2192 y, x indicates actual class and y indicates the predicted class.", "As you can see, if you increase the epsilon value, the perturbations become more evident and it becomes a perceptible change to our visual system. Nevertheless, our neural system is robust enough to predict the correct class.", "This method achieves this by finding how much each pixel in the given input image contributes to the loss value, and it adds the perturbation accordingly.", "Not only the Fast Gradient Sign Method, but we also have some other popular methods called the adversarial patch method, the single-pixel attack method, creating 3D models by adversarially perturbating them, and many more. Let\u2019s take a look at some of them.", "Google in the year 2018, came up with a unique idea of placing an adversarial patch in the image frame in the following way.", "This paper shows how it is possible to show the model any image and it classifies the given image as a toaster. This patch is designed in such a way that it can fool any underlying neural network that is responsible for classification into thinking it as a toaster, no matter what image you give it. You just need to place this sticker beside the object. It works pretty well and is capable enough to fool models which are not robust enough.", "Not only images, but you can also create a 3D model that is specifically designed to fool the model at any angle.", "Now that we\u2019ve seen how these adversarial examples fool a neural network, the same examples can also be used to train the neural network to make the model robust from attacking. This could also act as a good regularizer.", "From the above graph, it is evident that after training with adversarial examples the model is now less prone to get fooled.", "Do we humans have adversarial examples ourselves?", "And I think the answer is Yes! For example, if you look at some optical illusions like this,", "You\u2019ll notice that the lines don\u2019t look parallel at first. But when closely observe, these lines are parallel to one another.", "And yes, these are exactly what the adversarial examples are. They are the images where we see something that we shouldn\u2019t be seeing. So, we can see that our human visual system can also be fooled with certain examples but very clearly we are robust to adversarial examples that fool our neural networks.", "These adversarial examples are not just limited to images. Any model from a simple perceptron to models of natural language processing is prone to such attacks. But these can be curbed to an extent with some strategies such as Reactive and Proactive Strategies which will be discussed in detail in my upcoming articles.", "On a brighter side, I think these adversarial examples hint at some very interesting new research directions that we can use to improve our existing models. I hope you\u2019ve got to learn something new today!", "If you\u2019d like to get in touch, connect with me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An ardent learner in the field of AI, blockchain and loves to explore new tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F958ba5d82d8a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aakarsh0705?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakarsh0705?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "Aakarsh Yelisetty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ed920938e13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=post_page-4ed920938e13----958ba5d82d8a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F958ba5d82d8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F958ba5d82d8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.popsci.com/byzantine-science-deceiving-artificial-intelligence/", "anchor_text": "Source"}, {"url": "https://www.popsci.com/byzantine-science-deceiving-artificial-intelligence/", "anchor_text": "Source"}, {"url": "https://www.popsci.com/byzantine-science-deceiving-artificial-intelligence/", "anchor_text": "Source"}, {"url": "https://www.popsci.com/byzantine-science-deceiving-artificial-intelligence/", "anchor_text": "Source"}, {"url": "https://www.inovex.de/blog/machine-perception-face-recognition/", "anchor_text": "Source"}, {"url": "http://karpathy.github.io/2015/03/30/breaking-convnets/", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1412.6572.pdf", "anchor_text": "Explaining and Harnessing Adversarial Examples"}, {"url": "https://colab.research.google.com/drive/1trIwmGKQB-62pniDZnCby7y24i-BY217?usp=sharing", "anchor_text": "notebook"}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "paper"}, {"url": "https://www.labsix.org/physical-objects-that-fool-neural-nets/", "anchor_text": "Source"}, {"url": "http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/", "anchor_text": "Source"}, {"url": "https://boyslife.org/hobbies-projects/funstuff/1483/optical-illusions/", "anchor_text": "Source"}, {"url": "https://www.linkedin.com/in/aakarsh-yelisetty-6b691b171/", "anchor_text": "LinkedIn"}, {"url": "https://arxiv.org/pdf/1802.08195.pdf", "anchor_text": "https://arxiv.org/pdf/1802.08195.pdf"}, {"url": "https://pytorch.org/tutorials/beginner/fgsm_tutorial.html", "anchor_text": "https://pytorch.org/tutorials/beginner/fgsm_tutorial.html"}, {"url": "https://www.tensorflow.org/tutorials/generative/adversarial_fgsm", "anchor_text": "https://www.tensorflow.org/tutorials/generative/adversarial_fgsm"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----958ba5d82d8a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/security?source=post_page-----958ba5d82d8a---------------security-----------------", "anchor_text": "Security"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----958ba5d82d8a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----958ba5d82d8a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----958ba5d82d8a---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F958ba5d82d8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=-----958ba5d82d8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F958ba5d82d8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=-----958ba5d82d8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F958ba5d82d8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F958ba5d82d8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----958ba5d82d8a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----958ba5d82d8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakarsh0705?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakarsh0705?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aakarsh Yelisetty"}, {"url": "https://medium.com/@aakarsh0705/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "141 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ed920938e13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=post_page-4ed920938e13--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe011017c9cea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-fool-a-neural-network-958ba5d82d8a&newsletterV3=4ed920938e13&newsletterV3Id=e011017c9cea&user=Aakarsh+Yelisetty&userId=4ed920938e13&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}