{"url": "https://towardsdatascience.com/data-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80", "time": 1682996834.845175, "path": "towardsdatascience.com/data-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80/", "webpage": {"metadata": {"title": "Data-Efficient Hierarchical Reinforcement Learning \u2014 HIRO | by Sherwin Chen | Towards Data Science", "h1": "Data-Efficient Hierarchical Reinforcement Learning \u2014 HIRO", "description": "Traditional reinforcement learning algorithms have achieved encouraging success in recent years. Their nature of reasoning on the atomic scale, however, makes them hard to scale to complex tasks\u2026"}, "outgoing_paragraph_urls": [{"url": "https://xlnwel.github.io/blog/reinforcement%20learning/HIRO/", "anchor_text": "my personal blog", "paragraph_index": 3}], "all_paragraphs": ["Traditional reinforcement learning algorithms have achieved encouraging success in recent years. Their nature of reasoning on the atomic scale, however, makes them hard to scale to complex tasks. Hierarchical Reinforcement Learning(HRL) introduces high-level abstraction, whereby the agent is able to plan on different scales.", "In this post, we discuss an HRL algorithm proposed by Ofir Nachum et al. in Google Brain at NIPS 2018. The algorithm, known as HIerarchical Reinforcement learning with Off-policy correction(HIRO), is designed for goal-directed tasks, in which the agent tries to reach some goal state.", "Note that this post is the first of two consecutive posts. In the next post, we will discuss another HRL algorithm proposed by the same team as an improvement on HIRO, named Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.", "For better readability of mathematic expression, you may want to refer to my personal blog", "We first introduce three essential questions about HRL:", "HIRO can be well explained by answering the above questions(from here on, we only focus on a two-level HRL agent):", "1. In addition to state observations, we feed goals produced by the higher-level policy to lower-level policy so that the lower-level policy learns to exhibit different behavior for different goals it tries to achieve. Accordingly, to guide the learning process of the lower-level policy, we defined the goal-conditioned reward function as", "2. The high-level policy actions are defined to be goals, which the lower-level policy tries to achieve in a certain period of time. Goals are either sampled from the high-level policy every c steps, g\u209c\u223c\u03bc^{high}, when \u200bt\u22610(mod c), or otherwise computed through a fixed goal transition function. Mathematically, a goal is defined as", "3. To improve data efficiency, we separately train high-level and low-level policies using an off-policy algorithm(e.g., TD3). Specifically, for a two-level HRL agent, the lower-level policy is trained with experience (s\u209c, g\u209c, a\u209c, r\u209c, s_{t+1}, g_{t+1}), where intrinsic reward r\u209c and goal g\u209c are computed by Eq.1 and Eq.2, respectively; the higher-level policy is trained on temporally-extended experience (s\u209c, \\tilde g\u209c, \u2211 R_{t:t+c-1}, s_{t+c}), where \\tilde g\u209c is the relabeled goal (which we will discuss in the next section), and R is the reward provided by the environment. Since all the additional variables are defined by the agent itself, experiences collected from the environment can be shared with both policies.", "The goal produced by the high-level policy describes the desired relative changes in state space. This makes sense of the definition of goal transition function defined in Eq.\u200b2", "\u200bwhere s\u209c+g\u209c\u200b computes the desired state \u200bs_{t+c}. It also brings a nice interpretation for the goal-conditioned reward function defined in Eq.1\u200b: the reward function simply penalizes according to the Euclidean distance between the desired state s\u209c+g\u209c\u200b and the next state s_{t+1} the agent reaches by taking action a\u209c\u200b. Note that in this definition, goals are in the same form as the state observations.", "So far, we have made sense of goals for the lower-level policy, now let us consider its role in the high-level policy. First, we notice that, although goals are produced by the high-level policy as high-level actions, they are in fact carried out by lower-level policy. As the lower-level policy evolves, the lower-level actions taken to achieve the same goal changes, which in turns results in different states and rewards collected along the way. This invalidates old high-level transitions\u200b (s\u209c, \\tilde g\u209c, \u2211 R_{t:t+c-1}, s_{t+c}). To reinstate the experience, we relabel the goal such that it is likely to induce the same low-level behavior with the current instantiation of the lower-level policy. Mathematically, \\tilde g\u209c\u200b is chosen to maximize the probability \u03bc^{low}(a_{t:t+c-1}|s_{t:t+c-1}, \\tilde g_{t:t+c-1})\u200b, where the intermediate goals \u200b\\tilde g_{t+1:t+c-1} are computed using the goal transition function defined in Eq.\u200b2. In practice, we generally instead maximize its log probability, which could be computed as follows if the action is sampled from a Gaussian distribution", "To approximately maximize this quantity, we compute this log probability for a number of goals \\tilde g\u209c\u200b, and choose the maximal goal to relabel the experience. For example, we calculate this quantity on eight candidate goals sampled randomly from Gaussian distribution centered at s_{t+c}-s\u209c\u200b, also including the original goal g\u209c\u200b and a goal corresponding to the difference s_{t+c}-s\u209c \u200b in the candidate set, to have a total of 10 candidates. The one maximizing Eq.3\u200b is therefore chosen to be the relabeled goal.", "The following figure excerpted from the paper perfectly elucidates the algorithm, where both high-level and low-level policies are trained by TD3.", "Welcome to leave comments and reflections below to discuss the topic:)", "Ofir Nachum et al. Data-Ef\ufb01cient Hierarchical Reinforcement Learning", "Scott Fujimoto et al. Addressing Function Approximation Error in Actor-Critic Methods", "A learner, interested in deep learning and reinforcement learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5d8538c27a80&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@o.xlnwel?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Sherwin Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8f2458a76aaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&user=Sherwin+Chen&userId=8f2458a76aaf&source=post_page-8f2458a76aaf----5d8538c27a80---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d8538c27a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&user=Sherwin+Chen&userId=8f2458a76aaf&source=-----5d8538c27a80---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d8538c27a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&source=-----5d8538c27a80---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://xlnwel.github.io/blog/reinforcement%20learning/HIRO/", "anchor_text": "my personal blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5d8538c27a80---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5d8538c27a80---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5d8538c27a80---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d8538c27a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&user=Sherwin+Chen&userId=8f2458a76aaf&source=-----5d8538c27a80---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d8538c27a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&user=Sherwin+Chen&userId=8f2458a76aaf&source=-----5d8538c27a80---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d8538c27a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8f2458a76aaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&user=Sherwin+Chen&userId=8f2458a76aaf&source=post_page-8f2458a76aaf----5d8538c27a80---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3ae42f04e0de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&newsletterV3=8f2458a76aaf&newsletterV3Id=3ae42f04e0de&user=Sherwin+Chen&userId=8f2458a76aaf&source=-----5d8538c27a80---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Written by Sherwin Chen"}, {"url": "https://medium.com/@o.xlnwel/followers?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "413 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8f2458a76aaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&user=Sherwin+Chen&userId=8f2458a76aaf&source=post_page-8f2458a76aaf----5d8538c27a80---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3ae42f04e0de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80&newsletterV3=8f2458a76aaf&newsletterV3Id=3ae42f04e0de&user=Sherwin+Chen&userId=8f2458a76aaf&source=-----5d8538c27a80---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/attention-is-all-you-need-transformer-4c34aa78308f?source=author_recirc-----5d8538c27a80----0---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=author_recirc-----5d8538c27a80----0---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=author_recirc-----5d8538c27a80----0---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Sherwin Chen"}, {"url": "https://pub.towardsai.net/?source=author_recirc-----5d8538c27a80----0---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/attention-is-all-you-need-transformer-4c34aa78308f?source=author_recirc-----5d8538c27a80----0---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Attention Is All You Need \u2014 TransformerDiscussing the Transformer model"}, {"url": "https://pub.towardsai.net/attention-is-all-you-need-transformer-4c34aa78308f?source=author_recirc-----5d8538c27a80----0---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "\u00b76 min read\u00b7Jul 5, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F4c34aa78308f&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fattention-is-all-you-need-transformer-4c34aa78308f&user=Sherwin+Chen&userId=8f2458a76aaf&source=-----4c34aa78308f----0-----------------clap_footer----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/attention-is-all-you-need-transformer-4c34aa78308f?source=author_recirc-----5d8538c27a80----0---------------------35edad58_6143_4c49_8e1a_e733a547e755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c34aa78308f&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fattention-is-all-you-need-transformer-4c34aa78308f&source=-----5d8538c27a80----0-----------------bookmark_preview----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5d8538c27a80----1---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5d8538c27a80----1---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5d8538c27a80----1---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5d8538c27a80----1---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5d8538c27a80----1---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5d8538c27a80----1---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5d8538c27a80----1---------------------35edad58_6143_4c49_8e1a_e733a547e755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----5d8538c27a80----1-----------------bookmark_preview----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5d8538c27a80----2---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----5d8538c27a80----2---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----5d8538c27a80----2---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5d8538c27a80----2---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5d8538c27a80----2---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5d8538c27a80----2---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5d8538c27a80----2---------------------35edad58_6143_4c49_8e1a_e733a547e755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----5d8538c27a80----2-----------------bookmark_preview----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=author_recirc-----5d8538c27a80----3---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=author_recirc-----5d8538c27a80----3---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=author_recirc-----5d8538c27a80----3---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Sherwin Chen"}, {"url": "https://pub.towardsai.net/?source=author_recirc-----5d8538c27a80----3---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=author_recirc-----5d8538c27a80----3---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "How to Train MAML(Model-Agnostic Meta-Learning)An elaborate explanation for MAML and more"}, {"url": "https://pub.towardsai.net/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=author_recirc-----5d8538c27a80----3---------------------35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": "\u00b78 min read\u00b7Aug 23, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F90aa093f8e46&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fhow-to-train-maml-model-agnostic-meta-learning-90aa093f8e46&user=Sherwin+Chen&userId=8f2458a76aaf&source=-----90aa093f8e46----3-----------------clap_footer----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=author_recirc-----5d8538c27a80----3---------------------35edad58_6143_4c49_8e1a_e733a547e755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F90aa093f8e46&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fhow-to-train-maml-model-agnostic-meta-learning-90aa093f8e46&source=-----5d8538c27a80----3-----------------bookmark_preview----35edad58_6143_4c49_8e1a_e733a547e755-------", "anchor_text": ""}, {"url": "https://medium.com/@o.xlnwel?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "See all from Sherwin Chen"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----5d8538c27a80----0-----------------bookmark_preview----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----1-----------------clap_footer----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----5d8538c27a80----1-----------------bookmark_preview----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----5d8538c27a80----0---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----5d8538c27a80----0-----------------bookmark_preview----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----1-----------------clap_footer----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----5d8538c27a80----1---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----5d8538c27a80----1-----------------bookmark_preview----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----5d8538c27a80----2---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----5d8538c27a80----2---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----5d8538c27a80----2---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----5d8538c27a80----2---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----5d8538c27a80----2---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----2-----------------clap_footer----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----5d8538c27a80----2---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----5d8538c27a80----2-----------------bookmark_preview----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----5d8538c27a80----3---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----5d8538c27a80----3---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----5d8538c27a80----3---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Aniruddha Mukherjee"}, {"url": "https://medium.com/dsckiit?source=read_next_recirc-----5d8538c27a80----3---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "GDSC KIIT"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----5d8538c27a80----3---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "Reinforcement Learning: An Introduction and Guide to its FundamentalsPolicies, Rewards, the Bellman Equation, and the Markov Decision Process (MDP)"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----5d8538c27a80----3---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": "5 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdsckiit%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&user=Aniruddha+Mukherjee&userId=68f97387c191&source=-----467c6a2ed25e----3-----------------clap_footer----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----5d8538c27a80----3---------------------fcfb6a74_836b_436d_91cb_3819a47f1668-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&source=-----5d8538c27a80----3-----------------bookmark_preview----fcfb6a74_836b_436d_91cb_3819a47f1668-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----5d8538c27a80--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}