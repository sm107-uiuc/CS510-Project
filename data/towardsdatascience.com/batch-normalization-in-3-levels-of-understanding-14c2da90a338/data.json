{"url": "https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338", "time": 1683016066.032424, "path": "towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338/", "webpage": {"metadata": {"title": "Batch normalization in 3 levels of understanding | by Johann Huber | Towards Data Science", "h1": "Batch normalization in 3 levels of understanding", "description": "There is a lot of content about Batch Normalization (BN) on the internet. Yet, many of them are defending an outdated intuition about it. I spent a lot of time putting all this scattered information\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/p/14c2da90a338#1f09", "anchor_text": "Results from the original article", "paragraph_index": 2}, {"url": "https://medium.com/p/14c2da90a338#36f2", "anchor_text": "Normalization during evaluation", "paragraph_index": 3}, {"url": "https://medium.com/p/14c2da90a338#d000", "anchor_text": "Recurrent network & layer normalization", "paragraph_index": 4}, {"url": "https://medium.com/p/14c2da90a338#8c6f", "anchor_text": "First hypothesis : confusion around internal covariate shift (ICS)", "paragraph_index": 5}, {"url": "https://medium.com/p/14c2da90a338#e773", "anchor_text": "Second hypothesis : mitigate interdependency between distributions", "paragraph_index": 6}, {"url": "https://medium.com/p/14c2da90a338#fb36", "anchor_text": "Third hypothesis : make the optimisation landscape smoother", "paragraph_index": 7}, {"url": "https://medium.com/p/14c2da90a338#3fb5", "anchor_text": "Summary : What do we understand for now", "paragraph_index": 8}, {"url": "https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb", "anchor_text": "this repo", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Moving_average", "anchor_text": "Exponential Moving Average (EMA)", "paragraph_index": 19}, {"url": "https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb", "anchor_text": "this github repository", "paragraph_index": 31}, {"url": "https://www.deeplearningbook.org/", "anchor_text": "Deep learning", "paragraph_index": 51}, {"url": "https://www.youtube.com/watch?v=Xogn6veSyxA", "anchor_text": "source", "paragraph_index": 53}, {"url": "https://en.wikipedia.org/wiki/Orthogonality_(programming)", "anchor_text": "orthogonality", "paragraph_index": 58}, {"url": "https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/38", "anchor_text": "it just happens", "paragraph_index": 76}, {"url": "https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression", "anchor_text": "Pulmonary Fibrosis Progression Kaggle competition", "paragraph_index": 77}, {"url": "https://www.youtube.com/watch?v=Xogn6veSyxA", "anchor_text": "here", "paragraph_index": 81}, {"url": "https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu", "anchor_text": "example", "paragraph_index": 87}, {"url": "https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/dgqaksn/", "anchor_text": "reddit thread", "paragraph_index": 93}, {"url": "https://www.youtube.com/watch?v=Xogn6veSyxA", "anchor_text": "this brilliant video", "paragraph_index": 133}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou Hacquet-Delepine", "paragraph_index": 190}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "How does batch normalization help optimization?", "paragraph_index": 191}, {"url": "https://arxiv.org/pdf/1409.4842.pdf", "anchor_text": "Going deeper with convolutions", "paragraph_index": 192}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep residual learning for image recognition", "paragraph_index": 193}], "all_paragraphs": ["There is a lot of content about Batch Normalization (BN) on the internet. Yet, many of them are defending an outdated intuition about it. I spent a lot of time putting all this scattered information together to build a good intuition about this fundamental method, I thought a step-by-step walkthrough could be useful to some readers.", "In particular, this story aims to bring :", "\u2014 \u2014 2.1. Results from the original article", "\u2014 \u2014 2.3. Normalization during evaluation", "\u2014 \u2014 2.5. Recurrent network & layer normalization", "\u2014 \u2014 3.1. First hypothesis : confusion around internal covariate shift (ICS)", "\u2014 \u2014 3.2. Second hypothesis : mitigate interdependency between distributions", "\u2014 \u2014 3.3. Third hypothesis : make the optimisation landscape smoother", "\u2014 4. Summary : What do we understand for now", "Batch-Normalization (BN) is an algorithmic method which makes the training of Deep Neural Networks (DNN) faster and more stable.", "It consists of normalizing activation vectors from hidden layers using the first and the second statistical moments (mean and variance) of the current batch. This normalization step is applied right before (or right after) the nonlinear function.", "All the current deep learning frameworks have already implemented methods which apply batch normalization. It is usually used as a module which could be inserted as a standard layer in a DNN.", "Remark : For those who prefer read code than text, I wrote a simple implementation of Batch Normalization in this repo.", "Batch normalization is computed differently during the training and the testing phase.", "At each hidden layer, Batch Normalization transforms the signal as follow :", "The BN layer first determines the mean \ud835\udf07 and the variance \u03c3\u00b2 of the activation values across the batch, using (1) and (2).", "It then normalizes the activation vector Z^(i) with (3). That way, each neuron\u2019s output follows a standard normal distribution across the batch. (\ud835\udf00 is a constant used for numerical stability)", "It finally calculates the layer\u2019s output \u1e90(i) by applying a linear transformation with \ud835\udefe and \ud835\udefd, two trainable parameters (4). Such step allows the model to choose the optimum distribution for each hidden layers, by adjusting those two parameters :", "Remark : The reasons explaining BN layers effectiveness are subjected to misunderstanding and errors (even in the original article). A recent paper [2] disproved some erroneous hypotheses, improving the community\u2019s understanding of this method. We will discuss that matter in section C.3 : \u201cWhy does BN work ?\u201d.", "At each iteration, the network computes the mean \ud835\udf07 and the standard deviation \u03c3 corresponding to the current batch. Then it trains \ud835\udefe and \ud835\udefd through gradient descent, using an Exponential Moving Average (EMA) to give more importance to the latest iterations.", "Unlike the training phase, we may not have a full batch to feed into the model during the evaluation phase.", "Those values are computed using all the (\ud835\udf07_batch , \u03c3_batch) determined during training, and directly fed into equation (3) during evaluation (instead of calling (1) and (2)).", "Remark : We will discuss that matter in depth in section C.2.3 : \u201cNormalization during evaluation\u201d.", "In practice, we consider the batch normalization as a standard layer, such as a perceptron, a convolutional layer, an activation function or a dropout layer.", "Each of the popular frameworks already have an implemented Batch Normalization layer. For example :", "All of the BN implementations allow you to set each parameters independently. However, the input vector size is the most important one. It should be set to :", "Take a look at the online docs of your favorite framework and read the BN layer page : is there anything specific to their implementation ?", "Even if we don\u2019t understand all the underlying mechanisms of Batch Normalization yet (see C.3), there\u2019s something everyone agrees on : it works.", "To get a first insight, let\u2019s take a look on the official article\u2019s results [1] :", "The results are clear-cut : BN layers make the training faster, and allow a wider range of learning rate without compromising the training convergence.", "Remark : At this point, you should know enough about BN layers to use them. However, you will need to dig deeper if you want to get the most out of Batch Normalization !", "I\u2019ve reimplemented the Batch Normalization layer with Pytorch to reproduce the original paper results. The code is available on this github repository.", "I recommend that you take a look at some of the implementations of BN layer available online. It is very instructive to see how it is coded in your favorite framework !", "Before diving into the theory, let\u2019s start with what\u2019s certain about Batch Normalization.", "As previously stated, BN is widely used because it almost always makes deep learning models perform much better.", "The official article [1] carried out 3 experiments to show their method effectiveness.", "At first, they have trained a classifier on the MNIST dataset (handwritten digits). The model consists of 3 fully connected layers of 100 neurons each, all activated by sigmoid function. They have trained this model twice (with and without BN layers) for 50 000 iterations, using stochastic gradient descent (SGD), and the same learning rate (0.01). Notice that BN layers were put right before the activation function.", "You can easily reproduce those results without GPU, it\u2019s a great way to get more familiar with this concept !", "Looks good ! Batch Normalization increases our network performances, regarding both the loss and the accuracy.", "The 2nd experiment consists of taking a look at the activation values in the hidden layers. Here are the plots corresponding to the last hidden layer (right before the nonlinearity) :", "Without Batch Normalization, the activated values fluctuate significantly during the first iterations. On the contrary, activation curves are smoother when BN is used.", "Also, the signal is less noisy when adding BN layers. It seems to make the convergence of the model easier.", "This example does not show all the benefits of the Batch Normalization.", "The official article carried out a third experiment. They wanted to compare the model performances while adding BN layers on a larger dataset : ImageNet (2012). To do so, they trained a powerful neural network (at that time) called Inception [3]. Originally, this network doesn\u2019t have any BN layers. They added some and trained the model by modifying its learning rate (x1, x5, x30 former optimum). They also tried to replace every ReLU activation function by sigmoid in another network. Then, they compared the performances of those new networks with the original one.", "Here is what they got :", "What we can conclude from those curves :", "On such a large dataset, the improvement is much more significant than the one observed on the small MNIST dataset.", "The authors have successfully trained their Inception-with-BN network using a 30 times higher learning rate than the original one. Notice that a 5 times larger LR already makes the vanilla network diverge !", "That way, it is much easier to find an \u201cacceptable\u201d learning rate : the interval of LR which lies between underfitting and gradient explosion is much larger.", "Also, a higher learning rate helps the optimizer to avoid local minima convergence. Encouraged to explore, the optimizer will more easily converge on better solutions.", "We need to take a step back and look at the bigger picture. We can clearly see that we can get slightly better performances with ReLU-based models than with sigmoid ones, but that\u2019s not what matters here.", "To show why this results is meaningful, let me rephrase what Ian Goodfellow (inventor of GANs [6], author of the famous \u201cDeep learning\u201d handbook) said about it :", "Before BN, we thought that it was almost impossible to efficiently train deep models using sigmoid in the hidden layers. We considered several approaches to tackle training instability, such as looking for better initialization methods. Those pieces of solution were heavily heuristic, and way too fragile to be satisfactory. Batch Normalization makes those unstable networks trainable ; that\u2019s what this example shows.", "\u2014 Ian Goodfellow (rephrased from : source)", "Now we understand why BN had such an important impact on the deep learning field.", "Those results give an overview of Batch Normalization benefits on network performances. However, there are some side effects you should have in mind to get the most out of BN.", "BN relies on batch first and second statistical moments (mean and variance) to normalize hidden layers activations. The output values are then strongly tied to the current batch statistics. Such transformation adds some noise, depending on the input examples used in the current batch.", "Adding some noise to avoid overfitting \u2026 sounds like a regularization process, doesn\u2019t it ? ;)", "In practice, we shouldn\u2019t rely on batch normalization to avoid overfitting, because of orthogonality matters. Put simply, we should always make sure that one module addresses one issue. Relying on several modules to deal with different problems makes the development process much more difficult than needed.", "Still, it is interesting to be aware of the regularization effect, as it could explains some unexpected behavior from a network (especially during sanity checks).", "Remark : The greater the batch size, the lesser the regularization (as it reduces noise impact).", "There are two cases where a model could be called in evaluation mode :", "In the first case, we could apply Batch Normalization using current batch statistics for convenience. In the second one, using the same approach makes no sense, because we do not necessarily have an entire batch to predict.", "Let\u2019s see the example of a robot with an embedded camera. We could have a model that uses the current framework to predict the positions of any upcoming obstacles. So we want to compute inference on a single frame (i.e. one rgb image) per iteration. If the training batch size is N, what should we choose for the (N - 1) other inputs expected by the model to do a forward propagation ?", "Remember that for each BN layer, (\ud835\udefd, \ud835\udefe) were trained using a normalized signal. So we need to determine (\ud835\udf07, \u03c3) in order to have meaningful results.", "A solution would be to choose arbitrary values to fulfil the batch. By feeding a first batch to the model, we will get a certain result for the image we are interested in. If we build a second batch with other random values, we will have different predictions for the same image. Two different outputs for a single input is not a desirable model behavior.", "This trick is to define (\ud835\udf07_pop , \u03c3_pop), which are respectively the estimated mean and standard deviation of the targeted population. Those parameters are calculated as the mean of all the (\ud835\udf07_batch, \u03c3_batch) determined during training.", "That\u2019s how we do it !", "Remark : This trick might leads to instability during the evaluation phase : let\u2019s discuss it in the next section.", "Even though Batch Normalization works pretty well, it can sometimes cause stability issues. There are cases where BN layer makes activation values explode during the evaluation phase (making the model returns loss = NaN).", "We have just mentioned how (\ud835\udf07_pop , \u03c3_pop) are determined, in order to use them during evaluation : we compute the mean of all the (\ud835\udf07_batch, \u03c3_batch) calculated during training.", "Let\u2019s consider a model which was trained only on images containing sneakers. What if we have derby-like shoes in the test set ?", "We suppose that the activation values of hidden layers will have significantly different distributions during training and evaluation \u2014 maybe too much. In this case, the estimated (\ud835\udf07_pop, \u03c3_pop) does not properly estimate the real population mean and standard deviation. Applying (\ud835\udf07_pop, \u03c3_pop) might push the activation values away from (\ud835\udf07 = 0, \u03c3 = 1), resulting in overestimation of activation values.", "Remark : The shifting of distribution between training and testing set is called \u201ccovariate shift\u201d. We will talk about this effect again in section (C.3.).", "This effect is increased by a well known property of BN : during training, activation values are normalized using their own values. During inference, the signal is normalized using (\ud835\udf07_pop, \u03c3_pop), which have been computed during training. Thus, the coefficients used for normalization don\u2019t take into account the actual activation values themselves.", "In general, a training set must be \u201csimilar enough\u201d to the testing set : otherwise, it would be impossible to properly train a model on the targetted task. So in most cases, \ud835\udf07_pop and \u03c3_pop should be a good fit for the testing set. If not, we may conclude that the training set is not large enough, or that its quality is not good enough for the targeted task.", "But sometimes, it just happens. And there is not always a clean solution to this issue.", "I have personally faced it once, during the Pulmonary Fibrosis Progression Kaggle competition. The training dataset consisted of metadata, and 3D scans of lungs associated to each patients. The content of those scans was complex and diverse, but we only had ~ 100 patients to split into train and validation sets. As a result, the convolutional networks I wanted to use for feature extractions returned NaN as soon as the model switched from training to evaluation mode. Pleasant to debug.", "When you cannot easily get additional data to enhance your training set, you have to find a workaround. In the above case, I\u2019ve manually forced the BN layers to compute (\ud835\udf07_batch, \u03c3_batch), on the validation set too. (I agree, that\u2019s an ugly way to fix it, but I ran out of time. ;) )", "Adding BN layers to your network - assuming it cannot have negative impacts - is not always the best strategy !", "In practice, it is widely admitted that :", "While BN uses the current batch to normalize every single value, LN uses all the current layer to do so. In other words, the normalization is performed using other features from a single example instead of using the same feature across all current batch examples. This solution seems more efficient for recurrent networks. Note that it is quite difficult to define a consistent strategy for those kinds of neurons, as they rely on multiplication of the same weight matrix several times. Should we normalize each step independently ? Or should we compute the mean across all steps, and then apply normalization recursively ? (source of the intuition argument : here)", "I will not detail any further on that matter, as it\u2019s not the purpose of this article.", "Historically, BN layer is positioned right before the nonlinear function, which was consistent with the authors objectives and hypothesis at that time :", "In their article, they stated :", "\u201cWe would like to ensure that, for any parameter values, the network always produces activations with the desired distribution.\u201d", "\u2014 Sergey Ioffe & Christian Szegedy (source : [1])", "Some experiments showed that positioning BN layers right after the nonlinear function leads to better results. Here is an example.", "Fran\u00e7ois Chollet, creator of Keras and currently engineer at Google, stated that :", "\u201cI haven\u2019t gone back to check what they are suggesting in their original paper, but I can guarantee that recent code written by Christian [Szegedy] applies relu before BN. It is still occasionally a topic of debate, though.\u201d", "Still, many commonly used architecture of transfer learning apply BN before nonlinearity (ResNet, mobilenet-v2, \u2026).", "Notice that the article [2] - which challenges hypotheses defended by the original article [1] to explain BN effectiveness (see C.3.3)) - puts the BN layer before the activation function, without bringing solid reason for it.", "As far as I know, this question is still discussed.", "Further reading : Here is an interesting reddit thread - even if some arguments are not convincing - mostly in favor of BN after activation.", "In most cases, Batch Normalization improves the performances of deep learning models. That\u2019s great. But we want to know what\u2019s actually happening inside the black box.", "This is where things get a bit hairy.", "The problem is : we don\u2019t exactly know what makes Batch Normalization work so well yet. A few hypotheses are often discussed within the DL community: we will explore them step by step.", "Before diving into the discussion, here are what we\u2019ll see :", "I bet that exploring those hypotheses will help you to build a strong intuition about Batch Normalization.", "Despite its fundamental impact on DNN performances, Batch Normalization is still subject to misunderstanding.", "Confusions about BN are mostly due to a wrong hypothesis supported by the original article [1].", "Sergey Ioffe & Christian Szegedy introduced BN as follow :", "\u201cWe refer to the change in the distributions of internal nodes of a deep network, in the course of training, as Internal Covariate Shift. [\u2026] We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets.\u201d", "\u2014 Sergey Ioffe & Christian Szegedy (source : [1])", "In other words, BN is efficient because it - partially - tackles the internal Covariate Shift issue.", "This statement has been severely challenged by later works [2].", "Notation : From now, ICS refers to internal Covariate Shift.", "To understand what causes such a confusion, let\u2019s begin by discussing what covariate shift is, and how is it impacted by normalization.", "[1]\u2019s authors defined it clearly : covariate shift - in the distributional stability prospective - describes the shifting of a model input distribution. By extension, the internal covariate shift describes this phenomenon when it happens in the hidden layers of a deep neural network.", "Let\u2019s see why it could be an issue through an example.", "Let\u2019s assume we want to train a classifier which could answers the following question : Does this image contain a car ? If we wanted to extract all the car images from a very large unlabeled dataset, such a model would save us a huge amount of time.", "We would have an RGB image as input, some convolutional layers, followed by fully connected layers. The output should be a single value, fed into a logistic function to make the final value lies between 0 and 1 - describing the probability for the input image to contain a car.", "To train such a model, we would need a significant amount of labeled images.", "Now, assuming we only have \u201ccommon\u201d cars for training. How would the model react if we ask it to classify a formula 1 car ?", "In this example, there is a shift between the training and testing distribution. More broadly, a different car orientation, lightning, or whether condition would be enough to impact our model performances. Here, our model does not generalize well.", "If we had plot the extracted features in the features space, we would have something like this :", "Let\u2019s assume the crosses describe features associated to images which do not contain any car, while rings describe images containing a car. Here, a single function could split the two ensembles. But the function is likely to be less accurate on the top-right part of the diagram : there are not enough points to determine a good function. This might lead the classifier to do many errors during evaluation.", "To efficiently train our model, we would require many car images, in any possible context we could imagine. Even if this is still how we train our CNN nowadays, we want to make sure our model will generalize well using as few examples as possible.", "The problem could be summarized as follow :", "From the model point-of-view, training images are - statistically - too different from testing images.There is a covariate shift.", "We can face this issue using simpler models. It is well known that linear regression models are easier to optimize when the input values are normalized (i.e. making its distribution close to (\ud835\udf07 = 0, \u03c3 = 1)) : that\u2019s why we usually normalize the input values of a model.", "This solution was already well known before the publication of the BN paper. With BN, [1]\u2019s authors wanted to extend this method to the hidden layers to help training.", "In our car classifier, we can see hidden layers as units which are activated when they identify some \u201cconceptual\u201d features associated with cars : it could be a wheel, a tire or a door. We can suppose that the previously described effect could happen inside hidden units. A wheel with a certain orientation angle will activate neurons with a specific distribution. Ideally, we want to make some neurons react with a comparable distribution for any wheel orientations, so the model could efficiently conclude on the probability for the input image to contain a car.", "If there is a huge covariate shift in the input signal, the optimizer will have trouble generalizing well. On the contrary, if the input signal always follows a standard normal distribution, the optimizer will more easily generalize. With this in mind, [1]\u2019s authors applied the strategy of normalizing the signal in the hidden layers. They assumed that forcing to (\ud835\udf07 = 0, \u03c3 = 1) the intermediates signal distribution will help the network generalization in the \u201cconceptual\u201d levels of features.", "Though, we do not always want standard normal distribution in the hidden units. It would reduce the model representativity :", "The original article takes the sigmoid function as an example to show why normalization alone is an issue. If the input signal values lie between 0 and 1, the nonlinear function would only work \u2026 in its linear regime. Sounds problematic.", "To tackle this issue, they added two trainable parameters \ud835\udefd and \ud835\udefe, allowing the optimizer to define the optimum mean (using \ud835\udefd) and standard deviation (using \ud835\udefe) for a specific task.", "\u26a0Warning : The following hypothesis is now outdated. A lot of great content about BN still claim it as the reason that makes the method works in practice. Yet, recent works severely challenged it.", "For a couple of years after the release of [1], the DL community explained BN effectiveness as follow :", "BN \u279c Normalization of the input signal inside hidden units \u279c Adding two trainable parameters to adjust the distribution and get the most out of the nonlinearities \u279c Easier training", "Here, normalization to (\ud835\udf07 = 0, \u03c3 = 1) is what mostly explains BN\u2019s effectiveness. This hypothesis has been challenged (see section C.3.3), replaced by another hypothesis :", "BN \u279c Normalization of the input signal inside hidden units \u279c Reduces interdependency between hidden layers (in a distribution stability perspective) \u279c Easier training", "There\u2019s is a slight but very important difference. Here, the purpose of normalization is to reduce interdependency between layers (in a distribution stability perspective), so the optimizer could choose the optimum distribution by adjusting only two parameters ! Let\u2019s explore this hypothesis a little bit further.", "About this section : I could not find any solid evidence about the hypothesis discussed in this section. Therefore, I decided to mostly rely on Ian Goodfellow\u2019s explanations on that matter (in particular in this brilliant video).", "Where (a), (b), (c), (d) and (e) are the sequential layers of the network. Here is a very simple example, where all layers are linked by linear transformations. Let\u2019s assume we want to train such model using SGD.", "To update the weights of the layer (a), we need to calculate the gradient from the network\u2019s output :", "We first consider a network without BN layers. From the above equation, we conclude that if all gradients are large, grad(a) will be very large. On the contrary, if all gradients are small, grad(a) will be almost negligible.", "It is pretty easy to see how dependent are layers from each other by looking at the input distribution of hidden units : a modification of (a) weights will modify the input distribution of (b) weights, which will eventually modify the input signal of (d) and (e). This interdependency could be problematic for training stability : if we want to adjust the input distribution of a specific hidden unit, we need to consider the whole sequence of layers.", "However, SGD considers 1st order relationships between layers. So they don\u2019t take into account the higher degree relationships mentioned above !", "Adding BN layers significantly reduces the interdependence between each layers (in the distribution stability perspective) during training. Batch Normalization acts like a valve which holds back the flow, and allows its regulation using \ud835\udefd et \ud835\udefe. It is then no longer necessary to take all parameters into account to have clues about distribution inside the hidden units.", "Remark : The optimizer can then do larger weights modifications without deteriorating adjusted parameters in other hidden layers. It makes the hyperparameter tuning way easier !", "This example puts aside the hypothesis which claims that BN effectiveness is due to the normalization of intermidiates signal distribution to (\ud835\udf07 = 0, \u03c3 = 1).Here, BN aims to make the optimizer job easier, allowing it to adjust hidden layer distribution with only two parameters at a time.", "\u26a0 However, keep in mind that this is mostly speculation. Those discussions should be used as insights to build intuition about BN. We still don\u2019t exactly know why BN is efficient in practice !", "In 2019, a team of researchers from MIT carried out some interesting experiments about BN [2]. Their results severely challenge the hypothesis n\u00b01 (still shared by many serious blog posts and MOOCS !).", "We should have a look on this paper if we want to avoid \u201clocal minima hypotheses\u201d about the BN impact on training\u2026 ;)", "About this section : I\u2019ve synthesized results from [2] which could help us to build a better intuition about BN. I wasnt able to be exhaustive, this paper is dense, I recommend you to read it thoroughly if you\u2019re interested in those concepts.", "Let\u2019s jump straight into the 2nd experiment of [2]. Their goal was to check the correlation between ICS, and the benefits of BN on training performances (hypothesis n\u00b01).", "Notation : We\u2019ll now refer to this covariate shift by ICS_distrib.", "To do so, the researchers have trained three VGG networks (on CIFAR-10) :", "They measured the accuracy reached by each model, and the evolution of the distribution values w.r.t iterations. Here is what they got :", "We can see that the 3rd network has, as expected, a very high ICS. However, the noisy network is still trained faster than the standard one. Its reached performances are comparable to the ones obtained with a standard BN network. This result suggests that the BN effectiveness is not related to ICS_distrib. Oops !", "We shouldn\u2019t discard the ICS theory too fast : if the BN effectiveness does not come from ICS_distrib, it might be related to another definition of ICS. After all, the intuition behind the hypothesis n\u00b01 makes sense, doesn\u2019t it ?", "The main issue with ICS_distrib is that its definition is related to input distribution of hidden units. So there is no direct link with the optimization problem on its own.", "[2]\u2019s authors proposed another definition of ICS :", "Let\u2019s consider a fixed input X.", "We define the internal covariate shift from an optimization perspective as the difference between the gradient computed on a hidden layer k after backpropagating the error L(X)_it, and the gradient computed on the same layer k from the loss L(X)_it+1 computed after the iteration = it update of weights.", "This definition aims to focus on the gradients more than on the hidden layer input distributions, assuming that it could give us better clues on how ICS could have an impact on the underlying optimization problem.", "Notation: ICS_opti now refers to the ICS defined from an optimization perspective.", "In the next experiment, authors evaluate ICS_opti impact on training performances. To do so, they measure the variation of ICS_opti during training for a DNN with and without BN layers. To quantify the variation of gradient mentioned in the ICS_opti definition, they calculate :", "Results are a bit surprising : the network which relies on BN seems to have a higher ICS_opti than the standard network. Remember that the network with BN (blue curve) is trained faster than the standard network (red curve) !", "ICS seems definitively not related to training performances\u2026at least for the explored definition of ICS.", "Somehow, Batch Normalization have another impact on the network, which makes convergence easier.", "Now, let\u2019s study how does BN affects the optimization landscape. We may find clues there.", "Here is the last experiment covered in this story :", "From a single gradient, we update weights with different optimization steps (which act like a learning rate). Intuitively, we define a direction from a certain point (i.e. a network configuration \u03c9) in the feature space, and then explore the optimization landscape further and further in this direction.", "At each step, we measure the gradient and the loss. We can therefore compare different points of the optimization landscape with a starting point. If we measure large variations, the landscape is very unstable and the gradient is incertain : big steps might deteriorate optimization. On the contrary, if the measured variations are small, the landscape is stable and the gradient is trustworthy : we can apply larger steps without compromising optimization. In other words, we can use a larger learning rate, and make the convergence faster (a well known properties of BN).", "Let\u2019s have a look on the results :", "We can clearly see that the optimization landscape is way smoother with BN layers.", "We finally have results we can rely on to explain BN effectiveness : BN layer makes somehow the optimization landscape smoother. That makes the optimizer job easier : we can define a larger learning rate without being submitted to gradient vanishing (weights stuck on sudden flat surfaces) or to gradient explosion (weights fallen in an abrupt local minima).", "We can know formulate a 3rd hypothesis, proposed by [2] :", "BN \u279c Normalization of the input signal inside hidden units \u279c Makes the optimization landscape smoother \u279c Faster and more stable training", "It raises another question : How does BN make the optimization landscape smoother ?", "[2]\u2019s authors also explored those matters from a theoretical point of view. Their work is very instructive, helping to get a better grasp of the smoothing effect of Batch Normalization. In particular, they showed that BN makes the optimization landscape smoother while preserving all of the minima of the normal landscape. In other words, BN reparametrizes the underlying optimization problem, making the training faster and easier !", "\u26a0 In additional studies, [2]\u2019s authors observed that this effect is not unique to BN. They obtained similar training performances with other normalization methods (for example L1 or L2 normalization). Those observations suggest that BN effectiveness is mostly due to serendipity, leveraging on underlying mechanisms that we have not perfectly identified yet.", "To conclude this section, this paper severely challenges the widely admitted idea that BN effectiveness is mostly due to ICS reduction (in a training stability distribution perspective, as well as in an optimization perspective). However, it stresses the impact of BN smoothing effect on the optimization landscape.", "While this paper states a hypothesis about BN impact on training speed, it does not answer why BN helps generalization.", "They briefly argue that making the optimization landscape smoother could help the model to converge on flat minima, which have better generalizing properties. No more details on that matter, though.", "Their main contribution is to challenge the commonly admitted idea of BN effect on ICS \u2014 which is already significant !", "\u274c Wrong : [2] showed that in practice, there is no correlation between ICS and training performances.", "\u2753 Maybe : This hypothesis highlights the interdependency between parameters, making the optimization task harder. No solid proof, though.", "\u2753 Maybe : Their results are quite recent. To my knowledge, they have not been challenged so far. They provide empirical demonstrations and pieces of theoretical justifications, but some fundamental questions remain unanswered (such as \u201chow does BN help generalization ?\u201d).", "Discussion : It seems to me that the last two hypotheses are compatible. Intuitively, we could see the hypothesis n\u00b02 as a projection from a problem with many parameters, to many problems with a couple of parameters ; a kind of dimensionality reduction, which would helps generalization. Any ideas about it ?", "Many questions remain open, and Batch Normalization is still a topic of research nowadays. Discussing those hypotheses still helps to get a better understanding of this commonly used method, discarding some erroneous statements we had in mind for a couple of years.", "Those questions do not prevent us to leverage on the benefits of BN in practice, though !", "Batch Normalization (BN) is one of the most important advances in the field of Deep Learning (DL) in recent years. Relying on two successive linear transformations, this method makes Deep Neural network (DNN) training faster and more stable.", "The most widely admitted hypothesis about what makes BN efficient in practice is the reduction of interdependence between hidden layers during training. However, the normalization transformation impact of optimization landscape smoothness seems to be an important mechanism of BN effectiveness.", "Many commonly used DNN rely on BN nowadays (ex : ResNet [4], EfficientNet [5], \u2026).", "If you are interested in Deep Learning, you will for sure have to get familiar with this method !", "Even if BN appears to be efficient in practice for years, many questions about its underlying mechanisms remain unanswered.", "Here is a non-exhaustive list of opened questions about BN :", "Many thanks to Lou Hacquet-Delepine for all the drawn diagrams, as well as her overall help in proofreading !", "[2] Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization?, Advances in Neural Information Processing Systems", "[3] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u2026 & Rabinovich, A. (2015). Going deeper with convolutions, Proceedings of the IEEE conference on computer vision and pattern recognition", "[4] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in AI and robotics at the Institute for Intelligent Systems and Robotics (ISIR)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F14c2da90a338&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----14c2da90a338--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----14c2da90a338--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jtkarl?source=post_page-----14c2da90a338--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jtkarl?source=post_page-----14c2da90a338--------------------------------", "anchor_text": "Johann Huber"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdee40be29033&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&user=Johann+Huber&userId=dee40be29033&source=post_page-dee40be29033----14c2da90a338---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14c2da90a338&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14c2da90a338&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://unsplash.com/@jabari21?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "jabari timothy"}, {"url": "https://unsplash.com/s/photos/dam?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb", "anchor_text": "the notebook"}, {"url": "https://medium.com/p/14c2da90a338#b93c", "anchor_text": "In 30 seconds"}, {"url": "https://medium.com/p/14c2da90a338#ad2e", "anchor_text": "In 3 minutes"}, {"url": "https://medium.com/p/14c2da90a338#3164", "anchor_text": "Principle"}, {"url": "https://medium.com/p/14c2da90a338#dc3f", "anchor_text": "Training"}, {"url": "https://medium.com/p/14c2da90a338#5ce9", "anchor_text": "Evaluation"}, {"url": "https://medium.com/p/14c2da90a338#5920", "anchor_text": "In practice"}, {"url": "https://medium.com/p/14c2da90a338#5c4f", "anchor_text": "Overview of results"}, {"url": "https://medium.com/p/14c2da90a338#e5ba", "anchor_text": "Understanding Batch Normalization (BN)"}, {"url": "https://medium.com/p/14c2da90a338#6cd2", "anchor_text": "Implementation"}, {"url": "https://medium.com/p/14c2da90a338#018e", "anchor_text": "BN in practice"}, {"url": "https://medium.com/p/14c2da90a338#1f09", "anchor_text": "Results from the original article"}, {"url": "https://medium.com/p/14c2da90a338#9a0a", "anchor_text": "Regularization, a BN side effect"}, {"url": "https://medium.com/p/14c2da90a338#36f2", "anchor_text": "Normalization during evaluation"}, {"url": "https://medium.com/p/14c2da90a338#6775", "anchor_text": "Stability issues"}, {"url": "https://medium.com/p/14c2da90a338#d000", "anchor_text": "Recurrent network & layer normalization"}, {"url": "https://medium.com/p/14c2da90a338#3321", "anchor_text": "Before or after the non-linearity ?"}, {"url": "https://medium.com/p/14c2da90a338#acf0", "anchor_text": "Why does BN work ?"}, {"url": "https://medium.com/p/14c2da90a338#8c6f", "anchor_text": "First hypothesis : confusion around internal covariate shift (ICS)"}, {"url": "https://medium.com/p/14c2da90a338#e773", "anchor_text": "Second hypothesis : mitigate interdependency between distributions"}, {"url": "https://medium.com/p/14c2da90a338#fb36", "anchor_text": "Third hypothesis : make the optimisation landscape smoother"}, {"url": "https://medium.com/p/14c2da90a338#3fb5", "anchor_text": "Summary : What do we understand for now"}, {"url": "https://medium.com/p/14c2da90a338#ff33", "anchor_text": "Conclusion"}, {"url": "https://medium.com/p/14c2da90a338#6bf0", "anchor_text": "Opened questions"}, {"url": "https://medium.com/p/14c2da90a338#73dc", "anchor_text": "Acknowledgment"}, {"url": "https://medium.com/p/14c2da90a338#c2a9", "anchor_text": "References"}, {"url": "https://medium.com/p/14c2da90a338#75d3", "anchor_text": "Going further"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb", "anchor_text": "this repo"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://en.wikipedia.org/wiki/Moving_average", "anchor_text": "Exponential Moving Average (EMA)"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html", "anchor_text": "torch.nn.BatchNorm1d"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html", "anchor_text": "torch.nn.BatchNorm2d"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html", "anchor_text": "torch.nn.BatchNorm3d"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization", "anchor_text": "tf.nn.batch_normalization"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization", "anchor_text": "tf.keras.layers.BatchNormalization"}, {"url": "https://unsplash.com/@daniloalvesd?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Danilo Alvesd"}, {"url": "https://unsplash.com/s/photos/taps?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb", "anchor_text": "this github repository"}, {"url": "https://www.deeplearningbook.org/", "anchor_text": "Deep learning"}, {"url": "https://www.youtube.com/watch?v=Xogn6veSyxA", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Orthogonality_(programming)", "anchor_text": "orthogonality"}, {"url": "https://unsplash.com/@liacastelli?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Mar\u00edlia Castelli"}, {"url": "https://unsplash.com/s/photos/robotics?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@grailify?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Grailify"}, {"url": "https://unsplash.com/@jimmy2018?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Jia Ye"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/38", "anchor_text": "it just happens"}, {"url": "https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression", "anchor_text": "Pulmonary Fibrosis Progression Kaggle competition"}, {"url": "https://www.youtube.com/watch?v=Xogn6veSyxA", "anchor_text": "here"}, {"url": "https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu", "anchor_text": "example"}, {"url": "https://github.com/keras-team/keras/issues/1802", "anchor_text": "source"}, {"url": "https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/dgqaksn/", "anchor_text": "reddit thread"}, {"url": "https://unsplash.com/@rohitfarmer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Rohit Farmer"}, {"url": "https://unsplash.com/s/photos/child-computer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://unsplash.com/@dhivakrishna?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Dhiva Krishna"}, {"url": "https://unsplash.com/@ferhat?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Ferhat Deniz Fors"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://unsplash.com/@daniloalvesd?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Danilo Alvesd"}, {"url": "https://unsplash.com/s/photos/taps?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/watch?v=Xogn6veSyxA", "anchor_text": "this brilliant video"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://unsplash.com/@tracyzhang?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Tracy Zhang"}, {"url": "https://unsplash.com/s/photos/desert-landscape?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.microsoft.com/en-us/research/video/how-does-batch-normalization-help-optimization/", "anchor_text": "Andrew Ilyas"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou HD"}, {"url": "https://unsplash.com/@finding_dan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Finding Dan | Dan Grinwis"}, {"url": "https://unsplash.com/s/photos/desert?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.instagram.com/louhacquetdelepine/", "anchor_text": "Lou Hacquet-Delepine"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "Batch normalization: Accelerating deep network training by reducing internal covariate shift"}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "How does batch normalization help optimization?"}, {"url": "https://arxiv.org/pdf/1409.4842.pdf", "anchor_text": "Going deeper with convolutions"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep residual learning for image recognition"}, {"url": "https://arxiv.org/abs/1905.11946", "anchor_text": "Efficientnet: Rethinking model scaling for convolutional neural networks"}, {"url": "http://papers.nips.cc/paper/5423-generative-adversarial-nets", "anchor_text": "Generative adversarial nets,"}, {"url": "https://www.youtube.com/watch?v=Xogn6veSyxA", "anchor_text": "link"}, {"url": "https://www.microsoft.com/en-us/research/video/how-does-batch-normalization-help-optimization/", "anchor_text": "link"}, {"url": "https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout", "anchor_text": "link"}, {"url": "https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/dgqaksn/", "anchor_text": "link"}, {"url": "https://medium.com/tag/batch-normalization?source=post_page-----14c2da90a338---------------batch_normalization-----------------", "anchor_text": "Batch Normalization"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----14c2da90a338---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----14c2da90a338---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----14c2da90a338---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----14c2da90a338---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F14c2da90a338&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&user=Johann+Huber&userId=dee40be29033&source=-----14c2da90a338---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F14c2da90a338&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&user=Johann+Huber&userId=dee40be29033&source=-----14c2da90a338---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14c2da90a338&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----14c2da90a338--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F14c2da90a338&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----14c2da90a338---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----14c2da90a338--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----14c2da90a338--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----14c2da90a338--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----14c2da90a338--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----14c2da90a338--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----14c2da90a338--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----14c2da90a338--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----14c2da90a338--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jtkarl?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jtkarl?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Johann Huber"}, {"url": "https://medium.com/@jtkarl/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "120 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdee40be29033&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&user=Johann+Huber&userId=dee40be29033&source=post_page-dee40be29033--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F91fa3b2054c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatch-normalization-in-3-levels-of-understanding-14c2da90a338&newsletterV3=dee40be29033&newsletterV3Id=91fa3b2054c6&user=Johann+Huber&userId=dee40be29033&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}