{"url": "https://towardsdatascience.com/balancing-is-unbalancing-5f517936f626", "time": 1683018420.033438, "path": "towardsdatascience.com/balancing-is-unbalancing-5f517936f626/", "webpage": {"metadata": {"title": "Balancing is Unbalancing. The theory behind imbalanced\u2026 | by Ali Osia | Towards Data Science", "h1": "Balancing is Unbalancing", "description": "Imbalanced classification is a supervised ML problem where the class distribution is too far from uniform (e.g. 5% positive and 95% negative) and usually, the decisions on data with minority class\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Imbalanced classification is a supervised ML problem where the class distribution is too far from uniform (e.g. 5% positive and 95% negative) and usually, the decisions on data with minority class label are significant to be correct. In this case, training is more challenging, because using ordinary methods, the model is getting biassed to estimate the class with majority labels (majority class), while most of the time, we are concerned about correctly estimating minority class. Different techniques are developed to overcome this challenge such as resampling or weighting; all trying to somehow create a balanced dataset and then using common methods. While these techniques are beneficial in applications, all of them seemed heuristics to me, and I couldn\u2019t find an exact mathematical definition of the problem. So I decided to think of what exactly happens, and this document is my understanding of imbalanced data challenge.", "The main concern of supervised probabilistic machine learning is estimating p(y|x) with a parametric distribution q(y|x,\u03b8) using conditional likelihood maximization that can be done by employing cross-entropy loss function (aka log-loss). Furthermore, minimizing the cross-entropy loss function is equivalent to minimizing the KL divergence between the true conditional distribution and the estimated one:", "In this equation, H(Y|X) is constant, so in order to minimize the expected KL divergence, we should minimize the second term, which is equal to the log-loss. To minimize log-loss, first, we need to have samples from p(x,y) to estimate the expectation, and then, we can solve an optimization problem and find q(y|x,\u03b8) as an estimate of p(y|x).", "But what if we just have samples from p\u2019(x,y) = p(x|y) p\u2019(y) instead of p(x,y) where p\u2019(y) is the empirical marginal distribution which is different from p(y)? We can rewrite the log-loss as the following and use sample weights:", "It shows that to learn p(y|x) we need to set sample weights to p(y)/p\u2019(y), which is equal to 1 if the empirical marginal distribution is the same as the true marginal distribution. Therefore, if p(y+)=5%, but the dataset is balanced and p\u2019(y+)=50%, we need to set sample weights to 0.1. This is exactly the opposite of what we are doing for imbalanced datasets where we consider equal weights after balancing. Hence, the main question is what is wrong with using the true marginal distribution for imbalanced datasets? A difficulty lies here and balancing is a way out of that.", "When I was started thinking of this problem, first I remembered the bias towards the majority class and predicting all samples with that, and so I thought that the problem may be due to the hardness of optimization problem and getting stuck in local optima. In fact for imbalanced datasets, marginal and conditional distributions are almost the same with high probability, unlike balanced datasets. Therefore, by setting q(y|x)=p(y) and predicting p(y|x) with p(y), the cross-entropy is equal to H(Y) which is so small. Hence, I thought that predicting with majority class may be a strong local optima in the parameter space. But then I recalled that the loss function is convex for logistic regression and so it should train without any problem. In the following example, without using any balancing techniques, logistic regression works well on imbalanced data, and we can estimate p(y+|x) with \u03c3(w\u1d40x) very well:", "After this simulation, I reminded that in my experience, there is no region of feature space where the probability of minority class is near one (like the above figure), and usually the majority class is observable almost everywhere, but with different probabilities. As another example, consider the following figure, and let\u2019s see how logistic regression works on it:", "You can observe that the logistic regression model is not learned well, and we can not estimate p(y|x) with \u03c3(w\u1d40x). By these observations, we can ask an important question: what is the underlying assumptions about the data that should be met to enable us estimating p(y|x) with \u03c3(w\u1d40x)? The answer can be found in [1]: the class posterior distribution p(y|x) can be written as \u03c3(w\u1d40T(x)) if and only if the class conditional distributions (p(x|y+) and p(x|y-)) are from the same exponential family with sufficient statistic T(x). For example, a common case is when class conditional distributions are both Normal and we want to discriminate them; In this case, the sufficient statistic is [x, x\u00b2] which leads to a quadratic discriminant. If we want to have a linear discriminant, the covariance matrices of the normal distributions should be the same that leads to the cancellation of the quadratic terms (see 4.84 from [2]). Hence, assuming we have a feature extractor that brings us Normal class conditional distributions with the same covariance matrices, \u03c3(w\u1d40x) is a good estimator of p(y|x).", "Now let\u2019s assume that the input \u2014 or the extracted feature from input e.g. by using a neural network (see [3]) \u2014 have normal class conditional distributions and we are dealing with an imbalanced dataset where the majority class has a high probability almost everywhere, even in the region of minority class (exactly like Fig. 2.). In this case, the covariance matrix of the class conditionals can not be the same, and inherently, the scale of the majority class is larger. Also, we can not overcome this issue by transforming into a kernel space, because whatever the transformation is, the majority class is presented in the region of the minority class, in addition to some other regions where the minority class is absent. Therefore, logistic sigmoid of a linear function is not a good estimator of the posterior distribution.", "Suppose we are very radical in choosing the logistic regression model, and although it is a weak learner of the posterior distribution for imbalanced data, we want to use it. In this situation, balancing enables us to build a kernel space in which we have the same scale for class conditional distributions, and make logistic regression as a reasonable candidate. As a result, balancing is a kind of dodge: instead of estimating p(y|x) (which is not possible due to non-uniformity of p(y)) first, estimate p\u2019(y|x) in a balanced setting, and then reconstruct p(y|x). Hence, when we balance a dataset, we ruin the true marginal distribution and, more precisely, we unbalance it\u2019s nature.", "Assume we have predicted p\u2019(y|x) correctly, after resampling and building a balanced dataset by turning p(y) to p\u2019(y), where p\u2019(y+) and p\u2019(y-) are both 0.5. Now its time to reconstruct p(y|x) which can be done through the following:", "where we used p\u2019(x|y)=p(x|y) in the second equation (because we have just changed the marginal distribution of y and p(x|y) is the same as p\u2019(x|y)). Therefore having p\u2019(y|x) results in having a scaled version of p(y|x) which is sufficient to make decisions.", "Now let me discuss two other interesting intuitions, where in the first one I have focused on the importance of minority class, and in the second one, I have addressed how we can model the problem with boosting.", "Considering Fig. 2, if the dataset is imbalanced, the logistic model can not predict the posterior distribution very well, and there are many bad local optima, among them, just a few overestimate the posterior distribution of the minority class, and most, underestimate it. On contrary, usually, the importance of the minority class is higher and we prefer its probability to be overestimated than underestimated. By increasing the weights of the samples from the minority class, we are changing our loss function to the following and extending cross-entropy:", "where ECE is the extended cross-entropy, and \u03b1 shows the sample weights between 0 and 1 which is large for minority class and small for majority class (e.g. \u03b1\u208a=0.8 and \u03b1\u208b=0.2 where the sum is 1). The following two figures compare ECE for \u03b1\u208a=0.5 and \u03b1\u208a=0.8. It is obvious that in Fig. 4, we penalize underestimation of p(y+|x) much more than its overestimation.", "Also setting the derivative of ECE with respect to q to zero, we can find the minimizer of ECE as:", "which is equal to p(y+|x) if \u03b1\u208a=0.5. This equation also gives us a way to reconstruct p(y|x) from q(y|x) where q is estimated on a balanced dataset.", "Logistic regression is a weak classifier on imbalanced data, but as we know, ensembling some weak classifiers can make a strong one. Boosting is one of the methods that sequentially ensemble weak learners by increasing the weights of misclassified samples in each step. Balancing dataset is like considering a two-stage AdaBoost, where the first weak classifier learns nothing but majority class, and the second one trains on a 50\u201350 balanced dataset, and then it is used as the main classifier.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD in Artificial Intelligence, Data Scientist at InnoBrain"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5f517936f626&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5f517936f626--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5f517936f626--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aliosia?source=post_page-----5f517936f626--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aliosia?source=post_page-----5f517936f626--------------------------------", "anchor_text": "Ali Osia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247438009d85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&user=Ali+Osia&userId=247438009d85&source=post_page-247438009d85----5f517936f626---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f517936f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f517936f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://unsplash.com/@ammarelamir?utm_source=medium&utm_medium=referral", "anchor_text": "Ammar ElAmir"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5f517936f626---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5f517936f626---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/imbalanced-data?source=post_page-----5f517936f626---------------imbalanced_data-----------------", "anchor_text": "Imbalanced Data"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----5f517936f626---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----5f517936f626---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f517936f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&user=Ali+Osia&userId=247438009d85&source=-----5f517936f626---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f517936f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&user=Ali+Osia&userId=247438009d85&source=-----5f517936f626---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f517936f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5f517936f626--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5f517936f626&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5f517936f626---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5f517936f626--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5f517936f626--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5f517936f626--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5f517936f626--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5f517936f626--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5f517936f626--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5f517936f626--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5f517936f626--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aliosia?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aliosia?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ali Osia"}, {"url": "https://medium.com/@aliosia/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "68 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247438009d85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&user=Ali+Osia&userId=247438009d85&source=post_page-247438009d85--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2076a223d33c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-is-unbalancing-5f517936f626&newsletterV3=247438009d85&newsletterV3Id=2076a223d33c&user=Ali+Osia&userId=247438009d85&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}