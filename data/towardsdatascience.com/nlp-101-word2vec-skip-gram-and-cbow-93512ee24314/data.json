{"url": "https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314", "time": 1683001549.571876, "path": "towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314/", "webpage": {"metadata": {"title": "NLP 101: Word2Vec \u2014 Skip-gram and CBOW | by Ria Kulshrestha | Towards Data Science", "h1": "NLP 101: Word2Vec \u2014 Skip-gram and CBOW", "description": "Word embedding is just a fancy way of saying numerical representation of words. A good analogy would be how we use the RGB representation for colors. As a human, intuitively speaking, it doesn\u2019t make\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionalit", "paragraph_index": 8}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/", "anchor_text": "softmax layer", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68", "anchor_text": "part II of this post: NLP 101: Negative Sampling and GloVe", "paragraph_index": 22}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter", "paragraph_index": 25}], "all_paragraphs": ["Word embedding is just a fancy way of saying numerical representation of words. A good analogy would be how we use the RGB representation for colors.", "As a human, intuitively speaking, it doesn\u2019t make much sense in wanting to represent words or any other object in the universe using numbers because numbers are used for quantification and why would one need to quantify words?", "When in science, we say speed of my car is 45 km/hr we gain a sense of how fast/slow we are driving. If we say my friend is driving at 60 km/hr, we can compare which one of us is going faster. Furthermore, we can calculate where we will be at a certain point in time, when we will reach our destination given we know the distance of our journey etc etc. Similarly, outside of science, we use numbers to quantify a quality, when we quote the price of an object we try to quantify its worth, the size of a garment we try to quantify the body proportions it will fit best.", "All of these representations make sense because by using numbers we have made analysis and comparisons based on those qualities much much easier. What\u2019s worth more a shoe or a purse? Well, as different as those two objects are, one way to answer that is to compare their prices. Other than the quantification aspect, there isn\u2019t any thing else to be gained by this representation.", "Now that we know numerical representation of objects aids in analysis by quantifying a certain quality, the question is what quality of words do we want to quantify?", "The answer to that is, we want to quantify the semantics. We want to represent words in such a manner that it captures its meaning in a way humans do. Not the exact meaning of the word but a contextual one. For example, when I say the word see, we know exactly what action \u2014 the context \u2014 I\u2019m talking about, even though we might not be able to quote its meaning, the kind we would find in a dictionary, of the top of our head.", "The simplest word embedding you can have is using one-hot vectors. If you have 10,000 words in your vocabulary, then you can represent each word as a 1x10,000 vector.", "For a simple example, if we have 4 words \u2014 mango, strawberry, city, Delhi \u2014 in our vocabulary then we can represent them as following:", "There are a few problems with the above approach, firstly, our size of vectors depends on the size of our vocabulary(which can be huge). This is a wastage of space and increases algorithm complexity exponentially resulting in the curse of dimensionality.", "Secondly, these embedding will be closely coupled to their applications, making transfer-learning to a model using a different vocabulary of the same size, adding/removing words from vocabulary would almost impossible as it would require to re-train the whole model again.", "Lastly, the entire purpose of creating embedding is to capture the contextual meaning of the words, which this representation fails to do. There is no co-relation between words that have similar meaning or usage.", "Both are architectures to learn the underlying word representations for each word by using neural networks.", "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. While in the Skip-gram model, the distributed representation of the input word is used to predict the context.", "A prerequisite for any neural network or any supervised training technique is to have labeled training data. How do you a train a neural network to predict word embedding when you don\u2019t have any labeled data i.e words and their corresponding word embedding?", "We\u2019ll do so by creating a \u201cfake\u201d task for the neural network to train. We won\u2019t be interested in the inputs and outputs of this network, rather the goal is actually just to learn the weights of the hidden layer that are actually the \u201cword vectors\u201d that we\u2019re trying to learn.", "The fake task for Skip-gram model would be, given a word, we\u2019ll try to predict its neighboring words. We\u2019ll define a neighboring word by the window size \u2014 a hyper-parameter.", "Given the sentence: \u201cI will have orange juice and eggs for breakfast.\u201dand a window size of 2, if the target word is juice, its neighboring words will be ( have, orange, and, eggs). Our input and target word pair would be (juice, have), (juice, orange), (juice, and), (juice, eggs).Also note that within the sample window, proximity of the words to the source word plays no role. So have, orange, and, and eggs will be treated the same while training.", "The dimensions of the input vector will be 1xV \u2014 where V is the number of words in the vocabulary \u2014 i.e one-hot representation of the word. The single hidden layer will have dimension VxE, where E is the size of the word embedding and is a hyper-parameter. The output from the hidden layer would be of the dimension 1xE, which we will feed into an softmax layer. The dimensions of the output layer will be 1xV, where each value in the vector will be the probability score of the target word at that position. According to our earlier example if we have a vector [0.2, 0.1, 0.3, 0.4], the probability of the word being mango is 0.2, strawberry is 0.1, city is 0.3 and Delhi is 0.4.", "The back propagation for training samples corresponding to a source word is done in one back pass. So for juice, we will complete the forward pass for all 4 target words ( have, orange, and, eggs). We will then calculate the errors vectors[1xV dimension] corresponding to each target word. We will now have 4 1xV error vectors and will perform an element-wise sum to get a 1xV vector. The weights of the hidden layer will be updated based on this cumulative 1xV error vector.", "The fake task in CBOW is somewhat similar to Skip-gram, in the sense that we still take a pair of words and teach the model that they co-occur but instead of adding the errors we add the input words for the same target word.", "The dimension of our hidden layer and output layer will remain the same. Only the dimension of our input layer and the calculation of hidden layer activations will change, if we have 4 context words for a single target word, we will have 4 1xV input vectors. Each will be multiplied with the VxE hidden layer returning 1xE vectors. All 4 1xE vectors will be averaged element-wise to obtain the final activation which then will be fed into the softmax layer.", "Skip-gram: works well with a small amount of the training data, represents well even rare words or phrases.CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.", "In part II of this post: NLP 101: Negative Sampling and GloVe, we discuss:", "I\u2018m glad you made it till the end of this article. \ud83c\udf89I hope your reading experience was as enriching as the one I had writing this. \ud83d\udc96", "Do check out my other articles here.", "If you want to reach out to me, my medium of choice would be Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F93512ee24314&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----93512ee24314--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----93512ee24314--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----93512ee24314--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----93512ee24314--------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e----93512ee24314---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F93512ee24314&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F93512ee24314&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@sincerelymedia?utm_source=medium&utm_medium=referral", "anchor_text": "Sincerely Media"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionalit"}, {"url": "https://arxiv.org/pdf/1309.4168v1.pdf", "anchor_text": "Exploiting Similarities among Languages for Machine Translation"}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "McCormickml tutorial"}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/", "anchor_text": "softmax layer"}, {"url": "https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68", "anchor_text": "part II of this post: NLP 101: Negative Sampling and GloVe"}, {"url": "http://cs224d.stanford.edu/lecture_notes/notes1.pdf", "anchor_text": "Lecture notes CS224D: Deep Learning for NLP Part-I"}, {"url": "https://cs224d.stanford.edu/lecture_notes/LectureNotes2.pdf", "anchor_text": "Lecture notes CS224D: Deep Learning for NLP Part-II"}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "McCormick, C. (2016, April 19). Word2Vec Tutorial \u2014 The Skip-Gram Model."}, {"url": "https://towardsdatascience.com/back-propagation-721bfcc94e34", "anchor_text": "Yes, you should listen to Andrej Karpathy, and understand Back propagation"}, {"url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "anchor_text": "Evaluation of an NLP model \u2014 latest benchmarks"}, {"url": "https://towardsdatascience.com/attaining-attention-in-deep-learning-a712f93bdb1e", "anchor_text": "Understanding Attention In Deep Learning"}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "Transformers"}, {"url": "https://medium.com/@ria.kulshrestha16/keeping-up-with-the-berts-5b7beb92766", "anchor_text": "Google\u2019s BERT and OpenAI\u2019s GPT"}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here"}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----93512ee24314---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----93512ee24314---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----93512ee24314---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----93512ee24314---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/ai?source=post_page-----93512ee24314---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F93512ee24314&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----93512ee24314---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F93512ee24314&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----93512ee24314---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F93512ee24314&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----93512ee24314--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F93512ee24314&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----93512ee24314---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----93512ee24314--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----93512ee24314--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----93512ee24314--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----93512ee24314--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----93512ee24314--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----93512ee24314--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----93512ee24314--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----93512ee24314--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://ria-kulshrestha.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "768 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fccdcb0ec19a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-101-word2vec-skip-gram-and-cbow-93512ee24314&newsletterV3=406aa3cbd38e&newsletterV3Id=ccdcb0ec19a4&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}