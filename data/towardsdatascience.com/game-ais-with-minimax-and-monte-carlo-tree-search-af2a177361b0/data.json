{"url": "https://towardsdatascience.com/game-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0", "time": 1682995628.4157062, "path": "towardsdatascience.com/game-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0/", "webpage": {"metadata": {"title": "Game AIs with Minimax and Monte Carlo Tree Search | by Philipp Muens | Towards Data Science", "h1": "Game AIs with Minimax and Monte Carlo Tree Search", "description": "Do you remember your childhood days when you discovered the infamous game Tic-Tac-Toe and played it with your friends over and over again? You might\u2019ve wondered if there\u2019s a certain strategy you can\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Tic-tac-toe", "anchor_text": "Tic-Tac-Toe", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Tic-tac-toe", "anchor_text": "Tic-Tac-Toe", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Connect_Four", "anchor_text": "Connect Four", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Chess", "anchor_text": "Chess", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Go_(game)", "anchor_text": "Go", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search", "anchor_text": "Monte Carlo Tree Search", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax algorithm", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning", "anchor_text": "Alpha-Beta pruning", "paragraph_index": 41}, {"url": "https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)", "anchor_text": "Deep Blue", "paragraph_index": 43}, {"url": "https://www.ibm.com/", "anchor_text": "IBM", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov", "anchor_text": "defeated", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Garry_Kasparov", "anchor_text": "Garry Kasparov", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search", "anchor_text": "Monte Carlo Tree Search (MCTS)", "paragraph_index": 48}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo", "anchor_text": "Monte Carlo", "paragraph_index": 48}, {"url": "https://en.wikipedia.org/wiki/Law_of_large_numbers", "anchor_text": "Law of large numbers (LLN)", "paragraph_index": 51}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind", "paragraph_index": 75}, {"url": "https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks", "anchor_text": "Deep Neural Networks", "paragraph_index": 75}, {"url": "https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol", "anchor_text": "defeated", "paragraph_index": 75}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax", "paragraph_index": 77}, {"url": "https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning", "anchor_text": "Alpha-Beta pruning", "paragraph_index": 77}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search", "anchor_text": "Monte Carlo Tree Search (MCTS)", "paragraph_index": 77}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind", "paragraph_index": 78}, {"url": "https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf", "anchor_text": "combined MCTS with Deep Neural Networks", "paragraph_index": 78}, {"url": "https://ibm.com/", "anchor_text": "IBM", "paragraph_index": 78}, {"url": "https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)", "anchor_text": "compute the best possible Chess moves", "paragraph_index": 78}, {"url": "https://philippmuens.com", "anchor_text": "https://philippmuens.com", "paragraph_index": 82}], "all_paragraphs": ["Do you remember your childhood days when you discovered the infamous game Tic-Tac-Toe and played it with your friends over and over again?", "You might\u2019ve wondered if there\u2019s a certain strategy you can exploit that lets you win all the time (or at least force a draw). Is there such an algorithm that will show you how you can defeat your opponent at any given time?", "It turns out there is. To be precise there are a couple of algorithms which can be utilized to predict the best possible moves in games such as Tic-Tac-Toe, Connect Four, Chess and Go among others. One such family of algorithms leverages tree search and operates on game state trees.", "In this blog post we\u2019ll discuss 2 famous tree search algorithms called Minimax and Monte Carlo Tree Search (abbreviated to MCTS). We\u2019ll start our journey into tree search algorithms by discovering the intuition behind their inner workings. After that we\u2019ll see how Minimax and MCTS can be used in modern game implementations to build sophisticated Game AIs. We\u2019ll also shed some light into the computational challenges we\u2019ll face and how to handle them via performance optimization techniques.", "Let\u2019s imagine that you\u2019re playing some games of Tic-Tac-Toe with your friends. While playing you\u2019re wondering what the optimal strategy might be. What\u2019s the best move you should pick in any given situation?", "Generally speaking there are 2 modes you can operate in when determining the next move you want to play:", "These modes and their respective actions are basically the only strategies you need to follow to win the game of Tic-Tac-Toe.", "The \u201conly\u201d thing you need to do is to look at the current game state you\u2019re in and play simulations through all the potential next moves which could be played. You do this by pretending that you\u2019ve played a given move and then continue playing the game until the end, alternating between the X and O player. While doing that you\u2019re building up a game tree of all the possible moves you and your opponent would play.", "The following illustration shows a simplified version of such a game tree:", "Note that for the rest of this post we\u2019ll only use simplified game tree examples to save screen space", "Of course, the set of strategic rules we\u2019ve discussed at the top is specifically tailored to the game of Tic-Tac-Toe. However we can generalize this approach to make it work with other board games such as Chess or Go. Let\u2019s take a look at Minimax, a tree search algorithm which abstracts our Tic-Tac-Toe strategy so that we can apply it to various other 2 player board games.", "Given that we\u2019ve built up an intuition for tree search algorithms let\u2019s switch our focus from simple games such as Tic-Tac-Toe to more complex games such as Chess.", "Before we dive in let\u2019s briefly recap the properties of a Chess game. Chess is a 2 player deterministic game of perfect information. Sound confusing? Let\u2019s unpack it:", "In Chess, 2 players (Black and White) play against each other. Every move which is performed is ensured to be \u201cfulfilled\u201d with no randomness involved (the game doesn\u2019t use any random elements such as a die). During gameplay every player can observe the whole game state. There\u2019s no hidden information, hence everyone has perfect information about the whole game at any given time.", "Thanks to those properties we can always compute which player is currently ahead and which one is behind. There are several different ways to do this for the game of Chess. One approach to evaluate the current game state is to add up all the remaining white pieces on the board and subtract all the remaining black ones. Doing this will produce a single value where a large value favors white and a small value favors black. This type of function is called an evaluation function.", "Based on this evaluation function we can now define the overall goal during the game for each player individually. White tries to maximize this objective while black tries to minimize it.", "Let\u2019s pretend that we\u2019re deep in an ongoing Chess game. We\u2019re player white and have already played a couple of clever moves, resulting in a large number computed by our evaluation function. It\u2019s our turn right now but we\u2019re stuck. Which of the possible moves is the best one we can play?", "We\u2019ll solve this problem with the same approach we already encountered in our Tic-Tac-Toe gameplay example. We build up a tree of potential moves which could be performed based on the game state we\u2019re in. To keep things simple we pretend that there are only 2 possible moves we can play (in Chess there are on average ~30 different options for every given game state). We start with a (white) root node which represents the current state. Starting from there we\u2019re branching out 2 (black) child nodes which represent the game state we\u2019re in after taking one of the 2 possible moves. From these 2 child nodes we\u2019re again branching out 2 separate (white) child nodes. Each one of those represents the game state we\u2019re in after taking one of the 2 possible moves we could play from the black node. This branching out of nodes goes on and on until we\u2019ve reached the end of the game or hit a predefined maximum tree depth.", "The resulting tree looks something like this:", "Given that we\u2019re at the end of the tree we can now compute the game outcome for each end state with our evaluation function:", "With this information we now know the game outcome we can expect when we take all the outlined moves starting from the root node and ending at the last node where we calculated the game evaluation. Since we\u2019re player white it seems like the best move to pick is the one which will set us up to eventually end in the game state with the highest outcome our evaluation function calculated.", "While this is true there\u2019s one problem. There\u2019s still the black player involved and we cannot directly manipulate what move she\u2019ll pick. If we cannot manipulate this why don\u2019t we estimate what the black player will likely do based on our evaluation function? As a white player we always try to maximize our outcome. The black player always tries to minimize the outcome. With this knowledge we can now traverse back through our game tree and compute the values for all our individual tree nodes step by step.", "White tries to maximize the outcome:", "While black wants to minimize it:", "Once done we can now pick the next move based on the evaluation values we\u2019ve just computed. In our case we pick the next possible move which maximizes our outcome:", "What we\u2019ve just learned is the general procedure of the so-called Minimax algorithm. The Minimax algorithm got its name from the fact that one player wants to Mini-mize the outcome while the other tries to Max-imize it.", "Minimax is a simple and elegant tree search algorithm. Given enough compute resources it will always find the optimal next move to play.", "But there\u2019s a problem. While this algorithm works flawlessly with simplistic games such as Tic-Tac-Toe, it\u2019s computationally infeasible to implement it for strategically more involved games such as Chess. The reason for this is the so-called tree branching factor. We\u2019ve already briefly touched on that concept before but let\u2019s take a second look at it.", "In our example above we\u2019ve artificially restricted the potential moves one can play to 2 to keep the tree representation simple and easy to reason about. However the reality is that there are usually more than 2 possible next moves. On average there are ~30 moves a Chess player can play in any given game state. This means that every single node in the tree will have approximately 30 different children. This is called the width of the tree. We denote the trees width as w.", "But there\u2019s more. It takes roughly ~85 consecutive turns to finish a game of Chess. Translating this to our tree means that it will have an average depth of 85. We denote the trees depth as d.", "Given w and d we can define the formula w^d which will show us how many different positions we have to evaluate on average.", "Plugging in the numbers for Chess we get 30^85. Taking the Go board game as an example which has a width w of ~250 and an average depth d of ~150 we get 250^150. I encourage you to type those numbers into your calculator and hit enter. Needless to say that current generation computers and even large scale distributed systems will take \u201cforever\u201d to crunch through all those computations.", "Does this mean that Minimax can only be used for games such as Tic-Tac-Toe? Absolutely not. We can apply some clever tricks to optimize the structure of our search tree.", "Generally speaking we can reduce the search trees width and depth by pruning individual nodes and branches from it. Let\u2019s see how this works in practice.", "Recall that Minimax is built around the premise that one player tries to maximize the outcome of the game based on the evaluation function while the other one tries to minimize it.", "This gameplay behavior is directly translated into our search tree. During traversal from the bottom to the root node we always picked the respective \u201cbest\u201d move for any given player. In our case the white player always picked the maximum value while the black player picked the minimum value:", "Looking at our tree above we can exploit this behavior to optimize it. Here\u2019s how:", "While walking through the potential moves we can play given the current game state we\u2019re in we should build our tree in a depth-first fashion. This means that we should start at one node and expand it by playing the game all the way to the end before we back up and pick the next node we want to explore:", "Following this procedure allows us to identify moves which will never be played early on. After all, one player maximizes the outcome while the other minimizes it. The part of the search tree where a player would end up in a worse situation based on the evaluation function can be entirely removed from the list of nodes we want to expand and explore. We prune those nodes from our search tree and therefore reduce its width.", "The larger the branching factor of the tree, the higher the amount of computations we can potentially save!", "Assuming we can reduce the width by an average of 10 we would end up with w^d=(30\u221210)^85 = 20^85 computations we have to perform. That\u2019s already a huge win.", "This technique of pruning parts of the search tree which will never be considered during gameplay is called Alpha-Beta pruning. Alpha-Beta pruning got its name from the parameters \u03b1 and \u03b2 which are used to keep track of the best score either player can achieve while walking the tree.", "Using Alpha-Beta pruning to reduce the trees width helps us utilize the Minimax algorithm in games with large branching factors which were previously considered as computationally too expensive.", "In fact Deep Blue, the Chess computer developed by IBM which defeated the Chess world champion Garry Kasparov in 1997 heavily utilized parallelized Alpha-Beta based search algorithms.", "It seems like Minimax combined with Alpha-Beta pruning is enough to build sophisticated game AIs. But there\u2019s one major problem which can render such techniques useless. It\u2019s the problem of defining a robust and reasonable evaluation function. Recall that in Chess our evaluation function added up all the white pieces on the board and subtracted all the black ones. This resulted in high values when white had an edge and in low values when the situation was favorable for black. While this function is a good baseline and is definitely worthwhile to experiment with there are usually more complexities and subtleties one needs to incorporate to come up with a sound evaluation function.", "Simple evaluation metrics are easy to fool and exploit once the underlying internals are surfaced. This is especially true for more complex games such as Go. Engineering an evaluation function which is complex enough to capture the majority of the necessary game information requires a lot of thought and interdisciplinary domain expertise in Software Engineering, Math, Psychology and the game at hand.", "Isn\u2019t there a universally applicable evaluation function we could leverage for all games, no matter how simple or complex they are?", "Yes, there is! And it\u2019s called randomness. With randomness we let chance be our guide to figure out which next move might be the best one to pick.", "In the following we\u2019ll explore the so-called Monte Carlo Tree Search (MCTS) algorithm which heavily relies on randomness (the name \u201cMonte Carlo\u201d stems from the gambling district in Monte Carlo) as a core component for value approximations.", "As the name implies, MCTS also builds up a game tree and does computations on it to find the path of the highest potential outcome. But there\u2019s a slight difference in how this tree is constructed.", "Let\u2019s once again pretend that we\u2019re playing Chess as player white. We\u2019ve already played for a couple of rounds and it\u2019s on us again to pick the next move we\u2019d like to play. Additionally let\u2019s pretend that we\u2019re not aware of any evaluation function we could leverage to compute the value of each possible move. Is there any way we could still figure out which move might put us into a position where we could win at the end?", "As it turns out there\u2019s a really simple approach we can take to figure this out. Why don\u2019t we let both player play dozens of random games starting from the state we\u2019re currently in? While this might sound counterintuitive it make sense if you think about it. If both player start in the given game state, play thousands of random games and player white wins 80% of the time, then there must be something about the state which gives white an advantage. What we\u2019re doing here is basically exploiting the Law of large numbers (LLN) to find the \u201ctrue\u201d game outcome for every potential move we can play.", "The following description will outline how the MCTS algorithm works in detail. For the sake of simplicity we again focus solely on 2 playable moves in any given state (as we\u2019ve already discovered there are on average ~30 different moves we can play in Chess).", "Before we move on we need to get some minor definitions out of the way. In MCTS we keep track of 2 different parameters for every single node in our tree. We call those parameters t and n. t stands for \u201ctotal\u201d and represents the total value of that node. n is the \u201cnumber of visits\u201d which reflects the number of times we\u2019ve visited this node while walking through the tree. When creating a new node we always initialize both parameters with the value 0.", "In addition to the 2 new parameters we store for each node, there\u2019s the so-called \u201cUpper Confidence Bound 1\u201d (UCT) formula which looks like this", "This formula basically helps us in deciding which upcoming node and therefore potential game move we should pick to start our random game series (called \u201crollout\u201d) from. In the formula xi\u200b represents the average value of the game state we\u2019re working with, C is a constant called \u201ctemperature\u201d we need to define manually (we just set it to 1.5 in our example here. More on that later), N represents the parent node visits and ni\u200b represents the current nodes visits. When using this formula on candidate nodes to decide which one to explore further, we\u2019re always interested in the largest result.", "Don\u2019t be intimidated by the Math and just note that this formula exists and will be useful for us while working with out tree. We\u2019ll get into more details about the usage of it while walking through our tree.", "With this out of the way it\u2019s time apply MCTS to find the best move we can play.", "We start with the same root node of the tree we\u2019re already familiar with. This root node is our start point and reflects the current game state. Based on this node we branch off our 2 child nodes:", "The first thing we need to do is to use the UCT formula from above and compute the results for both child nodes. As it turns out we need to plug in 0 for almost every single variable in our UCT formula since we haven\u2019t done anything with our tree and its nodes yet. This will result in \u221e for both calculations.", "We\u2019ve replaced the 0 in the denominator with a very small number because division by zero is not defined", "Given this we\u2019re free to choose which node we want to explore further. We go ahead with the leftmost node and perform our rollout phase which means that we play dozens of random games starting with this game state.", "Once done we get a result for this specific rollout (in our case the percentage of wins for player white). The next thing we need to do is to propagate this result up the tree until we reach the root node. While doing this we update both t and n with the respective values for every node we encounter. Once done our tree looks like this:", "Next up we start at our root node again. Once again we use the UCT formula, plug in our numbers and compute its score for both nodes:", "Given that we always pick the node with the highest value we\u2019ll now explore the rightmost one. Once again we perform our rollout based on the move this node proposes and collect the end result after we\u2019ve finished all our random games.", "The last thing we need to do is to propagate this result up until we reach the root of the tree. While doing this we update the parameters of every node we encounter.", "We\u2019ve now successfully explored 2 child nodes in our tree. You might\u2019ve guessed it already. We\u2019ll start again at our root node and calculate every child nodes UCT score to determine the node we should further explore. In doing this we get the following values:", "The largest value is the one we\u2019ve computed for the leftmost node so we decide to explore that node further.", "Given that this node has no child nodes we add two new nodes which represent the potential moves we can play to the tree. We initialize both of their parameters (t and n) with 0.", "Now we need to decide which one of those two nodes we should explore further. And you\u2019re right. We use the UCT formula to calculate their values. Given that both have t and n values of zero they\u2019re both \u221e so we decide to pick the leftmost node. Once again we do a rollout, retrieve the value of those games and propagate this value up to the tree until we reach the trees root node, updating all the node parameters along the way.", "The next iteration will once again start at the root node where we use the UCT formula to decide which child node we want to explore further. Since we can see a pattern here and I don\u2019t want to bore you I\u2019m not going to describe the upcoming steps in great detail. What we\u2019ll be doing is following the exact same procedure we\u2019ve used above which can be summarized as follows:", "We iterate over this algorithm until we run out of time or reached a predefined threshold value of visits, depth or iterations. Once this happens we evaluate the current state of our tree and pick the child node(s) which maximize the value t. Thanks to dozens of games we\u2019ve played and the Law of large numbers we can be very certain this move is the best one we can possibly play.", "That\u2019s all there is. We\u2019ve just learned, applied and understood Monte Carlo Tree Search!", "You might agree that it seems like MCTS is very compute intensive since you have to run through thousands of random games. This is definitely true and we need to be very clever as to where we should invest our resources to find the most promising path in our tree. We can control this behavior with the aforementioned \u201ctemperature\u201d parameter C in our UCT formula. With this parameter we balance the trade-off between \u201cexploration vs. exploitation\u201d.", "A large C value puts us into \u201cexploration\u201d mode. We\u2019ll spend more time visiting least-explored nodes. A small value for C puts us into \u201cexploitation\u201d mode where we\u2019ll revisit already explored nodes to gather more information about them.", "Given the simplicity and applicability due to the exploitation of randomness, MCTS is a widely used game tree search algorithm. DeepMind extended MCTS with Deep Neural Networks to optimize its performance in finding the best Go moves to play. The resulting Game AI was so strong that it reached superhuman level performance and defeated the Go World Champion Lee Sedol 4\u20131.", "In this blog post we\u2019ve looked into 2 different tree search algorithms which can be used to build sophisticated Game AIs.", "While Minimax combined with Alpha-Beta pruning is a solid solution to approach games where an evaluation function to estimate the game outcome can easily be defined, Monte Carlo Tree Search (MCTS) is a universally applicable solution given that no evaluation function is necessary due to its reliance on randomness.", "Raw Minimax and MCTS are only the start and can easily be extended and modified to work in more complex environments. DeepMind cleverly combined MCTS with Deep Neural Networks to predict Go game moves whereas IBM extended Alpha-Beta tree search to compute the best possible Chess moves to play.", "I hope that this introduction to Game AI algorithms sparked your interest in Artificial Intelligence and helps you understand the underlying mechanics you\u2019ll encounter the next time you pick up a board game on your computer.", "Do you have any questions / feedback? Feel free to reach out to me via E-Mail or Twitter!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\ud83d\udc68\u200d\ud83d\udcbb Maker \u2014 \ud83d\udc68\u200d\ud83c\udfeb Lifelong learner \u2014 Co-creator of the Serverless Framework \u2014 https://philippmuens.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faf2a177361b0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----af2a177361b0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----af2a177361b0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://pmuens.medium.com/?source=post_page-----af2a177361b0--------------------------------", "anchor_text": ""}, {"url": "https://pmuens.medium.com/?source=post_page-----af2a177361b0--------------------------------", "anchor_text": "Philipp Muens"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa770eb52909c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&user=Philipp+Muens&userId=a770eb52909c&source=post_page-a770eb52909c----af2a177361b0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf2a177361b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf2a177361b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/towards-data-science/inside-ai/home", "anchor_text": "Inside AI"}, {"url": "https://unsplash.com/@veeterzy?utm_source=medium&utm_medium=referral", "anchor_text": "veeterzy"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Tic-tac-toe", "anchor_text": "Tic-Tac-Toe"}, {"url": "https://en.wikipedia.org/wiki/Tic-tac-toe", "anchor_text": "Tic-Tac-Toe"}, {"url": "https://en.wikipedia.org/wiki/Connect_Four", "anchor_text": "Connect Four"}, {"url": "https://en.wikipedia.org/wiki/Chess", "anchor_text": "Chess"}, {"url": "https://en.wikipedia.org/wiki/Go_(game)", "anchor_text": "Go"}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax"}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search", "anchor_text": "Monte Carlo Tree Search"}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax"}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax algorithm"}, {"url": "https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning", "anchor_text": "Alpha-Beta pruning"}, {"url": "https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)", "anchor_text": "Deep Blue"}, {"url": "https://www.ibm.com/", "anchor_text": "IBM"}, {"url": "https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov", "anchor_text": "defeated"}, {"url": "https://en.wikipedia.org/wiki/Garry_Kasparov", "anchor_text": "Garry Kasparov"}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search", "anchor_text": "Monte Carlo Tree Search (MCTS)"}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo", "anchor_text": "Monte Carlo"}, {"url": "https://en.wikipedia.org/wiki/Law_of_large_numbers", "anchor_text": "Law of large numbers (LLN)"}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind"}, {"url": "https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks", "anchor_text": "Deep Neural Networks"}, {"url": "https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol", "anchor_text": "defeated"}, {"url": "https://en.wikipedia.org/wiki/Minimax", "anchor_text": "Minimax"}, {"url": "https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning", "anchor_text": "Alpha-Beta pruning"}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search", "anchor_text": "Monte Carlo Tree Search (MCTS)"}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind"}, {"url": "https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf", "anchor_text": "combined MCTS with Deep Neural Networks"}, {"url": "https://ibm.com/", "anchor_text": "IBM"}, {"url": "https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)", "anchor_text": "compute the best possible Chess moves"}, {"url": "https://philippmuens.com/minimax-and-mcts", "anchor_text": "philippmuens.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----af2a177361b0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----af2a177361b0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----af2a177361b0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----af2a177361b0---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/inside-ai?source=post_page-----af2a177361b0---------------inside_ai-----------------", "anchor_text": "Inside Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf2a177361b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&user=Philipp+Muens&userId=a770eb52909c&source=-----af2a177361b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf2a177361b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&user=Philipp+Muens&userId=a770eb52909c&source=-----af2a177361b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf2a177361b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----af2a177361b0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faf2a177361b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----af2a177361b0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----af2a177361b0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----af2a177361b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----af2a177361b0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----af2a177361b0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----af2a177361b0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----af2a177361b0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----af2a177361b0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----af2a177361b0--------------------------------", "anchor_text": ""}, {"url": "https://pmuens.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://pmuens.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Philipp Muens"}, {"url": "https://pmuens.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "131 Followers"}, {"url": "https://philippmuens.com", "anchor_text": "https://philippmuens.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa770eb52909c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&user=Philipp+Muens&userId=a770eb52909c&source=post_page-a770eb52909c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fa770eb52909c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgame-ais-with-minimax-and-monte-carlo-tree-search-af2a177361b0&user=Philipp+Muens&userId=a770eb52909c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}