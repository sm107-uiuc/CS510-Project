{"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b", "time": 1683017967.681761, "path": "towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b/", "webpage": {"metadata": {"title": "Reinforcement Learning Explained Visually (Part 5): Deep Q Networks, step-by-step | by Ketan Doshi | Towards Data Science", "h1": "Reinforcement Learning Explained Visually (Part 5): Deep Q Networks, step-by-step", "description": "A Gentle Guide to DQNs with Experience Replay, in Plain English. How they work, and why those architectural choices were made."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e", "anchor_text": "fourth one on Q-Learning", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e", "anchor_text": "previous article", "paragraph_index": 56}], "all_paragraphs": ["This is the fifth article in my series on Reinforcement Learning (RL). We now have a good understanding of the concepts that form the building blocks of an RL problem, and the techniques used to solve them. We have also taken a detailed look at the Q-Learning algorithm which forms the foundation of Deep Q Networks (DQN) which is the focus of this article.", "With DQNs, we are finally able to being our journey into Deep Reinforcement Learning which is perhaps the most innovative area of Reinforcement Learning today. We\u2019ll go through this algorithm step-by-step including some of the game-changing innovations like Experience Replay to understand exactly how they helped DQNs achieve their world-beating results when they were first introduced.", "Here\u2019s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.", "If you haven\u2019t read the earlier articles, particularly the fourth one on Q-Learning, it would be a good idea to read them first, as this article builds on many of the concepts that we discussed there.", "Q Learning builds a Q-table of State-Action values, with dimension (s, a), where s is the number of states and a is the number of actions. Fundamentally, a Q-table maps state and action pairs to a Q-value.", "However, in a real-world scenario, the number of states could be huge, making it computationally intractable to build a table.", "To address this limitation we use a Q-function rather than a Q-table, which achieves the same result of mapping state and action pairs to a Q value.", "Since neural networks are excellent at modeling complex functions, we can use a neural network, which we call a Deep Q Network, to estimate this Q function.", "This function maps a state to the Q values of all the actions that can be taken from that state.", "It learns the network\u2019s parameters (weights) such that it can output the Optimal Q values.", "The underlying principle of a Deep Q Network is very similar to the Q Learning algorithm. It starts with arbitrary Q-value estimates and explores the environment using the \u03b5-greedy policy. And at its core, it uses the same notion of dual actions, a current action with a current Q-value and a target action with a target Q-value, for its update logic to improve its Q-value estimates.", "The DQN architecture has two neural nets, the Q network and the Target networks, and a component called Experience Replay. The Q network is the agent that is trained to produce the Optimal State-Action value.", "Experience Replay interacts with the environment to generate data to train the Q Network.", "The Q Network is a fairly standard neural network architecture and could be as simple as a linear network with a couple of hidden layers if your state can be represented via a set of numeric variables. Or if your state data is represented as images or text, you might use a regular CNN or RNN architecture.", "The Target network is identical to the Q network.", "The DQN gets trained over multiple time steps over many episodes. It goes through a sequence of operations in each time step:", "Now let\u2019s zoom in on this first phase.", "Experience Replay selects an \u03b5-greedy action from the current state, executes it in the environment, and gets back a reward and the next state.", "It saves this observation as a sample of training data.", "Next, we\u2019ll zoom in on the next phase of the flow.", "All prior Experience Replay observations are saved as training data. We now take a random batch of samples from this training data, so that it contains a mix of older and more recent samples.", "This batch of training data is then inputted to both networks. The Q network takes the current state and action from each data sample and predicts the Q value for that particular action. This is the \u2018Predicted Q Value\u2019.", "The Target network takes the next state from each data sample and predicts the best Q value out of all actions that can be taken from that state. This is the \u2018Target Q Value\u2019.", "The Predicted Q Value, Target Q Value, and the observed reward from the data sample is used to compute the Loss to train the Q Network. The Target Network is not trained.", "You are probably wondering why we need a separate Experience Replay memory at all? Why don\u2019t we simply take an action, observe results from the environment, and then feed that data to the Q Network?", "The answer to that is straightforward. We know that neural networks typically take a batch of data. If we trained it with single samples, each sample and the corresponding gradients would have too much variance, and the network weights would never converge.", "Alright, in that case, the obvious answer is why don\u2019t we take a few actions in sequence one after the other and then feed that data as a batch to the Q Network? That should help to smoothen out the noise and result in more stable training, shouldn\u2019t it?", "Here the answer is much more subtle. Recall that when we train neural networks, a best practice is to select a batch of samples after shuffling the training data randomly. This ensures that there is enough diversity in the training data to allow the network to learn meaningful weights that generalize well and can handle a range of data values.", "Would that occur if we passed a batch of data from sequential actions? Let\u2019s take a scenario of a robot learning to navigate a factory floor. Let\u2019s say that at a certain point in time, it is trying to find its way around a particular corner of the factory. All of the actions that it would take over the next few moves would be confined to that section of the factory.", "If the network tried to learn from that batch of actions, it would update its weights to deal specifically with that location in the factory. But it would not learn anything about other parts of the factory. If sometime later, the robot moves to another location, all of its actions and hence the network\u2019s learnings for a while would be narrowly focused on that new location. It might then undo what it had learned from the original location.", "Hopefully, you\u2019re starting to see the problem here. Sequential actions are highly correlated with one another and are not randomly shuffled, as the network would prefer. This results in a problem called Catastrophic Forgetting where the network unlearns things that it had learned a short while earlier.", "This is why the Experience Replay memory was introduced. All of the actions and observations that the agent has taken from the beginning (limited by the capacity of the memory, of course) are stored. Then a batch of samples is randomly selected from this memory. This ensures that the batch is \u2018shuffled\u2019 and contains enough diversity from older and newer samples (eg. from several regions of the factory floor and under different conditions) to allow the network to learn weights that generalize to all the scenarios that it will be required to handle.", "The second puzzling thing is why we need a second neural network? And that network is not getting trained, so that makes it all the more puzzling.", "Firstly, it is possible to build a DQN with a single Q Network and no Target Network. In that case, we do two passes through the Q Network, first to output the Predicted Q value, and then to output the Target Q value.", "But that could create a potential problem. The Q Network\u2019s weights get updated at each time step, which improves the prediction of the Predicted Q value. However, since the network and its weights are the same, it also changes the direction of our predicted Target Q values. They do not remain steady but can fluctuate after each update. This is like chasing a moving target \ud83d\ude04.", "By employing a second network that doesn\u2019t get trained, we ensure that the Target Q values remain stable, at least for a short period. But those Target Q values are also predictions after all and we do want them to improve, so a compromise is made. After a pre-configured number of time-steps, the learned weights from the Q Network are copied over to the Target Network.", "It has been found that using a Target Network results in more stable training.", "Now that we understand the overall flow, let\u2019s look at the detailed operation of the DQN. First, the network is initialized.", "Execute a few actions with the environment to bootstrap the replay data.", "Initialize the Q Network with random weights and copy them to the Target Network.", "Starting with the first time step, the Experience Replay starts the training data generation phase and uses the Q Network to select an \u03b5-greedy action. The Q Network acts as the agent while interacting with the environment to generate a training sample. No DQN training happens during this phase.", "The Q Network predicts the Q-values of all actions that can be taken from the current state. We use those Q-values to select an \u03b5-greedy action.", "Experience Replay executes the \u03b5-greedy action and receives the next state and reward.", "It stores the results in the replay data. Each such result is a sample observation which will later be used as training data.", "We now start the phase to train the DQN. Select a training batch of random samples from the replay data as input for both networks.", "To simplify the explanation, let\u2019s follow a single sample from the batch. The Q network predicts Q values for all actions that can be taken from the state.", "From the output Q values, select the one for the sample action. This is the Predicted Q Value.", "The next state from the sample is input to the Target network. The Target network predicts Q values for all actions that can be taken from the next state, and selects the maximum of those Q values.", "Use the next state as input to predict the Q values for all actions. The target network selects the max of all those Q-values.", "The Target Q Value is the output of the Target Network plus the reward from the sample.", "Compute the Mean Squared Error loss using the difference between the Target Q Value and the Predicted Q Value.", "Back-propagate the loss and update the weights of the Q Network using gradient descent. The Target network is not trained and remains fixed, so no Loss is computed, and back-propagation is not done. This completes the processing for this time-step.", "The Target network is not trained so no Loss is computed, and back-propagation is not done.", "The processing repeats for the next time-step. The Q network weights have been updated but not the Target network\u2019s. This allows the Q network to learn to predict more accurate Q values, while the target Q values remain fixed for a while, so we are not chasing a moving target.", "After T time-steps, copy the Q network weights to the Target network. This lets the Target network get the improved weights so that it can also predict more accurate Q values. Processing continues as before.", "The Q network weights and the Target network are again equal.", "In the previous article, we had seen that Q-Learning used the Target Q Value, the Current Q Value, and observed reward to update the Current Q Value using its update equation.", "The DQN works in a similar way. Since it is a neural network, it uses a Loss function rather than an equation. It also uses the Predicted (ie. Current) Q Value, Target Q Value, and observed reward to compute the Loss to train the network, and thus improve its predictions.", "In the next article, we will continue our Deep Reinforcement Learning journey, and look at another popular algorithm using Policy Gradients.", "And finally, if you liked this article, you might also enjoy my other series on Transformers as well as Audio Deep Learning.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5a5317197f4b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47----5a5317197f4b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a5317197f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a5317197f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@joshriemer?utm_source=medium&utm_medium=referral", "anchor_text": "Josh Riemer"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060", "anchor_text": "Intro to Basic Concepts and Terminology"}, {"url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-2-solution-approaches-7e37cbf2334e", "anchor_text": "Solution Approaches"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-3-model-free-solutions-step-by-step-c4bbb2b72dcf", "anchor_text": "Model-free algorithms"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e", "anchor_text": "Q-Learning"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-6-policy-gradients-step-by-step-f9f448e73754", "anchor_text": "Policy Gradient"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e", "anchor_text": "fourth one on Q-Learning"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Transformers Explained Visually (Part 1): Overview of FunctionalityA Gentle Guide to Transformers for NLP, and why they are better than RNNs, in Plain English. How Attention helps\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504", "anchor_text": "Audio Deep Learning Made Simple (Part 1): State-of-the-Art TechniquesA Gentle Guide to the world of disruptive deep learning audio applications and architectures. And why we all need to\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5a5317197f4b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5a5317197f4b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5a5317197f4b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5a5317197f4b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5a5317197f4b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a5317197f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----5a5317197f4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a5317197f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----5a5317197f4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a5317197f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5a5317197f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5a5317197f4b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5a5317197f4b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5a5317197f4b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5a5317197f4b--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://ketanhdoshi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fae94feabe1c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b&newsletterV3=54f9ca55ed47&newsletterV3Id=ae94feabe1c9&user=Ketan+Doshi&userId=54f9ca55ed47&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}