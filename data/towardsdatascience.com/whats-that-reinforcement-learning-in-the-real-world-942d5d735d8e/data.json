{"url": "https://towardsdatascience.com/whats-that-reinforcement-learning-in-the-real-world-942d5d735d8e", "time": 1683011767.8143911, "path": "towardsdatascience.com/whats-that-reinforcement-learning-in-the-real-world-942d5d735d8e/", "webpage": {"metadata": {"title": "\u201cWhat\u2019s that? Reinforcement Learning in the Real-world?\u201d | by mugoh mwaura | Towards Data Science", "h1": "\u201cWhat\u2019s that? Reinforcement Learning in the Real-world?\u201d", "description": "Reinforcement Learning offers a distinctive way of solving the Machine Learning puzzle. It\u2019s sequential decision-making ability, and suitability to tasks requiring a trade-off between immediate and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://sites.google.com/view/RL4RealLife2019", "anchor_text": "others", "paragraph_index": 1}, {"url": "https://blog.openai.com/faulty-reward-functions/", "anchor_text": "learning the laziest possible behaviour", "paragraph_index": 3}, {"url": "http://alexirpan.com/", "anchor_text": "Irpan", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Imitation", "anchor_text": "imitation learning", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Apprenticeship_learning", "anchor_text": "Inverse RL", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Processes", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Markov_property", "anchor_text": "Markov Property", "paragraph_index": 15}, {"url": "https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html", "anchor_text": "Meta-Learning", "paragraph_index": 17}, {"url": "https://openai.com/blog/generalizing-from-simulation/", "anchor_text": "proven invaluable at adding policy robustness", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1802.03236", "anchor_text": "Robust MDP formulation", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1804.02477", "anchor_text": "Programmatically Interpretable Reinforcement Learning", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "non-linear", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Lasso_%28statistics%29", "anchor_text": "l1 regularization,", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Multi-armed_bandit", "anchor_text": "contextual bandit", "paragraph_index": 26}, {"url": "http://arxiv.org/abs/1904.12901", "anchor_text": "Challenges of Real-World Reinforcement Learning", "paragraph_index": 35}], "all_paragraphs": ["Reinforcement Learning offers a distinctive way of solving the Machine Learning puzzle. It\u2019s sequential decision-making ability, and suitability to tasks requiring a trade-off between immediate and long-term returns are some components that make it desirable in settings where supervised-learning or unsupervised learning approaches would, in comparison, not fit as well.", "By having agents start with zero knowledge then learn qualitatively good behaviour through interaction with the environment, it\u2019s almost fair to say Reinforcement Learning (RL) is the closest thing we have to Artificial General Intelligence yet. We can see RL being used in robotics control, treatment design in healthcare, among others; but why aren\u2019t we boasting of many RL agents being scaled up to real-world production systems?", "There\u2019s a reason why games, like Atari, are such nice RL benchmarks \u2014 they let us care only about maximizing the score and not worry about designing a reward function. An ideal reward function would both encourage the agent to behave the way we want it to while still being learnable.", "We may choose to reward the agent only at goal state and nowhere else. In such a maze-solving task, for instance, the agent would get a +1 reward only by finding the target room. Such is a sparse reward. It specifies the desired objective simply and clearly but is also almost not learnable because the agent receives no feedback on how close it is to the desired goal. A customary solution to this is reshaping the function to provide increasing rewards as the agent gets closer to the target. However, the policy might misinterpret such reward formulations and end up learning the laziest possible behaviour that fits local optima instead of what we wish it should learn.", "This makes reward functions notoriously odd to design.", "A decent example is from Irpan, where a robot arm was trained to reach a target on a tabletop. The lovely robot, however, chose to learn to smash down the table, which brought the target-point towards to it.", "Notably, a reward function may be optimizing a single metric, only for others equally significant to be discovered during evaluation of the learned behaviour. Many systems need a multi-dimensional global reward function optimizing many smaller goals. For success, this needs incorporation of multiple system-metrics in the reward function, which might be challenging to attain, or worse, some metrics remain unknown to us. There hasn\u2019t been much done on multi-objective reward functions specific to RL though.", "In a real-world setting, the reward function must condition the policy such that it performs as expected on all instances of the task objective \u2014 not just on a subset of them. If a robot is handing out cookies and a smile to customers, it should do so for every customer, not solely on a mean figure of customers as it pleases.", "Instead of hard-coding a reward function that improves the system-policy, imitation learning trains a policy using demonstrations of an expert \u2014 it\u2019s essentially supervised learning. The system learns to copy the exact behaviour of an expert without caring what the expert was trying to achieve.", "Inverse RL improves the imitation technique by attempting to infer the intention of the expert from demonstrated behaviour, rather than copying even non-optimal actions the expert might have taken unintentionally. It learns a possible reward function that fits the expert demonstrations.", "Safety is one good reason we don\u2019t release wheeled machines on our roads and ask them to learn by making mistakes. It\u2019s about ensuring an autonomous system poses a non-critical risk of irretrievably wrecking itself or being a hazard to human life.", "An existing approach to safe RL is specifying a safety constraint on the policy. The safety constraint can be on any behaviour considered potentially unsafe, such as avoiding high-speed collision during navigation. A solution optimizing for safety in this manner is known as a Constrained Markov Decision Processes (MDP). The constraint can be fixed and predetermined or fluid to allow for the trade-off between maximizing the expected return and satisfying the minimum safety threshold. Similarly, by using a Probabilistic objective MDP, we can opt to motivate the agent to take risky actions for imminent nobler rewards within a selected number of steps, or to focus on keeping safe.", "The issue with a Constrained MDP is that to learn what keeping safe entails, the system must gain experience by taking actions resulting in unsafe consequences. Such risky behaviour would typically be encouraged during training to produce a policy that generalises to avoiding them in a real-world use case.", "Encouraging unsafe behaviour during training can involve learning an approximator that predicts the safety of being in a nearby state, which can then be used to aid the agent\u2019s exploration strategy.", "High dimensional inputs require patience in being processed, and real-world systems use multiple sensors and images to get an accurate representation of the state. It, therefore, takes time before the observed state reaches the agent. Sending a selected action to the actuators will have a dose of communication delay as well. The desired next state will not be arrived at immediately on action execution either because moving system parts take time to adjust \u2014 rotating a robotic arm from 45 to 100 degrees, relative to something, won\u2019t be achieved in an instant.", "All these mean that the agent \u03c0(a\u209c|s\u209c) might be taking actions conditioned on states from the past, instead of the current, time step t. This would violate the Markov Property requiring the next state s\u209c \u208a\u2081 to be conditionally dependent on the current state and action pair (s\u209c, a\u209c) but independent of previous states and actions. The Markov Property is the foundation of the RL decision making process p(s\u209c \u208a\u2081| s\u209c, \u03c0(a\u209c|s\u209c)), so violating it is a big deal.", "Adding memory to the agent can be used to optimize for two factors:", "Real systems will often experience wear and tear at movable joints or partial clogging of vents, of which, if the agent is unaware, results in an unobserved-change in the learned environment dynamics. For this type of problem, we can use a non-constant reward function \u2014 varying with time \u2014 to model the non-stationarity. It\u2019s also decent trying finding a policy that best adapts to changes introduced by non-stationarity through Meta-Learning.", "A further case arises in model-based RL where agents seeking intrinsic curiosity (rewarded only for exploration in new/novel states) would need a stationary function of the observations to ensure stability during on-policy model training. However, such an agent is deemed to experience non-stationary rewards as what is considered a new state will be old and non-rewarding in future time-steps. Since the dynamics model learns new features that correct past mistakes as it evolves, it\u2019s also an added source of non-stationarity.", "We can mitigate this by randomly initializing and fixing features of a convolution network as the observation function, at the expense of sufficient representation the state, or learn from the image pixels which requires no feature learning.", "Non-stationarity is one source of stochatisticy that may be experienced in a real system \u2014 it\u2019s also bound to have noise from sensor readings and action executions. Hence the need to ensure the agent is robust to different subsets of these issues. Successful Sim2real transfer leans much on the robustness of the learned policy to under-modelled features of the real-world system.", "Domain randomization, which has proven invaluable at adding policy robustness, introduces disturbances to the agent\u2019s environment and averages the errors learned during training. Additionally, training a policy that can identify its current environment and tweak itself as best fits the situation (Meta-Learning) would mitigate the effects of environmental noise.", "The Robust MDP formulation concentrates on improving the worst-case value function, which then yields an agent that can withstand various environmental disturbances. At each timestep, the agent uses a separate transition function (s\u2019| s, a) to minimize the prediction value in the long-term.", "Quite little has been done on interpretable RL. In real systems, we need clear assurance on what the agent\u2019s intentions will be and confidence in our ability to point out the origins of potential failure cases. Since the policy might create a new, unexpected solution to a system problem, proper awareness of its short and long-term intent is crucial.", "An exciting approach \u2014 Programmatically Interpretable Reinforcement Learning \u2014 attempts to distil a non-linear policy into a human-verifiable, explicitly coded system program.", "The curse of dimensionality follows everywhere. For example, robotic systems have continuous action spaces in their many degrees of freedom on the joint angles and velocity associated for each degree. Unlike supervised and unsupervised settings where some regression methods, like l1 regularization, give a reduced input feature space, this is more difficult to achieve in RL since we need to collect data throughout the state-action-space to achieve better convergence on our global policy. Additionally, such regression tricks in (Un)Supervised learning aim to increase the variance of the model. But in RL, we have a bellyful of variance already, and the concern is finding ways to reduce it.", "Lower input-dimensionality has been achieved by reducing the number of candidate-actions. One such way is eliminating actions seeming irrelevant. Though this might tempt us to learn another function approximator, it would whelm the parameter space more. Action elimination has been studied by framing unhelpful actions as part of an MDP problem and using a contextual bandit to filter them out. Additionally, given a vector of candidate-actions, performing a nearest-neighbour search on the closest action can lower the dimensionality of discrete action spaces.", "For high-dimensional states, a more compact state representation can be achieved by employing inverse dynamics. This predicts the action given next and previous states (a\u209c| s\u209c\u208b\u2081, s\u209c\u208a\u2081). The intuition used is that features of the observation space learned are those under the agent\u2019s immediate control, and those ignored are environmental aspects the agent holds no direct influence on.", "Off-policy learning allows us to train the agent using trajectories (s\u2081, a\u2081, s\u2082, a\u2082, \u2026) under a separate policy from the one the agent is running on. In real systems, as the agent collects new data, it will be used to improve the current policy performance. Since such an improvement might go wrong, which would be critical in production, the new policy needs to be evaluated off-policy first before being deployed.", "Off-policy evaluation poses the problem of evaluating the probabilities under a distribution while using samples from a related, but different, distribution. Importance sampling is widely used to alleviate this distribution mismatch. We can also learn a dynamics model of the system, and evaluate the new policy using the model transition predictions.", "A simulated environment gives unlimited training data and zero-worry of effects from bad agent-actions. All training data in a real-system comes from the system itself, but not for free.", "Producing RL data in real-world systems is expensive \u2014 the agent learns by acting multiple times, which might result in system-wear before it learns to behave as intended. So the agent must observe safety, severely limiting exploration, while behaving reasonably well.", "Running on real-time is also slow and cannot be sped up, unlike in simulators. Reward horizons can be as long as months, e.g. in healthcare treatment policies, limiting the frequency at which the agent can correct mistakes.", "Model-based RL can give the advantage of sample efficiency by training on synthetic samples. This can be helpful if the model is a close-enough representation of the real system. For the initial policy, instead of random initialization, using expert demonstrations as training transitions will help reduce long-term sample complexity. Additionally, Meta-learning can enable a learned-policy to adapt to an unseen task from the training distribution, reducing the number of samples needed to train policies for multiple similar objectives.", "That\u2019s an overview of the known central challenges slowing real-world applications of Reinforcement Learning. The solutions highlighted are by no means complete fixes, or exhaustive, but have the aim of showing the different perspectives which have been chosen to approach the same problem and progress with time.", "[2] Dulac-Arnold G., Mankowitz D. Hester T., Challenges of Real-World Reinforcement Learning, ICML, 2019.", "Fascinated by bread | Learning to smile | RL & Decision control"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F942d5d735d8e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@mugoh?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "mugoh mwaura"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----942d5d735d8e---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F942d5d735d8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----942d5d735d8e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F942d5d735d8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&source=-----942d5d735d8e---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/photos/XRcEsQKTWGk", "anchor_text": "Torsten Dederichs"}, {"url": "https://unsplash.com/photos/XRcEsQKTWGk", "anchor_text": "Unsplash"}, {"url": "https://sites.google.com/view/RL4RealLife2019", "anchor_text": "others"}, {"url": "https://blog.openai.com/faulty-reward-functions/", "anchor_text": "learning the laziest possible behaviour"}, {"url": "http://alexirpan.com/", "anchor_text": "Irpan"}, {"url": "https://en.wikipedia.org/wiki/Imitation", "anchor_text": "imitation learning"}, {"url": "https://en.wikipedia.org/wiki/Apprenticeship_learning", "anchor_text": "Inverse RL"}, {"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Processes"}, {"url": "https://en.wikipedia.org/wiki/Markov_property", "anchor_text": "Markov Property"}, {"url": "https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html", "anchor_text": "Meta-Learning"}, {"url": "https://openai.com/blog/generalizing-from-simulation/", "anchor_text": "proven invaluable at adding policy robustness"}, {"url": "https://arxiv.org/abs/1802.03236", "anchor_text": "Robust MDP formulation"}, {"url": "https://unsplash.com/@hgudka97", "anchor_text": "Harshil Gudka"}, {"url": "https://unsplash.com/photos/0prglfrYY08", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1804.02477", "anchor_text": "Programmatically Interpretable Reinforcement Learning"}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "non-linear"}, {"url": "https://en.wikipedia.org/wiki/Lasso_%28statistics%29", "anchor_text": "l1 regularization,"}, {"url": "https://en.wikipedia.org/wiki/Multi-armed_bandit", "anchor_text": "contextual bandit"}, {"url": "https://journals.sagepub.com/doi/abs/10.1177/0278364913495721", "anchor_text": "Reinforcement Learning in Robotics: A Survey"}, {"url": "http://arxiv.org/abs/1904.12901", "anchor_text": "Challenges of Real-World Reinforcement Learning"}, {"url": "https://arxiv.org/abs/1808.0355", "anchor_text": "Large Scale Study Curiosity-Driven Learning"}, {"url": "https://www.alexirpan.com/2018/02/14/rl-hard.html", "anchor_text": "https://www.alexirpan.com/2018/02/14/rl-hard.html"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----942d5d735d8e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----942d5d735d8e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----942d5d735d8e---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F942d5d735d8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----942d5d735d8e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F942d5d735d8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----942d5d735d8e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F942d5d735d8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----942d5d735d8e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1bcb39f5cedf%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----942d5d735d8e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Written by mugoh mwaura"}, {"url": "https://medium.com/@mugoh/followers?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "43 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----942d5d735d8e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1bcb39f5cedf%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-that-reinforcement-learning-in-the-real-world-942d5d735d8e&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----942d5d735d8e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----942d5d735d8e----0---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----942d5d735d8e----0---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----942d5d735d8e----0---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "mugoh mwaura"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----942d5d735d8e----0---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----942d5d735d8e----0---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "So Model Predictive Control is this Enticing for Model-Based RL?A colored view of how Model Predictive Control Completes any Model-Based Reinforcement Learning Picture"}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----942d5d735d8e----0---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "\u00b79 min read\u00b7Jun 14, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe69bb5255ce9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----e69bb5255ce9----0-----------------clap_footer----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----942d5d735d8e----0---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe69bb5255ce9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9&source=-----942d5d735d8e----0-----------------bookmark_preview----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----942d5d735d8e----1---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----942d5d735d8e----1---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----942d5d735d8e----1---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----942d5d735d8e----1---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----942d5d735d8e----1---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----942d5d735d8e----1---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----942d5d735d8e----1---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----942d5d735d8e----1-----------------bookmark_preview----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----942d5d735d8e----2---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----942d5d735d8e----2---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----942d5d735d8e----2---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----942d5d735d8e----2---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----942d5d735d8e----2---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----942d5d735d8e----2---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----942d5d735d8e----2---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----942d5d735d8e----2-----------------bookmark_preview----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----942d5d735d8e----3---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----942d5d735d8e----3---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----942d5d735d8e----3---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "mugoh mwaura"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----942d5d735d8e----3---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----942d5d735d8e----3---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "Uncertainty Aware Reinforcement LearningA peek into building agents that know what they are doing"}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----942d5d735d8e----3---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": "\u00b714 min read\u00b7Jul 5, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c95c25c220d3----3-----------------clap_footer----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----942d5d735d8e----3---------------------f1873d26_525c_4b79_9c17_fbc589e13baa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=-----942d5d735d8e----3-----------------bookmark_preview----f1873d26_525c_4b79_9c17_fbc589e13baa-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "See all from mugoh mwaura"}, {"url": "https://towardsdatascience.com/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----942d5d735d8e----0-----------------bookmark_preview----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----942d5d735d8e----1-----------------bookmark_preview----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----942d5d735d8e----0---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----942d5d735d8e----0-----------------bookmark_preview----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----1-----------------clap_footer----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----942d5d735d8e----1---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----942d5d735d8e----1-----------------bookmark_preview----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----942d5d735d8e----2---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----942d5d735d8e----2---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----942d5d735d8e----2---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----942d5d735d8e----2---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----942d5d735d8e----2---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----942d5d735d8e----2---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----2-----------------clap_footer----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----942d5d735d8e----2---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----942d5d735d8e----2-----------------bookmark_preview----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----942d5d735d8e----3---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----942d5d735d8e----3---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----942d5d735d8e----3---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Piotr Krosniak"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----942d5d735d8e----3---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "Vaccine Supply Chain Optimization with AI-Powered Capacitated Vehicle Routing Problem(CVRP)- Part 1The world is facing a global health crisis, and one of the most important challenges is to ensure an efficient and timely distribution of\u2026"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----942d5d735d8e----3---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": "6 min read\u00b7Jan 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&user=Piotr+Krosniak&userId=b791abcfafd5&source=-----ca79519e9ad7----3-----------------clap_footer----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----942d5d735d8e----3---------------------3a393f2d_a596_4b96_a045_03fec0c076bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&source=-----942d5d735d8e----3-----------------bookmark_preview----3a393f2d_a596_4b96_a045_03fec0c076bc-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----942d5d735d8e--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}