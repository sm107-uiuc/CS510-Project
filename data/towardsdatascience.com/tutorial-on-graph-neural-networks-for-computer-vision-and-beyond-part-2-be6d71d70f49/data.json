{"url": "https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49", "time": 1682997656.845698, "path": "towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49/", "webpage": {"metadata": {"title": "Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs | by Boris Knyazev | Towards Data Science", "h1": "Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs", "description": "Explanation of how spectral, multiscale, dynamic and anisotropic convolution works for graphs compared to images, comparing different graph neural networks."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial", "paragraph_index": 0}, {"url": "https://ieeexplore.ieee.org/document/572108", "anchor_text": "Alessandro Sperduti and Antonina Starita on \u201cSupervised Neural Networks for the Classification of Structures\u201d", "paragraph_index": 4}, {"url": "https://ieeexplore.ieee.org/document/572108", "anchor_text": "Sperduti & Starita, 1997", "paragraph_index": 5}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf & Welling, ICLR, 2017", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1611.08097", "anchor_text": "Bronstein et al.\u2019s review", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1810.00826", "anchor_text": "Xu et al., ICLR, 2019", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801", "anchor_text": "another post", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "convolution theorem", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Laplacian_matrix", "anchor_text": "graph Laplacian", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "spectral convolution of signals on regular grids", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al.", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1606.09375", "anchor_text": "Defferrard et al.", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1811.09595", "anchor_text": "Knyazev et al., NeurIPS-W, 2018", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers", "anchor_text": "the power property", "paragraph_index": 25}, {"url": "https://github.com/bknyaz/examples/blob/master/splines_cheb.py", "anchor_text": "my github repo", "paragraph_index": 31}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al.", "paragraph_index": 32}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf & Welling", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Identity_matrix", "anchor_text": "identity matrix", "paragraph_index": 33}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial", "paragraph_index": 36}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis, CVPR, 2017", "paragraph_index": 37}, {"url": "https://arxiv.org/abs/1611.08402", "anchor_text": "Monti et al., CVPR, 2017", "paragraph_index": 37}, {"url": "https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss", "anchor_text": "Cross Entropy", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1605.09673", "anchor_text": "Brabandere et al., NIPS, 2016", "paragraph_index": 38}, {"url": "https://en.wikipedia.org/wiki/No_free_lunch_theorem", "anchor_text": "free lunch", "paragraph_index": 39}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1907.09000", "anchor_text": "BMVC paper", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1611.08402", "anchor_text": "Monti et al.", "paragraph_index": 43}, {"url": "https://scikit-learn.org/stable/modules/mixture.html", "anchor_text": "Gaussian Mixture Model", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1907.09000", "anchor_text": "BMVC paper", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1811.09595", "anchor_text": "Knyazev et al., NeurIPS-W, 2018", "paragraph_index": 43}, {"url": "https://github.com/rusty1s/pytorch_geometric", "anchor_text": "PyTorch Geometric (PyG)", "paragraph_index": 44}, {"url": "https://medium.com/u/6cf41cb2c546?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Mohamed Amer", "paragraph_index": 45}, {"url": "https://mohamedramer.com/", "anchor_text": "homepage", "paragraph_index": 45}, {"url": "https://www.gwtaylor.ca/", "anchor_text": "homepage", "paragraph_index": 45}, {"url": "https://www.linkedin.com/in/carolynaugusta/", "anchor_text": "Carolyn Augusta", "paragraph_index": 45}, {"url": "https://github.com/bknyaz/", "anchor_text": "Github", "paragraph_index": 46}, {"url": "https://www.linkedin.com/in/boris-knyazev-39690948/", "anchor_text": "LinkedIn", "paragraph_index": 46}, {"url": "https://twitter.com/BorisAKnyazev", "anchor_text": "Twitter", "paragraph_index": 46}, {"url": "https://bknyaz.github.io/", "anchor_text": "My homepage", "paragraph_index": 46}, {"url": "http://twitter.com/misc", "anchor_text": "@misc", "paragraph_index": 47}, {"url": "http://bknyaz.github.io/", "anchor_text": "http://bknyaz.github.io/", "paragraph_index": 49}], "all_paragraphs": ["I\u2019m presenting an overview of important Graph Neural Network works, by distilling key ideas and explaining simple intuition behind milestone methods using Python and PyTorch. This post continues the first part of my tutorial.", "In the \u201cGraph of Graph Neural Network (GNN) and related works\u201d above, I added papers on graphs that I have come across in the last year. In this graph, a directed edge between two works denotes that one paper is based on the other (while not necessary citing it) and a color of the work denotes:", "Note, that some other important works and edges are not shown to avoid further clutter, and only a tiny fraction of works, highlighted in bold boxes, will be covered in this post. Disclaimer: I still found room to squeeze our own recent works there \ud83d\ude0a.", "Most of the important methods are covered in this non-exhaustive list of works:", "The first work where graphs were classified using a neural network seems to be a 1997 paper by Alessandro Sperduti and Antonina Starita on \u201cSupervised Neural Networks for the Classification of Structures\u201d.", "Sperduti & Starita, 1997: \u201cUntil now neural networks have been used for classifying unstructured patterns and sequences. However, standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach.\u201d", "From 1997, the body of works on learning from graphs has grown so much and in so many diverse directions that it is very hard to keep track without some smart automated system. I believe we are converging to using methods based on neural networks (based on our formula (2) explained in the first part of my tutorial), or some combination of neural networks and other methods.", "To recap the notation we used in the first part, we have some undirected graph G with N nodes. Each node in this graph has a C-dimensional feature vector, and features of all nodes are represented as an N\u00d7C dimensional matrix X\u207d\u02e1\u207e. In a typical graph network, such as GCN (Kipf & Welling, ICLR, 2017), we feed these features X\u207d\u02e1\u207e to a graph neural layer with C\u00d7F dimensional trainable weights W\u207d\u02e1\u207e , so that the output of this layer is an N\u00d7F matrix X\u207d\u02e1\u207a\u00b9\u207e encoding updated (and hopefully better in some sense) node features. \ud835\udcd0 is an N\u00d7N matrix, where the entry \ud835\udcd0\u1d62\u2c7c indicates if node i is connected (adjacent) to node j. This matrix is called an adjacency matrix. I use \ud835\udcd0 instead of plain A to highlight that this matrix can be normalized in a way to facilitate feature propagation in a deep network. For the purpose of this tutorial, we can assume that \ud835\udcd0=A, i.e. each i-th row of the matrix product \ud835\udcd0X\u207d\u02e1\u207e will contain a sum of features of node i neighbors.", "In the rest of this part of the tutorial, I\u2019ll briefly explain works of my choice showed in bold boxes in the overview graph. I recommend Bronstein et al.\u2019s review for a more comprehensive and formal analysis.", "Note that even though I dive into some technical details of spectral graph convolution below, many recent works (e.g., GIN in Xu et al., ICLR, 2019) are built without spectral convolution and show great results in some tasks. However, knowing how spectral convolution works is still helpful to understand and avoid potential problems with other methods.", "I explain spectral graph convolution in detail in my another post.", "I\u2019ll briefly summarize it here for the purpose of this part of the tutorial. A formal definition of spectral graph convolution, which is very similar to the convolution theorem in signal/image processing, can be written as:", "where V are eigenvectors and \u039b are eigenvalues of the graph Laplacian L, which can be found by eigen-decomposition: L=V\u039bV\u1d40; W_spectral are filters. Throughout this tutorial I\u2019m going to assume \u201csymmetric normalized Laplacian\u201d. It is computed based only on an adjacency matrix A of a graph, which can be done in a few lines of Python code as follows:", "Here, we assume that A is symmetric, i.e. A = A\u1d40 and our graph is undirected, otherwise node degrees are not well-defined and some assumptions must be made to compute the Laplacian. In the context of computer vision and machine learning, the graph Laplacian defines how node features will be updated if we stack several graph neural layers in the form of formula (2).", "So, given graph Laplacian L, node features X and filters W_spectral, in Python spectral convolution on graphs looks very simple:", "where we assume that our node features X\u207d\u02e1\u207e are 1-dimensional, e.g. MNIST pixels, but it can be extended to a C-dimensional case: we will just need to repeat this convolution for each channel and then sum over C as in signal/image convolution.", "Formula (3) is essentially the same as spectral convolution of signals on regular grids using the Fourier Transform, and so creates a few problems for machine learning:", "These issues prevent scaling to datasets with large graphs of variable structure.", "To solve the first issue, Bruna et al. proposed to smooth filters in the spectral domain, which makes them more local in the spatial domain according to the spectral theory. The idea is that you can represent our filter W_spectral from formula (3) as a sum of \ud835\udc3e predefined functions, such as splines, and instead of learning N values of W, we learn K coefficients \u03b1 of this sum:", "While the dimensionality of fk does depend on the number of nodes N, these functions are fixed, so we don\u2019t learn them. The only thing we learn are coefficients \u03b1, and so W_spectral is no longer dependent on N. To make our approximation in formula (4) reasonable, we want K<<N to reduce the number of trainable parameters from N to K and, more importantly, make it independent of N, so that our GNN can digest graphs of any size.", "While solves the first issue, this smoothing method does not address the second issue.", "The main drawback of spectral convolution and its smooth version above is that it still requires eigen-decomposition of an N\u00d7N dimensional graph Laplacian L, which creates two main problems:", "Now, what does Chebyshev graph convolution have to do with all that?", "It turns out that it solves both problems at the same time! \ud83d\ude03", "That is, it avoids computing costly eigen-decomposition and the filters are no longer \u201cattached\u201d to eigenvectors (yet they still are functions of eigenvalues \u039b). Moreover, it has a very useful parameter, usually denoted as K having a similar intuition as K in our formula (4) above, determining the locality of filters. Informally: for K=1, we feed just node features X\u207d\u02e1\u207e to our GNN; for K=2, we feed X\u207d\u02e1\u207e and \ud835\udcd0X\u207d\u02e1\u207e; for K=3, we feed X\u207d\u02e1\u207e, \ud835\udcd0X\u207d\u02e1\u207e and \ud835\udcd0\u00b2X\u207d\u02e1\u207e; and so forth for larger K (I hope you\u2019ve noticed the pattern). See more accurate and formal definition in Defferrard et al. and my code below, plus additional analysis is given in (Knyazev et al., NeurIPS-W, 2018).", "Due to the power property of adjacency matrices, when we perform \ud835\udcd0\u00b2X\u207d\u02e1\u207e we actually average (or sum depending on how \ud835\udcd0 is normalized) over 2-hop neighbors, and analogously for any n in \ud835\udcd0\u207fX\u207d\u02e1\u207e as illustrated below, where we average over n-hop neighbors.", "Note that to satisfy the orthogonality of the Chebyshev basis, \ud835\udcd0 assumes no loops in the graph, so that in each i-th row of matrix product \ud835\udcd0X\u207d\u02e1\u207e we will have features of the neighbors of node i, but not the features of node i itself. Features of node i will be fed separately as a matrix X\u207d\u02e1\u207e.", "If K equals the number of nodes N, the Chebyshev convolution closely approximates a spectral convolution, so that the receptive field of filters will be the entire graph. But, as in the case of convolutional networks, we don\u2019t want our filters to be as big as the input images for a number of reasons that I already discussed, so in practice, K takes reasonably small values.", "In my experience, this is one of the most powerful GNNs, achieving great results in a very wide range of graph tasks. The main downside is the necessity to loop over K in the forward/backward pass (since Chebyshev polynomials are recursive, so it\u2019s not possible to parallelize them), which slows down the model.", "Same as with Splines discussed above, instead of training filters, we train coefficients, but this time, of the Chebyshev polynomial.", "To generate the Chebyshev basis, you can use the following Python code:", "The full code to generate spline and Chebyshev bases is in my github repo.", "To illustrate how a Chebyshev filter can look on a irregular grid, I follow the experiment from Bruna et al. again and sample 400 random points from the MNIST grid in the same way as I did to show eigenvectors of the graph Laplacian. I trained a Chebyshev graph convolution model on the MNIST images sampled from these 400 locations (same irregular grid is used for all images) and one of the filter for K=1 and K=20 is visualized below.", "As you may have noticed, if you increase K of the Chebyshev convolution, it increases the total number of trainable parameters. For example, for K=2, our weights W\u207d\u02e1\u207e will be 2C\u00d7F instead of just C\u00d7F. This is because we concatenate features X\u207d\u02e1\u207e and \ud835\udcd0X\u207d\u02e1\u207e into a single N\u00d72C matrix. More training parameters means the model is more difficult to train and more data must be labeled for training. Graph datasets are often extremely small. Whereas in computer vision, MNIST is considered a tiny dataset, because images are just 28\u00d728 dimensional and there are only 60k training images, in terms of graph networks MNIST is quite large, because each graph would have N=784 nodes and 60k is a large number of training graphs. In contrast to computer vision tasks, many graph datasets have only around 20\u2013100 nodes and 200\u20131000 training examples. These graphs can represent certain small molecules and labeling chemical/biological data is usually more expensive than labeling images. Therefore, training Chebyshev convolution models can lead to severe overfitting of the training set (i.e. the model will have the training loss close to 0 yet will have a large validation or test error). So, GCN of Kipf & Welling essentially \u201cmerged\u201d matrices of node features X\u207d\u02e1\u207e and \ud835\udcd0X\u207d\u02e1\u207e into a single N\u00d7C matrix. As a result, the model has two times fewer parameters to train compared to Chebyshev convolution with K=2, yet has the same receptive field of 1 hop. The main trick involves adding \u201cself-loops\u201d to your graph by adding an identity matrix I to \ud835\udcd0 and normalizing it in a particular way, so now in each i-th row of matrix product \ud835\udcd0X\u207d\u02e1\u207e we will have features of the neighbors of node i, as well as features of node i.", "This model seems to be a standard baseline choice well-suited for many application due to its lightweight, good performance and scalability to larger graphs.", "The difference between GCN and Chebyshev convolution is illustrated below.", "The code above follows the same structure as in the first part of my tutorial, where I compared classical NN and GNN. One of the main steps both in GCN and Chebyshev convolution is computation of the rescaled graph Laplacian L. This rescaling is done to make eigenvalues in the range [-1,1] to facilitate training (this might be not a very important step in practice as weights can adapt during training). In GCN, self-loops are added to the graph by adding an identity matrix before computing the Laplacian as discussed above. The main difference between the two methods is that in the Chebyshev convolution we recursively loop over K to capture features in the K-hop neighborhood. We can stack such GCN or Chebyshev layers interleaved with nonlinearities to build a Graph Neural Network.", "Now, let me politely interrupt \ud83d\ude03 our spectral discussion and give a general idea behind two other exciting methods: Edge-conditioned filters by Simonovsky & Komodakis, CVPR, 2017 and MoNet by Monti et al., CVPR, 2017, which share some similar concepts.", "As you know, in ConvNets we learn the weights (filters) by optimizing some loss like Cross Entropy. In the same way, we learn our W\u207d\u02e1\u207e in GNNs. Imagine that instead of learning these weights, you have another network that predicts them. So during training, we learn the weights of that auxiliary network, which takes an image or a graph as an input and returns weights W\u207d\u02e1\u207e (\u0398 in their work) as the output. The idea is based on Dynamic Filter Networks (Brabandere et al., NIPS, 2016), where \u201cdynamic\u201d means that filters W\u207d\u02e1\u207e will be different depending on the input as opposed to standard models in which filters are fixed (or static) after training.", "This is a very general form of convolution that, besides images, can be easily applied to graphs or point clouds as they did in their CVPR paper and got excellent results. However, there is no \u201cfree lunch\u201d, and training such models is quite challenging, because the regular grid constraint is now relaxed and the scope of solutions increases dramatically. This is especially true for larger graphs with many edges or for convolution in deeper layers, which often have hundreds of channels (number of features, C), so you might end up generating thousands of numbers in total for each input! In this regard, standard ConvNets are so good, because we don\u2019t waste the model\u2019s capacity on training to predict these weights, instead we directly enforce that the filters should be the same for all inputs. But this prior makes ConvNets limited and we cannot directly apply them to graphs or point clouds. So, as always, there\u2019s some trade-off between flexibility and performance in a particular task.", "When applied to images, like MNIST, the Edge-conditioned model can learn to predict anisotropic filters \u2014 filters that are sensitive to orientation, such as edge detectors. Compared to Gaussian filters discussed in the first part of my tutorial, these filters are able to better capture certain patterns in images, such as strokes in digits.", "I want to highlight one more time that whenever we have a complicated model with auxiliary networks, it becomes a chicken-or-the-egg problem in some sense. To solve it, one of the networks \u2014 the auxiliary or the main one \u2014 should receive a very strong signal, so that it can implicitly supervise another network. In our BMVC paper, which is similar to Simonovsky & Komodakis\u2019s work, we apply additional constraints on the edge-generating network to facilitate training. I will describe our work in detail in later posts.", "MoNet is different from other works discussed in this post, as it assumes to have the notion of node coordinates, and therefore is more suited for geometric tasks such as 3D mesh analysis or image/video reasoning. It is somewhat similar to edge-conditioned filters of Simonovsky & Komodakis, since they also introduce an auxiliary learnable function \ud835\udc37(\ud835\udc64, \ud835\udf03, \u03c1) that predicts weights. The difference is that these weights depend on the node polar coordinates (angle \ud835\udf03 and radius \u03c1); and trainable parameters \ud835\udc64 of that function are constrained to be means and variances of Gaussians, so that instead of learning N\u00d7N matrices, we only learn fixed-size vectors (means and variances) independently of the graph size N. In terms of standard ConvNets, it would be the same as learning only 2 values (the mean and variance of a Gaussian) for each filter instead of learning 9, 25 or 121 values for 3\u00d73, 5\u00d75 or 11\u00d711 dimensional filters respectively. This parameterization would greatly reduce the number of parameters in a ConvNet, but the filters would be very limited in their power to capture image features.", "Monti et al. train \ud835\udc3d means and variances of Gaussians and the process of transforming node coordinates is similar to fitting them into a Gaussian Mixture Model. The model is quite computationally intensive to train if we want our filters to be global enough, but it can be a good choice for visual tasks (see our BMVC paper for comparison), yet it is often worse than simple GCN on non-visual tasks (Knyazev et al., NeurIPS-W, 2018). Since function D depends on coordinates, the generated filters are also anisotropic and have a shape of oriented and elongated Gaussians as illustrated below.", "Despite a lengthy discussion, we have only scratched the surface. Applications of graph neural networks are expanding far beyond typical graph reasoning tasks, like molecule classification. The number of different graph neural layers is increasing very quickly, similar to how it was for convolutional networks a few years ago, so it\u2019s hard to keep track of them. On that note, PyTorch Geometric (PyG) \u2014 a nice toolbox to learn from graphs \u2014 frequently populates its collection with novel layers and tricks.", "Acknowledgement: A large portion of this tutorial was prepared during my internship at SRI International under the supervision of Mohamed Amer (homepage) and my PhD advisor Graham Taylor (homepage). I also thank Carolyn Augusta for useful feedback.", "Find me on Github, LinkedIn and Twitter. My homepage.", "If you want to cite this blog post in your paper, please use:@misc{knyazev2019tutorial, title={Tutorial on Graph Neural Networks for Computer Vision and Beyond}, author={Knyazev, Boris and Taylor, Graham W and Amer, Mohamed R}, year={2019}}", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student at University of Guelph, Machine Learning Research Group http://bknyaz.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbe6d71d70f49&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@BorisAKnyazev?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@BorisAKnyazev?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Boris Knyazev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3bd901bbd24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&user=Boris+Knyazev&userId=3bd901bbd24&source=post_page-3bd901bbd24----be6d71d70f49---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe6d71d70f49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe6d71d70f49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial"}, {"url": "https://arxiv.org/abs/1503.00759", "anchor_text": "A Review of Relational Machine Learning for Knowledge Graphs"}, {"url": "https://arxiv.org/abs/1611.08097", "anchor_text": "Geometric deep learning: going beyond Euclidean data"}, {"url": "https://arxiv.org/abs/1709.05584", "anchor_text": "Representation Learning on Graphs: Methods and Applications"}, {"url": "http://tkipf.github.io/misc/SlidesCambridge.pdf", "anchor_text": "Structured deep models: Deep learning on graphs and beyond"}, {"url": "https://arxiv.org/abs/1806.01261", "anchor_text": "Relational inductive biases, deep learning, and graph networks"}, {"url": "https://arxiv.org/abs/1812.04202", "anchor_text": "Deep Learning on Graphs: A Survey"}, {"url": "https://arxiv.org/abs/1812.08434", "anchor_text": "Graph Neural Networks: A Review of Methods and Applications"}, {"url": "https://arxiv.org/abs/1901.00596", "anchor_text": "A Comprehensive Survey on Graph NeuralNetworks"}, {"url": "https://www.repository.cam.ac.uk/handle/1810/292230", "anchor_text": "The resurgence of structure in deep neural networks"}, {"url": "https://sungsoo.github.io/2018/02/01/geometric-deep-learning.html", "anchor_text": "video tutorials"}, {"url": "https://ieeexplore.ieee.org/document/572108", "anchor_text": "Alessandro Sperduti and Antonina Starita on \u201cSupervised Neural Networks for the Classification of Structures\u201d"}, {"url": "https://ieeexplore.ieee.org/document/572108", "anchor_text": "Sperduti & Starita, 1997"}, {"url": "https://ieeexplore.ieee.org/document/572108", "anchor_text": "Sperduti & Starita, 1997"}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial"}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial"}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf & Welling, ICLR, 2017"}, {"url": "https://arxiv.org/abs/1611.08097", "anchor_text": "Bronstein et al.\u2019s review"}, {"url": "https://arxiv.org/abs/1810.00826", "anchor_text": "Xu et al., ICLR, 2019"}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al., 2014, ICLR 2014"}, {"url": "https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801", "anchor_text": "another post"}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "convolution theorem"}, {"url": "https://en.wikipedia.org/wiki/Laplacian_matrix", "anchor_text": "graph Laplacian"}, {"url": "https://en.wikipedia.org/wiki/Convolution_theorem", "anchor_text": "spectral convolution of signals on regular grids"}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al."}, {"url": "https://arxiv.org/abs/1606.09375", "anchor_text": "Defferrard et al., NeurIPS, 2016"}, {"url": "https://arxiv.org/abs/1606.09375", "anchor_text": "Defferrard et al."}, {"url": "https://arxiv.org/abs/1811.09595", "anchor_text": "Knyazev et al., NeurIPS-W, 2018"}, {"url": "https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers", "anchor_text": "the power property"}, {"url": "https://github.com/bknyaz/examples/blob/master/splines_cheb.py", "anchor_text": "my github repo"}, {"url": "https://arxiv.org/abs/1312.6203", "anchor_text": "Bruna et al."}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf & Welling, ICLR, 2017"}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf & Welling"}, {"url": "https://en.wikipedia.org/wiki/Identity_matrix", "anchor_text": "identity matrix"}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial"}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis, CVPR, 2017"}, {"url": "https://arxiv.org/abs/1611.08402", "anchor_text": "Monti et al., CVPR, 2017"}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis, CVPR, 2017"}, {"url": "https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss", "anchor_text": "Cross Entropy"}, {"url": "https://arxiv.org/abs/1605.09673", "anchor_text": "Brabandere et al., NIPS, 2016"}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis, CVPR, 2017"}, {"url": "https://en.wikipedia.org/wiki/No_free_lunch_theorem", "anchor_text": "free lunch"}, {"url": "https://medium.com/p/3d9fada3b80d", "anchor_text": "the first part of my tutorial"}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis, CVPR, 2017"}, {"url": "https://arxiv.org/abs/1907.09000", "anchor_text": "BMVC paper"}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis"}, {"url": "https://arxiv.org/abs/1611.08402", "anchor_text": "Monti et al., CVPR, 2017"}, {"url": "https://arxiv.org/abs/1704.02901", "anchor_text": "Simonovsky & Komodakis"}, {"url": "https://arxiv.org/abs/1611.08402", "anchor_text": "Monti et al."}, {"url": "https://scikit-learn.org/stable/modules/mixture.html", "anchor_text": "Gaussian Mixture Model"}, {"url": "https://arxiv.org/abs/1907.09000", "anchor_text": "BMVC paper"}, {"url": "https://arxiv.org/abs/1811.09595", "anchor_text": "Knyazev et al., NeurIPS-W, 2018"}, {"url": "https://github.com/rusty1s/pytorch_geometric", "anchor_text": "PyTorch Geometric (PyG)"}, {"url": "https://medium.com/u/6cf41cb2c546?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Mohamed Amer"}, {"url": "https://mohamedramer.com/", "anchor_text": "homepage"}, {"url": "https://www.gwtaylor.ca/", "anchor_text": "homepage"}, {"url": "https://www.linkedin.com/in/carolynaugusta/", "anchor_text": "Carolyn Augusta"}, {"url": "https://github.com/bknyaz/", "anchor_text": "Github"}, {"url": "https://www.linkedin.com/in/boris-knyazev-39690948/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/BorisAKnyazev", "anchor_text": "Twitter"}, {"url": "https://bknyaz.github.io/", "anchor_text": "My homepage"}, {"url": "http://twitter.com/misc", "anchor_text": "@misc"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----be6d71d70f49---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/graph-neural-networks?source=post_page-----be6d71d70f49---------------graph_neural_networks-----------------", "anchor_text": "Graph Neural Networks"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----be6d71d70f49---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----be6d71d70f49---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----be6d71d70f49---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe6d71d70f49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&user=Boris+Knyazev&userId=3bd901bbd24&source=-----be6d71d70f49---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe6d71d70f49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&user=Boris+Knyazev&userId=3bd901bbd24&source=-----be6d71d70f49---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe6d71d70f49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbe6d71d70f49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----be6d71d70f49---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----be6d71d70f49--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----be6d71d70f49--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----be6d71d70f49--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@BorisAKnyazev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@BorisAKnyazev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Boris Knyazev"}, {"url": "https://medium.com/@BorisAKnyazev/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "664 Followers"}, {"url": "http://bknyaz.github.io/", "anchor_text": "http://bknyaz.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3bd901bbd24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&user=Boris+Knyazev&userId=3bd901bbd24&source=post_page-3bd901bbd24--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8482c870bfd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49&newsletterV3=3bd901bbd24&newsletterV3Id=8482c870bfd3&user=Boris+Knyazev&userId=3bd901bbd24&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}