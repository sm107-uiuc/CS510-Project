{"url": "https://towardsdatascience.com/decision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704", "time": 1683004583.51613, "path": "towardsdatascience.com/decision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704/", "webpage": {"metadata": {"title": "Decision Trees: Understanding the Basis of Ensemble Methods | by Kelly Slatery | Towards Data Science", "h1": "Decision Trees: Understanding the Basis of Ensemble Methods", "description": "Ensemble methods are a fantastic way to capitalize on the benefits of decision trees, while reducing their tendency to overfit. However, they can get pretty complex and can turn into black box\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html", "anchor_text": "sklearn Decision Tree documentation", "paragraph_index": 6}, {"url": "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html", "anchor_text": "sklearn Iris dataset", "paragraph_index": 6}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "sklearn documentation", "paragraph_index": 9}, {"url": "https://stackoverflow.com/questions/25287466/binning-of-continuous-variables-in-sklearn-ensemble-and-trees", "anchor_text": "this Stackoverflow comment", "paragraph_index": 13}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "sklearn documentation", "paragraph_index": 16}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "sklearn documentation", "paragraph_index": 17}], "all_paragraphs": ["Ensemble methods are a fantastic way to capitalize on the benefits of decision trees, while reducing their tendency to overfit. However, they can get pretty complex and can turn into black box models. One of the best things we can do as data scientists and machine learning engineers is make sure we know what\u2019s really going on under the hood when we call that sklearn .fit() method. So that\u2019s what we\u2019re going to do today with Decision Trees.", "In this article, we are going to go over:", "Decision Trees are great for a variety of reasons. Let\u2019s take a look!", "As we\u2019ve seen, there are many positives to using Decision Trees\u2026depending on the circumstances. It may not be the best choice if we have a small sample size, and for regression, it may not be the best choice if we think we\u2019ll be predicting target values outside of what our training sample contains. Circumstances asides, Decision Trees, like all models, also have some disadvantages.", "Now that we\u2019ve got a pretty good idea of some of the pros and cons of simple Decision Trees, we\u2019ll talk about what makes them such a good base estimator for ensemble methods. Some of the most popular ensemble methods based on Decision Trees are:", "All of these ensemble methods take a decision tree and then apply either bagging (bootstrap aggregating) or boosting as a way to reduce variance and bias. For a quick overview of ensemble methods and the bagging and boosting techniques, check out these slides I made for my Data Science course at General Assembly:", "It\u2019s hard to talk about how decision trees work without an example. This image was taken from the sklearn Decision Tree documentation and is a great representation of a Decision Tree Classifier on the sklearn Iris dataset. I added the labels in red, blue, and grey for easier interpretation.", "We see here that each split is binary, or splits into only 2 child nodes. While this is not a necessity for Decision Trees, many implementations, including sklearn\u2019s, are limited to binary splits because considering anything greater is just too computationally heavy \u2014 the tree would never fit. It also doesn\u2019t make a huge difference because binary splits can achieve the same result as a multiway split by simply nesting two binary splits! Due to the complexity of the Decision Tree algorithm, however, the splitting calculations made, when limited to only binary splits, might result in slightly different splits from an algorithm that allowed for more. Again, limiting to binary splits is not a major issue, but simply something to consider. So finally: how does the tree decide where to split??", "The \u201cDecision Tree Algorithm\u201d may sound daunting, but it is simply the math that determines how the tree is built (\u201csimply\u201d\u2026we\u2019ll get into it!). The algorithm currently implemented in sklearn is called \u201cCART\u201d (Classification and Regression Trees), which works for only numerical features, but works with both numerical and categorical targets (regression and classification). At each node, it determines the feature and split threshold of that feature which will yield the \u201clargest information gain\u201d for the model. This \u201cinformation gain\u201d is measured based on the splitting criteria specified by the user.", "If you are so mathematically inclined, here is the formula for information gain, taken from the sklearn documentation (where G = information gain, n = number of data points on left/right side of the threshold, Nm = total number of data points, H = the chosen splitting criteria function, Q = the data at node m, \u0398 = the feature and threshold being evaluated):", "But basically, all you need to know is that \u201cinformation gain\u201d measures how well a given split splits our data so that the target values of the data points in each child node are most homogenous (classification) or closest to each other (regression).", "So how are the feature and split threshold chosen at each node? This is a good question with a somewhat complicated answer! In general terms, the algorithm will scan over every possible threshold split for every feature, calculate the \u201cinformation gain\u201d for each of these different splits, and then choose the split that yielded the highest information gain (how this is measured depends on the splitting criteria described below). Now, this is more intuitive when the feature is a column of 1s and 0s, or a discrete set of options, so there are few possible splits to consider. However, when we have a continuous numerical feature, it gets more complicated.", "The sklearn documentation and hours of Google searching \u201cCART algorithm decision trees how to find split threshold for continuous variables sklearn\u201d failed me in uncovering how split thresholds were determined for continuous variables \u2014 was every unique value considered as a splitting point, increasing computational load exponentially? Were the values binned into a set number of bins if there were more unique values than that set number of bins? Or perhaps the algorithm just splits at the mean, median, or right down the middle!", "After, again, many hours of searching, this Stackoverflow comment from 2014 finally answered the question: when determining what split thresholds to consider for a continuous variable, the sklearn Decision Tree Algorithm (CART) does NOT bin! Instead, it performs the computationally heavy task of checking every possible split \u2014 that is, it sorts all the values for that features, then finds each mean between neighboring values and finds the information gain score for that split (unless the two values are less than 1e-7 apart, an arbitrary small step).", "So now that we know what is being evaluated for determining the split, that begs the question of how we decide which feature and threshold to use at each node: splitting criteria!", "On what feature and what value of that feature a node is split depends on the user-specified criteria: gini or entropy for classification, MSE or MAE for regression. While we could go into great detail about any and all of these, for the purposes of this article, it is most helpful to consider that all of these criteria are measuring how helpful a split is in creating the most homogenous child nodes possible based on their target value.", "For classification, gini and entropy use different formulas for evaluating \u201chow many observations of each class would be split into each child node if we were to use this split?\u201d Their respective formulas from the sklearn documentation, again, for those mathematically inclined, are:", "On the other hand, for regression problems, MSE (mean squared error) and MAE (mean absolute error) evaluate how close to the mean target value of each child node each target value in that node would be, with MSE weighting larger errors as worse than smaller errors. These are both also common metrics used for loss functions in Linear Regression. The formulas from the sklearn documentation are below:", "And there you have it \u2014 what makes a decision tree, and consequently, what makes up the basis of a lot of our beloved ensemble methods! If you\u2019ve made it this far, thanks for sticking with me, or for following your curiosity, or for your dedication to finishing whatever project you might be working on. I hope this overview of Decision Trees and their underlying workings will be helpful as you continue to sharpen your mind and your data science skills! Happy data processing!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Recent UVA graduate, current data scientist and cognitive linguist."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe075d5bfa704&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kellyslatery?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kellyslatery?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "Kelly Slatery"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddd91c2d8a81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&user=Kelly+Slatery&userId=ddd91c2d8a81&source=post_page-ddd91c2d8a81----e075d5bfa704---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe075d5bfa704&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe075d5bfa704&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://imgflip.com/memegenerator/Two-Buttons", "anchor_text": "https://imgflip.com/memegenerator/Two-Buttons"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": ".feature_importances_"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor", "anchor_text": "attribute"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html", "anchor_text": "sklearn.preprocessing.LabelEncoder"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor", "anchor_text": "Regressor"}, {"url": "https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html", "anchor_text": "sklearn Decision Tree documentation"}, {"url": "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html", "anchor_text": "sklearn Iris dataset"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "https://scikit-learn.org/stable/modules/tree.html"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "sklearn documentation"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "https://scikit-learn.org/stable/modules/tree.html"}, {"url": "https://stackoverflow.com/questions/25287466/binning-of-continuous-variables-in-sklearn-ensemble-and-trees", "anchor_text": "this Stackoverflow comment"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "sklearn documentation"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "https://scikit-learn.org/stable/modules/tree.html"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "https://scikit-learn.org/stable/modules/tree.html"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "sklearn documentation"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "https://scikit-learn.org/stable/modules/tree.html"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "https://scikit-learn.org/stable/modules/tree.html"}, {"url": "https://towardsdatascience.com/understanding-decision-tree-classification-with-scikit-learn-2ddf272731bd", "anchor_text": "https://towardsdatascience.com/understanding-decision-tree-classification-with-scikit-learn-2ddf272731bd"}, {"url": "https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952", "anchor_text": "https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952"}, {"url": "https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html", "anchor_text": "https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html"}, {"url": "https://medium.com/@dataakkadian/what-are-parametric-vs-nonparametric-models-8bfa20726f4d", "anchor_text": "https://medium.com/@dataakkadian/what-are-parametric-vs-nonparametric-models-8bfa20726f4d"}, {"url": "https://stats.stackexchange.com/questions/294033/why-are-decision-trees-not-computationally-expensive", "anchor_text": "https://stats.stackexchange.com/questions/294033/why-are-decision-trees-not-computationally-expensive"}, {"url": "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html", "anchor_text": "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html"}, {"url": "https://stackoverflow.com/questions/25287466/binning-of-continuous-variables-in-sklearn-ensemble-and-trees", "anchor_text": "https://stackoverflow.com/questions/25287466/binning-of-continuous-variables-in-sklearn-ensemble-and-trees"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "https://scikit-learn.org/stable/modules/tree.html"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor", "anchor_text": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e075d5bfa704---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----e075d5bfa704---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/selection-criteria?source=post_page-----e075d5bfa704---------------selection_criteria-----------------", "anchor_text": "Selection Criteria"}, {"url": "https://medium.com/tag/sklearn?source=post_page-----e075d5bfa704---------------sklearn-----------------", "anchor_text": "Sklearn"}, {"url": "https://medium.com/tag/decision-tree-regressor?source=post_page-----e075d5bfa704---------------decision_tree_regressor-----------------", "anchor_text": "Decision Tree Regressor"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe075d5bfa704&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&user=Kelly+Slatery&userId=ddd91c2d8a81&source=-----e075d5bfa704---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe075d5bfa704&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&user=Kelly+Slatery&userId=ddd91c2d8a81&source=-----e075d5bfa704---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe075d5bfa704&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe075d5bfa704&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e075d5bfa704---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e075d5bfa704--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e075d5bfa704--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e075d5bfa704--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kellyslatery?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kellyslatery?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kelly Slatery"}, {"url": "https://medium.com/@kellyslatery/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "45 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddd91c2d8a81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&user=Kelly+Slatery&userId=ddd91c2d8a81&source=post_page-ddd91c2d8a81--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fddd91c2d8a81%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-understanding-the-basis-of-ensemble-methods-e075d5bfa704&user=Kelly+Slatery&userId=ddd91c2d8a81&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}