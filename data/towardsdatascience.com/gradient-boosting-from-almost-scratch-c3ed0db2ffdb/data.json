{"url": "https://towardsdatascience.com/gradient-boosting-from-almost-scratch-c3ed0db2ffdb", "time": 1683003363.250767, "path": "towardsdatascience.com/gradient-boosting-from-almost-scratch-c3ed0db2ffdb/", "webpage": {"metadata": {"title": "Gradient Boosting from Almost Scratch | by John Clements | Towards Data Science", "h1": "Gradient Boosting from Almost Scratch", "description": "Over the past month, I\u2019ve been slowly working my way through Joel Grus\u2019 Data Science from Scratch 2nd Edition and I have thoroughly enjoyed building simple versions of machine learning algorithms\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.amazon.com/Data-Science-Scratch-Principles-Python-ebook/dp/B07QPC8RZX", "anchor_text": "Data Science from Scratch 2nd Edition", "paragraph_index": 0}, {"url": "https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf", "anchor_text": "slides", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning", "anchor_text": "CARTs", "paragraph_index": 1}, {"url": "https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf", "anchor_text": "slides", "paragraph_index": 2}, {"url": "https://xgboost.readthedocs.io/en/latest/index.html", "anchor_text": "XGBoost", "paragraph_index": 2}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html", "anchor_text": "documentation", "paragraph_index": 4}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html", "anchor_text": "ridge regression model", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://github.com/jkclem/GradBoost", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://github.com/jkclem/GradBoost/blob/master/example/GradBoost%20Notebook.ipynb", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://medium.com/@jkclements2016", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://bit.ly/3cN4oQ1", "anchor_text": "https://bit.ly/3cN4oQ1", "paragraph_index": 26}], "all_paragraphs": ["Over the past month, I\u2019ve been slowly working my way through Joel Grus\u2019 Data Science from Scratch 2nd Edition and I have thoroughly enjoyed building simple versions of machine learning algorithms. I\u2019ve always learned better by doing and this book plays to that learning style. Unfortunately, it doesn\u2019t have a chapter for implementing gradient boosting. Although I read through Tianqi Chen\u2019s great slides, I thought I\u2019d gain a better understanding of gradient boosting by implementing a simple version of it myself.", "Gradient boosting is a relatively simple algorithm. It works by training weak models (most frequently CARTs) on the residuals from the preceding iteration. New predictions are generated by adding the predicted residuals multiplied by a learning rate between 0 and 1 to the previous prediction. The learning rate prevents the model from taking too large a step towards the actual values and overshooting them. Small learning rates take longer to converge towards the best fit. Finding the right one is a balancing act.", "I highly recommend Tianqi Chen\u2019s slides for a more thorough treatment of gradient boosting, or look at the documentation for XGBoost, which is his amazing gradient boosting package.", "I said gradient boosting from almost scratch; I use scikit-learn\u2019s models as my weak learners and use numpy for for certain mathematical functions and for its array structure.", "The first model I am using is scikit-learn\u2019s regression tree. Therefore the potential loss functions are Mean Squared Error, Friedman\u2019s adjusted Mean Square Error, and Mean Absolute Error. They each contain regularization terms, which is lucky for me. A more thorough explanation of each is contained in scikit-learn\u2019s documentation.", "In addition, I am using their ridge regression model whose loss function is the sum of the squared errors + the l2 norm of the coefficients multiplied by a penalty coefficient. You can learn more about it in most linear models or machine learning textbooks, or go here.", "The code I used to implement the boosting algorithm is below. A link to the whole notebook will be available at the bottom of the article as an incentive to keep reading.", "Now let\u2019s generate some testing data to see if this function works. I chose to simulate data, so I know there is a relationship between some of the independent variables and the target variable. I used scikit-learn\u2019s make_regression function to generate 1,000 observations, with 20 independent variables, three-quarters of which actually contain useful information. I then split the data in half into training and testing data sets.", "When using gradient boosting, too few iterations leads to an under-fit model and too many iterations leads to an over-fit one. Before we make predictions, it is helpful to see how the training mean square error evolves with more boosting iterations, so we can try and fit a well calibrated model.", "When actually attempting to solve a real problem, you would determine the number of boosting rounds, as well as other parameters, using a grid search and k-folds cross validation. I did something a little simpler, for ease of demonstration. I just plotted the training Mean Square Error vs. number of boosting rounds for 5 to 100 rounds, with step size 5 and chose a semi-arbitrary cut-off.", "The regression trees in this implementation have a depth of 3 and the loss function is specified as the regularized mean square error. The ridge regressions choose the best l2-norm penalty coefficient at each step among {0.01, 0.1, 1, 10} using 3-fold cross validation. The learning rate is set to 0.1 for both models.", "When using the trees as our weak learner, the mean squared error falls quickly until it reaches an inflection point around 30 boosting iterations. After that point, it gently slopes down as it approaches 100 rounds. When using the ridge regression model, the training mean square error plunges until 20 rounds and it really levels off after 30. We\u2019ll use 100 as our number of boosting iterations using trees and 30 for the ridge regression boosting model. But first, let\u2019s look at the results from 0 and 10 boosting iterations.", "In the plots above, the red line is where the predicted values equal the actual values, representing perfect predictions. The closer to that line for the testing data, the more accurate our model. If the training predictions are a tight fit, but the testing predictions are all over the place, the model is over-fit to the training data.", "With 0 boosting rounds, we are just guessing the average. As you would expect, this naive \u201cmodel\u201d under-fits the data. Let\u2019s see how things evolve after 10 boosting rounds.", "At 10 rounds, the predictions from each model are starting to rotate towards the perfect prediction line. This process is sped up with higher learning rates, but it is possible to overshoot the mark. The cloud of predictions is much tighter for the model using ridge regression as the weak learner. This is probably because the data we generated contains linear relationships and ridge regression is a linear model. Now I think it is time to make our final predictions and compare.", "At 100 iterations, our boosted trees model is decent, but it performs worse in the testing data set the farther from the mean of the target variable. It also looks a bit over-fit. Still it is a much better fit than the 10 round model. Our model with ridge regression as the weak learner is a much better fit. Again, this is likely due to the linear structure in the simulated data. Speaking of linear structure, let\u2019s check how these models fare against a simple multiple linear regression.", "The plot suggests multiple linear regression is our best fitting model. Let\u2019s check the testing set mean square errors to be sure.", "The multiple linear regression wins! Let this be a reminder to visualize and investigate your data. Always think carefully about the data generating process that shapes its structure before throwing algorithms at problems. Or don\u2019t. It\u2019s your life.", "Let\u2019s see what would happen with more boosting rounds, say 1000 for the Boosted Trees and 100 for the Boosted Ridge Regression.", "The Boosted Trees have clearly over-fit the training data, but the results from the Boosted Ridge Regressions are pretty good. Again, the relationships in the data are linear. Let\u2019s look at the new mean square errors.", "At 100 boosting rounds, the Boosted Ridge Regression performs close to the multiple linear regression, which represents the true model (plus a couple noise variables that should be dropped out). Turns out, sometimes throwing algorithms at a problem works. Still, it\u2019s better to know your data.", "I hope this article has given you a better idea of what is going on under the hood (sorry for the cliche) of one of the most powerful machine learning techniques. Now you can think about what your computer is doing while you go get coffee and wait for your grid search to finish.", "You can get my code and play around with it here.", "Check out the Jupyter Notebook that this article is based on here.", "You can find more of my articles here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning | Statistics | Econometrics | LinkedIn: https://bit.ly/3cN4oQ1"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc3ed0db2ffdb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jkclements2016?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jkclements2016?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "John Clements"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F468189676a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&user=John+Clements&userId=468189676a43&source=post_page-468189676a43----c3ed0db2ffdb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3ed0db2ffdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3ed0db2ffdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.amazon.com/Data-Science-Scratch-Principles-Python-ebook/dp/B07QPC8RZX", "anchor_text": "Data Science from Scratch 2nd Edition"}, {"url": "https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf", "anchor_text": "slides"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning", "anchor_text": "CARTs"}, {"url": "https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf", "anchor_text": "slides"}, {"url": "https://xgboost.readthedocs.io/en/latest/index.html", "anchor_text": "XGBoost"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html", "anchor_text": "documentation"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html", "anchor_text": "ridge regression model"}, {"url": "https://en.wikipedia.org/wiki/Tikhonov_regularization", "anchor_text": "here"}, {"url": "https://github.com/jkclem/GradBoost", "anchor_text": "here"}, {"url": "https://github.com/jkclem/GradBoost/blob/master/example/GradBoost%20Notebook.ipynb", "anchor_text": "here"}, {"url": "https://medium.com/@jkclements2016", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c3ed0db2ffdb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c3ed0db2ffdb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----c3ed0db2ffdb---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/statistics?source=post_page-----c3ed0db2ffdb---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c3ed0db2ffdb---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3ed0db2ffdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&user=John+Clements&userId=468189676a43&source=-----c3ed0db2ffdb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3ed0db2ffdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&user=John+Clements&userId=468189676a43&source=-----c3ed0db2ffdb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3ed0db2ffdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc3ed0db2ffdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c3ed0db2ffdb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c3ed0db2ffdb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jkclements2016?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jkclements2016?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "John Clements"}, {"url": "https://medium.com/@jkclements2016/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "676 Followers"}, {"url": "https://bit.ly/3cN4oQ1", "anchor_text": "https://bit.ly/3cN4oQ1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F468189676a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&user=John+Clements&userId=468189676a43&source=post_page-468189676a43--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F26152b908c24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-from-almost-scratch-c3ed0db2ffdb&newsletterV3=468189676a43&newsletterV3Id=26152b908c24&user=John+Clements&userId=468189676a43&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}