{"url": "https://towardsdatascience.com/model-compression-via-pruning-ac9b730a7c7b", "time": 1683016918.018817, "path": "towardsdatascience.com/model-compression-via-pruning-ac9b730a7c7b/", "webpage": {"metadata": {"title": "Model Compression via Pruning. Pruning Neural Network | by Kelvin | Towards Data Science", "h1": "Model Compression via Pruning", "description": "To obtain fast and accurate inference on edge devices, a model has to be optimized for real-time inference. Fine-tuned state-of-the-art models like VGG16/19, ResNet50 have 138+ million and 23+\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/distilling-knowledge-in-neural-network-d8991faa2cdc", "anchor_text": "Knowledge Distillation", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1710.01878", "anchor_text": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "paragraph_index": 14}], "all_paragraphs": ["To obtain fast and accurate inference on edge devices, a model has to be optimized for real-time inference. Fine-tuned state-of-the-art models like VGG16/19, ResNet50 have 138+ million and 23+ million parameters respectively and inference is often expensive on resource-constrained devices.", "Previously I\u2019ve talked about one model compression technique called \u201cKnowledge Distillation\u201d using a smaller student network to mimic the performance of a larger teacher network (Both student and teacher network has different network architecture).", "Today, the focus will be on \u201cPruning\u201d one model compression technique that allows us to compress the model to a smaller size with zero or marginal loss of accuracy. In short, pruning eliminates the weights with low magnitude (That does not contribute much to the final model performance). Both original and pruned model has the same architecture, with the pruned model being sparser (weights with the low magnitude being set to zeros).", "The authors in paper [1] compares two distinct methods of 1) training a large model, and perform pruning to obtain a sparse model with a small number of nonzero parameters (large-sparse); and 2) training a small-dense model with a size comparable to the large-sparse model. This comparison is to determine the trade-off between model accuracy and the model size of large-sparse vs small-dense models. The pruning method studied by paper [1] develop a simple gradual pruning algorithm that requires minimal tuning and can be seamlessly incorporated within the training process and demonstrate its applicability on a wide variety of different network architectures ranging from InceptionV3, MobileNets, stacked LSTMs, and seq2seq models used in Google\u2019s Neural Machine Translation.", "The amount of sparsity for gradual pruning can be illustrated as a monotonic increasing function.", "The pruning method is done by having a binary/bit mask variable which has the same size and shape as the layer (weight) as shown in the following Fig 3. The bit mask is used to determine which weights should be used for training the model. As we gradually train the model, the sparsity of the model increased over a span of pruning steps as shown in Fig 2. above.", "The Bit mask can be computed by defining a pruning type strategy E.g. We define a pruning strategy that prunes every even column of the weight Fig 3. the binary/bit mask are gradually updated during training to achieves the target level of sparsity. Typically in many pruning strategy, only the weight with the smallest magnitude weight are being pruned.", "The following Table 1. extracted from paper [1] shows the model size and accuracy trade-off for varying the sparsity level of the InceptionV3 model. We see that by inducing a sparsity of 50%, the performance achieved between the baseline model (0% sparsity) and the pruned model (50% sparsity) is negligible. It is evident with a gradual increase of sparsity, the performance of the model will gradually degrade as well.", "MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases, by pruning the already optimized MobileNet model, the pruned MobileNet achieved much better performance in terms of model size and accuracy. MobileNets have a width multiplier parameter that allows trading off the accuracy of the model with the number of parameters and computational cost. The baseline MobileNet has a width parameter of 1.0.", "Performance of the large-sparse pruned model is much more favorable than any of the small-dense MobileNet as shown in Fig 4.", "The following Table 2. is the MobileNet prunning result extracted from paper [1] comparing the 1.0 MobileNet large-sparse model (sparsity > 0% and width = 1.0) and the small-dense model (sparsity = 0% and width < 1.0). The results underlined in red shows a 1 to 1 comparison between the large-sparse pruned MobileNet with 75% sparsity of 1.09 million parameters obtained a Top-1 and Top-5 accuracy of 67.7% and 88.5% respectively as compared to the small-dense 0.5 MobileNet with 0% sparsity of 1.32 million parameters obtained a Top-1 and Top-5 accuracy of 63.7% and 85.4% respectively (underlined in red). Similar comparision has been shown for the 1.0 MobileNets (sparsity 90% and width = 1.0) vs the 0.25 MobileNets (sparsity = 0% and width = 0.25) with the same number of non-zero (NNZ) parameters (underlined in blue).", "The following Table 3. shows the MobileNet model size between the small-dense and large-sparse models.", "To experience with model optimization using pruning, PyTorch [2] and Tensorflow [3] provides easy to use pruning API that allows us to optimize our model effortlessly.", "With an ever-increasing of IoT devices, machine learning on edge is still an ever-growing challenging problem that needs to be addressed. Pruning is one model compression technique that allows the model to be optimized for real-time inference for resource-constrained devices. It was shown that large-sparse models often outperform small-dense models across various different architectures. Model pruning can also be used in tandem with other model compression techniques such as quantization and low-rank matrix factorization to further reduce the model size.", "[1] M. Zhu, S. Gupta, To prune, or not to prune: exploring the efficacy of pruning for model compression (2017)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning enthusiast. Writing tips are welcomed."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac9b730a7c7b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kelvinbksoh?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelvinbksoh?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "Kelvin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ea38e941e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&user=Kelvin&userId=7ea38e941e0c&source=post_page-7ea38e941e0c----ac9b730a7c7b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac9b730a7c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac9b730a7c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/distilling-knowledge-in-neural-network-d8991faa2cdc", "anchor_text": "Knowledge Distillation"}, {"url": "https://arxiv.org/abs/1710.01878", "anchor_text": "link"}, {"url": "https://arxiv.org/abs/1710.01878", "anchor_text": "link"}, {"url": "https://arxiv.org/abs/1710.01878", "anchor_text": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"url": "https://pytorch.org/tutorials/intermediate/pruning_tutorial.html", "anchor_text": "https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras", "anchor_text": "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"}, {"url": "https://medium.com/tag/model-compression?source=post_page-----ac9b730a7c7b---------------model_compression-----------------", "anchor_text": "Model Compression"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ac9b730a7c7b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ac9b730a7c7b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/edge-ai?source=post_page-----ac9b730a7c7b---------------edge_ai-----------------", "anchor_text": "Edge Ai"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ac9b730a7c7b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac9b730a7c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&user=Kelvin&userId=7ea38e941e0c&source=-----ac9b730a7c7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac9b730a7c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&user=Kelvin&userId=7ea38e941e0c&source=-----ac9b730a7c7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac9b730a7c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fac9b730a7c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ac9b730a7c7b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ac9b730a7c7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelvinbksoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelvinbksoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kelvin"}, {"url": "https://medium.com/@kelvinbksoh/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "32 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ea38e941e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&user=Kelvin&userId=7ea38e941e0c&source=post_page-7ea38e941e0c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8b60176305e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-compression-via-pruning-ac9b730a7c7b&newsletterV3=7ea38e941e0c&newsletterV3Id=8b60176305e1&user=Kelvin&userId=7ea38e941e0c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}