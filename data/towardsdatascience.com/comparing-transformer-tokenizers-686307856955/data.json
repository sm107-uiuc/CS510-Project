{"url": "https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955", "time": 1683001454.241876, "path": "towardsdatascience.com/comparing-transformer-tokenizers-686307856955/", "webpage": {"metadata": {"title": "Comparing Transformer Tokenizers. Comparing Tokenizer vocabularies of\u2026 | by Gergely D. N\u00e9meth | Towards Data Science", "h1": "Comparing Transformer Tokenizers", "description": "If someone used word embeddings like Word2vec or GloVe, adapting to the new contextualised embeddings like BERT can be difficult. In this story, we will investigate one of the differences: subword\u2026"}, "outgoing_paragraph_urls": [{"url": "http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html", "anchor_text": "explored the multilingual BERT vocabulary", "paragraph_index": 0}, {"url": "https://colab.research.google.com/drive/1cPxYzWFZ99e64WaWPCJumdZfkxDjGiEI", "anchor_text": "available on Google Colab", "paragraph_index": 1}, {"url": "https://code.google.com/archive/p/word2vec/", "anchor_text": "this word2vec dictionary", "paragraph_index": 3}, {"url": "http://dumps.wikimedia.org/enwiki/20140102/", "anchor_text": "Wikipedia 2014", "paragraph_index": 12}, {"url": "https://catalog.ldc.upenn.edu/LDC2011T07", "anchor_text": "Gigaword 5", "paragraph_index": 12}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 12}], "all_paragraphs": ["If someone used word embeddings like Word2vec or GloVe, adapting to the new contextualised embeddings like BERT can be difficult. In this story, we will investigate one of the differences: subword tokens. The inspiration for this story was a similar post that explored the multilingual BERT vocabulary.", "For this experiment, we will use the Huggingface transformer library [1]. They implemented multiple state-of-the-art architectures. The codes of this experiment are available on Google Colab.", "Subword tokens ( or word pieces) can be used to split words into multiple pieces, therefore, reducing the vocabulary size for covering every word [2]. The idea behind word pieces is as old as the written language. Characters are the most well-known word pieces and the English words can be written with 26 characters.", "However, finding the right size for the word pieces is not yet regularised. Characters can represent every word with 26ish keys while the original word embeddings used a different key for every word (3M key for this word2vec dictionary) [3]. The novel transformers\u2019 tokenizers use vocabulary size somewhere between the two.", "BERT [4] uses WordPiece [2] tokens, where the non-word-initial pieces start with ##. The different BERT models have different vocabularies. For example, the uncased base model has 994 tokens reserved for possible fine-tuning ([unused0] to [unused993]). The cased model has only 101 unused tokens as it needs more tokens to cover uppercase letters as well as lowercase ones. The multilingual model has only 100 unused tokens, however, it\u2019s total vocabulary size is four times the uncased\u2019s size. The multilingual model stores more special characters to cover the words of 104 languages.", "The word tokenization tokenized with the model bert-base-cased:  [\u2018token\u2019, \u2018##ization\u2019]", "Huggingface\u2019s GPT2 [5] and RoBERTa [6] implementations use the same vocabulary with 50000 word pieces. They use the BPE (byte pair encoding [7]) word pieces with \\u0120 as the special signalling character, however, the Huggingface implementation hides it from the user.", "BPE is a frequency-based character concatenating algorithm: it starts with two-byte characters as tokens and based on the frequency of n-gram token-pairs, it includes additional, longer tokens. For example, if the letter e and r are frequently together in the language, a new token, er is added to the vocabulary. Next, if h and er are frequently together, her is added to the vocabulary. The algorithm goes on until it reaches the wanted size.", "The word tokenization tokenized with the model gpt: [\u2018token\u2019, \u2018ization\u2019]", "Compared to the previous vocabularies, the XLM [8] uses a suffix signalling: </w> at the end of the word piece indicates that this is the end of a word. XLM uses a BPE-based vocabulary shared between multiple languages and models.", "The following experiment measures the similarity between the different vocabularies by calculating the intersection of them. The only preprocessing step was the elimination of the special characters mentioned in the previous section. The cell in the ith row and jth column is calculated by the formula: size(intersection(vocab(i),vocab(j)))/size(vocab(j)) .", "To properly analyse the table one should note that the different models have different vocabulary sizes. The intersection of the bert-base-cased (28996wps) and the bert-base-multilingual-cased (119547wps) can only cover one-fourth of the multilingual vocabulary even if there is a perfect match between the two vocabularies.", "The Word2Vec [3] and GloVe [9] are static word embeddings using words as keys. For additional information, the table includes the 30000 most common word of these embeddings as well as the total vocabularies. We use the 3M GoogleNews model for Word2Vec and the Wikipedia 2014 + Gigaword 5 model for GloVe.", "From this table, we can see that 87% of all the tokens in bert-base-cased are represented as words in the Word2Vec model. Also, bert-base-uncased and xlm-mlm-en-2048 are relatively similar to each other compared to the others as they have over 70% similarity.", "With the different word pieces comes the different tokenisation. Here are some examples of different word tokenisations:", "The following graph shows the increase in the equally tokenized words in the uncased modification of the most common words of Word2vec. As we can see, in the first 1000 words, 763 words are tokenized the same way, however, this number increases only to 1986 in the first 4000 words and 3055 if we look at the first 10000 most common words.", "In this story, we saw that the different Transformer models use different tokenisers and different subword tokens. Because of this, the comparison of the models in token-level is difficult. The possibility of a global, standardized subword model is an open question. Even for English, the vocabulary of the tokenizers varies highly.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F686307856955&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----686307856955--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----686307856955--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@neged.ng?source=post_page-----686307856955--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@neged.ng?source=post_page-----686307856955--------------------------------", "anchor_text": "Gergely D. N\u00e9meth"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71eefc6da84b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=post_page-71eefc6da84b----686307856955---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F686307856955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F686307856955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html", "anchor_text": "explored the multilingual BERT vocabulary"}, {"url": "https://colab.research.google.com/drive/1cPxYzWFZ99e64WaWPCJumdZfkxDjGiEI", "anchor_text": "available on Google Colab"}, {"url": "https://pixabay.com/users/PDPics-44804/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=390055", "anchor_text": "PDPics"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=390055", "anchor_text": "Pixabay"}, {"url": "https://code.google.com/archive/p/word2vec/", "anchor_text": "this word2vec dictionary"}, {"url": "http://dumps.wikimedia.org/enwiki/20140102/", "anchor_text": "Wikipedia 2014"}, {"url": "https://catalog.ldc.upenn.edu/LDC2011T07", "anchor_text": "Gigaword 5"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://arxiv.org/abs/1910.03771", "anchor_text": "Huggingface\u2019s transformers: State-of-the-art natural language processing."}, {"url": "https://arxiv.org/abs/1609.08144", "anchor_text": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation."}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Efficient estimation of word representations in vector space."}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Bert: Pre-training of deep bidirectional transformers for language understanding."}, {"url": "https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf", "anchor_text": "Language models are unsupervised multitask learners."}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "A robustly optimized bert pretraining approach."}, {"url": "https://arxiv.org/abs/1508.07909", "anchor_text": "Neural machine translation of rare words with subword units"}, {"url": "http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining", "anchor_text": "Cross-lingual Language Model Pretraining."}, {"url": "https://www.aclweb.org/anthology/D14-1162", "anchor_text": "Glove: Global vectors for word representation."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----686307856955---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----686307856955---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----686307856955---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----686307856955---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----686307856955---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F686307856955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=-----686307856955---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F686307856955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=-----686307856955---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F686307856955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----686307856955--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F686307856955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----686307856955---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----686307856955--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----686307856955--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----686307856955--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----686307856955--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----686307856955--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----686307856955--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----686307856955--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----686307856955--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@neged.ng?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@neged.ng?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gergely D. N\u00e9meth"}, {"url": "https://medium.com/@neged.ng/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "190 Followers"}, {"url": "https://ellisalicante.org", "anchor_text": "https://ellisalicante.org"}, {"url": "https://www.linkedin.com/in/gergely-nemeth-092b10137/", "anchor_text": "https://www.linkedin.com/in/gergely-nemeth-092b10137/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71eefc6da84b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=post_page-71eefc6da84b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb43768ab187d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-transformer-tokenizers-686307856955&newsletterV3=71eefc6da84b&newsletterV3Id=b43768ab187d&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}