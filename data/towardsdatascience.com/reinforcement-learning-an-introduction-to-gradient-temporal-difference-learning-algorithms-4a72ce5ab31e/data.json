{"url": "https://towardsdatascience.com/reinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e", "time": 1683003710.486082, "path": "towardsdatascience.com/reinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e/", "webpage": {"metadata": {"title": "Reinforcement Learning\u2014 An Introduction to Gradient Temporal Difference Learning Algorithms | by Louis de Benoist | Towards Data Science", "h1": "Reinforcement Learning\u2014 An Introduction to Gradient Temporal Difference Learning Algorithms", "description": "Reinforcement learning is one of the hottest fields to be in right now, with concrete applications growing at an incredibly rapid pace, from beating video games to robotics. At its essence\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.linkedin.com/in/louis-de-benoist/", "anchor_text": "www.linkedin.com/in/louis-de-benoist/", "paragraph_index": 44}], "all_paragraphs": ["Reinforcement learning is one of the hottest fields to be in right now, with concrete applications growing at an incredibly rapid pace, from beating video games to robotics. At its essence, reinforcement learning (RL) deals with decision making \u2014i.e. it attempts to answer the question of how an agent should act in a given environment.", "Reinforcement learning deals with decision making", "Loosely speaking, all of RL comes down to either finding or evaluating a policy, which is just a way of behaving. For example, a policy could be a playing strategy in chess.", "A policy takes a state \u2014 in the chess example, the position of all the pieces on the board \u2014 and assigns an action to it.", "For example, given the state of your chess board, your policy might ask you to move your queen forward. When you exit a state, you get some reward. This is very intuitive. For example, if we use happiness/pain to measure our reward as humans, putting our hand in boiling water (exiting the prior state of having a comfortably cold hand) will yield some pain (and thus, a reward).", "In this article, we\u2019ll focus on something called policy evaluation, which boils down *pun intended* to evaluating a policy. What does this mean? Given a policy, we want to find its associated value function. This means being able to assign a value to each state provided we follow some given policy. But what do we mean by value? If we\u2019re in some state, the value of that state is simply the expected amount of reward that we\u2019ll get from that state onward.", "TD does not necessarily converge in the off-policy setting", "Variants of TD learning, introduced by Richard Sutton in the 1980s, are some of the most robust and used policy evaluation algorithms, especially in the on-policy setting, i.e. when you are following the policy that you are evaluating. However, in the off-policy setting \u2014 when following a policy different than the one that you are evaluating \u2014 convergence of TD learning algorithms is not guaranteed in some cases (e.g. when using linear approximations, which we\u2019ll talk about in the next section). This has led Sutton et al. to introduce GTD and GTD2 (an improved version of GTD) in 2009 to guarantee convergence in the off-policy setting [1] [2].", "When you first start learning about RL, chances are you begin learning about Markov chains, Markov reward process (MRP), and finally Markov Decision Processes (MDP). Then, you usually move on to typical policy evaluation algorithms, such as Monte Carlo (MC) and Temporal Difference (TD) learning, followed by control algorithms, such as SARSA and Q-learning.", "Table lookup isn\u2019t practical for most RL applications", "In all of these cases, you\u2019ve probably been working with a tabular representation of your state-value and action-value functions. This means that you were updating the value of each state directly. In practice, this is quite impractical for MDPs with a huge number of states or in the case where we are working with continuous state spaces. For example, if you\u2019re trying to guide a helicopter in 3D space, your state might be its current position, velocity, angular velocity, acceleration, etc. As you can guess, there\u2019s infinite amount of states, meaning that there\u2019s just no way of storing everything in a table.", "What\u2019s the solution to this problem? Instead of directly updating each state, we\u2019re going to use a function (parametrized by some weight vector \u03b8) to approximate the true value function. This function could be anything (you could use neural networks, for example), but we\u2019re going to focus on linear function approximation. Mathematically, we\u2019re looking for a function", "where \u03d5(s) is a feature vector containing information about a given state (for example: velocity, acceleration, etc.) and \u03b8 is the weight vector that we are trying to learn. The estimate for the value function at some state is just the dot product between \u03d5(s) and \u03b8. In the above equation, \u03b3 \u2208 (0,1] is called the discount rate; intuitively, it means that we place more importance on getting reward in the short term rather than in the long term \u2014 which is consistent with how we work as humans, besides being mathematically convenient.", "Our goal is to find the weight vector \u03b8 that will allow our value function approximation to be as accurate as possible.", "If we use s\u2019 to denote the next state, we can use the Bellman equation to write the TD error as follows:", "We recall that the Bellman operator, which we\u2019ll use later, is defined as TV = R + \u03b3 PV, where P is the transition matrix. Any given value function V must satisfy the Bellman equation, meaning that V = TV. Using the TD error, we can obtain the traditional TD update:", "for some learning rate \u03b1(k). Now, this might look complicated if this is new to you, but it\u2019s nothing fancy! Indeed, this is just a simple stochastic gradient descent! In the case of linear approximators, we notice that the gradient reduces to \u03d5(k), which makes everything nice and simple.", "Now that we have a value function approximator, the question becomes: how do we make this approximator as good as possible? To do this, we first need to define some sort of error function.", "An objective function is just a function of \u03b8 that we will try to minimize with respect to \u03b8.", "When doing TD learning, the most natural way of measuring error is by measuring how much V(\u03b8) fits the Bellman equation. This is called the Mean Squared Bellman Error (MSBE) and it is defined as", "where D is a diagonal matrix containing weights measuring how often each state occurs and", "TV(\u03b8) will usually not be representable as V(\u03b8) for any \u03b8", "This objective function was used in a lot of prior research, but most TD algorithms do not converge to the minimum of the MSBE. This is due to the Bellman operator following the underlying state dynamics of the Markov chain, meaning that TV(\u03b8) will usually not be representable as V(\u03b8) for any \u03b8 (if we consider linear function approximators, for example, it makes sense that we can\u2019t exactly represent nonlinear functions). What\u2019s typically done, then, is that the optimal solution is then projected to the nearest representable value function with the operator \u03a0 defined as", "If we consider a linear architecture, where V(\u03b8) = \u03a6\u03b8 for some matrix \u03a6 whose rows are the \u03d5(s), then we can write \u03a0 independently of \u03b8 as", "All TD algorithms up to now converge to a value of \u03b8 (the TD fixpoint) such that", "The derivation of GTD2 makes use of a different objective function, the Mean Square Projected Bellman Error (MSPBE), which takes into account the projection \u03a0,", "There is a very neat geometrical relationship between the MSBE and MSPE, which we can see in the following image (taken from Sutton\u2019s original paper on GTD2):", "It\u2019s interesting to note that for GTD, the objective function that was used was the norm of the expected TD update (NEU),", "We will see very shortly that the two equations are indeed quite similar.", "We will now derive the GDT2 algorithm, an algorithm introduced in 2009 by Sutton et al. which converges in the off-policy scenario. We begin by acknowledging the following important relationships, which will be useful in simplifying some equations later on.", "Now, we can write the MSPBE in terms of expectations as", "We should see something interesting here!", "We see that the MSPBE only differs from the NEU by the inclusion of the inverse of the feature covariance matrix.", "To avoid needing two independent samples, we can use a modifiable parameter w \u2208 R\u207f to form a quasi-stationary estimate of all but one of the expectations in the gradient of the objective function", "which can directly be sampled. From this, we obtain the GTD2 algorithm:", "Now, we want to find an iterative update for w. First, we realize that", "We see that the right side of the above is a solution to the following least squares problem:", "which can be solved by stochastic gradient descent with the following update:", "The \u03b8 and w updates together make up the GTD2 algorithm which can easily be implemented in your favorite programming language. Note that, in order for the algorithm to converge, we need to have", "Thanks for getting this far! I hope you now have a better understanding of policy evaluation and, more specifically, some new algorithms in reinforcement learning, such as gradient temporal difference learning. Furthermore, I hope you gained some insight into the math that actually goes on behind the algorithm and have more intuition with respect to how they are derived.", "This article heavily relies on the original GTD2 paper by Sutton et al. in 2009, cited below, as well as the initial GTD paper, also but Sutton et. al.", "[1] Sutton, R. S., Maei, H. R, Precup, D., Bhatnagar, S., Silver, D., Szepesvari, Cs. & Wiewiora, E. (2009). Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th International Conference on Machine Learning, pp. 993\u20131000. Omnipress.", "[2] Sutton, R. S., Szepesvari, Cs., Maei, H. R. (2009). A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation. Advances in Neural Information Processing Systems 21. MIT Press.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning @ University of Cambridge | \u00c9cole Polytechnique. www.linkedin.com/in/louis-de-benoist/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4a72ce5ab31e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@louis.debenoist?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@louis.debenoist?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "Louis de Benoist"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c5850bd7dd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&user=Louis+de+Benoist&userId=4c5850bd7dd8&source=post_page-4c5850bd7dd8----4a72ce5ab31e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a72ce5ab31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a72ce5ab31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@rafifatmaka?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Rafif Prawira"}, {"url": "https://unsplash.com/s/photos/maze?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4a72ce5ab31e---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4a72ce5ab31e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4a72ce5ab31e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----4a72ce5ab31e---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4a72ce5ab31e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a72ce5ab31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&user=Louis+de+Benoist&userId=4c5850bd7dd8&source=-----4a72ce5ab31e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a72ce5ab31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&user=Louis+de+Benoist&userId=4c5850bd7dd8&source=-----4a72ce5ab31e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a72ce5ab31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4a72ce5ab31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4a72ce5ab31e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4a72ce5ab31e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@louis.debenoist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@louis.debenoist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Louis de Benoist"}, {"url": "https://medium.com/@louis.debenoist/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "106 Followers"}, {"url": "http://www.linkedin.com/in/louis-de-benoist/", "anchor_text": "www.linkedin.com/in/louis-de-benoist/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c5850bd7dd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&user=Louis+de+Benoist&userId=4c5850bd7dd8&source=post_page-4c5850bd7dd8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff6d15b42c1e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-introduction-to-gradient-temporal-difference-learning-algorithms-4a72ce5ab31e&newsletterV3=4c5850bd7dd8&newsletterV3Id=f6d15b42c1e5&user=Louis+de+Benoist&userId=4c5850bd7dd8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}