{"url": "https://towardsdatascience.com/how-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787", "time": 1683015160.7046888, "path": "towardsdatascience.com/how-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787/", "webpage": {"metadata": {"title": "How to Apply BERT to Arabic and Other Languages | by Chris McCormick | Towards Data Science", "h1": "How to Apply BERT to Arabic and Other Languages", "description": "Up to this point, Nick and I have been writing tutorials almost exclusively on NLP applications using the English language\u2026 While the general algorithms and ideas extend to all languages, the huge\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1911.02116.pdf", "anchor_text": "paper", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Languages_used_on_the_Internet#Internet_users_by_language", "anchor_text": "this table", "paragraph_index": 26}, {"url": "https://oscar-corpus.com/", "anchor_text": "OSCAR", "paragraph_index": 28}, {"url": "https://colab.research.google.com/drive/1M7pDk5bbZh_wB4GMtVjDqVG2l9hCK1Wk", "anchor_text": "here", "paragraph_index": 36}, {"url": "https://cims.nyu.edu/~sbowman/multinli/", "anchor_text": "Multi-Genre Natural Language Inference", "paragraph_index": 44}, {"url": "https://arxiv.org/pdf/1809.05053.pdf", "anchor_text": "here", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/1809.05053", "anchor_text": "arXiv", "paragraph_index": 48}, {"url": "https://cims.nyu.edu/~sbowman/", "anchor_text": "Sam Bowman", "paragraph_index": 52}, {"url": "https://huggingface.co/asafaya/bert-base-arabic", "anchor_text": "here", "paragraph_index": 55}, {"url": "https://arxiv.org/pdf/1911.02116.pdf", "anchor_text": "paper", "paragraph_index": 69}, {"url": "https://bit.ly/3irTX3y", "anchor_text": "here", "paragraph_index": 79}, {"url": "https://www.youtube.com/playlist?list=PLam9sigHPGwM27p3FQpLK1nt0eioiM-cq", "anchor_text": "here", "paragraph_index": 79}], "all_paragraphs": ["Up to this point, Nick and I have been writing tutorials almost exclusively on NLP applications using the English language\u2026 While the general algorithms and ideas extend to all languages, the huge number of resources that support English language NLP do not extend to all languages. For example, BERT and BERT-like models are an incredibly powerful tool, but model releases are almost always in English, perhaps followed by Chinese, Russian, or Western European language variants.", "For this reason, we\u2019re going to look at an interesting category of BERT-like models referred to as Multilingual Models, which help extend the power of large BERT-like models to languages beyond English.", "by Chris McCormick and Nick Ryan", "Multilingual models take a rather bizarre approach to addressing multiple languages\u2026", "Rather than treating each language independently, a multilingual model is pre-trained on text coming from a mix of languages!", "In this post and the accompanying Notebooks, we\u2019ll be playing with a specific multilingual model named XLM-R from Facebook (short for \u201cCross-Lingual Language Model \u2014 Roberta\u201d).", "While the original BERT was pre-trained on English Wikipedia and BooksCorpus (a collection of self-published books) XLM-R was pre-trained on Wikipedia and Common Crawl data from 100 different languages! Not 100 different models trained on 100 different languages, but a single BERT-type model that was pre-trained on all of this text together.", "There really isn\u2019t anything here that\u2019s trying to deliberately differentiate between languages. For example, in XLM-R:", "And yet, instead of predicting nonsense or having only the barest understanding of any of its many input languages, XLM-R performs surprisingly well, even compared to models trained on a single language!", "If your application is in another language (we\u2019ll use Arabic as the example from here on out), you can use XLM-R the same way you would regular BERT. You can fine-tune XLM-R on your Arabic training text, and then use it to make predictions in Arabic.", "However, XLM-R allows you to leverage another technique that\u2019s even more surprising\u2026", "Let\u2019s say you are trying to build a model to automatically identify nasty (or \u201ctoxic\u201d) user comments in Arabic. There\u2019s already a great dataset out there for this called \u201cWikipedia Toxic Comments\u201d with roughly 225k labeled comments\u2013except that it\u2019s all in English!", "What are your options? Gathering a similar-sized dataset in Arabic would have to be costly. Applying Machine Translation in some way could be interesting, but has its limitations (I\u2019ll discuss translation more in a later section).", "XLM-R provides another avenue called \u201cCross-Lingual Transfer\u201d. You can fine-tune XLM-R on the Wikipedia Toxic Comments dataset in English, and then apply it to Arabic comments!", "XLM-R is able to take it\u2019s task-specific knowledge that it learned in English and apply it to Arabic, even though we never showed it any Arabic examples! It\u2019s the concept of transfer learning applied from one language to another\u2013thus, \u201cCross-Lingual Transfer\u201d.", "In the Notebooks accompanying this post, we\u2019ll see that training XLM-R purely on ~400k English samples actually yields better results than fine-tuning a \u201cmonolingual\u201d Arabic model on (a much smaller) Arabic dataset.", "This impressive feat is referred to as Zero-Shot Learning or Cross-Lingual Transfer.", "Multilingual models and cross-lingual transfer are cool tricks, but wouldn\u2019t it be better if Facebook just trained and published a separate model for each of these different languages?", "That would probably produce the most accurate models, yes\u2013if only there was as much text available online in every language as there is English!", "A model pre-trained on text from only a single language is called monolingual, while those trained on text from multiple languages are called multilingual.", "The following bar plot shows, for a small selection of languages, how much text data the authors of XLM-R were able to gather for pre-training.", "Note that the scale is logarithmic, so there is roughly 10x more English data than Arabic or Turkish, and 1,000x more English than Yiddish.", "Different languages have different amounts of training data available to create large, BERT-like models. These are referred to as high, medium, and low-resource languages. High-resource languages like English, Chinese, and Russian have lots of freely available text online that can be used as training data. As a result, NLP researchers have largely focused on developing large language models and benchmarks in these languages.", "I adapted the above bar plot from Figure 1 of the XLM-R paper. Here is their full bar plot, which shows the amount of data they gathered for 88 of the 100 languages.", "The languages are labeled using two-letter ISO codes\u2013you can look them up in the table here.", "Here are the first ten codes in the bar plot (note that there are another ~10 languages after German with a similar amount of data).", "Note that this ranking of \u201cquantity of data\u201d does not match the rankings of how many users there are on the internet in each language. Check out this table on Wikipedia. Chinese (code zh) is number 21 in the bar plot, but by far has the most users (after English).", "Similarly, the amount of effort and attention given to different languages by NLP researchers does not follow the ranking in the bar plot\u2013otherwise Chinese and French would be in the top 5.", "There is a recent project called OSCAR which provides large amounts of text for pre-training BERT-like models in different languages\u2013definitely worth checking out if you\u2019re looking for unlabeled text to use for pre-training in your language!", "It\u2019s also possible to involve \u201cMachine Translation\u201d (machine learning models that automatically translate text) to try and help with this problem of limited language resources. Here are two common approaches.", "You could rely entirely on English models, and translate any and all Arabic text in your application to English.", "This approach has the same problems as the monolingual model approach. The best translation tools use machine learning, and have the same limitation around available training data. In other words, the translation tools for medium and low resource languages aren\u2019t good enough to be an easy solution to our problem\u2013currently, a multilingual BERT model like XLM-R is probably the better way to go.", "Approach #2 \u2014 Augment Training Data", "If there already exists a large amount of labeled English text for your task, then you could translate this labeled text into Arabic and use it to augment your available Arabic training data.", "If there is a decent monolingual model available in your language, and there is a large English dataset available for your task, then this is a great technique. We applied this technique to Arabic in one of our accompanying Notebooks and it outperformed XLM-R (at least in our initial results\u2013we didn\u2019t perform a rigorous benchmark).", "As you might imagine, XLM-R has a very different vocabulary than the original BERT, in order to accomodate 100 different languages.", "I\u2019ve published a Notebook here where I\u2019ve poked around XLM-R\u2019s vocabulary to get a sense for what it contains and to gather various statistics.", "The most commonly used task for evaluating multilingual models is called Natural Language Inferencing (NLI). The reason for this is that there is an excellent multilingual benchmark dataset available called XNLI.", "We\u2019ll look at XNLI in the next section, but here\u2019s an explanation of the basic NLI task, in case you aren\u2019t familiar.", "In NLI, we are given two sentences: (1) a \u201cpremise\u201d and (2) a \u201chypothesis\u201d and asked to determine whether:", "As I understand it, NLI is primarily a benchmarking task rather than a practical application\u2013it requires the model to develop some sophisticated skills, so we use it to evaluate and benchmark models like BERT.", "Benchmarking multilingual models on NLI is done with a combination of two datasets named \u201cMNLI\u201d and \u201cXNLI\u201d.", "MNLI will provide us with a large number of English training examples to fine-tune XLM-Roberta on the general task of NLI.", "XNLI will provide us with a small number of NLI test examples in different languages. We\u2019re going to take our XLM-Roberta model (which we\u2019ll fine-tune only on the English MNLI examples) and apply it to the Arabic test cases from XNLI.", "The Multi-Genre Natural Language Inference (MultiNLI or MNLI) corpus was published in 2018, and is a collection of more than 400,000 English sentence pairs annotated with textual entailment information.", "In MNLI, \u2018Multi\u2019 refers to Multi-Genre, not Multilingual. Confusing, I know! It\u2019s called \u201cMulti-Genre\u201d because it is intended as a successor to the Stanford NLI corpus (SNLI), which is composed entirely of somewhat simple sentences drawn from image captions. MNLI increases the difficulty of the task by adding multiple and more difficult \u201cgenres\u201d of text like transcribed conversations, government documents, travel guides, etc.", "This corpus contains 392,000 training examples, 20,000 \u201cdevelopment examples\u201d (test samples to use while developing your model), and 20,000 \u201ctest examples\u201d (the final test set on which benchmark scores are reported).", "Here are a few randomly-selected training examples", "\u201cXNLI\u201d stands for Cross-lingual Natural Language Inference corpus. The paper (here) was first submitted to arXiv in September, 2018.", "This dataset consists of a smaller subset of examples from the MLNI dataset which have been human-translated to 14 different languages (for a total of 15 languages, if you include English):", "XNLI does not provide training data for these different languages, so it\u2019s intended as a benchmark for the crosslingual approach that we will be taking here.", "For each language there are 5,000 test set sentence pairs and 2,500 development set sentence pairs.", "Sam Bowman at NYU was behind both the MNLI and XNLI datasets. XNLI was done as a collaboration with Facebook.", "Here are a few random examples from the test set for Arabic.", "We created two Notebooks for this post\u2013one for applying a monolingual model, and another for applying a multilingual model (XLM-R).", "For the monolingual approach, I used a community-submitted model, asafaya/bert-base-arabic, from here. The documentation for this model shows that it was pre-trained on a large amount of Arabic text, and that it has a high number of downloads in the past 30 days (meaning it\u2019s a popular choice).", "I fine-tuned this model with two different approaches.", "Approach #1 \u2014 Using a Small, Labeled Dataset", "We can use the small validation set (2,500 human-translated Arabic examples) from XNLI as our training set. That\u2019s a pretty small training set, especially compared to the ~400k examples in English MNLI! I imagine this approach is the most similar to what you could expect from trying to gather a labeled dataset yourself.", "This approach yielded an accuracy of 61.0% on the Arabic XNLI test set. This is the lowest score of the various approaches we tried (there is a results table in a later section).", "Approach #2 \u2014 Using Machine-Translated Examples", "The authors of XNLI also provided Machine-Translated copies of the large English MNLI dataset for each of the 14 non-English languages.", "This will provide us with ample training data, but presumably the quality of the data will be lower because the samples were translated by an imperfect machine learning model rather than a human.", "This approach gave us an accuracy of 73.3% on the Arabic XNLI test set.", "For the Multilingual approach, I fine-tuned XLM-R against the full English MNLI training set.", "Using the huggingface/transformers library, applying XLM-R is nearly identical to applying BERT, you just use different class names.", "To use the monolingual approach, you can load the model and tokenizer with the following:", "The rest of the code is identical. However, we did encounter a critical difference around parameter choices\u2026 We found that XLM-R required a smaller learning rate than BERT\u2013we used 5e-6. When we tried 2e-5 (the smallest of the recommended learning rates for BERT), XLM-R training completely failed (the model\u2019s performance never improved over random guessing). Note that 5e-6 is one-fourth of 2e-5.", "With this cross-lingual transfer approach, we got an accuracy of 71.6% on the Arabic XNLI test set. Compare that to the monolingual model fine-tuned on Arabic examples, which only scored 61.0%!", "The authors of XML-RoBERTa reported a score of 73.8% on Arabic in their paper in Table 1:", "The model in the bottom row of the table is larger\u2013it matches the scale of BERT-large. We used the smaller \u2018base\u2019 size in our example.", "Our lower accuracy may have to do with parameter choices like batch size, learning rate, and overfitting.", "Again, my intent with these Notebooks was to provide working example code; not to perform rigorous benchmarks. To really compare approaches, more hyperparameter tuning should be done, and results should be averaged over multiple runs.", "But here are the results we got with minimal tuning!", "For rows 2\u20134 of the table, you can further improve these results by fine-tuning on the Arabic XNLI validation examples. (I tried this quickly with XLM-R and confirmed the score went up to 74.2%!)", "Given that I was more easily able to achieve good results with arabic-bert-base, and knowing that it requires less memory (due to a smaller vocabulary), I think I would go with the monolingual model in this case.", "However, this is only possible because a team pre-trained and released a good monolingual model for Arabic!", "I was originally thinking to use Indonesian as my example language for this project, but", "For Indonesian, I would still want to try both approaches, but I suspect that XLM-R would come out ahead.", "The two Notebooks referenced in this post (one implements the Multilingual experiments and the other implements the Monolingual experiments) are available to purchase from my site here. I also provide a walkthrough of these Notebooks on YouTube here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5c3410ddd787&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@chris.mccormick?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris.mccormick?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "Chris McCormick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ab96a248d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&user=Chris+McCormick&userId=7ab96a248d6d&source=post_page-7ab96a248d6d----5c3410ddd787---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c3410ddd787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c3410ddd787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@dimitry_b?utm_source=medium&utm_medium=referral", "anchor_text": "Dimitry B"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1911.02116.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1911.02116.pdf", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Languages_used_on_the_Internet#Internet_users_by_language", "anchor_text": "this table"}, {"url": "https://oscar-corpus.com/", "anchor_text": "OSCAR"}, {"url": "https://colab.research.google.com/drive/1M7pDk5bbZh_wB4GMtVjDqVG2l9hCK1Wk", "anchor_text": "here"}, {"url": "https://cims.nyu.edu/~sbowman/multinli/", "anchor_text": "Multi-Genre Natural Language Inference"}, {"url": "https://arxiv.org/pdf/1809.05053.pdf", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1809.05053", "anchor_text": "arXiv"}, {"url": "https://cims.nyu.edu/~sbowman/", "anchor_text": "Sam Bowman"}, {"url": "https://huggingface.co/asafaya/bert-base-arabic", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1911.02116.pdf", "anchor_text": "paper"}, {"url": "https://huggingface.co/cahya/bert-base-indonesian-522M", "anchor_text": "cahya/bert-base-indonesian-522M"}, {"url": "https://bit.ly/3irTX3y", "anchor_text": "here"}, {"url": "https://www.youtube.com/playlist?list=PLam9sigHPGwM27p3FQpLK1nt0eioiM-cq", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5c3410ddd787---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----5c3410ddd787---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/multilingual?source=post_page-----5c3410ddd787---------------multilingual-----------------", "anchor_text": "Multilingual"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5c3410ddd787---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----5c3410ddd787---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c3410ddd787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&user=Chris+McCormick&userId=7ab96a248d6d&source=-----5c3410ddd787---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c3410ddd787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&user=Chris+McCormick&userId=7ab96a248d6d&source=-----5c3410ddd787---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c3410ddd787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5c3410ddd787&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5c3410ddd787---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5c3410ddd787--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5c3410ddd787--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5c3410ddd787--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris.mccormick?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris.mccormick?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris McCormick"}, {"url": "https://medium.com/@chris.mccormick/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "113 Followers"}, {"url": "http://ChrisMcCormick.AI", "anchor_text": "ChrisMcCormick.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ab96a248d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&user=Chris+McCormick&userId=7ab96a248d6d&source=post_page-7ab96a248d6d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F729901d8bde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-apply-bert-to-arabic-and-other-languages-5c3410ddd787&newsletterV3=7ab96a248d6d&newsletterV3Id=729901d8bde&user=Chris+McCormick&userId=7ab96a248d6d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}