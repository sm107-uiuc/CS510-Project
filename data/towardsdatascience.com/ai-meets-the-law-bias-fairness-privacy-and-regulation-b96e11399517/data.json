{"url": "https://towardsdatascience.com/ai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517", "time": 1683017840.2319438, "path": "towardsdatascience.com/ai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517/", "webpage": {"metadata": {"title": "AI meets the law: Bias, fairness, privacy and regulation | by Jeremie Harris | Towards Data Science", "h1": "AI meets the law: Bias, fairness, privacy and regulation", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "https://twitter.com/npbaldin", "anchor_text": "follow Nicolai on Twitter here", "paragraph_index": 5}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 5}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 120}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:", "The fields of AI bias and AI fairness are still very young. And just like most young technical fields, they\u2019re dominated by theoretical discussions: researchers argue over what words like \u201cprivacy\u201d and \u201cfairness\u201d mean, but don\u2019t do much in the way of applying these definitions to real-world problems.", "Slowly but surely, this is all changing though, and government oversight has had a big role to play in that process. Laws like GDPR \u2014 passed by the European Union in 2016 \u2014are starting to impose concrete requirements on companies that want to use consumer data, or build AI systems with it. There are pros and cons to legislating machine learning, but one thing\u2019s for sure: there\u2019s no looking back. At this point, it\u2019s clear that government-endorsed definitions of \u201cbias\u201d and \u201cfairness\u201d in AI systems are going to be applied to companies (and therefore to consumers), whether they\u2019re well-developed and thoughtful or not.", "Keeping up with the philosophy of AI is a full-time job for most, but actually applying that philosophy to real-world corporate data is its own additional challenge. My guest for this episode of the podcast is doing just that: Nicolai Baldin is a former Cambridge machine learning researcher, and now the founder and CEO of Synthesized, a startup that specializes in helping companies apply privacy, AI fairness and bias best practices to their data. Nicolai is one of relatively few people working on concrete problems in these areas, and has a unique perspective on the space as a result.", "Here were some of my favourite take-homes from the conversation:", "You can follow Nicolai on Twitter here, or follow me on Twitter here.", "Jeremie (00:00):Hey everyone, welcome back. Today, we\u2019re talking to Nicolai Baldin, who completed his PhD at the University of Cambridge studying machine learning and statistics, and who\u2019s now the founder and CEO of Synthesized.io, which is the whole reason I wanted to talk to him for this episode of the podcast.", "Jeremie (00:13):Synthesized is currently, one of relatively few companies focused entirely on reducing bias and improving fairness in AI systems, as well as improving user privacy in compliance with complex regulations like GDPR. Basically, they help companies anonymize user data and share data without the risk of users\u2019 identities being compromised in the process, or users\u2019 rights being infringed. Now, this is actually a really difficult problem to solve. And it intersects with philosophy, ethics and morality quite a bit, which is why Nicolai has to be well-versed in all those areas as you\u2019ll see in our conversation.", "Jeremie (00:43):One of the most interesting things about Nicolai and his work is that it\u2019s entirely focused on concrete applications of AI bias, AI fairness and privacy theory. Most of the conversations we\u2019ve had so far, and actually most of the work currently being done in these fields is still theoretical. It\u2019s about people figuring out how we should build systems in the future that won\u2019t infringe users\u2019 rights. And it\u2019s about making sure that systems are built safely and fairly in general. But actually, solving those problems in practice is a whole other kettle of fish. And that\u2019s a big part of what we\u2019ll be talking about today.", "Jeremie (01:11):Hopefully, you get as much out of this conversation as I did. I think the topic is fascinating. And I think Nicola is just the right person to talk about it. I hope you enjoy the episode.", "Jeremie (01:20):Hi, Nicolai. Thanks so much for joining me for the podcast.", "Jeremie (01:25):Cool. How are things going over there in the UK, by the way? Because I think we were talking earlier, you were mentioning that you\u2019re in a second round of lockdown starting soon.", "Nicolai (01:33):Yeah. Actually, the second lockdown starts this Thursday. So I\u2019m actually in the office right now. And that\u2019s probably the last time I\u2019m seeing all my colleagues face to face, and then they\u2019re going into their lockdown for four weeks initially. But yeah, let\u2019s see.", "Jeremie (01:48):Well, thanks for making the time to chat today. I\u2019m especially excited about the idea, the prospect of talking to you just because we speak to so many people on this podcast from academic research backgrounds, people worried about AI bias, AI ethics, AI safety. But it\u2019s quite rare to talk to people who are actually then having to implement these things in the industry and at scale. And I know that\u2019s what you\u2019re doing with Synthesized. So actually, maybe the first place to start here is, what are you working on right now? What is Synthesized? Can you give us a little bit of background on that?", "Nicolai (02:19):Sure. So the mission of Synthesized is to empower any data scientist, any ML engineer, any test engineer with a high volume of high quality datasets for development and testing purposes, staying compliant with regulations. And our vision is to build this so-called decentralized data access platform powered by machine learning to really facilitate innovation, again, in a compliant, privacy-preserving manner.", "Jeremie (02:40):And what do you mean by decentralized? How does that enter into the privacy equation?", "Nicolai (02:47):It\u2019s probably something, well, I would say complimentary to the privacy question. So it\u2019s more about the efficiency game. So we\u2019ve seen lots of centralized solutions such as Databricks, such as Snowflake, and we really built something which is, in a way, decentralized. And is not, I would say, forcing clients to move into a certain say, category, into a set of, I would say, databases or data warehouses, data lakes. It\u2019s really in a way, a decentralized solution which is able to work with an arbitrary number of data sources, again, databases, data warehouses, data lakes.", "Jeremie (03:21):It feels like one of the advantages of that from a privacy standpoint would be that if you\u2019re decentralized, if you\u2019re doing more computation at the source, that means data doesn\u2019t have to fly around as much as usual. So there are fewer opportunities for leaks. Is that part of the idea?", "Jeremie (03:37):Okay, sweet. Cool. Well, so I think that background is helpful just for people to understand, Because you\u2019re doing all this stuff, because you\u2019re moving a lot of very private data around. You\u2019re also based in the UK. And I think as we\u2019ll get into, there are some policy implications in terms of regulatory compliance, that European firms face that aren\u2019t necessarily quite as common in North America.", "Jeremie (03:57):So I think there are a whole bunch of different things that you\u2019re actually sinking your teeth into, from a practical perspective. GDPR is probably the most famous regulation, and I think that\u2019s a good place to start. You have a lot of experience with it. Can you introduce what GDPR is, and what compliance with GDPR requires?", "Nicolai (04:13):Yeah, absolutely. I think maybe before I do that, so let me paint maybe the bigger picture.", "Nicolai (04:18):I think the world has become data-driven with terabytes of data being collected, analyzed and stored and all this. And needless to say, as we discuss safety, privacy, ethical use of data, have become key elements of any data-driven company. Only recently have the companies started to adjust in order to comply with the new regulations. And I would say it\u2019s not surprising that we got to this stage, given the amount of inefficiencies which many companies have, and also the speed with which the digital world has been growing.", "Nicolai (04:50):And well, essentially, GDPR for \u2026 well, I think we have technical audience here. And GDPR again, well, it\u2019s a regulation on data protection and privacy in the European Union and the European Economic Area. It was introduced in May 2018. And I think still considered as, without exaggeration, the strongest set of rules on again, data protection and privacy, which is meant to enhance how people can access information about them, and also places limits on what companies can actually do with the data.", "Nicolai (05:25):Again, maybe to give you an example, well, GDPR, essentially well, controls how websites, companies or organizations are allowed to handle personal data, which is anything from names, well, email addresses, location data, browser history and many other things. And again, maybe I\u2019ll go on a bit more in detail. So basically, the GDPR has I would say, key two elements. So it\u2019s data controllers and data processors. So data controllers, essentially, they exercise the overall control over the purposes and means of the processing of personal data, whereas the processors essentially act on behalf. And only, it\u2019s very important, only on the instruction of the data controller.", "Nicolai (06:11):It may sound I guess, a little bit technical, but just to give an example. So if you have, say, an AI service, then basically if you put some data in it, and you would like to essentially get some output, some result from the AI service and ML ops or some amalgam engine, which can be accessed online, then when you essentially upload your data to the service, you become the data controller. And when you essentially ask the service to do something to give you a result, say maybe about some insights about hidden data, the software, the service becomes the data processor. So that\u2019s essentially like a clear \u2026 well, an example of these two entities.", "Nicolai (06:53):In the US, you have obviously some, well, similar data protection legislation, such as HIPAA, CCPA. But the question is, why do we have it in Europe? And what\u2019s the purpose? And it was introduced to essentially protect the privacy of EU citizens. And I think it\u2019s also important to emphasize that before GDPR, many companies had their own data registrations, data protection acts. So Germany had its own. UK, we had something else, and France, Spain had different legislations with regards to data privacy.", "Nicolai (07:32):And the idea of GDPR was to really unify, to really harmonize all these legislations and put them together. So now, we have only say, 99 articles, which essentially the guidelines, which form the guidelines on how companies should essentially manage and store data about people. And also essentially, well, outlines all the rights which people have about the data they, well, give away to companies.", "Jeremie (08:01):And so, what are some of the biggest restrictions on companies\u2019 ability to use data that come out of the GDPR?", "Nicolai (08:08):Right now, again, it really places limits on what companies can collect about people. And again, if they exercise, if they become essentially, the data controllers, they have to comply with the given set of rules. And again, those vary from essentially what they can do with the data. And they also have to be able to, in a way, tell the users what the data is used for, and also be able to remove the data when it\u2019s actually asked by the users. And there is I think, eight rights.", "Nicolai (08:40):Again, I would say the GDPR was designed not only to place, I would say the tools, unlike the data controllers and processors, but also in a way to protect the rights of, well, the users\u2019 right of data of the users. And that means again, which varies from okay, how companies will access original data, but also again, ensuring that companies can delete them. You can request essentially some of the data to be deleted, the so-called right to be forgotten. And there are, I think eight rules outlines, laid out by GDPR to essentially protect the rights of individuals as well.", "Nicolai (09:19):I think in all, there are definitely some pros and cons. And what we\u2019ve seen, again, working with many companies is that definitely one of the pros is that, I guess the users\u2019 privacy is protected and people have obviously, control over their personal data. But also, one of the cons which we saw is that because still, companies are trying to understand whether they are data controllers or data processors, and also trying to understand how to essentially implement those rules, as a result of that, the innovation has slowed down.", "Nicolai (09:51):And well, maybe again, just to give you an example, with insurance companies we worked with, we used to work for about a year ago. So it may be took us up to three months essentially, for them to decide how they\u2019re going to be, well, uploading their datasets into our platform, even though it was on premise, even though it was essentially the data which they already had consent to use. But at the same time, they were still struggling to understand again, how they\u2019re going to be using the datasets with our platform.", "Jeremie (10:21):It\u2019s interesting how these issues come up, because you see similar debates around, for example, the US tax code, just how it\u2019s this giant Frankenstein monster of a document. There\u2019s no way that any human being can plausibly understand and read the tax code. And yeah, it ends up being this big hurdle.", "Jeremie (10:41):I guess that\u2019s one objection to the way GDPR has been implemented. Are there other issues that you found when you\u2019re helping companies enforce it or develop infrastructure to allow them to apply GDPR? Are there other big gaps that you notice in the legislation that seemed maybe unnecessary?", "Nicolai (10:59):I think maybe there are still \u2026 well, companies are still trying to, I guess, figure out whether they are data processors or data controllers. And the reason being is that it\u2019s quite a big difference in terms of what companies have to essentially be doing. And the boundaries, again, the line is very thin. So it\u2019s, essentially, we\u2019ve seen again, many companies essentially switching from being a data processor to data controller because they just realized okay, we\u2019re actually a data controller with regards to, say, a new feature which we are rolling out.", "Nicolai (11:29):So essentially, what might happen is that as you develop new features on the platform, as you\u2019re rolling out to the users, your status may change. And it\u2019s very important to essentially ensure that okay, you have all the practices and all the rules to comply with this new \u2026 well, with this change as well.", "Jeremie (11:48):So depending on context, a single company could have data processor responsibilities in one context and then take the controller responsibilities in a different one? That\u2019s how it breaks down?", "Nicolai (11:58):Exactly. And I remember again, we had this discussion with one of our, well, lawyers and external law firms. And they were essentially sharing that it\u2019s actually a big debate right now among the AI companies, which is, again, are they data controllers? Are they data processors? And what are the obligations and all of that? So again, there is no silver bullet, and you really have to go really deep to understand some of the examples as well.", "Nicolai (12:26):And I think even on the ICO website they\u2019re right now, with regards to, especially machine learning and artificial intelligence, they\u2019re trying to, in a way, show some examples [inaudible 00:12:35]. In this case, you\u2019re becoming a data controller, in this case, you\u2019re a data processor. And you realize that there\u2019s quite a lot of ambiguity when even on the well, official website, you have examples of again, this is company, this is data processor and this one is data controller.", "Nicolai (12:50):And really emphasizing okay, if you essentially train this neural network, and if you essentially train it in an isolated environment but you don\u2019t really have any access to, well, I would say change in some of the parameters, I would say in some scenario, you may become essentially, a data processor. Whereas if you essentially exercise some additional control over the data, over the neural network, you essentially become the data controller. So it\u2019s kind of [crosstalk 00:13:17].", "Nicolai (13:17):So it\u2019s quite a thing. And again, many AI companies in the UK and Europe have spent quite a bit of time to figure out, how to essentially comply with the new regulation.", "Jeremie (13:30):Just because you said it, I remember for my startup, when we were \u2026 GDPR came out, I don\u2019t know, 2016?", "Jeremie (13:37):Something like that. Or 2017. We went through this phase of freaking out, trying to figure out, which case are we going to fall into? And what our lawyer told us in one case was like, \u201cYeah, actually, no one knows yet. We\u2019re waiting for the first precedent to be set by actual courts that are going to have to rule on this.\u201d And then everybody\u2019s just navigating this uncertainty until there\u2019s that precedence where you actually don\u2019t have that much to go on in terms of determining which camp you fall into, kind of interesting that this data processor, data controller thing is so ambiguous.", "Jeremie (14:09):And it sounds like there\u2019s also a mapping then on to different parts of the data science lifecycle. So like, if you\u2019re doing just hyper parameter tuning, maybe you\u2019re just a data processor. But then if it goes into feature engineering or data cleaning, then maybe that\u2019s more data controlling. Or is that what might happen?", "Nicolai (14:28):Exactly. So it really depends on again, how exactly you use the data in the data \u2026 well, ML development cycle. And again, if it\u2019s a simple algorithm, not even necessarily a stochastic ML algorithm. Not even an ML model if it\u2019s just, again, something simple, deterministic, then you may get away with essentially the data processor.", "Nicolai (14:52):But at the same time when you exercise again, additional control, when you exercise again, some \u2026 you would like to do some sophisticated features using model selection, model \u2026 well, validations of that, you\u2019re likely to become essentially the data controller because there\u2019s going to be some exposure of the datasets to the ML team within the business. So at that stage, you\u2019re likely to become the data controller.", "Jeremie (15:17):And do you think there\u2019s enough value from GDPR, from a privacy standpoint, to justify the added complexity on companies? I know it\u2019s a tough question to answer, probably, but do you have an instinct there?", "Nicolai (15:29):Absolutely. I think we\u2019re going to touch on them. Yeah, I want to touch on a few things here. But one of the things is definitely different data, well, user agreements. Because when you essentially start using, say, an AI service, you get this big, well, user agreement, right?", "Nicolai (15:49):And pages. And you need to go through it and understand how your data is going to be treated. GDPR, where it\u2019s actually helpful is that those data agreements now have the same structure, and in a way that you know that privacy is by default. And you know that some of the elements already are meant to be covered by GDPR, and there is no way how you can essentially say, \u201cHey, man, I\u2019m going to forget about the GDPR and I\u2019m going to allow you to do something.\u201d Because again, it\u2019s impossible.", "Nicolai (16:21):So again, unlike maybe the US and other countries in the European Union, privacy is by default, which is very important. So I think the main benefit is that we can now protect, well, the privacy of EU citizens, which is the main goal, right?", "Jeremie (16:42):Yeah. I feel like this sort of thing is probably \u2026 my intuition is it\u2019s going to become more important over time. If only because, if you are giving away your name, your email, your date of birth to Facebook in 2007, I think you\u2019d have a reasonable expectation that there isn\u2019t that much they could do with that data, because the state of machine learning just wasn\u2019t that advanced at the time.", "Jeremie (17:05):But now, we\u2019re starting to find that you can pull out so much more than anyone ever thought, both in terms of compromising privacy and in terms of just making really good predictive models. So it\u2019s almost as if like, giving away the same amount of information implies giving away more power, more rights, more privacy today than it did back in the day. Is that a fair assessment?", "Nicolai (17:28):I think it\u2019s definitely fair. And maybe to add to that, again, I like to say, \u201cHey, man, there is no way that in two to three years, people are going to say, \u2018Hey, we don\u2019t care about privacy, we don\u2019t care about algorithms being fair, we don\u2019t care about data being unbiased.\u2019\u201d And to be honest, we expect only more regulations to come into this area. And again, we believe that data should be treated ethically and in a privacy-preserving manner.", "Nicolai (17:54):And because again, also, it\u2019s interesting that we have inadvertently created quite a bit of \u2026 well, I don\u2019t really like the word, but hype around the company as well. And I think it\u2019s a clear indication of how important this topic has become in the society. And we definitely expect more regulations to come into this play. And, again, GDPR at the moment, we focus primarily on privacy, but the topics of biases and the topics of fairness are likely to be covered very soon as well.", "Jeremie (18:25):And just to actually play devil\u2019s advocate on this, because I remember I had a friend in grad school who got \u2026 this is back in 2015, he got a ping from Google every Friday, telling him what the traffic was like between his office and the sushi restaurant he\u2019d like to go to every Friday. And I turned to him, I was like, \u201cDude, is this not creepy? Why would you want this on your phone?\u201d", "Jeremie (18:51):And he goes, \u201cWell, you know what? I\u2019ve kind of gotten used to it. It\u2019s a convenience that I like.\u201d But do you think there\u2019s a possibility that instead of our legislation making privacy more possible by basically just being firmer on these companies, that instead we\u2019re going to see a cultural shift in the direction of less caring about privacy?", "Nicolai (19:12):I don\u2019t think we are going to see, again, people caring less about privacy. And by the way, what we\u2019ve definitely seen is that many companies, especially in the data space, they like to say, \u201cHey, we\u2019re going to help you unlock data.\u201d So we believe that data should stay locked away or locked down, which is becoming quite popular in the UK and other European countries.", "Nicolai (19:37):There is a big difference between unlocking data and unlocking data\u2019s full potential. And in a way, so the question is, how can we keep original data locked away, but at the same time, build better services and enable them, well, provide them to users, which is not a goal of many, many companies including us? And we really built \u2026 so I want to touch on it as well, but the pattern we\u2019ve built, it really allows to facilitate and speed up innovation by keeping original data locked away. And I think this is extremely important to distinguish these two things. Because again, original data has to stay locked away.", "Nicolai (20:14):And we need to essentially bring privacy back to the \u2026 well, back to the users, to ensure privacy. But at the same time, how can we enable better services to be built, better models to be served to customers? The fraud detection models, models in healthcare? So again, all those models, they require datasets. And yeah, the question is, how do we essentially ensure the innovation is still going, but at the same time, well, we respect the privacy, we respect the data protection of this nation?", "Jeremie (20:45):And when you say, keeping the data locked away, I guess there are a couple of different ways I could imagine that applying. On the one hand, only the company that I\u2019ve explicitly given access to my data, gets to see that data. And I guess there\u2019d be a whole bunch of questions about third party authorization. Like maybe I need to use AWS to train my model, so I have to send my data to AWS.", "Jeremie (21:10):And then there\u2019s a separate question about, I gave you my data, like the Facebook example we were talking about. I give your company my data, with the understanding that technology only allows you to do a certain number of things with it. But then deepfakes comes up, and all of a sudden my photos on Instagram might turn up in a pornographic video or something. And had I known that was a possibility, I never would have given my data away to this company in the first place, or that kind of data.", "Jeremie (21:39):These seem like two different issues. So at the moment when you say, keeping the data locked away, I guess that has more to do with the first case right, just sending it out to other parties?", "Nicolai (21:49):Yeah, absolutely. So we really want to ensure that, again, if you\u2019re talking about healthcare, we\u2019re talking about say, financial services, we don\u2019t really want our personal data be essentially flying around within the bank or within a hospital, because it\u2019s obviously very sensitive data points. But at the same time, again, the question is, we still want to get better services to the public. And that\u2019s the puzzle we need to solve. And there is a trade-off between those things.", "Nicolai (22:22):And I think one of the things again, we\u2019re going to be talking about, but yeah, what we\u2019ve designed is that it allows companies, and yeah, we work with some of the insurance companies, some of the banks in the UK and Europe, it allows them to innovate much faster. But at the same time, staying compliant with regulations such as GDPR. And we really expect actually banks to make more investments into this area. And not only the banks, but again, healthcare institutions, insurance companies, governments. Because again, there is no way how it\u2019s going to be less. Well, we believe it\u2019s going to be even stricter.", "Nicolai (22:57):And definitely, if there is time to invest in this, then differently, now is the right time because the sooner the better. And if you invest in this right now, then basically in two or three years, you can become, well, more competitive, because you can essentially invest in something else when other companies are going to be investing in privacy, fairness and other things as well. So it\u2019s definitely happening right down.", "Jeremie (23:22):Actually, speaking of privacy and fairness, things like that, you\u2019ve written a lot about algorithmic bias, data bias, that sort of thing.", "Jeremie (23:30):Maybe it\u2019s worth touching on that. Actually, yeah. Can you first introduce those concepts, and the distinction draw between the two?", "Nicolai (23:38):Yeah, absolutely. So I guess, well, the main difference, again, two things. So it\u2019s algorithmic biases and biases in data. And typically, again, the public, we get exposure, well, to some algorithms. Say, when we applied for credit, we get a credit score when we go to say hospital. Well, sometimes again, some of the decisions are made by machines, and some of those machines are deterministic. Some of them are stochastic.", "Nicolai (24:05):And typically, if it\u2019s a deterministic system, again, if it\u2019s, say, a recommendation system or if it\u2019s a credit scoring algorithm, they basically implement some rules. And again, imagine, say, a rule-based system. Very simple one; say, in the insurance company, in a hypothetical insurance company. So if I essentially apply for a credit and my age is higher than the threshold, then I\u2019m going to have a certain score. And this is a simple algorithm.", "Nicolai (24:35):Well, this is a bias. This is an algorithmic bias. And well, this simple algorithm, it\u2019s deterministic. And well, it\u2019s a discrimination of course. And because age is a legally protected attribute in the UK, so you cannot really do that. And so that\u2019s illegal.", "Nicolai (24:53):Bias in data is something else. Biased data is independent of the algorithms, and whether they are deterministic, whether it\u2019s based on machine learning or some other techniques. And essentially, bias in data is imagine again, I\u2019m playing hypothetical insurance company scenario. And they have this delinquency score. And they have, say, other attributes in the dataset such as age, such as gender, race and many other things.", "Nicolai (25:22):So the bias in data is when you basically condition on the subgroup of those sensitive variables, and you essentially compare the distribution over the delinquency with the distribution over the entire dataset, over the delinquency when they essentially don\u2019t do any conditioning. Then essentially, we get the bias when there is again, skewness in the distribution, conditional distribution, when they essentially focus on the given minority class. And that\u2019s, again, the bias in data.", "Nicolai (25:55):Well, if the algorithm which is ML algorithm, is taken in this dataset, then essentially, the results of this algorithm are also going to be biased, even though it\u2019s a ML. And essentially, ML is, well, essentially designed to find those biases. Then you know that again, this ML algorithm is biased. And hence, I think it\u2019s very important to look at probably three main things. Again, it\u2019s how the dataset was collected? What\u2019s the quality of the dataset? And also important to benchmark this dataset against a larger population. Because it may happen that not only this subset of this dataset is biased, but it may happen that the entire dataset is biased. So it\u2019s really impossible to understand how to essentially find those biases within this, because all of it is biased.", "Nicolai (26:50):And this is again, another thing. By the way, at Synthesized, we are releasing the platform to the public, well, in the middle of November. So probably after or, well, before this. The platform is going to be essentially free access, and it\u2019s going to allow any data scientist, any ML engineer, any test engineer, upload a dataset, identify all biases in data and also mitigate those biases. So we\u2019re going to be essentially assigning the so-called fairness score to the dataset. And we\u2019re also going to be saying, \u201cOkay, hey, here are all the biases we found and here is how we can essentially mitigate them.\u201d So we\u2019re going to be doing that as well as part of the platform release. And we really want this to be used by HR departments in big organizations. We really want this to be used by institutions, by academics.", "Nicolai (27:41):And we touched on this previously. So I would say many companies right now, and many people especially, well, in different academic circles, like to talk about biased system fairness. Very few companies do actual work. And for us, it was very important to enable this service so that again, even not necessarily a technically skilled, technically savvy person can essentially upload the dataset and understand those biases and fairness, can essentially, well, mitigate those things and essentially get a better, well-balanced dataset.", "Nicolai (28:16):I can touch on this as well, but we essentially designed the so-called rebalancing technique, which is able to essentially correct those biases in data and make sure that the algorithms are going to be fair as well, which are built using those datasets.", "Jeremie (28:31):I think it\u2019s a really fascinating problem to tackle, partly because it\u2019s so unclear what the word bias and what the word fairness means. There\u2019s implicitly a prior whenever we talk about bias or fairness. You just said, for example, a dataset is biased or a subset of a dataset is biased if let\u2019s say, the average number of credit defaults in a group of red haired people is different from the average number of credit defaults for the broader population. We then conclude, oh, that\u2019s biased.", "Jeremie (29:05):Whereas you also said, wait, but machine learning algorithms, all they do all day is latch on to that kind of bias. That\u2019s where all their value comes from. And so, how would you define \u2026 maybe you just use a strict legal definition, so protected classes like gender and age, those are strictly just biases. But I guess philosophically, do you have a sense of where you would put that dividing line between like, \u201cOh, this is a bias\u201d versus, \u201cNo, this is a useful feature or a useful trend or pattern that we can actually base our algorithm on\u201d?", "Nicolai (29:38):I think as you rightfully mentioned, all datasets are biased.", "Nicolai (29:44):And machine learning is essentially meant to, well, train to essentially find those biases. The question is, again, what does it mean? And is the bias against, say, legally protected set of attributes? That\u2019s, again, the main issue. So we\u2019ve seen people talking about, say, good discrimination versus bad discrimination. I believe that there is no \u2026 well, we can\u2019t really separate those things. Again, discrimination is when the bias is against the legally protected attributes. So like, again, sex, age, race, and many other things.", "Nicolai (30:18):And if essentially, the bias is against those legally protected attributes, then it\u2019s discrimination and it\u2019s illegal. Whereas of course, there are many other attributes. And of course, there is some skewness and there is difference in terms of the distribution over those subsets of data, again, versus the overall distribution, the overall dataset. And if those attributes are not, well, obviously safe to use, then this is what machine learning is meant to be doing. And we have, as we discussed again, discriminative models which are essentially meant to find those things.", "Nicolai (30:55):And again, if we are not really focusing on those sensitive attributes, then again, those biases are essentially used to train machine learning models. But we need to be really careful with making sure that we, in a way, treat, well, legally protected attributes, well, respectfully. And this is what by the way, we are doing at Synthesized as well.", "Jeremie (31:21):It\u2019s really interesting how different parts of this problem are abstracted away by different groups. The government is in charge of determining like okay, these are legally protected classes. So anything that leverages these attributes is performing some kind of bad discrimination. Whereas implicitly, everything else is \u2026 It strikes me that often, when I talk to people who are working in government, legislating these things, there\u2019s this subtext that we don\u2019t really fully know what we\u2019re doing, partly because you don\u2019t tend to see a lot of high caliber data scientists in government, just because of the way the incentive landscape works.", "Jeremie (31:58):You can make a lot more money working in the private sector. And so it\u2019s one of these \u2026 I don\u2019t know. It just feels like one of these industries where you face that uphill battle against incentives right out the gate. You\u2019re wrestling against the best ML engineers at Google and Facebook and Amazon, and all you have is the resources of like the EU, which obviously is really big, but at the same time, there are always going to be these nuances that people can hide different applications of data in weird places, especially if they know what they\u2019re doing.", "Nicolai (32:29):Absolutely. I would even say that there is the entire guess area in statistics and machine learning around sufficient statistics. And sometimes it\u2019s possible to, in a way, hide those biases in again, like other attributes as well, which in a way, correlated with those biases. And even though again, those attributes are not sensitive, say, I don\u2019t know, like location, but it\u2019s sometimes possible to again, link them. And it\u2019s very important.", "Nicolai (32:55):And this is what we\u2019ve done a lot, spending quite a bit of time on is to ensure that those things essentially become sensitive as well. There is these legally protected attributes, which are protected by the well, data protection laws. And then we have some other derived attributes from those things. And it\u2019s very important to, well, monitor them and also ensure that there is no kind of bias, there is no discrimination against those things. It\u2019s not easy, and this is what we\u2019ve spent quite a bit of time on that at the company. Well, at Synthesized as well.", "Nicolai (33:27):But also, well, I guess the important question is, how do we incentivize other companies to do the same? So, how do we incentivize companies to ensure that the datasets are unbiased, that the algorithms are fair? Again, we believe that there is big outside incentives for companies to self-regulate, because again, it\u2019s essentially bad reputation amongst clients if the clients of a business know that this company takes fairness and biases extremely seriously when making decisions. And also, it\u2019s a very good reputation for current and future employees.", "Nicolai (34:05):And, again, because people actually started looking [inaudible 00:34:10]. And if they know that this business is really careful with all these issues, then it\u2019s definitely good for their reputation as well. Another thing which is quite \u2026 it\u2019s also inevitable that companies are going to \u2026 well, the regulations are going to be stricter and stricter, and companies are going to invest more capital into these features. And it\u2019s really important to start investing into this area right now, because it might be a little bit too late, say in two to three years. Because again, there are going to be many other things to essentially invest in. And it\u2019s really important to essentially design the technologists which take fairness and biases at its core, as opposed to just trying to fix stuff which they\u2019re developing right now.", "Nicolai (34:56):So just to give an example, say a data warehouse or data lake, or say data infrastructure layer. So imagine a big insurance company right now design something without thinking about privacy, biases, fairness. And then two, to three years they realize, \u201cOkay, so how can that \u2026 right now, all this entire infrastructure is not compatible with the data protection law.\u201d So it\u2019s very important when designing those big systems to treat privacy, biases, fairness, and incorporate them. Well, the relevant tools at the core of the technology. So really ensuring that we are going to be compliant with best practices.", "Nicolai (35:37):So it\u2019s definitely a real important sector, an area to invest for many, many businesses. And we are talking about, again, like insurance companies, banks, healthcare institutions, governments. We really expect data architects, data engineers and chief data officers, chief technology officers to start looking into this area, well, more and more. So that\u2019s definitely, definitely coming.", "Jeremie (36:02):Is there a question? Because from what I remember of GDPR, again, from panicking about it back in the day, there\u2019s a pretty big scale dependency for the companies. Companies at a certain scale have obviously, higher reporting requirements and higher enforcement requirements, so on. Can you speak to that a little bit? When do a lot of these things kick in?", "Nicolai (36:21):I would definitely say that, also thinking about the growth of the companies, to start looking into this as soon as possible. Because again, hey, if you have growth potential and well, today, you have say, I don\u2019t know, 100 employees, 100 data scientists, in the next two years, this number might double. And it may triple. So it\u2019s very important to really incorporate these principles at the beginning.", "Nicolai (36:46):And, it\u2019s also not very hard. Because without solutions, so what you\u2019ve designed actually, so it enables companies to become in a way, GDPR compliant in just under three to five days. So you can just say, \u201cHey, man, you just have this entire data access platform and that\u2019s it.\u201d So we essentially take all the risks away and make the company efficient again. And there is not only a huge risk-saving, but it\u2019s also productivity gain. It\u2019s also saving some of the data engineering because you know that you don\u2019t really need to spend time on your data engineers, your [inaudible 00:37:21], your, well, legal departments to look into this area, because it\u2019s all ready by default. So the privacy, the fairness is by design of the system.", "Nicolai (37:32):And again, it\u2019s many interesting solutions exists in the market, and we are definitely pushing into this area of the game like it\u2019s actually possible to become GDPR compliant, and essentially making sure that those principles, and it\u2019s at the core of the technology, right, as opposed to trying to fix the past, we\u2019re really making well, allowing the companies and businesses to think about the future, and how the entire again, like the market is going to develop and really incorporate best practices into the data infrastructure right now.", "Jeremie (38:05):Yeah, I guess one of the challenges of doing this too, is it seems like a one size fits all approach would be kind of challenging to pull off. Because let\u2019s say I secretly am Facebook, and I secretly want to use your age to decide which ad to show you. And I shouldn\u2019t be able to do that by GDPR, or whatever other regulation, I can always like leverage a whole bunch of other proxies, as you mentioned, like there are things that are related to age, but that I could still defend as having to use them in the context of some other totally valid use case. And from one company to the next, figure out where to draw that line seems like it would be just so hard for so many features, let alone interactions between features.", "Jeremie (38:43):Because, if I have one feature that tells me, 10% of the information about your age, and then I\u2019ve got another feature that tells me, overlaps, like 30%, and then another, eventually, I could use those, but any one of those features could seem totally defensible on its own. How do you think about that problem? And how do you approach it if you do at this stage with with companies?", "Nicolai (39:05):Yeah, absolutely. And yeah, that\u2019s, that\u2019s why I kind of feel like we\u2019ve spent more than two years on developing the platform. And it\u2019s really ensuring that it\u2019s not only we check for the sensitive attributes, but also we find all the things where kind of the bias can propagate. And again, like the simple examples, location, right, even if you control the, all the legally protected attributes, it\u2019s still possible to get some leakage of the kind of bias into the location and so it\u2019s important to check for those things. And yeah, this is what we have been doing as well. And there are like various ways how to do so; like again some correlation analysis, some again like you can do some modeling as well. See, okay, how much I can understand [inaudible 00:39:44] importance when building models and see again, like very slight kind of correlation between different elements in their data set as well. And yeah, this is what we\u2019ve been focusing on a lot recently.", "Jeremie (39:56):Do you think that the big picture questions, like from a philosophical standpoint, is actually answerable? Do you think it\u2019s actually possible to philosophically go through and say, you know, we\u2019re going to pull out all of the age dependency in this data set? I guess the reason I\u2019m asking is, it seems like even end users might not want that. It seems like an almost irreducibly fuzzy problem, but maybe my instincts are off on this.", "Nicolai (40:27):Yeah, absolutely. So I think you\u2019ve touched on a very important issue, which is, the users may well sometimes may want to sacrifice some information just to get better services. Again, like healthcare, we\u2019re talking about insurance, business, finance. I mean, to be honest personally, I\u2019m okay with giving my data away. If it helps to essentially build better models and better solutions. Even I understand that unfortunately, many healthcare institutions they don\u2019t really know how to stay compliant, well, still kind of figuring out how to stay compliant with GDPR and many other well, with HIPA in the US and CCPA. Am I okay with giving my data to the healthcare? Oh yes, definitely. I mean, if it helps Bob with better solutions, absolutely. Please, please do it.", "Nicolai (41:18):In this case, we can actually see that even if I\u2019m given consent, it\u2019s still difficult to essentially use my data. But yes it\u2019s definitely an important issue.", "Jeremie (41:32):Are you generally optimistic about, from a policy standpoint, our ability to keep up with this stuff and roll out policies that actually makes sense for end users and for companies? I\u2019m thinking, especially as the technology gets more and more advanced. We\u2019ve talked about these deepfakes and unlocking all these new applications, some of which are disturbing and shocking of old data. I expect this is going to continue, but do you think policy is going to be able to keep up with this in the coming decades?", "Nicolai (42:03):I would say it\u2019s inevitable. The question is when? But yeah. So definitely. Again, like right now, even in terms of fairness and biases, again, the regulator started to look into this. And again, like many, I think it\u2019s also very, very important to distinguish between external policies and also internal \u2026 well, data governance policies within the bank, right?", "Nicolai (42:26):And with bigger banks, so they actually have much stricter policies than say, GDPR when we talk about data privacy. And when GDPR came into force two years ago, many banks had already had much stricter policies internally.", "Nicolai (42:42):And it\u2019s more about, \u201cOkay. Yeah, that makes sense.\u201d And many forward-looking businesses have already started investing into the obvious issues around privacy and bias, especially in the insurance space, especially in the healthcare space. Governments as well. So it\u2019s definitely something which is very important, because those guys are going to be \u2026 well, are going to be accountable. So especially, if you\u2019re talking about the government and healthcare, for any discriminatory decisions they would make.", "Nicolai (43:11):And even if the regulators are not there, there is going to be some reputational risk, there is going to be some other issues involved. And hence, it\u2019s very important to even independently, of forthcoming regulations to start investing into this area because even \u2026 Yeah, I think you\u2019re right, so that the public is becoming more and more aware of the stakes. And it\u2019s essentially driving the innovation within the businesses. We have many banks right now actually investing a lot, well, in fairness and well, privacy-preserving technologies and really developing internal solutions as well.", "Nicolai (43:46):And the banks in the US as well, even though GDPR doesn\u2019t apply. But banks understand that it\u2019s inevitable. So it\u2019s not really a question right now. And again, it\u2019s no way people are going to say, \u201cWe don\u2019t care about data being unbiased, we don\u2019t care about algorithms being fair.\u201d So it\u2019s definitely, definitely coming. And it\u2019s very important to start looking into this very seriously.", "Jeremie (44:12):Interesting how the role the private sector is playing on anticipatory, almost pre regulation of their own activities. It\u2019s an angle that seems like it\u2019s going to be pretty relevant, especially since these private sector companies are going to be the ones coming up with the algorithms. They\u2019ll know what the future looks like in many cases before government.", "Jeremie (44:31):Awesome. Well, thanks, Nicolai. This is a lot of fun. Actually, before I let you go, I just want to ask, do you have any links that you can recommend for people who want to learn more either about Synthesized\u2019s platform. Or I know you\u2019ve done a lot of blogging too, on bias and fairness in AI on your website, so maybe that\u2019d be a nice link to share.", "Nicolai (44:49):Yeah, absolutely. So we do publish a lot about fairness, biases, algorithmic biases, biases and data, also about privacy-preserving technologies. And our focus is both on essentially, data scientists and engineers, but also on well, C level executives within a big corporate. On policymakers as well. So we try to well, I would say approach this problem from both angles. So one of the things we briefly touched on is the so-called synthetic data. So we essentially write a lot about it.", "Jeremie (45:21):Okay. So one other area I did want to ask you about is the whole area, a rich area of synthetic data. And it\u2019s something that I think I\u2019ve heard a lot of talk about. I\u2019ve heard a lot of companies discussing, \u201cMaybe we can take this approach.\u201d I\u2019d love to get your sense of first off, what is synthetic data? And then what\u2019s its overlap with privacy and with regulation?", "Nicolai (45:43):Absolutely. Well, whilst I would say there is lots of different definitions of synthetic data, so it\u2019s very important to, I guess, well define what we mean by truly synthetic data, even like synthesized data. So in a way, it\u2019s the result of this so-called generative model, which learns how original data should look like, and creates a completely new simulated data point which has the same flavor, taste and smell. It smells like original data, but it\u2019s just not original data.", "Nicolai (46:12):And again, it\u2019s supposed to anonymize data sets, which is just essentially a one-to-one mapping from say, original data point to say, a new data point which is typically obfuscated or say masked. So synthesized or truly synthetic data is something which is designed by the ML system, by a generative model, and it has the same statistical properties. Again, if we talk about, well, statistical terms, statistical properties as the original data points. And some of the business benefits are really around, well, building better solutions, because you can essentially, in a way, create a variety of examples, similar examples, which do not exist in reality.", "Nicolai (46:53):Think about, say fraud, so fraud detection. So, what about you create a completely new class of fraudsters, and you\u2019re able to backtest that fraud detection system in a much more efficient manner? And this is, again, one of the key benefits of, well, truly synthetic or synthesized datasets, which is you\u2019re able to create, again, a completely new and larger variety of representative examples.", "Nicolai (47:17):I think another, I would say useful benefit is definitely facilitating innovation because it allows you to innovate, keep additional data locked away, but at the same time, without destroying the quality, facilitate innovation by means of simulated datasets. And again, as opposed to anonymizing data, as opposed to scrambling data, we create a completely \u2026 well, really high quality datasets which have the same properties as the original data, and hence facilitating the innovation in the healthcare space, in the insurance space, in the banking space. But at the same time, respecting privacy of individuals. And obviously, staying compliant with GDPR and [inaudible 00:47:58] and other key, important benefit of synthetic or synthesized datasets.", "Jeremie (48:03):And so, is this the main strategy you used at Synthesized where I guess, as the name implies, it\u2019s mostly about generating synthetic datasets, so that then you can send that synthetic data around and not risk leaking real people\u2019s anonymized data?", "Nicolai (48:16):So what we\u2019ve built is the entire data platform, but, well, a very important part of the platform is really this technology. So the ML core, which is able to essentially create a variety of simulated examples, again, for development and testing purposes, to facilitate innovation in that privacy-preserving compliant manner. But there is a much bigger way for technology coming right now, which is the so-called simulations and well, data synchronization. And we write a lot about it.", "Nicolai (48:47):And well, this is essentially what the platform is able to do. And as part of the product release, so enable again, any data scientist, any ML engineer, any test engineer to essentially, well, again, upload the dataset, understand all biases and mitigate those biases with the so-called data synthesis technology, the technology we\u2019ve developed. And this is going to be available to the public as well.", "Nicolai (49:10):Yeah, it\u2019s synthesized.io. And you can also, well, connect to me on LinkedIn. And well, we also publish a lot on Twitter. And yeah, please just connect. And \u2026 yeah.", "Jeremie (49:28):Great. Lots of stuff there for people to check out for sure. And thanks, Nicolai, so much for joining me for the podcast.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb96e11399517&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b96e11399517--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b96e11399517--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----b96e11399517--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----b96e11399517--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----b96e11399517---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb96e11399517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb96e11399517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/tagged/tds-podcast", "anchor_text": "TDS podcast"}, {"url": "https://youtu.be/G9UKiMWpaQM", "anchor_text": "here"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify"}, {"url": "https://twitter.com/npbaldin", "anchor_text": "follow Nicolai on Twitter here"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://www.synthesized.io/blog", "anchor_text": "Synthesized.io\u2019s blog"}, {"url": "https://medium.com/tag/ai-bias?source=post_page-----b96e11399517---------------ai_bias-----------------", "anchor_text": "Ai Bias"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b96e11399517---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-privacy?source=post_page-----b96e11399517---------------data_privacy-----------------", "anchor_text": "Data Privacy"}, {"url": "https://medium.com/tag/fairness-and-bias?source=post_page-----b96e11399517---------------fairness_and_bias-----------------", "anchor_text": "Fairness And Bias"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----b96e11399517---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb96e11399517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&user=Jeremie+Harris&userId=59564831d1eb&source=-----b96e11399517---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb96e11399517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&user=Jeremie+Harris&userId=59564831d1eb&source=-----b96e11399517---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb96e11399517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b96e11399517--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb96e11399517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b96e11399517---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b96e11399517--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b96e11399517--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b96e11399517--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b96e11399517--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b96e11399517--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b96e11399517--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b96e11399517--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b96e11399517--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "122K Followers"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-meets-the-law-bias-fairness-privacy-and-regulation-b96e11399517&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.amazon.ca/Quantum-Physics-Made-Fundamental-Everything/dp/0735244138", "anchor_text": "Quantum Physics Made Me Do It: A Simple Guide to the Fundamental Nature of Everything2023"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}