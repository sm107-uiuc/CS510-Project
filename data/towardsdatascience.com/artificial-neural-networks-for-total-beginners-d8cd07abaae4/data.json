{"url": "https://towardsdatascience.com/artificial-neural-networks-for-total-beginners-d8cd07abaae4", "time": 1683000230.360794, "path": "towardsdatascience.com/artificial-neural-networks-for-total-beginners-d8cd07abaae4/", "webpage": {"metadata": {"title": "Artificial Neural Networks for Total Beginners | by Rich Stureborg | Towards Data Science", "h1": "Artificial Neural Networks for Total Beginners", "description": "Machine Learning drives much of the technology we interact with nowadays, with applications in everything from search results on Google to ETA prediction on the road to tumor diagnosis. But despite\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/conv-nets-for-dummies-a-bottom-up-approach-c1b754fb14d6", "anchor_text": "this article about CNNs", "paragraph_index": 44}], "all_paragraphs": ["Machine Learning drives much of the technology we interact with nowadays, with applications in everything from search results on Google to ETA prediction on the road to tumor diagnosis. But despite its clear importance to our every-day life, most of us are left wondering how this stuff works. We might have heard the term \u201cartificial neural network,\u201d but what does that really mean? Is it a robot that thinks in the same way a human does? Is it a supercomputer owned by Apple? Or is it just a fancy math equation?", "Machine Learning actually covers everything from simple decision trees (similar to the ones you made in your Intro to Business Management course) to Neural Networks, the complex algorithm that mimics the function of a brain. This article will dive into neural networks since they are what\u2019s behind most of the very impressive machine learning these days.", "To understand what machine learning is, consider the task of trying to predict the height of a tree based on the soil content in the ground. Now, since this is machine learning we are talking about, let\u2019s assume we can get some really good data on this task: thousands of soil samples from all over the world.", "There are a lot of measurements you can make on soil contents. Things like moisture levels, iron levels, grain size, acidity, etc. They all have some effect on the health of a tree, and how tall it grows. So let\u2019s say that we examine thousands of trees in the world (all of the same kind, of course) and collect both data about their soil contents as well as the trees\u2019 heights. We have just created a perfect dataset for machine learning, with both features (the soil contents) as well as labels (the heights). Our goal is to predict the labels using the features.", "That definitely seems like a daunting task. Even if there is a relationship between soil contents and tree height, it certainly seems impossible to be able to make accurate predictions, right? Well, machine learning isn\u2019t always perfectly analogous to how our brains work, even if neural networks are modeled from brains. The important thing to remember is that these models aren\u2019t making wild guesses as we humans might. Instead, they are coming up with exact equations that determine their predictions. Let\u2019s start with simplifying the problem a bit first.", "It\u2019s quite easy to imagine that a single feature like moisture will have a significant effect on tree height. Too dry, and the tree won\u2019t grow, but too moist and the roots may rot. We could make an equation based on this single measurement, but it wouldn\u2019t be very accurate because there are many many more factors that go into the growth of a tree.", "See how the hypothetical relationship above is not a great estimate? The line follows the general trends of the dots, but if that\u2019s what you use to make your predictions on height you\u2019ll be wrong most of the time. Consider the case where there is a perfect amount of moisture, but the soil is way too acidic. The tree won\u2019t grow very well, but our model only considers moisture, so it will assume that it will. If we consider both measurements, however, we might get a more accurate prediction. That is, we would only say that the tree will be very tall when both the moisture and acidity are at good levels, but if one or both of them are at bad levels we may predict that the tree will be short.", "So what if we consider more factors? We could look at the effect of moisture and acidity at the same time by combining the relationships into one equation.", "Excellent. Now we have a more complex equation that describes the tree\u2019s height, and it considers two features (measurements). Now we can combine even more features to make an even more complex equation. For the sake of clarity, I will call the final, combined equation our \u201cmodel\u201d. It models how the features affect height. Combining simple equations like this into a multi-dimensional model is pretty straight forward, and we can create a very complex model pretty fast. But for every tweak you can make on one of the simple equations (choosing a slightly different equation for the relationship between height and moisture), there are now thousands if not millions of more \u2018models\u2019 that we have to try, all slightly different from one another. One of these models might be great at modeling the relationship between soil content and height, but most are probably really bad at it.", "This is where machine learning comes in. It will create a model composed of many simpler equations, and then test how well it works. Based on its error (that is, how wrong the predictions are) it then tweaks the simpler equations only slightly, and tests how well that one works. When it tweaks the simpler equations, it is simply altering one of the graphs in the image above to look slightly different. It may shift the graph to the right or up and down, or it could slightly elongate peaks or increase the size of the valleys. Through a process similar to evolution, it will arrive at the best \u2014 or at least a good \u2014 solution. In fact, that\u2019s why it\u2019s called \u201cmachine learning\u201d. The machine learns the pattern on its own, without humans having to tell it even simple information like \u201cmoisture is good for trees\u201d.", "If you\u2019re curious about how the machine learning model picks the next combination of equations, you should read further about model training. Specifically, the concepts to master are stochastic gradient descent and backpropagation.", "Sidenote: If you ever studied the Fourier series at university, it is useful to think of them as an analogy for a neural network. In school, we learn that you can create complex waves like a square wave using a combination of simple sine waves. Well, we can also create a machine learning model from many simple equations in a similar fashion.", "Neural networks are specifically designed based on the inner workings of biological brains. These models imitate the functions of interconnected neurons by passing input features through several layers of what are referred to as perceptrons (think \u2018neurons\u2019), each transforming the input using a set of functions. This section will explain the components of a perceptron, the smallest component of a neural network.", "A perceptron (above) is typically made up of three main math operations: scalar multiplication, a summation, and then a transformation using a distinct equation called an activation function. Since a perceptron represents a single neuron in the brain, we can put together many perceptrons to represent a brain. That would be called a neural network, but more on that later.", "The inputs are simply the measures of our features. For a single soil sample, this would be an array of values for each measurement. For example, we may have an input of:", "representing 58% moisture, 1.3mm grain size, and 11 micrograms iron per kg soil weight. These inputs are what will be modified by the perceptron.", "Weights represent scalar multiplications. Their job is to assess the importance of each input, as well as directionality. For example, does more iron contribute a lot or a little to height? Does it make the tree taller or shorter? Getting these weights right is a very difficult task, and there are many different values to try.", "Let\u2019s say we tried values for all three weights at 0.1 increments on the range of -10 to 10. The weights that showed the best results were w0 = 0.2, w1 = 9.6, w3 = -0.9. Notice that these weights don\u2019t have to add up to 100. The important thing is how large and in what direction they are compared to one another. If we then multiply these weights by the inputs we had from before, we get the following result:", "These values will then be passed onto the next component of the perceptron, the transfer function.", "The transfer function is different from the other components in that it takes multiple inputs. The job of the transfer function is to combine multiple inputs into one output value so that the activation function can be applied. This is usually done with a simple summation of all the inputs to the transfer function.", "On its own, this scalar value is supposed to represent some information about the soil content. This value has already factored in the importance of each measurement, using the weights. Now it is a single value that we can actually use. You can almost think of this as an arbitrary weighted index of the soil\u2019s components. If we have a lot of these indexes, it might become easier to predict tree height using them. Before the value is sent out of the perceptron as the final output, however, it is transformed using an activation function.", "An activation function will transform the number from the transfer function into a value that dramatizes the input. Often times, the activation function will be non-linear. If you haven\u2019t taken linear algebra in university you might think that non-linear means that the function doesn\u2019t look like a line, but it\u2019s a bit more complicated than this. For now, just remember that introducing non-linearity to the perceptron helps avoid the output varying linearly with the inputs and therefore allows for greater complexity to the model. Below are two common activation functions.", "ReLU is a simple function that compares zero with the input and picks the maximum. That means that any negative input comes out as zero, while positive inputs are unaffected. This is useful in situations where negative values don\u2019t make much sense, or for removing linearity without having to do any heavy computations.", "The sigmoid function does a good job of separating values into different thresholds. It is particularly useful for values such as z-scores, where values towards the mean (zero) need to be looked at carefully since a small change near the mean may significantly affect a specific behavior, but where values far from the mean probably indicate the same thing about the data. For example, if soil has lots and lots of moisture, a small addition to moisture probably won\u2019t affect tree height, but it if has a very average level of moisture then removing some small amount of moisture could affect the tree height significantly. It emphasizes the difference in values if they are closer to zero.", "When you think of activation functions, just remember that it\u2019s a nonlinear function that makes the input more dramatic. That is, inputs closer to zero are typically affected more than inputs far away from zero. It basically forces values like 4 and 4.1 to be much closer, while values like 0 and 0.1 become more spread apart. The purpose of this is to allow us to pick more distinct decision boundaries. If, for example, we are trying to classify a tree as either \u201ctall,\u201d \u201cmedium,\u201d or \u201cshort,\u201d values of 5 or -5 are very obviously representing tall and short. But what about values like 1.5? Around these numbers, it may be more difficult to determine a decision boundary, so by dramatizing the input it may be easier to split the three categories.", "We pick an activation function before training our model, so the function itself is always the same. It is not one of the parameters we toggle when testing thousands of different models. That only happens to the weights. The output of the ReLU activation function will be:", "Up until now, I have ignored one element of the perceptron that is essential to its success. It is an additional input of 1. This input always stays the same, in every perceptron. It is multiplied by a weight just like the other inputs are, and its purpose is to allow the value before the activation function to be shifted up and down, independent of the inputs themselves. This allows the other weights (for the actual inputs, not the weight for the bias) to be more specific since they don\u2019t have to also try to balance the total sum to be around 0.", "To be more specific, bias might shift graphs like the left graph to something like the right graph:", "And that\u2019s it! We\u2019ve now built a single perceptron. We\u2019ve now created a model that imitates the brain\u2019s neuron. We also understand that while that sounds fancy, it really just means that we can create complex multi-dimensional equations by altering a few weights. As you saw, the components are surprisingly simple. In fact, they can be summarized by the following equation:", "From here on out I will be representing this equation (i.e. a single perceptron) with a green circle. All of the components we have seen so far: inputs, bias, weights, transfer function, and an activation function are all present in every single green circle. When an arrow points into this green circle, it represents an individual input node, and when the arrow points out of the green circle it represents the final output value.", "To represent a network of perceptrons we simply plug the output of one into the input of another. We connect many of these perceptrons in chains, flowing from one end to another. This is called a Multi-Layer Perceptron (MLP), and as the name suggests there are multiple layers of interconnected perceptrons. For simplicity, we will look at a fully-connected MLPs, where every perceptron in one layer is connected to every perceptron in the next layer.", "You might be wondering what a \u2018layer\u2019 is. A layer is just a row of perceptrons that are not connected to each other. Perceptrons in an MLP are connected to every perceptron in the layer before it and every perceptron in the layer after it, but not to any of the perceptrons within the same layer. Let\u2019s look at an MLP with two input values, 2 hidden layers and an output of a single value. Let\u2019s say the first hidden layer has two perceptrons and the second hidden layer has three.", "The perceptrons here will all take in the inputs (arrows pointing towards the circle), perform the operations described in the previous section, and then push the output forward (arrow pointing out of the circle). This is done many times to create more and more complex equations, all considering the same information multiple times to make an accurate prediction. Now, although this article is meant to remove \u201cthe magic\u201d from neural networks, it is very difficult to explain why this helps make more accurate predictions. In fact, the method I am describing is often referred to as a \u201cblack box\u201d approach, because we don\u2019t know why the equations it picks are important. It is currently an active area of research. What we can understand, however, is what the neural network is doing. That is as simple as following the weights through each and every perceptron.", "The reason we call the layers between the input layer and output layers \u201chidden\u201d is because once the values are fed from the input, it doesn\u2019t serve us well to look at how that value is transformed until it exits the last output node. This is because these intermediary values are never used to evaluate the performance of our model (i.e. getting error values for predictions made on sample data).", "And that\u2019s really it. Combining many of these perceptrons helps us create even more sophisticated equations that a single perceptron can create.", "The output value of an MLP like this is capable of making predictions on height using soil content measurements. Of course, picking the correct weights inside every single perceptron takes a lot of computational power, but this is exactly what a \u2018neural network\u2019 does.", "Here I will take two measurements from before through an entire neural network. The structure will be the same as the network I showed above. This will be very tedious, but you may follow along if you wish. I will be ignoring the bias for the sake of simplicity.", "Here are the values of the two features I will use. They represent 58% moisture and 1.3mm grain size.", "I will use the following (random) weights and activation functions for each perceptron. Recall that the ReLU activation function turns negative values into 0 and does not transform positive values:", "So let\u2019s get to it! The first two perceptrons both take the two inputs (blue), multiplies them by the associated weights (yellow), adds them (purple), and then applies the ReLU function (green):", "These outputs become the inputs for each perceptron in the third layer. So every perceptron in the second hidden layer (there are three) will use 338.9 and 42 as inputs. Those perceptrons follow the following equations:", "For the next layer, however, notice that we now have three, not two, inputs: 89.9, 16.22, and 0. All three inputs have to be included in the equation of the last perceptron, and therefore it will have three weights (in yellow below). Its equation is still as straightforward as the others.", "As a summary, here are the values each perceptron produced given its inputs:", "And there you have it! This neural network predicted a tree with a height of 165.72 feet! Now we have to compare the predicted results to the actual height of the sample tree in our data. Calculating some error value can be as straightforward as taking the difference between our predicted height and the actual height. Then we repeat this process with slightly different weights over and over until we find weights that predict tree height well for many samples. But that takes much too long for a human to do, so we need a machine to compute the optimal weights.", "Thanks for reading :) Consider giving the article a round of applause, it really helps me out! To learn more about machine learning, check out this article about CNNs, the model behind computer vision.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D. candidate at Duke University, working on research regarding vaccine misinformation and credibility online."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd8cd07abaae4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@stureborg.r?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@stureborg.r?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "Rich Stureborg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbeba0fb6561a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&user=Rich+Stureborg&userId=beba0fb6561a&source=post_page-beba0fb6561a----d8cd07abaae4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8cd07abaae4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8cd07abaae4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/conv-nets-for-dummies-a-bottom-up-approach-c1b754fb14d6", "anchor_text": "this article about CNNs"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d8cd07abaae4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d8cd07abaae4---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/beginner?source=post_page-----d8cd07abaae4---------------beginner-----------------", "anchor_text": "Beginner"}, {"url": "https://medium.com/tag/artificial-neural-network?source=post_page-----d8cd07abaae4---------------artificial_neural_network-----------------", "anchor_text": "Artificial Neural Network"}, {"url": "https://medium.com/tag/ai?source=post_page-----d8cd07abaae4---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8cd07abaae4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&user=Rich+Stureborg&userId=beba0fb6561a&source=-----d8cd07abaae4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8cd07abaae4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&user=Rich+Stureborg&userId=beba0fb6561a&source=-----d8cd07abaae4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8cd07abaae4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd8cd07abaae4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d8cd07abaae4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d8cd07abaae4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@stureborg.r?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@stureborg.r?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rich Stureborg"}, {"url": "https://medium.com/@stureborg.r/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "55 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbeba0fb6561a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&user=Rich+Stureborg&userId=beba0fb6561a&source=post_page-beba0fb6561a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F787307130fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-neural-networks-for-total-beginners-d8cd07abaae4&newsletterV3=beba0fb6561a&newsletterV3Id=787307130fc&user=Rich+Stureborg&userId=beba0fb6561a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}