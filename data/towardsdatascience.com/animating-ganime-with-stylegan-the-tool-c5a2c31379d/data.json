{"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d", "time": 1683001230.755337, "path": "towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d/", "webpage": {"metadata": {"title": "Animating gAnime with StyleGAN: The Tool | by Nolan Kent | Towards Data Science", "h1": "Animating gAnime with StyleGAN: The Tool", "description": "This is a tutorial/technical blog for a research tool I\u2019ve been working on as a personal project. While a significant portion of the blog assumes you have access to the tool while reading it, I\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.gwern.net/Danbooru2018", "anchor_text": "anime dataset", "paragraph_index": 7}, {"url": "https://www.gwern.net/Faces", "anchor_text": "number of projects", "paragraph_index": 7}, {"url": "https://github.com/nolan-dev/GANInterface", "anchor_text": "https://github.com/nolan-dev/GANInterface", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2", "anchor_text": "feature maps", "paragraph_index": 8}, {"url": "https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.14.0.zip", "anchor_text": "tensorflow.dll", "paragraph_index": 91}, {"url": "https://github.com/lutzroeder/netron", "anchor_text": "netron", "paragraph_index": 92}, {"url": "https://github.com/nolan-dev/stylegan_reimplementation/commit/da7c6abd039f1547f3d60c38be356b160e1dd120", "anchor_text": "here\u2019s the correction", "paragraph_index": 96}, {"url": "https://nvlabs.github.io/few-shot-vid2vid/", "anchor_text": "video-to-video synthesis", "paragraph_index": 98}], "all_paragraphs": ["This is a tutorial/technical blog for a research tool I\u2019ve been working on as a personal project. While a significant portion of the blog assumes you have access to the tool while reading it, I attempted to include enough screenshots that it should be clear how it works even if you do not have time to try it out yourself.", "In the tutorial we\u2019ll interact with a trained StyleGAN model to create (the frames for) animations such as this:", "In the animation above, the transformations that change the mouth, eyes, and hair are mostly separate. This is often preferable to other methods (that I\u2019m aware of) for creating talking animations with GANs, which may cause side effects such as hair loss:", "We\u2019ll also build simple heuristic facial feature detectors by using feature maps at various layers in the network:", "These detectors can then be used to automate meaningful modifications in order to isolated portions of images:", "None of these steps require labels for the training set, but there is a bit of manual work involved.", "You can download a compiled version of the tool from one of the following links:", "It comes with a model trained on an anime dataset (which a number of projects are based on). Unfortunately, due to the nature of the dataset, there is a lack of gender diversity. I\u2019m currently trying to train a model to produce higher quality masculine images, but it will take a while to complete. The dataset also contains NSFW images, and while I\u2019ve generated thousands of images and never encountered anything NSFW, I didn\u2019t vet every image in the training set (the risk may increase with large modifications to feature maps in early layers). If you run into problems, you can create an issue in https://github.com/nolan-dev/GANInterface and I\u2019ll try to respond.", "This blog has two parts: a basic tutorial, and an advanced tutorial. The basic tutorial demonstrates how to use the tool and shouldn\u2019t require much technical knowledge to complete (though it does include some technical explanations for those interested). The advanced tutorial demonstrates how to customize the tool and is more technical \u2014 you should be familiar with feature maps in convolutional neural networks, for example.", "I introduced the tool in a previous blog and shared the source code, but getting it working from there is complicated and requires a model trained with my custom StyleGAN implementation. I hope supplying a compiled version of the tool and a pre-trained model makes it easier to try out. The tool is part of a larger project (including a reimplementation of StyleGAN) which the previous blog discusses, but reading it is not a prerequisite for this blog. Here\u2019s a link if you\u2019re interested though:", "As this is a research tool, I\u2019ve been adding and subtracting features regularly to get a better idea of how the model works and the best way to interact with it. There are a lot of minor features, but major ones include:", "Like the previous blog, my goal for this blog is to get others\u2019 perspectives on the topic, detail my experience working on the project, and receive constructive criticism/corrections. The tutorial format of this blog is meant to mitigate the underdeveloped UI of the tool, and make it possible to use without dealing with the messy source code. Unfortunately, it\u2019s Windows only, however it has been tested on a free tier Windows AWS instance (Microsoft Windows Server 2019 Base, image generation will be slow with this however).", "A quick note before we get started: I modified the tool while writing this, so some screenshots are slightly different from the current version, but everything is in roughly the same place.", "Once you\u2019ve downloaded and opened the zip file linked above, you\u2019ll be presented with several files/folders:", "I\u2019ll explain some of these in more detail in the advanced section (3.3), but at this point the only important file is GanStudio.exe. When you run it, click ok to the disclaimers (and hopefully don\u2019t get any errors), you\u2019ll be presented with something like the following:", "Due to the complexity of the UI, the first time I reference a part of the tool in this tutorial I\u2019ll have a nearby screenshot with the relevant part outlined in red. Many of the UI elements have tooltips.", "Using this tool involves interacting with windows explorer, and it\u2019s easiest to view generated files with \u201cLarge\u201d or \u201cExtra large\u201d icons. Select one of these by right clicking on an explorer window and selecting \u2018View\u2019:", "In many cases it is also helpful to sort images by date modified, also achieved through the right click menu:", "To test out image generation, click Generate New (3). This will produce a new latent code, and display the image for it. Note generating the first image usually takes longer than subsequent images.", "The image was created randomly, but was interpolated to be close to the \u2018average\u2019 image. This results in higher quality images, but reduces variation. If you have the quality slider (1) in the same place as the above image, your image will likely be similar: a brownish haired girl with purple and/or blue eyes.", "To keep images in this tutorial consistent with what the tool will produce, I\u2019ve provided a sample image. Click \u2018Import Image\u2019 (above, 2). This will create an open file dialog box in the \u2018portraits\u2019 directory. Navigate to the directory above \u2018portraits\u2019 and select \u2018tutorial_files\u2019:", "Double click on sample_01.png to load it. All images you generate are saved to the \u2018portraits\u2019 folder and you can load them again using this method.", "GANs cannot normally load an arbitrary image, however this tool will append the latent code that generated the image to every PNG file it writes to the disk. The Import Image button reads the latent code that was written to the image you select. As long as the tool has loaded the model that generated the image it will be able to recreate it.", "To start modifying attributes, select the \u2018Attributes_0\u2019 tab (above). Attributes include hair/eye color, intensity of background, accessories, and mouth state (smile/open). Moving a slider that corresponds to an attribute to the right will increase the influence of that attribute on the image, moving it to the left will decrease said influence. Some of them work better than others. After selecting a location, press \u2018Update This Image\u2019 (above). Here are some examples:", "One downside of modifying attributes in this way is that they are not always spatially isolated; modifying an attribute that should only influence one part of an image will also influence other parts. This is particularly problematic if we want to create animations. To see the problem in action perform the following steps (screenshot below for reference):", "This will produce 5 images with the \u2018open mouth\u2019 attribute shifting from 0 to the selected location on the slider:", "Throwing these into a gif generator produces the following animation:", "As you can see, features all over the image change even though we only selected an attribute related to the mouth.", "In this section we\u2019ll make a modification to just the mouth, without altering other features. Unfortunately, at this point images imported with the Import Image button will not reflect the changes made here.", "Repeat the instructions in the Load Image section to get back to the base image (or reset the attribute sliders and update). Well use the \u2018Spatial\u2019 tab (below) to modify isolated parts of an image.", "The UI is complicated, however for this section we\u2019ll only be using a couple parts. The first thing we need to do is indicate which part of the image we want to change:", "This makes a selection around the mouth, and ensures that our changes to the image will only influence the area selected.", "This will produce a light green square, unless \u201cSwap green for blue in visualization\u201d is selected. I\u2019ll have that option selected for this tutorial to improve visibility, and hopefully be more colorblind friendly when we start dealing with the red boxes that indicate negative influence.", "If you selected a location in error, you can remove squares by holding \u2018control\u2019 when you make a selection:", "The following are all of the ways you can \u2018draw\u2019 on the image. Some of these aren\u2019t needed yet, but will be useful later:", "With the mouth selected, move the slider below the \u2018mouth_open\u2019 tab to the right until the number on the bottom left is around 100. This slider is the \u201cFeature Map Multiplier Slider\u201d, which influences the active tab exponentially as it is moved to the right (positive influence) or left (negative influence). With the number in the bottom left of the slider set to around 100, select \u2018Update This Image\u2019:", "This should produce the following image:", "As the name of the tab would imply, this opened the mouth. Let\u2019s try and animate with this method. Select Batch->Fmap->Combinatoric (I\u2019ll expand on why it\u2019s called that in the advanced tutorial):", "Select 5 for images to generate:", "Select 0 for the start point. The batch will consist of 5 images with the slider regularly spaced between the start and end point (0, 20, 40, 80, 100). Because the mouth is closed by default in this image, a start point of 0 (no influence) means closed.", "This will produce 5 images with less spatial entanglement than the attribute method:", "A gif generator produces the following:", "This same process can be used with the other tabs, and different tabs can be combined. If you\u2019re using the Combinatoric batch generator, you\u2019ll need to keep the multiplier bar at zero for all tabs except the active one to avoid producing combinations of multiple tabs. This can be done by pressing \u2018Set All to Zero\u2019 before changing the active tab\u2019s multiplier:", "Here\u2019s a list of some of the possible changes. Note that using large multipliers has a good chance of producing strange artifacts:", "With the following start and end points:", "This section assumes some familiarity with convolutional neural networks and StyleGAN.", "Tabs in the \u2018spatial\u2019 section (mouth_open, hairband, etc) correspond to values added to specific feature maps at a specific layer. Try selecting the mouth_open tab. In the combo box above the tabs, it should show a resolution: 16x16. Early layers in StyleGAN have low resolution feature maps, while later layers have high resolution feature maps (resolution regularly doubles). As the images we generate are 256x256 pixels, the layer that corresponds to 16x16 is early in the network. To view which feature maps are modified by the mouth_open tab, press \u2018View All Fmap Mults\u2019 with \u2018Filter Zero\u2019 checked and select the \u2018Feature Map Input\u2019 tab:", "This means spatial locations selected in the image by clicking on it are multiplied by -2, then multiplied by the feature map multiplier slider, and the result is added to feature map 33 on the layer that has a 16x16 resolution.", "Some tabs influence multiple feature maps:", "I found these multipliers manually by playing around with the tool. I used two methods:", "In the next two sections I\u2019ll walk through examples for these methods.", "Given that the network can generate images with open and closed mouths (required to fool the discriminator) and that feature maps at each layer are representations of the final image, it makes sense that modifying the feature maps could be used to open or close a mouth. However, there\u2019s no guarantee that modifying a single feature map will result in a meaningful change \u2014 it may be that we need to modify a combination of many feature maps to get the desired result. That said, single feature maps are easier to work with (at least with the current tool), so seeing how each one influences an image can serve as a starting point.", "Here\u2019s how I found feature map 33 to open/close mouths. First, I added a 16x16 tab with the \u2018Add Tab\u2019 button (below). I chose this resolution because it produces boxes that are reasonably mouth-sized. Smaller resolutions would change areas beyond the mouth, while larger resolutions often result in changes with finer granularity than opening or closing a mouth (choice of resolution is heuristic at this point). By clicking \u2018View All Fmap Mults\u2019 again we see that no feature maps are set for the new tab. Then I moved the slider to around 190, again a heuristic decision based on past experience with the model. Finally, as we did before, I selected the two boxes that contain the mouth.", "Then, I selected Batch -> Fmap -> Axis-aligned, and selected 512 images to get generated.", "This will actually produce 1024 images, as for each feature map it both adds and subtracts the value specified in the multiplier bar (190 in this case) to the spatial location marked in the image (the mouth). Batch generation pops up a window that shows how many images have been generated and allows you to interrupt the process. Clicking on the counter next to \u2018Generating image\u2019 opens the directory to which they are being written:", "The sample that starts with \u201833_n_sample\u2019 (n stands for negative) clearly has an open mouth, while \u201833_p_sample\u2019 does not. This means that when 190 was subtracted from feature map 33 around the mouth, the mouth opened.", "I set feature map 33 to -1 using the \u2018Set Fmap\u2019 box (below). This makes it so moving the slider to the right will open the mouth (which feels more intuitive than setting feature map 33 to 1 and having the tab named \u2018mouth_close\u2019), and I renamed the tab to mouth_open using the \u2018Rename Tab\u2019 button. The Save Tabs button next to Rename Tab can be used to save the tabs.", "This method relies on finding existing images with the desired attribute. This requires a base of images to work off of, which can be generated with Batch->New latents. In these cases I\u2019ll usually move the Quality bar a bit past the middle to ensure there\u2019s a reasonable amount of variation.", "It may take a few hundred samples to get several samples with hairbands. I added one to the tutorial_files directory which I\u2019ll load for this tutorial (sample_02.png). Before loading it, create a 16x16 tab and make sure \u2018Fmaps To Get\u2019 is set to \u2018All\u2019 or \u2018Current Tab\u2019 (below). These options get and store extra output from the network when a new image is created: the feature maps for the current tab, or for all tabs. This can slow things down a bit, so it\u2019s not the default option (also as of this writing it has something like a 0.5% chance of causing a crash, relevant for large batches).", "This adds a bunch of buttons to the \u2018Feature Map Output\u2019 tab:", "These buttons correspond to feature maps. They are sorted by magnitude of the dot product of the feature map and the selection on the image (after they\u2019re flattened). This makes it so feature maps with large magnitudes around the hairband will show up earlier in the list of buttons.", "Here\u2019s an example of the 310th feature map. Blue corresponds to positive values, red to negative. The larger absolute value at a location, the thicker and more saturated the box drawn there.", "This feature map seems to have large positive values around the hairband, but also around the mouth. While it\u2019s clearly used for more than just hairbands, we can try modifying it to see what the result is. Set feature map 310 to 1 for that tab, erase the selections around the hairband (ctrl+click and drag), and load sample_01.png again.", "Try selecting hair, increasing the weight to around 100, and updating the image:", "\u2026not has much changed. However, as we didn\u2019t produce any weird artifacts, it may not hurt to increase the magnitude beyond 100.", "Around 600 we do get what looks like a hairband. My initial hypothesis is that one reason we need to use a large magnitude is because hairbands are uncommon.", "For the hairband tab included with the tool I set several other feature maps, which were active around the hairband in example images, to 3. Setting them to a number larger than 1 helps normalize the expected range of the tabs so that setting the multiplier to around 100 should express the wanted attribute.", "One problem with the way we\u2019ve been modifying attributes is that it requires manually selecting the squares we want to change. This is definitely a downside when compared to modifying the latent vector in the \u2018mouth_open\u2019 direction, which did not require us to know the mouth\u2019s location (even though it also modifies non-mouth features). This prevents the method from scaling well; while easier than drawing, every modification still requires human intervention. However, as we saw in the previous section, some feature maps correlate with the location of attributes: maybe feature map 310 could be used to generically detect hairbands, for example. Let\u2019s see if we can find a way to detect the mouth in an image using just a linear combination of feature maps.", "First, repeat the process used to get feature maps active around a hairband, only this time select the mouth:", "Like before, we can click a button to show a feature map:", "This method is a bit slow, however, when it comes to viewing and comparing a large number of feature maps. Instead, type 20 into the text box below the \u2018Add From Output\u2019 button and press the button. Then, press \u2018View Multiple Fmaps\u2019.", "\u2018Add From Output\u2019 with 20 adds the feature maps from the first 20 buttons to the \u2018Fmaps\u2019 text box, and \u2018View Multiple Fmaps\u2019 displays them all side by side.", "The first 4 (along with several others), which correspond to feature maps 234, 34, 370, and 498 all look like they could be mouth detectors. However, we don\u2019t know if they consistently detect the mouth for new images. To test this, we can generate several new images with the Quality bar to the right of the center for decent variance. First make sure \u2018Record Feature Maps\u2019 is checked. Use \u2018Reset History\u2019 to clear existing records. Also, make sure \u2018Fmaps To Get\u2019 is set to \u2018Current Tab\u2019 (the tool does not record history for all resolutions, just the resolution that corresponds to the current tab). Then, we can generate a number of new images by using Batch-> New latents and the tool will record their feature maps.", "In this case I\u2019ll generate 10 new images, which will be different from any of yours. To view the same feature map for all the images, type the feature map into the Fmaps box and select \u2018View History\u2019. I\u2019ll do that for each of 234, 34, and 370.", "It doesn\u2019t hurt that the mouth doesn\u2019t change position much, but these feature maps do seem to track it reliably.", "The same process can be applied to other attributes at other layers. Here are some examples:", "In many cases I get the most consistent results by combining multiple feature maps. To do this, I made python scripts callable from the tool (which could be useful for future features as well), as I\u2019d rather do multidimensional data processing with numpy. The tool uses the PATH environmental variable to find pythonw.exe, so that will need to be set before running the tool. The scripting functionality is the newest feature in the tool and even less developed than the rest. Here\u2019s an example:", "spatial_map.py is stored in the \u2018scripts\u2019 directory, and you\u2019ll need to install its dependencies to use it. The tool passes the path to a directory where the feature maps specified in \u2018Script Feature Maps\u2019 are written. It then combines those feature maps and outputs the result, which is read by the tool and used to make selections in the image. Here\u2019s an example that uses some of the feature maps we found earlier which correlated with mouth location:", "Moving the mouth slider to ~100 and updating the image opens the mouth like normal.", "This lets us automate image creation with certain attributes.", "By selecting \u2018Run and Modify\u2019 under \u2018Run Script When Generating\u2019, setting mouth_open to around 100, and generating new latents, we can ensure new images have an open mouth. By setting mouth_open to -100 and applying the settings to the directory with open mouth images we can generate the same images with closed mouths.", "Applying settings to a directory can be done with Misc->Apply Current Sliders To Directory and selecting the directory with generated images:", "Alternatively, this can be achieved in one go using the Batch->Fmap->New Latent Combinatoric:", "Note that this option will do a combinatoric modification based on all tabs with non-zero multipliers. For example, if the \u2018hair\u2019 tab has a non-zero multiplier and we select 3 \u2018hair\u2019 images to be generated and 2 \u2018mouth_open\u2019, it will create 6 variations of each image with the following attributes:", "A couple more points on scripts:", "This covers most of the tool\u2019s functionality. In the next section I discuss the architecture in more detail.", "There are 4 main components of the tool:", "These are all visible in the zip file:", "Images are written to the \u2018portraits\u2019 directory, images marked as favorites are written to the \u2018favorites\u2019 directory, and the \u2018scripts\u2019 directory contains python scripts the tool can load. The rest of the files are libraries used by other components of the tool.", "The data directory contains several files:", "All of these are model-specific, which is why I tied the directory to the model by appending the graph.pb file\u2019s hash to the directory name.", "TensorflowInterface.dll is the component that loads graph.pb, and it uses tensorflow.dll to interact with it. It does this through 3 main functions which it exports:", "Much of this interaction is done using the names of tensors in the model. Though the source code is available, I think one of the best ways to get these names (and a better understanding of the model) is through a tool like netron. In the following image, netron is used to examine the inputs to an add operation that incorporates the feature map modifications this tool makes.", "Combined with the previous blog in this series, I\u2019ve written a fairly comprehensive record of the work I\u2019ve done for the project so far. Two exceptions that these blogs do not cover in detail are the ability to generate 512x256 (height x width) images and the model I trained with more male images.", "The model used in this blog can actually produce 512x256 images, however I sliced off the bottom early on in the network:", "This improved image creation speed and reduced the size of the UI, but most importantly it kept the images suitable for all audiences.", "I originally thought the model with male images was worse due to the increased variance and lower average quality in images, as to include a significant number of male images I had to reduce the \u2018favorites\u2019 threshold (see original blog). However, when I was updating the code for my StyleGAN reimplementation I noticed I had introduced a bug (here\u2019s the correction) when I was implementing style mixing (not yet included in the tool). The bug meant that the weights used to convert from intermediate latent to style were shared between two adaptive instance normalization layers. This reduced the capacity of the network, and could have been a reason that the new model was worse than the old, less diverse model (used in this blog). This is one of the most annoying types of bugs to deal with: it doesn\u2019t prevent the model from working, but it can reduce performance in a way that isn\u2019t really noticeable until the model is done training, and the reduction in performance may be attributed to other factors. Andrej Karpathy put it very well in this series of tweets:", "For personal projects, including the tool discussed in this blog, I prefer using the \u201cmove fast & fix stuff\u201d approach (malware analysis made me almost enjoy debugging). However this does not work for implementing deep learning models, and while I tend to move much more slowly to avoid mistakes like this when using TensorFlow, sometimes they still pop up if I\u2019ve spent too much time on a different part of a project. This is something I\u2019m actively trying to improve on, and one strategy I\u2019ve found helpful is viewing the model\u2019s graph in TensorBoard or netron to get a different perspective.", "As this tool is mostly based on viewing and modifying feature maps, I\u2019m interested in adapting it to work on generative models other than GANs. It would be great if models that can take arbitrary images as input could demonstrate the same type of unsupervised facial feature detection in their internal representations as StyleGAN does. I also want to compare this approach to animation with video-to-video synthesis, and do more research into other work on using generative models for animation."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc5a2c31379d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@nkent?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Nolan Kent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F352fdc0a3e4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=post_page-352fdc0a3e4c----c5a2c31379d---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5a2c31379d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----c5a2c31379d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5a2c31379d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&source=-----c5a2c31379d---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://drive.google.com/file/d/1cv2SiWQKtlC-XCAeAiGHh2C8XCAZe9xd/view?usp=sharing", "anchor_text": "GanStudio_x64_v1.zipEdit descriptiondrive.google.com"}, {"url": "https://mega.nz/#!VCIRyIRI!t_g2OQYkuqtPAdd5wgsSHUEYhYI47ip84jydGZMI-bg", "anchor_text": "MEGAMEGA provides free cloud storage with convenient and powerful always-on privacy. Claim your free 50GB nowmega.nz"}, {"url": "https://www.gwern.net/Danbooru2018", "anchor_text": "anime dataset"}, {"url": "https://www.gwern.net/Faces", "anchor_text": "number of projects"}, {"url": "https://github.com/nolan-dev/GANInterface", "anchor_text": "https://github.com/nolan-dev/GANInterface"}, {"url": "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2", "anchor_text": "feature maps"}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e", "anchor_text": "Animating gAnime with StyleGAN: Part 1Introducing a tool for interacting with generative modelstowardsdatascience.com"}, {"url": "https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.14.0.zip", "anchor_text": "tensorflow.dll"}, {"url": "https://github.com/lutzroeder/netron", "anchor_text": "netron"}, {"url": "https://github.com/lutzroeder/netron", "anchor_text": "Netron"}, {"url": "https://github.com/nolan-dev/stylegan_reimplementation/commit/da7c6abd039f1547f3d60c38be356b160e1dd120", "anchor_text": "here\u2019s the correction"}, {"url": "https://nvlabs.github.io/few-shot-vid2vid/", "anchor_text": "video-to-video synthesis"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c5a2c31379d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c5a2c31379d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c5a2c31379d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c5a2c31379d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/generative-art?source=post_page-----c5a2c31379d---------------generative_art-----------------", "anchor_text": "Generative Art"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5a2c31379d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----c5a2c31379d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5a2c31379d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----c5a2c31379d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5a2c31379d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F352fdc0a3e4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=post_page-352fdc0a3e4c----c5a2c31379d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F352fdc0a3e4c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----c5a2c31379d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Written by Nolan Kent"}, {"url": "https://medium.com/@nkent/followers?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "183 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F352fdc0a3e4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=post_page-352fdc0a3e4c----c5a2c31379d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F352fdc0a3e4c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----c5a2c31379d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e?source=author_recirc-----c5a2c31379d----0---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----c5a2c31379d----0---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----c5a2c31379d----0---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Nolan Kent"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c5a2c31379d----0---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e?source=author_recirc-----c5a2c31379d----0---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Animating gAnime with StyleGAN: Part 1Introducing a tool for interacting with generative models"}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e?source=author_recirc-----c5a2c31379d----0---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "17 min read\u00b7Oct 27, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cf764578e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----4cf764578e----0-----------------clap_footer----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e?source=author_recirc-----c5a2c31379d----0---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cf764578e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&source=-----c5a2c31379d----0-----------------bookmark_preview----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c5a2c31379d----1---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c5a2c31379d----1---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c5a2c31379d----1---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c5a2c31379d----1---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c5a2c31379d----1---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c5a2c31379d----1---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c5a2c31379d----1---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----c5a2c31379d----1-----------------bookmark_preview----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c5a2c31379d----2---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----c5a2c31379d----2---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----c5a2c31379d----2---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c5a2c31379d----2---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c5a2c31379d----2---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c5a2c31379d----2---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c5a2c31379d----2---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----c5a2c31379d----2-----------------bookmark_preview----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----c5a2c31379d----3---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----c5a2c31379d----3---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----c5a2c31379d----3---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Nolan Kent"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c5a2c31379d----3---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----c5a2c31379d----3---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "Malware Analysis with Visual Pattern RecognitionThe secret to quickly reverse-engineering binary files"}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----c5a2c31379d----3---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": "18 min read\u00b7May 27, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a4d087c9d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmalware-analysis-with-visual-pattern-recognition-5a4d087c9d26&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----5a4d087c9d26----3-----------------clap_footer----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----c5a2c31379d----3---------------------834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a4d087c9d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmalware-analysis-with-visual-pattern-recognition-5a4d087c9d26&source=-----c5a2c31379d----3-----------------bookmark_preview----834f73c6_2bc6_47f1_8cb4_b91f8a3d5475-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "See all from Nolan Kent"}, {"url": "https://towardsdatascience.com/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----c5a2c31379d----0-----------------bookmark_preview----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Josep Ferrer"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Stop doing this on ChatGPT and get ahead of the 99% of its usersUnleash the Power of AI Writing with Effective Prompts"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "\u00b78 min read\u00b7Mar 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&user=Josep+Ferrer&userId=8213af8f3ccf&source=-----f3441bf7a25a----1-----------------clap_footer----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "71"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&source=-----c5a2c31379d----1-----------------bookmark_preview----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----0-----------------clap_footer----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----c5a2c31379d----0---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----c5a2c31379d----0-----------------bookmark_preview----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Youssef Hosni"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Getting Started With Stable DiffusionStable Diffusion is a text-to-image latent diffusion model created by researchers and engineers from CompVis, Stability AI, and LAION. It\u2019s\u2026"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "\u00b712 min read\u00b7Nov 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&user=Youssef+Hosni&userId=859af34925b7&source=-----f343639e4931----1-----------------clap_footer----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----c5a2c31379d----1---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&source=-----c5a2c31379d----1-----------------bookmark_preview----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----c5a2c31379d----2---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----c5a2c31379d----2---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----c5a2c31379d----2---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----c5a2c31379d----2---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----c5a2c31379d----2---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----c5a2c31379d----2---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----c5a2c31379d----2---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----c5a2c31379d----2-----------------bookmark_preview----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c5a2c31379d----3---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----c5a2c31379d----3---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----c5a2c31379d----3---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----c5a2c31379d----3---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c5a2c31379d----3---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c5a2c31379d----3---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c5a2c31379d----3---------------------8c53fc50_c72d_41f8_bd03_a937957eccb8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----c5a2c31379d----3-----------------bookmark_preview----8c53fc50_c72d_41f8_bd03_a937957eccb8-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----c5a2c31379d--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}