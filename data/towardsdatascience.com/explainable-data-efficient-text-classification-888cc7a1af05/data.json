{"url": "https://towardsdatascience.com/explainable-data-efficient-text-classification-888cc7a1af05", "time": 1683005467.069095, "path": "towardsdatascience.com/explainable-data-efficient-text-classification-888cc7a1af05/", "webpage": {"metadata": {"title": "Explainable, data-efficient text classification | by Tomasz Pietruszka | Towards Data Science", "h1": "Explainable, data-efficient text classification", "description": "Key to practical deep learning \u2014 transfer learning - In computer vision - In natural language processing (NLP) - Contextualized word representations - Current state of the art Proposed architecture \u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.tesmanian.com/blogs/tesmanian-blog/flashing-arrow-signal-light-from-the-tesla-s-fsd-preview-hints-completion-is-near", "anchor_text": "mass-market cars understand their surroundings in real-time", "paragraph_index": 2}, {"url": "https://techcrunch.com/2019/11/01/hailing-a-driverless-ride-in-a-waymo/", "anchor_text": "cars drive fully autonomously", "paragraph_index": 2}, {"url": "https://www.nature.com/articles/s42003-018-0110-y", "anchor_text": "similarly to how humans do", "paragraph_index": 3}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet", "paragraph_index": 3}, {"url": "http://cocodataset.org", "anchor_text": "COCO", "paragraph_index": 3}, {"url": "https://happywhale.com/", "anchor_text": "recognize individual whales by their pictures", "paragraph_index": 3}, {"url": "https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/overview", "anchor_text": "detect deforestation using satellite imagery", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751", "anchor_text": "use models pre-trained on ImageNet for all kinds of applications", "paragraph_index": 4}, {"url": "https://www.robertoreif.com/blog/2018/6/26/classification-of-retinal-cross-sectional-optical-coherence-tomography-images", "anchor_text": "achieve 85% accuracy in a 4-category classification of disorders present in OCT images of retinas", "paragraph_index": 6}, {"url": "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf", "anchor_text": "DeViSE", "paragraph_index": 6}, {"url": "https://www.aclweb.org/anthology/C16-1116/", "anchor_text": "categorizing questions into 50 categories with 97.2% accuracy", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Winograd_Schema_Challenge", "anchor_text": "Winograd Schema", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "TF-IDF", "paragraph_index": 10}, {"url": "https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf", "anchor_text": "might be a good idea", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2Vec", "paragraph_index": 12}, {"url": "https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html", "anchor_text": "analogies between words correspond to arithmetic relationships between their vectors", "paragraph_index": 12}, {"url": "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/", "anchor_text": "taken from Wikipedia", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/replicating-the-toronto-bookcorpus-dataset-a-write-up-44ea7b87d091", "anchor_text": "public domain books", "paragraph_index": 19}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMO", "paragraph_index": 20}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "SentencePiece", "paragraph_index": 23}, {"url": "https://www.aclweb.org/anthology/C18-1139/", "anchor_text": "FLAIR", "paragraph_index": 23}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT", "paragraph_index": 25}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT", "paragraph_index": 26}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc", "anchor_text": "dot-product attention", "paragraph_index": 34}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "dataset: IMDB", "paragraph_index": 35}, {"url": "https://www.imdb.com", "anchor_text": "imdb.com", "paragraph_index": 35}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT", "paragraph_index": 36}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT", "paragraph_index": 39}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT", "paragraph_index": 43}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "machine translation", "paragraph_index": 49}, {"url": "https://arxiv.org/pdf/1502.03044.pdf", "anchor_text": "image caption generation", "paragraph_index": 49}, {"url": "https://ulmfit.purecode.pl", "anchor_text": "interactive demo", "paragraph_index": 51}, {"url": "https://github.com/tpietruszka/ulmfit_experiments", "anchor_text": "older version of the code", "paragraph_index": 57}, {"url": "http://2019.poleval.pl/", "anchor_text": "PolEval 2019", "paragraph_index": 57}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "SentencePiece", "paragraph_index": 58}, {"url": "http://2019.poleval.pl/files/poleval2019.pdf", "anchor_text": "proceedings", "paragraph_index": 59}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT", "paragraph_index": 59}, {"url": "https://github.com/n-waves/multifit", "anchor_text": "MultiFiT", "paragraph_index": 59}, {"url": "https://github.com/tsvm/factcheck-cqa", "anchor_text": "Fact-Checking Questions", "paragraph_index": 63}, {"url": "https://github.com/tpietruszka/ulmfit_attention", "anchor_text": "Code", "paragraph_index": 67}], "all_paragraphs": ["In this article you can find:", "Key to practical deep learning \u2014 transfer learning- In computer vision- In natural language processing (NLP)- Contextualized word representations- Current state of the artProposed architecture- ULMFiT \u2014 recap - Branching Attention \u2014 proposed classifier headExperiments and results- IMDB samples- Head-only training- Benefits of head-only training- Attention visualization- Full IMDB, other datasets- What did not work, possible improvementsConclusions", "Applying deep neural networks to unstructured data, most commonly images, can yield impressive results. Academic papers and prototypes aside, mass-market cars understand their surroundings in real-time, by analyzing video feeds. In some areas, cars drive fully autonomously. In these cases, and many others, deep learning is used to transform raw numbers of the images\u2019 individual pixels into some level of understanding of the scene.", "While understanding images comes naturally to humans, for a computer it is no easy feat. It takes a lot of computations and enormous amounts of data to teach a machine how to see the world somewhat similarly to how humans do. In practical terms, it also takes a lot of time and money. The example above? It was trained using more than 14 million manually annotated images (ImageNet, then COCO). Yet, there are many practical applications in all kinds of niches \u2014 countless industrial ones, and many used by NGOs \u2014 e.g. to recognize individual whales by their pictures or detect deforestation using satellite imagery.", "Key to using deep learning without huge datasets and budgets? Re-purposing already trained models, and the knowledge they contain; more formally: transfer learning. Computer vision has undergone a revolution of sorts when in the mid-2010s it became standard practice to use models pre-trained on ImageNet for all kinds of applications.", "The pre-training task is simple: for each image, decide what is the main object in it by choosing one of the (20,000+) pre-defined categories. To solve it, the neural networks learn how to extract meaningful representations of images from raw pixel data. First, convolutional layers learn to detect simple features, such as edges and corners. Next layers use that information to recognize more complex shapes \u2014 perhaps wings and beaks when looking at images of birds. The final layers can recognize specific types of objects \u2014 e.g. birds, cars or planes of various types.", "We can use the pre-trained networks to compute representations of images and train our models to use these representations. Alternatively, we can remove several last layers of the pre-trained network, replace them with new, randomly-initialized ones, and train the whole network to perform the task at hand. This way it is possible to quickly build prototype systems, using datasets with only a few thousand, or a few hundred labeled images. For example, 400 training samples were sufficient to achieve 85% accuracy in a 4-category classification of disorders present in OCT images of retinas \u2014 a very different kind of images than the pictures of animals and vehicles the original network was trained on. With some additional effort, e.g. for classification tasks, the data requirements can be lowered further. Taking it to the extreme, classifying images into categories without seeing any examples of them, is also possible (zero-shot learning, e.g. DeViSE).", "Automated processing of natural language is a challenging problem. Systems performing various tasks in this domain have existed for several decades, but until relatively recently they were predominantly rule-based. These systems can perform very well (example: categorizing questions into 50 categories with 97.2% accuracy) and their behavior can be easily understood and debugged. Unfortunately, since they are based on manually-developed systems of rules and knowledge bases, their development is labor-intensive. Their complexity grows immensely with expected functionality, so they are generally only applicable to well-defined, narrow tasks. Furthermore, their operation can be disrupted by typos or grammatical errors.", "The motivation for developing statistical, data-driven NLP systems is clear. In many cases, it is easier to gather a dataset relevant to the task at hand, than it is to develop a massive set of rules. It translates into lower costs and a shorter time of system development. Additionally, it provides an opportunity for the system to perform better with time, as more data is gathered. Besides, a lot of text data is freely available \u2014 in forms of books, online publications, etc. \u2014 and arguably contains most of humanity\u2019s knowledge. Making use of it, directly, as a knowledge base could enable the construction of new, very capable systems.", "Building a system able to fully understand natural language is an extremely hard, unsolved problem. Resolving linguistic ambiguities often requires context, knowledge of idioms, detecting sarcasm, and even general knowledge and human \u201ccommon sense\u201d (see Winograd Schema Challenge). Nevertheless, the problem is well worth pursuing \u2014 because of immediate, direct applications, as well as a possibility of getting closer to understanding general intelligence.", "The first statistical methods in NLP started much simpler. They represented text documents as counts of words they contain (representation called bag-of-words). Improvements of this approach suggested using frequencies instead of counts, and often frequencies in relation to how common a given word is generally (TF-IDF). These methods completely disregard word order, so only use a small part of available information. Using n-grams (sequences of n words) instead of individual words, with similar further processing, is a viable way of incorporating word order information. Unfortunately, using high n values is unfeasible, as the number of possible word combinations grows exponentially with n.", "The count-based representations described above remained the foundation of state-of-the-art statistical solutions for a long time. Because of their simplicity and computational efficiency, for some applications, they remain important baselines. For example, for topic classification and sentiment analysis, trying Naive Bayes or SVM classifiers, based on bag-of-words or bi-gram features, might be a good idea (in case of the full IMDB task described below, it achieves 90\u201391% accuracy, compared to 95% with ULMFiT \u2014 but is orders of magnitude faster).", "The first wave of changes came in 2013, with the advent of Word2Vec. Models from this category produced a numerical representation (called \u201cword vector\u201d, or \u201cembedding\u201d) for each word in their vocabulary. They were built as shallow neural networks, trained on short sequences from a large corpus of freely available text. Representations of each word were based on its usual context and already captured a lot of the language\u2019s semantics. Famously, analogies between words correspond to arithmetic relationships between their vectors, e.g. king \u2014 man + woman ~= queen.", "Each word in the vocabulary was represented as a fixed-length vector (e.g. 300 numbers) \u2014 set of coordinates of a point in the embedding space. Since the vocabulary size (number of unique words) is usually orders of magnitude higher, the representation is distributed \u2014 words and concepts are represented by certain combinations of coordinates, rather than a single element of the vector.", "As part of a complete system, word embeddings would be used to encode each word of a document, producing a variable-length sequence of vectors (length corresponding to the number of words). Many algorithms using these representations were developed \u2014 from averaging all the word vectors and training an SVM classifier on the results, to passing the sequence through recurrent or convolutional networks.", "Word embeddings have one central problem. When used to encode words of a document, they encode each word individually \u2014 ignoring the context. The vector representing root will be the same in:", "Words with more than one possible meaning are a problem for most practical applications of NLP. For some applications, e.g. sentiment analysis, syntactic ambiguity has to be resolved as well. This can range from simple problems (was a phrase negated?) to very complex, sometimes impossible ones (was the phrase used ironically?).", "Let us consider a practical scenario. We want to build a system which will estimate public sentiment towards brands, by classifying tweets about them as positive or negative, and then calculate a fraction of positive ones. We need training data \u2014 we might download a number of relevant tweets about various brands, and then manually label them (or crowdsource the task).", "If we use word embeddings and feed the encoded text to a neural network, we will be able to train it to perform the classification task \u2014 distinguishing the positive messages from negative ones. But because word embeddings do not capture context, just individual words, our network has to learn the whole structure of the language, and even higher-level concepts like irony or sarcasm, at the same time! All based on our precious, manually-labeled data. This is clearly inefficient.", "To help neural networks \u201cunderstand\u201d the structure of languages, researchers have developed several network architectures and training algorithms, which compute contextualized representations of words. The key difference compared to word embedding-based approaches: this understanding of context is trained on unlabeled, freely available text from a given language (or multiple languages). Data is often taken from Wikipedia, public domain books, or just scraping the Internet.", "In 2018\u20132019 they started a \u201crevolution\u201d of sorts, analogous to the one in computer vision several years earlier. Deep neural networks are commonly used to compute rich, contextualized representations of text \u2014 as opposed to the context-unaware word vectors. These representations can then be used by another neural network (e.g. ELMO). More commonly, the pre-trained network has its last layer replaced with a set of different ones, designed for the task at hand (often called a downstream task). The weights of the new layers are initialized randomly and trained using the labeled data to perform the downstream task. The process is much easier since the hardest part \u2014 language understanding \u2014 has mostly been done.", "The initial training, meant to train the network\u2019s \u201cunderstanding\u201d of text is based on unlabeled data. Labels \u2014 things for the model to predict \u2014 are automatically generated from the data itself. Training with such labels is often referred to as unsupervised pre-training.", "The most common approaches to unsupervised pre-training are:", "Approaches to tokenization \u2014 splitting text into its basic chunks \u2014 also vary. The simplest, but sub-optimal strategy would be to split them every time a space character is encountered. Word-based tokenization is used in some cases, but usually includes rules specific to each language (e.g. splitting \u201cdon\u2019t\u201d into \u201cdo\u201d and \u201cn\u2019t\u201d). Sub-word tokenization, (e.g. SentencePiece) is perhaps the most common \u2014 it is derived from data, based on the frequency of particular character sequences, often treating white space as yet another character. It is also worth noting, that character-level language models are also sometimes used in practice (e.g. FLAIR).", "Two groups of network architectures dominate the space:", "This article describes a novel network architecture for classification models working on text documents \u2014 a modification of ULMFiT. Using that paper\u2019s nomenclature, a different classifier head is proposed.", "Training an ULMFiT model from scratch consists of 3 steps:", "The network architecture used in the ULMFiT paper is depicted below. It contains:", "The classifier head performs Concat Pooling, then passes its results through one fully connected layer and the output layer.", "Concat poling is a simple concatenation of 3 elements:", "The described architecture is meant to directly address two main problems with using average-pooling, and/or max-pooling to aggregate text representations generated by recurrent language models.", "Therefore, the classifier head contains two \u201cbranches\u201d, each answering one question:", "Both are implemented as simple, fully connected neural networks. Numbers and sizes of their layers are additional hyper-parameters, the choice of which is discussed in the \u201cResults\u201d section. The networks are applied independently to each sequence element. Values returned by the ATT branch \u2014 a scalar (e\u2c7c) for each sequence element \u2014 are then passed through a Softmax function to obtain proper weights (a\u2c7c) for a weighted average. A weighted average of vectors (b\u2c7c) returned by AGG becomes the final representation of the sequence (C).", "Diagram of the \u201cBranching Attention\u201d classifier head:", "note: if the AGG branch is skipped, and ATT only has the output layer, the whole aggregation reduces to dot-product attention, with a single, trainable query. It does not produce particularly good results, as discussed in the next section.", "Most experiments were performed using a popular sentiment classification dataset: IMDB. It contains 50,000 movie reviews, written by users of the imdb.com website. They are labeled as positive if the user\u2019s rating of the movie was 7 or above, negative for 4 or lower. The classes are balanced, there are equal numbers of positive and negative reviews for each movie. There are no more than 30 reviews per movie. Document lengths vary greatly \u2014 many are relatively short, but 2% are longer than 1000 words. It is also worth noting, that the labels are somewhat \u201cnoisy\u201d, e.g. there are some positive reviews labeled as negative.", "To simulate a small dataset situation, yet make statistically significant comparisons between architectures, the experiments were conducted in the following way: models were trained on 1000-element samples of the IMDB training dataset, and evaluated on the whole test set. This was repeated 20 times for each architecture and hyper-parameter set. The training-related hyper-parameters and a single dropout multiplier (as recommended by ULMFiT authors) were tuned separately for Branching Attention and Concat Pooling heads.", "Below, the results of the proposed architecture (Branching Attention) are compared with the baseline (Concat Pooling). Two additional variants are provided, as a minimal ablation study:", "While by no means ground-breaking, the results above seem encouraging. Without changing the encoder \u2014 the most important part of the model \u2014 the number of incorrectly classified samples was reduced by over 10%. The proposed architecture seems to make better use of the encoder\u2019s representation of text.", "In the best configuration presented above, the Branching Attention head had 30% fewer parameters compared to the original ULMFiT architecture. In other configurations, performing only slightly worse, the number of parameters decreased by up to 85%.", "It is worth noting that removing either branch of the Branching Attention head causes a significant drop in performance \u2014 even below the baseline levels. While not shown here for brevity, reducing the depth of either branch (to a single layer) also results in a small, but consistent, drop in performance. The results support using the complete network architecture, with both branches, as previously described.", "The distributions of accuracy scores from individual experiment runs are shown below, as a box plot. It is meant to visualize the variance of results for each configuration, resulting from different sampling of the training dataset and different random initialization.", "Worth noting: the single, negative outlier present for all four architectures, denoted as a dot, comes from training on the same sample of the training dataset \u2014 presumably of inferior quality. Likely, if training was repeated with the same dataset each time (only changing the random seed), the variance of accuracy for each configuration would be much lower. However, the use of different dataset samples was meant to ensure that any conclusions are more general \u2014 not specific to a particular dataset. Overall, the improvement of mean accuracy cannot be reasonably explained as random noise.", "What other benefits can we get from using a classifier head with a trainable aggregation, instead of a fixed one? We might not need to modify the parameters of the encoder at all. According to ULMFiT, we should first optimize the classifier head, and then gradually add layers of the encoder to the optimizer\u2019s scope \u2014 this approach was described in the previous section. However, training exclusively the classifier\u2019s head has important practical benefits, as discussed in the next section. The table below summarizes results obtained this way \u2014 only optimizing parameters of the classifier\u2019s head.", "Somewhat surprisingly, for the full Branching Attention head, the results did not deteriorate at all. Training classifiers this way can be a viable, practical approach, especially for relatively small datasets. As expected, the gap between the proposed architecture and Concat Pooling has increased.", "The variant without the AGG branch performed above expectations \u2014 better than when the whole network was optimized (in the previous section). It might indicate that the training process when optimizing the full network could be improved upon \u2014 but such attempts were not successful.", "Why is it important that the proposed architecture performs much better than the baseline when only training the classifier head? There are several practical advantages to building systems this way. Training is faster and requires less memory. A less obvious benefit lies in the low number of parameters that are unique to the model being trained.", "New use cases open in scenarios where many classifiers operate on texts from the same domain, e.g. \u201ctweets\u201d or \u201cnews articles\u201d for a given language. It becomes very cheap to train, store and run a new model:", "Where can it be useful? There are many possible scenarios, examples might include:", "By analyzing attention weights in a neural network we can understand it better \u2014 we can see which parts of the input are relevant to the task at hand. Interesting visualizations have been demonstrated e.g. in machine translation and image caption generation.", "With Branching Attention, in some circumstances, we can take it a step further. If the classification problem is binary, we can achieve quite good results by setting the last (or only) dimension of the AGG branch to 1. Effectively, for each input token, we will obtain a single, scalar value, and the final decision will be a weighted average of these values (after a simple transformation). In the case of sentiment analysis, we can show \u201clocal\u201d sentiment after processing each token, as well as importance scores of specific parts of the document.", "See the example below, and check out the interactive demo. In the demo, it is possible to visualize both weights and sentiment at the same time (as in the example), just the weights or just the sentiment. It is also possible to test the model on a different piece of text.", "Opacity of the color behind each token means the attention weight associated with it. Hue denotes the value of a feature (sentiment) calculated for that token, considering its left context, in a red-to-green scale.", "To verify if the proposed architecture performs well generally, or just for the particular dataset type and size, several other experiments were conducted.", "When training on the full IMDB dataset, in its default train/test split, the results are approximately equal to those of a Concat Pooling-based classifier (albeit Branching Attention seems to be less sensitive to the choice of training-related hyper-parameters). Intuitively, the large training dataset might contain enough information to modify the encoder in such a way, that its output will be meaningfully aggregated by average- and max-pooling.", "When training on the full IMDB dataset, but optimizing only the classifier head, Branching Attention performs significantly better than the baseline. As expected, it performs worse than when the whole network is optimized. Nevertheless, this approach can be useful in some cases, as discussed in the \u201cBenefits of head-only training\u201d section.", "Automatic Cyber-bullying Detection \u2014 PolEval 2019 Task 6", "Using an older version of the code, a system based on Branching Attention was entered into the PolEval 2019 contest \u2014 task 6. Its goal was to prepare a system assessing which tweets, written in Polish, might contain cyber-bullying content. A crowd-labeled training set of ~10,000 example tweets was provided. It was highly imbalanced \u2014 with an overwhelming majority of examples not containing cyber-bullying.", "The system used a sub-word tokenization mechanism \u2014 SentencePiece \u2014 to deal with misspellings common on Twitter. This type of tokenization is generally beneficial for Polish \u2014 as it is a fusional language. Its abundant pre- and post-fixes dramatically increase the number of unique words \u2014 which is a problem with traditional, word-based tokenization approaches. The language model was trained on Polish Wikipedia and then fine-tuned on a large, unlabelled dataset of Polish-language tweets.", "In the contest, the system came in 3rd (proceedings, page 106), behind an ULMFiT \u2014 like system with a larger, improved language model (similar to, and by co-authors of, MultiFiT). However, it ranked just ahead of a BERT-based system.", "Performance of Branching Attention and Concat Pooling was then compared in the same way as in the \u201cIMDB sample\u201d experiment, with hyper-parameters tuned on the IMDB dataset. Both models were trained 20 times, each time on a different sub-sample of the training dataset, and with different random initialization. To directly compare models\u2019 performance and avoid issues with setting a detection threshold, the Average Precision metric was chosen.", "In a completely different context \u2014 different language, tokenization strategy, different (more complex) task, without further hyper-parameter optimization, Branching Attention provided a modest, but noticeable performance improvement. It can be considered a successful test of the proposed architecture.", "Question categorization \u2014 SemEval-2019 Task 8A", "Another verification of the method\u2019s results was performed using the Fact-Checking Questions dataset. The dataset was extracted from \u201cQatar Living\u201d \u2014 a community question-answering forum. Each document \u2014 first post in a topic \u2014 consists of a category, a subject line, and the post\u2019s content. The fields were concatenated together, with delimiters between them. The task: assigning a category to each question: \u201cfactual\u201d, \u201copinion/advice\u201d or \u201csocial\u201d.", "In an experimental setup identical to the one described above, with the same hyper-parameters, the obtained results were:", "Once again, the network using Branching Attention performed significantly better \u2014 despite both networks sharing the same pre-trained encoder.", "Several attempts to improve the network\u2019s performance were made:", "Code and the fine-tuned language model, needed to reproduce this article\u2019s results, are provided.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, ex-quant. Experienced in machine learning for time-series, interested in natural language processing"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F888cc7a1af05&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tomek.pietruszka?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tomek.pietruszka?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "Tomasz Pietruszka"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb75fbfaf9618&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&user=Tomasz+Pietruszka&userId=b75fbfaf9618&source=post_page-b75fbfaf9618----888cc7a1af05---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F888cc7a1af05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F888cc7a1af05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "http://2019.poleval.pl/index.php/tasks/task6", "anchor_text": "Automatic Cyber-bullying Detection"}, {"url": "http://ulmfit.purecode.pl", "anchor_text": "try it yourself"}, {"url": "https://www.tesmanian.com/blogs/tesmanian-blog/flashing-arrow-signal-light-from-the-tesla-s-fsd-preview-hints-completion-is-near", "anchor_text": "mass-market cars understand their surroundings in real-time"}, {"url": "https://techcrunch.com/2019/11/01/hailing-a-driverless-ride-in-a-waymo/", "anchor_text": "cars drive fully autonomously"}, {"url": "https://pjreddie.com/darknet/yolo/", "anchor_text": "YOLO v3"}, {"url": "https://www.nature.com/articles/s42003-018-0110-y", "anchor_text": "similarly to how humans do"}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet"}, {"url": "http://cocodataset.org", "anchor_text": "COCO"}, {"url": "https://happywhale.com/", "anchor_text": "recognize individual whales by their pictures"}, {"url": "https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/overview", "anchor_text": "detect deforestation using satellite imagery"}, {"url": "https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751", "anchor_text": "use models pre-trained on ImageNet for all kinds of applications"}, {"url": "https://www.robertoreif.com/blog/2018/6/26/classification-of-retinal-cross-sectional-optical-coherence-tomography-images", "anchor_text": "achieve 85% accuracy in a 4-category classification of disorders present in OCT images of retinas"}, {"url": "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf", "anchor_text": "DeViSE"}, {"url": "https://www.aclweb.org/anthology/C16-1116/", "anchor_text": "categorizing questions into 50 categories with 97.2% accuracy"}, {"url": "https://en.wikipedia.org/wiki/Winograd_Schema_Challenge", "anchor_text": "Winograd Schema"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "TF-IDF"}, {"url": "https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf", "anchor_text": "might be a good idea"}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2Vec"}, {"url": "https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html", "anchor_text": "analogies between words correspond to arithmetic relationships between their vectors"}, {"url": "https://www.aclweb.org/anthology/P19-1315/", "anchor_text": "Towards Understanding Linear Word Analogies"}, {"url": "https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html", "anchor_text": "blog post"}, {"url": "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/", "anchor_text": "taken from Wikipedia"}, {"url": "https://towardsdatascience.com/replicating-the-toronto-bookcorpus-dataset-a-write-up-44ea7b87d091", "anchor_text": "public domain books"}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMO"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html", "anchor_text": "ELECTRA"}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "SentencePiece"}, {"url": "https://www.aclweb.org/anthology/C18-1139/", "anchor_text": "FLAIR"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Transformer networks"}, {"url": "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a", "anchor_text": "self-attention mechanism"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "here"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace Transformers"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://www.aclweb.org/anthology/D19-1572.pdf", "anchor_text": "MultiFiT"}, {"url": "https://arxiv.org/abs/1911.11423", "anchor_text": "SHA-RNN"}, {"url": "https://arxiv.org/pdf/1909.01792.pdf", "anchor_text": "Mogrifier LSTM"}, {"url": "https://www.aclweb.org/anthology/D19-1572.pdf", "anchor_text": "MultiFiT"}, {"url": "https://github.com/fastai/", "anchor_text": "Fast.ai"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://github.com/n-waves/multifit", "anchor_text": "MultiFiT repository"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTM"}, {"url": "https://arxiv.org/pdf/1708.02182.pdf", "anchor_text": "AWD-LSTM"}, {"url": "https://openai.com/blog/unsupervised-sentiment-neuron/", "anchor_text": "can be approximately true"}, {"url": "https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc", "anchor_text": "dot-product attention"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "dataset: IMDB"}, {"url": "https://www.imdb.com", "anchor_text": "imdb.com"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://www.yuspify.com/blog/cold-start-problem-recommender-systems/", "anchor_text": "cold start problem"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "machine translation"}, {"url": "https://arxiv.org/pdf/1502.03044.pdf", "anchor_text": "image caption generation"}, {"url": "https://ulmfit.purecode.pl", "anchor_text": "interactive demo"}, {"url": "https://github.com/tpietruszka/ulmfit_experiments", "anchor_text": "older version of the code"}, {"url": "http://2019.poleval.pl/", "anchor_text": "PolEval 2019"}, {"url": "https://github.com/google/sentencepiece", "anchor_text": "SentencePiece"}, {"url": "http://2019.poleval.pl/files/poleval2019.pdf", "anchor_text": "proceedings"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://github.com/n-waves/multifit", "anchor_text": "MultiFiT"}, {"url": "https://github.com/tsvm/factcheck-cqa", "anchor_text": "Fact-Checking Questions"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMO"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://ulmfit.purecode.pl/", "anchor_text": "demo"}, {"url": "https://www.aclweb.org/anthology/P18-1031.pdf", "anchor_text": "ULMFiT"}, {"url": "https://github.com/tpietruszka/ulmfit_attention", "anchor_text": "Code"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----888cc7a1af05---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----888cc7a1af05---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----888cc7a1af05---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----888cc7a1af05---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----888cc7a1af05---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F888cc7a1af05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&user=Tomasz+Pietruszka&userId=b75fbfaf9618&source=-----888cc7a1af05---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F888cc7a1af05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&user=Tomasz+Pietruszka&userId=b75fbfaf9618&source=-----888cc7a1af05---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F888cc7a1af05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F888cc7a1af05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----888cc7a1af05---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----888cc7a1af05--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----888cc7a1af05--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----888cc7a1af05--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tomek.pietruszka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tomek.pietruszka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tomasz Pietruszka"}, {"url": "https://medium.com/@tomek.pietruszka/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "17 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb75fbfaf9618&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&user=Tomasz+Pietruszka&userId=b75fbfaf9618&source=post_page-b75fbfaf9618--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd25f672aec9d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplainable-data-efficient-text-classification-888cc7a1af05&newsletterV3=b75fbfaf9618&newsletterV3Id=d25f672aec9d&user=Tomasz+Pietruszka&userId=b75fbfaf9618&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}