{"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600", "time": 1682993327.023621, "path": "towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600/", "webpage": {"metadata": {"title": "The Variational Autoencoder as a Two-Player Game \u2014 Part III | by Max Frenzel, PhD | Towards Data Science", "h1": "The Variational Autoencoder as a Two-Player Game \u2014 Part III", "description": "Variational Autoencoders and their application to natural language can be understood as a cooperative game. This final part looks at the unique difficulties of encoding text and how to overcome them."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II", "paragraph_index": 45}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II", "paragraph_index": 73}, {"url": "http://www.aclweb.org/anthology/K16-1002", "anchor_text": "One approach, called \u201cword dropout\u201d", "paragraph_index": 75}, {"url": "https://arxiv.org/pdf/1708.04729.pdf", "anchor_text": "In one of them", "paragraph_index": 81}, {"url": "https://arxiv.org/abs/1702.08139", "anchor_text": "dilated convolutions", "paragraph_index": 87}, {"url": "http://www.aclweb.org/anthology/K16-1002", "anchor_text": "One more method (known as KL cost annealing)", "paragraph_index": 95}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I", "paragraph_index": 96}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II", "paragraph_index": 96}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I", "paragraph_index": 108}, {"url": "http://www.maxfrenzel.com", "anchor_text": "www.maxfrenzel.com", "paragraph_index": 125}], "all_paragraphs": ["Welcome back to the final part of this three part series on variational autoencoders (VAEs) and their application to encoding text.", "In Part I we met Alice and Bob, who were preparing for the Autoencoding Olympics. While following their training process, we learned about the concept of autoencoders and some of the basics of deep learning.", "Unfortunately, due to some training issues we uncovered, we had to watch them fail miserably in their quest for a gold medal.", "However, they managed to redeem themselves in Part II. By following an extra difficult training regimen, known as a Variational Autoencoding, they managed to overcome their problems and returned to the Olympics to dominate their competition and claim a decisive victory.", "Now let\u2019s rejoin Alice and Bob after they return home with their gold medals.", "Alice and Bob are overjoyed by their victory. But they are quickly looking for a new challenge.", "They decide to aim for a new discipline in the next Olympics. This discipline requires them not to encode images, but sentences. So Bob hangs up his paint brush and grabs a pen instead, and Alice gets reading.", "The basic rules of the game are essentially still the same.", "Alice has to read a sentence, which she needs to encode and send to Bob, who then has to try to reconstruct the sentence from Alice\u2019s code.", "Again we have to keep in mind that, just like in the image case in Part I, our AI Alice and Bob have absolutely no preconceived notion of language, not even the meaning of individual words, let alone complex sentences.", "Initially, the sentence \u201cA tall man stands at the side of the road.\u201d is just as likely to them and carries just as much meaning as \u201cChurch doll regret lake unarmed machine changed appears knot precede.\u201d", "Their basic toolbox is what\u2019s called their vocabulary. The set of words they can play with.", "The problem is they have no idea about the meaning of those words or how to combine them into sequences that carry meaning.", "It is as if they were handed a dictionary, but without any explanations. Just a long list of all the words in the English language. (Not that the explanations would have helped, since those themselves are formed of the same words that have no meaning to our players yet.)", "Once again, Alice and Bob have to make sense of the external world from scratch, via their interactions with the sentences their are provided and the feedback Bob gets from their coach Charlie.", "The way Charlie judges Bob\u2019s prediction is slightly different in this variation of the game though.", "Previously Charlie waited for Bob to paint the entire image.", "Now on thr other hand he does not wait for Bob to finish the entire sentence. Instead he gives Bob a score and feedback after every single word he predicts.", "Even more crucially, Charlie tells Bob what would have been the correct word.", "This simplifies Bob\u2019s task tremendously. Instead of predicting the entire sentence based only on Alice\u2019s code, he can predict one word at a time, relying on the words he has already seen to refine his prediction of the next word.", "Every one of us has an inbuilt (or rather learned) language model.", "Consider the sentence fragment \u201cThe dog chases the\u2026\u201d.", "What would you think the next word is? To determine this you just invoked your own language model.", "Different people have different language models given their background and experiences, but in this case I would bet that almost everyone would have guessed \u201ccat\u201d.", "But what if I now told you that this sentence was taken from a quirky science fiction story about an alien invasion?", "You might still think \u201ccat\u201d is most likely, but you\u2019re probably not quite so sure anymore. Or maybe you even expect something different as most likely. You conditioned your language model on an additional piece of information I gave you.", "This conditioning is exactly what Bob needs to learn to get a high score. In particular, he needs to condition his language model on Alice\u2019s code. And Alice once again needs to figure out a clever way to convey as much information as possible in the two numbers she is allowed to send to Bob.", "The problem is that because of the way Charlie provides his feedback, Bob can actually become pretty decent at the game without conditioning his language model.", "Like in the example sentence above, in many cases the guess \u201ccat\u201d would have been right, leading to a good score. Only in a few outlier sentences will this be a bad guess.", "But a language model, if unconditioned, can be misleading.", "Let\u2019s assume Bob, with corrections from Charlie, has so far guessed the text fragment \u201cThe dog chases the\u2026\u201d and look at what might happen next.", "Let\u2019s assume that the full sentence is \u201cThe dog chases the flamboyant spaceship from Alpha Centauri\u201d. By the time Bob gets to guess the last word his language model might have recovered from the initial shock of the \u201cflamboyant spaceship\u201d and he might make a reasonable guess about \u201cCentauri\u201d even without conditioning on Alice\u2019s code. But the total score for the entire sentence will already have suffered a lot.", "The trick, given the limited information flow allowed, lies in Alice encoding exactly the kind of information she thinks is surprising to Bob and letting him rely on his own language model for the rest, and also hoping that he actually uses her information in the first place.", "This is what information theorists call an efficient code.", "Encode exactly what\u2019s the most surprising and omit the rest.", "This is also closely related to the concept of entropy you might have heard thrown around in various contexts. But a thorough discussion of that would require many articles in its own right.", "In the past, people have tried coming up with efficient codes for all sorts of problems manually. But now, if trained correctly, models like VAEs can actually automatically find highly efficient codes for very complex problems.", "Note that in reality the decoder doesn\u2019t actually predict just a single word at each step. It actually predicts a probability for each word in its vocabulary.", "So while in the example Bob might have given \u201ccat\u201d a 99.9% probability, he would have also given every single other word he knows a finite probability, including maybe 0.0000037% for the correct word \u201cflamboyant\u201d.", "This is what allows our critic Charlie to give Bob a precise score. He only receives a perfect score if he assigns 100% probability to the correct word. The lower the probability he gives to the correct solution, the worse his score.", "Sounds all good, doesn\u2019t it? Bob only needs to condition his language model on Alice\u2019s code and they are all set again, right?", "Well, yes. But\u2026 It turns out that in this new discipline Alice is struggling quite a bit.", "In the case of the cat and dog images, we saw in Part I that Bob can get some early victories (painting a gray/brown blob with two smaller round blobs as eyes) without consulting Alice\u2019s code (which at that time was still random due to the lack of feedback from Bob).", "But it doesn\u2019t get him very far. Fairly soon he is stuck and needs to figure out how to use Alice\u2019s code.", "However, in this new discipline Bob can actually get pretty decent at the game without considering Alice\u2019s code (and hence without giving her any useful feedback).", "Also, in Part II we noted that in the variational setting, a higher accuracy in code transmission comes at the cost of a higher penalty.", "Now, since Alice realizes that Bob isn\u2019t using her code anyway, she figures out that she might as well increase the uncertainty so that they don\u2019t pay an additional penalty for precisely sending a code that is useless anyway.", "As noted above, initially neither Bob nor Alice have any language model whatsoever. Text is just a random mess to them.", "But very early on, by counting the occurrence of words, Bob might realize that \u201cThe\u201d or \u201cA\u201d are the most common words at the beginning of a sentence, so he might just start every sentence with these.", "And just based on word frequency, these are the most common words in general. So initially Bob might figure out a strategy of repeating the same word over and over again (\u201cthe the the the the the\u201d) because he notices this gives him a higher score than just random guessing.", "But soon he\u2019ll notice that \u201cthe\u201d is usually followed by a noun. One of the most common nouns in English is \u201ctime\u201d, so as a first improvement Bob might learn to say \u201cthe time the time the time the\u2026\u201d.", "Slowly figuring out more and longer of these common word combinations, Bob builds his language model.", "He learns the statistics of the English language.", "Or at least the particular language that is used in their training data. A language model trained on tweets is very different from a language model trained on the bible.", "By the time Bob gets stuck by himself and can\u2019t improve the score any further, he\u2019s already learned quite a lot, whereas Alice is still stuck at the very beginning.", "Alice has a much harder time learning.", "Bob gets direct feedback after every single word he predicts, whereas the only feedback Alice gets on her understanding of the entire sentence comes from the two feedback numbers Bob sends her.", "And if Bob doesn\u2019t use her code at all he also can\u2019t give her useful feedback.", "So Alice basically just gets random noise from Bob to improve her already random code.", "The variational setup further complicates this since Bob doesn\u2019t even get Alice\u2019s actual ideal code choice, but a value with some added uncertainty.", "The already random code is further randomised.", "So now with Bob being pretty decent all by himself and Alice having learned absolutely nothing yet, we risk being stuck at what is called a local maximum.", "Bob gets a decent score he can\u2019t improve by himself, but every time he tries to listen to Alice and use the code she sends him to influence his predictions, their score gets worse.", "The learning process is similar to mountain climbers without a map of the terrain looking for a summit in dense fog.", "The only thing they can do is go upwards in the steepest direction. Once they reach a point where any direction goes down hill, they assume they reached the ultimate summit. But they don\u2019t know that if they\u2019d just go down hill for a little while they could get to an even higher peak.", "So Bob, just like a climber who thinks he already accomplished the climb, abandons his attempts for improvement by using Alice\u2019s code.", "His process is already so refined that it\u2019s very sensitive to the change introduced by trying to condition his language model on the code.", "What he doesn\u2019t know is that if he could just sacrifice their score for a little while and try using the code and giving Alice some feedback, they could get unstuck.", "Alice could learn enough to provide Bob with useful codes that allow him to make predictions with much higher accuracy than he was ever able to by himself.", "But Bob is too nearsighted and self-confident to sacrifice their score.", "How can we help Alice with her difficult task of convincing Bob to use her code and provide good feedback?", "This question has been around in the academic community for a while. Ever since people started using VAEs on text and encountered exactly this problem.", "And it is still not completely solved. But many ideas have emerged that make variational autoencoding of text good enough to be useful in practice.", "Just like introducing the variational aspect in Part II made autoencoding harder but improved performance, so most strategies here also involve making the problem seemingly harder instead of easier.", "In particular, people tried making the task more challenging for Bob so that he can\u2019t quite as easily race ahead of Alice in terms of learning.", "One approach, called \u201cword dropout\u201d, has our critic Charlie sometimes staying silent.", "He still always gives Bob a score for his word prediction, but from time to time he doesn\u2019t reveal the correct answer, giving Bob\u2019s own language model less information to work with.", "Let\u2019s again look at our example sentence \u201cThe dog chases the\u2026\u201d. Let\u2019s assume Charlie stayed silent on the second word. So to Bob the sentence now looks like \u201cThe \u2026 chases the\u2026\u201d.", "I\u2019m sure your own language model has a much harder time predicting the next word for this sentence with incomplete information.", "The same is true for Bob. And he starts looking at Alice\u2019s code for clues.", "Other approaches are even more radical in modifying Charlie\u2019s behavior.", "In one of them, instead of giving Bob a score after every word, Charlie waits for Bob to finish the entire sentence before giving him any feedback.", "This is similar to what Charlie did in the image case (and the neural network architecture that\u2019s used for Bob in practice is actually also quite similar to the image case, both using convolutional neural networks).", "This way Bob basically can\u2019t rely on his own language model at all because he doesn\u2019t know if his predictions of the sentence started out correct or not.", "This gives him a huge incentive to consult Alice for additional information very early on.", "In practice this approach has not been too successful, the task becoming too difficult for Bob.", "But it has not been completely abandoned yet. It is still an active area of research.", "The most successful approaches (e.g. one building Bob\u2019s brain from dilated convolutions instead of the more common recurrent neural networks) actually all don\u2019t aim at making the task harder per se. Instead they are making Bob dumber, or more forgetful.", "If we make Bob dumber in just the right way, essentially giving him a slight learning disability, we basically give the disadvantaged Alice a chance to keep up with Bob\u2019s progress.", "Imagine for example that Bob get\u2019s very forgetful.", "Poor Bob can now only remember 2 words at a time. In the middle of a sentence he has no idea anymore how it started.", "Our example sentence now looks just like \u201c\u2026 chases the\u2026\u201d to him.", "Missing this crucial contextual information, he is eagerly looking for any information that can aid him figure out the next word. He happily turns to Alice\u2019s code.", "Approaches of this kind (although considerably more sophisticated) have proven extremely successful in ensuring that Alice and Bob learn in sync and don\u2019t get trapped at local maxima.", "Initially their score rises more slowly than it did when Bob was still smart. But crucially it keeps rising without them getting stuck.", "One more method (known as KL cost annealing) that can be combined with any of the other approaches looks at the way Charlie penalizes Alice for the code\u2019s uncertainty.", "As we saw in the image case in Part I and Part II, using the variational machines was crucial to learn examples outside of the training dataset. But it also increased the difficulty of Alice passing on useful information to Bob.", "The new method allows Charlie some flexibility in how much he wants to penalize Alice for using a small uncertainty.", "At the beginning, when they haven\u2019t learned anything yet, Charlie is very forgiving, allowing Alice to specify her code as precise as she wants without any penalty.", "But as the training progresses, Charlie gradually ramps up the penalty to its full extent.", "As we have seen, this might initially lead to \u201choles\u201d in the code, regions of code space that Bob has no clue how to decode. But later, as we reintroduce the penalty and Alice starts to use higher uncertainties, they will naturally have to learn to smooth out their code and make the holes disappear.", "Alice essentially gets a penalty that adjusts to her learning curve.", "Particularly, the task is no longer so difficult straight away that she doesn\u2019t end up learning anything.", "This, together with dumbing down Bob, allows them to get very successful at encoding text.", "So what kind of code might they learn?", "Bob needs to condition his language model on what Alice provides him.", "Even if they only figure out a simple code where certain code regions tell Bob that the text is a tweet, others a news article, and yet others restaurant reviews, can dramatically help Bob make better guesses about the next word.", "Since Bob also doesn\u2019t know when to actually stop his prediction, Alice might also learn to encode the length of the sentence.", "Just like in the image example in Part I with dog/cat on one axis and dark/light on another, they might learn a simple code with sentence length encoded in one dimension, and \u201cformality level\u201d encoded in the other dimension.", "Once again, as training continues and the two jointly learn, they might figure out much smarter (and probably less easily interpretable) ways of encoding lots of information in the two number.", "We also saw that the code should be smooth and continuous.", "That means if Alice wants to encode a sentence and not use too low of an uncertainty, she needs to make sure that her code is robust.", "For example she should use very similar codes for the sentences \u201cI went to his house\u201d and \u201cI went to his flat\u201d, so that some randomness that changes one code into the other won\u2019t have too dramatic consequences when Bob decodes it.", "The sentence \u201cThis problematic tendency in learning is compounded by the LSTM decoder\u2019s sensitivity to subtle variation in the hidden states, such as that introduced by the posterior sampling process\u201d however should have a very different code.", "Accidentally sending that instead of \u201cI went to his house\u201d would lead to a very crappy score from Charlie.", "Having refined their training process and thus their code, Alice and Bob once again return victorious from the Autoencoding Olympics.", "There are still many other disciplines, such as audio encoding.", "There are also completely different tasks which are not strictly autoencoding but still share many similarities, such as translation, where Alice encodes in one language and Bob decodes in a different language.", "Sounds even tricker, and indeed it is!", "But in this case there are slightly different rules that allow Bob to use what is called \u201cattention\u201d.", "Essentially, in addition to the code that Alice sends him for the whole sentence, he is also allowed to ask her for a \u201cbonus-code\u201d for each new word he needs to predict. This technique is basically what allowed Google Translate to become so good in recent years.", "Text summarisation is also a closely related discipline. Here Alice encodes a long text, and Bob has to decode it into a summary.", "There are also countless more training methods, including yet to be discovered ones, that will help them keep up with the increasingly strong competition in their already mastered disciplines.", "But for now, Alice and Bob need a well-deserved rest.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI Researcher, Writer, Digital Creative. Passionate about helping you build your rest ethic. Author of the international bestseller Time Off. www.maxfrenzel.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd8d56c301600&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8d56c301600--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8d56c301600--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://maxfrenzel.medium.com/?source=post_page-----d8d56c301600--------------------------------", "anchor_text": ""}, {"url": "https://maxfrenzel.medium.com/?source=post_page-----d8d56c301600--------------------------------", "anchor_text": "Max Frenzel, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1ac593c7124f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&user=Max+Frenzel%2C+PhD&userId=1ac593c7124f&source=post_page-1ac593c7124f----d8d56c301600---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8d56c301600&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8d56c301600&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://kittyzilla.tokyo", "anchor_text": "KITTYZILLA"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II"}, {"url": "http://www.aclweb.org/anthology/K16-1002", "anchor_text": "One approach, called \u201cword dropout\u201d"}, {"url": "https://arxiv.org/pdf/1708.04729.pdf", "anchor_text": "In one of them"}, {"url": "https://arxiv.org/abs/1702.08139", "anchor_text": "dilated convolutions"}, {"url": "http://www.aclweb.org/anthology/K16-1002", "anchor_text": "One more method (known as KL cost annealing)"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46", "anchor_text": "Part II"}, {"url": "https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b", "anchor_text": "Part I"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d8d56c301600---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d8d56c301600---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d8d56c301600---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d8d56c301600---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/illustration?source=post_page-----d8d56c301600---------------illustration-----------------", "anchor_text": "Illustration"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8d56c301600&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&user=Max+Frenzel%2C+PhD&userId=1ac593c7124f&source=-----d8d56c301600---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8d56c301600&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&user=Max+Frenzel%2C+PhD&userId=1ac593c7124f&source=-----d8d56c301600---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8d56c301600&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d8d56c301600--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd8d56c301600&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d8d56c301600---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d8d56c301600--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d8d56c301600--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d8d56c301600--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d8d56c301600--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d8d56c301600--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d8d56c301600--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d8d56c301600--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d8d56c301600--------------------------------", "anchor_text": ""}, {"url": "https://maxfrenzel.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://maxfrenzel.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Frenzel, PhD"}, {"url": "https://maxfrenzel.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "http://www.maxfrenzel.com", "anchor_text": "www.maxfrenzel.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1ac593c7124f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&user=Max+Frenzel%2C+PhD&userId=1ac593c7124f&source=post_page-1ac593c7124f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5930f70ecad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600&newsletterV3=1ac593c7124f&newsletterV3Id=5930f70ecad8&user=Max+Frenzel%2C+PhD&userId=1ac593c7124f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.amazon.com/dp/1734794402", "anchor_text": "Time Off: A Practical Guide to Building Your Rest Ethic and Finding Success Without the Stress2020"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}