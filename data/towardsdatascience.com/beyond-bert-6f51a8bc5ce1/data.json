{"url": "https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1", "time": 1683004050.0626152, "path": "towardsdatascience.com/beyond-bert-6f51a8bc5ce1/", "webpage": {"metadata": {"title": "Beyond BERT?. Transformers in 2020. | by Sergi Castella i Sap\u00e9 | Towards Data Science", "h1": "Beyond BERT?", "description": "2019 was the year of BERT and much has been written about it. Truth be told, it\u2019s hard to overestimate the impact Transformers have had in the NLP community: LSTMs now sound old-fashioned (or do\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/2019-the-year-of-bert-354e8106f7ba", "anchor_text": "2019 was the year of BERT", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1911.11423", "anchor_text": "or do they?", "paragraph_index": 0}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "into production in record-breaking time", "paragraph_index": 0}, {"url": "https://ruder.io/state-of-transfer-learning-in-nlp/", "anchor_text": "Transfer Learning", "paragraph_index": 0}, {"url": "https://www.zeta-alpha.com/deep-learning-nlp-transformers", "anchor_text": "Transformers at Work", "paragraph_index": 1}, {"url": "https://www.zeta-alpha.com", "anchor_text": "Zeta Alpha", "paragraph_index": 1}, {"url": "https://huggingface.co", "anchor_text": "Huggingface", "paragraph_index": 3}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers library", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "student-teacher setting", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1406.2661.pdf", "anchor_text": "GANs", "paragraph_index": 4}, {"url": "https://www.reddit.com/r/explainlikeimfive/", "anchor_text": "subreddit", "paragraph_index": 8}, {"url": "https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory", "anchor_text": "PG-19 Language Modelling Benchmark", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1911.05507.pdf", "anchor_text": "Compressive Transformer", "paragraph_index": 10}, {"url": "https://w4ngatang.github.io/static/papers/superglue.pdf", "anchor_text": "SuperGLUE", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1911.01547", "anchor_text": "The Measure of Intelligence", "paragraph_index": 12}, {"url": "https://github.com/fchollet/ARC", "anchor_text": "Abstract Reasoning Corpus", "paragraph_index": 12}, {"url": "https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview", "anchor_text": "Kaggle competition", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/1906.04341.pdf", "anchor_text": "What does BERT look at?", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1902.10186.pdf", "anchor_text": "Attention is not an Explanation", "paragraph_index": 15}, {"url": "https://www.aclweb.org/anthology/D19-1445.pdf", "anchor_text": "Revealing the Dark Secrets of BERT", "paragraph_index": 15}, {"url": "https://huggingface.co/transformers/", "anchor_text": "\ud83e\udd17/transformers", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1911.11423", "anchor_text": "Single Headed Attention RNN: Stop Thinking With Your Head", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "Generative Adversarial Networks", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1911.05507", "anchor_text": "Compressive Transformers for Long-Range Sequence Modelling", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1905.00537", "anchor_text": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "paragraph_index": 22}, {"url": "https://arxiv.org/pdf/1906.04341.pdf", "anchor_text": "What Does BERT Look At? An Analysis of BERT\u2019s Attention.", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1902.10186.pdf", "anchor_text": "Attention is not Explanation", "paragraph_index": 24}, {"url": "https://www.aclweb.org/anthology/D19-1445.pdf", "anchor_text": "Revealing the Dark Secrets of BERT", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/1910.01108.pdf", "anchor_text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "paragraph_index": 26}, {"url": "http://bit.ly/3ITDNNE", "anchor_text": "bit.ly/3ITDNNE", "paragraph_index": 28}, {"url": "http://linkedin.com/in/sergicastella", "anchor_text": "linkedin.com/in/sergicastella", "paragraph_index": 28}], "all_paragraphs": ["2019 was the year of BERT and much has been written about it. Truth be told, it\u2019s hard to overestimate the impact Transformers have had in the NLP community: LSTMs now sound old-fashioned (or do they?\u00b2), state-of-the-art papers have been coming steadily along 2019 and, at Google, BERT made it into production in record-breaking time. All of the above while enabling Transfer Learning, which is now the coolest kid in NLP-town.", "The development around these models has been remarkable so far, but could Transformers have peaked already? What areas of research should we be looking at most closely? What\u2019s still exciting about these attention-based networks in 2020? These ideas were the focus of discussion recently at the event Transformers at Work\u00b9 at Zeta Alpha, where many interesting angles on the topic were considered.", "2019 saw an explosion in architecture variants for Transformer models and it\u2019s difficult to keep up (certainly forgetting some): big cousins (Transformer-XL, GPT-2, Ernie, XLNet, RoBERTa, CTRL), smaller cousins (ALBERT, DistilBERT) or most recently nephews like Reformer or the Compressive Transformer.", "It\u2019s now clear that growing models is still successful to improve the state-of-the-art for many tasks, but should we? How much value does it add? Models that get smaller but preserve performance were a trend we started seeing in 2019 and want to keep steady for 2020. Maybe some innovative approaches will appear besides model pruning or distillation? The folks at Huggingface \u2014 creators of the ubiquitous Transformers library \u2014 got us talking about this refreshing trend with the training approach to DistilBERT\u00b9\u2070, which naturally connects to my next point.", "Shiny new architectures get a lot buzz and attention (pun intended); but in ML, the learning signal runs the show from the backstage. Broadly speaking, a model performance is limited by the weakest factor in the combination of model expressivity and training signal quality (objective or reward in RL or loss in DL). As an example, DistilBERT is trained in a student-teacher setting\u00b9\u2070 in which the student network (smaller) tries to imitate the behaviour of the teacher network (original). By adding this term instead of training only on the original Language Modelling task, the loss function for the student network is much richer, allowing the network to learn more expressively. If you still don\u2019t believe me, think of what happened with GANs\u00b3 in 2014: a simple network coupled to an interesting loss function (another network) and\u2026\ud83d\udca5 magic!", "Self-supervision and Language Modelling as a general purpose training signal for language tasks should be credited for NLP progress just as much as architectural revolutions, so for 2020 I want to see innovations in this domain.", "As you might\u2019ve heard, the magnetic North Pole and the Earth\u2019s one don\u2019t perfectly align; actually, the magnetic one is continuously jiggling around year after year. Still, if you\u2019re around The Netherlands and want to go towards the true North Pole, a conventional compass will be an excellent guide; well at least better than none at all. As you get closer to your destination, however, the bias of your compass will become increasingly evident, making it unsuited for the task.", "An analogy can clearly be drawn here for AI research. Objective measurement is the cornerstone of scientific development, even a biased metric is usually better than none at all. How progress is measured is a big driver for how a field evolves and what research gets done at the end of the day; and that\u2019s precisely why we need to thoroughly design evaluations in alignment with the incentives that will yield optimal development. Standard NLP tasks have been an amazing compass for research in the past few years, however, the closer we are to solving a dataset, the worse it becomes as a metric for progress, which is the reason why it\u2019s exciting to see new benchmarks gain momentum in 2020.", "As an example, at Facebook Research they\u2019ve been recently working on a new dataset and benchmark for Long Form Question Answering: ELI5 (Explain to Me Like I\u2019m 5) \u2014 yes, it\u2019s based on the famous homonymous subreddit \u2014 . The aim of this new dataset is to propel research in the field of Open Domain Question Answering, pushing the boundaries on the tasks Transformers currently excel at.", "[\u2026] a Long Form Question Answering dataset that emphasizes the dual challenges of isolating relevant information within long source documents and generating paragraph-length explanations in response to complex, diverse questions.\u00b3", "Another example of an interesting new dataset is the PG-19 Language Modelling Benchmark from DeepMind: a benchmark for long-range language modelling (book scale!), along with yet another Transformer reincarnation by the name of Compressive Transformer\u2075. Hopefully, this task will help to overcome the current limitations of Seq2Seq models dealing with (very) long term dependencies.", "Even the ubiquitous GLUE Benchmark is getting a much needed facelift. SuperGLUE\u2076 arrived as a strong contender to be the near-future de-facto general-purpose benchmark for Language Understanding. It includes \u2014 among others \u2014 more challenging tasks and more comprehensive human baselines.", "This section wouldn\u2019t be complete without mentioning one of my favourite recent papers on the broader topic of The Measure of Intelligence by Fran\u00e7ois Chollet, which flirts with a philosophical spin on the issue, bringing nonetheless a concrete proposal onto the table: the Abstract Reasoning Corpus and its challenging Kaggle competition. Keep these great initiatives coming!", "There is something attractively mysterious about systems we don\u2019t fully comprehend. Often, our perception of intelligence in an algorithm is inversely proportional to how deeply we understand its machinery. Not that long ago, people used to think that intelligence was required to master the game of chess; then Deep Blue beat Gary Kasparov in 1996 and we understood how it could be done, so that machine ceased to need intelligence.", "Building a solid understanding around \u2018why questions\u2019 is crucial for making progress, which is why models might look great in task leaderboards, but we shouldn\u2019t draw premature conclusions about their capabilities without carefully investigating their inner workings. Mapping this idea into the space of Transformers, a lot of work has been devoted to unpacking why these models work as well as they do; but the recent literature has not fully converged yet on a clear conclusion.", "For instance, in studying the behaviour of BERT\u2019s pretrained model, \u201cWhat does BERT look at?\u2077\u201d concluded that certain attention heads are accountable for detecting linguistic phenomena; whereas against many intuitions, \u201cAttention is not an Explanation\u2078\u201d asserts that attention is not a reliable signal to interpret what BERT understands. \u201cRevealing the Dark Secrets of BERT\u2079\u201d provides valuable insights into what happens during fine-tuning, but the scope of their conclusions is limited: no clear linguistic phenomena is being captured by attention, BERT is heavily overparametrized (surprising!\ud83e\udd2f), and the fact that BERT doesn\u2019t need to be very smart to solve most tasks. This kind of qualitative exploration is easy to overlook because it doesn\u2019t show up in the metrics, but we should always keep an eye on it.", "In conclusion, many secrets about why Transformers work remain to be unveiled, which is why it\u2019s exciting to wait for new research to come up in this realm during 2020.", "Those were my top picks, although many other topics also deserved a spotlight in this post, such as how frameworks like \ud83e\udd17/transformers will keep growing to empower research, the scope of Transfer Learning widening or new approaches effectively combining Symbolic reasoning with DL methods.", "What\u2019s your take? What are you most excited about Transformers in 2020?", "[2] Stephen Merity, 2019. Single Headed Attention RNN: Stop Thinking With Your Head", "[3] Ian Goodfellow et. al. 2014. Generative Adversarial Networks", "[5] Jack W. Rae et. al. 2019. Compressive Transformers for Long-Range Sequence Modelling", "[6] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh et. al. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "[7] Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning, 2019. What Does BERT Look At? An Analysis of BERT\u2019s Attention.", "[8] Sarthak Jain, Byron C. Wallace, 2019. Attention is not Explanation", "[9] Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky, 2019. Revealing the Dark Secrets of BERT", "[10] V. Sanh, L. Debut, J. Chaumond, T. Wolf, 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\ud83d\udd0e Working on search technology at Zeta Alpha // \ud83c\udf99 Neural IR Podcast: bit.ly/3ITDNNE // \ud83d\udc54 Linkedin: linkedin.com/in/sergicastella"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6f51a8bc5ce1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sergicastella?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "Sergi Castella i Sap\u00e9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e27e64320ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=post_page-1e27e64320ad----6f51a8bc5ce1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f51a8bc5ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f51a8bc5ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=medium&utm_medium=referral", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/2019-the-year-of-bert-354e8106f7ba", "anchor_text": "2019 was the year of BERT"}, {"url": "https://arxiv.org/abs/1911.11423", "anchor_text": "or do they?"}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "into production in record-breaking time"}, {"url": "https://ruder.io/state-of-transfer-learning-in-nlp/", "anchor_text": "Transfer Learning"}, {"url": "https://www.zeta-alpha.com/deep-learning-nlp-transformers", "anchor_text": "Transformers at Work"}, {"url": "https://www.zeta-alpha.com", "anchor_text": "Zeta Alpha"}, {"url": "https://huggingface.co", "anchor_text": "Huggingface"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers library"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT"}, {"url": "https://unsplash.com/@noguidebook?utm_source=medium&utm_medium=referral", "anchor_text": "Rachel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "student-teacher setting"}, {"url": "https://arxiv.org/pdf/1406.2661.pdf", "anchor_text": "GANs"}, {"url": "https://gluebenchmark.com", "anchor_text": "gluebenchmark.com"}, {"url": "https://www.reddit.com/r/explainlikeimfive/", "anchor_text": "subreddit"}, {"url": "https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory", "anchor_text": "PG-19 Language Modelling Benchmark"}, {"url": "https://arxiv.org/pdf/1911.05507.pdf", "anchor_text": "Compressive Transformer"}, {"url": "https://w4ngatang.github.io/static/papers/superglue.pdf", "anchor_text": "SuperGLUE"}, {"url": "https://arxiv.org/abs/1911.01547", "anchor_text": "The Measure of Intelligence"}, {"url": "https://github.com/fchollet/ARC", "anchor_text": "Abstract Reasoning Corpus"}, {"url": "https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview", "anchor_text": "Kaggle competition"}, {"url": "https://arxiv.org/pdf/1906.04341.pdf", "anchor_text": "What does BERT look at?"}, {"url": "https://arxiv.org/pdf/1902.10186.pdf", "anchor_text": "Attention is not an Explanation"}, {"url": "https://www.aclweb.org/anthology/D19-1445.pdf", "anchor_text": "Revealing the Dark Secrets of BERT"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "\ud83e\udd17/transformers"}, {"url": "https://www.zeta-alpha.com", "anchor_text": "Zeta Alpha Vector"}, {"url": "https://arxiv.org/abs/1911.11423", "anchor_text": "Single Headed Attention RNN: Stop Thinking With Your Head"}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "Generative Adversarial Networks"}, {"url": "https://research.fb.com/wp-content/uploads/2019/07/ELI5-Long-Form-Question-Answering.pdf?", "anchor_text": "ELI5: Long Form Question Answering"}, {"url": "https://arxiv.org/abs/1911.05507", "anchor_text": "Compressive Transformers for Long-Range Sequence Modelling"}, {"url": "https://arxiv.org/abs/1905.00537", "anchor_text": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"url": "https://arxiv.org/pdf/1906.04341.pdf", "anchor_text": "What Does BERT Look At? An Analysis of BERT\u2019s Attention."}, {"url": "https://arxiv.org/pdf/1902.10186.pdf", "anchor_text": "Attention is not Explanation"}, {"url": "https://www.aclweb.org/anthology/D19-1445.pdf", "anchor_text": "Revealing the Dark Secrets of BERT"}, {"url": "https://arxiv.org/pdf/1910.01108.pdf", "anchor_text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"url": "https://medium.com/tag/transformers?source=post_page-----6f51a8bc5ce1---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6f51a8bc5ce1---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6f51a8bc5ce1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----6f51a8bc5ce1---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/bert?source=post_page-----6f51a8bc5ce1---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f51a8bc5ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=-----6f51a8bc5ce1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f51a8bc5ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=-----6f51a8bc5ce1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f51a8bc5ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6f51a8bc5ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6f51a8bc5ce1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6f51a8bc5ce1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sergi Castella i Sap\u00e9"}, {"url": "https://medium.com/@sergicastella/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.2K Followers"}, {"url": "http://bit.ly/3ITDNNE", "anchor_text": "bit.ly/3ITDNNE"}, {"url": "http://linkedin.com/in/sergicastella", "anchor_text": "linkedin.com/in/sergicastella"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e27e64320ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=post_page-1e27e64320ad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce123c69a0ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-bert-6f51a8bc5ce1&newsletterV3=1e27e64320ad&newsletterV3Id=ce123c69a0ef&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}