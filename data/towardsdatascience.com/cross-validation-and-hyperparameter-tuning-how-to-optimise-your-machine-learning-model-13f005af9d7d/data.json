{"url": "https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d", "time": 1683012034.510719, "path": "towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d/", "webpage": {"metadata": {"title": "Cross-Validation and Hyperparameter Tuning: How to Optimise your Machine Learning Model | by Jonas Benner | Aug, 2020 | Medium | Towards Data Science", "h1": "Cross-Validation and Hyperparameter Tuning: How to Optimise your Machine Learning Model", "description": "This article explains why and how to use Cross-Validation to understand your Machine Learning model's accuracy better. It also demonstrates how we can leverage Cross-Validation to tune hyperparameters and boost model performance."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@bennerjonas10/using-machine-learning-to-predict-fitbit-sleep-scores-496a7d9ec48", "anchor_text": "part 2", "paragraph_index": 1}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://xgboost.readthedocs.io/en/latest/parameter.html", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://www.linkedin.com/in/jonas-benner-257093120/", "anchor_text": "LinkedIn", "paragraph_index": 46}], "all_paragraphs": ["In the first two parts of this article I obtained and preprocessed Fitbit sleep data, split the data into training, validation and test set, trained three different Machine Learning models and compared their performance.", "In part 2, we saw that using the default hyperparameters for Random Forest and Extreme Gradient Boosting and evaluating model performance on the validation set led to Multiple Linear Regression performing best and Random Forest as well as Gradient Boosting Regressor performing slightly worse.", "In this part of the article I will discuss shortcomings of using only one validation set, how we address those shortcomings and how we can tune model hyperparameters to boost performance. Let\u2019s dive in.", "In part 2 of this article we split the data into training, validation and test set, trained our models on the training set and evaluated them on the validation set. We have not touched the test set yet as it is intended as a hold-out set that represents never before seen data that will be used to evaluate how well the Machine Learning models generalise once we feel like they are ready for that final test.", "Because we only split the data into one set of training data and one set of validation data, the performance metrics of our models are highly reliant on those two sets. They are only trained and evaluated once so the performance depends on that one evaluation and may perform very differently when trained and evaluated on different subsets of the same data, just because of the nature of how the subsets are picked.", "What if we could do this split into training and validation test multiple times, each time on different subsets of the data, train and evaluate our models each time and look at the average performance of the models across multiple evaluations? Exactly that is the idea behind K-fold Cross-Validation.", "In K-fold Cross-Validation (CV) we still start off by separating a test/hold-out set from the remaining data in the data set to use for the final evaluation of our models. The data that is remaining, i.e. everything apart from the test set, is split into K number of folds (subsets). The Cross-Validation then iterates through the folds and at each iteration uses one of the K folds as the validation set while using all remaining folds as the training set. This process is repeated until every fold has been used as a validation set. Here is what this process looks like for a 5-fold Cross-Validation:", "By training and testing the model K number of times on different subsets of the same training data we get a more accurate representation of how well our model might perform on data it has not seen before. In a K-fold CV we score the model after every iteration and compute the average of all scores to get a better representation of how the model performs compared to only using one training and validation set.", "Because the Fitbit sleep data set is relatively small, I am going to use 4-fold Cross-Validation and compare the three models used so far: Multiple Linear Regression, Random Forest and Extreme Gradient Boosting Regressor.", "Note that a 4-fold CV also nicely compares to the training and validation split from part 2 because we split the data into 75% training and 25% validation data. A 4-fold CV essentially does the same, just four times, and using different subsets each time. I have created a function that takes as inputs a list of models that we would like to compare, the feature data, the target variable data and how many folds we would like to create. The function computes the performance measures we used previously and returns a table with the averages for all models as well as the scores for each type of measure per fold, in case we would like to investigate further. Here is the function:", "Now we can create a list of models to be used and call the above function with a 4-fold Cross-Validation:", "The resulting comparison table looks like this:", "Using a 4-fold CV, the Random Forest Regressor outperforms the other two models on all performance measures. But in part 2 we saw that the Multiple Linear Regression had the best performance metrics, why has that changed?", "In order to understand why the Cross-Validation results in different scores than the simple training and validation split from part 2 we need to have a closer look at how the models perform on each fold. The cv_comparison() function from above also returns a list of all the scores for each different model for every fold. Let\u2019s have a look at how the R-squared of the three models compares for each fold. In order to have the results in table format, let\u2019s quickly transform it into a DataFrame as well:", "The above table makes it clear why the scores obtained from the 4-fold CV differ to that of the training and validation set. The R-squared varies a lot from fold to fold, especially for Extreme Gradient Boosting and Multiple Linear Regression. This also shows why it is so important to use Cross-Validation, especially for small data sets. If you only rely on one simple training and validation set your results may be vastly different depending on what split of the data you end up with.", "Now that we know what Cross-Validation is and why it is important let\u2019s see if we can get more out of our models by tuning the hyperparameters.", "Unlike model parameters, which are learned during model training and can not be set arbitrarily, hyperparameters are parameters that can be set by the user before training a Machine Learning model. Examples of hyperparameters in a Random Forest are the number of decision trees to have in the forest, the maximum number of features to consider at each split or the maximum depth of the tree.", "As I mentioned previously, there is no one-size-fits-all solution to finding optimum hyperparameters. A set of hyperparameters that performs well for one Machine Learning problem may perform poorly on another one. So how do we figure out what the optimal hyperparameters are?", "One possible way is to manually tune the hyperparameters using educated guesses as starting points, changing some hyperparameters, training the model, evaluating its performance and repeating these steps until we are happy with the performance. That sounds like an unnecessarily tedious approach and it is.", "Compare hyperparameter tuning to tuning a guitar. You could choose to tune a guitar by ear, which requires a lot of practice and patience and may never lead to an optimal result, especially if you are a beginner. Luckily, there are electric guitar tuners which help you find the correct tones by interpreting the sound waves of your guitar strings and displaying what it reads. You still have to tune the strings using the machine head but the process will be much quicker and the electric tuner ensures your tuning is close to optimal. So what\u2019s the Machine Learning equivalent of an electric guitar tuner?", "One of the most popular approaches to tune Machine Learning hyperparameters is called RandomizedSearchCV() in scikit-learn. Let\u2019s dissect what this means.", "In Randomised Grid Search Cross-Validation we start by creating a grid of hyperparameters we want to optimise with values that we want to try out for those hyperparameters. Let\u2019s look at an example of a hyperparameter grid for our Random Forest Regressor and how we can set it up:", "First, we create a list of possible values for each hyperparameter we want to tune and then we set up the grid using a dictionary with the key-value pairs as shown above. In order to find and understand the hyperparameters of a Machine Learning model you can check out the model\u2019s official documentation, see the one for Random Forest Regressor here.", "The resulting grid looks like this:", "As the name suggests, Randomised Grid Search Cross-Validation uses Cross-Validation to evaluate model performance. Random Search means that instead of trying out all possible combinations of hyperparameters (which would be 27,216 combinations in our example) the algorithm randomly chooses a value for each hyperparameter from the grid and evaluates the model using that random combination of hyperparameters.", "Trying out all possible combinations would be really computationally expensive and take a long time. Choosing hyperparameters at random speeds up the process significantly and often provides a similarly good solution to trying out all possible combinations. Let\u2019s see how the Randomised Grid Search Cross-Validation is used.", "Using the previously created grid, we can find the best hyperparameters for our Random Forest Regressor. I will use a 3-fold CV because the data set is relatively small and run 200 random combinations. Therefore, in total, the Random Grid Search CV will train and evaluate 600 models (3 folds for 200 combinations). Because Random Forests tend to be slowly computed compared to other Machine Learning models such as Extreme Gradient Boosting, running this many models takes a few minutes. Once the process is completed we can obtain the best hyperparameters.", "Here is how to use RandomizedSearchCV():", "We will use these hyperparameters in our final model, which we test on the test set.", "For our Extreme Gradient Boosting Regressor the process is essentially the same as for the Random Forest. Some of the hyperparameters that we try to optimise are the same and some are different, due to the nature of the model. You can find the full list and explanations of the hyperparameters for XGBRegressor here. Once again, we create the grid:", "The resulting grid looks like this:", "In order to make the performance evaluations comparable I will use a 3-fold CV with 200 combinations for Extreme Gradient Boosting as well:", "The optimal hyperparameters are the following:", "Again, these will be used in the final model.", "Although it might appear obvious to some people I just want to mention it here: the reason why we do not do hyperparameter optimisation for Multiple Linear Regression is because there are no hyperparameters to be tweaked in the model, it is simply a Multiple Linear Regression.", "Now that we have obtained the optimal hyperparameters (at least in terms of our Cross-Validation) we can finally evaluate our models on the test data that we have been holding out since the very beginning of this analysis!", "After evaluating the performance of our Machine Learning models and finding optimal hyperparameters it is time to put the models to their final test \u2014 the all-mighty hold-out set.", "In order to do so, we train the models on the entire 80% of the data that we used for all of our evaluations so far, i.e. everything apart from the test set. We use the hyperparameters that we found in the previous part and then compare how our models perform on the test set.", "Let\u2019s create and train our models:", "I defined a function that scores all of the final models and creates a DataFrame that makes the comparison easy:", "Calling that function with our three final models and adjusting the column headers results in the following final evaluation:", "And the winner is: Random Forest Regressor!", "The Random Forest achieves an R-squared of 80% and an accuracy of 97.6% on the test set, meaning its predictions were only off by about 2.4% on average. Not bad!", "The performance of the Multiple Linear Regression is not far behind but the Extreme Gradient Boosting failed to live up to its hype in this analysis.", "The process of coming up with this whole analysis and actually conducting it was a lot of fun. I have been trying to figure out how Fitbit computes Sleep Scores for a while now and am glad to understand it a bit better. On top of that, I managed to build a Machine Learning model that can predict Sleep Scores with great accuracy. That being said, there are a few things I want to highlight:", "I hope you enjoyed this thorough analysis of how to use Machine Learning to predict Fitbit Sleep Scores and learned something about the importance of different sleep stages as well as the time spent asleep along the way.", "I highly appreciate constructive feedback and you can reach out to me on LinkedIn any time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "MSc in Finance from Bocconi University \u2022 Using Python and Machine Learning to explain the world around me"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F13f005af9d7d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bennerjonas10?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bennerjonas10?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "Jonas Benner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb70b21b73e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&user=Jonas+Benner&userId=db70b21b73e5&source=post_page-db70b21b73e5----13f005af9d7d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13f005af9d7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13f005af9d7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@alexisbaydoun?utm_source=medium&utm_medium=referral", "anchor_text": "Alexis Baydoun"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@bennerjonas10/using-machine-learning-to-predict-fitbit-sleep-scores-496a7d9ec48", "anchor_text": "part 2"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html", "anchor_text": "here"}, {"url": "https://xgboost.readthedocs.io/en/latest/parameter.html", "anchor_text": "here"}, {"url": "https://medium.com/@bennerjonas10/using-machine-learning-to-predict-fitbit-sleep-scores-496a7d9ec48", "anchor_text": "part 2"}, {"url": "https://www.linkedin.com/in/jonas-benner-257093120/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/data-science?source=post_page-----13f005af9d7d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----13f005af9d7d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----13f005af9d7d---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/programming?source=post_page-----13f005af9d7d---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----13f005af9d7d---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13f005af9d7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&user=Jonas+Benner&userId=db70b21b73e5&source=-----13f005af9d7d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13f005af9d7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&user=Jonas+Benner&userId=db70b21b73e5&source=-----13f005af9d7d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13f005af9d7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F13f005af9d7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----13f005af9d7d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----13f005af9d7d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----13f005af9d7d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----13f005af9d7d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bennerjonas10?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bennerjonas10?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jonas Benner"}, {"url": "https://medium.com/@bennerjonas10/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "156 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb70b21b73e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&user=Jonas+Benner&userId=db70b21b73e5&source=post_page-db70b21b73e5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5345cb7dfe6f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d&newsletterV3=db70b21b73e5&newsletterV3Id=5345cb7dfe6f&user=Jonas+Benner&userId=db70b21b73e5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}