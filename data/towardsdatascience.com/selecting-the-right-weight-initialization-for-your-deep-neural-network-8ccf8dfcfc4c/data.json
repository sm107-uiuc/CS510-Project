{"url": "https://towardsdatascience.com/selecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c", "time": 1682999773.871436, "path": "towardsdatascience.com/selecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c/", "webpage": {"metadata": {"title": "Selecting the right weight initialization for your deep neural network | by Gideon Mendels | Towards Data Science", "h1": "Selecting the right weight initialization for your deep neural network", "description": "The weight initialization technique you choose for your neural network can determine how quickly the network converges or whether it converges at all. Although the initial values of these weights are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94", "anchor_text": "here", "paragraph_index": 3}, {"url": "http://deeplearning.ai/", "anchor_text": "Deeplearning.ai", "paragraph_index": 4}, {"url": "https://www.deeplearning.ai/ai-notes/initialization/", "anchor_text": "an interactive post", "paragraph_index": 4}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "Krizhevsky et al. 2012", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "the He et al. (2015) paper", "paragraph_index": 12}, {"url": "https://pouannes.github.io/blog/initialization/", "anchor_text": "\u2018How to initialize deep neural networks? Xavier and Kaiming initialization\u2019", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1511.06856", "anchor_text": "data-dependent initializations", "paragraph_index": 14}, {"url": "https://openai.com/blog/block-sparse-gpu-kernels/", "anchor_text": "sparse weight matrices", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1312.6120", "anchor_text": "random orthogonal matrix initializations", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/how-the-lottery-ticket-hypothesis-is-challenging-everything-we-knew-about-training-neural-networks-e56da4b0da27", "anchor_text": "Lottery Ticket Hypothesis", "paragraph_index": 15}, {"url": "https://internetpolicy.mit.edu/publications/#lottery", "anchor_text": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "paragraph_index": 16}, {"url": "https://medium.com/u/b1d410cb9700", "anchor_text": "TensorFlow", "paragraph_index": 17}, {"url": "https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-pruning-api-42cac9157a6a", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://www.deeplearning.ai/ai-notes/initialization/", "anchor_text": "here", "paragraph_index": 20}, {"url": "http://Comet.ml", "anchor_text": "Comet.ml", "paragraph_index": 24}], "all_paragraphs": ["The weight initialization technique you choose for your neural network can determine how quickly the network converges or whether it converges at all. Although the initial values of these weights are just one parameter among many to tune, they are incredibly important. Their distribution affects the gradients and, therefore, the effectiveness of training.", "In neural networks, weights represent the strength of connections between units in adjacent network layers. The linear transformation of these weights and the values in the previous layer passes through a non-linear activation function to produce the values of the next layer. This process happens layer to layer during forward propagation; through back propagation, the optimum values of these weights can be found out so as to produce accurate outputs given an input.", "We will not be covering mathematical derivations of these initialization approaches. If you are interested in this type of material, we do link to several great resources within and at the end of the article (in the \u2018Further Reading\u2019 section)\ud83d\udcda.", "Improperly initialized weights can negatively affect the training process by contributing to the vanishing or exploding gradient problem. With the vanishing gradient problem, the weight update is minor and results in slower convergence \u2014 this makes the optimization of the loss function slow and in a worst case scenario, may stop the network from converging altogether. Conversely, initializing with weights that are too large may result in exploding gradient values during forward propagation or back-propagation (see more here).", "Deeplearning.ai recently published an interactive post where you can choose different initialization methods and watch the network train. Here\u2019s an example:", "You\u2019ll notice how setting an initialization method that\u2019s too small barely allows the network to learn (ie. reduce the cost function) while an initialization method that\u2019s too large causes divergence (check the decision boundary).", "In 2012, AlexNet, the winner of that year\u2019s ImageNet Large Scale Visual Recognition Challenge (ILVSRC), popularized the weight initialization approach of \u201cinitialization with Gaussian (normal) noise with mean equal to zero and standard deviation set to 0.01 with bias equal to one for some layers\u201d (see Krizhevsky et al. 2012).", "However, this normal random initialization approach does not work for training very deep networks, especially those that use the ReLU (rectified linear unit) activation function, because of the vanishing and exploding gradient problem referenced earlier.", "To address these issues, Xavier and Bengio (2010) proposed the \u201cXavier\u201d initialization which considers the size of the network (number of input and output units) while initializing weights. This approach ensures that the weights stay within a reasonable range of values by making them inversely proportional to the square root of the number of units in the previous layer (referred to as fan-in). See the diagram below on how to find the fan-in and fan-out for a given unit:", "The choice of activation function ends up playing an important role in determining how effective the initialization method is. Activation functions are differentiable and introduce non-linear properties (i.e. curvature) into our neural networks that are crucial for solving the complex tasks that machine learning and deep learning are designed to tackle.", "The activation function is the non linear transformation that we do over the input signal. This transformed output is then sent to the next layer of units as input. Some examples of these non-linear activation functions are:", "Rectified Linear unit (ReLU) (and leaky ReLU) are commonly used since they are relatively robust to the vanishing/exploding gradient issues. For activation functions like ReLU, Kaiming He et al. (2015) introduced a more robust weight initialization method that accounts for the fact that it\u2019s not symmetric (see performance differenced cited in the He et al. paper below). Both methods use a similar theoretical analysis: they find a good variance for the distribution from which the initial parameters are drawn. This variance is adapted to the activation function used and is derived without explicitly considering the type of the distribution.", "Figure from the He et al. (2015) paper showing how their refined initialization strategy (red) reduces the error rate much faster than the Xavier method (blue) for (P)ReLUs and accounts for the fact that it is not symmetric.", "For an accessible proof of the Xavier and He initialization methods, see Pierre Ouannes\u2019 excellent post \u2018How to initialize deep neural networks? Xavier and Kaiming initialization\u2019.", "It\u2019s important to note that weight initialization is still an active area of research. Several interesting research projects have popped up includingdata-dependent initializations, sparse weight matrices, and random orthogonal matrix initializations.", "One of the most interesting developments in this area is MIT\u2019s Lottery Ticket Hypothesis, which details how these large neural nets contain smaller \u201csubnetworks\u201d that are up to 10 times smaller than the full network. According to the research team, these subnetworks can learn just as well, making equally precise predictions sometimes faster than the full neural networks.", "The authors of \u2018The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\u2019, Carbin and Frankle, tested their lottery ticket hypothesis and the existence of subnetworks by performing a process called pruning, which involves eliminating unneeded connections from trained networks based on their network prioritization or weight to fit them on low-power devices.", "In fact, TensorFlow recently announced a new weight pruning API:", "Weight pruning means eliminating unnecessary values in the weight tensors. We are practically setting the neural network parameters\u2019 values to zero to remove what we estimate are unnecessary connections between the layers of a neural network. This is done during the training process to allow the neural network to adapt to the changes.", "Read more about the weight pruning API here.", "Try out this interactive demo here. The size of the weight values matter but making sure that the weights are randomly initialized is also critical. This random initialization approach is based off of a known property called Break Symmetry where:", "If the weights are initialized with just zeros, every neuron in the network would compute the same output, and gradients would undergo the exact same parameter updates. In other words, the units in the neural network will learn the same features during training if their weights are initialized to be the same value.", "With transfer learning, instead of starting from randomly initialized weights, you use weights saved from a previous network as the initial weights for your new experiment (i.e. fine-tuning a pre-trained network).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-founder/CEO of Comet.ml \u2014 a machine learning experimentation platform helping data scientists track, compare, explain, reproduce ML experiments."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8ccf8dfcfc4c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gidim?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gidim?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "Gideon Mendels"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab67b462fe4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&user=Gideon+Mendels&userId=ab67b462fe4b&source=post_page-ab67b462fe4b----8ccf8dfcfc4c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ccf8dfcfc4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ccf8dfcfc4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94", "anchor_text": "here"}, {"url": "http://deeplearning.ai/", "anchor_text": "Deeplearning.ai"}, {"url": "https://www.deeplearning.ai/ai-notes/initialization/", "anchor_text": "an interactive post"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "Krizhevsky et al. 2012"}, {"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "the He et al. (2015) paper"}, {"url": "https://pouannes.github.io/blog/initialization/", "anchor_text": "\u2018How to initialize deep neural networks? Xavier and Kaiming initialization\u2019"}, {"url": "https://arxiv.org/abs/1511.06856", "anchor_text": "data-dependent initializations"}, {"url": "https://openai.com/blog/block-sparse-gpu-kernels/", "anchor_text": "sparse weight matrices"}, {"url": "https://arxiv.org/abs/1312.6120", "anchor_text": "random orthogonal matrix initializations"}, {"url": "https://towardsdatascience.com/how-the-lottery-ticket-hypothesis-is-challenging-everything-we-knew-about-training-neural-networks-e56da4b0da27", "anchor_text": "Lottery Ticket Hypothesis"}, {"url": "https://internetpolicy.mit.edu/publications/#lottery", "anchor_text": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"url": "https://medium.com/u/b1d410cb9700", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-pruning-api-42cac9157a6a", "anchor_text": "here"}, {"url": "https://www.deeplearning.ai/ai-notes/initialization/", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8ccf8dfcfc4c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ccf8dfcfc4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&user=Gideon+Mendels&userId=ab67b462fe4b&source=-----8ccf8dfcfc4c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ccf8dfcfc4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&user=Gideon+Mendels&userId=ab67b462fe4b&source=-----8ccf8dfcfc4c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ccf8dfcfc4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8ccf8dfcfc4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8ccf8dfcfc4c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8ccf8dfcfc4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gidim?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gidim?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gideon Mendels"}, {"url": "https://medium.com/@gidim/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "384 Followers"}, {"url": "http://Comet.ml", "anchor_text": "Comet.ml"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab67b462fe4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&user=Gideon+Mendels&userId=ab67b462fe4b&source=post_page-ab67b462fe4b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa7f4041e398e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-weight-initialization-for-your-deep-neural-network-8ccf8dfcfc4c&newsletterV3=ab67b462fe4b&newsletterV3Id=a7f4041e398e&user=Gideon+Mendels&userId=ab67b462fe4b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}