{"url": "https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8", "time": 1683007090.16892, "path": "towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8/", "webpage": {"metadata": {"title": "Latent Semantic Analysis: intuition, math, implementation | by Ioana | Towards Data Science", "h1": "Latent Semantic Analysis: intuition, math, implementation", "description": "TL;DR \u2014 Text data suffers heavily from high-dimensionality. Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction techniques that follows the same method as Singular Value\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn", "paragraph_index": 0}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html", "anchor_text": "20 newsgroups", "paragraph_index": 0}, {"url": "https://github.com/Ioana-P/pca_and_clustering_for_edu_purposes/blob/master/newsgroups_LSA.ipynb", "anchor_text": "Github repo", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558", "anchor_text": "tf-idf vectorisation", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "Wikipedia entry on SVD", "paragraph_index": 25}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html", "anchor_text": "TruncatedSVD class", "paragraph_index": 38}, {"url": "https://towardsdatascience.com/@yassine.hamdaoui?source=post_page-----6c2b61b78558----------------------", "anchor_text": "Hamdaoui", "paragraph_index": 51}, {"url": "https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558", "anchor_text": "TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python", "paragraph_index": 51}], "all_paragraphs": ["TL;DR \u2014 Text data suffers heavily from high-dimensionality. Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction techniques that follows the same method as Singular Value Decomposition. LSA ultimately reformulates text data in terms of r latent (i.e. hidden) features, where r is less than m, the number of terms in the data. I\u2019ll explain the conceptual and mathematical intuition and run a basic implementation in Scikit-Learn using the 20 newsgroups dataset.", "Language is more than the collection of words in front of you. When you read a text your mind conjures up images and notions. When you read many texts, themes begin to emerge, even if they\u2019re never stated explicitly. Our innate ability to understand and process language defies an algorithmic expression (for the moment). LSA is one of the most popular Natural Language Processing (NLP) techniques for trying to determine themes within text mathematically. LSA is an unsupervised learning technique that rests on two pillars:", "Note that LSA is an unsupervised learning technique \u2014 there is no ground truth. The latent concepts might or might not be there! In the dataset we\u2019ll use later we know there are 20 news categories and we can perform classification on them, but that\u2019s only for illustrative purposes. It\u2019ll often be the case that we\u2019ll use LSA on unstructured, unlabelled data.", "Like all Machine Learning concepts, LSA can be broken down into 3 parts: the intuition, the maths and the code. Feel free to use the links in Contents to skip to the part most relevant to you. The full code is available in this Github repo.", "A note on terminology: generally when decomposition of this kind is done on text data, the terms SVD and LSA (or LSI) are used interchangeably. From now on I\u2019ll be using LSA, for simplicity\u2019s sake.", "This article assumes some understanding of basic NLP preprocessing and of word vectorisation (specifically tf-idf vectorisation).", "In simple terms: LSA takes meaningful text documents and recreates them in n different parts where each part expresses a different way of looking at meaning in the text. If you imagine the text data as a an idea, there would be n different ways of looking at that idea, or n different ways of conceptualising the whole text. LSA reduces our table of data to a table of latent (hidden) concepts.", "Suppose that we have some table of data, in this case text data, where each row is one document, and each column represents a term (which can be a word or a group of words, like \u201cbaker\u2019s dozen\u201d or \u201cDowning Street\u201d). This is the standard way to represent text data (in a document-term matrix, as shown in Figure 2). The numbers in the table reflect how important that word is in the document. If the number is zero then that word simply doesn\u2019t appear in that document.", "Different documents will be about different topics. Let\u2019s say all the documents are politics articles and there are 3 topics: foreign policy (F.P.), elections and reform.", "Let\u2019s say that there are articles strongly belonging to each category, some that are in two and some that belong to all 3 categories. We could plot a table where each row is a different document (a news article) and each column is a different topic. In the cells we would have a different numbers that indicated how strongly that document belonged to the particular topic (see Figure 3).", "Now if we shift our attention conceptually to the topics themselves, we should ask ourselves the following question: do we expect certain words to turn up more often in either of these topics?", "If we\u2019re looking at foreign policy, we might see terms like \u201cMiddle East\u201d, \u201cEU\u201d, \u201cembassies\u201d. For elections it might be \u201cballot\u201d, \u201ccandidates\u201d, \u201cparty\u201d; and for reform we might see \u201cbill\u201d, \u201camendment\u201d or \u201ccorruption\u201d. So, if we plotted these topics and these terms in a different table, where the rows are the terms, we would see scores plotted for each term according to which topic it most strongly belonged. Naturally there will be terms that feature in all three documents (\u201cprime minister\u201d, \u201cParliament\u201d, \u201cdecision\u201d) and these terms will have scores across all 3 columns that reflect how much they belong to either category \u2014 the higher the number, the greater its affiliation to that topic. So, our second table (Figure 4) consists of terms and topics.", "Now the last component is a bit trickier to explain as a table. It\u2019s actually a set of numbers, one for each of our topics. What do the numbers represent? They represent how much each of the topics explains our data.", "How do they \u201cexplain\u201d the data? Well, suppose that actually, \u201creform\u201d wasn\u2019t really a salient topic across our articles, and the majority of the articles fit in far more comfortably in the \u201cforeign policy\u201d and \u201celections\u201d. Thus \u201creform\u201d would get a really low number in this set, lower than the other two. An alternative is that maybe all three numbers are actually quite low and we actually should have had four or more topics \u2014 we find out later that a lot of our articles were actually concerned with economics! By sticking to just three topics we\u2019ve been denying ourselves the chance to get a more detailed and precise look at our data. The technical name for this array of numbers is the \u201csingular values\u201d.", "So that\u2019s the intuition so far. You\u2019ll notice that our two tables have one thing in common (the documents / articles) and all three of them have one thing in common \u2014 the topics, or some representation of them.", "Now let\u2019s explain how this is a dimensionality reduction technique. It\u2019s easier to see the merits if we specify a number of documents and topics. Suppose we had 100 articles and 10,000 different terms (just think of how many unique words there would be all those articles, from \u201camendment\u201d to \u201czealous\u201d!). In our original document-term matrix that\u2019s 100 rows and 10,000 columns. When we start to break our data down into the 3 components, we can actually choose the number of topics \u2014 we could choose to have 10,000 different topics, if we genuinely thought that was reasonable. However, we could probably represent the data with far fewer topics, let\u2019s say the 3 we originally talked about. That means that in our document-topic table, we\u2019d slash about 99,997 columns, and in our term-topic table, we\u2019d do the same. The columns and rows we\u2019re discarding from our tables are shown as hashed rectangles in Figure 6. M is the original document-term table; U is the document-topic table, \ud835\udeba (sigma) is the array of singular values and V-transpose (the superscript T means that the original matrix T has been flipped along its diagonal) is the document-topic table, but flipped on its diagonal (I\u2019ll explain why in the math section).", "As for the set of numbers denoting topic importance, from a set of 10,000 numbers, each number getting smaller and smaller as it corresponds to a less important topic, we cut down to only 3 numbers, for our 3 remaining topics. This is why the Python implementation for LSA is called Truncated SVD by the way: we\u2019re cutting off part of our table, but we\u2019ll get to the code later. It\u2019s also worth noting that we don\u2019t know what the 3 topics are in advance, we merely hypothesised that there would be 3 and, once we\u2019ve gotten our components, we can explore them and see what the terms are.", "Of course, we don\u2019t just want to return to the original dataset: we now have 3 lower-dimensional components we can use. In the code and maths parts we\u2019ll go through which one we actually take forward. In brief, once we\u2019ve truncated the tables (matrices), the product we\u2019ll be getting out is the document-topic table (U) times the singular values (\ud835\udeba). This can be interpreted as the documents (all our news articles) along with how much they belong to each topic then weighted by the relative importance of each topic. You\u2019ll notice that in that case something\u2019s been left out of this final table \u2014 the words. Yes, we\u2019ve gone beyond the words, we\u2019re discarding them but keeping the themes, which is a much more compact way to express our text.", "For the maths, I\u2019ll be going through two different interpretations of SVD: first the general geometric decomposition that you can use with a real square matrix M and second the separable-models decomposition which is more pertinent to our example. SVD is also used in model-based recommendation systems. It is very similar to Principal Component Analysis (PCA), but it operates better on sparse data than PCA does (and text data is almost always sparse). Whereas PCA performs decomposition on the correlation matrix of a dataset, SVD/LSA performs decomposition directly on the dataset as it is.", "We will be factorising this matrix into constituent matrices. When I say factorising this is essentially the same as when we\u2019re taking a number and representing it its factors, which when multiplied together, give us the original number, e.g. A = B * C * D .", "This is also why it\u2019s called Singular Value Decomposition \u2014 we\u2019re decomposing it into its constituent parts.", "The extra dimension that wasn\u2019t available to us in our original matrix, the r dimension, is the amount of latent concepts. Generally we\u2019re trying to represent our matrix as other matrices that have one of their axes being this set of components. You will also note that, based on dimensions, the multiplication of the 3 matrices (when V is transposed) will lead us back to the shape of our original matrix, the r dimension effectively disappearing.", "What matters in understanding the math is not the algebraic algorithm by which each number in U, V and \ud835\udeba is determined, but the mathematical properties of these products and how they relate to each other.", "First of all, it\u2019s important to consider first what a matrix actually is and what it can be thought of \u2014 a transformation of vector space. In the top left corner of Figure 7 we have two perpendicular vectors. If we have only two variables to start with then the feature space (the data that we\u2019re looking at) can be plotted anywhere in this space that is described by these two basis vectors. Now moving to the right in our diagram, the matrix M is applied to this vector space and this transforms it into the new, transformed space in our top right corner. In the diagram below the geometric effect of M would be referred to as \u201cshearing\u201d the vector space; the two vectors \ud835\udf481 and \ud835\udf482 are actually our singular values plotted in this space.", "Now, just like with geometric transformations of points that you may remember from school, we can reconsider this transformation M as three separate transformations:", "I also recommend the excellent Wikipedia entry on SVD as it has a particularly good explanation and GIF of the process.", "So, in other words, where x is any column vector:", "One of the properties of the matrices U and V* is that they\u2019re unitary, so we can say that the columns of both of these matrices form two sets of orthonormal basis vectors. In other words, the column vectors you can get from U would form their own coordinate space, such that if there were two columns U1 and U2, you could write out all of the coordinates of the space as combinations of U1 and U2. The same applies to the columns of V, V1 and V2, and this would generalise to n-dimensions (you\u2019d have n-columns).", "We can arrive at the same understanding of PCA if we imagine that our matrix M can be broken down into a weighted sum of separable matrices, as shown below.", "The matrices \ud835\udc34\ud835\udc56 are said to be separable because they can be decomposed into the outer product of two vectors, weighted by the singular value \ud835\udf48i. Calculating the outer product of two vectors with shapes (m,) and (n,) would give us a matrix with a shape (m,n). In other words, every possible product of any two numbers in the two vectors is computed and placed in the new matrix. The singular value not only weights the sum but orders it, since the values are arranged in descending order, so that the first singular value is always the highest one.", "In Figure 8 you can see how you could visualise this. Previously we had the tall U, the square \u03a3 and the long \ud835\udc49-transpose matrices. Now you can picture taking the first vertical slice from U, weighting (multiplying) all its values by the first singular value and then, by doing an outer product with the first horizontal slice of \ud835\udc49-transpose, creating a new matrix with the dimensions of those slices. Then we add those products together and we get M. Or, if we don\u2019t do the full sum but only complete it partially, we get the truncated version.", "The values in \ud835\udeba represent how much each latent concept explains the variance in our data. When these are multiplied by the u column vector for that latent concept, it will effectively weigh that vector.", "If we were to decompose this to 5 components, this would look something like this:", "where there would be originally r number of u vectors; 5 singular values and n number of \ud835\udc63-transpose vectors.", "In this last section we\u2019ll see how we can implement basic LSA using Scikit-Learn.", "The cleaning of text data is often a very different beast from cleaning of numerical data. You\u2019ll often find yourself having prepared your vectoriser, you model and you\u2019re ready to Gridsearch and then extract features, only to find that the most important features in cluster x is the string \u201c___\u201d \u2026 so you go back\u2026and do more cleaning. The code block below came about as a result of me realizing that I needed to remove website URLs, numbers and emails from the dataset.", "Our models work on numbers, not string! So we tokenise the text (turning all documents into smaller observational entities \u2014 in this case words) and then turn them into numbers using Sklearn\u2019s TF-IDF vectoriser. I recommend with any transformation process (especially ones that take time to run) you do them on the first 10 rows of your data and inspect results: are they what you expected to see? Is the shape of the dataframe what you hoped for? Once you\u2019re feeling confident of your code, feed in the whole corpus.", "This should give you your vectorised text data \u2014 the document-term matrix. Repeat the steps above for the test set as well, but only using transform, not fit_transform.", "Just for the purpose of visualisation and EDA of our decomposed data, let\u2019s fit our LSA object (which in Sklearn is the TruncatedSVD class) to our train data and specifying only 20 components.", "Now let\u2019s visualise the singular values \u2014 is the barplot below showing us what we expected of them?", "Let\u2019s explore our reduced data through the term-topic matrix, V-tranpose. TruncatedSVD will return it to as a numpy array of shape (num_documents, num_components), so we\u2019ll turn it into a Pandas dataframe for ease of manipulation.", "Let\u2019s slice our term-topic matrix into Pandas Series (single column data-frames), sort them by value and plot them. The code below plots this for our 2nd latent component (recall that in python we start counting from 0) and returns the plot in Figure 10:", "These are the words that rank highly along our 2nd latent component. What about the words at the other end of this axis (see Fig 11)?", "You can make your own mind up about that this semantic divergence signifies. Adding more preprocessing steps would help us cleave through the noise that words like \u201csay\u201d and \u201csaid\u201d are creating, but we\u2019ll press on for now. Let\u2019s do one more pair of visualisations for the 6th latent concept (Figures 12 and 13).", "At this point it\u2019s up to us to infer some meaning from these plots. The negative end of concept 5\u2019s axis seems to correlate very strongly with technological and scientific themes (\u2018space\u2019, \u2018science\u2019, \u2018computer\u2019), but so does the positive end, albeit more focused on computer related terms (\u2018hard\u2019, \u2018drive\u2019, \u2018system\u2019).", "Now just to be clear, determining the right amount of components will require tuning, so I didn\u2019t leave the argument set to 20, but changed it to 100. You might think that\u2019s still a large number of dimensions, but our original was 220 (and that was with constraints on our minimum document frequency!), so we\u2019ve reduced a sizeable chunk of the data. I\u2019ll explore in another post how to choose the optimal number of singular values. For now we\u2019ll just go forward with what we have.", "Although LSA is an unsupervised technique often used to find patterns in unlabelled data, we\u2019re using it here to reduce the dimensions of labelled data before feeing it into a model. We\u2019ll compare our accuracy on the LSA data with the accuracy on our standard TF-IDF data to gauge how much useful information the LSA has captured from the original dataset. We now have a train dataset of shape (11314, 100). The number of documents is preserved and we have created 100 latent concepts. Now let\u2019s run a model on this and on our standard TF-IDF data. The aim of the implementation below isn\u2019t to get a great model, but to compare the two very different datasets. I\u2019ve included basic cross validation through GridSearchCV and performed a tiny amount of tuning for the tolerance hyperparameter. If you were to do this for the sake of building an actual model, you would go much farther than what\u2019s written below. This is just to help you get a basic implementation going:", "The drop in performance is significant, but you can work this into an optimisation pipeline and tweak the number of latent components. How does this perform on our test data (7532 documents) though?", "Accuracy has dropped greatly for both, but notice how small the gap between the models is! Our LSA model is able to capture about as much information from our test data as our standard model did, with less than half the dimensions! Since this is a multi-label classification it would be best to visualise this with a confusion matrix (Figure 14). Our results look significantly better when you consider the random classification probability given 20 news categories. If you\u2019re not familiar with a confusion matrix, as a rule of thumb, we want to maximise the numbers down the diagonal and minimise them everywhere else.", "And that concludes our implementation of LSA in Scikit-Learn. We\u2019ve covered the intuition, mathematics and coding of this technique.", "I hope you\u2019ve enjoyed this post and would appreciate any amount of claps. Feel free to leave any feedback (positive or constructive) in the comments, especially about the math section, since I found that the most challenging to articulate.", "[3] Hamdaoui Y, TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python (2019), Towards Data Science", "Data scientist & educator. Combining analytical rigour & pedagogical expertise to turn opaque data and ML methods into insights & narrative \u2014 Linkedin: ioanapr."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa194aff870f8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://ioanap.medium.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Ioana"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff61787328f9d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&user=Ioana&userId=f61787328f9d&source=post_page-f61787328f9d----a194aff870f8---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa194aff870f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&user=Ioana&userId=f61787328f9d&source=-----a194aff870f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa194aff870f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&source=-----a194aff870f8---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-Learn"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html", "anchor_text": "20 newsgroups"}, {"url": "https://github.com/Ioana-P/pca_and_clustering_for_edu_purposes/blob/master/newsgroups_LSA.ipynb", "anchor_text": "Github repo"}, {"url": "https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558", "anchor_text": "tf-idf vectorisation"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "Singular Value Decomposition"}, {"url": "https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg#filelinks;", "anchor_text": "link"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "Wikipedia entry on SVD"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html", "anchor_text": "TruncatedSVD class"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html", "anchor_text": "Confusion matrix"}, {"url": "http://3ce3", "anchor_text": "return to Contents"}, {"url": "https://www.manning.com/books/natural-language-processing-in-action", "anchor_text": "Natural Language Processing in Action"}, {"url": "https://www.manning.com/books/natural-language-processing-in-action", "anchor_text": "https://www.manning.com/books/natural-language-processing-in-action"}, {"url": "http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html", "anchor_text": "Scikit-learn: Machine Learning in Python"}, {"url": "https://towardsdatascience.com/@yassine.hamdaoui?source=post_page-----6c2b61b78558----------------------", "anchor_text": "Hamdaoui"}, {"url": "https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558", "anchor_text": "TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "Singular Value Decomposition"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "https://en.wikipedia.org/wiki/Singular_value_decomposition"}, {"url": "https://medium.com/tag/intuitive-maths?source=post_page-----a194aff870f8---------------intuitive_maths-----------------", "anchor_text": "Intuitive Maths"}, {"url": "https://medium.com/tag/python?source=post_page-----a194aff870f8---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a194aff870f8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----a194aff870f8---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----a194aff870f8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa194aff870f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&user=Ioana&userId=f61787328f9d&source=-----a194aff870f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa194aff870f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&user=Ioana&userId=f61787328f9d&source=-----a194aff870f8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa194aff870f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff61787328f9d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&user=Ioana&userId=f61787328f9d&source=post_page-f61787328f9d----a194aff870f8---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd6d78f2ad7ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&newsletterV3=f61787328f9d&newsletterV3Id=d6d78f2ad7ea&user=Ioana&userId=f61787328f9d&source=-----a194aff870f8---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Written by Ioana"}, {"url": "https://ioanap.medium.com/followers?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "132 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff61787328f9d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&user=Ioana&userId=f61787328f9d&source=post_page-f61787328f9d----a194aff870f8---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd6d78f2ad7ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-semantic-analysis-intuition-math-implementation-a194aff870f8&newsletterV3=f61787328f9d&newsletterV3Id=d6d78f2ad7ea&user=Ioana&userId=f61787328f9d&source=-----a194aff870f8---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094?source=author_recirc-----a194aff870f8----0---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=author_recirc-----a194aff870f8----0---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=author_recirc-----a194aff870f8----0---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Ioana"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a194aff870f8----0---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094?source=author_recirc-----a194aff870f8----0---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Latent Dirichlet Allocation: Intuition, math, implementation and visualisationA tour of one of the most popular topic modelling techniques and a guide to implementing and visualising it using pyLDAvis"}, {"url": "https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094?source=author_recirc-----a194aff870f8----0---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "\u00b714 min read\u00b7Sep 26, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F63ccb616e094&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094&user=Ioana&userId=f61787328f9d&source=-----63ccb616e094----0-----------------clap_footer----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094?source=author_recirc-----a194aff870f8----0---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63ccb616e094&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094&source=-----a194aff870f8----0-----------------bookmark_preview----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a194aff870f8----1---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a194aff870f8----1---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----a194aff870f8----1---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a194aff870f8----1---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a194aff870f8----1---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a194aff870f8----1---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----a194aff870f8----1---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----a194aff870f8----1-----------------bookmark_preview----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a194aff870f8----2---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----a194aff870f8----2---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----a194aff870f8----2---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a194aff870f8----2---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a194aff870f8----2---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a194aff870f8----2---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----a194aff870f8----2---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----a194aff870f8----2-----------------bookmark_preview----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/self-attention-5b95ea164f61?source=author_recirc-----a194aff870f8----3---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=author_recirc-----a194aff870f8----3---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=author_recirc-----a194aff870f8----3---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Ioana"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----a194aff870f8----3---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/self-attention-5b95ea164f61?source=author_recirc-----a194aff870f8----3---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "Self-AttentionWhat are the very basic mathematics behind self-attention, the first layer in a transformer model?"}, {"url": "https://towardsdatascience.com/self-attention-5b95ea164f61?source=author_recirc-----a194aff870f8----3---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": "\u00b76 min read\u00b7Jul 1, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b95ea164f61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-5b95ea164f61&user=Ioana&userId=f61787328f9d&source=-----5b95ea164f61----3-----------------clap_footer----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/self-attention-5b95ea164f61?source=author_recirc-----a194aff870f8----3---------------------3d0b8e31_bc0c_4fd3_a948_77e308d59755-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b95ea164f61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-5b95ea164f61&source=-----a194aff870f8----3-----------------bookmark_preview----3d0b8e31_bc0c_4fd3_a948_77e308d59755-------", "anchor_text": ""}, {"url": "https://ioanap.medium.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "See all from Ioana"}, {"url": "https://towardsdatascience.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----0-----------------clap_footer----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----a194aff870f8----0-----------------bookmark_preview----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Angel Das"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in PythonIntroduction to embeddings in natural language processing using Artificial Neural Network and Gensim"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "\u00b713 min read\u00b7Nov 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&user=Angel+Das&userId=8418ab50405a&source=-----a8873b225ab6----1-----------------clap_footer----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&source=-----a194aff870f8----1-----------------bookmark_preview----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/hands-on-topic-modeling-with-python-1e3466d406d7?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://idilismiguzel.medium.com/?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://idilismiguzel.medium.com/?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Idil Ismiguzel"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/hands-on-topic-modeling-with-python-1e3466d406d7?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Hands-On Topic Modeling with PythonA tutorial on topic modeling using Latent Dirichlet Allocation (LDA) and visualization with pyLDAvis"}, {"url": "https://towardsdatascience.com/hands-on-topic-modeling-with-python-1e3466d406d7?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "\u00b711 min read\u00b7Dec 14, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e3466d406d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-topic-modeling-with-python-1e3466d406d7&user=Idil+Ismiguzel&userId=6d965c736f2&source=-----1e3466d406d7----0-----------------clap_footer----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/hands-on-topic-modeling-with-python-1e3466d406d7?source=read_next_recirc-----a194aff870f8----0---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e3466d406d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-topic-modeling-with-python-1e3466d406d7&source=-----a194aff870f8----0-----------------bookmark_preview----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----1-----------------clap_footer----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----a194aff870f8----1---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----a194aff870f8----1-----------------bookmark_preview----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a194aff870f8----2---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----a194aff870f8----2---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----a194aff870f8----2---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a194aff870f8----2---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a194aff870f8----2---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a194aff870f8----2---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----a194aff870f8----2---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----a194aff870f8----2-----------------bookmark_preview----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada?source=read_next_recirc-----a194aff870f8----3---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://joshnjuny.medium.com/?source=read_next_recirc-----a194aff870f8----3---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://joshnjuny.medium.com/?source=read_next_recirc-----a194aff870f8----3---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Seungjun (Josh) Kim"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----a194aff870f8----3---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada?source=read_next_recirc-----a194aff870f8----3---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "Let us Extract some Topics from Text Data \u2014 Part III: Non-Negative Matrix Factorization (NMF)Learn more about the unsupervised algorithm derived from linear algebra that uses an intuitive approach to topic modelling"}, {"url": "https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada?source=read_next_recirc-----a194aff870f8----3---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": "\u00b710 min read\u00b7Dec 14, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8eba8c8edada&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada&user=Seungjun+%28Josh%29+Kim&userId=d34abe9e1001&source=-----8eba8c8edada----3-----------------clap_footer----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada?source=read_next_recirc-----a194aff870f8----3---------------------01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8eba8c8edada&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flet-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada&source=-----a194aff870f8----3-----------------bookmark_preview----01fdbd9d_94eb_4c89_9ed9_9f4a602b2e90-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a194aff870f8--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----a194aff870f8--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}