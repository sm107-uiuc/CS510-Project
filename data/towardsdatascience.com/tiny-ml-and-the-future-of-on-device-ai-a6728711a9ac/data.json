{"url": "https://towardsdatascience.com/tiny-ml-and-the-future-of-on-device-ai-a6728711a9ac", "time": 1683016999.742625, "path": "towardsdatascience.com/tiny-ml-and-the-future-of-on-device-ai-a6728711a9ac/", "webpage": {"metadata": {"title": "Tiny ML and the future of on-device AI | by Jeremie Harris | Towards Data Science", "h1": "Tiny ML and the future of on-device AI", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/emerging-problems-in-data-science-and-machine-learning-36d37f6531a8", "anchor_text": "emerging problems in data science and machine learning", "paragraph_index": 0}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=EJBnKXdIO-8", "anchor_text": "YouTube", "paragraph_index": 0}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple", "paragraph_index": 0}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google", "paragraph_index": 0}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify", "paragraph_index": 0}, {"url": "https://twitter.com/MatthewPStewart", "anchor_text": "follow Matthew on Twitter here", "paragraph_index": 5}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/tiny-machine-learning-the-next-ai-revolution-495c26463868", "anchor_text": "Matthew\u2019s latest article introducing Tiny ML here", "paragraph_index": 6}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 83}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast on YouTube, Apple, Google, Spotify.", "When it comes to machine learning, we\u2019re often led to believe that bigger is better. It\u2019s now pretty clear that all else being equal, more data, more compute, and larger models add up to give more performance and more generalization power. And cutting edge language models have been growing at an alarming rate \u2014 by up to 10X each year.", "But size isn\u2019t everything. While larger models are certainly more capable, they can\u2019t be used in all contexts: take, for example, the case of a cell phone or a small drone, where on-device memory and processing power just isn\u2019t enough to accommodate giant neural networks or huge amounts of data. The art of doing machine learning on small devices with significant power and memory constraints is pretty new, and it\u2019s now known as \u201ctiny ML\u201d. Tiny ML unlocks an awful lot of exciting applications, but also raises a number of safety and ethical questions.", "And that\u2019s why I wanted to sit down with Matthew Stewart, a Harvard PhD researcher focused on applying tiny ML to environmental monitoring. Matthew has worked with many of the world\u2019s top tiny ML researchers, and our conversation focused on the possibilities and potential risks associated with this promising new field.", "Here were some of my favourite take-homes from our conversation:", "You can follow Matthew on Twitter here, and you can follow me on Twitter here.", "I\u2019d also recommend checking out Matthew\u2019s latest article introducing Tiny ML here.", "Jeremie (00:00):Hey everyone, Jeremie here. I\u2019m the host of the Towards Data Science podcast and I\u2019m also on the team over at the SharperScience mentorship program, and today I am really excited, because we\u2019re talking to Matthew Stewart, who is a PhD student at Harvard. He\u2019s working on a series of different problems in the environmental monitoring space, but specifically he started to focus lately on something called TinyML, and this is an area of machine learning that\u2019s focused on getting machine learning onto embedded devices, drones and sort of IoT type devices where you have strong constraints in terms of the amount of energy that can be used for computation, the amount of space, the amount of memory that can be put on these devices, and there are all kinds of implications. Ethical, philosophical, engineering implications, even, of this technology. So what I want to do is talk to Matthew about what he\u2019s been working on, where he sees the space going, and what some of the sort of ethical constraints are around this work, what ethical problems we might see emerging in the space going into the future. So really excited about this one, thanks so much for joining us, and without further ado, here we go.", "Jeremie (01:05):Well, Matthew, thanks so much for joining us for the podcast.", "Matthew Stewart (01:07):Thanks for having me, yeah, good to be back.", "Jeremie (01:11):Yeah, we\u2019re really happy to have you here again. Last time we spoke, we talked a little bit about machine learning as applied to environmental monitoring. I think that\u2019s still kind of part of what you\u2019re working on, but your work right now also involves this relatively new category of machine learning applications called TinyML, and I think, at least for me, I hadn\u2019t really heard of TinyML, I think I might\u2019ve read a blog post or two about it or something like that, but I had no idea really what it amounted to until we talked about doing this podcast. So one thing I was hoping you could start with is just introducing the idea of TinyML and what it is, if you don\u2019t mind.", "Matthew Stewart (01:47):Yeah, sure. So it\u2019s kind of a difficult topic to introduce, because no one really knows what it is at this stage. Even myself, I was in a TinyML talk with one of the creators of, I think it was a Qualcomm engineer, and he was talking about TinyML, and I asked him, \u201cWhat is it? Is it a framework, is it a system of functions or something?\u201d And he said it\u2019s basically just industry best practices at this stage. It\u2019s trying to run things like machine learning models, especially neural networks on memory and sort of compute-strained devices like microcontrollers, mainly for [inaudible 00:02:31] purposes, and he talked about the fact that mainly it\u2019s using it on devices that are constrained to one milliwatt, because if you have a microcontroller that\u2019s running a coin battery, if you\u2019re only using one milliwatt of power, it can last for about a year, and that opens a lot of interesting applications.", "Matthew Stewart (02:50):So yeah, that\u2019s kind of what it is. The reason that it\u2019s sort of become important is because, I mean we\u2019ve all seen over the past few years, like cloud platforms get more important, you\u2019ve got things like Amazon Web Services, Microsoft Azure now, and we\u2019ve seen very much a push towards compute-centric machine learning. And now it\u2019s interesting, because you\u2019ve got this weird sort of bifurcation where you\u2019ve got big data going one direction, and now you\u2019ve got this TinyML and microcontrollers going in the other direction, but they\u2019re not necessarily unrelated, because you need to use the models that you made using the big data in order to sort of train and produce and analyze data on the microcontrollers. So you got this sort of weird hybrid mode going on here, where you\u2019re using it for applications that are edge-based, but they still need some sort of interaction with this machine learning system.", "Jeremie (03:51):Right, so that machine learning system, I mean, you pump out models because you said they could be really big models, you could train a computer vision algorithm, you mentioned VGGNet before we started recording here, but it could be anything, and some of these models are gigabytes in size. How do you put a model that\u2019s multiple gigabytes in size onto a small edge device?", "Matthew Stewart (04:15):Yes, that\u2019s a great question. And I guess the first thing you have to, before you even get to the model itself and all of the parameters involved, you also have to think about the framework itself. So it\u2019s not like there\u2019s a TinyML TensorFlow version going on, or at least there wasn\u2019t until someone went ahead and built it. So I know one of the principal developers of that, Pete Warden, he works at Google on TensorFlow for mobile applications, and he helped develop this TensorFlow Lite, which is sort of a very lightweight version of TensorFlow, which removes a lot of the higher level things you can do, like debugging and visualizations, so that you just have the raw things that you really need in order to do the learning itself.", "Matthew Stewart (05:06):But for some microcontrollers, even that\u2019s too big, and that\u2019s, we\u2019re only talking about 500 kilobytes at this point, that\u2019s how big this TensorFlow Lite is. When you go to something like an Arduino Nano, you\u2019re looking at very small amounts of flash memory, it can be 28 kilobytes or something, and they\u2019ve actually made now something \u2026 I think it\u2019s TensorFlow Nite Nano or Micro, and it\u2019s like 20 kilobytes, so it\u2019s-", "Matthew Stewart (05:33):\u2026 really, really tiny. But yeah, you can run neural networks on this tiny framework on a microcontroller, it\u2019s actually quite incredible.", "Matthew Stewart (05:50):Sorry, you mean the network itself, or the framework?", "Jeremie (05:54):Yeah, the network, so when you put the, let\u2019s say you want an Arduino with some kind of visualization capability so it can navigate its environment and know its way around. The model it\u2019s running, obviously, is constrained to something like 28 kilobytes, so there\u2019s some process that gets you from a really large VGGNet or ResNet or something like that that gets you down to Arduino size, like is that \u2026", "Matthew Stewart (06:19):Yes, definitely. So once you have this framework, this TensorFlow Lite Micro or Nano, so that\u2019s, we\u2019re talking about just the libraries here that you\u2019re running the functions on the neural network, but then you\u2019ve also got the problem of the neural network itself, which is the configuration of the neural network, and also the inference that\u2019s going to be going on, and then you\u2019ve got to store the parameters somewhere, so you\u2019ve got a lot of issues to deal with.", "Matthew Stewart (06:44):And this is one reason we don\u2019t do the training, or at least it\u2019s not commonly done at the moment to do training on the devices, because there\u2019s just not enough bandwidth in the sensors or microcontrollers to be able to do this. We were talking before about when you have an automatic differentiation algorithm, which is how neural networks are trained, you need to get to sort of machine level precision, so we\u2019re talking maybe zero with 15 or 16 decimal points after it, that level of accuracy in terms of the gradient to train it accurately on these large neural networks, and you just can\u2019t do that when you\u2019re running these tiny microcontrollers that have eight bit, what\u2019s it called \u2026", "Matthew Stewart (07:35):Yes. Eight bit architectures, you just can\u2019t do it. You need at least 16 or 32 bit architectures. So typically what\u2019s done is you train things in the cloud, and then you have to just compress them onto a microcontroller, and this is done using sort of a pipeline that some people call, there\u2019s a famous paper called Deep Compression which describes how to do this through several means. So what you can do is you can go from a 32 bit representation down to something like eight bit, and that\u2019s going to reduce the size of the network by a factor of four or something. Then you can prune the neural network, so you can cut out parameters that aren\u2019t useful. You can do things like network distillation, where you\u2019re basically compressing the knowledge that\u2019s in the neural network into a smaller representation, and that can severely reduce how many parameters you have to [inaudible 00:08:27].", "Matthew Stewart (08:27):And you can do further things like weight sharing so you\u2019re not saving all these individual parameters, you\u2019re grouping them via k-means clustering and stuff. And then you can also encode that even further using something called Huffman encoding, which is, I like to think of it analogously to like jpeg compression, it can reduce the size by like 90% and you still maintain all of the information, so it\u2019s pretty incredible.", "Jeremie (08:53):Sorry, that point actually seems pretty important, right? This idea that you can conserve the qualities, the characteristics, and the behavior of a large model when you compress it down to a much smaller size. Is that a source of concern, like if you look at a very safety-conscious application, like let\u2019s say we were making self-driving cars, and we wanted a self-driving car that for whatever reason had much smaller capacity than the system that it did all the training, in that case I would imagine you\u2019d want to be pretty sure that you haven\u2019t just allowed some edge case to sneak by when you went through that process. Are there was of checking that performance, of making sure that some of those edge cases are captured? I imagine it\u2019s a really difficult problem.", "Matthew Stewart (09:41):So I guess the first thing I would mention is you probably wouldn\u2019t use TinyML for something so safety critical.", "Matthew Stewart (09:48):So when you have things like Tesla\u2019s Autopilot in its car, it\u2019s running a very powerful computer inside there, and I mean, you can do that because the car has the constraints available to it, or it\u2019s not constrained by thing like location and availability, it can be powered by the batteries that are in the car. Whereas if you have a sensor system that you\u2019re putting on a tree, and you\u2019re trying to get it to listen to whether there are birds around and what kind of birds those are, maybe someone\u2019s only going to look at that every year or so physically, but they want to be able to get data from it and they want to be able to sort of relay information.", "Matthew Stewart (10:31):So they\u2019re slightly different use cases, but in terms of dealing with edge cases, it\u2019s the same with other machine learning frameworks, it\u2019s all about the data. 90% of any sort of data science or machine learning is getting a good dataset and making sure that it accounts for all of these potential edge cases, or at least has a way to deal with them. So a recent example I heard about is when you have a sound detection system, so for example, let\u2019s say that you have a TinyML system outside your house which is listening for glass breaking. Say if it hears glass breaking it\u2019s going to call the police, because it thinks someone\u2019s broke into your house. But it\u2019s very difficult for it to distinguish between glass breaking on your window and maybe you dropping a glass in the kitchen by accident. So you could just drop a kitchen glass and then all of a sudden you have the police turn up at your door, if you didn\u2019t specifically train it on that sound.", "Matthew Stewart (11:32):But it\u2019s pretty hard to interpret all of these different edge cases that can happen, which is probably why they\u2019re called edge cases, and that\u2019s something that definitely someone who\u2019s making the system, or specifically the dataset has to think about, like what are the potential issues that could occur? And I would say it\u2019s an important thing to consider just when you have event-driven TinyML systems like this TinyML device that\u2019s calling the police.", "Jeremie (12:00):Yeah. Well, and I guess, I mean that\u2019s a very general category of issue, too, like out of distribution learning, when you run into a sample that just was not captured in the subspace of data that it was trained on initially, I guess what I\u2019m wondering with respect to TinyML is how can people be confident that the behavior of a large model maps well onto the behavior of a smaller model? Like I guess in a way you could show both, like a certain validation set that neither model has seen before during training, and see how they correlate in terms of performance. Is that roughly what\u2019s done, or \u2026", "Matthew Stewart (12:38):Yeah, so basically what\u2019s done is you compare, so you take the network, and then you basically try and run it as if it\u2019s on this framework. I mean, there\u2019s various ways you could do this. You could do this on an emulator, on your local machine, you can take your local machine and you can download this TensorFlow Lite Nano or Micro and you can run and just see what sort of accuracy you\u2019re going to get. The only thing that you don\u2019t have is you don\u2019t have debugging capabilities and all of these things, but you\u2019re at least going to get the output, and you can encode that however you want.", "Matthew Stewart (13:13):But yeah, so when you\u2019re looking at the performance between the two, a common thing to do in papers is they say, \u201cOkay, we did a certain level of sparsity,\u201d so let\u2019s say we removed 50% of the parameters, or we compressed it by a factor of 10, how does that affect the top one accuracy or the top five accuracy, is pretty common for classification models, so top one accuracy is, that\u2019s your vanilla accuracy on a binary classification, it\u2019s like, I\u2019ve gotten 80% on this set, whereas top five is, say I have 100 classes, the chances of the correct class being in the top five is this accuracy. And you can see that you can compress it a lot, you can compress it maybe, sort of 80, 90%, and you\u2019re only going to lose maybe two or 3% of the accuracy.", "Matthew Stewart (14:05):It\u2019s very astonishing, yeah. So yeah, I\u2019m not too worried in terms of those kind of issues, of, well, we\u2019re going to compress it and all of a sudden it\u2019s going to become useless, that does inevitably happen once you compress it too much, if you start going into the realms of like, seven, six, five bits, yeah, you\u2019re going to get a lot of quantization losses.", "Jeremie (14:27):But at the same time, though, I imagine, it almost seems like there are two questions from a safety or ethical standpoint. The first is about performance, like how does the, as you say, how do the top 1% performance change when you move from the large to the compressed model, but then the second is just like, how correlated are the large and small models? If we had to do a lot of, let\u2019s say, not customization, but manually catching edge cases with a large model, to what extent can we be confident that \u2026 Like, do we have to redo all our safety assessment on the smaller model? Maybe there are weird quirks that we\u2019d want to conserve that aren\u2019t necessarily reflect explicitly in top 1% performance that then map onto the smaller one, just any model idiosyncrasies that might change in that way? And I don\u2019t know if this is something that is being tackled right now, I know it\u2019s early days, but \u2026", "Matthew Stewart (15:22):Yeah, I think it\u2019s hard to say. I haven\u2019t really seen \u2026 So there\u2019s definitely issues. So I\u2019m thinking about the voice applications mostly, because, so let\u2019s say, there\u2019s a famous framework at the moment that people have been developing an open source project called Common Voice, where you can go on the website and you can record a few sentences in your native language or whatever, and then it gets added to this dataset via some kind of voting system, and \u2026", "Matthew Stewart (15:50):When you go to other things like sound detection, for example, there\u2019s a lot less data available. So like the example I said of maybe a gunshot firing or something. That\u2019s very difficult to get that information, and it\u2019s distorted by the environment, maybe background noise, so it\u2019s hard to get a sort of representative sound of what that is. And that definitely makes it difficult, because I don\u2019t think I\u2019ve seen anyone that\u2019s tried to put this into practice and try to protect a gunshot, because there\u2019s a lot of challenges just in the data collection itself.", "Matthew Stewart (16:29):And then you\u2019ve also go, you run into imbalances, so most of the dataset is not going to be this gunshot, it\u2019s going to be background noises of city traffic or whatever, and as we kind of know from basic statistics, the in group error reduces by a factor of 1/n, so if you have a small amount of positive samples of this gunshot, but you have a very large amount of the other samples, which is just sort of general traffic, you\u2019re going to end up having low accuracy on the gunshots compared to the other parameters, so that presents its own issues itself. And then you have things like false triggers, like a champagne cork sounds kind of like a gunshot. So those are definitely issues that we\u2019re going to see them, and it will be interesting to see how people deal with it, but I would not say I\u2019m an expert enough to tell people how they should deal with these issues.", "Jeremie (17:26):Yeah, well it may be too early for those experts necessarily to exist, I guess, because TinyML\u2019s so young. But it\u2019s interesting how tightly coupled this idea of TinyML is to environmental monitoring, because to some degree, the whole point of creating TinyML systems, doing machine learning on-device is so that this device can then navigate an environment, really be out there in some meaningful sense. What are some of the applications that you see coming around the horizon, let\u2019s say first off with just on-device inference, rather than any kind of training? With just the inference piece, what are some of the applications that you\u2019re seeing right now, and what do you see maybe happening in the future?", "Matthew Stewart (18:07):Sure. So obviously, this is very much industry-driven, so I think we\u2019re going to see mostly things that are sort of lucrative things to be able to predict. Those are the items we\u2019re going to see first, and example of some of those would be predictive maintenance. And Pete Warden, so this is the guy in Google who\u2019s, he wrote the TinyML book which came out in February, and he talks in one of his blog posts, one of the first ones about TinyML about the concept of a Cyber Hans, which is basically, so he talks about Hans being like a guy who can just touch a machine and go, \u201cOh, this machine needs this thing repairing,\u201d and it\u2019s the idea of having all of this experience and just knowing when a certain vibration feels wrong, like what\u2019s going on inside a machine? But the idea is you could literally have maybe sort of a machine learning model which is built for this specific machine, and it knows when it\u2019s got a certain frequency of vibration, or you\u2019ve got certain temperature readings, that there\u2019s a specific issue wrong with the machine, that can tell the operator beforehand, before it breaks, that this issue is there, and it can save them a lot of money because they can fix it before it comes a problem.", "Matthew Stewart (19:22):So obviously that\u2019s a very useful application, because you could put that in essentially any mechanical device. You could put it in a car, power plants, machinery, all sorts of things.", "Jeremie (19:34):And would one of the advantages here the fact that you don\u2019t have to send the data to some central server somewhere where presumably it could be intercepted along the way, there are privacy implications to that, is that sort of one of the big advantages of this approach, or is it something else?", "Matthew Stewart (19:50):That is definitely one of the benefits of TinyML. So I mean, you\u2019re basically taking the data, you\u2019re taking the compute from the cloud, you\u2019re putting it very much data centric. So in the cloud, we\u2019ve seen very much compute centric, you bring all of your data into like a data warehouse and you run your inference on it and your training. In this sense, we\u2019re taking the model and we\u2019re putting it right where the data is, and then it can just do its thing. And it doesn\u2019t necessarily have to send anything unless there\u2019s an issue. So in this sense of the predictive maintenance, it would basically just do a red flag when something\u2019s wrong, like maybe the sensor\u2019s broken or maybe the machine itself is broken, which means that you have a lot less crowding of the airwaves, if you like, doesn\u2019t need to be constantly sending to and from this information.", "Matthew Stewart (20:43):But yeah, if that information is sensitive, then that could be very useful for certain applications, and you still have issues of people trying to hack edge devices, but it\u2019s also very difficult to hack a device when it has such little compute capabilities and stuff, like there\u2019s not too much you can do with a lot of these end devices. But if you make that even less by reducing the ability to communicate and the frequency at which they communicate, it definitely makes it more secure.", "Jeremie (21:18):I guess the privacy thing is also sort of a double-edged sword. I mean, so on the one hand you\u2019re not sending data out, as you said, going to the compute, you\u2019re bringing the compute to the data. But at the same time, it implies having a lot more devices around us collecting data and going about their work potentially collecting, as you say, picking up sounds from the environment that include people talking about things or doing any number of the horribly embarrassing things that we all do with our day to day lives. So is that something that\u2019s been a focus yet? I mean, I can understand, I guess right now you\u2019re probably just focused on, how can we get these systems to actually work, and we\u2019ll worry about the details later type thing. But is there any thought so far in that direction?", "Matthew Stewart (22:04):Yeah, I would say, so the idea of the quantified self springs to mind, so sensors that are on or within your body, which are relaying information, so in my field obviously we care about air quality, so people for a long time have been trying to make these tiny devices that people can wear so it monitors how much CO2 and CO and ox and stuff they\u2019re breathing in, because it can be related to specific diseases, for example, or exposure in the workplace. And yeah, I suppose if someone like Facebook got access to that, they would have a field day being like, \u201cOh, this person\u2019s,\u201d I don\u2019t know, \u201cWe can sell this to an insurance company,\u201d or something. Yeah, so there\u2019s definitely going to be issues with that, but I would say yeah, it\u2019s too early to say exactly what those could be. So yeah, before the podcast started, we were talking about Neuralink being sort of a TinyML application, because it\u2019s sort of like a tiny chip implanted in your brain, and we saw it in the demo a few weeks ago in pigs. And if you were able to access that information, you could probably hack it in such a way that you could control someone\u2019s limbs, which would be very unfortunate.", "Matthew Stewart (23:22):And yeah, we\u2019re getting into Black Mirror territory, and it\u2019s very hard to say whether that would be possible in the future, and also I\u2019m sure the people developing this technology would be thinking about this, and they would want to dispel that thought in people\u2019s minds, because otherwise no one would buy this technology.", "Jeremie (23:41):Yeah, and it\u2019s an interesting thing that just keeps coming up over and over again. I guess the clich\u00e9 way of expressing it is with great power comes great responsibility, but in the AI alignment community, for example, people talk a lot about this idea that there\u2019s trade-off between how much you want your machine learning model to be able to do, and how confident you can be in its safety. Because almost by definition, part of the value of machine learning is the part where we abdicate our ability to oversee its actions. Like, machine learning wouldn\u2019t be useful if humans had to double check every calculation, and so we kind of have to step away at a certain point and say, \u201cAll right, now you\u2019re on your own,\u201d type thing, let\u2019s just hope that we\u2019ve programmed the system well.", "Jeremie (24:26):I guess that\u2019s one of those things with Neuralink and whatnot. At a certain point, if you do want to be able to fix blindness, Alzheimer\u2019s, and all these things, inevitably you\u2019re going to be tampering with the kind of circuitry, neural circuitry that is, that precisely controls other super important functions that then implicitly could be hacked.", "Matthew Stewart (24:46):Yeah, it\u2019s going to be interesting in the future, because we are playing with things we\u2019re starting to not really understand, and again, that\u2019s obviously why the field of explainability has become a lot more prevalent in recent years. Because if you go to a business, like a boardroom, and you tell people you have this fancy new algorithm which is going to save millions of lives because it can predict every type of cancer, they\u2019re going to be pretty skeptical unless you can visually explain to them. And these are businesspeople, so you can\u2019t just throw mean squared errors at them and things, you have to have good explanations.", "Matthew Stewart (25:24):I think that sort of goes without saying in any AI field, and equally so for TinyML. Or perhaps even more so, because you don\u2019t even have these debugging capabilities and you have these other constraints, and the sensor could be far away in another country and it\u2019s relaying strange information, and do you believe it? Yeah.", "Jeremie (25:47):So is there an implication there, I mean, when you compress a model down, you go from something with let\u2019s say billions, even, of parameters, down to something with maybe tens of thousands if you\u2019re looking at a couple kilobytes. As you do that, I imagine that \u2026 Well, actually I guess I\u2019m a little unclear about this. Like, I could imagine it being, to some degree, helpful for interpretability, just because you\u2019re reducing the number of parameters in the system, and thereby reducing, presumably, the amount of noise to some degree. Maybe that makes it more interpretable, and yet at the same time doing interpretable ML on-device seems like its own challenge with a whole other \u2026 I mean, TensorFlow Lite probably wouldn\u2019t cut it at that point. But do you have any thoughts on, is that something that you see over the horizon? Would people be able to do explainable ML on-device?", "Matthew Stewart (26:43):Yeah, this is very niche. So I haven\u2019t seen that yet, but it would not surprise me, because there\u2019s just so many applications, and like I said, it\u2019s industry-driven. And industry, when you get into the AI/machine learning realm, they\u2019re very skeptical unless you can visually explain things like maybe what, like if you have a random forest model, what is it doing? You have to have those, what\u2019s the word, the importance values for each parameter, saying what it\u2019s actually using to make these inferences. Because they can be wrong sometimes, and you don\u2019t want to be wrong if it\u2019s someone\u2019s life on the line because you\u2019re deciding whether to give them surgery or no. So yeah, I think, like I said before, because these are such compute constrained devices, I don\u2019t know how critical they\u2019re going to be in environments like that, but also I would say because you\u2019ve got, we\u2019ve mentioned before, you\u2019re going to have a lot of these sensors at once, you\u2019re not just going to have one, really.", "Matthew Stewart (27:43):So let\u2019s imagine that you have, I don\u2019t know, a farm, and you\u2019ve got all these sensors in the ground which are sort of looking at soil contents and sunlight, like solar radiance, things like that, and it\u2019s trying to predict when you should harvest, and then you also have other sensors which have gas sensors on, perhaps it\u2019s like an apple orchard and they\u2019re measuring ethene, which is an emission you get from apples when they\u2019re ripe, they\u2019re ready for picking. If you combine all those together, maybe you would be much more sure. So you could argue that you might not actually need explainable ML if you have enough of those functionalities added together.", "Jeremie (28:19):It\u2019s sort of an ensembling approach to triangulating the ground truth of an environment.", "Matthew Stewart (28:26):Yeah. I mean, I\u2019m just speculating, but that\u2019s sort of \u2026 I don\u2019t know, it makes sense in my mind.", "Jeremie (28:32):Well, it\u2019s interesting the kinds of choices, that just the very fact of moving something on-device, doing TinyML kind of forces people to make. Because in a way, what it makes me think of is like, back in the early days of machine learning, when we really didn\u2019t have much compute horsepower available, much data available, every tiny bit of compute we had, we had to direct towards functionality, we had to try to say like, \u201cAll right, well, we\u2019ll sacrifice everything to make good predictions,\u201d and it\u2019s only to some degree been more recently as people have realized, like, okay, now there\u2019s almost a glut of compute power, we can use this for a lot of other things, people have gone, \u201cOkay, let\u2019s now try to redirect dome of it to interpretability, safety, and even in some cases, alignment.\u201d", "Jeremie (29:14):It\u2019s interesting that this is starting to happen with TinyML in a context where we\u2019re really directly porting some of the super abilities we already have into that environment, so in a way it\u2019s almost like starting this process over again, except with all the power that we\u2019ve accumulated from decades of advanced research, and none of the ability to do the kind of, the safety stuff that might otherwise accompany this stuff. So it\u2019s kind of an interesting, I guess, wild west phase for the space.", "Matthew Stewart (29:44):Yeah, so I guess it is interesting, because it makes us go back to think what are the really important factors that we need to do this inference or this learning on-device, because when you only have 28 kilobytes of memory, you really have to think about what\u2019s important to you, because you can\u2019t even debug things, so you really have to know what\u2019s going on. So yeah, it presents, I wouldn\u2019t say \u2026 They\u2019re certainly different problems to what we had before, but they\u2019re analogous in a lot of ways. It\u2019s almost like the opposite, right, because before we had sort of the capabilities, but we just didn\u2019t have the systems in place, whereas now we have the systems in place, but the capabilities are just not on the systems. Yeah, so it will be interesting to see how it develops, especially because we have a very wide array of devices that can do TinyML. I\u2019m reluctant to call applications on mobile phones TinyML, but that\u2019s certainly, one of the first sort of applications of it was you take a neural network and you want to compress it into an application, and let\u2019s say you\u2019re trying to put VGGNet on an app, and that\u2019s like 100 megabytes, and every time you have a 100 megabyte increase on the App Store, Apple gets very, very, like, \u201cWhat are you doing with this update?\u201d They look at it a lot and it takes a lot of time.", "Matthew Stewart (31:10):And also, you have to be connected to wifi to download an app that big, so it\u2019s obviously a lot better if you can reduce that by a factor of 10 or 100, so these are just common reasons for wanting to be able to compress things. But yeah, mobile phones are a sort of under-realized example of TinyML, but they also, even in the mobile phone themselves, is probably the most prominent version of TinyML that you\u2019ve actually been exposed to, because I can think of two applications off the bat, which one of them is when you pick up your phone, if you have one of the most recent iPhones and suddenly the screen turns on. I mean, that\u2019s TinyML working itself, because it\u2019s using the inertial measurement unit inside, with a very tiny, I don\u2019t exactly know what it is, it might be a neural network, but there\u2019s this tiny chip which is constantly waiting for that to change, and then once it changes, it relays to the screen to turn on, and that in itself is a TinyML application.", "Matthew Stewart (32:12):And the same when you have a phone waiting to hear you say, \u201cOkay Google,\u201d or \u201cHey Siri.\u201d Yeah, so this is called wake word detection, so the \u201cHey Siri\u201d is a wake word, and you try to make them esoteric so that people don\u2019t just randomly say them. But yeah, these run on DSPs, like digital signal processors that they have very small amounts of memory, so I think the Google one has like a 14 kilobyte neural network and it\u2019s operating on a digital signal processor, so it\u2019s incredibly tiny.", "Matthew Stewart (32:48):But you even get these applications on devices that you already use, you just don\u2019t realize that that is an application of it, it\u2019s [crosstalk 00:32:55]-", "Matthew Stewart (32:56):\u2026 devices we already have a these capabilities.", "Jeremie (32:59):Yeah, right. Well I guess it\u2019s, a lot of these use cases, I don\u2019t want to call them trivial, but they\u2019re certainly not, as you said, they\u2019re not mission critical as far as safety goes, and it\u2019s interesting, like the picking up of the phone, these are marginally useful things, and we\u2019re already kind of at that limit, and you can almost see that tension between safety criticality, capabilities, and then interrogatability? Explainability, basically. Like you\u2019re trading off the computer power on the one hand, and then you\u2019re also, I guess the other lever you have to pull is how safety critical is this application? I guess those three things have to dance a very delicate dance to some degree.", "Jeremie (33:44):And I guess that dance gets harder and harder as you move smaller, too, right? Because talking about phones, they have at least some amount of compute power. What are some of the smaller devices that you\u2019ve looked at in your research?", "Matthew Stewart (33:55):So yeah, I guess like phones, phones are pretty powerful these days. They\u2019re more powerful than the first laptops I ever got. I think the most recent iPhones have 4 gigabytes of static RAM, I could be wrong on that, but that\u2019s a lot. And even things like the most recent Raspberry Pis, so this is like a microcomputer, for people that don\u2019t know what that is, but the most recent one I think also has several gigabytes of RAM, and it\u2019s the size of my hand, it\u2019s a very small device.", "Matthew Stewart (34:28):But then you can go to actual microcontrollers, or things like Arduinos and Arduino Nanos and Micros and they have all sorts of names, and then yeah, you\u2019re getting to flash memory use where you can\u2019t just connect it to a computer, you have to go in IDE and you have to compile the code and then flash it onto the devices, and that\u2019s when you get into the issues of, I can\u2019t debug things, because you can\u2019t communicate backwards and forwards, really, except with maybe a sensor.", "Matthew Stewart (35:00):And actually, that was a big issue for when they first started out trying to make TinyML, because the issue was how you get it to say \u201cHello world,\u201d because that\u2019s like the first thing you do on a device, and what is the TinyML equivalent to hello world? And yeah, I think Pete Warden and some other really smart guys, they came up with the idea of, we don\u2019t need it to say, \u201cHello world,\u201d we just need it to be able to blink an LED.", "Matthew Stewart (35:26):So what they did was they took a sine wave and they trained an algorithm, a neural network, to be able to reproduce a sine wave. So possibly the most trivial machine learning algorithm you could ever imagine, it\u2019s very contrived, but yeah, so then it obviously goes through this sort of \u2026 Yeah, I\u2019m not going to say continuous, but it\u2019s like going between a bit value of 256 and zero, and that correlates to a light intensity on the LED and you can see it sort of flashing on and off.", "Matthew Stewart (36:03):But yeah, I mean, the ability to not be able to communicate with these devices makes it very, sort of, difficult to do a lot of the typical, traditional things that we\u2019re used to doing with these machine learning models.", "Jeremie (36:20):Yeah, it\u2019s always the paradox of new inventions, because you have this early rush to do stuff with it, which in a sense is necessary in order to promote interest so that enough people notice it to go, \u201cOh, hey, maybe there\u2019s a problem of interpretability here or explainability here,\u201d and then invest resources in that direction. That\u2019s really cool. So what do you think are the developments that we can \u2026 Maybe this is not a fair question. What are the developments that we can expect from this field in the next, let\u2019s say 12 months. Are people, is the average person going to be surprised by what we\u2019re able to do with on-device machine learning?", "Matthew Stewart (36:59):Yeah, so that\u2019s a tough question, because I really don\u2019t know what people are planning to do with it. I\u2019m sure we\u2019ll see more of things like on phones, and myself, I work with drones, so maybe a lot more drone-based machine learning capabilities on nano drones specifically, but I think agriculture is going to be a huge-", "Jeremie (37:22):Sorry, how big is a nano drone? Just out of curiosity, what\u2019s the scale?", "Matthew Stewart (37:32):Yeah. So it\u2019s actually a mildly controversial thing in the field, because there\u2019s lots of different classification for drones, systems that have been developed, and they overlap, some of them, and some of them use weight, and some of them use wing span, some of them use propulsion system. But yeah, so the reason a nano drone is nice is because it\u2019s small enough that if it hits somebody, it won\u2019t take them out. It\u2019ll just give you a bit of a shock, and then it\u2019ll probably break the drone, but it certainly won\u2019t hurt the human. So it\u2019s like you have the kinetic energy of the rotors, it\u2019s not particularly high if you have a very small payload, so they\u2019d have to spin very fast. Yeah, so that\u2019s one of the reasons nano drones are good, so if you want to look at air quality in a warehouse, like an Amazon warehouse or whatever, you can easily do that using nano drones, which is a good way for looking at things like emissions compliance, which is becoming a big deal these days with all these new regulations coming in. So that\u2019s something that I\u2019m expecting to see.", "Matthew Stewart (38:47):But again, this is a pretty niche application. I think agriculture is a big one, because that\u2019s very much an edge intensive area. You want to know what\u2019s going on in the field, you want to know if you have to water something, if there\u2019s bugs going on, maybe you can detect stressors in plants and know that they need some love.", "Jeremie (39:12):Yeah. Well, I mean, personally, I\u2019m fascinated by the idea of regulatory enforcement using these mechanisms, especially in industries where historically, enforcement of regulations has basically been impossible. You mention checking air quality in an Amazon warehouse or something like that, or going out into a field, our entire system of laws and jurisprudence has evolved in a context where implicitly, an awful lot of these laws simply can\u2019t be enforced because we don\u2019t have the data to do that enforcement. I\u2019m really curious whether having the data is going to result in a shift in laws where we realize, oh my God, if we\u2019re actually going to enforce the letter of the law here, now that we can, this is a totally unrealistic law to impose. You might find companies just collapsing under the weight of the law that\u2019s already out there just with better enforcement.", "Matthew Stewart (40:07):Yeah, I mean, you kind of already see that, actually. So yeah, this is maybe a bit out of scope, but this is a very important aspect in the environmental science field, so I forget whether it\u2019s New Mexico or California, but they have an ozone limit that\u2019s been set by some international regulation of 60 parts per billion, which is reasonably low, and it\u2019s much lower than most other countries, because the US is very developed compared to a lot of countries. But you get into difficult territory, because you actually get a lot of emissions from China that come over the Pacific Ocean. They partition into this sort of chemical which is able to withstand the journey without degrading across the Pacific, then it reforms on the western coastline, and it reforms ozone. So then actually you have, California cannot meet its own emission regulations because of emissions from China, and then you get into some weird, like, is this law okay, should we blame China for this? You get into some international battles there.", "Jeremie (41:13):I imagine there\u2019s, yeah, I could, if I were a betting man, I could place a bet on the next decade bringing about a field of research into something like quantitative law, where people have to basically apply data science professionally to legal practice and start to say, \u201cWell, what\u2019s realistic here?\u201d If we had this ability to monitor systems, monitor people even, which is I guess a whole other direction that this might take. It\u2019s like, yeah, I mean, how do you crunch these numbers, how do you determine whether somebody is net net in violation of something, because presumably, I mean, if you look at a warehouse, I\u2019m sure you\u2019ll find a distribution of different emissions in some pockets. Maybe it\u2019ll be above threshold, maybe in the vast majority it\u2019ll be below, and then you have to get a little bit more nuanced. Like, is it the average, is it the fact that you have one pocket that\u2019s dangerous that matters, and so on. Anyway, I\u2019m really fascinated to see where this all takes us in the next few years.", "Matthew Stewart (42:09):Yeah, I could honestly see that happening. I mean, it might sound like a sort of batshit crazy idea initially, but yeah, I guess-", "Matthew Stewart (42:18):\u2026 the data science application to the legal field.", "Jeremie (42:21):Yeah. Well, you\u2019re definitely playing a big part in building this future, hopefully for better, because so much of this is environmental applications. But are there any final thoughts you wanted to share on the sort of TinyML or environmental data science kind of things?", "Matthew Stewart (42:36):I guess, only that it\u2019s sort of very, very early stages, and there\u2019s not many resources on it. But certainly, if anyone wants to sort of play around with TinyML, all you need is something like an Arduino Nano, and perhaps an Arducam, and then you can go on to TensorFlow Lite and have a bunch of examples, you can sort of play around with it, and yeah, you could actually develop devices that could be used in your house as security cameras, magic wands I think was one of the examples. So yeah, I encourage people if they\u2019re interested to play around with that and see what they can create, because everyone\u2019s on a level playing field at this point, there\u2019s no real experts on this area.", "Jeremie (43:22):A comforting thought to wrap up the conversation on. Thanks so much Matt, I really appreciate it. Do you want to share your Twitter handle? And we\u2019ll share it on the blog post that\u2019ll come with the podcast as well, just [crosstalk 00:43:32]-", "Jeremie (43:38):Nice, awesome. Well, really appreciate it and thanks for coming on.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa6728711a9ac&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----a6728711a9ac---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa6728711a9ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa6728711a9ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/tagged/tds-podcast", "anchor_text": "TDS podcast"}, {"url": "https://www.youtube.com/watch?v=EJBnKXdIO-8", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/emerging-problems-in-data-science-and-machine-learning-36d37f6531a8", "anchor_text": "emerging problems in data science and machine learning"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://www.youtube.com/watch?v=EJBnKXdIO-8", "anchor_text": "YouTube"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify"}, {"url": "https://twitter.com/MatthewPStewart", "anchor_text": "follow Matthew on Twitter here"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://towardsdatascience.com/tiny-machine-learning-the-next-ai-revolution-495c26463868", "anchor_text": "Matthew\u2019s latest article introducing Tiny ML here"}, {"url": "https://medium.com/tag/tinyml?source=post_page-----a6728711a9ac---------------tinyml-----------------", "anchor_text": "Tinyml"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a6728711a9ac---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/iot?source=post_page-----a6728711a9ac---------------iot-----------------", "anchor_text": "IoT"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----a6728711a9ac---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----a6728711a9ac---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa6728711a9ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&user=Jeremie+Harris&userId=59564831d1eb&source=-----a6728711a9ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa6728711a9ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&user=Jeremie+Harris&userId=59564831d1eb&source=-----a6728711a9ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa6728711a9ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa6728711a9ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a6728711a9ac---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a6728711a9ac--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a6728711a9ac--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a6728711a9ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "122K Followers"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-ml-and-the-future-of-on-device-ai-a6728711a9ac&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.amazon.ca/Quantum-Physics-Made-Fundamental-Everything/dp/0735244138", "anchor_text": "Quantum Physics Made Me Do It: A Simple Guide to the Fundamental Nature of Everything2023"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}