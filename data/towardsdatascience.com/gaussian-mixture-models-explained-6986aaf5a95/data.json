{"url": "https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95", "time": 1682996473.5242822, "path": "towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95/", "webpage": {"metadata": {"title": "Gaussian Mixture Models Explained | by Oscar Contreras Carrasco | Towards Data Science", "h1": "Gaussian Mixture Models Explained", "description": "In the world of Machine Learning, we can distinguish two main areas: Supervised and unsupervised learning. The main difference between both lies in the nature of the data as well as the approaches\u2026"}, "outgoing_paragraph_urls": [{"url": "https://bit.ly/2MpiZp4", "anchor_text": "https://bit.ly/2MpiZp4", "paragraph_index": 45}], "all_paragraphs": ["In the world of Machine Learning, we can distinguish two main areas: Supervised and unsupervised learning. The main difference between both lies in the nature of the data as well as the approaches used to deal with it. Clustering is an unsupervised learning problem where we intend to find clusters of points in our dataset that share some common characteristics. Let\u2019s suppose we have a dataset that looks like this:", "Our job is to find sets of points that appear close together. In this case, we can clearly identify two clusters of points which we will colour blue and red, respectively:", "Please note that we are now introducing some additional notation. Here, \u03bc1 and \u03bc2 are the centroids of each cluster and are parameters that identify each of these. A popular clustering algorithm is known as K-means, which will follow an iterative approach to update the parameters of each clusters. More specifically, what it will do is to compute the means (or centroids) of each cluster, and then calculate their distance to each of the data points. The latter are then labeled as part of the cluster that is identified by their closest centroid. This process is repeated until some convergence criterion is met, for example when we see no further changes in the cluster assignments.", "One important characteristic of K-means is that it is a hard clustering method, which means that it will associate each point to one and only one cluster. A limitation to this approach is that there is no uncertainty measure or probability that tells us how much a data point is associated with a specific cluster. So what about using a soft clustering instead of a hard one? This is exactly what Gaussian Mixture Models, or simply GMMs, attempt to do. Let\u2019s now discuss this method further.", "A Gaussian Mixture is a function that is comprised of several Gaussians, each identified by k \u2208 {1,\u2026, K}, where K is the number of clusters of our dataset. Each Gaussian k in the mixture is comprised of the following parameters:", "Let us now illustrate these parameters graphically:", "Here, we can see that there are three Gaussian functions, hence K = 3. Each Gaussian explains the data contained in each of the three clusters available. The mixing coefficients are themselves probabilities and must meet this condition:", "Now how do we determine the optimal values for these parameters? To achieve this we must ensure that each Gaussian fits the data points belonging to each cluster. This is exactly what maximum likelihood does.", "In general, the Gaussian density function is given by:", "Where x represents our data points, D is the number of dimensions of each data point. \u03bc and \u03a3 are the mean and covariance, respectively. If we have a dataset comprised of N = 1000 three-dimensional points (D = 3), then x will be a 1000 \u00d7 3 matrix. \u03bc will be a 1 \u00d7 3 vector, and \u03a3 will be a 3 \u00d7 3 matrix. For later purposes, we will also find it useful to take the log of this equation, which is given by:", "If we differentiate this equation with respect to the mean and covariance and then equate it to zero, then we will be able to find the optimal values for these parameters, and the solutions will correspond to the Maximum Likelihood Estimates (MLE) for this setting. However, because we are dealing with not just one, but many Gaussians, things will get a bit complicated when time comes for us to find the parameters for the whole mixture. In this regard, we will need to introduce some additional aspects that we discuss in the next section.", "We are now going to introduce some additional notation. Just a word of warning. Math is coming on! Don\u2019t worry. I\u2019ll try to keep the notation as clean as possible for better understanding of the derivations. First, let\u2019s suppose we want to know what is the probability that a data point xn comes from Gaussian k. We can express this as:", "Which reads \u201cgiven a data point x, what is the probability it came from Gaussian k?\u201d In this case, z is a latent variable that takes only two possible values. It is one when x came from Gaussian k, and zero otherwise. Actually, we don\u2019t get to see this z variable in reality, but knowing its probability of occurrence will be useful in helping us determine the Gaussian mixture parameters, as we discuss later.", "Likewise, we can state the following:", "Which means that the overall probability of observing a point that comes from Gaussian k is actually equivalent to the mixing coefficient for that Gaussian. This makes sense, because the bigger the Gaussian is, the higher we would expect this probability to be. Now let z be the set of all possible latent variables z, hence:", "We know beforehand that each z occurs independently of others and that they can only take the value of one when k is equal to the cluster the point comes from. Therefore:", "Now, what about finding the probability of observing our data given that it came from Gaussian k? Turns out to be that it is actually the Gaussian function itself! Following the same logic we used to define p(z), we can state:", "Ok, now you may be asking, why are we doing all this? Remember our initial aim was to determine what the probability of z given our observation x? Well, it turns out to be that the equations we have just derived, along with the Bayes rule, will help us determine this probability. From the product rule of probabilities, we know that", "Hmm, it seems to be that now we are getting somewhere. The operands on the right are what we have just found. Perhaps some of you may be anticipating that we are going to use the Bayes rule to get the probability we eventually need. However, first we will need p(xn), not p(xn, z). So how do we get rid of z here? Yes, you guessed it right. Marginalization! We just need to sum up the terms on z, hence", "This is the equation that defines a Gaussian Mixture, and you can clearly see that it depends on all parameters that we mentioned previously! To determine the optimal values for these we need to determine the maximum likelihood of the model. We can find the likelihood as the joint probability of all observations xn, defined by:", "Like we did for the original Gaussian density function, let\u2019s apply the log to each side of the equation:", "Great! Now in order to find the optimal parameters for the Gaussian mixture, all we have to do is to differentiate this equation with respect to the parameters and we are done, right? Wait! Not so fast. We have an issue here. We can see that there is a logarithm that is affecting the second summation. Calculating the derivative of this expression and then solving for the parameters is going to be very hard!", "What can we do? Well, we need to use an iterative method to estimate the parameters. But first, remember we were supposed to find the probability of z given x? Well, let\u2019s do that since at this point we already have everything in place to define what this probability will look like.", "From Bayes rule, we know that", "From our earlier derivations we learned that:", "So let\u2019s now replace these in the previous equation:", "And this is what we\u2019ve been looking for! Moving forward we are going to see this expression a lot. Next we will continue our discussion with a method that will help us easily determine the parameters for the Gaussian mixture.", "Well, at this point we have derived some expressions for the probabilities that we will find useful in determining the parameters of our model. However, in the past section we could see that simply evaluating (3) to find such parameters would prove to be very hard. Fortunately, there is an iterative method we can use to achieve this purpose. It is called the Expectation \u2014 Maximization, or simply EM algorithm. It is widely used for optimization problems where the objective function has complexities such as the one we\u2019ve just encountered for the GMM case.", "Let the parameters of our model be", "Let us now define the steps that the general EM algorithm will follow\u00b9.", "Step 1: Initialise \u03b8 accordingly. For instance, we can use the results obtained by a previous K-Means run as a good starting point for our algorithm.", "Well, actually we have already found p(Z|X, \u03b8). Remember the \u03b3 expression we ended up with in the previous section? For better visibility, let\u2019s bring our earlier equation (4) here:", "For Gaussian Mixture Models, the expectation step boils down to calculating the value of \u03b3 in (4) by using the old parameter values. Now if we replace (4) in (5), we will have:", "Sounds good, but we are still missing p(X, Z|\u03b8*). How can we find it? Well, actually it\u2019s not that difficult. It is just the complete likelihood of the model, including both X and Z, and we can find it by using the following expression:", "Which is the result of calculating the joint probability of all observations and latent variables and is an extension of our initial derivations for p(x). The log of this expression is given by", "Nice! And we have finally gotten rid of this troublesome logarithm that affected the summation in (3). With all of this in place, it will be much easier for us to estimate the parameters by just maximizing Q with respect to the parameters, but we will deal with this in the maximization step. Besides, remember that the latent variable z will only be 1 once everytime the summation is evaluated. With that knowledge, we can easily get rid of it as needed for our derivations.", "In the maximization step, we will find the revised parameters of the mixture. For this purpose, we will need to make Q a restricted maximization problem and thus we will add a Lagrange multiplier to (8). Let\u2019s now review the maximization step.", "Step 3 (Maximization step): Find the revised parameters \u03b8* using:", "Which is what we ended up with in the previous step. However, Q should also take into account the restriction that all \u03c0 values should sum up to one. To do so, we will need to add a suitable Lagrange multiplier. Therefore, we should rewrite (8) in this way:", "And now we can easily determine the parameters by using maximum likelihood. Let\u2019s now take the derivative of Q with respect to \u03c0 and set it equal to zero:", "Then, by rearranging the terms and applying a summation over k to both sides of the equation, we obtain:", "From (1), we know that the summation of all mixing coefficients \u03c0 equals one. In addition, we know that summing up the probabilities \u03b3 over k will also give us 1. Thus we get \u03bb = N. Using this result, we can solve for \u03c0:", "Similarly, if we differentiate Q with respect to \u03bc and \u03a3, equate the derivative to zero and then solve for the parameters by making use of the log-likelihood equation (2) we defined, we obtain:", "And that\u2019s it! Then we will use these revised values to determine \u03b3 in the next EM iteration and so on and so forth until we see some convergence in the likelihood value. We can use equation (3) to monitor the log-likelihood in each step and we are always guaranteed to reach a local maximum.", "It would be nice to see how we can implement this algorithm using a programming language, wouldn\u2019t it? Next, we will see parts of the Jupyter notebook I have provided so you can see a working implementation of GMMs in Python.", "Just as a side note, the full implementation is available as a Jupyter notebook at https://bit.ly/2MpiZp4", "I have used the Iris dataset for this exercise, mainly for simplicity and fast training. From our previous derivations, we stated that the EM algorithm follows an iterative approach to find the parameters of a Gaussian Mixture Model. Our first step was to initialise our parameters. In this case, we can use the values of K-means to suit this purpose. The Python code for this would look like:", "Next, we execute the expectation step. Here we calculate", "And the corresponding Python code would look like:", "Note that in order to calculate the summation we just make use of the terms in the numerator and divide accordingly.", "We then have the maximization step, where we calculate", "The corresponding Python code for this would be the following:", "Note that in order to simplify the calculations a bit, we have made use of:", "Finally, we also have the log-likelihood calculation, which is given by", "The Python code for this would be", "We have pre-computed the value of the second summation in the expectation step, so we just make use of that here. In addition, it is always useful to create graphs to see how the likelihood is making progress.", "We can clearly see that the algorithm converges after about 20 epochs. EM guarantees that a local maximum will be reached after a given number of iterations of the procedure.", "Finally, as part of the implementation we also generate an animation that shows us how the cluster settings improve after each iteration.", "Note how the GMM improves the centroids estimated by K-means. As we converge, the values for the parameters for each cluster do not change any further.", "Gaussian Mixture Models are a very powerful tool and are widely used in diverse tasks that involve data clustering. I hope you found this post useful! Feel free to approach with questions or comments. I would also highly encourage you to try the derivations yourself as well as look further into the code. I look forward to creating more material like this soon.", "[1] Bishop, Christopher M. Pattern Recognition and Machine Learning (2006) Springer-Verlag Berlin, Heidelberg.", "[2] Murphy, Kevin P. Machine Learning: A Probabilistic Perspective (2012) MIT Press, Cambridge, Mass,"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6986aaf5a95&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@OscarContrerasC?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Oscar Contreras Carrasco"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F91a848e356c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=post_page-91a848e356c8----6986aaf5a95---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6986aaf5a95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----6986aaf5a95---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6986aaf5a95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&source=-----6986aaf5a95---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://bit.ly/2MpiZp4", "anchor_text": "https://bit.ly/2MpiZp4"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6986aaf5a95---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gaussian-mixture-model?source=post_page-----6986aaf5a95---------------gaussian_mixture_model-----------------", "anchor_text": "Gaussian Mixture Model"}, {"url": "https://medium.com/tag/gmm?source=post_page-----6986aaf5a95---------------gmm-----------------", "anchor_text": "Gmm"}, {"url": "https://medium.com/tag/clustering?source=post_page-----6986aaf5a95---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----6986aaf5a95---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6986aaf5a95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----6986aaf5a95---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6986aaf5a95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----6986aaf5a95---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6986aaf5a95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F91a848e356c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=post_page-91a848e356c8----6986aaf5a95---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F36bc253d12d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&newsletterV3=91a848e356c8&newsletterV3Id=36bc253d12d7&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----6986aaf5a95---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Written by Oscar Contreras Carrasco"}, {"url": "https://medium.com/@OscarContrerasC/followers?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "358 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://www.linkedin.com/in/oscar-contreras/", "anchor_text": "https://www.linkedin.com/in/oscar-contreras/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F91a848e356c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=post_page-91a848e356c8----6986aaf5a95---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F36bc253d12d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-explained-6986aaf5a95&newsletterV3=91a848e356c8&newsletterV3Id=36bc253d12d7&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----6986aaf5a95---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/pmf-for-recommender-systems-cbaf20f102f0?source=author_recirc-----6986aaf5a95----0---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=author_recirc-----6986aaf5a95----0---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=author_recirc-----6986aaf5a95----0---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Oscar Contreras Carrasco"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6986aaf5a95----0---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/pmf-for-recommender-systems-cbaf20f102f0?source=author_recirc-----6986aaf5a95----0---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "PMF for Recommender SystemsProbabilistic Matrix Factorization and Collaborative Filtering"}, {"url": "https://towardsdatascience.com/pmf-for-recommender-systems-cbaf20f102f0?source=author_recirc-----6986aaf5a95----0---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "\u00b79 min read\u00b7May 7, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcbaf20f102f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpmf-for-recommender-systems-cbaf20f102f0&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----cbaf20f102f0----0-----------------clap_footer----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/pmf-for-recommender-systems-cbaf20f102f0?source=author_recirc-----6986aaf5a95----0---------------------d994ad2d_e649_4eab_bc39_971986e30160-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcbaf20f102f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpmf-for-recommender-systems-cbaf20f102f0&source=-----6986aaf5a95----0-----------------bookmark_preview----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6986aaf5a95----1---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----6986aaf5a95----1---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----6986aaf5a95----1---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6986aaf5a95----1---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6986aaf5a95----1---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6986aaf5a95----1---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6986aaf5a95----1---------------------d994ad2d_e649_4eab_bc39_971986e30160-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----6986aaf5a95----1-----------------bookmark_preview----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----6986aaf5a95----2---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----6986aaf5a95----2---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----6986aaf5a95----2---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6986aaf5a95----2---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----6986aaf5a95----2---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----6986aaf5a95----2---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----6986aaf5a95----2---------------------d994ad2d_e649_4eab_bc39_971986e30160-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----6986aaf5a95----2-----------------bookmark_preview----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3?source=author_recirc-----6986aaf5a95----3---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=author_recirc-----6986aaf5a95----3---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=author_recirc-----6986aaf5a95----3---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Oscar Contreras Carrasco"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6986aaf5a95----3---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3?source=author_recirc-----6986aaf5a95----3---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "Support Vector Machines for ClassificationIntuition for Support Vector Machines (SVM) for classification and implementation in Python"}, {"url": "https://towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3?source=author_recirc-----6986aaf5a95----3---------------------d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": "\u00b712 min read\u00b7Jul 7, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&user=Oscar+Contreras+Carrasco&userId=91a848e356c8&source=-----fc7c1565e3----3-----------------clap_footer----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/support-vector-machines-for-classification-fc7c1565e3?source=author_recirc-----6986aaf5a95----3---------------------d994ad2d_e649_4eab_bc39_971986e30160-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc7c1565e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machines-for-classification-fc7c1565e3&source=-----6986aaf5a95----3-----------------bookmark_preview----d994ad2d_e649_4eab_bc39_971986e30160-------", "anchor_text": ""}, {"url": "https://medium.com/@OscarContrerasC?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "See all from Oscar Contreras Carrasco"}, {"url": "https://towardsdatascience.com/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Kay Jan Wong"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "7 Evaluation Metrics for Clustering AlgorithmsIn-depth explanation with Python examples of unsupervised learning evaluation metrics"}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "\u00b710 min read\u00b7Dec 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbdc537ff54d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2&user=Kay+Jan+Wong&userId=fee8693930fb&source=-----bdc537ff54d2----0-----------------clap_footer----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdc537ff54d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2&source=-----6986aaf5a95----0-----------------bookmark_preview----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Thomas A Dorfer"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Density-Based Clustering: DBSCAN vs. HDBSCANWhich algorithm to choose for your data"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "\u00b75 min read\u00b7Dec 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&user=Thomas+A+Dorfer&userId=7c54f9b62b90&source=-----39e02af990c7----1-----------------clap_footer----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&source=-----6986aaf5a95----1-----------------bookmark_preview----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6986aaf5a95----0---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----6986aaf5a95----0-----------------bookmark_preview----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Carla Martins"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "How to Compare and Evaluate Unsupervised Clustering Methods?Using Python, Scikit-Learn, and Google Colab"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "\u00b720 min read\u00b7Feb 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&user=Carla+Martins&userId=a1022761a1b&source=-----84f3617e3769----1-----------------clap_footer----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----6986aaf5a95----1---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&source=-----6986aaf5a95----1-----------------bookmark_preview----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6986aaf5a95----2---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----6986aaf5a95----2---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----6986aaf5a95----2---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----6986aaf5a95----2---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6986aaf5a95----2---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6986aaf5a95----2---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6986aaf5a95----2---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----6986aaf5a95----2-----------------bookmark_preview----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6986aaf5a95----3---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----6986aaf5a95----3---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----6986aaf5a95----3---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----6986aaf5a95----3---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6986aaf5a95----3---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6986aaf5a95----3---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----6986aaf5a95----3---------------------f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----6986aaf5a95----3-----------------bookmark_preview----f74b0fa8_a7be_41e2_86cb_ec74d852b90d-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----6986aaf5a95--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}