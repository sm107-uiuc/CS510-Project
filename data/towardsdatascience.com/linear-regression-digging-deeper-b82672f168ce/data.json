{"url": "https://towardsdatascience.com/linear-regression-digging-deeper-b82672f168ce", "time": 1682997029.7778602, "path": "towardsdatascience.com/linear-regression-digging-deeper-b82672f168ce/", "webpage": {"metadata": {"title": "Linear regression: digging deeper | by Arthur Mello | Towards Data Science", "h1": "Linear regression: digging deeper", "description": "In our first article on linear regression, we covered the basics of one of the simplest, yet powerful, forms of machine learning. In tutorials, data is usually carefully chosen to fit our needs\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/dataseries/linear-regression-the-basics-4daad1aeb845", "anchor_text": "first article on linear regression", "paragraph_index": 0}, {"url": "https://medium.com/dataseries/linear-regression-the-basics-4daad1aeb845", "anchor_text": "our previous article on the basics of linear regression", "paragraph_index": 1}, {"url": "https://data.library.virginia.edu/understanding-q-q-plots/", "anchor_text": "this link", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Breusch\u2013Pagan_test", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Akaike_information_criterion", "anchor_text": "Akaike Information Criterion (AIC)", "paragraph_index": 16}, {"url": "https://github.com/arthurmello/statistics/tree/master/1.%20Linear%20regression", "anchor_text": "here", "paragraph_index": 19}], "all_paragraphs": ["In our first article on linear regression, we covered the basics of one of the simplest, yet powerful, forms of machine learning. In tutorials, data is usually carefully chosen to fit our needs without presenting many difficulties. In real life, however, things are not always that simple: we often run into data that violates many of the basic theoretical assumptions that are necessary for our linear model to make sense. What do we do then, when we run into such data? Do we just give up? Of course not! There are many techniques that we can use in order to make our linear regression feasible, depending on the issues that arise. In this article we\u2019ll go through some of the caveats you should pay attention to, and how to address them.", "We\u2019ll now resume the last model we built for predicting house prices. If you haven\u2019t read our previous article on the basics of linear regression, this is a model that uses the house area, the number of floors it has and the year it was built in, to estimate the house price.", "First, let\u2019s look at how to interpret each of the plots generated by R when we run regression models, so we can spot some of those issues. When we run the command plot() with a linear regression model, it returns 4 plots, each with different interpretations:", "In this plot, we have the fitted values (the values estimated by our model) on the X axis and the residuals (the difference between the estimated values and the actual values) on the Y axis. The shape of the plot, given by the red line, indicates if our data is actually linear (which is a basic condition for a linear regression). In this case, the straight line is a good sign, indicating a relatively linear shape. If that line was a big curve or parabola, that would indicate our data follows a non-linear pattern. If that was the case, then we would need another kind of regression, depending on the shape of the curve, such as a polynomial regression.", "In this plot, we can check if our residuals are normally distributed, which is another assumption for linear models. In this case, we have an issue: the dots are supposed to follow a more or less straight line, which they clearly don\u2019t here. From the graph, we can also see that we have some big outliers to the right and that our distribution is skewed and it has \u201cheavy tails\u201d. Usually, this is not a huge issue: if your sample is big enough you can ignore it since it won\u2019t affect too badly your predictions. For more details on this plot, check out this link.", "In the Scale-Location plot, we check for heteroskedasticity/homoskedasticity: one of the assumptions of linear regressions is that the residuals have a constant variance (homoscedasticity). When this assumption is violated, we say there\u2019s a heteroscedasticity issue. Homoscedasticity is a key assumption for linear regression models, and to make sure it is met, we would want the red line on the plot to be more or less straight and horizontal, which is clearly not the case here. If you are not sure you have a straight line, you can also run a Breusch-Pagan test, included in the lmtest library, to be more precise:", "To simplify things, a really small p-value (less than 0.05, for instance) indicates heteroscedasticity (you can learn a bit more about this test here).", "Ok, so how can we adapt our model in order to consider this deviance? We have to take into account these different scales for standard deviation when running our regression. There are some ways of doing it, one of them being robust regressions (which also help to address outliers). Fortunately, there\u2019s a function that will do it for us, in a package called robust:", "As we can see, our residual standard error has decreased and our R-squared has improved, which is a good sign.", "This is the plot that helps us spot outliers that are so far from the rest of the observations that they mess up our regression. We look at the right corners of the plot and find the dotted red lines. Everything that is past them might have biased our model. Usually, we can identify those dots by their row numbers (that are plotted) and re-run our regression without them.", "So, besides non-linearity, residuals that are not normally distributed, heteroskedasticity, outliers, there are actually other tricky aspects in linear regression? Indeed, and some of them are quite relevant.", "This is a violation of a very important assumption in linear regression, often neglected: the explanatory variables have to be independent. When they are not, then we say there\u2019s multicollinearity in our model. One of the ways of checking that is calculating the VIF (Variance Inflation Factor) of the model. Roughly speaking, it tells us how much the variance of an estimator is inflated, compared to a hypothetical situation where there is no multicollinearity.", "There is no hard rule but, in general, we can say that if VIF is 1, then there is no correlation, if it\u2019s between 1 and 5, there\u2019s moderate correlation, and if it\u2019s greater than 5, there\u2019s high correlation (so our model should be fine). When there\u2019s high correlation, then we should look for highly correlated variables and remove one of them from our model.", "A better way of dealing with multicollinearity is by doing a Ridge regression, a Lasso regression or an Elastic Net regression instead. Generally speaking, they will add weights to the explanatory variables, to make sure the irrelevant ones get a small weight. Another type of regression is called Stepwise regression, which is also useful to help to select variables: it tries out different combinations of features and then selects the best model, according to a few criteria.", "This is a very interesting statistical paradox, illustrated by the image below.", "Although our data might present a negative correlation when we test in different groups, the overall relationship in the population might be the inverse. This issue arises when there\u2019s a third variable that we are not accounting for in our model (in the picture, that would be the variable represented by the colour), showing that selecting the right features is crucial.", "Even after trying several different models and making sure they all comply with our assumptions, we still might end up with a few models that work well. So, how can we choose the best ones? Well, there are a few classic criteria to compare linear regression models, the main one being the Akaike Information Criterion (AIC), which measures how much information is lost by each model. Since a model is, by definition, a simplification of reality, it will always lose some information. What the AIC does is compare these models to see which one loses less information so, the smaller the AIC, the better.", "In our case, model 3 has the smallest AIC and is, therefore, the best model according to this criterion.", "As we have seen, even though it is a relatively simple procedure, there are many subtleties in doing linear regressions. These were the main issues you might encounter when doing them, and some of the possible solutions. Obviously, there is much more to it so, if you want to study further, here are some useful information sources:", "You can access the full R script here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist and educator. I write about data analysis and machine learning applied to marketing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb82672f168ce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b82672f168ce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b82672f168ce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@arthurmello_?source=post_page-----b82672f168ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=post_page-----b82672f168ce--------------------------------", "anchor_text": "Arthur Mello"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9d32d5e0ac40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&user=Arthur+Mello&userId=9d32d5e0ac40&source=post_page-9d32d5e0ac40----b82672f168ce---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb82672f168ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb82672f168ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/dataseries/linear-regression-the-basics-4daad1aeb845", "anchor_text": "first article on linear regression"}, {"url": "https://medium.com/dataseries/linear-regression-the-basics-4daad1aeb845", "anchor_text": "our previous article on the basics of linear regression"}, {"url": "https://data.library.virginia.edu/understanding-q-q-plots/", "anchor_text": "this link"}, {"url": "https://en.wikipedia.org/wiki/Breusch\u2013Pagan_test", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Akaike_information_criterion", "anchor_text": "Akaike Information Criterion (AIC)"}, {"url": "https://en.wikipedia.org/wiki/Linear_regression", "anchor_text": "https://en.wikipedia.org/wiki/Linear_regression"}, {"url": "https://learningstatisticswithr.com/book/regression.html#a-caveat", "anchor_text": "https://learningstatisticswithr.com/book/regression.html"}, {"url": "https://data.library.virginia.edu/diagnostic-plots/", "anchor_text": "https://data.library.virginia.edu/diagnostic-plots/"}, {"url": "https://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/", "anchor_text": "https://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/"}, {"url": "https://github.com/arthurmello/statistics/tree/master/1.%20Linear%20regression", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b82672f168ce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----b82672f168ce---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/r?source=post_page-----b82672f168ce---------------r-----------------", "anchor_text": "R"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b82672f168ce---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/regression-analysis?source=post_page-----b82672f168ce---------------regression_analysis-----------------", "anchor_text": "Regression Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb82672f168ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&user=Arthur+Mello&userId=9d32d5e0ac40&source=-----b82672f168ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb82672f168ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&user=Arthur+Mello&userId=9d32d5e0ac40&source=-----b82672f168ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb82672f168ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b82672f168ce--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb82672f168ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b82672f168ce---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b82672f168ce--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b82672f168ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b82672f168ce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b82672f168ce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b82672f168ce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b82672f168ce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b82672f168ce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b82672f168ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arthur Mello"}, {"url": "https://medium.com/@arthurmello_/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9d32d5e0ac40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&user=Arthur+Mello&userId=9d32d5e0ac40&source=post_page-9d32d5e0ac40--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1fb0cddb25fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-digging-deeper-b82672f168ce&newsletterV3=9d32d5e0ac40&newsletterV3Id=1fb0cddb25fe&user=Arthur+Mello&userId=9d32d5e0ac40&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}