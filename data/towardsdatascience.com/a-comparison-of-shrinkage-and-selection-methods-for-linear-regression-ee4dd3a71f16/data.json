{"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "time": 1682995679.190273, "path": "towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16/", "webpage": {"metadata": {"title": "A Comparison of Shrinkage and Selection Methods for Linear Regression | by Micha\u0142 Oleszak | Towards Data Science", "h1": "A Comparison of Shrinkage and Selection Methods for Linear Regression", "description": "Explanation and Python code for shrinkage and selection methods: Ridge, Lasso, Elastic Net, Partial Least Squares, Least Angle Regression, Principal Components Regression, Best Subset Regression."}, "outgoing_paragraph_urls": [{"url": "http://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "on the website of Hastie\u2019s et al. \u201cThe elements of statistical learning\u201d textbook", "paragraph_index": 4}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles", "paragraph_index": 44}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com", "paragraph_index": 46}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal", "paragraph_index": 46}], "all_paragraphs": ["In this article, we will look at seven popular methods for subset selection and shrinkage in linear regression. After an introduction to the topic justifying the need for such methods, we will look at each approach one by one, covering both mathematical properties and a Python application.", "This article is based on a chapter from the excellent Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer. Some technical details might be paraphrased or quoted directly.", "In the linear regression context, subsetting means choosing a subset from available variables to include in the model, thus reducing its dimensionality. Shrinkage, on the other hand, means reducing the size of the coefficient estimates (shrinking them towards zero). Note that if a coefficient gets shrunk to exactly zero, the corresponding variable drops out of the model. Consequently, such a case can also be seen as a kind of subsetting.", "Shrinkage and selection aim at improving upon the simple linear regression. There are two main reasons why it could need improvement:", "Before jumping straight to the methods themselves, let us first look at the data set we will be analyzing. It comes from a study by Stamey et al. (1989) who investigated the impact of different clinical measurements on the level of prostate-specific antigen (PSA). The task is to identify the risk factors for prostate cancer, based on a set if clinical and demographic variables. The data, together with some descriptions of the variables, can be found on the website of Hastie\u2019s et al. \u201cThe elements of statistical learning\u201d textbook, in the Data section.", "We will start by importing the modules used throughout this article, loading the data, and splitting it into training and testing sets, keeping the targets and the features separately. We will then discuss each of the shrinkage and selection methods, fit it to the training data, and use the test set to check how well can it predict the PSA levels on new data.", "Let us start with the simple linear regression, which will constitute our benchmark. It models the target variable, y, as a linear combination of p predictors, or features, X:", "This model has p + 2 parameters that have to be estimated from the training data:", "These parameters are typically estimated using Ordinary Least Square (OLS). OLS minimizes the sum of squared residuals, given by", "It is helpful to think about this minimization criterion graphically. With only one predictor X, we are in a 2D space, formed by this predictor and the target. In this setting, the model fits such a line in the X-Y space that is the closest to all data points, with the proximity measured as the sum of squared vertical distances of all data points \u2014 see the left panel below. If there are two predictors, X1 and X2, space grows to 3D and now the model fits a plane that is closest to all points in the 3D space \u2014 see the right panel below. With more than two features, the plane becomes the somewhat abstract hyperplane, but the idea is still the same. These visualizations also help to see how the intercept gives the model more flexibility: if it is included, it allows the line or plane to not cross the space\u2019s origin.", "The minimization problem described above turns out to have an analytical solution, and the \u03b2-parameters can be calculated as", "Including a column of ones in the X matrix allows us to express the intercept part of the \u03b2-hat vector in the formula above. The \u201chat\u201d above the \u201c\u03b2\u201d denotes that it is an estimated value, based on the training data.", "In statistics, there are two critical characteristics of estimators to be considered: the bias and the variance. The bias is the difference between the true population parameter and the expected estimator. It measures the inaccuracy of the estimates. The variance, on the other hand, measures the spread between them.", "Clearly, both bias and variance can harm the model\u2019s predictive performance if they are too large. The linear regression, however, tends to suffer from variance, while having a low bias. This is especially the case if there are many predictive features in the model or if they are highly correlated with each other. This is where subsetting and regularization come to rescue. They allow reducing the variance at the cost of introducing some bias, ultimately reducing the total error of the model.", "Before discussing these methods in detail, let us fit a linear regression to out prostate data and check its out-of-sample Mean Prediction Error (MAE).", "A straightforward approach to choosing a subset of variables for linear regression is to try all possible combinations and pick one that minimizes some criterion. This is what Best Subset Regression aims for. For every k \u2208 {1, 2, \u2026, p}, where p is the total number of available features, it picks the subset of size k that gives the smallest residual sum of squares. However, the sum of squares cannot be used as a criterion to determine k itself, as it is necessarily decreasing with k: the more variables are included in the model, the smaller are its residuals. This does not guarantee better predictive performance though. That\u2019s why another criterion should be used to select the final model. For models focused on prediction, a (possibly cross-validated) error on test data is a common choice.", "As Best Subset Regression is not implemented in any Python package, we have to loop over k and all subsets of size k manually. The following chunk of code does the job.", "One drawback of Best Subset Regression is that it does not tell us anything about the impact of the variables that are excluded from the model on the response variable. Ridge Regression provides an alternative to this hard selection of variables that splits them into included in and excluded from the model. Instead, it penalizes the coefficients to shrink them towards zero. Not exactly zero, as that would mean exclusion from the model, but in the direction of zero, which can be viewed as decreasing model\u2019s complexity in a continuous way, while keeping all variables in the model.", "In Ridge Regression, the Linear Regression loss function is augmented in such a way to not only minimize the sum of squared residuals but also to penalize the size of parameter estimates:", "Solving this minimization problem results in an analytical formula for the \u03b2s:", "where I denotes an identity matrix. The penalty term \u03bb is a hyperparameter to be chosen: the larger its value, the more are the coefficients shrunk towards zero. One can see from the formula above that as \u03bb goes to zero, the additive penalty vanishes, and \u03b2-ridge becomes the same as \u03b2-OLS from linear regression. On the other hand, as \u03bb grows to infinity, \u03b2-ridge approaches zero: with high enough penalty, coefficients can be shrunk arbitrarily close to zero.", "But does this shrinkage really result in reducing the variance of the model at the cost of introducing some bias as promised? Yes, it does, which is clear from the formulae for ridge regression estimates\u2019 bias and variance: as \u03bb increases, so does the bias, while the variance goes down!", "Now, how to choose the best value for \u03bb? Run cross-validation trying a set of different values and pick one that minimizes cross-validated error on test data. Luckily, Python\u2019s scikit-learn can do this for us.", "Lasso, or Least Absolute Shrinkage and Selection Operator, is very similar in spirit to Ridge Regression. It also adds a penalty for non-zero coefficients to the loss function, but unlike Ridge Regression which penalizes the sum of squared coefficients (the so-called L2 penalty), LASSO penalizes the sum of their absolute values (L1 penalty). As a result, for high values of \u03bb, many coefficients are exactly zeroed under LASSO, which is never the case in Ridge Regression.", "Another important difference between them is how they tackle the issue of multicollinearity between the features. In Ridge Regression, the coefficients of correlated variables tend to be similar, while in LASSO one of them is usually zeroed and the other is assigned the entire impact. Because of this, Ridge Regression is expected to work better if there are many large parameters of about the same value, i.e. when most predictors truly impact the response. LASSO, on the other hand, is expected to come on top when there are a small number of significant parameters and the others are close to zero, i.e. when only a few predictors actually influence the response.", "In practice, however, one doesn\u2019t know the true values of the parameters. So, the choice between Ridge Regression and LASSO can be based on an out-of-sample prediction error. Another option is to combine these two approaches in one \u2014 see the next section!", "LASSO\u2019s loss function looks as follows:", "Unlike in Ridge Regression, this minimization problem cannot be solved analytically. Fortunately, there are numerical algorithms able to deal with it.", "Elastic Net first emerged as a result of critique on LASSO, whose variable selection can be too dependent on data and thus unstable. Its solution is to combine the penalties of Ridge Regression and LASSO to get the best of both worlds. Elastic Net aims at minimizing the loss function that includes both the L1 and L2 penalties:", "where \u03b1 is the mixing parameter between Ridge Regression (when it is zero) and LASSO (when it is one). The best \u03b1 can be chosen with scikit-learn\u2019s cross-validation-based hyperparameter tuning.", "So far we have discussed one subsetting method, Best Subset Regression, and three shrinkage methods: Ridge Regression, LASSO, and their combination, Elastic Net. This section is devoted to an approach located somewhere in between subsetting and shrinking: Least Angle Regression (LAR). This algorithm starts with a null model, with all coefficients equal to zero, and then works iteratively, at each step moving the coefficient of one of the variables towards its least squares value.", "More specifically, LAR starts with identifying the variable most correlated with the response. Then it moves the coefficient of this variable continuously toward its least squares value, thus decreasing its correlation with the evolving residual. As soon as another variable \u201ccatches up\u201d in terms of correlation with the residual, the process is paused. The second variable then joins the active set, i.e. the set of variables with non-zero coefficients, and their coefficients are moved together in a way that keeps their correlations tied and decreasing. This process continues until all the variables are in the model and ends at the full least-squares fit. The name \u201cLeast Angle Regression\u201d comes from the geometrical interpretation of the algorithm in which the new fit direction at a given step makes the smallest angle with each of the features that already have non-zero coefficients.", "The code chunk below applies LAR to the prostate data.", "We have already discussed methods for choosing variables (subsetting) and decreasing their coefficients (shrinkage). The last two methods explained in this article take a slightly different approach: they squeeze the input space of the original features into a lower-dimensional space. Mainly, they use X to create a small set of new features Z that are linear combinations of X and then use those in regression models.", "The first of these two methods is Principal Components Regression. It applies Principal Components Analysis, a method allowing to obtain a set of new features, uncorrelated with each other, and having high variance (so that they can explain the variance of the target), and then uses them as features in simple linear regression. This makes it similar to Ridge Regression, as both of them operate on the principal components space of the original features (for PCA-based derivation of Ridge Regression see [1] in Sources at the bottom of this article). The difference is that PCR discards the components with the least informative power, while Ridge Regression simply shrinks them stronger.", "The number of components to retain can be viewed as a hyperparameter and tuned via cross-validation, as is the case in the code chunk below.", "The final method discussed in this article is Partial Least Squares (PLS). Similarly to Principal Components Regression, it also uses a small set of linear combinations of the original features. The difference lies in how these combinations are constructed. While Principal Components Regression uses only X themselves to create the derived features Z, Partial Least Squares additionally uses the target y. Hence, while constructing Z, PLS seeks directions that have high variance (as these can explain variance in the target) and high correlation with the target. This stays in contrast to the principal components approach, which focuses on high variance only.", "Under the hood of the algorithm, the first of the new features, z1, is created as a linear combination of all features X, where each of the Xs is weighted by its inner product with the target y. Then, y is regressed on z1 giving PLS \u03b2-coefficients. Finally, all X are orthogonalized with respect to z1. Then the process starts anew for z2 and goes on until the desired numbers of components in Z are obtained. This number, as usual, can be chosen via cross-validation.", "It can be shown that although PLS shrinks the low-variance components in Z as desired, it can sometimes inflate the high-variance ones, which might lead to higher prediction errors in some cases. This seems to be the case for our prostate data: PLS performs the worst among all the discussed methods.", "With many, possibly correlated features, linear models fail in terms of prediction accuracy and model\u2019s interpretability due to large variance of the model\u2019s parameters. This can be alleviated by reducing the variance, which can only happen at the cost of introducing some bias. Yet, finding the best bias-variance trade-off can optimize the model\u2019s performance.", "Two broad classes of approaches allowing to achieve this are subsetting and shrinkage. The former selects a subset of variables, while the latter shrinks the coefficients of the model towards zero. Both approaches result in a reduction of the model\u2019s complexity, which leads to the desired decrease in parameters\u2019 variance.", "This article discussed a couple of subsetting and shrinkage methods:", "As you will see from the applications to the prostate data if you run the code chunks above, most of these methods perform similarly in terms of prediction accuracy. The first 5 methods\u2019 errors range between 0.467 and 0.517, beating least squares\u2019 error of 0.523. The last two, PCR and PLS, perform worse, possibly due to the fact that there are not that many features in the data, hence gains from dimensionality reduction are limited.", "Thanks for reading! I hope you have learned something useful that will benefit your projects \ud83d\ude80", "If you liked this post, try one of my other articles. Can\u2019t choose? Pick one of these:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer & Data Science Instructor | Top Writer in AI & Statistics | michaloleszak.com | Book 1:1 @ hiretheauthor.com/michal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fee4dd3a71f16&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaloleszak.medium.com/?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8----ee4dd3a71f16---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee4dd3a71f16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee4dd3a71f16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "on the website of Hastie\u2019s et al. \u201cThe elements of statistical learning\u201d textbook"}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles"}, {"url": "https://towardsdatascience.com/boost-your-grasp-on-boosting-acf239694b1", "anchor_text": "Boost your grasp on boostingDemystifying the famous competition-winning algorithm.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb", "anchor_text": "Linear Classifiers: An OverviewThis article discusses the mathematical properties and practical Python applications of four popular linear\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/uncertainty-from-imputation-8dbb34a19612", "anchor_text": "Uncertainty from imputationAre you taking it into account in your predictions?towardsdatascience.com"}, {"url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "anchor_text": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ee4dd3a71f16---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/statistics?source=post_page-----ee4dd3a71f16---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/regression?source=post_page-----ee4dd3a71f16---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/tag/linear-models?source=post_page-----ee4dd3a71f16---------------linear_models-----------------", "anchor_text": "Linear Models"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ee4dd3a71f16---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee4dd3a71f16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----ee4dd3a71f16---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee4dd3a71f16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----ee4dd3a71f16---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee4dd3a71f16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fee4dd3a71f16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ee4dd3a71f16---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ee4dd3a71f16--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://michaloleszak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com"}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38bf302f5b56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16&newsletterV3=c58320fab2a8&newsletterV3Id=38bf302f5b56&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}