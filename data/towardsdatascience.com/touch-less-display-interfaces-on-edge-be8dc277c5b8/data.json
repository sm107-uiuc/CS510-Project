{"url": "https://towardsdatascience.com/touch-less-display-interfaces-on-edge-be8dc277c5b8", "time": 1683014198.703464, "path": "towardsdatascience.com/touch-less-display-interfaces-on-edge-be8dc277c5b8/", "webpage": {"metadata": {"title": "Touch-less Display Interfaces on Edge | by Anand P V | Towards Data Science", "h1": "Touch-less Display Interfaces on Edge", "description": "Personal Note: Won Grand Prize at \u201cDeep Learning Superhero Challenge\u201d with Intel organized by Hackster.io for this project. It gives solace to finally hit the bull\u2019s-eye, after a chain of near\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.hackster.io/ananduthaman/touch-less-display-interfaces-on-edge-ac9f43", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_face_detection_adas_binary_0001_description_face_detection_adas_binary_0001.html", "anchor_text": "MobileNet backbone", "paragraph_index": 12}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_head_pose_estimation_adas_0001_description_head_pose_estimation_adas_0001.html", "anchor_text": "model", "paragraph_index": 13}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_facial_landmarks_35_adas_0002_description_facial_landmarks_35_adas_0002.html", "anchor_text": "custom CNN", "paragraph_index": 14}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_landmarks_regression_retail_0009_description_landmarks_regression_retail_0009.html", "anchor_text": "Landmark Regression model", "paragraph_index": 15}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_gaze_estimation_adas_0002_description_gaze_estimation_adas_0002.html", "anchor_text": "VGG-like CNN", "paragraph_index": 16}, {"url": "https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf", "anchor_text": "classic facial landmark paper", "paragraph_index": 43}, {"url": "https://docs.openvinotoolkit.org/latest/openvino_inference_engine_samples_speech_libs_and_demos_Live_speech_recognition_demo.html", "anchor_text": "here", "paragraph_index": 50}, {"url": "https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-vtune/top.html", "anchor_text": "Intel VTune", "paragraph_index": 56}, {"url": "https://www.linkedin.com/in/ananduthaman/", "anchor_text": "here", "paragraph_index": 59}, {"url": "https://www.hackster.io/ananduthaman/touch-less-display-interfaces-on-edge-ac9f43", "anchor_text": "here", "paragraph_index": 60}, {"url": "https://classroom.udacity.com/nanodegrees/nd131", "anchor_text": "https://classroom.udacity.com/nanodegrees/nd131", "paragraph_index": 61}], "all_paragraphs": ["Personal Note: Won Grand Prize at \u201cDeep Learning Superhero Challenge\u201d with Intel organized by Hackster.io for this project. It gives solace to finally hit the bull\u2019s-eye, after a chain of near misses, despite the bounties \u2014 US$ 1000 & Intel Movidius NCS 2 sticks. Sure, there is light at the end of the tunnel :)", "Interactive public kiosks are now so widely used viz. Banking (ATMs), Airport (Check-in), Government (e-Governance), Retail (Product Catalogue), Healthcare (Appointment), Schools (Attendance), Corporate (Registration), Events (Info) and the list goes on. While businesses move towards kiosks to better service delivery, touch-free interaction of all public devices has become an imperative to mitigate the spread of ubiquitous Corona virus.", "Gesture or Speech Navigation might seem to address the above, but such devices are resource constrained to analyze such inputs. Have you noticed your mobile voice assistant, be it Siri or GAssist, gives up when mobile goes offline? Your voice-enabled car infotainment system fails to respond, while you drive remote roads. Even a conventional computer won\u2019t be able to run multiple AI models concurrently.", "Ain\u2019t it nice to do it all on your device itself? Imagine a bed-side assistant device which can take visual or voice cues from bedridden patients. This is possible with the advent of Intel OpenVINO. It enables and accelerates deep learning inference from the edge, by doing hardware-conscious optimizations. OpenVINO supports CPU, iGPU, VPU, FPGA and GNAs. If you wanna get your hands wet, a Raspberry Pi along with Intel Movidius NCS 2 would be your best bet to toy with.", "In this blog, we will try to build a Human-Computer Interaction (HCI) module which intelligently orchestrates 5 concurrently-run AI models, one feeding onto another. AI models for face detection, head pose estimation, facial landmarks computation and angle of gaze estimation identify gesture control inputs and trigger mapped actions. A child thread is deployed to run offline speech recognition, which communicates with the parent process to give parallel control commands based on user utterance, to assist and augment gesture control.", "If you like the project, kindly give a thumbs up here", "Each component in the architecture diagram is explained below.", "There are 4 control modes defined in the system, to determine the mode of user input. We can switch between control modes using gestures.", "To translate the 3D gaze orientation angles to 2D screen dimension, the system has to know the yaw and pitch angles corresponding to opposite corners of the screen. Given these 2 angles of opposite corners, we can interpolate the (x, y) location in the screen for intermediate (yaw, pitch) angles.", "Therefore, the user will be prompted to look at opposite corners of the screen, when the application is initiated. Such a calibration step is needed to map the variation in gaze angles to the size and shape of the screen, in order for the \u201cgaze mode\u201d to function properly.", "Without calibration also the system can function, albeit at the expense of generality. To demonstrate, the relative change in head orientation is taken as the metric to move mouse pointer, when the system is in \u201chead pose\u201d mode.", "Four Pre-trained OpenVINO models are executed on the input video stream, one feeding onto another, to detect a) Face Location b) Head Pose c) Facial Landmarks and d) Gaze Angles.", "a) Face Detection: A pruned MobileNet backbone with efficient depth-wise convolutions is used. The model outputs (x, y) coordinates of the face in the image, which is fed as input to steps (b) and (c)", "b) Head Pose Estimation: The model outputs Yaw, Pitch and Roll angles of head, taking face image as input from step (a)", "c) Facial Landmarks: a custom CNN used to estimate 35 facial landmarks.", "This model takes cropped face image from step (a) as input and computes facial landmarks, as above. Such a detailed map is required to identify facial gestures, though it is double as heavy in compute demand (0.042 vs 0.021 GFlops), compared to the Landmark Regression model, which gives just 5 facial landmarks.", "d) Gaze Estimation: custom VGG-like CNN for gaze direction estimation.", "The network takes 3 inputs: left eye image, right eye image, and three head pose angles \u2014 (yaw, pitch, and roll) \u2014 and outputs 3-D gaze vector in Cartesian coordinate system.", "To feed one model output to another model as input, the return values of each model need to be decoded and post-processed.", "For instance, to determine gaze angle, the head orientation need to be numerically combined with the vector output from gaze model, as below.", "Similarly, the facial landmarks model returns ratio of input image size. Hence, we need to multiply output by image width and height to compute (x, y) coordinates of 35 landmarks.", "While output of facial landmark and gaze estimation models can be easily post-processed as above, the conversion of head pose estimation model output is a bit more involved.", "Note the \u201cHead Pose Estimation\u201d model outputs only the attitude, i.e. Yaw, Pitch and Roll angles of the head. To obtain the corresponding direction vector, we need to compute the rotation matrix, using attitude.", "i) Yaw is a counterclockwise rotation of \u03b1 about the z-axis. The rotation matrix is given by,", "ii) Pitch is a counterclockwise rotation of \u03b2 about the y-axis. The rotation matrix is given by,", "iii) Roll is a counterclockwise rotation of \u03b3 about the x-axis. The rotation matrix is given by", "We can place a 3D body in any orientation, by rotating along 3 axes, one after the other. Hence, to compute the direction vector, you need to multiply the above 3 matrices.", "So far, we have controlled the mouse pointer using head and gaze. But to use a kiosk, you also need to trigger events, such as \u2018Left Click\u2019, \u2018Right Click\u2019, \u2018Scroll\u2019, \u2018Drag\u2019 etc.", "In order to do so, a set of pre-defined gestures need to be mapped to each event, and be recognized from the visual input. Two events can be mapped to \u2018wink\u2019 event of left and right eye, but they need to be identified as \u2018wink\u2019.", "You can easily notice that the number of white pixels will suddenly increase when the eyes are open, and decrease when closed. We can just count the white pixels to differentiate open vs closed eye.", "But in real world, above logic is not reliable because white pixel value itself can range. We can always use Deep Learning or ML techniques to classify but its advisable to use a numerical solution, in the interest of efficiency, especially when you code for edge devices.", "Lets see how to numerically detect winks using signals in 4 steps!", "2. Compute spread of non-zero pixels in the histogram. When an eye is closed, the spread will take a sudden dip and vice-versa.", "3. Try to fit a inverse sigmoid curve at the tail-end of the above signal.", "4. If successful fit is found, then confirm the \u2018step down\u2019 shape of fitted curve and declare it as \u2018wink\u2019 event. (no curve fit = eye is not winking)", "If above steps are not clear, then see how the histogram spread graph falls, when an open eye is closed.", "Given the above signal, you can imagine that the curve would take shape of \u2018S\u2019 when the eye is opened for a few seconds. This can be mathematically parameterized using a sigmoid function.", "But since we need to detect \u2018wink\u2019 event shown above, the shape of the curve will take the form of an inverse sigmoid function. To flip the sigmoid function about the x-axis, find f(-x)", "Take any online function visualizer to plot the above function and change parameters to see how reverse \u2018S\u2019 shape is changing (to fit the above Fig. Histogram Spread)", "Thus, if any similar shape is found by parametric curve fit algorithm, at the tail end of the histogram spread curve, then we can call it a \u2018wink\u2019. The curve fit algo tries to solve a nonlinear least-squares problem.", "Note: An efficient way to compute the above can be,", "Alternatively, we can also use the below algo to find eye winks.", "The above method is more efficient than curve fitting, but can lead to many false positives, as peak detection is not always reliable, especially at low light. Middle of the road approach would be to use median and standard deviation to estimate the shape of the curve.", "Eye Aspect Ratio (EAR) is computed in this classic facial landmark paper to determine eye blinks.", "We cannot use above formula to determine eye gesture, as our model does not estimate such a dense landmark map. However, inspired by EAR, we can compute MAR based on the available 4 landmarks obtained from OpenVINO model, as below.", "Two gesture events can be identified using MAR:", "We have liberty to attach 2 commands corresponding to these two gestures.", "To enhance control, we can enable sound based navigation also, along with gesture control. However, system then needs to continuously monitor user utterances to identify commands while it is analyzes image frames from input video stream.", "Naturally therefore, it is prudent to run the speech recognition model in a different thread and let the child-thread communicate with the parent process. The child thread will recognize vocal commands to move the mouse or to write on the screen and pass it on to the parent using shared Queue data structure in Python (as shown below).", "The parent process will run all the above AI models and the computation required for gesture recognition, to enable head and gaze control modes. Thus, it is possible to take gesture and sound control commands in parallel, but for the sake of usability, in this project we chose to take sound commands separately in Control Mode 3.", "To decode sound waves, we use OpenVINO Feature Extraction & Decoder Library which takes in and transcribe the audio coming from the microphone. We have used the speech library as mentioned here to run speech recognition on the edge, without going online.", "As the recognition model is optimized at the expense the accuracy, some tweaks are required to identify spoken command. Firstly, we limit the command vocabulary to say, \u2018up\u2019, \u2018down\u2019, \u2018left\u2019 & \u2018right\u2019 only. Secondly, similar sounding synonyms of command words are stored in a dictionary to find the best match. For instance, \u2018right\u2019 command could be recognized as \u2018write\u2019.", "The function is so written that commands and also synonyms can easily be extended. To enable user entry, speech to write function is also enabled. This has enabled to user to type in alphabets and numbers. Eg: PNR number.", "The gesture control commands are configured as below. However, you can easily change the gesture-command mapping.", "Mouse pointer is controlled using pyautogui library. Functions such as move(), moveTo(), click(), drag(), scroll(), write() etc are used to trigger events corresponding to the above gestures.", "The gaze of an eye or pose of a head will continuously change at least a bit, even if unintended. Such natural motions should not be considered as a command, otherwise the mouse pointer will become jittery. Hence, we introduced a \u2018stickiness\u2019 parameter within which the motion is ignored. This has greatly increased the stability and usability of gesture control.", "Finally, Intel VTune profiler is used to find hotspots and optimize the application code. A shell script vtune_script.sh is fed into the VTune GUI which initiates the project with suitable arguments.", "The project demonstrates the capability of Intel OpenVINO to handle multiple Edge AI models in sequence and in parallel. Many control inputs are also sourced to demonstrate the flexibility. But to deploy a custom solution you can choose controls, as you deem fit.", "For instance, Gaze control may be ideal for big screen while head pose control for laptop screen. Either way, Sound Control can help to accept custom form entries or vocal commands. Gesture-action mapping can also be modified. Yet the point you can drive home is the possibility to chain multiple hardware optimized AI models on the Edge, coupled with efficient numerical computing to solve interesting problems.", "If you have any query or suggestion, you can reach me here", "If you like the project, kindly give a thumbs up here", "[2] Intel\u00ae Edge AI for IoT Nanodegree by Udacity. Idea inspired from Final Course Project. https://classroom.udacity.com/nanodegrees/nd131", "[3] Real-Time Eye Blink Detection using Facial Landmarks by Tereza Soukupova and Jan Cech, Faculty of E.E., Czech Technical University in Prague.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI / ML R&D Specialist, KLA-Tencor | CSE, IIT B Alumnus | Ex-Vice President \u2014 AI & DS, IIA | Ex-TSMC | Reached interview twice for IAS | AI Consultant & Mentor"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbe8dc277c5b8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://anandai.medium.com/?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": ""}, {"url": "https://anandai.medium.com/?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "Anand P V"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc093ed453e2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&user=Anand+P+V&userId=c093ed453e2c&source=post_page-c093ed453e2c----be8dc277c5b8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe8dc277c5b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe8dc277c5b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.hackster.io/contests/DLSuperheroes", "anchor_text": "https://www.hackster.io/contests/DLSuperheroes"}, {"url": "https://www.cybiant.com/wp-content/uploads/2020/01/CKC-ANN-Gif.gif", "anchor_text": "Image Source"}, {"url": "https://www.hackster.io/ananduthaman/touch-less-display-interfaces-on-edge-ac9f43", "anchor_text": "here"}, {"url": "https://github.com/AdroitAnandAI/No-Touch-Display-Interface-at-Edge-IoT", "anchor_text": "here"}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_face_detection_adas_binary_0001_description_face_detection_adas_binary_0001.html", "anchor_text": "MobileNet backbone"}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_head_pose_estimation_adas_0001_description_head_pose_estimation_adas_0001.html", "anchor_text": "model"}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_facial_landmarks_35_adas_0002_description_facial_landmarks_35_adas_0002.html", "anchor_text": "custom CNN"}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_facial_landmarks_35_adas_0002_description_facial_landmarks_35_adas_0002.html", "anchor_text": "Image Courtesy"}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_landmarks_regression_retail_0009_description_landmarks_regression_retail_0009.html", "anchor_text": "Landmark Regression model"}, {"url": "https://docs.openvinotoolkit.org/2019_R1/_gaze_estimation_adas_0002_description_gaze_estimation_adas_0002.html", "anchor_text": "VGG-like CNN"}, {"url": "http://fooplot.com", "anchor_text": "Curve Plot Credits"}, {"url": "https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf", "anchor_text": "classic facial landmark paper"}, {"url": "https://docs.openvinotoolkit.org/latest/openvino_inference_engine_samples_speech_libs_and_demos_Live_speech_recognition_demo.html", "anchor_text": "here"}, {"url": "https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-vtune/top.html", "anchor_text": "Intel VTune"}, {"url": "https://github.com/AdroitAnandAI/No-Touch-Display-Interface-at-Edge-IoT", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/ananduthaman/", "anchor_text": "here"}, {"url": "https://www.hackster.io/ananduthaman/touch-less-display-interfaces-on-edge-ac9f43", "anchor_text": "here"}, {"url": "https://docs.openvinotoolkit.org", "anchor_text": "https://docs.openvinotoolkit.org"}, {"url": "https://classroom.udacity.com/nanodegrees/nd131", "anchor_text": "https://classroom.udacity.com/nanodegrees/nd131"}, {"url": "https://medium.com/tag/touchscreen?source=post_page-----be8dc277c5b8---------------touchscreen-----------------", "anchor_text": "Touchscreen"}, {"url": "https://medium.com/tag/intel?source=post_page-----be8dc277c5b8---------------intel-----------------", "anchor_text": "Intel"}, {"url": "https://medium.com/tag/openvino?source=post_page-----be8dc277c5b8---------------openvino-----------------", "anchor_text": "Openvino"}, {"url": "https://medium.com/tag/covid-19?source=post_page-----be8dc277c5b8---------------covid_19-----------------", "anchor_text": "Covid-19"}, {"url": "https://medium.com/tag/coronavirus?source=post_page-----be8dc277c5b8---------------coronavirus-----------------", "anchor_text": "Coronavirus"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe8dc277c5b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&user=Anand+P+V&userId=c093ed453e2c&source=-----be8dc277c5b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe8dc277c5b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&user=Anand+P+V&userId=c093ed453e2c&source=-----be8dc277c5b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe8dc277c5b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbe8dc277c5b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----be8dc277c5b8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----be8dc277c5b8--------------------------------", "anchor_text": ""}, {"url": "https://anandai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://anandai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anand P V"}, {"url": "https://anandai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "233 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc093ed453e2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&user=Anand+P+V&userId=c093ed453e2c&source=post_page-c093ed453e2c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdc22cac35dc6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftouch-less-display-interfaces-on-edge-be8dc277c5b8&newsletterV3=c093ed453e2c&newsletterV3Id=dc22cac35dc6&user=Anand+P+V&userId=c093ed453e2c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}