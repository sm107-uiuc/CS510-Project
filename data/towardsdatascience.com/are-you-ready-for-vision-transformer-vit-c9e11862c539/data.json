{"url": "https://towardsdatascience.com/are-you-ready-for-vision-transformer-vit-c9e11862c539", "time": 1683014978.8821309, "path": "towardsdatascience.com/are-you-ready-for-vision-transformer-vit-c9e11862c539/", "webpage": {"metadata": {"title": "Are You Ready for Vision Transformer (ViT)? | by Yoshiyuki Igarashi | Towards Data Science", "h1": "Are You Ready for Vision Transformer (ViT)?", "description": "\u201cAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\u201d May Bring Another Breakthrough to Computer Vision"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Residual Networks (ResNet)", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=Jv1VDdI4vy4", "anchor_text": "What is wrong with convolutional neural nets", "paragraph_index": 0}, {"url": "https://openreview.net/forum?id=HJWLfGWRb", "anchor_text": "Capsule Nets", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=LxfUGhug-iQ", "anchor_text": "CS231n", "paragraph_index": 2}, {"url": "https://openreview.net/forum?id=YicbFdNTTy", "anchor_text": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "paragraph_index": 4}, {"url": "https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/", "anchor_text": "Detection Transformers (DETR)", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1911.04252v4", "anchor_text": "Noisy Student", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/minimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9", "anchor_text": "Minimal Requirements to Pretend You are Familiar with BERT", "paragraph_index": 8}, {"url": "http://cnnlocalization.csail.mit.edu/", "anchor_text": "Class Activation Map", "paragraph_index": 12}, {"url": "https://github.com/google-research/vision_transformer", "anchor_text": "The official repository for Vision Transformer is ready", "paragraph_index": 14}, {"url": "https://www.linkedin.com/in/yoshiyuki-igarashi/", "anchor_text": "https://www.linkedin.com/in/yoshiyuki-igarashi/", "paragraph_index": 16}], "all_paragraphs": ["Lives on earth face a cycle of rise and fall. It is applicable not only for creatures but also for technologies. Technologies in data science have been filled with hypes and biased success stories. Having said that, there are technologies that have lead to the growth of data science: Convolutional Neural Network (CNN). Since AlexNet in 2012, different architectures of CNNs have brought a tremendous contribution to real business operations and academic researches. Residual Networks (ResNet) by Microsoft Research in 2015 brought a real breakthrough to build \u201cdeep\u201d CNNs; however, an honorable retirement of this technology would be approaching. Geoffrey Hinton, a father of neural network and one of the 2018 Turing Award winners, has been mentioning the flaws of CNN for years. You can find one of his seminars \u201cWhat is wrong with convolutional neural nets?\u201d in 2017. A major flaw of CNN exists in Pooling layers because it loses a lot of valuable information and it ignores the relationship between the part of images and the whole. In replacement of CNN, Geoffrey Hinton and his team had published a paper on Capsule Nets in 2018; however, it has not replaced CNNs yet.", "I learned about this paper from Andrej Karpathy\u2019s tweet on Oct 3, 2020.", "Andrej Karpathy is a senior director of Artificial Intelligence at Tesla, and he used to teach a class CS231n in 2016, which covered the topics on Computer Vision at Stanford University. Even though the contents were outdated, he showed great skill to present difficult concepts in simple words. I had learned a lot from his class.", "The purpose of this post is to give a heads-up to machine learning engineers and data scientists who have not understood Transformer to prepare themselves before the \u201cinnovative tech company\u201d launches a GitHub repository for Vision Transformer.", "I usually check the names of authors/organizations to identify the credibility of papers before reading. This paper, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, was submitted on Sep 28, 2020, and the author's names have not been revealed yet since the paper is under double-blind review. I would not explicitly mention the company\u2019s name. However, you would be able to make an educated guess who can afford to spend 2,500 TPU days to train a model (highlighted below), and there is another clue that the model was trained on JFT-300M, a private dataset of 300 million images.", "This is not the first paper applying Transformer to Computer Vision. Facebook released Detection Transformers (DETR) in May 2020; however, DETR used Transformer in conjunction with CNN. ViT is the most successful application of Transformer for Computer Vision, and this research is considered to have made three contributions.", "High Accuracy with Less Computation Time for Training", "ViT has decreased the training time by 80% against Noisy Student (published by Google in Jun 2020) even though ViT has reached the approximately same accuracy as Table 2 on the paper (above) shows. Noisy Student adopted the EfficientNet architecture, and I will write another blog post about EfficientNet to help readers to see how far CNNs have traveled since ResNet in the near future.", "The core mechanism behind the Transformer architecture is Self-Attention. It gives the capability to understand the connection between inputs. When Transformers are applied for NLP, it computes the relation between words in a bi-directional manner, which means the order of input does not matter unlike RNN. A model with Transformer architecture handles variable-sized input using stacks of Self-Attention layers instead of CNNs and RNNs. You can learn more about Transformer in my last post written in layman\u2019s terms for business people, \u201cMinimal Requirements to Pretend You are Familiar with BERT\u201d.", "A major challenge of applying Transformers without CNN to images is applying Self-Attention between pixels. If the size of the input image is 640x640, the model needs to calculate self-attention for 409K combinations. Also, you can imagine that it is not likely that a pixel at a corner of an image will have a meaningful relationship with another pixel at the other corner of the image. ViT has overcome this problem by segmenting images into small patches (like 16x16). The atom of a sentence is a word, and this research defined a patch as the atom of an image instead of a pixel to efficiently tease out patterns.", "Efficacy of Transformer with Small Patches", "The paper has analyzed the internal representations of ViT by analyzing the intermediate results from Multi-Head Attention. The paper has discovered that the model is able to encode the distance of patches in the similarity of position embeddings. Another discovery is that the paper found ViT integrates information across the entire image even in the lowest layers in Transformers. As a side-note, ViT-Large has 24 layers with the hidden size of 1,024 and 16 attention heads. The quote from the paper is \u201cWe find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model.\u201d", "Analyzing the model performance qualitatively is often as important as analyzing quantitatively to understand the robustness of predictions. I usually use Class Activation Map (by MIT in 2015) to validate the robustness of model performance by reviewing class activation maps from the images with correct predictions, false-positives, and false-negatives to create and test different hypotheses.", "I rarely read papers under review because the contents of submitted papers will be revised and many of them would be even declined by journals. But, I wrote this post because the contents are really innovative, and I also liked the poetic title of the paper! I plan to make some updates on this post when the paper is officially published.", "The official repository for Vision Transformer is ready. Enjoy a life with ViT!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer \u2014 9+ years\u2019 experience to deliver end-to-end solutions for real business problems. https://www.linkedin.com/in/yoshiyuki-igarashi/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc9e11862c539&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e11862c539--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e11862c539--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=post_page-----c9e11862c539--------------------------------", "anchor_text": ""}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=post_page-----c9e11862c539--------------------------------", "anchor_text": "Yoshiyuki Igarashi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe623f98d9c7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=post_page-e623f98d9c7c----c9e11862c539---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9e11862c539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9e11862c539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@daria_shevtsova?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Daria Shevtsova"}, {"url": "https://unsplash.com/s/photos/death?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Residual Networks (ResNet)"}, {"url": "https://www.youtube.com/watch?v=Jv1VDdI4vy4", "anchor_text": "What is wrong with convolutional neural nets"}, {"url": "https://openreview.net/forum?id=HJWLfGWRb", "anchor_text": "Capsule Nets"}, {"url": "https://twitter.com/karpathy/status/1312279279741276161?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet", "anchor_text": "This tweet was created by Andrej Karpathy"}, {"url": "https://www.youtube.com/watch?v=LxfUGhug-iQ", "anchor_text": "CS231n"}, {"url": "https://openreview.net/forum?id=YicbFdNTTy", "anchor_text": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"url": "https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/", "anchor_text": "Detection Transformers (DETR)"}, {"url": "https://arxiv.org/abs/1911.04252v4", "anchor_text": "Noisy Student"}, {"url": "https://towardsdatascience.com/minimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9", "anchor_text": "Minimal Requirements to Pretend You are Familiar with BERT"}, {"url": "http://cnnlocalization.csail.mit.edu/", "anchor_text": "Class Activation Map"}, {"url": "https://github.com/google-research/vision_transformer", "anchor_text": "The official repository for Vision Transformer is ready"}, {"url": "https://openreview.net/forum?id=YicbFdNTTy", "anchor_text": "Google An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer by Jay Alamer"}, {"url": "https://towardsdatascience.com/minimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9", "anchor_text": "Minimal Requirements to Pretend You are Familiar with BERT"}, {"url": "https://towardsdatascience.com/simple-copy-paste-is-a-game-changer-for-computer-vision-5858a9445caa", "anchor_text": "Simple Copy-Paste is a Game Changer for Computer Vision Problems"}, {"url": "https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e", "anchor_text": "Understanding PyTorch with an example: a step-by-step tutorial"}, {"url": "https://huggingface.co/transformers/quickstart.html", "anchor_text": "HuggingFace\u2019s quick start"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c9e11862c539---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----c9e11862c539---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c9e11862c539---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/transformers?source=post_page-----c9e11862c539---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c9e11862c539---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9e11862c539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=-----c9e11862c539---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9e11862c539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=-----c9e11862c539---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9e11862c539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e11862c539--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc9e11862c539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c9e11862c539---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e11862c539--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c9e11862c539--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c9e11862c539--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c9e11862c539--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c9e11862c539--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c9e11862c539--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c9e11862c539--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c9e11862c539--------------------------------", "anchor_text": ""}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yoshiyuki Igarashi"}, {"url": "https://igarashi-yoshiyuki.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "104 Followers"}, {"url": "https://www.linkedin.com/in/yoshiyuki-igarashi/", "anchor_text": "https://www.linkedin.com/in/yoshiyuki-igarashi/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe623f98d9c7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=post_page-e623f98d9c7c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3b62919d413d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&newsletterV3=e623f98d9c7c&newsletterV3Id=3b62919d413d&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}