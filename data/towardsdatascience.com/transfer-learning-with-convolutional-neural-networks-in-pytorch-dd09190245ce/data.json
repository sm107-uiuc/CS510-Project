{"url": "https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce", "time": 1682994000.158457, "path": "towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce/", "webpage": {"metadata": {"title": "Transfer Learning with Convolutional Neural Networks in PyTorch | by Will Koehrsen | Towards Data Science", "h1": "Transfer Learning with Convolutional Neural Networks in PyTorch", "description": "Although Keras is a great library with a simple API for building neural networks, the recent excitement about PyTorch finally got me interested in exploring this library. While I\u2019m one to blindly\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.oreilly.com/ideas/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch", "anchor_text": "recent excitement about PyTorch", "paragraph_index": 0}, {"url": "https://hub.packtpub.com/what-is-pytorch-and-how-does-it-work/", "anchor_text": "adoption by researchers", "paragraph_index": 0}, {"url": "https://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/", "anchor_text": "inclusion in the fast.ai library", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/learn-by-sharing-4461cc93f8c1", "anchor_text": "best way to learn a new technology", "paragraph_index": 1}, {"url": "https://github.com/WillKoehrsen/pytorch_challenge/blob/master/Transfer%20Learning%20in%20PyTorch.ipynb", "anchor_text": "Jupyter Notebook on GitHub", "paragraph_index": 3}, {"url": "https://www.udacity.com/facebook-pytorch-scholarship", "anchor_text": "Udacity PyTorch scholarship challenge", "paragraph_index": 3}, {"url": "http://www.vision.caltech.edu/Image_Datasets/Caltech101/", "anchor_text": "Caltech 101 dataset", "paragraph_index": 4}, {"url": "https://machinelearningmastery.com/transfer-learning-for-deep-learning/", "anchor_text": "transfer learning", "paragraph_index": 5}, {"url": "http://cs231n.github.io/transfer-learning/", "anchor_text": "For object recognition with a CNN", "paragraph_index": 5}, {"url": "http://www.image-net.org/", "anchor_text": "Imagenet dataset", "paragraph_index": 6}, {"url": "http://ruder.io/transfer-learning/", "anchor_text": "approach has proven successful for a wide range of domains", "paragraph_index": 8}, {"url": "http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf", "anchor_text": "data augmentation", "paragraph_index": 14}, {"url": "https://stats.stackexchange.com/questions/239076/about-cnn-kernels-and-scale-rotation-invariance", "anchor_text": "invariant", "paragraph_index": 14}, {"url": "https://blog.floydhub.com/ten-techniques-from-fast-ai/", "anchor_text": "test time augmentation is possible in the", "paragraph_index": 16}, {"url": "https://blog.floydhub.com/ten-techniques-from-fast-ai/", "anchor_text": "fast.ai", "paragraph_index": 16}, {"url": "https://github.com/pytorch/vision/issues/39", "anchor_text": "but if we read through what Imagenet requires", "paragraph_index": 17}, {"url": "http://www.image-net.org/", "anchor_text": "Imagenet", "paragraph_index": 24}, {"url": "https://pytorch.org/docs/stable/torchvision/models.html", "anchor_text": "seen here", "paragraph_index": 24}, {"url": "https://pytorch.org/docs/stable/notes/cuda.html", "anchor_text": "PyTorch is the ease of moving different parts of a model to one or more gpus", "paragraph_index": 31}, {"url": "https://medium.com/@colinshaw_36798/fully-utilizing-your-deep-learning-gpus-61ee7acd3e57", "anchor_text": "make full use of your hardware", "paragraph_index": 31}, {"url": "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/", "anchor_text": "negative log likelihood", "paragraph_index": 33}, {"url": "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html", "anchor_text": "PyTorch uses automatic differentiation", "paragraph_index": 33}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "backpropagation", "paragraph_index": 34}, {"url": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf", "anchor_text": "powerpoint", "paragraph_index": 34}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "optimizer is Adam", "paragraph_index": 35}, {"url": "https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html", "anchor_text": "gradient descent", "paragraph_index": 35}, {"url": "https://github.com/WillKoehrsen/pytorch_challenge/blob/master/Transfer%20Learning%20in%20PyTorch.ipynb", "anchor_text": "notebook", "paragraph_index": 39}, {"url": "https://en.wikipedia.org/wiki/Early_stopping", "anchor_text": "Early stopping", "paragraph_index": 41}, {"url": "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5", "anchor_text": "dropout", "paragraph_index": 47}, {"url": "https://www.coursera.org/lecture/machine-learning/model-selection-and-train-validation-test-sets-QGKbr", "anchor_text": "diagnosing a network", "paragraph_index": 51}, {"url": "https://stats.stackexchange.com/questions/95391/what-is-the-definition-of-top-n-accuracy", "anchor_text": "topk accuracy", "paragraph_index": 55}, {"url": "https://forums.fast.ai/t/change-to-how-tta-works/8474/3", "anchor_text": "test time augmentation", "paragraph_index": 62}, {"url": "http://www.goldsborough.me/ml/ai/python/2018/02/04/20-17-20-a_promenade_of_pytorch/", "anchor_text": "benefits of PyTorch", "paragraph_index": 63}, {"url": "https://www.netguru.co/blog/deep-learning-frameworks-comparison", "anchor_text": "speed", "paragraph_index": 63}, {"url": "https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec", "anchor_text": "tensor auto differentiation", "paragraph_index": 63}, {"url": "https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1", "anchor_text": "dynamic nature of PyTorch graphs", "paragraph_index": 63}, {"url": "http://keras.io", "anchor_text": "Keras", "paragraph_index": 63}, {"url": "https://colab.research.google.com", "anchor_text": "available resources", "paragraph_index": 65}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 66}, {"url": "https://willk.online", "anchor_text": "willk.online", "paragraph_index": 66}], "all_paragraphs": ["Although Keras is a great library with a simple API for building neural networks, the recent excitement about PyTorch finally got me interested in exploring this library. While I\u2019m one to blindly follow the hype, the adoption by researchers and inclusion in the fast.ai library convinced me there must be something behind this new entry in deep learning.", "Since the best way to learn a new technology is by using it to solve a problem, my efforts to learn PyTorch started out with a simple project: use a pre-trained convolutional neural network for an object recognition task. In this article, we\u2019ll see how to use PyTorch to accomplish this goal, along the way, learning a little about the library and about the important concept of transfer learning.", "While PyTorch might not be for everyone, at this point it\u2019s impossible to say which deep learning library will come out on top, and being able to quickly learn and use different tools is crucial to succeed as a data scientist.", "The complete code for this project is available as a Jupyter Notebook on GitHub. This project was born out of my participation in the Udacity PyTorch scholarship challenge.", "Our task will be to train a convolutional neural network (CNN) that can identify objects in images. We\u2019ll be using the Caltech 101 dataset which has images in 101 categories. Most categories only have 50 images which typically isn\u2019t enough for a neural network to learn to high accuracy. Therefore, instead of building and training a CNN from scratch, we\u2019ll use a pre-built and pre-trained model applying transfer learning.", "The basic premise of transfer learning is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. For object recognition with a CNN, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction. The idea is the convolutional layers extract general, low-level features that are applicable across images \u2014 such as edges, patterns, gradients \u2014 and the later layers identify specific features within an image such as eyes or wheels.", "Thus, we can use a network trained on unrelated categories in a massive dataset (usually Imagenet) and apply it to our own problem because there are universal, low-level features shared between images. The images in the Caltech 101 dataset are very similar to those in the Imagenet dataset and the knowledge a model learns on Imagenet should easily transfer to this task.", "Following is the general outline for transfer learning for object recognition:", "This approach has proven successful for a wide range of domains. It\u2019s a great tool to have in your arsenal and generally the first approach that should be tried when confronted with a new image recognition problem.", "With all data science problems, formatting the data correctly will determine the success or failure of the project. Fortunately, the Caltech 101 dataset images are clean and stored in the correct format. If we correctly set up the data directories, PyTorch makes it simple to associate the correct labels with each class. I separated the data into training, validation, and testing sets with a 50%, 25%, 25% split and then structured the directories as follows:", "The number of training images by classes is below (I use the terms classes and categories interchangeably):", "We expect the model to do better on classes with more examples because it can better learn to map features to labels. To deal with the limited number of training examples we\u2019ll use data augmentation during training (more later).", "As another bit of data exploration, we can also look at the size distribution.", "Imagenet models need an input size of 224 x 224 so one of the preprocessing steps will be to resize the images. Preprocessing is also where we will implement data augmentation for our training data.", "The idea of data augmentation is to artificially increase the number of training images our model sees by applying random transformations to the images. For example, we can randomly rotate or crop the images or flip them horizontally. We want our model to distinguish the objects regardless of orientation and data augmentation can also make a model invariant to transformations of the input data.", "An elephant is still an elephant no matter which way it\u2019s facing!", "Augmentation is generally only done during training (although test time augmentation is possible in the fast.ai library). Each epoch \u2014 one iteration through all the training images \u2014 a different random transformation is applied to each training image. This means that if we iterate through the data 20 times, our model will see 20 slightly different versions of each image. The overall result should be a model that learns the objects themselves and not how they are presented or artifacts in the image.", "This is the most important step of working with image data. During image preprocessing, we simultaneously prepare the images for our network and apply data augmentation to the training set. Each model will have different input requirements, but if we read through what Imagenet requires, we figure out that our images need to be 224x224 and normalized to a range.", "To process an image in PyTorch, we use transforms , simple operations applied to arrays. The validation (and testing) transforms are as follows:", "The end result of passing through these transforms are tensors that can go into our network. The training transformations are similar but with the addition of random augmentations.", "First up, we define the training and validation transformations:", "Then, we create datasets and DataLoaders . By using datasets.ImageFolder to make a dataset, PyTorch will automatically associate images with the correct labels provided our directory is set up as above. The datasets are then passed to a DataLoader , an iterator that yield batches of images and labels.", "We can see the iterative behavior of the DataLoader using the following:", "The shape of a batch is (batch_size, color_channels, height, width). During training, validation, and eventually testing, we\u2019ll iterate through the DataLoaders, with one pass through the complete dataset comprising one epoch. Every epoch, the training DataLoader will apply a slightly different random transformation to the images for training data augmentation.", "With our data in shape, we next turn our attention to the model. For this, we\u2019ll use a pre-trained convolutional neural network. PyTorch has a number of models that have already been trained on millions of images from 1000 classes in Imagenet. The complete list of models can be seen here. The performance of these models on Imagenet is shown below:", "For this implementation, we\u2019ll be using the VGG-16. Although it didn\u2019t record the lowest error, I found it worked well for the task and was quicker to train than other models. The process to use a pre-trained model is well-established:", "Loading in a pre-trained model in PyTorch is simple:", "This model has over 130 million parameters, but we\u2019ll train only the very last few fully-connected layers. Initially, we freeze all of the model\u2019s weights:", "Then, we add on our own custom classifier with the following layers:", "When the extra layers are added to the model, they are set to trainable by default ( require_grad=True ). For the VGG-16, we\u2019re only changing the very last original fully-connected layer. All of the weights in the convolutional layers and the the first 5 fully-connected layers are not trainable.", "The final outputs from the network are log probabilities for each of the 100 classes in our dataset. The model has a total of 135 million parameters, of which just over 1 million will be trained.", "One of the best aspects of PyTorch is the ease of moving different parts of a model to one or more gpus so you can make full use of your hardware. Since I\u2019m using 2 gpus for training, I first move the model to cuda and then create a DataParallel model distributed over the gpus:", "(This notebook should be run on a gpu to complete in a reasonable amount of time. The speedup over a cpu can easily by 10x or more.)", "The training loss (the error or difference between predictions and true values) is the negative log likelihood (NLL). (The NLL loss in PyTorch expects log probabilities, so we pass in the raw output from the model\u2019s final layer.) PyTorch uses automatic differentiation which means that tensors keep track of not only their value, but also every operation (multiply, addition, activation, etc.) which contributes to the value. This means we can compute the gradient for any tensor in the network with respect to any prior tensor.", "What this means in practice is that the loss tracks not only the error, but also the contribution to the error by each weight and bias in the model. After we calculate the loss, we can then find the gradients of the loss with respect to each model parameter, a process known as backpropagation. Once we have the gradients, we use them to update the parameters with the optimizer. (If this doesn\u2019t sink in at first, don\u2019t worry, it takes a little while to grasp! This powerpoint helps to clarify some points.)", "The optimizer is Adam, an efficient variant of gradient descent that generally does not require hand-tuning the learning rate. During training, the optimizer uses the gradients of the loss to try and reduce the error (\u201coptimize\u201d) of the model output by adjusting the parameters. Only the parameters we added in the custom classifier will be optimized.", "The loss and optimizer are initialized as follows:", "With the pre-trained model, the custom classifier, the loss, the optimizer, and most importantly, the data, we\u2019re ready for training.", "Model training in PyTorch is a little more hands-on than in Keras because we have to do the backpropagation and parameter update step ourselves. The main loop iterates over a number of epochs and on each epoch we iterate through the train DataLoader . The DataLoader yields one batch of data and targets which we pass through the model. After each training batch, we calculate the loss, backpropagate the gradients of the loss with respect to the model parameters, and then update the parameters with the optimizer.", "I\u2019d encourage you to look at the notebook for the complete training details, but the basic pseudo-code is as follows:", "We can continue to iterate through the data until we reach a given number of epochs. However, one problem with this approach is that our model will eventually start overfitting to the training data. To prevent this, we use our validation data and early stopping.", "Early stopping means halting training when the validation loss has not decreased for a number of epochs. As we continue training, the training loss will only decrease, but the validation loss will eventually reach a minimum and plateau or start to increase. We ideally want to stop training when the validation loss is at a minimum in the hope that this model will generalize best to the testing data. When using early stopping, every epoch in which the validation loss decreases, we save the parameters so we can later retrieve those with the best validation performance.", "We implement early stopping by iterating through the validation DataLoader at the end of each training epoch. We calculate the validation loss and compare this to the lowest validation loss. If the loss is the lowest so far, we save the model. If the loss has not improved for a certain number of epochs, we halt training and return the best model which has been saved to disk.", "Again, the complete code is in the notebook, but pseudo-code is:", "To see the benefits of early stopping, we can look at the training curves showing the training and validation losses and accuracy:", "As expected, the training loss only continues to decrease with further training. The validation loss, on the other hand, reaches a minimum and plateaus. At a certain epoch, there is no return (or even a negative return) to further training. Our model will only start to memorize the training data and will not be able to generalize to testing data.", "Without early stopping, our model will train for longer than necessary and will overfit to the training data.", "Another point we can see from the training curves is that our model is not overfitting greatly. There is some overfitting as is always be the case, but the dropout after the first trainable fully connected layer prevents the training and validation losses from diverging too much.", "In the notebook I take care of some boring \u2014 but necessary \u2014 details of saving and loading PyTorch models, but here we\u2019ll move right to the best part: making predictions on new images. We know our model does well on training and even validation data, but the ultimate test is how it performs on a hold-out testing set it has not seen before. We saved 25% of the data for the purpose of determining if our model can generalize to new data.", "Predicting with a trained model is pretty simple. We use the same syntax as for training and validation:", "The shape of our probabilities are ( batch_size , n_classes ) because we have a probability for every class. We can find the accuracy by finding the highest probability for each example and compare these to the labels:", "When diagnosing a network used for object recognition, it can be helpful to look at both overall performance on the test set and individual predictions.", "Here are two predictions the model nails:", "These are pretty easy, so I\u2019m glad the model has no trouble!", "We don\u2019t just want to focus on the correct predictions and we\u2019ll take a look at some wrong outputs shortly. For now let\u2019s evaluate the performance on the entire test set. For this, we want to iterate over the test DataLoader and calculate the loss and accuracy for every example.", "Convolutional neural networks for object recognition are generally measured in terms of topk accuracy. This refers to the whether or not the real class was in the k most likely predicted classes. For example, top 5 accuracy is the % the right class was in the 5 highest probability predictions. You can get the topk most likely probabilities and classes from a PyTorch tensor as follows:", "Evaluating the model on the entire testing set, we calculate the metrics:", "These compare favorably to the near 90% top 1 accuracy on the validation data. Overall, we conclude our pre-trained model was able to successfully transfer its knowledge from Imagenet to our smaller dataset.", "Although the model does well, there\u2019s likely steps to take which can make it even better. Often, the best way to figure out how to improve a model is to investigate its errors (note: this is also an effective self-improvement method.)", "Our model isn\u2019t great at identifying crocodiles, so let\u2019s look at some test predictions from this category:", "Given the subtle distinction between crocodile and crocodile_head , and the difficulty of the second image, I\u2019d say our model is not entirely unreasonable in these predictions. The ultimate goal in image recognition is to exceed human capabilities, and our model is nearly there!", "Finally, we\u2019d expect the model to perform better on categories with more images, so we can look at a graph of accuracy in a given category versus the number of training images in that category:", "There does appear to be a positive correlation between the number of training images and the top 1 test accuracy. This indicates that more training data augmentation could be helpful, or, even that we should use test time augmentation. We could also try a different pre-trained model, or build another custom classifier. At the moment, deep learning is still an empirical field meaning experimentation is often required!", "While there are easier deep learning libraries to use, the benefits of PyTorch are speed, control over every aspect of model architecture / training, efficient implementation of backpropagation with tensor auto differentiation, and ease of debugging code due to the dynamic nature of PyTorch graphs. For production code or your own projects, I\u2019m not sure there is yet a compelling argument for using PyTorch instead of a library with a gentler learning curve such as Keras, but it\u2019s helpful to know how to use different options.", "Through this project, we were able to see the basics of using PyTorch as well as the concept of transfer learning, an effective method for object recognition. Instead of training a model from scratch, we can use existing architectures that have been trained on a large dataset and then tune them for our task. This reduces the time to train and often results in better overall performance. The outcome of this project is some knowledge of transfer learning and PyTorch that we can build on to build more complex applications.", "We truly live in an incredible age for deep learning, where anyone can build deep learning models with easily available resources! Now get out there and take advantage of these resources by building your own project.", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd09190245ce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd09190245ce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd09190245ce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----dd09190245ce--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----dd09190245ce--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----dd09190245ce---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd09190245ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd09190245ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.flickr.com/photos/pinks2000/19160002254", "anchor_text": "Source"}, {"url": "https://www.oreilly.com/ideas/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch", "anchor_text": "recent excitement about PyTorch"}, {"url": "https://hub.packtpub.com/what-is-pytorch-and-how-does-it-work/", "anchor_text": "adoption by researchers"}, {"url": "https://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/", "anchor_text": "inclusion in the fast.ai library"}, {"url": "https://towardsdatascience.com/learn-by-sharing-4461cc93f8c1", "anchor_text": "best way to learn a new technology"}, {"url": "https://github.com/WillKoehrsen/pytorch_challenge/blob/master/Transfer%20Learning%20in%20PyTorch.ipynb", "anchor_text": "Jupyter Notebook on GitHub"}, {"url": "https://www.udacity.com/facebook-pytorch-scholarship", "anchor_text": "Udacity PyTorch scholarship challenge"}, {"url": "http://www.vision.caltech.edu/Image_Datasets/Caltech101/", "anchor_text": "Caltech 101 dataset"}, {"url": "https://machinelearningmastery.com/transfer-learning-for-deep-learning/", "anchor_text": "transfer learning"}, {"url": "http://cs231n.github.io/transfer-learning/", "anchor_text": "For object recognition with a CNN"}, {"url": "http://www.image-net.org/", "anchor_text": "Imagenet dataset"}, {"url": "https://www.slideshare.net/xavigiro/transfer-learning-d2l4-insightdcu-machine-learning-workshop-2017", "anchor_text": "source"}, {"url": "http://ruder.io/transfer-learning/", "anchor_text": "approach has proven successful for a wide range of domains"}, {"url": "http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf", "anchor_text": "data augmentation"}, {"url": "https://stats.stackexchange.com/questions/239076/about-cnn-kernels-and-scale-rotation-invariance", "anchor_text": "invariant"}, {"url": "https://blog.floydhub.com/ten-techniques-from-fast-ai/", "anchor_text": "test time augmentation is possible in the"}, {"url": "https://blog.floydhub.com/ten-techniques-from-fast-ai/", "anchor_text": "fast.ai"}, {"url": "https://github.com/pytorch/vision/issues/39", "anchor_text": "but if we read through what Imagenet requires"}, {"url": "http://www.image-net.org/", "anchor_text": "Imagenet"}, {"url": "https://pytorch.org/docs/stable/torchvision/models.html", "anchor_text": "seen here"}, {"url": "https://pytorch.org/docs/stable/torchvision/models.html", "anchor_text": "Source"}, {"url": "https://pytorch.org/docs/stable/notes/cuda.html", "anchor_text": "PyTorch is the ease of moving different parts of a model to one or more gpus"}, {"url": "https://medium.com/@colinshaw_36798/fully-utilizing-your-deep-learning-gpus-61ee7acd3e57", "anchor_text": "make full use of your hardware"}, {"url": "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/", "anchor_text": "negative log likelihood"}, {"url": "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html", "anchor_text": "PyTorch uses automatic differentiation"}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "backpropagation"}, {"url": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf", "anchor_text": "powerpoint"}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "optimizer is Adam"}, {"url": "https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html", "anchor_text": "gradient descent"}, {"url": "https://github.com/WillKoehrsen/pytorch_challenge/blob/master/Transfer%20Learning%20in%20PyTorch.ipynb", "anchor_text": "notebook"}, {"url": "https://en.wikipedia.org/wiki/Early_stopping", "anchor_text": "Early stopping"}, {"url": "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5", "anchor_text": "dropout"}, {"url": "https://www.coursera.org/lecture/machine-learning/model-selection-and-train-validation-test-sets-QGKbr", "anchor_text": "diagnosing a network"}, {"url": "https://stats.stackexchange.com/questions/95391/what-is-the-definition-of-top-n-accuracy", "anchor_text": "topk accuracy"}, {"url": "https://forums.fast.ai/t/change-to-how-tta-works/8474/3", "anchor_text": "test time augmentation"}, {"url": "http://www.goldsborough.me/ml/ai/python/2018/02/04/20-17-20-a_promenade_of_pytorch/", "anchor_text": "benefits of PyTorch"}, {"url": "https://www.netguru.co/blog/deep-learning-frameworks-comparison", "anchor_text": "speed"}, {"url": "https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec", "anchor_text": "tensor auto differentiation"}, {"url": "https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1", "anchor_text": "dynamic nature of PyTorch graphs"}, {"url": "http://keras.io", "anchor_text": "Keras"}, {"url": "https://colab.research.google.com", "anchor_text": "available resources"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://willk.online", "anchor_text": "willk.online"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dd09190245ce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----dd09190245ce---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----dd09190245ce---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----dd09190245ce---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/education?source=post_page-----dd09190245ce---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd09190245ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----dd09190245ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd09190245ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----dd09190245ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd09190245ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd09190245ce--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdd09190245ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dd09190245ce---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd09190245ce--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dd09190245ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dd09190245ce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dd09190245ce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dd09190245ce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dd09190245ce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dd09190245ce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dd09190245ce--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}