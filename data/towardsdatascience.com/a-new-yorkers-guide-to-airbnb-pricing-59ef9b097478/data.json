{"url": "https://towardsdatascience.com/a-new-yorkers-guide-to-airbnb-pricing-59ef9b097478", "time": 1683010683.514776, "path": "towardsdatascience.com/a-new-yorkers-guide-to-airbnb-pricing-59ef9b097478/", "webpage": {"metadata": {"title": "A New Yorker\u2019s Guide to Airbnb Pricing | by Dan Segal | Towards Data Science", "h1": "A New Yorker\u2019s Guide to Airbnb Pricing", "description": "So you want to try a bite of the big apple? You want to wake up in a city that doesn\u2019t sleep? And you don\u2019t care if it\u2019s Chinatown or on Riverside. Well you\u2019re hardly the first. At 10 million\u2026"}, "outgoing_paragraph_urls": [{"url": "https://opendata.cityofnewyork.us/", "anchor_text": "datasets", "paragraph_index": 1}, {"url": "https://djsegal.github.io/pad_pricer", "anchor_text": "Pad Pricer", "paragraph_index": 2}, {"url": "http://insideairbnb.com/get-the-data.html", "anchor_text": "data", "paragraph_index": 3}, {"url": "http://insideairbnb.com/get-the-data.html", "anchor_text": "data store", "paragraph_index": 4}, {"url": "https://github.com/djsegal/metis/blob/master/X_airbnb_revisited/airbnb_pricer/airbnb/get_airbnb_links.py", "anchor_text": "github", "paragraph_index": 4}, {"url": "https://raw.githubusercontent.com/djsegal/metis/master/X_airbnb_revisited/airbnb_pricer/airbnb/skipped_columns.py", "anchor_text": "columns", "paragraph_index": 5}, {"url": "https://github.com/djsegal/metis/blob/master/X_airbnb_revisited/airbnb_pricer/airbnb/compile_airbnb_data.py", "anchor_text": "github", "paragraph_index": 9}, {"url": "http://geopandas.org/", "anchor_text": "GeoPandas", "paragraph_index": 14}, {"url": "http://zipatlas.com/us/ny/new-york/zip-code-comparison/population-density.htm", "anchor_text": "1", "paragraph_index": 14}, {"url": "http://zipatlas.com/us/ny/brooklyn/zip-code-comparison/population-density.htm", "anchor_text": "2", "paragraph_index": 14}, {"url": "http://blog.davidkaleko.com/feature-engineering-cyclical-features.html", "anchor_text": "blog post", "paragraph_index": 20}, {"url": "https://gist.github.com/djsegal/3db97acec43ccf75d6313db4aa5af15c", "anchor_text": "github", "paragraph_index": 45}, {"url": "https://gist.github.com/djsegal/54d6047cd1495925517896f51d5dea2d", "anchor_text": "remove", "paragraph_index": 45}, {"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "L1 norm", "paragraph_index": 49}, {"url": "https://gist.github.com/djsegal/8ef31dfc8e3e861c363e858b359f71d9", "anchor_text": "LogTransformedTargetRegressor", "paragraph_index": 50}, {"url": "https://gist.github.com/djsegal/25ca2a2526b5d13bcc86686a2d8aba67", "anchor_text": "that", "paragraph_index": 56}, {"url": "https://towardsdatascience.com/clustering-the-us-population-observation-weighted-k-means-f4d58b370002", "anchor_text": "Carl Anderson", "paragraph_index": 60}, {"url": "https://gist.github.com/djsegal/12a0ed89558005f6aa64adf73799903f", "anchor_text": "way", "paragraph_index": 62}, {"url": "https://stackoverflow.com/a/39540339/5187080", "anchor_text": "stack overflow", "paragraph_index": 65}, {"url": "https://github.com/djsegal/metis/blob/master/X_airbnb_revisited/airbnb_pricer/utils/get_dist_to_clusters.py", "anchor_text": "get distance to clusters", "paragraph_index": 66}, {"url": "https://scikit-learn.org/stable/modules/mixture.html", "anchor_text": "Bayesian Gaussian mixtures", "paragraph_index": 68}, {"url": "https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49", "anchor_text": "subway", "paragraph_index": 69}, {"url": "https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j", "anchor_text": "restaurant", "paragraph_index": 69}, {"url": "https://github.com/djsegal/metis/blob/54b84108a58d3e95679b519eb361e6916a693709/X_airbnb_revisited/airbnb_pricer/restaurant_data.py#L26-L86", "anchor_text": "restaurants", "paragraph_index": 75}, {"url": "https://github.com/djsegal/metis/blob/54b84108a58d3e95679b519eb361e6916a693709/X_airbnb_revisited/airbnb_pricer/subway/subway_data.py#L29-L67", "anchor_text": "subway stations", "paragraph_index": 75}, {"url": "https://gist.github.com/djsegal/637d46a5ac04cc05832a3793c4b0299a", "anchor_text": "method", "paragraph_index": 89}, {"url": "https://djsegal.github.io/pad_pricer/", "anchor_text": "Pad Pricer", "paragraph_index": 95}, {"url": "https://github.com/djsegal/pad_pricer", "anchor_text": "javascript", "paragraph_index": 99}, {"url": "https://djsegal.github.io/pad_pricer/", "anchor_text": "Pad Pricer", "paragraph_index": 99}], "all_paragraphs": ["So you want to try a bite of the big apple? You want to wake up in a city that doesn\u2019t sleep? And you don\u2019t care if it\u2019s Chinatown or on Riverside. Well you\u2019re hardly the first. At 10 million residents and 60 million annual visitors, the city has seen its fair share of people cross the gates of JFK or LGA.", "For these 60 million annual visitors, many will probably stay at one of Airbnb\u2019s 50k rentals. This poses an interesting problem about using NYC\u2019s available datasets to accurately price Airbnb rentals throughout the year. We will go about this problem in an iterative approach where we build out a model first and then improve it incrementally.", "The end result is a web app that prices rentals in real-time: Pad Pricer.", "As one may have guessed, the first step in pricing Airbnb rentals is to get a listing of rentals from Airbnb. Luckily for us, Airbnb is more than happy to provide this data (in a monthly format). To get to a point where we\u2019re able to predict a rental price, though, we\u2019ll need to go about a three step approach:", "In order to get all the NYC monthly rentals from Airbnb\u2019s data store, we\u2019ll use BeautifulSoup to scrape the website\u2019s index page for their links and then store the urls \u2013 as well as the month and year \u2013 inside a pandas dataframe. For this process, months and years are computed based on a datetime string associated with each url. The get_airbnb_links code can be found on github and the results are as followed:", "The next step is reading all the url's from the above list into their own dataframes using panda\u2019s read_csv function. In anticipation for the next step, we will then drop many of the unneeded columns as they are just taking up space. Because this process of downloading csv files is relatively slow, we can run them asynchronously using the following code snippet:", "The code for this step is available here:", "The final step in building the initial Airbnb pricing dataset is to compile the 48 dataframes generated in the last step. Past a simple concatenation of these dataframes, we must also clean the resulting table. This involves both: updating many columns to their most recent value (e.g. for bed and rating) as well as doing some filtering on the dataset.", "For clarity, our dataset initially contained 2 million unique entries (pairs of: id, month and year). After filtering based on the following criteria, we were left with 300k \u2013 a 85% drop.", "Although these restriction may seem intense, it was done in order to only focus on the market makers in the field and weed out some of the more amateur hosts. The actual code can be viewed on github.", "We have the data, now let\u2019s build a model. Our goal is to create the most simple, atomic model first. Then increase its complexity until we have an ensemble regression fit that incorporates data from: Airbnb (Rentals), the NYC Department of Health (Food), Yelp (Cafes/Bars), and the MTA (Trains). But first, let\u2019s start at square one\u2026", "Occam\u2019s Razor says that \u201cthe simplest solution is almost always the best.\u201d Although that will be shown to not be the case here, it is usually the best place to start. Picking up where we left off last step, we have around 300k entries in an Airbnb pricing dataset for NYC (covering the last 4 years).", "As you can see from this table, the id is associated with multiple rows. Besides the price, month and year, the other columns all contain a lot of redundant information. Therefore, we are going to split this dataset in three, where each has an id column \u2013 to join on \u2013 as well as the following columns:", "Starting from the table closest to the original one, the Price dataset is just the table from step one with all the constant fields dropped (i.e. now just price, month and year). The Rental data is then the assumed constant fields associated with each NYC apartment: accommodates, bathrooms, bedrooms, and beds. For clarity, the length of the Rental data is 20x smaller than the one for Price!", "The Location dataset requires a little more explanation. First, the zipcode was actually given in the initial dataset. The is_brooklyn flag then refers to how we just focus on: Manhattan and Brooklyn. Next, geometry is a field that stores the latitude and longitude of the rental; what this means practically is we are using a GeoPandas dataframe to perform GIS explorations later. Lastly, density is a field we got from joining on tables from ZipAtlas [1, 2].", "Although we seem to have actually gone more complicated here, it was done in preparation of building the simplest model: one built on the Price dataset.", "With minimum fanciness, our goal is to get to a model that predicts a rental\u2019s nightly price based off: the month and year. As you may have guessed, this will perform terribly. It will, however, provide the bootstrap spot to start (over)complicating our model.", "The R^2 you get from this is 0.14%! This is a truly terrible model. It was only able to explain less than a quarter of a percent of the variance seen in the rental\u2019s nightly price. This makes sense as it\u2019s basically saying the price has to be only linearly related to both: months and years \u2013 which is simply not true.", "A better \u2013 albeit still terrible \u2013 model would be to add a flag for each month and each year. This in turn detangles the coupling between months and years. In simpler terms, instead of a month variable that can be any number between 1 and 12, you have 12 variables that are either True or False (e.g. is_jan, is_feb, etc.). Practically, we went from a table with two features, month and year, to 16 boolean ones. This 16 comes from the 12 months and the 4 years covered by our dataset.", "Through code, we accomplish this expansion, from 2 variables to 16, using Scikit-learn\u2019s OneHotEncoder. To formalize our build process, we\u2019ll combine this encoder with the LinearRegression model from before \u2013 using a Pipeline. Although only raising the R^2 to 0.15%, we\u2019re now on the path to really start making some progress.", "edit: an alternative to one-hotting months would be to treat month as a cyclical feature \u2013 as demonstrated in this blog post.", "Now that we atomized the dataset and built a simple model, it\u2019s time to start re-adding the data from the Rental and Location datasets \u2013 as well as other geographical-based metrics. For this step (and substep) in particular, we will just be adding the Rental data, i.e. accommodates, bedrooms, bathrooms, and beds. This will require some tweaking of the pipeline, but at the end we\u2019ll have a model that actually starts competing.", "Before looking at the monstrosity below it\u2019s important to spell out exactly what we\u2019re doing now. We are first merging our Price dataset with the Rental dataset. Next, we are one-hotting the month and year, while leaving the other features exactly the same (see: remainder=\"passthrough\"). Then with only a small change to the pipeline we are back up and running!", "If you look at the R^2 value, we\u2019re now at 25%! That\u2019s more than two orders of magnitude improvement over our initial dumb model. Before we can take too much solace in this victory, though, we have to take the data science a little more seriously.", "Although we may be proud of a model that gets an R^2 of 25%, it\u2019s hardly telling of how well it would perform in the wild. In fact, we could probably fit a model that gets above 90% on our data by simply memorizing it (probably using a DecisionTree). However it would do absolutely terrible on new data. Therefore we\u2019ll now refine our pipeline a little more to take the data science a little more seriously.", "The first step in learning to trust our new model is to separate the data into two groups: train and test. This allows us to use one set to train the model on and then another to test it with. For a mid-sized dataset like ours, a 80/20 split between train and test is usually convention. (Some camps would even fight for a 60/20/20 split with a new group called validation, which allows the test data to be used only at the very end as a guard to deployment.)", "The simplest way you could create a train and test dataset from an initial one is by using Scikit-learn\u2019s train_test_split function with the test_size = 0.2. This would however lead to two issues:", "To resolve the first issue, we used binning on id counts and selected 10% of the unique id's (based on their bin counts). Next to get the other 10% of the test set, we actually used train_test_split from Scikit-learn \u2013 we just fed the stratify parameter to allow us to bin on price as well!", "Using the above and below code, we now see that the model is still hovering around an R^2 of 25%. Practically, the R^2 did not change! What this actually means is we are probably under-fitting the model. If the error did massively change from the last step \u2013 i.e. if the test scores were significantly worse than the train ones \u2013 we would be in a situation where we were over-fitting the model.", "// this just means we have room for improvement", "We now have a certified under-fit model with both a train and test dataset. To improve its fit we will start by scaling the various features. Starting with the target, we will do a simple log transform. This is because rental prices vary according to a lognormal distribution, which means a better way to scale error would to be relative to size. This can be added using the following update:", "Performing this operation \u2014 and changing y to refer to it \u2013 actually reduces the R^2 from 25% to 22%. This is where it becomes important to emphasize that R^2 is a useful metric for modelers, but the true metric valuable for industry may be something very different (i.e. the MAE or RMSE). For ease, we will focus on the mean absolute error (MAE) for our model when fitting price. This value is measured in dollars and shows the average value each prediction is off. (Note that even for the log MAE case, we will not use log-dollars.)", "So even though the R_2 went down from the log transform on the target, its MAE did as well. This is an example where metric trade-offs are important! Moving on, we will next transform the features using a Box-Cox power transform. What this does is modify the features to be more normally distributed (one of the core assumptions of linear regression). As the adage goes, though: there is no free lunch. What you pay for an improved model \u2013 in situations like this \u2013 is usually a reduction in interpretability.", "Before we added the log transform on the target just now, our model was actually very simple. It had a linear relationship on its 20 feature variables (and bias offset). What this means practically is that each variable was associated with a simple per_unit value \u2013 here measured in dollars. For example, every additional person an apartment could accommodate netted you a $10 boost in nightly rental rate.", "Once we moved to a log transform, these effects transformed from linear $/unit values to scalars that grow or shrink the target (think going from sums to products). When we power-transform the variables shortly the interpretation will get even harder. Lastly, when we add model ensembling, interpretation will become harder still. It is crucial to ask at every one of these steps if the trade-off is worth it.", "Connected to this idea of interpretability is what variables actually mean and what control do you really have over them. For example, month and year are variables set by when you host. However, one-hot year features prevent you from predicting on future years (while month's do not). It might, therefore, make sense to drop the year feature, but keep the month's.", "On a similar note is how to deliver information to an end-user. In the above table: each bathroom increases rental rates by $110/night, whereas each bedroom only gets you $30. This doesn\u2019t mean you should turn a bedroom into a bathroom. What is probably happening is that bathrooms is becoming a proxy for square-footage. Whereas bedrooms is probably becoming entangled with the number of accommodates and beds.", "Before moving onto the next step it is important to show how Scikit-learn\u2019s power transformer (e.g. a Box-Cox transform) fits into the pipeline:", "Once you make this modification, the R^2 actually goes down a little! Therefore, it might make sense to remove it at some point if it continues to humble performance (i.e. its a potential hyper-parameter for GridSearchCV).", "At this point, we have an okay model that\u2019s had some feature scaling. We will now further the feature engineering process by changing to polynomial regression (over simple linear regression). What this means practically is we will now have terms like accommodates_per_bathrooms that might be able to capture more subtle nuance. To prevent the problem of parameter explosions \u2013 i.e. the curse of dimensionality \u2013 we will need to introduce regularization.", "So far we\u2019ve had a relatively simple pipeline (all things considered). There was some one-hotting going on for the months. Then some Power-Transformation done to make the features more Gaussian. And then we wrapped it up with a Linear Regression \u2013 done with the target scaled logarithmically.", "Now we\u2019re going to make it slightly more complicated\u2026", "Just so everyone\u2019s on the same page, at this point we have a dataframe where each row is a rental listing (for a given month) with the following features:", "The month feature was chosen to be handled with one-hot vectors (e.g. is_jan, is_feb, \u2026, is_dec). The price was the target and is handled with a log transform. Now we\u2019re going to do a few things to the last four features: accommodates, bathrooms, bedrooms, beds.", "These four variables will provide us with an outlet for designing a sub-pipeline. By this I mean we will be utilizing ColumnTransformer to pipe certain commands in for only these four columns (e.g. accommodates and beds). This is show below:", "So what does this (sub-)pipeline do? The first part is a step where we setup reciprocal features for the four values, i.e. per_accommodates and per_bathrooms. This custom code is available on: github. Next, we do a simply polynomial expansion of degree 2, with minimum new features. Additionally, before we do a Box-Cox power transform on the data, we remove:", "To connect this new sub_pipeline with the existing one, we now need to modify the ColumnTransformer used in the main pipeline:", "Alright, so now we definitely have too many features for LinearRegression to be appropriate. The curse of dimensionality says that we are probably now running into severe overfitting situations. And even if that\u2019s not the case yet \u2013 i.e. we might just still have a bad model \u2013 the coefficients probably change so much per training batch that there will be little to no consistency between collections of data. This is a bad thing!", "Therefore, we are going to add feature selection to the model \u2013 to dial down the effects of feature creation. Practically this means swapping Linear Regression out for a two step process:", "This feature selection will be done with LassoCV wrapped inside Scikit-learn\u2019s SelectFromModel. Here, Lasso (see: L1 norm) is a regularized regression technique that squashes out features that don\u2019t improve the model (and CV stands for cross-validation). SelectFromModel, on the other hand, is a function that runs LassoCV with the target of squeezing out unimportant features. In practices this can remove more than 60% of features (from a list of 100+ initial ones).", "The next step is the actual solver that uses ElasticNetCV, instead of LassoCV. This is done because it adds some elements of Ridge Regression (L2 norm), which empirically improves the R^2 values. Before moving on it is important to point out two additional concerns. The first is that we are now using a custom LogTransformedTargetRegressor that works with SelectFromModel.", "Second is that these regularized regression techniques \u2013 i.e. Lasso, Ridge, and Elastic Net \u2013 require features to be scaled. For example, if unscaled, our model would try to scale kilometer quantities 1000 times more strictly than metered ones (if we added geographic features). Therefore, we add a StandardScalar that transforms all the features to have a mean of 0 and a standard deviation of 1. This new pipeline is as follows:", "Now if we check the R_2 for the train and test sets, both are around 23%. We actually improved the model slightly and brought the two scores closer together. The latter of which is evidence that we now have a relatively well-generalized model (the opposite of an overfit one). Now time to get back to improving the R_2 by more than single percentage points!", "The next step on our journey to pricing Airbnb rentals with high accuracy is adding some GIS (geographic information system) data related to New York and how it functions. This can include more obvious details like: population density and whether the rental is in Brooklyn. But can also get more involved, such as finding the distance from a rental to the nearest subway stop or how many Sushi spots are in its general vicinity.", "During Step 2.1 we extracted location features into their own dataset. These included: geometry, zipcode, is_brooklyn, and density. For now we will focus on the parameters for: population density and checking if a rental is in Brooklyn. This means that we will currently ignore geometry and zip-codes.", "The reason for ignoring zip codes is because they would likely need to be one-hotted and there are just too many for that to be relevant. They were used however to get density values. The geometry field, on the other-hand, is not useful at this stage because linearly fitting to latitude and longitude is generally not a good idea. We will, however, use it later on to check for distances to locations, i.e. parks, subway entrances, and restaurants.", "And just like that, we increased our R^2 to 35%! That ain\u2019t too bad: a 10% absolute boost and a 40% relative one. Looking at our current parameter importances gives us somewhat of a clue what is going on with our model:", "So what is this telling us? The number one indicator for airbnb rental price is how many people it can accommodate, as well as how many bathrooms (and bedrooms) are divvied up between them. Is-Brooklyn then adds the obvious effect of Manhattan being the most expensive borough, whereas density went in the opposite direction as what we thought. By this I mean we initially anticipated density to track popular areas where people wanted to visit.", "This effect where density worked in the opposite direction of what we hoped says that there is still room to improve our model. We will now add Metro and Restaurant geo-data to get a better image of how much entertainment the different rentals provide.", "The final point to make is that our initial parameters (the month one-hots) are at the bottom of the list now. They basically imply that colder months are slightly less expensive, whereas summer ones are slightly more. This might support the use of a seasonal one-hotted vector instead of 12 monthly ones.", "Before bringing in new data source, I claim that there is probably one more piece of information we can get from the locations database. Using the average nightly rates, I believe we can find the clusters that correlate to the spots with the highest per-nightly stays. This is analogous to the problem faced by Carl Anderson as he redistricted the mainland united states.", "// completing the analogy: nightly prices are now stand-ins for population", "Okay, so the way to produce the hotspots \u2013 shown in orange and red \u2013 is to do unsupervised learning using KMeans clustering. Although this probably deserves to be in the pipeline, we are saying that it adds the fields from the very beginning (even before the test_train_split).", "Therefore, this plot\u2019s clusters are the actual ones chosen! The 10 red diamonds are the highest price concentrations in the two boroughs. Whereas the orange star is the center of mass for the entire dataset: the (averaged) most expensive spot in the city.", "The way to incorporate these unsupervised cluster nodes into the model is by finding the minimum distance between each rental and the red markers (and additionally the distance of each rental to the orange, central star marker). This will give us two new features: dist_to_center and dist_to_hub. Both will need to transformed to be more Gaussian. Although a simple log might work for center, we\u2019ll instead just use Box-Cox again and be done with thinking about rescaling (as we add more dist features).", "Alright, so we need to calculate the distance between a list of rentals and a list of clusters. Here, distance will be a Euclidian distance with latitude and longitude scaled with a naive relation from stack overflow:", "After using this knowledge inside of a \u201cget distance to clusters\u201d function \u2013 and waiting the minute runtime \u2013 we have a list of distances related to the orange and red markers in the above clusters map! The last step before getting a new R^2 value is adding the features to the main DataFrame and updating the Pipeline used to fit it:", "And just like that, we\u2019re nearing the 50% R_2 point! Using clustering info improved this accuracy metric from 35% to 45% \u2013 and we\u2019re just getting started. Before adding the subway and restaurant info next step, though, it is useful to point out that our unsupervised clustering did cheat, by looking at test data. For production, it would probably be advised to introduce this into the pipeline (as well as adding Queens to increase the rental corpus size).", "edit: instead of using KMeans for clustering hotspots we could have used Bayesian Gaussian mixtures. This would allow clusters to be oblong and oriented at arbitrary angle \u2013 instead of round and roughly equal sized.", "We\u2019re now at the homestretch for adding more data. Our last sub-step is to augment the GIS data-pool by scraping a couple of NYC\u2019s publicly available datasets. These include: a list of all the MTA\u2019s subway stations and a collection of every DOH-compliant restaurant in Manhattan and Brooklyn. This gives us two more scales to think about when understanding Airbnb nightly rental prices.", "For clarity \u2013 in the previous subset \u2013 we basically looped over our list of 15k (unique) rentals to find their distance to: the 10 diamond-shaped cluster-hubs as well as the orange star, which signified the high-cost epicenter. What this means computationally is that we looped through two lists at once: the rentals and the cluster centers. In Big-O notation, our search for the minimum distance took around O(N\u00b7M) where N is the number of rentals and M is the size of another list of geolocations", "So far, M has been 1 and 10 \u2014 for the orange and red case, respectively. For trains and restaurants, these will be O(100) and O(10k), so it makes sense to setup a framework for solving this problem at scale. This is because the simple cases, like M \u2264 10, still took a minute to run. Before discussing how to build the scalable k-d trees needed to solve this problem, it is useful to reemphasize the following orders of magnitude for N and M:", "One major implication of this is the trade-off of importance between: distance (1D) and areal density (2D). For the two cluster examples before \u2013 i.e. for M=1 and M=10 \u2013 only the distance was really relevant. As you move more towards the restaurant\u2019s end of the spectrum, O(N)~O(M), what becomes important is the density: how many restaurants are there in a half-mile radius? This half-mile radius then become a hyper-parameter, r_bubble, that can be tweaked to gain a better perspective of how a city values distance.", "Another comment to make is that like Subway Stations, we are not using all the information found in the Restaurant\u2019s dataset. For example, we could use the line information for subway stations (i.e. L trains, A/C/E trains, etc.) as well as cuisine types (e.g. Pho, Coffee, and Tacos) to bring other scales and cultural data into the mix. For now, let\u2019s just do simple distances and counts for trains and restaurants\u2026", "Without cluttering this document up too much, we are basically scraping NYC\u2019s open-data and then cleaning it up where we deem appropriate. This could be for: bad health grade restaurants or just to remove the unneeded columns associated with subway stations.", "The code for these two scrapers is available on Github: one for restaurants and the other for subway stations. Our goal now is to setup k-d trees that will be built around geo-spatial data sets to ultimately speed up our distance and count measures.", "Succinctly, k-d trees are a data structure that makes spatial search fast. To simplify making these trees, we will use the following helper function:", "Note here that we use PySAL\u2019s implementation of KDTrees to conveniently switch our latitude/longitude measurements to the Imperial mile.", "So how\u2019d we do? Drum roll, please\u2026 48%. After doing a bunch of heavy lifting, we got a couple measly percentage points \u2013 which could be chalked up to noise. Why did this happen? Most likely because this linear model saw too much collinearity in the system. It therefore put the most weight on the two extremes:", "It then highlighted the facts we learned from our 35% and 25% R^2 models. From the former, we learned that rental price went down with population density and if you were outside Manhattan (i.e. in Brooklyn). The latter of which focused on the importance of the accommodates feature:", "It\u2019s been a fun ride so far. We started at a model so terrible it couldn\u2019t even get an R^2 of 1%. Then we moved to a 25% one with features like accommodates and 35% with population density and a check on borough. To get to 50% we added a bunch of features related to the latitude/longitude of a rental, but just couldn\u2019t seem to make any more headway after that.", "Now we\u2019ll try three more options:", "Once we\u2019ve tried everything we\u2019ll take a step back and simplify the model and distill what we\u2019ve actually learned.", "Before trying a bunch of new models and making an ensemble out of them, it is interesting to look at the only real boolean variable in our dataset: is_brooklyn. This feature very neatly divides the dataset in two and \u2013 as shown in the table at the end of Step 4.3 \u2013 strongly affects the nightly rental price of an Airbnb apartment. The two ways we will try to approach this problem are:", "The first of these examples is handled by a custom-build BooleanProduct. This allows variables to scale a little extra (like if food_density is more important for Brooklynites). In practice, this means you end up with features that look like: \"food_density _is_brooklyn\". The second method requires another custom-built BooleanForkRegressor, which finds the most near-equal data-dividing boolean variable; here the is_brooklyn field.", "Drum roll please, still basically 50%. It seems like we can\u2019t level up anymore without finding a new data source that will somehow give us the next 10%. Luckily for us, we haven\u2019t tried the slew of other regression models that Scikit-learn provides.", "We collected all the data, did the feature engineering, and just can\u2019t seem to get above a 50% R^2 threshold. Fear not! Scikit-learn provides a long list of regression algorithms, when ElasticNetCV (the one we\u2019ve been using) stops cutting the mustard. The ones we\u2019ll focus on are: Random Forests and Boosting methods. This, for example, excludes SVM and neural network regression schemes, as the former was too slow for fitting and the latter required too much tuning (for the sake of this tutorial).", "Before adding these regressors to the pipeline, it\u2019s useful to see their form and evaluation metrics. These can be seen in the code and table below:", "From this you see that the two models we\u2019ve been working on so far were actually on the poorer performing end of the spectrum! Random forests and their even more Extra counterparts performed the best \u2013 by far. Then at default parameters, XGBoost seemed to perform better than our models, whereas AdaBoost did a little worse. This now requires an explanation of the Ensemble method, which seems to perform as well as the best models.", "For clarity, Ensembling methods are ones that utilize several different models and stitch together their answer. The way these Frankensteins stitch together their result determines what variety they are: simple averaging for Bagging regressors and a meta-regressor for Stacking regressors. In this context, a meta-regressor is another regressor that is trained on the results of the other regressors instead of on the features! For clarity, our ensemble method \u2014 in the table above \u2014 is a stacking regressor with a RidgeCV meta-regressor.", "Stepping back, an obvious question to ask now might be: why? Why is the model so complicated? Did we need to do all these steps? What have we actually learned? A sceptic myself, I\u2019d say those are all good questions. The model is probably over-engineered. Besides an error measurement in dollars, we\u2019ve left a lot to be desired in terms of usability. And along the way we\u2019ve only really had one or two insights that led to a better answer than a simple random forest based on original features (e.g. accommodates and location).", "The next sub-step will try to do a little course correction and simplify our model as well as provide some end-user interpretability. Before then, it seems valuable to highlight one of the good things that came from all our work. As shown below, we now have a collection of feature importances for every used models! The variables we said were important before are mainly still relevant. However, it seems like a new feature have become important for tree methods, i.e. accommodates*bedrooms. This feature is probably a measure of: bigger is better/pricier.", "The final piece in our price prediction puzzle is handling latitude and longitude correctly. Up until now, it has been treated as an implicit variable, a means to measure distance to various locations \u2013 such as trains, restaurants, and hotspots. Now we will explain why this was done and then leave the exercise of adding it to the reader.", "The reason we initially threw out longitude and latitude from the regression models is because they were initially all linear. Unless prices strictly increase with their distance north, it makes no sense to describe this behavior as linear. Practically, this means that latitude would likely improve predictions for a city like New York \u2013 which was built on an American grid system. But would likely fail to capture any such nuance for European cities, which usually have a radial/polar system where prices drop off from the city center.", "Tree Models, such as Random Forests and XGBoost, on the other hand are nonlinear models. They have no problem cornering off expensive bubbles of the city. Usually with 16 (2\u2074) levels of depth, a tree could identify places like the village, the theatre district, and Williamsburg. Therefore, we can simply add Latitude and Longitude to every forest (i.e. nonlinear) regression model.", "This seems a little unfair to the linear models, however, so we could give each of them the results from a weak learner that just gives a price prediction based on latitude and longitude. These tweaks are what is used in my 4-city price predictor tool: Pad Pricer \u2013 which covers NYC, SF, Berlin and Paris.", "Rewind time. *Record Scratches* You\u2019re probably wondering how we got here. Yea, that\u2019s our model. Kind of crazy to think how simple it was back in Step 1. Where did the time/steps go. Well here we are at Step 5.3. We have something to show for our work (i.e. a model that can predict a nightly rental price within $30, on average), but it\u2019s convoluted and seems to perform just as well as a very simple random forest. Instead of beating ourselves up over it, though, we\u2019re going to learn from what we did and simply, simplify, simplify.", "Minimalism is the hallmark of a good machine learning model. Just like under/over-fitting, you need just enough infrastructure to get 90% of the way there, but not enough to bore the reader. It now makes sense to list what aspects of our model bring joy/value:", "This means we can throw out: all the subway data, seasonal data, and other underachieving features. Additionally, we will now only use Random Forests instead of all the ones we were using before (i.e. our original ElasticNet one).", "The end product is a model so simple it can be written in javascript \u2013 instead of python! This is how Pad Pricer can update rates in real-time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F59ef9b097478&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----59ef9b097478--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59ef9b097478--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@djsegal?source=post_page-----59ef9b097478--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@djsegal?source=post_page-----59ef9b097478--------------------------------", "anchor_text": "Dan Segal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F836515184818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&user=Dan+Segal&userId=836515184818&source=post_page-836515184818----59ef9b097478---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59ef9b097478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59ef9b097478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://500px.com/photo/144166669/Night-time-Brooklyn-Bridge-Panorama-by-mikenusbaum", "anchor_text": "@mikenusbaum"}, {"url": "https://opendata.cityofnewyork.us/", "anchor_text": "datasets"}, {"url": "https://djsegal.github.io/pad_pricer", "anchor_text": "Pad Pricer"}, {"url": "http://insideairbnb.com/get-the-data.html", "anchor_text": "data"}, {"url": "http://insideairbnb.com/get-the-data.html", "anchor_text": "data store"}, {"url": "https://github.com/djsegal/metis/blob/master/X_airbnb_revisited/airbnb_pricer/airbnb/get_airbnb_links.py", "anchor_text": "github"}, {"url": "https://raw.githubusercontent.com/djsegal/metis/master/X_airbnb_revisited/airbnb_pricer/airbnb/skipped_columns.py", "anchor_text": "columns"}, {"url": "https://github.com/djsegal/metis/blob/master/X_airbnb_revisited/airbnb_pricer/airbnb/load_airbnb_links.py", "anchor_text": "load_airbnb_links.pyDownloads table of urls associated with Airbnb monthly rentals (in parallel)github.com"}, {"url": "https://github.com/djsegal/metis/blob/master/X_airbnb_revisited/airbnb_pricer/airbnb/compile_airbnb_data.py", "anchor_text": "github"}, {"url": "http://geopandas.org/", "anchor_text": "GeoPandas"}, {"url": "http://zipatlas.com/us/ny/new-york/zip-code-comparison/population-density.htm", "anchor_text": "1"}, {"url": "http://zipatlas.com/us/ny/brooklyn/zip-code-comparison/population-density.htm", "anchor_text": "2"}, {"url": "http://blog.davidkaleko.com/feature-engineering-cyclical-features.html", "anchor_text": "blog post"}, {"url": "https://gist.github.com/djsegal/3db97acec43ccf75d6313db4aa5af15c", "anchor_text": "github"}, {"url": "https://gist.github.com/djsegal/54d6047cd1495925517896f51d5dea2d", "anchor_text": "remove"}, {"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "L1 norm"}, {"url": "https://gist.github.com/djsegal/8ef31dfc8e3e861c363e858b359f71d9", "anchor_text": "LogTransformedTargetRegressor"}, {"url": "https://gist.github.com/djsegal/25ca2a2526b5d13bcc86686a2d8aba67", "anchor_text": "that"}, {"url": "https://towardsdatascience.com/clustering-the-us-population-observation-weighted-k-means-f4d58b370002", "anchor_text": "Carl Anderson"}, {"url": "https://gist.github.com/djsegal/12a0ed89558005f6aa64adf73799903f", "anchor_text": "way"}, {"url": "https://stackoverflow.com/a/39540339/5187080", "anchor_text": "stack overflow"}, {"url": "https://github.com/djsegal/metis/blob/master/X_airbnb_revisited/airbnb_pricer/utils/get_dist_to_clusters.py", "anchor_text": "get distance to clusters"}, {"url": "https://gist.github.com/djsegal/237d96c0eef26002c4aaf6c6ef957374", "anchor_text": "PassThroughTransformer"}, {"url": "https://scikit-learn.org/stable/modules/mixture.html", "anchor_text": "Bayesian Gaussian mixtures"}, {"url": "https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49", "anchor_text": "subway"}, {"url": "https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j", "anchor_text": "restaurant"}, {"url": "https://github.com/djsegal/metis/blob/54b84108a58d3e95679b519eb361e6916a693709/X_airbnb_revisited/airbnb_pricer/restaurant_data.py#L26-L86", "anchor_text": "restaurants"}, {"url": "https://github.com/djsegal/metis/blob/54b84108a58d3e95679b519eb361e6916a693709/X_airbnb_revisited/airbnb_pricer/subway/subway_data.py#L29-L67", "anchor_text": "subway stations"}, {"url": "https://gist.github.com/djsegal/637d46a5ac04cc05832a3793c4b0299a", "anchor_text": "method"}, {"url": "https://djsegal.github.io/pad_pricer/", "anchor_text": "Pad Pricer"}, {"url": "https://en.wikipedia.org/wiki/Attractor", "anchor_text": "Attractors"}, {"url": "https://github.com/djsegal/pad_pricer", "anchor_text": "javascript"}, {"url": "https://djsegal.github.io/pad_pricer/", "anchor_text": "Pad Pricer"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----59ef9b097478---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/machine-learning-pipeline?source=post_page-----59ef9b097478---------------machine_learning_pipeline-----------------", "anchor_text": "Machine Learning Pipeline"}, {"url": "https://medium.com/tag/airbnb?source=post_page-----59ef9b097478---------------airbnb-----------------", "anchor_text": "Airbnb"}, {"url": "https://medium.com/tag/regression?source=post_page-----59ef9b097478---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/tag/nyc?source=post_page-----59ef9b097478---------------nyc-----------------", "anchor_text": "NYC"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59ef9b097478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&user=Dan+Segal&userId=836515184818&source=-----59ef9b097478---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59ef9b097478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&user=Dan+Segal&userId=836515184818&source=-----59ef9b097478---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59ef9b097478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59ef9b097478--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F59ef9b097478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----59ef9b097478---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----59ef9b097478--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----59ef9b097478--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----59ef9b097478--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----59ef9b097478--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----59ef9b097478--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----59ef9b097478--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----59ef9b097478--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----59ef9b097478--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@djsegal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@djsegal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dan Segal"}, {"url": "https://medium.com/@djsegal/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "16 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F836515184818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&user=Dan+Segal&userId=836515184818&source=post_page-836515184818--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F836515184818%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-new-yorkers-guide-to-airbnb-pricing-59ef9b097478&user=Dan+Segal&userId=836515184818&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}