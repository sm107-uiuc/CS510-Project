{"url": "https://towardsdatascience.com/an-introduction-to-gradient-descent-c9cca5739307", "time": 1682996181.918349, "path": "towardsdatascience.com/an-introduction-to-gradient-descent-c9cca5739307/", "webpage": {"metadata": {"title": "An Introduction to Gradient Descent | by Yang S | Towards Data Science", "h1": "An Introduction to Gradient Descent", "description": "Gradient is a vector that is tangent of a function and points in the direction of greatest increase of this function. Gradient is zero at a local maximum or minimum because there is no single\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["This blog will cover following questions and topics:", "Gradient is a vector that is tangent of a function and points in the direction of greatest increase of this function. Gradient is zero at a local maximum or minimum because there is no single direction of increase. In mathematics, gradient is defined as partial derivative for every input variable of function. For example, we have a function:", "The figure of function is shown below and we can see that the minimum of function is (0, 0).", "In this case, x-component of gradient is the partial derivative respect to x and y-component of gradient is the partial derivative respect to y. The gradient of function above is:", "If we want to find the direction that increases the function at most at point (1, 2), we can plug (1, 2) into the formula above and get:", "As gradient is a vector pointing at the greatest increase of a function, negative gradient is a vector pointing at the greatest decrease of a function. Therefore, we can minimize a function by iteratively moving a little bit in the direction of negative gradient. That is the logic of gradient descent.", "We can construct an iterative procedure:", "The parameter alpha in the formula above is called learning rate, which is a small constant in most cases and ranges from 0 to 1. The iterative procedure will not stop until it converges. Taking previous example, we have already known that the gradient is:", "Therefore, iterative procedure of gradient descent can be written as:", "Finally, assuming alpha is smaller than 1, then we can get:", "The conclusion is same as what we observe in the figure above.", "Another intuitive way to explain gradient descent is that \u2018Consider the 3-dimensional graph below in the context of a function. Our goal is to move from the mountain in the top right corner to the dark blue sea in the bottom left. The arrows represent the direction of steepest descent (negative gradient) from any given point\u2013the direction that decreases the function as quickly as possible.\u2019 [3]", "In machine learning, we focus more on the relationship between cost function and parameters, instead of dependent and independent variables. The general idea of gradient descent in machine learning is to tweak parameters iteratively in order to minimize a cost function.", "In some cases, we can apply a closed-form equation to calculate the parameters directly that best fit the model to training dataset. For example, to minimize MSE for linear regression, parameters can be written as:", "However, in other cases we do not have closed-form equation, such as logistic regression. Therefore, an iterative optimization approach like gradient descent is applied.", "An important parameter in gradient descent is learning rate, which determine the size of each step. When learning rate is too big, gradient descent may jump across the valley and end up on the other side. This will lead to the cost function diverge. On the other hand, when learning rate is too small, it will take the algorithm long time to converge. Therefore, proper learning rate is needed before gradient descent starts.", "Normalization plays important role to Gradient Descent. If features are not normalized, features with large scale will dominate the update, so the algorithm will generate a zigzag learning path. It takes many unnecessary steps and longer time to reach to the minimum. After all features are normalized, the cost function is a more spherical shape. Gradient Descent algorithm goes straight towards minimum. One way to perform normalization is to minus mean and divide by standard deviation. You can also apply StandardScaler function in Scikit-Learn directly.", "We will look into several variants of gradient descent that are widely used in machine learning: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent.", "Batch Gradient Descent uses the whole batch of training data at every step. It calculates the error for each record and takes an average to determine the gradient. The advantage of Batch Gradient Descent is that the algorithm is more computational efficient and it produces a stable learning path, so it is easier to convergence. However, Batch Gradient Descent takes longer time when the training set is large.", "In the other extreme case, Stochastic Gradient Descent just picks one instance from training set at every step and update gradient only based on that single record. The advantage of Stochastic Gradient Descent is that the algorithm is much faster at every iteration, which remediate the limitation in Batch Gradient Descent. However, the algorithm produces less regular and stable learning path compared to Batch Gradient Descent. Instead of decreasing smoothly, the cost function will bounce up and down. After rounds of iterations, the algorithm may find a good parameter, but the final result is not necessary global optimal.", "Mini-Batch Gradient Descent combines the concept of Batch and Stochastic Gradient Descent. At each step, the algorithm compute gradient based on a subset of training set instead of full data set or only one record. The advantage of Mini-Batch Gradient Descent is that the algorithm can take advantage of matrix operation during calculation and cost function can decrease more smoothly and stable than Stochastic Gradient Descent.", "In this part, I will use famous data set iris to show how gradient decent works in logistic regression.", "Next, load data. Note that for simplicity, I only choose 2 kinds of iris.", "From figure above, we can see that three kinds of gradient descents generate similar linear decision boundry.", "In this post, you learned a lot about gradient descent. You now know the basic mathematics behind gradient and you have an understanding how the algorithm works behind the scenes. Second, you learned why learning rate and normalization are so important to the success of algorithm. Finally, you learned about the most common types of gradient descent and how to implement those algorithms in python. This knowledge enables you to better understand machine learning and deep learning. You can read more blogs by clicking on the following link:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc9cca5739307&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9cca5739307--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9cca5739307--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@songyangdetang_41589?source=post_page-----c9cca5739307--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@songyangdetang_41589?source=post_page-----c9cca5739307--------------------------------", "anchor_text": "Yang S"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9112dd0e1e2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&user=Yang+S&userId=9112dd0e1e2e&source=post_page-9112dd0e1e2e----c9cca5739307---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9cca5739307&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9cca5739307&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html"}, {"url": "https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/", "anchor_text": "https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/"}, {"url": "https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0", "anchor_text": "https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0"}, {"url": "https://www.jeremyjordan.me/batch-normalization/", "anchor_text": "https://www.jeremyjordan.me/batch-normalization/"}, {"url": "https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3", "anchor_text": "https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3"}, {"url": "https://medium.com/@songyangdetang_41589/table-of-contents-689c8af0c731", "anchor_text": "A Tour of Machine Learning and Deep LearningThis series of blogs will have an introduction on the deep learning from both theory and implementation aspect.medium.com"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9cca5739307&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&user=Yang+S&userId=9112dd0e1e2e&source=-----c9cca5739307---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9cca5739307&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&user=Yang+S&userId=9112dd0e1e2e&source=-----c9cca5739307---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9cca5739307&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9cca5739307--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc9cca5739307&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c9cca5739307---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9cca5739307--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c9cca5739307--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c9cca5739307--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c9cca5739307--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c9cca5739307--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c9cca5739307--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c9cca5739307--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c9cca5739307--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@songyangdetang_41589?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@songyangdetang_41589?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yang S"}, {"url": "https://medium.com/@songyangdetang_41589/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "197 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9112dd0e1e2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&user=Yang+S&userId=9112dd0e1e2e&source=post_page-9112dd0e1e2e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3f278c8a5b80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-c9cca5739307&newsletterV3=9112dd0e1e2e&newsletterV3Id=3f278c8a5b80&user=Yang+S&userId=9112dd0e1e2e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}