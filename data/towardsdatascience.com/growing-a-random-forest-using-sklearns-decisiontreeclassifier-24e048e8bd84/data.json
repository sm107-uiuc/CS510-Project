{"url": "https://towardsdatascience.com/growing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84", "time": 1683017327.311143, "path": "towardsdatascience.com/growing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84/", "webpage": {"metadata": {"title": "Growing a Random Forest using Sklearn\u2019s DecisionTreeClassifier | by Bernardo Garcia del Rio | Towards Data Science", "h1": "Growing a Random Forest using Sklearn\u2019s DecisionTreeClassifier", "description": "Random Forest is one of the most widely used machine learning algorithm based on ensemble learning methods. The principal ensemble learning methods are boosting and bagging. Random Forest is a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier", "paragraph_index": 4}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html?highlight=make_moons#sklearn.datasets.make_moons", "anchor_text": "Sklearn\u2019s make_moons", "paragraph_index": 8}, {"url": "https://scikit-learn.org/stable/modules/cross_validation.html", "anchor_text": "cross-validation", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Model_validation", "anchor_text": "model validation", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Statistics", "anchor_text": "statistical", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Generalization_error", "anchor_text": "generalize", "paragraph_index": 12}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearch#sklearn.model_selection.GridSearchCV", "anchor_text": "Sklearn\u2019s GridSearchCV", "paragraph_index": 23}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier", "paragraph_index": 28}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier", "paragraph_index": 30}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train%20test#sklearn.model_selection.train_test_split", "anchor_text": "Sklearn\u2019s train_test_split", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_(CART)", "anchor_text": "here", "paragraph_index": 36}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html?highlight=shuffle%20split#sklearn.model_selection.ShuffleSplit", "anchor_text": "Sklearn\u2019s ShuffleSplit", "paragraph_index": 43}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier", "anchor_text": "Sklearn\u2019s RandomForestClassifier", "paragraph_index": 52}, {"url": "https://www.linkedin.com/in/bernardo-garc%C3%ADa-del-r%C3%ADo-b4a988", "anchor_text": "https://www.linkedin.com/in/bernardo-garc%C3%ADa-del-r%C3%ADo-b4a988", "paragraph_index": 57}], "all_paragraphs": ["Random Forest is one of the most widely used machine learning algorithm based on ensemble learning methods.", "The principal ensemble learning methods are boosting and bagging. Random Forest is a bagging algorithm.", "In simple words, bagging algorithms create different smaller copies of the training set or subsets, train a model on each of these subsets and then combine the results of all the models to make predictions.", "These models trained on small subsets are called \u201cweak learners\u201d because are usually models that cannot fit complex data. The \u201cweak models\u201d that Random Forest uses are Decision Trees.", "Let\u2019s understand the basics of Decision Trees with an example using Sklearn\u2019s DecisionTreeClassifier before jumping into how to grow a forest.", "Decision Tree is a hierarchical graph representation of a dataset that can be used to make decisions.", "It is a non-parametric method as it does not assume any parameter or pre-defined shape of the tree that can be used either for classification and regression.", "Let\u2019s generate some synthetic data and build a Decision Tree to understand how it works.", "For this example, I have generated 10,000 data points and added Gaussian noise with a standard deviation of 0.4. The Sklearn\u2019s make_moons dataset is a toy dataset that comes handy to visualise classification algorithms. I have also imported all the libraries and classes that I will use later in this post.", "By plotting the data, we can see how make_moons class generates two interleaving half circles. This is 2D binary data so our classes are {0, 1}. Typical binary classification problems are fraud detection or spam detection.", "The corresponding Decision Tree for the training set is below:", "The first node, the one at the top, is called Root Node. In this node we give the algorithm all the training data available, in our example 8000 instances because we have put apart 2000 instances of the dataset to evaluate performance on data the model has never seen before. This is the most simple technique of cross-validation.", "Cross-validation is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.", "The algorithm finds the feature-threshold pair that maximizes the gain information and makes a split by evaluating if the value of the selected feature is less than / equal to or greater than the threshold. The threshold is the selected value in the feature that maximises the information gain.", "So what do we mean by information gain? First, we need to define entropy. Entropy is a measure of the uncertainty about a random variable. If all values of a random variable have the same probability, then we have the maximum entropy. If the random variable can take only one value, then the entropy takes its minimum value.", "For example, in a pure node, which is leaf node where all the instances were predicted the same class, the entropy would be 0.", "Information gain is a measure based on entropy. It measures the reduction in entropy when we split a subset according to the selected feature-threshold pair. When the algorithm chooses a feature-threshold pair that maximises the information gain, it is minimising the entropy.", "So going back to our Decision Tree, the selected feature is X1 and the threshold is 0.29. After the first split, we have two new nodes. 4252 instances have taken the left branch (True) and 3748 instances have taken the right branch (False).", "These two new nodes are leaf nodes until another split is made. Leaf nodes are nodes of a Decision Tree that do not have additional nodes coming off them so a decision about the class of the instances is made.", "In our example, if we look at the (blue) node that received the 4252 instances that took the left branch, the algorithm has found another feature-threshold pair that maximises the information gain and made another split. The selected feature is X0 and the threshold -0.466.", "The algorithm does the exercise of finding feature-threshold pairs that maximises information gain recursively until a certain tree depth is reached or the improvement in information gain is less than a given parameter (we will see in hyper-parameters fine-tuning).", "The maximum depth of the tree can be limited using the hyperparameter max_depth of Sklearn\u2019s DecisionTreeClassifier. We can also set the maximum leaf nodes (max_leaf_nodes), the minimum number of samples required to split an internal node (min_samples_split), etc.", "It could be possible to train a Decision Tree until there are not more feature-threshold pairs that could be evaluated. However, it is likely that we would end up overfitting model.", "After fine-tuning the hyperparameters using Sklearn\u2019s GridSearchCV, our tree has 18 leaves or leaf nodes because maximum number of leaf nodes was limited to this value. Fine-tuning the hyperparameters of a Decision Tree is like setting out constraints to the tree growth.", "If we look at the leaf at the bottom right corner, the class predicted for the 324 instances in this node is 0. The feature X0 takes a value greater than 0.511.", "We have been ignoring the term \u201cGini\u201d that appears in each node of the tree. \u201cGini\u201d stands for Gini impurity and it is a measure of the goodness of the split in each decision node.", "Decision nodes are nodes that are split by using If/Else conditions. They are also known as internal nodes or split nodes.", "By default, Sklearn\u2019s DecisionTreeClassifier uses the Gini impurity as a function to measure the quality of a split. Gini Impurity is the frequency with which a randomly chosen element in the dataset is incorrectly classified if it were randomly labeled according to the class distribution in the dataset.", "We can think Gini impurity as the equivalent of the information gain. I have used the information gain in the explanation as it seems more intuitive to me. Indeed, we can set the criterion to \u201centropy\u201d and Sklearn\u2019s DecisionTreeClassifier will compute the information gain to measure the goodness of the split.", "In practice, there is not real difference between using the Gini impurity or the information gain for 99% of the problems.", "Let\u2019s go through the code to build a Decision Tree using Sklearn\u2019s DecisionTreeClassifier:", "First of all, we split the dataset into training and test set using Sklearn\u2019s train_test_split.", "Then we need to choose some hyperparameters to run GridSearchCV. This is a bit of an iterative process and experience is also an important factor here.", "I chose some of them and run GridSearchCV several times, adjusting the parameters each time until I found an optimal combination. For example, if I started with max_leaf_nodes: [20 , 30, 40] and max_leaf_nodes was 20 in the best estimator, I would go back, set max_leaf_nodes = [15, 20, 25] and run GridSearchCV again.", "We have now an optimal Decision Tree so let\u2019s test it on data that the model has never seen before:", "The accuracy on the test set is 86.10%, slightly higher that the accuracy on the training set (85.97%). This is uncommon. Most of the time you will find that the performance on the training set is higher than on the test set. This finding might be just due to coincidence and it is probably due to the train-test split. If we repeated the process many times, we would usually see training accuracy higher than testing accuracy in the majority of the tests.", "Finally, it is worth mentioning that there are many algorithms that can build a Decision Tree, some more powerful than others. Sklearn uses an optimised version of the CART (Classification and Regression Trees) algorithm. You can learn more about CART here.", "Ok so now that we have grasped the essence of Decision Tree let\u2019s see how Random Forest leverages this hierarchical graph representation of a dataset to make better predictions.", "As mentioned at the beginning of the post, Random Forest uses ensemble learning methods to learn from data.", "It is a bagging algorithm that combines the predictions of many \u201cweak models\u201d trained on small subsets to make the final predictions.", "In simple words, the basic idea behind a Random Forest is that if a Decision Tree is good, many Decision Trees together should be better.", "A key factor is that \u201cweak learners\u201d trained on small subsets, Decision Trees in the case of Random Forests, must be slightly different and slightly better than a random guessing.", "We said that each \u201cweak learner\u201d is trained on a small subset of data. To create these subsets we use random sampling with replacement. Note we are introducing the first layer of randomness here.", "Sklearn\u2019s ShuffleSplit comes handy for this task. For our Random Forest, we are going to generate 1,000 subsets containing 100 instances of the training set. The code to carry out this task is below:", "Now, we train 1,000 Decision Trees, one for each subsets. We are growing our Forest. As these trees are trained on very small sets, we would expect them to be quite shallow and not too accurate.", "Note we are going to use best estimator obtained for the Decision Tree above so we can compare performance.", "We can check this by computing the average accuracy as below:", "The average accuracy of the \u201cweak Decision Trees\u201d is 81.39%, lower than 86.10% that was the accuracy of the Decision Tree trained on the 8000 samples.", "Let\u2019s grow our Forest and evaluate its performance.", "The testing accuracy achieved by the Random Forest is 86.85%, slightly higher than 86.10% of the Decision Tree.", "Ok we have slightly improved our predictions but not by much so..does that mean that Random Forests are not much better than a single Decision Tree?", "Well the truth is that this is a simplified example. Random Forests usually add a second layer of randomness by randomly limiting the features available at each split in the learning process. This avoids correlation between trees and results in better performance.", "You could train a Sklearn\u2019s RandomForestClassifier on this dataset and see if it outbeats our simple Random Forest. Please share your results!!", "As final note, Random Forests implicitly address the problem of overfitting because it reduces the final variance of the model by using multiple samples of the dataset.", "Hope you enjoyed this post and found it interesting. Please share your feedback for future articles.", "I would like to thank Aur\u00e9lien G\u00e9ron for suggesting this exercise in his book \u201cHands-On Machine Learning with Scikit-Learn, Keras and Tensorflow 2nd Edition\u201d. I cannot recommend more this book for people who wants to learn data science or need to brush up some basic concepts.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI / NLP Engineer building the future of writing at Flowrite. Connect with me on LinkedIn \u2014 https://www.linkedin.com/in/bernardo-garc%C3%ADa-del-r%C3%ADo-b4a988"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F24e048e8bd84&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://bernardogarciadelrio.medium.com/?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": ""}, {"url": "https://bernardogarciadelrio.medium.com/?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "Bernardo Garcia del Rio"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b5db1c122af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&user=Bernardo+Garcia+del+Rio&userId=9b5db1c122af&source=post_page-9b5db1c122af----24e048e8bd84---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24e048e8bd84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24e048e8bd84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@skamenar?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Steven Kamenar"}, {"url": "https://unsplash.com/s/photos/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html?highlight=make_moons#sklearn.datasets.make_moons", "anchor_text": "Sklearn\u2019s make_moons"}, {"url": "https://scikit-learn.org/stable/modules/cross_validation.html", "anchor_text": "cross-validation"}, {"url": "https://en.wikipedia.org/wiki/Model_validation", "anchor_text": "model validation"}, {"url": "https://en.wikipedia.org/wiki/Statistics", "anchor_text": "statistical"}, {"url": "https://en.wikipedia.org/wiki/Generalization_error", "anchor_text": "generalize"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearch#sklearn.model_selection.GridSearchCV", "anchor_text": "Sklearn\u2019s GridSearchCV"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "Sklearn\u2019s DecisionTreeClassifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train%20test#sklearn.model_selection.train_test_split", "anchor_text": "Sklearn\u2019s train_test_split"}, {"url": "https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_(CART)", "anchor_text": "here"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html?highlight=shuffle%20split#sklearn.model_selection.ShuffleSplit", "anchor_text": "Sklearn\u2019s ShuffleSplit"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier", "anchor_text": "Sklearn\u2019s RandomForestClassifier"}, {"url": "https://medium.com/tag/random-forest-classifiers?source=post_page-----24e048e8bd84---------------random_forest_classifiers-----------------", "anchor_text": "Random Forest Classifiers"}, {"url": "https://medium.com/tag/random-forest?source=post_page-----24e048e8bd84---------------random_forest-----------------", "anchor_text": "Random Forest"}, {"url": "https://medium.com/tag/decision-tree-classifier?source=post_page-----24e048e8bd84---------------decision_tree_classifier-----------------", "anchor_text": "Decision Tree Classifier"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----24e048e8bd84---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----24e048e8bd84---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24e048e8bd84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&user=Bernardo+Garcia+del+Rio&userId=9b5db1c122af&source=-----24e048e8bd84---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24e048e8bd84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&user=Bernardo+Garcia+del+Rio&userId=9b5db1c122af&source=-----24e048e8bd84---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24e048e8bd84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F24e048e8bd84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----24e048e8bd84---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----24e048e8bd84--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----24e048e8bd84--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----24e048e8bd84--------------------------------", "anchor_text": ""}, {"url": "https://bernardogarciadelrio.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://bernardogarciadelrio.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bernardo Garcia del Rio"}, {"url": "https://bernardogarciadelrio.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "42 Followers"}, {"url": "https://www.linkedin.com/in/bernardo-garc%C3%ADa-del-r%C3%ADo-b4a988", "anchor_text": "https://www.linkedin.com/in/bernardo-garc%C3%ADa-del-r%C3%ADo-b4a988"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b5db1c122af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&user=Bernardo+Garcia+del+Rio&userId=9b5db1c122af&source=post_page-9b5db1c122af--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1a973d390662&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgrowing-a-random-forest-using-sklearns-decisiontreeclassifier-24e048e8bd84&newsletterV3=9b5db1c122af&newsletterV3Id=1a973d390662&user=Bernardo+Garcia+del+Rio&userId=9b5db1c122af&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}