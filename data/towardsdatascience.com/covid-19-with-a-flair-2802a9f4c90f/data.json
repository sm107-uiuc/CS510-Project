{"url": "https://towardsdatascience.com/covid-19-with-a-flair-2802a9f4c90f", "time": 1683005019.289899, "path": "towardsdatascience.com/covid-19-with-a-flair-2802a9f4c90f/", "webpage": {"metadata": {"title": "COVID-19 with a Flair. Modelling the Coronavirus discussion\u2026 | by Marcell Ferencz | Towards Data Science", "h1": "COVID-19 with a Flair", "description": "Unless you\u2019ve been living under a rock that is lucky enough to be lying outside of the vast reach of COVID-19, you\u2019ll be aware that the virus is taking the world by storm. At the time of writing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.worldometers.info/coronavirus/", "anchor_text": "250,000 cases have been confirmed", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation", "anchor_text": "LDA", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Latent_semantic_analysis", "anchor_text": "LSA", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "ord embedding", "paragraph_index": 9}, {"url": "https://www.aclweb.org/anthology/D19-1445/", "anchor_text": "No one is sure why", "paragraph_index": 10}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "Jay Alammar\u2019s blog", "paragraph_index": 12}, {"url": "https://t.co/a5dx9skaa5", "anchor_text": "https://t.co/a5dx9skaa5", "paragraph_index": 19}, {"url": "https://github.com/flairNLP/flair", "anchor_text": "Flair", "paragraph_index": 22}, {"url": "https://research.zalando.com/", "anchor_text": "Zalando Research", "paragraph_index": 22}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html", "anchor_text": "Agglomerative Clustering", "paragraph_index": 28}, {"url": "https://en.m.wikipedia.org/wiki/Hierarchical_clustering", "anchor_text": "memory complexity of O(n\u00b2)", "paragraph_index": 28}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html", "anchor_text": "Principal Component Analysis", "paragraph_index": 28}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TfidfVectorizer()", "paragraph_index": 35}, {"url": "https://www.linkedin.com/in/marcell-ferencz-9892a3a6", "anchor_text": "LinkedIn", "paragraph_index": 50}], "all_paragraphs": ["Unless you\u2019ve been living under a rock that is lucky enough to be lying outside of the vast reach of COVID-19, you\u2019ll be aware that the virus is taking the world by storm. At the time of writing, some 250,000 cases have been confirmed, with the death count surpassing 10,000 people. Mass congregations are discouraged, shops and restaurants are closing, countries are shutting their borders and working from home is mandatory, rather than a privilege. It\u2019s a big deal.", "It accounts for virtually all discussion in the media, enjoying priority over such topics as the 2020 US presidential election or the UK finally leaving the EU for good in less than 9 months. People are flooding social media with COVID information, which can only mean one thing: data. Fresh data waiting to be analysed. And analyse it we will.", "Why analyse text data? What\u2019s the imaginary business case\u00a0here?", "In the age of social media when every individual has access to a platform on which to broadcast their views, it has never been easier to receive direct and instantaneous feedback from customers. Because people post their opinion online to be heard, organisations not only have a gift, but an obligation as well, to utilise this and extract actionable insight form submissions posted by their customer base.", "Social media data, however, is vast. Very vast. A medium sized organisation would be hard pressed to keep a tab on, understand, summarise and present all their customers\u2019 views, complaints and praises posted online; even if they hired an entire team to do so. And why would they, when they could use data science.", "We will attempt to uncover underlying topics in a snapshot of the Coronavirus discussion. To better understand how we\u2019ll achieve this, let me take you on a journey of topic modelling history. Please save your applause for the end.", "Picture it. It\u2019s the year 2003. Cristiano Ronaldo has just made his debut for Manchester United and Mike just proposed to Phoebe in Friends. Three computer scientists suggest using an algorithm that was previously pioneered in genetics, in the field of topic modelling. The algorithm in question is Latent Dirichlet Allocation, dubbed LDA because no one can pronounce the second word. The method uses a probabilistic approach to allocate documents into topics based on word co-occurrence. It\u2019s a landslide success and the model is widely adopted by the fledgling NLP community.", "Classical approaches to topic modelling, such as LDA or LSA, have been around for a while now. These are built on document-term matrix representations of text data and can work very effectively at relatively low cost. They do however lack the ability to capture any information about the position or order of words in our text, nor how similar they may be to each other.", "You blink\u200a\u2014\u200ait\u2019s now 2013, but you\u2019re still watching TV. Leo DiCaprio raises his glass with a wry smile for the first time on The Great Gatsby, and Miley Cyrus comes in like a wrecking ball. Bruno Mars is there. Meanwhile, Tomas Mikolov is experimenting with improving the Google Search Engine using shallow neural networks, and finds that he is able to map words to N-dimensional vector space which captures the meaning of words with respect to each other in numerical positions. A light bulb pops into existence over his head, and he publishes the word2vec\u00a0model, unleashing with it upon the world the plague of the word2word nomenclature (see doc2vec, node2vec, seq2seq, graph2vec\u2026).", "Word embedding approaches are a major step in numerical text representation. This family of techniques maps words in a corpus to vector space, most commonly using neural networks (e.g. word2vec or GloVe). One major benefit word embedding brings over the aforementioned probabilistic models is their ability to represent similarity between words, given context in the training data, as proximity in vector space; for example, cat and kitten will be much closer to each other than cat and carpentry. The approach has been widely used for years, but is still limited in that it can only map words to a single vector, unable to capture different meanings for the same word in various contexts.", "The picture goes blurry again. When your vision clears, you notice that the date on your phone reads 11 October 2018. Corona means nothing more than a brand of beer to you. Google are still working tirelessly on maintaining their status as the Search Engine, and this time it\u2019s Jacob Devlin to the rescue. He publishes a paper on using a Transformer-Encoder deep learning algorithm for text prediction tasks. He names it Bidirectional Encoder Representations from Transformers, which luckily abbreviates to BERT. It works well. Really well. No one is sure why.", "BERT manages to one-up word embedding approaches by not only blowing them out of the park on many downstream NLP tasks, but more importantly for us, it can assign different vector representations for the same word in different context. This is especially important for homonyms: the word address has an entirely different meaning when I give someone my address and if I address someone, for example.", "BERT has been covered on this site many times before, but if you haven\u2019t come across it yet, I recommend taking a detour to read up about it. Jay Alammar\u2019s blog provides a really good summary of it, and his dedication to illustrate the models using Sesame Street characters is formidable. Do come back though, because it\u2019s about to get interesting.", "We\u2019ve now seen how far text representation has come (it has since gone further), so let\u2019s put it to the test.", "We will be using a sample of Tweets posted on the subject of the Coronavirus outbreak. We will then use BERT to represent these in vector space, using the average of their word embedding values. We can then postulate that, if words of similar meaning are closer to each other in vector space, we can group nearby Tweets together to find clusters of common topics. Our high level workflow will be:", "All with the ultimate aim of answering the question: What are people talking about on Twitter in relation to the Coronavirus?", "In this section I will run through my approach and share some of my code. You may skip this part if you\u2019re only interested in the destination, not the journey.", "I used the Twitter Search API to find Tweets on the 11th of March containing the words \u2018COVID\u2019 or \u2018Coronavirus\u2019, which netted me 17,998 Tweets in the English language from around the globe.", "Let\u2019s take a look at an example:", "\u201c@RepKinzinger I know you have a lot of incompetence to \u201covercome,\u201d And you\u2019ve been so \u201cincredibly vigilant\u201d in your Coronavirus monitoring. But would you tiki torch trumpists care to explain what agreements and acquiescence your party made to your \u201cfriends\u201d in Moscow? https://t.co/a5dx9skaa5\u201d", "We run the data through a series of pre-processing steps; I won\u2019t go into detail as this topic has been covered many times elsewhere. I turned everything to lower case, removed hyperlinks, mentions, non-alphanumerical characters and newlines, removed stopwords and lemmatised\u00a0the\u00a0rest. The above text now looks like this:", "\u201cknow lot incompetence overcome incredibly vigilant coronavirus monitoring would tiki torch trumpists care explain agreements acquiescence party make friends moscow\u201d", "We will use the Flair Python library, a framework developed by Zalando Research built on PyTorch, to embed our Tweets using a combination of pre-trained word embedding models.", "Note: I used Google Colab to embed the Tweets, which took round about 30 mintes to do. Your mileage may vary, but if, like me, you don\u2019t have a particularly powerful machine, I\u2019d recommend making use of that free GPU access.", "We\u2019ll\u00a0initialise\u00a0the\u00a0word\u00a0embedding\u00a0models:", "This will give us a tensor of size (1, 7168) for each Tweet, so we\u2019ll initialise an empty tensor of size (17998, 7168) and iteratively fill it with our\u00a0document\u00a0vectors:", "This will take some time, so grab a drink. Maybe do the dishes for once.", "We now have a tensor with (17998, 7168) dimensions\u00a0populated\u00a0with\u00a0embeddings\u00a0for\u00a0each\u00a0Tweet. We are done with PyTorch at this point, so we\u2019ll detach the tensor from the GPU and convert it to a NumPy array:", "We want to cluster these vectors into topics, and we\u2019ll invoke Agglomerative Clustering with Ward affinity from scikit-learn to do so. Bottom-up hierarchical clustering algorithms have a memory complexity of O(n\u00b2), so we\u2019ll use Principal Component Analysis to speed up this process. After all, we just finished watching a progress bar for 30 minutes.", "As a side note, I did test a number of clustering algorithms (K-means, BIRCH, DBSCAN, Agglomerative with complete/average affinity), but Ward seems to perform the best in most cases. I attribute this to its ability to identify smaller fringe clusters, and does not seem to be hell-bent on splitting my data points into equal-sized groups, so it\u2019s good for picking out underlying topics which do not necessarily correspond to the main discussion.", "Let\u2019s reduce the dimensionality of our vectors to length 768 \u2014 I picked this number somewhat arbitrarily, but BERT on its own produces vectors of this size, so it should be good enough for us, while also reducing the data size by something like 80%.", "We\u2019ll initialise the algorithm with 10 clusters, fit our data and allocate the cluster labels to our main DataFrame:", "This yields the following topic distribution:", "We can see the benefits of our choice of clustering algorithm in action. Major topics, such as 0 and 3 were picked up, but we managed to separate some fringe discussions like 5 and 8.", "We can visualise the topic clusters in 2-D:", "We have now allocated each of our Tweets to a topic, but how do we make sense of them? We will find the words and phrases (uni- and bi-grams) in each topic with the highest TF-IDF scores; that is, we will identify the terms which appear a lot in one topic but don\u2019t appear a lot in other topics. To do this, we\u2019ll use scikit-learn\u2019s TfidfVectorizer()\u00a0in\u00a0a\u00a0custom\u00a0function. Because we are dealing with a few large documents (treating each topic as its own document), we\u2019ll limit our Document-Frequency to 50%, ensuring that the terms extracted do not appear in over half\u00a0of the\u00a0total. This step helps exclude very common words (like Coronavirus), which wouldn\u2019t be very helpful in identifying the topic.", "We group our Tweets into their allocated topics to form long documents, then apply the above function to them to find the 10 most important terms in each topic:", "We\u2019ll visualise the results: each chart represents a topic and its 10 most important terms. The longer the bar, the more representative the term:", "We can spot some topics around the economic stimulus package (topic 0), virus testing (topic 3) and sports (topic 9). We will do some more digging into the others later on.", "How good are our topics? This is a non-trivial question as we are using unsupervised techniques on live data (we don\u2019t have any training sets). All we can do is compare them to each other. We\u2019ll postulate that \u2018good\u2019 topics are more compact in vector space, i.e. their document vectors are closer to each other, than in bad ones. To assess this, we will look at how close each Twitter vector in its respective topic is to the centroid of the topic vectors.", "We find the centroids of the vectors by averaging them across each topic:", "We then calculate the euclidean distance of each Tweet vector to their respective topic centroid:", "We can visualise the distribution of distances to the topic centroid:", "The closer the distribution to the left of the graph, the more compact the topic is. Topics 3, 4, 6, 7 and 8 seem to be strong contenders; 8 is woefully spread out, indicating a lack of consistent content.", "We looked at how similar Tweets are within each topic, but we can also look at how similar the topics are to each other. We will construct a euclidean distance matrix between the 10 topic centroids to find the distance between the topic averages. The closer the averages, the more overlap we\u2019d expect between the topics.", "The distance matrix shows the distance across all the topics. The darker the colour (and lower the number) of the cell, the closer the topics corresponding to its row and column are. Topics 3 and 7 or 0 and 2 are quite close together; Topics 1 and 4 very far from each other; topic 8, the black sheep in the family, is as far away from everyone as the others combined.", "Thanks to those who stuck it out with me during the previous section\u200a\u2014\u200ait was an arduous task, but we have uncovered some useful\u00a0information, so let\u2019s recap on them.", "The top terms provide some much needed context to the topics, allowing us to make very reasonable guesses as to what is (broadly) being discussed in each:", "We have demonstrated the effectiveness of a lesser taken path to topic modelling using state of the art language models by extracting some coherent topics from a large collection of real data. Our approach also allowed us to evaluate our topics\u2019 relation to each other, which seem to coincide with our interpretation of them. I\u2019d chalk this one up as a success.", "Did I do something wrong? Could I have done something better? Did I do something well?", "Please don\u2019t hesitate to reach out to me\u00a0on\u00a0LinkedIn; I\u2019m always happy to be challenged or just have a chat if you\u2019re interested in my work.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2802a9f4c90f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@m.ferencz?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.ferencz?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "Marcell Ferencz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe39ae6af99a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&user=Marcell+Ferencz&userId=e39ae6af99a5&source=post_page-e39ae6af99a5----2802a9f4c90f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2802a9f4c90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2802a9f4c90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/gHrEvF7Ugb4", "anchor_text": "Unsplash"}, {"url": "https://www.worldometers.info/coronavirus/", "anchor_text": "250,000 cases have been confirmed"}, {"url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation", "anchor_text": "LDA"}, {"url": "https://en.wikipedia.org/wiki/Latent_semantic_analysis", "anchor_text": "LSA"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "ord embedding"}, {"url": "https://www.aclweb.org/anthology/D19-1445/", "anchor_text": "No one is sure why"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "Jay Alammar\u2019s blog"}, {"url": "https://t.co/a5dx9skaa5", "anchor_text": "https://t.co/a5dx9skaa5"}, {"url": "https://github.com/flairNLP/flair", "anchor_text": "Flair"}, {"url": "https://research.zalando.com/", "anchor_text": "Zalando Research"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html", "anchor_text": "Agglomerative Clustering"}, {"url": "https://en.m.wikipedia.org/wiki/Hierarchical_clustering", "anchor_text": "memory complexity of O(n\u00b2)"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html", "anchor_text": "Principal Component Analysis"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TfidfVectorizer()"}, {"url": "https://www.nytimes.com/2020/03/13/us/politics/trump-coronavirus-relief-congress.html", "anchor_text": "Trump\u2019s Coronavirus stimulus package"}, {"url": "https://www.nytimes.com/interactive/2020/03/11/science/how-coronavirus-hijacks-your-cells.html", "anchor_text": "COVID \u2018hijacks your cells\u2019"}, {"url": "https://www.breitbart.com/politics/2020/03/11/dr-fauci-coronavirus-in-u-s-would-be-worse-without-trumps-travel-bans/?utm_source=dlvr.it&utm_medium=twitter", "anchor_text": "Trump\u2019s travel bans"}, {"url": "https://www.npr.org/sections/health-shots/2020/03/13/815363944/trump-administration-announces-measures-to-speed-coronavirus-testing?t=1584307194625", "anchor_text": "Coronavirus testing in the US"}, {"url": "https://www.nbcnews.com/news/world/coronavirus-italy-deepens-lockdown-covid-19-spreads-n1156351", "anchor_text": "Italy\u2019s lockdown due to the outbreak"}, {"url": "https://www.searchenginejournal.com/twitter-suggests-appropriate-ways-for-brands-to-tweet-about-covid-19/354550/", "anchor_text": "how to post about Coronavirus"}, {"url": "https://www.mirror.co.uk/sport/football/news/coronavirus-england-vs-italy-friendly-21678827", "anchor_text": "who reportedly tested positive for the virus"}, {"url": "https://www.linkedin.com/in/marcell-ferencz-9892a3a6", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/coronavirus?source=post_page-----2802a9f4c90f---------------coronavirus-----------------", "anchor_text": "Coronavirus"}, {"url": "https://medium.com/tag/bert?source=post_page-----2802a9f4c90f---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/topic-modeling?source=post_page-----2802a9f4c90f---------------topic_modeling-----------------", "anchor_text": "Topic Modeling"}, {"url": "https://medium.com/tag/natural-language-process?source=post_page-----2802a9f4c90f---------------natural_language_process-----------------", "anchor_text": "Natural Language Process"}, {"url": "https://medium.com/tag/embedding?source=post_page-----2802a9f4c90f---------------embedding-----------------", "anchor_text": "Embedding"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2802a9f4c90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&user=Marcell+Ferencz&userId=e39ae6af99a5&source=-----2802a9f4c90f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2802a9f4c90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&user=Marcell+Ferencz&userId=e39ae6af99a5&source=-----2802a9f4c90f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2802a9f4c90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2802a9f4c90f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2802a9f4c90f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2802a9f4c90f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.ferencz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.ferencz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marcell Ferencz"}, {"url": "https://medium.com/@m.ferencz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "44 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe39ae6af99a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&user=Marcell+Ferencz&userId=e39ae6af99a5&source=post_page-e39ae6af99a5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe39ae6af99a5%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcovid-19-with-a-flair-2802a9f4c90f&user=Marcell+Ferencz&userId=e39ae6af99a5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}