{"url": "https://towardsdatascience.com/knowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d", "time": 1683009693.873773, "path": "towardsdatascience.com/knowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d/", "webpage": {"metadata": {"title": "Knowing known unknowns with deep neural networks | by Jacob Reinhold | Towards Data Science", "h1": "Knowing known unknowns with deep neural networks", "description": "Deep neural networks (DNNs) are easy-to-implement, versatile machine learning models that can achieve state-of-the-art performance in many domains (for example, computer vision, natural language\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.deeplearningbook.org/", "anchor_text": "Deep neural networks", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/2004.08955v1.pdf", "anchor_text": "computer vision", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/2005.14165", "anchor_text": "natural language processing", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/2005.09629v1.pdf", "anchor_text": "speech recognition", "paragraph_index": 0}, {"url": "https://dl.acm.org/doi/abs/10.1145/3285029", "anchor_text": "recommendation systems", "paragraph_index": 0}, {"url": "https://www.economist.com/technology-quarterly/2020/06/11/an-understanding-of-ais-limitations-is-starting-to-sink-in", "anchor_text": "number", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1604.00289", "anchor_text": "of", "paragraph_index": 0}, {"url": "https://www.aaai.org/ojs/index.php/aimagazine/article/view/2756", "anchor_text": "articles", "paragraph_index": 0}, {"url": "https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7", "anchor_text": "blog posts", "paragraph_index": 0}, {"url": "http://rebooting.ai/", "anchor_text": "books", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=GiPe1OiKQuk", "anchor_text": "Rumsfeldian", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1706.04599.pdf", "anchor_text": "the softmax outputs are rarely close to 0.5", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "anchor_text": "entropy", "paragraph_index": 1}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/cddf290b1d3722b0c88bbc3c82df38a3", "anchor_text": "toy regression example", "paragraph_index": 6}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/745385337944dbcc98b47578b5a769f9", "anchor_text": "toy classification example", "paragraph_index": 6}, {"url": "https://dictionary.cambridge.org/dictionary/english/uncertainty", "anchor_text": "defined by the Cambridge Dictionary", "paragraph_index": 7}, {"url": "https://www.goodreads.com/book/show/1446901.The_Emergence_of_Probability", "anchor_text": "The Emergence of Probability", "paragraph_index": 12}, {"url": "https://plato.stanford.edu/entries/probability-interpret/", "anchor_text": "What does probability mean", "paragraph_index": 12}, {"url": "https://www.sciencedirect.com/science/article/pii/S0167473008000556?casa_token=yWEYH7OP70sAAAAA:FnmF4crmHui1-NCqSx99tkamdl-ITjwAQ7TSlxlHFTpoYp_9xPtAVqtw0CH4ByYr7-gRC71c5Q", "anchor_text": "Aleatory or Epistemic? Does it matter?", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Autodromo_Nazionale_di_Monza", "anchor_text": "Monza racing track", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1505.05424", "anchor_text": "cornucopia", "paragraph_index": 17}, {"url": "http://proceedings.mlr.press/v37/hernandez-lobatoc15.pdf", "anchor_text": "proposed", "paragraph_index": 17}, {"url": "http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf", "anchor_text": "methods", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Discriminative_model", "anchor_text": "discriminative", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1506.02142", "anchor_text": "Yarin Gal and Zoubin Ghahramani", "paragraph_index": 19}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "dropout", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1703.04977", "anchor_text": "estimated variance", "paragraph_index": 21}, {"url": "https://ieeexplore.ieee.org/abstract/document/374138", "anchor_text": "of the output", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Laplace_distribution", "anchor_text": "Laplace", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability#Definition", "anchor_text": "likelihood", "paragraph_index": 24}, {"url": "https://alexgkendall.com/media/papers/alex_kendall_phd_thesis_compressed.pdf", "anchor_text": "Pg. 41 of Alex Kendall\u2019s thesis", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1808.01200", "anchor_text": "entropy", "paragraph_index": 30}, {"url": "https://pubmed.ncbi.nlm.nih.gov/29259224/", "anchor_text": "sample variance", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1703.02910", "anchor_text": "mutual information", "paragraph_index": 30}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/cddf290b1d3722b0c88bbc3c82df38a3", "anchor_text": "regression notebook", "paragraph_index": 32}, {"url": "https://ieeexplore.ieee.org/abstract/document/7797130", "anchor_text": "better performance", "paragraph_index": 35}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/745385337944dbcc98b47578b5a769f9", "anchor_text": "classification notebook", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/2002.04626", "anchor_text": "pre-print here", "paragraph_index": 41}, {"url": "https://phillipi.github.io/pix2pix/", "anchor_text": "pix2pix", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1505.04597", "anchor_text": "U-Net", "paragraph_index": 41}, {"url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tompson_Efficient_Object_Localization_2015_CVPR_paper.pdf", "anchor_text": "spatial dropout", "paragraph_index": 41}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2", "anchor_text": "here", "paragraph_index": 41}, {"url": "http://jcreinhold.com", "anchor_text": "jcreinhold.com", "paragraph_index": 46}], "all_paragraphs": ["Deep neural networks (DNNs) are easy-to-implement, versatile machine learning models that can achieve state-of-the-art performance in many domains (for example, computer vision, natural language processing, speech recognition, recommendation systems). DNNs, however, are not perfect. You can read any number of articles, blog posts, and books discussing the various problems with supervised deep learning. In this article we\u2019ll focus on a (relatively) narrow but major issue: the inability for a standard DNN to reliably show when it is uncertain about a prediction. For a Rumsfeldian take on it: The inability of DNNs to know \u201cknown unknowns.\u201d", "As a simple example of this failure mode in DNNs, consider training a DNN for a binary classification task. You might reasonably presume that the softmax (or sigmoid) output of a DNN could be used to measure how certain or uncertain the DNN is in its prediction; you would expect that seeing a softmax output close to 0 or 1 would indicate certainty, and an output close to 0.5 would indicate uncertainty. In reality, the softmax outputs are rarely close to 0.5 and are, more frequently than not, close to 0 or 1 regardless of whether the DNN is making a correct prediction. Unfortunately, this fact makes naive uncertainty estimates unreliable (for instance, entropy over the softmax outputs).", "To be fair, uncertainty estimates are not needed for every application of a DNN. If a social media company uses a DNN to detect faces in images so that its users can more easily tag their friends, and the DNN fails, then the failure of the method is nearly inconsequential. A user might be slightly inconvenienced, but in low-stakes environments like social media or advertising, uncertainty estimates aren\u2019t vital to creating value from a DNN.", "In high-stakes environments, however, like self-driving cars, health care, or military applications, a measure of how uncertain the DNN is in its prediction could be vital. Uncertainty measurements can reduce the risk of deploying a model because they can alert a user to the fact that a scenario is either inherently difficult to do prediction in, or the scenario has not been seen by the model before.", "In a self-driving car, it seems plausible that a DNN should be more uncertain about predictions at night (at least in the measurements coming from optical cameras) because of the lower signal-to-noise ratio. In health care, a DNN that diagnoses skin cancer should be more uncertain if it were shown a particularly blurry image, especially if the model had not seen such blurry examples in the training set. In a model to segment satellite imagery, a DNN should be more uncertain if an adversary changed how they disguise certain military installations. If the uncertainty inherent in these situations were relayed to the user, the information could be used to change the behavior of the system in a safer way.", "In this article, we explore how to estimate two types of statistical uncertainty alongside a prediction in a DNN. We first discuss the definition of both types of uncertainty, and then we highlight one popular and easy-to-implement technique to estimate these types of uncertainty. Finally, we show and implement some examples for both classification and regression that makes use of these uncertainty estimates.", "For those who are most interested in looking at code examples, here are two Jupyter Notebooks one with a toy regression example and the other with a toy classification example. There are also PyTorch-based code snippets in the \u201cExamples and Applications\u201d section below.", "Uncertainty is defined by the Cambridge Dictionary as: \u201ca situation in which something is not known.\u201d There are several reasons why something may not be known, and \u2014 taking a statistical perspective \u2014 we will discuss two types of uncertainty called aleatory (sometimes referred to as aleatoric) and epistemic uncertainty.", "Aleatory uncertainty relates to an objective or physical concept of uncertainty \u2014 it is a type of uncertainty that is intrinsic to the data-generating process. Since aleatory uncertainty has to do with an intrinsic quality of the data, we presume it cannot be decreased by collecting more data; that is, it is irreducible.", "Aleatory uncertainty can be explained best with a simple example: Suppose we have a coin which has some positive probability of being heads or tails. Then, even if the coin is biased, we cannot predict \u2014 with certainty \u2014 what the next toss will be, regardless of how many observations we make. (For instance, if the coin is biased such that heads turn up with probability 0.9, we might reasonably guess that heads will show up in the next toss, but we cannot be certain that it will happen.)", "Epistemic uncertainty relates to a subjective or personal concept of uncertainty \u2014 it is a type of uncertainty due to knowledge or ignorance of the true data-generating process. Since this type of uncertainty has to do with knowledge, we presume that it can be decreased (for example, when more data has been collected and used for training); that is, it is reducible.", "Epistemic uncertainty can be explained with a regression example. Suppose we are fitting a linear regression model and we have independent variables x between -1 and 1, and corresponding dependent variables y for all x. Suppose we chose a linear model because we believe that when x is between -1 and 1, the model is linear. We don\u2019t, however, know what happens when a test sample x* is far outside this range; say at x* = 100. So, in this scenario, there is uncertainty about the model specification (for example, the true function may be quadratic) and there is uncertainty because the model hasn\u2019t seen data in the range of the test sample. These uncertainties can be bundled into uncertainty regarding the knowledge of true data-generating distribution, which is epistemic uncertainty.", "The terms aleatory and epistemic, with regards to probability and uncertainty, seem to have been brought into the modern lexicon by Ian Hacking in his book \u201cThe Emergence of Probability,\u201d which discusses the history of probability from 1600\u20131750. The terms are not clear for the uninitiated reader, but their definitions are related to the deepest question in the foundations of probability and statistics: What does probability mean? If you are familiar with terms frequentist and Bayesian, then you will see the respective relationship between aleatory (objective) and epistemic (subjective) uncertainty. I\u2019m not about to solve this philosophical issue in this blog post, but know that the definitions of aleatory and epistemic uncertainty are nuanced, and what falls into which category is debatable. For a more comprehensive (but still applied) review of these terms, take a look at the article: \u201cAleatory or Epistemic? Does it matter?\u201d", "Why is it important to distinguish between aleatory and epistemic uncertainty? Suppose we are developing a self-driving car, and we take a prototype that was trained on normal roads and have it drive through the Monza racing track, which has extremely banked turns.", "Since the car hasn\u2019t seen the situation before, we would expect the image segmentation DNN in the self-driving car, for example, to be uncertain because it has never seen the sky nearly to the left of ground. In this case, the uncertainty would be classified as epistemic because the DNN doesn\u2019t have knowledge of roads like this.", "Suppose instead that we take the same self-driving car and take it for a drive on a rainy day; assume that the DNN has been trained on lots of rainy-day conditions. In this situation, there is more uncertainty about objects on the road simply due to lower visibility. In this case, the uncertainty would be classified as aleatory because there is inherently more randomness in the data.", "These two situations should be dealt with differently. In the race track, the uncertainty could tell the developers that they need to gather a particular type of training data to make the model more robust, or the uncertainty could tell the car could try to safely maneuver to a location where it can hand-off control to the driver. In the rainy-day situation, the uncertainty could alert the system to simply slow down or enable certain safety features.", "There has been a cornucopia of proposed methods to estimate uncertainty in DNNS in recent years. Generally, uncertainty estimation is formulated in the context of Bayesian statistics. In a standard DNN for classification, we are implicitly training a discriminative model where we obtain maximum-likelihood estimates of the neural network weights (depending on the loss function chosen to train the network). This point-estimate of the network weights is not amenable to understanding what the model knows and does not know. If we instead find a distribution over the weights, as opposed to the point-estimate, we can sample network weights with which we can compute corresponding outputs.", "Intuitively, this sampling of network weights is like creating an ensemble of networks to do the task: We sample a set of \u201cexperts\u201d to make a prediction. If the experts are inconsistent, there is high epistemic uncertainty. If the experts think it is too difficult to make an accurate prediction, there is high aleatory uncertainty.", "In this article, we\u2019ll take a look at a popular and easy-to-implement method to estimate uncertainty in DNNs by Yarin Gal and Zoubin Ghahramani. They showed that dropout can be used to learn an approximate distribution over the weights of a DNN (as previously discussed). Then, during prediction, dropout is used to sample weights from this fitted approximate distribution \u2014 akin to creating the ensemble of experts.", "Epistemic uncertainty is estimated by taking the sample variance of the predictions from the sampled weights. The intuition behind relating sample variance to epistemic uncertainty is that the sample variance will be low when the model predicts nearly identical outputs, and it will be high when the model makes inconsistent predictions; this is akin to when the set of experts consistently makes a prediction and when they do not, respectively.", "Simultaneously, aleatory uncertainty is estimated by modifying a DNN to have a second output, as well as using a modified loss function. Aleatory uncertainty will correspond to the estimated variance of the output. This predicted variance has to do with an intrinsic quantity of the data, which is why it is related to aleatory uncertainty; this is akin to when the set of experts judges the situation too difficult to make a prediction.", "Altogether the final network structure is something like what is shown in Fig. 2. There is an input x which is fed to a DNN with dropout after every layer (dropout after every layer is what is originally specified, but \u2014 in practice \u2014 dropout after every layer often makes training too difficult). The output of this DNN is an estimated target \u0177 and an estimated variance or scale parameter \u03c3\u0302.", "This DNN is trained with a loss function like:", "If the network is being trained for a regression task. The first loss function shown above is an MSE variant with uncertainty, whereas the second is an L1 variant. These are derived from assuming a Gaussian and Laplace distribution for the likelihood, respectively, where each component is independent and the variance (or scale parameter) is estimated and fitted by the network.", "As mentioned above, these loss functions have mathematical derivations, but we can intuit why this variance parameter captures a type of uncertainty: The variance parameter provides a trade-off between the variance and the MSE or L1 loss term. If the DNN can easily estimate the true value of the target (that is, get \u0177 close to the true y), then the DNN should estimate a low variance term on that so as to minimize the loss. If, however, the DNN cannot estimate the true value of the target (for example, there is low signal-to-noise ratio), then the network can minimize the loss by estimating a high variance. This will reduce the MSE or L1 loss term because that term will be divided by the variance; however, the network should not always do this because of the log variance term which penalizes high variance estimates.", "If the network is being trained for a classification (or segmentation) task, the loss would look something like this two-part loss function:", "The intuition here with this loss function is: When the DNN can easily estimate the right class of a component, the value \u0177 will be high for that class and the DNN should estimate a low variance so as to minimize the added noise (so that all samples will be concentrated around the correct class). If, however, the DNN cannot easily estimate the class of the component, the \u0177 value should be low and adding noise can increase the guess, by chance, for the correct class which can overall minimize the loss function. (See Pg. 41 of Alex Kendall\u2019s thesis for more discussion on this loss function.)", "Finally, in testing, the network is sampled T times to create T estimated targets and T estimated variance outputs. These T outputs are then combined in various ways to make the final estimated target and uncertainty estimates as shown in Fig. 3.", "Mathematically, the epistemic and aleatory uncertainty are (for the MSE regression variant):", "There are various interpretations of epistemic uncertainty for the classification case: entropy, sample variance, mutual information. Each has been shown to be useful in its own right, and the choice of what type to choose will be application dependent.", "To make the theory more concrete, we\u2019ll go through two toy examples for estimating uncertainty with DNNs in a regression and classification task with PyTorch. The code below are excerpts from full implementations which are available in Jupyter notebooks (mentioned at the beginning of the next two subsections). Finally, we\u2019ll discuss calculating uncertainty in a real-world data example with medical images.", "In the regression notebook, we fit a very simple neural network \u2014 consisting of two fully-connected layers with dropout on the hidden layer \u2014 to one-dimensional input and output data with the MSE variant of the uncertainty loss (implemented below).", "Note that instead of fitting the variance term directly, we fit the log of the variance term for numerical stability.", "In the regression scenario, we could also use the L1 variant of the uncertainty loss which is in the notebook and implemented below.", "Sometimes using L1 loss instead of MSE loss results in better performance for regression tasks, although this is application dependent.", "The aleatory and epistemic uncertainty estimates in this scenario are then computed as in the implementation below (see the notebook for more context).", "In Fig. 4, we visualize the fit function and the uncertainty results. In the plot to the far right, we show the thresholded epistemic uncertainty which demonstrates the capabilities of uncertainty estimates to detect out-of-distribution data (at least in this toy scenario).", "In the classification notebook, we, again, fit a neural network composed of two fully-connected layers with dropout on the hidden layer. In this case, we are trying to do binary classification. Consequently, the loss function is as implemented below.", "There are numerous uncertainty estimates we could compute in this scenario. In the below implementation, we calculate epistemic, entropy, and aleatory uncertainty. Entropy could reasonably be argued to belong to one of aleatory and epistemic uncertainty, but below it is separated out so that aleatory and epistemic uncertainty are calculated as previously described.", "In Fig. 5, we visualize the resulting epistemic and aleatory uncertainty, as well as entropy, over the training data. As we can see the training data classes overlaps near zero, and the uncertainty measures peak there. In this toy example, all three measures of uncertainty are highly correlated. Discussion as to why is provided in the notebook for the interested reader.", "In this last example, I\u2019ll show some results and applications of uncertainty in a real-world example published as a conference paper (pre-print here). The task explored is an image-to-image translation task, akin to the notable pix2pix example, but with medical images. In this case, we wanted to make a computed tomography (CT) image of the brain look like the corresponding magnetic resonance (MR) image of the brain. This is a regression loss and we used the MSE variant of the uncertainty loss to train a U-Net modified to have spatial dropout (see here for a discussion as to why spatial dropout) after every layer, and to output two images instead of only one; one output is the estimated MR image and the other is the pixel-wise variance.", "Example inputs and outputs are shown in Fig. 6. The CT image on the far left has an anomaly in the left hemisphere of the occipital lobe (lower-left of the brain in the image; it is more easily visualized in the corresponding MR image to the right). The DNN was only trained on healthy images, so the DNN should be ignorant of such anomalous data, and it should reflect this \u2014 according to the theory of epistemic uncertainty as previously discussed \u2014 by having high sample variance (that is, high epistemic uncertainty) in that region.", "When this image was input to the network, we calculated the epistemic and aleatory uncertainty. The anomaly is clearly highlighted in the epistemic uncertainty, but there are many other regions which are also predicted to have high epistemic uncertainty. If we take the pixel-wise ratio of epistemic and aleatory uncertainty, we get the image shown on the far-right, labeled \u201cScibilic\u201d (which is discussed more in the pre-print). This image is easily thresholded to predict the anomaly (the out-of-distribution region of the image).", "This method of anomaly detection is by no means foolproof. It is quite fickle actually, but it shows a way to apply this type of uncertainty estimation for real-world data.", "Uncertainty estimates in machine learning have the potential to reduce the risk of deploying models in high-stakes scenarios. Aleatory and epistemic uncertainty estimates can show the user or developer different information about the performance of a DNN and can be used to modify the system for better safety. We discussed and implemented one approach to uncertainty estimation with dropout. The approach is not perfect, dropout-based uncertainty provides a way to get some \u2014 often reasonable \u2014 measure of uncertainty. Whether the measure is trustworthy enough to be used in deployment is another matter. The question practitioners should ask themselves when implementing this method is whether the resulting model with uncertainty estimates is more useful \u2014 for example, safer \u2014 than a model without uncertainty estimates.", "machine learning | medical image analysis | jcreinhold.com | @JacobCReinhold"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcaac1c4c1f5d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@jcreinhold?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Jacob Reinhold"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6cf1d0b0aa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=post_page-d6cf1d0b0aa7----caac1c4c1f5d---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcaac1c4c1f5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----caac1c4c1f5d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcaac1c4c1f5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&source=-----caac1c4c1f5d---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.deeplearningbook.org/", "anchor_text": "Deep neural networks"}, {"url": "https://arxiv.org/pdf/2004.08955v1.pdf", "anchor_text": "computer vision"}, {"url": "https://arxiv.org/abs/2005.14165", "anchor_text": "natural language processing"}, {"url": "https://arxiv.org/pdf/2005.09629v1.pdf", "anchor_text": "speech recognition"}, {"url": "https://dl.acm.org/doi/abs/10.1145/3285029", "anchor_text": "recommendation systems"}, {"url": "https://www.economist.com/technology-quarterly/2020/06/11/an-understanding-of-ais-limitations-is-starting-to-sink-in", "anchor_text": "number"}, {"url": "https://arxiv.org/abs/1604.00289", "anchor_text": "of"}, {"url": "https://www.aaai.org/ojs/index.php/aimagazine/article/view/2756", "anchor_text": "articles"}, {"url": "https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7", "anchor_text": "blog posts"}, {"url": "http://rebooting.ai/", "anchor_text": "books"}, {"url": "https://www.youtube.com/watch?v=GiPe1OiKQuk", "anchor_text": "Rumsfeldian"}, {"url": "https://arxiv.org/pdf/1706.04599.pdf", "anchor_text": "the softmax outputs are rarely close to 0.5"}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "anchor_text": "entropy"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/cddf290b1d3722b0c88bbc3c82df38a3", "anchor_text": "toy regression example"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/745385337944dbcc98b47578b5a769f9", "anchor_text": "toy classification example"}, {"url": "https://dictionary.cambridge.org/dictionary/english/uncertainty", "anchor_text": "defined by the Cambridge Dictionary"}, {"url": "https://www.goodreads.com/book/show/1446901.The_Emergence_of_Probability", "anchor_text": "The Emergence of Probability"}, {"url": "https://plato.stanford.edu/entries/probability-interpret/", "anchor_text": "What does probability mean"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0167473008000556?casa_token=yWEYH7OP70sAAAAA:FnmF4crmHui1-NCqSx99tkamdl-ITjwAQ7TSlxlHFTpoYp_9xPtAVqtw0CH4ByYr7-gRC71c5Q", "anchor_text": "Aleatory or Epistemic? Does it matter?"}, {"url": "https://en.wikipedia.org/wiki/Autodromo_Nazionale_di_Monza", "anchor_text": "Monza racing track"}, {"url": "https://arxiv.org/abs/1505.05424", "anchor_text": "cornucopia"}, {"url": "http://proceedings.mlr.press/v37/hernandez-lobatoc15.pdf", "anchor_text": "proposed"}, {"url": "http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf", "anchor_text": "methods"}, {"url": "https://en.wikipedia.org/wiki/Discriminative_model", "anchor_text": "discriminative"}, {"url": "https://arxiv.org/abs/1506.02142", "anchor_text": "Yarin Gal and Zoubin Ghahramani"}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "dropout"}, {"url": "https://arxiv.org/abs/1703.04977", "anchor_text": "estimated variance"}, {"url": "https://ieeexplore.ieee.org/abstract/document/374138", "anchor_text": "of the output"}, {"url": "https://en.wikipedia.org/wiki/Laplace_distribution", "anchor_text": "Laplace"}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability#Definition", "anchor_text": "likelihood"}, {"url": "https://alexgkendall.com/media/papers/alex_kendall_phd_thesis_compressed.pdf", "anchor_text": "Pg. 41 of Alex Kendall\u2019s thesis"}, {"url": "https://arxiv.org/abs/1808.01200", "anchor_text": "entropy"}, {"url": "https://pubmed.ncbi.nlm.nih.gov/29259224/", "anchor_text": "sample variance"}, {"url": "https://arxiv.org/abs/1703.02910", "anchor_text": "mutual information"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/cddf290b1d3722b0c88bbc3c82df38a3", "anchor_text": "regression notebook"}, {"url": "https://ieeexplore.ieee.org/abstract/document/7797130", "anchor_text": "better performance"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/cddf290b1d3722b0c88bbc3c82df38a3", "anchor_text": "Jupyter Notebook"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/745385337944dbcc98b47578b5a769f9", "anchor_text": "classification notebook"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/745385337944dbcc98b47578b5a769f9", "anchor_text": "Jupyter Notebook"}, {"url": "https://arxiv.org/abs/2002.04626", "anchor_text": "pre-print here"}, {"url": "https://phillipi.github.io/pix2pix/", "anchor_text": "pix2pix"}, {"url": "https://arxiv.org/abs/1505.04597", "anchor_text": "U-Net"}, {"url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tompson_Efficient_Object_Localization_2015_CVPR_paper.pdf", "anchor_text": "spatial dropout"}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2", "anchor_text": "here"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----caac1c4c1f5d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/uncertainty?source=post_page-----caac1c4c1f5d---------------uncertainty-----------------", "anchor_text": "Uncertainty"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----caac1c4c1f5d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----caac1c4c1f5d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/statistics?source=post_page-----caac1c4c1f5d---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcaac1c4c1f5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----caac1c4c1f5d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcaac1c4c1f5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----caac1c4c1f5d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcaac1c4c1f5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6cf1d0b0aa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=post_page-d6cf1d0b0aa7----caac1c4c1f5d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8c3f698931d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&newsletterV3=d6cf1d0b0aa7&newsletterV3Id=8c3f698931d6&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----caac1c4c1f5d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Written by Jacob Reinhold"}, {"url": "https://medium.com/@jcreinhold/followers?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "218 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://jcreinhold.com", "anchor_text": "jcreinhold.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6cf1d0b0aa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=post_page-d6cf1d0b0aa7----caac1c4c1f5d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8c3f698931d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&newsletterV3=d6cf1d0b0aa7&newsletterV3Id=8c3f698931d6&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----caac1c4c1f5d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----caac1c4c1f5d----0---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----caac1c4c1f5d----0---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----caac1c4c1f5d----0---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Jacob Reinhold"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----caac1c4c1f5d----0---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----caac1c4c1f5d----0---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Dropout on convolutional layers is weirdWhy dropout on convolutional layers is fundamentally different from dropout on fully-connected layers."}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----caac1c4c1f5d----0---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "6 min read\u00b7Feb 10, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c6ab14f19b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdropout-on-convolutional-layers-is-weird-5c6ab14f19b2&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----5c6ab14f19b2----0-----------------clap_footer----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----caac1c4c1f5d----0---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c6ab14f19b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdropout-on-convolutional-layers-is-weird-5c6ab14f19b2&source=-----caac1c4c1f5d----0-----------------bookmark_preview----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----caac1c4c1f5d----1---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----caac1c4c1f5d----1---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----caac1c4c1f5d----1---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----caac1c4c1f5d----1---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----caac1c4c1f5d----1---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----caac1c4c1f5d----1---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----caac1c4c1f5d----1---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----caac1c4c1f5d----1-----------------bookmark_preview----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----caac1c4c1f5d----2---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----caac1c4c1f5d----2---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----caac1c4c1f5d----2---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----caac1c4c1f5d----2---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----caac1c4c1f5d----2---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----caac1c4c1f5d----2---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----caac1c4c1f5d----2---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----caac1c4c1f5d----2-----------------bookmark_preview----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5?source=author_recirc-----caac1c4c1f5d----3---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----caac1c4c1f5d----3---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----caac1c4c1f5d----3---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Jacob Reinhold"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----caac1c4c1f5d----3---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5?source=author_recirc-----caac1c4c1f5d----3---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "Deep Learning with Magnetic Resonance and Computed Tomography ImagesAn introduction to the data, preprocessing techniques and deep network design for medical images"}, {"url": "https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5?source=author_recirc-----caac1c4c1f5d----3---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": "18 min read\u00b7Jan 6, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe9f32273dcb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----e9f32273dcb5----3-----------------clap_footer----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5?source=author_recirc-----caac1c4c1f5d----3---------------------53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe9f32273dcb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&source=-----caac1c4c1f5d----3-----------------bookmark_preview----53ae8991_fcb9_44f1_81bf_7cc3239bf01f-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "See all from Jacob Reinhold"}, {"url": "https://towardsdatascience.com/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----caac1c4c1f5d----0-----------------bookmark_preview----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----caac1c4c1f5d----1-----------------bookmark_preview----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----caac1c4c1f5d----0---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----caac1c4c1f5d----0-----------------bookmark_preview----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----1-----------------clap_footer----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----caac1c4c1f5d----1---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----caac1c4c1f5d----1-----------------bookmark_preview----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----caac1c4c1f5d----2---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----caac1c4c1f5d----2---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----caac1c4c1f5d----2---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----caac1c4c1f5d----2---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----caac1c4c1f5d----2---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Understanding NeRFsA massive breakthrough in scene representation"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----caac1c4c1f5d----2---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "\u00b711 min read\u00b73 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----2a082e13c6eb----2-----------------clap_footer----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----caac1c4c1f5d----2---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&source=-----caac1c4c1f5d----2-----------------bookmark_preview----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----caac1c4c1f5d----3---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----caac1c4c1f5d----3---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----caac1c4c1f5d----3---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----caac1c4c1f5d----3---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----caac1c4c1f5d----3---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----3-----------------clap_footer----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----caac1c4c1f5d----3---------------------3d8aa377_0177_452b_9efe_e8e4962e9a88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----caac1c4c1f5d----3-----------------bookmark_preview----3d8aa377_0177_452b_9efe_e8e4962e9a88-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----caac1c4c1f5d--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}