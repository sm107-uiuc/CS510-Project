{"url": "https://towardsdatascience.com/model-design-and-selection-with-scikit-learn-18a29041d02a", "time": 1682997020.62637, "path": "towardsdatascience.com/model-design-and-selection-with-scikit-learn-18a29041d02a/", "webpage": {"metadata": {"title": "Model Design and Selection with Scikit-learn | by Frank Ceballos | Towards Data Science", "h1": "Model Design and Selection with Scikit-learn", "description": "Purpose: The purpose of this article is to build a pipeline from start to finish in order to access the predictive performance of 18 machine learning models on a synthetic data set. Materials and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb", "anchor_text": "article", "paragraph_index": 3}, {"url": "https://github.com/frank-ceballos/Model_Design-Selection", "anchor_text": "You\u2019re welcome to fork my repository that contains the entire contents of this article.", "paragraph_index": 3}, {"url": "https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/", "anchor_text": "article", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/intro-to-feature-selection-methods-for-data-science-4cae2178a00a", "anchor_text": "too", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient", "paragraph_index": 16}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "RFECV", "paragraph_index": 19}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "Scikit-learn Pipeline object", "paragraph_index": 21}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "Scikit-learn GridSearchCV object", "paragraph_index": 21}, {"url": "https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998", "anchor_text": "Here is a nice Medium article showing a more detailed explanation", "paragraph_index": 22}, {"url": "https://www.codecademy.com/articles/normalization", "anchor_text": "article", "paragraph_index": 24}, {"url": "https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/", "anchor_text": "Here John Ramey shows us how to do it", "paragraph_index": 28}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "Scikit-learn RFE object", "paragraph_index": 32}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle", "paragraph_index": 37}, {"url": "https://machinelearningmastery.com/randomness-in-machine-learning/", "anchor_text": "Here is an excellent article that talks about the randomness of machine learning.", "paragraph_index": 37}, {"url": "https://explained.ai/rf-importance/", "anchor_text": "Here is an amazing article about how to determine more robustly the feature importances.", "paragraph_index": 37}, {"url": "https://github.com/frank-ceballos/Model_Design-Selection", "anchor_text": "You can find all the code for this article in my GitHub repository.", "paragraph_index": 41}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html", "anchor_text": "VotingClassifier", "paragraph_index": 44}, {"url": "https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840", "anchor_text": "here", "paragraph_index": 44}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn", "paragraph_index": 46}], "all_paragraphs": ["Purpose: The purpose of this article is to build a pipeline from start to finish in order to access the predictive performance of 18 machine learning models on a synthetic data set.", "Materials and methods: Using Scikit-learn, we generate a Madelon-like data set for a classification task. The main components of our workflow can be summarized as follows: (1) The training and test set are created. (2) Features are then scaled via Z-score normalization. (3) A feature selection algorithm is applied to reduce the number of features. (4) A machine learning algorithm is trained and evaluated. The predictive power of the 18 trained classifiers will be evaluated using the area under the receiver operating curve (AUC).", "Hardware: We train and evaluate our models on a workstation equipped with Inter(R)Core(TM) i7\u20138700 with 12 CPU @ 3.70 Ghz and NVIDIA GeForce RTX 2080.", "Note: In the case you\u2019re starting from scratch, I will advise you to follow this article and install all the necessary libraries. You\u2019re welcome to fork my repository that contains the entire contents of this article.", "We will generate a Madelon-like synthetic data set using Scikit-learn for a classification task. The Madelon data set is an artificial data set that contains 32 clusters placed on the vertices of a five-dimensional hyper-cube with sides of length 1. The clusters are randomly labeled 1 or -1 (2 classes).", "The data set that we will generate will contain 30 features, where 5 of them will be informative, 15 will be redundant (but informative), 5 of them will be repeated, and the last 5 will be useless since they will be filled with random noise. The columns of the data set will be ordered as follows:", "Let\u2019s start by importing the libraries.", "Now we can generate our data set.", "By randomly sampling without replacement, we create our training and test sets.", "We will train and tune 18 classifiers and evaluate their performance using the area under the receiver operating curve (AUC). It\u2019s way beyond the scope of this article to discuss any of the technicalities of each classifier; however, for the interested reader, you can follow the links in the list shown below. Each classifier is given a label denoted by the string contained between parentheses.", "Here we will create a dictionary whose key-value pairs consist of", "By no means do the hyper-parameters used here represent the optimal hyper-parameter grid for each classifier. You\u2019re welcome to change the hyper-parameter grid as you wish.", "Machine learning can involve problems with thousands of features for each training instance. Determining the optimal subset of features from a large cohort is a common task in machine learning. The advantages that one gains by doing so are numerous. For example, finding the most descriptive features reduces a model\u2019s complexity, makes it easier to find the best solution, and most importantly, it decreases the time it takes to train the model. In some instances, a slight performance boost can be gained.", "Fortunately, it is often possible to considerably reduce the number of features using well established methods. However, it must be noted that by removing features your system might perform slightly worse (since you\u2019re trying to make a prediction with less information).", "There are three common methods for selecting features. Namely, filter, wrapper, and embedded methods. It\u2019s beyond the scope of this article to explain them fully. Therefore, if you\u2019re not familiar with these methods, I will advise you to read this article,and this one too.", "In our workflow, we will first apply a filter method to rapidly reduce the number of features and then apply a wrapper method to determine the minimum number of features we need to maximize the classifier performance.", "Let\u2019s assume that if two features or more are highly correlated, we can randomly select one of them and discard the rest without losing any information. To measure the correlation between features, we will use Spearman\u2019s correlation coefficient. If two features have a Spearman\u2019s correlation value of 1, it means that they are perfectly correlated, 0 not correlated, and -1 highly correlated but in the opposite direction (one feature increases while the other decreases).", "In this step of the feature selection algorithm, we first compute the absolute value of the coefficient matrix using all the features, see Figure 1. We then determine a group of features that have a correlation coefficient greater than 0.95. From each group of correlated features, we will select one of them and discard the rest. You\u2019re welcome to change this threshold I arbitrarily set.", "This should remove 6 features from the data set, which is not a lot \u2014 namely, features 13 and feature 21\u201325. However, in real data sets I\u2019ve worked with, this step has reduced the number of features by up to 50 %. Just note that if you have thousands of features, this might be computationally expensive.", "After removing highly correlated features, we will further reduce the number of features by applying a recursive feature elimination algorithm. The Scikit-learn recursive feature elimination with cross validation (RFECV) object only allows you to use estimators/classifiers that have feature_importances_ or coef_attributes. From experience, I\u2019ve notice that RFECV often overestimates the number of features you really need.", "First you need to select the base estimator to use with RFECV. For the sake of illustration, I will select a Random Forest Classifier as the base. You\u2019re welcome to choose any of the following as the base estimator.", "Once the base estimator is determined, we will tune its hyper-parameters. The reasons to do this are to reduce the risk over-fitting and to maximize the estimator\u2019s performance. To do so, we will create a Scikit-learn Pipeline object that will be used with the Scikit-learn GridSearchCV object.", "GridSearchCV will perform an exhaustive search over the hyper-parameter grid and will report the hyper-parameters that will maximize the cross-validated classifier performance. Here is a nice Medium article showing a more detailed explanation. We will set the number of folds to 5.", "The following are the steps in our Pipeline.", "Step 1 \u2014 Feature Scaling: It\u2019s a common task to scale your features before using them in your algorithm. This is done to ensure that all the features in your data set have the same scale. Therefore, features with larger values won\u2019t dominate over features with smaller values. You can refer to this article for a more thorough explanation. We will use the samples in the training set to scale the data (training and test) via Z-score normalization. All features are centered around zero and have a standard deviation of 1.", "Step 2 \u2014 Classifier: Defining the Classifier object to use in the Pipeline.", "The processing time to tune the Random Forest classifier took 4.8 minutes.", "2.b. Recursively Selecting Features with the Tuned Estimator", "Once we tuned our base estimator, we will create another pipeline similar to the first one, but this one will have the tuned classifier in the second step. Now a technicality arises. Since Scikit-learn Pipeline object does not have feature_importances_ or coef_attributes, we will have to create our own pipeline object if we want to use it with RFECV. Here John Ramey shows us how to do it. Thanks, John!", "Finally, we can use RFECV with our new pipeline. Phew! Go grab a beer, mate.", "Now let\u2019s visualize the results. Plotting in python is kinda crazy, but whatever.", "What a beautiful figure! In Figure 2, we can see the classifier\u2019s performance as a function of a number of features. As you can see, the performance peak\u2019s around 10 features with an AUC of about 0.89; however, if you were to inspect the length of the selected_features list, you will notice that RFECV determined that you needed over 18 features to reach the peak performance.", "It\u2019s problematic that we started with 30 features knowing that only 5 of them were truly necessary and that after our feature selection algorithm we ended up with over 18 representative features. To solve this problem, take a look at Figure 2, visually determine how many features you want to use (10 for example), and use the Scikit-learn RFE object with then_features_to_select parameter set to 10. Notice that after 7 features the performance gain as features are added is minimal. You could use this as your threshold but I like to include a little redundancy since I do not know the optimal number of features for the other 17 classifiers. From Scikit-learn RFE documentation:", "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features \u2026 That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.", "Now you might be wondering why didn\u2019t we use RFE to begin with instead of RFECV. Well, in real life scenarios, you will not know beforehand how many features you will really need. By using RFECV we are able to obtain the optimal subset of features; however, it\u2019s been my experience that it oftentimes overestimates. Nevertheless, from RFECV we obtain the performance curve from which we can make an informed decision of how many features we need. A disadvantage of using RFE is that the results are not cross-validated.", "Once we determine the selected features, we can investigate their importance according to the classifier. I speculate that some of the redundant features are actually more informative to the classifier than the real features. Let\u2019s see if that\u2019s true.", "We will first train the tuned Random Forest classifier with the selected features. Then we will use the feature_importances_ attribute and create a bar plot with it. Note that the following code will only work if the classifier you selected as the base contains a feature_importances_ attribute.", "Among the most important features are feature 6 and 19 which belong to the class of redundant features. It might seem counter-intuitive that the redundant features seem to be more important than the informative features (features 1\u20135). Then again, it can often be seen in Kaggle competitions that feature engineering can give you a boost. It\u2019s important to note that feature importances assigned by a machine learning classifier that is random in nature are not robust. For example, if you were to rerun RFE, you might obtain slightly different results since we didn\u2019t fix the seed in the Random Forest. That\u2019s why you need to cross-validate your results if your plan is to draw some conclusion from your feature importances. Here is an excellent article that talks about the randomness of machine learning. Additionally, do not trust the feature importances if your classifier is not tuned. Here is an amazing article about how to determine more robustly the feature importances.", "Now that we determined a subset of representative features, we are going to tune and train 18 models to investigate the highest performing models among them. To do this, we will iterate over the classifiers defined in Script 4 and use Script 7 to tune them using the hyper-parameters defined in Script 5. We will make minor changes to Script 7 and add a few additional lines of code to evaluate the tuned classifier performance on a test set and save the results.", "Script 13 took about 30 minutes to run in my workstation. I estimate that in a duo core CPU this would take about 3 hours. All the results will be stored in the dictionary object named results. The contents of the results dictionary can be accessed by the classifier_label (see Classifiers section). For each classifier, we store the following objects:", "From Figure 4, we can visually determine that SVC, NuSVC, Gradient Boosting, and AdaBoost classifiers obtained the highest performance in the test set. Look into the contents of the pandas dataframe object auc_scores to see the numerical results.", "If you reached the end of this article, congrats! We actually went through a lot of material. I hope that this will be of aid to you. You can find all the code for this article in my GitHub repository. You\u2019re welcome to fork it. If you would like to use it, simply modify Script 2 and make sure of the following:", "If your data set contains about 1000 samples and 30 features, it should take about 30\u201345 minutes for the whole process to execute \u2014 assuming you have similar hardware than me.", "Now some advice to determine what to do next to further improve the performance of these classifiers.", "The easiest thing would be to select the top five performing classifiers and to run a Grid Search with different parameters. Once you have a feeling about where the best parameters should be, you can run a finer grid search about that point in parameter space. After you\u2019ve further tuned these classifiers, select the best three out of five and use them in VotingClassifier in Scikit-learn. This would most likely result in a higher performance but would increase the complexity of your modeling. You can also consider stacking \u2014 to learn more about it click here.", "You can also look into feature engineering. This would give you the most bang for your buck. I\u2019m planning on writing an article on this subject.", "Find me at LinkedIn. Until next time!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F18a29041d02a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----18a29041d02a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----18a29041d02a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----18a29041d02a--------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----18a29041d02a--------------------------------", "anchor_text": "Frank Ceballos"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe----18a29041d02a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18a29041d02a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18a29041d02a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@killerfvith?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Alex wong"}, {"url": "https://unsplash.com/search/photos/architecture?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb", "anchor_text": "article"}, {"url": "https://github.com/frank-ceballos/Model_Design-Selection", "anchor_text": "You\u2019re welcome to fork my repository that contains the entire contents of this article."}, {"url": "https://sebastianraschka.com/Articles/2014_python_lda.html", "anchor_text": "Linear Discrimination Analysis"}, {"url": "https://xavierbourretsicotte.github.io/LDA_QDA.html", "anchor_text": "Quadratic Discriminant Analysis"}, {"url": "https://towardsdatascience.com/boosting-and-adaboost-clearly-explained-856e21152d3e", "anchor_text": "Adaboost Classifier"}, {"url": "https://towardsdatascience.com/using-bagging-and-boosting-to-improve-classification-tree-accuracy-6d3bb6c95e5b", "anchor_text": "Bagging Classifier"}, {"url": "https://www.quora.com/What-is-the-extra-trees-algorithm-in-machine-learning", "anchor_text": "Extra Trees Classifier"}, {"url": "https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae", "anchor_text": "Gradient Boosting Classifier"}, {"url": "https://medium.com/@taplapinger/tuning-a-random-forest-classifier-1b252d1dde92", "anchor_text": "Random Forest"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html", "anchor_text": "Ridge Classifier"}, {"url": "https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4", "anchor_text": "SGD Classifier"}, {"url": "https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/", "anchor_text": "Bernoulli NB"}, {"url": "https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/", "anchor_text": "Gaussian NB"}, {"url": "https://medium.com/@mohtedibf/in-depth-parameter-tuning-for-knn-4c0de485baf6", "anchor_text": "K Nearest Neighbors Classifier"}, {"url": "https://medium.com/@annishared/build-your-first-neural-network-in-python-c80c1afa464", "anchor_text": "MLP Classifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "anchor_text": "Linear SVC"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html", "anchor_text": "Nu SVC"}, {"url": "https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769", "anchor_text": "SVC"}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d", "anchor_text": "Decision Tree Classifier"}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d", "anchor_text": "Extra Tree Classifier"}, {"url": "https://www.kaggle.com/danielgrimshaw/sklearn-model-exploration", "anchor_text": "Kaggle notebook"}, {"url": "https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/", "anchor_text": "article"}, {"url": "https://towardsdatascience.com/intro-to-feature-selection-methods-for-data-science-4cae2178a00a", "anchor_text": "too"}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "RFECV"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "Scikit-learn Pipeline object"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "Scikit-learn GridSearchCV object"}, {"url": "https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998", "anchor_text": "Here is a nice Medium article showing a more detailed explanation"}, {"url": "https://www.codecademy.com/articles/normalization", "anchor_text": "article"}, {"url": "https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/", "anchor_text": "Here John Ramey shows us how to do it"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "Scikit-learn RFE object"}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle"}, {"url": "https://machinelearningmastery.com/randomness-in-machine-learning/", "anchor_text": "Here is an excellent article that talks about the randomness of machine learning."}, {"url": "https://explained.ai/rf-importance/", "anchor_text": "Here is an amazing article about how to determine more robustly the feature importances."}, {"url": "https://github.com/frank-ceballos/Model_Design-Selection", "anchor_text": "You can find all the code for this article in my GitHub repository."}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html", "anchor_text": "VotingClassifier"}, {"url": "https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----18a29041d02a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----18a29041d02a---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/tag/python?source=post_page-----18a29041d02a---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----18a29041d02a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/modeling?source=post_page-----18a29041d02a---------------modeling-----------------", "anchor_text": "Modeling"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18a29041d02a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&user=Frank+Ceballos&userId=176ad7e37afe&source=-----18a29041d02a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18a29041d02a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&user=Frank+Ceballos&userId=176ad7e37afe&source=-----18a29041d02a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18a29041d02a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----18a29041d02a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F18a29041d02a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----18a29041d02a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----18a29041d02a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----18a29041d02a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----18a29041d02a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----18a29041d02a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----18a29041d02a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----18a29041d02a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----18a29041d02a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----18a29041d02a--------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Frank Ceballos"}, {"url": "https://frank-ceballos.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "856 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3961b72dd61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-design-and-selection-with-scikit-learn-18a29041d02a&newsletterV3=176ad7e37afe&newsletterV3Id=f3961b72dd61&user=Frank+Ceballos&userId=176ad7e37afe&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}