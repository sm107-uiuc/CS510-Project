{"url": "https://towardsdatascience.com/the-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33", "time": 1683006406.533429, "path": "towardsdatascience.com/the-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33/", "webpage": {"metadata": {"title": "The Unreasonable Ineffectiveness of Deep Learning on Tabular Data | by Paul Tune | Towards Data Science", "h1": "The Unreasonable Ineffectiveness of Deep Learning on Tabular Data", "description": "Deep neural networks have led to breakthroughs in various domains that have long been considered a challenge. Two notable examples are computer vision and natural language processing (NLP). We have\u2026"}, "outgoing_paragraph_urls": [{"url": "https://qz.com/1307091/the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley/", "anchor_text": "breakthrough development of AlexNet", "paragraph_index": 1}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet", "paragraph_index": 1}, {"url": "https://thispersondoesnotexist.com/", "anchor_text": "lifelike portraits of fake people from SyleGAN", "paragraph_index": 1}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "deployed BERT in its search engine", "paragraph_index": 2}, {"url": "https://aidungeon.io/", "anchor_text": "AI Dungeon", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Inductive_bias", "anchor_text": "inductive biases", "paragraph_index": 4}, {"url": "https://www.canva.com/", "anchor_text": "Canva", "paragraph_index": 16}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 16}, {"url": "https://xgboost.readthedocs.io/en/latest/", "anchor_text": "XGBoost", "paragraph_index": 17}, {"url": "https://catboost.ai/", "anchor_text": "CatBoost", "paragraph_index": 17}, {"url": "https://lightgbm.readthedocs.io/en/latest/", "anchor_text": "LightGBM", "paragraph_index": 17}, {"url": "https://github.com/ptuls/tabnet-modified", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://iclr.cc/", "anchor_text": "ICLR 2020", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep Residual Learning for Image Recognition", "paragraph_index": 52}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "Generative Adversarial Networks", "paragraph_index": 53}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners", "paragraph_index": 54}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 55}, {"url": "https://arxiv.org/abs/1603.08983", "anchor_text": "Adaptive Computation Time for Recurrent Neural Networks", "paragraph_index": 56}, {"url": "https://arxiv.org/abs/1909.06312", "anchor_text": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data", "paragraph_index": 57}, {"url": "https://arxiv.org/abs/1608.06993", "anchor_text": "Densely Connected Convolutional Networks", "paragraph_index": 58}], "all_paragraphs": ["Deep neural networks have led to breakthroughs in various domains that have long been considered a challenge. Two notable examples are computer vision and natural language processing (NLP).", "We have seen speedy development in computer vision, beginning with the breakthrough development of AlexNet, winning the ImageNet challenge in 2012. We have ResNet\u00b9 in 2015 achieving superhuman accuracy for the first time on the ImageNet benchmark dataset. We then witnessed the birth of generative adversarial networks (GANs)\u00b2 in 2014, to the rapid improvements that ultimately led to the lifelike portraits of fake people from SyleGAN today.", "In NLP, deep neural networks models are now state-of-the-art, outperforming conventional machine learning algorithms on benchmark datasets. Models such as GPT-2\u00b3 and BERT\u2074 are the new gold standard. Google has deployed BERT in its search engine in 2019, the single largest update to its search engine in the past few years. GPT-2 is used in chatbot applications, and in some interesting ways such as a popular text adventure game called AI Dungeon. It wouldn\u2019t be long now that these solutions will become commonplace in commercial products.", "It is clear from these breakthroughs that deep neural networks have taken over the domain of unstructured data.", "The success of deep learning boils down to the exploitation of inductive biases in these data, such as spatial correlations in the case of the convolutional neural network.", "Yet it seems that there is no deep learning equivalent for tabular data.", "Like it or not, unless you\u2019re working in a company that works on signal processing applications or natural language processing, most data exists in relational databases and Excel spreadsheets. In fact, this state of affairs is not going away anytime soon!", "Data from relational databases and spreadsheets are examples of structured data. Structured data are highly organized in a tabular structure to allow efficient operations on the table columns such as search and joins. Typically a schema defined for the data: each attribute is named, and its value type is specified, which could be a string or integer for example. A unique identifier is also assigned in each schema. Usually, in order to perform a task, data needs to be drawn from several databases, each possessing their own schema.", "Consider a movie streaming platform which provides monthly subscriptions.", "The figure below shows an example of a several of the company\u2019s databases with their associated schema. Here, we can see that there are tables storing", "Relational databases are a well-studied, relatively mature technology. They are the de facto standard for storing tabular data. Given that so much data in the world exists in a tabular format, such as the above, it is no surprise that there is a strong motivation to predict from tabular data.", "For example, we can predict the churn of a user based on the data from the tables using:", "Surprisingly, despite the success of deep neural networks elsewhere, they don\u2019t work particularly well on tabular data, and don\u2019t seem to beat the performance of shallow neural networks.", "Why consider deep neural networks at all? What are the advantages of deep neural networks that learn effectively from tabular data?", "Deep neural networks have a powerful feature preprocessing ability. We can construct feature preprocessors by stacking layer upon layer to learn a hierarchical representation of the data. Moreover, this feature preprocessing ability is finetuned via backpropagation as the network learns from labels in the supervised setting. Fittingly, the focus of a data scientist shifts from feature engineering to constructing a good architecture. Let\u2019s face it, next to data cleaning, feature engineering is the second-least favorite activity of a data scientist.", "The success of deep learning in computer vision and NLP owes in large part to the remarkable ability of these models to transfer what they have learned to an adjacent task. For instance, a data scientist can download a computer vision model, fully pretrained on ImageNet, and finetune it on her own dataset to classify diseased crops in agriculture. We are then able to share models with others, speeding up development of applications. Similarly, it is conceivable that a deep learning model can be pretrained on large sets of tabular data, providing a method to \u201cpretrain\u201d a base of knowledge (akin to a knowledge graph) and use it a module to speed up training of another model for a related task.", "An end-to-end deep learning model provides a way to link structured and unstructured data via the magic of backpropagation, all in a single model. At Canva (where I work), one application would be to combine user demographic information (such as their profession) with image features from design templates and images they have chosen. We can then provide a better prediction on the kind of design styles they like, and templates they might want to use. User demographic information exists in structured tabular data in relational databases, while the latter exists as unstructured image data. Linking a tabular data learning model with a convolutional neural network can be easily done in a deep learning framework such as TensorFlow. This is in contrast to gradient boosted trees where gradient information is not available to link it to another model.", "Undoubtedly, the reigning champions of learning from tabular currently are decision tree-based algorithms. Gradient boosted trees are often the tool of choice, evidenced from their popularity in Kaggle contests, with XGBoost, CatBoost and LightGBM being crowd favorites.", "At first glance, it is puzzling why deep learning are not competitive with tree-based algorithms. After all, features from combined databases do form high dimensional spaces, which deep learning exploits very well.", "For a model to learn effectively from tabular data, there are several key issues that have to be addressed:", "Moreover, from a business intelligence perspective, predictions are not only the main goal, but finding the factors in the data for intrepretability of the predictions is just as, if not more, important.", "Correlations between columns in a relational database and spreadsheets are common. There is often a small set of features (or combination of features) that contribute to the predictive power of the task. In short, our inductive bias is that there are (highly) correlated features, so selecting the minimum set of features is the best strategy.", "Decision trees and their more advanced siblings, the random forest and gradient boosted trees, select and combine the features very well, via a greedy heuristic.", "A key difference with the unstructured data, however, is that tabular data is often heterogenous. The features constructed from tables come from various unrelated sources, each with their own units (e.g., seconds vs. hours) and associated numerical scaling issues. The second difference is that the features themselves are sparse: unlike data from images, audio and language, there can be little variation in a column of a table. There are also typically more categorical features, in which the order (and value) of the features themselves are not important, and unlike numerical features, are discrete by nature. To handle this, preprocessing often has be done carefully for neural networks by learning dense embeddings.", "These differences lead to features in a high-dimensional space that is generally not dense and continuous, making it difficult to exploit for a typical deep neural network. What needs to be done is to preprocess the space in such a way that the space is \u201cfriendlier\u201d for a deep neural network.", "Fortunately, recent developments in attention and sparse regularization now pave the way to learn more efficiently from structured data.", "Recently there has been a focus on improving the performance of deep neural networks on tabular data. Naturally, these works overcome the problems of canonical deep neural networks by emulating the best qualities of tree-based algorithms. We cover two recent works in this area, with a little more emphasis on the first since I\u2019ve been experimenting with it recently.", "TabNet is a deep neural network specifically designed to learn from tabular data\u2075. Its design departs from other works which are modified tree-based hybrids.", "The key to TabNet is a learnable mask on the input features. Moreover, the learnable mask should be sparse, i.e., favor the selection of a small set of features that solves the prediction task.", "An advantage of the learnable mask over trees is that the feature selection is not an all-or-nothing proposition. In decision trees, a hard threshold is placed on the value of a feature, so if the value exceeds a threshold T, then go down the right branch of the tree, otherwise go left. With a learnable mask, soft decisions can be made, meaning that the decision can be made on a larger range of values, not just a single threshold value.", "The TabNet encoder, seen in the figure above, consists of sequential decision steps which encodes and selects features via a learnable mask. TabNet trains on each row from a table, selects (attends to) the relevant features in each step using a sparse learnable mask, and aggregates the predictions from each step to emulate an ensemble-like effect when making predictions. More steps in the encoder means that more ensembles are constructed.", "The two main components of the encoder are:", "There is also a sparsity regularization loss function via Shannon entropy to control the overall sparsity of the masks in each decision step during training.", "We can visualize the feature importance of each decision step, as well as an aggregated version. In the figure below, we plot the masks for the aggregated mask, and masks for decision steps 1, 2 and 3 for the Poker Hands dataset\u2077. The rows of the image denote the number of test samples, while each column denote a feature. We can see some feature reuse between the different steps, with importance placed on some features for some test samples.", "Interestingly, TabNet can be trained in an unsupervised manner. If the model is modified to an encoder-decoder structure, then TabNet works as an autoencoder on tabular data! The paper discusses one potential architecture for the decoder, which is a rather simple one that consists of feature transformers. Training involves deliberately censoring cells (as seen in the figure below) in the table in order to get the model to learn relationships between the cell and adjacent columns by predicting the missing values. Once training is complete, TabNet can be used as a pretrained model for classification and regression tasks.", "In my own experiments with TabNet, I have observed that:", "It would be interesting if TabNet can be augmented with sequential adaptivity, so that the number of decision steps taken can be learned on the fly as per the adaptive computation trick used in recurrent neural networks\u2078. Then it would be possible for the model to automatically figure out the number of decision steps it needs.", "Another architecture change would be to replace the sparsemax function with EntMax instead (see below). It has been shown that EntMax performs better than sparsemax in other applications, so it would be worth trying.", "Also an application of TabNet\u2019s unsupervised mode is to allow it to impute missing values in tabular data. This is especially useful when analytics collection may be problematic, resulting in outages and missing data.", "Update: I modified TabNet here. Check it out!", "Recently accepted at ICLR 2020, a NODE module\u2079 is composed of a prespecified number m of oblivious decision trees (ODTs), each with equal depth d. The twist, compared to other tree-based algorithms, is that these ODTs are differentiable, so error gradients can be backpropagated through them. Just like TabNet, NODE is an end-to-end deep learning model.", "Let\u2019s begin by looking at a vanilla ODT. Unlike asymmetric trees in the figure above (which are the base components of XGBoost and LightGBM), ODTs split data along d splitting features and compares each feature to a learned threshold. We can see from the figure that the split is made on a hard threshold, for example, all persons with weight above 65 kg always traverses down the right branch. Also note that exactly one feature is chosen for split at each level, giving rise to an all-or-nothing proposition.", "The advantage of having balanced trees is that they are computationally efficient since each ODT can be represented as a decision table, and the splits can be computed in parallel. During inference, no if statements are evaluated since the result can be looked up from the pre-computed decision table. Secondly, the depth restriction of ODTs minimizes overfitting.", "Each ODT is a weak learner, but when an ensemble of them are used together, the ensemble becomes a strong learner.", "As an aside, ODTs are the main component used in CatBoost. It is not surprising that ODTs are the base component of choice for NODE, as the authors of the paper are from Yandex, which developed CatBoost.", "Now, the difference between NODE\u2019s and CatBoost\u2019s ODT lies in the choice of splitting feature and the splitting function used in the tree. Instead of a hard versions of these functions, soft, differentiable ones are used. NODE uses the EntMax\u00b9\u2070 rather than sparsemax to promote sparsity in both of these functions. EntMax is a recent development which generalizes the sparsemax and softmax functions, with the ability to learn sparse but smoother decision rules than sparsemax, giving better trade-off between sparsity and decision.", "The NODE layer will, over time during training, choose a minimal set of features to split at each level (rather than just 1), and the splitting threshold covers a range of values (rather than just a single threshold).", "The deep in the architecture comes from stacking NODE layers one atop another. Each NODE layer takes in inputs from the original feature input and all other NODE layers preceding it. This is similar to the sequential decision steps in TabNet, as each NODE layer shares processed features with subsequent NODE layers. With this structure, the model can capture shallow and deep decision rules. The architecture was inspired from DenseNet\u00b9\u00b9.", "The paper showed that NODE is competitive with other tree-based algorithms. It outperforms most gradient boosted tree algorithms, including CatBoost, but is sometimes beaten by XGBoost when tested on 6 publicly available datasets.", "One interesting direction for NODE is to incorporate asymmetric trees in the layers, as it turns out, according to tests in the paper, that asymmetric trees perform better on some datasets than ODTs.", "There is now an interest in developing deep neural networks that will hopefully unseat tree-based algorithms from their reigning status as the best learners on tabular data.", "While these recent developments have been very exciting, I end on some practical considerations:", "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition, Proc. Computer Vision and Pattern Recognition (CVPR), IEEE (2015).", "[2] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative Adversarial Networks, Proceedings of the International Conference on Neural Information Processing Systems (NIPS 2014), pp. 2672\u2013268.", "[3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language Models are Unsupervised Multitask Learners (2019).", "[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018).", "[8] Alex Graves, Adaptive Computation Time for Recurrent Neural Networks (2016).", "[9] Sergei Popov, Stanislav Morozov, Artem Babenko, Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data, International Conference on Learning Representations (ICLR) (2020).", "[11] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger, Densely Connected Convolutional Networks (2017).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning engineer at Canva. I work on computer vision and churn modelling. My interests are data science, information theory, finance and investing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffd784ea29c33&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@anamolous.behaviour?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anamolous.behaviour?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "Paul Tune"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3f15d28c014e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&user=Paul+Tune&userId=3f15d28c014e&source=post_page-3f15d28c014e----fd784ea29c33---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd784ea29c33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd784ea29c33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@alinnnaaaa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Alina Grubnyak"}, {"url": "https://unsplash.com/s/photos/network?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://qz.com/1307091/the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley/", "anchor_text": "breakthrough development of AlexNet"}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet"}, {"url": "https://thispersondoesnotexist.com/", "anchor_text": "lifelike portraits of fake people from SyleGAN"}, {"url": "https://www.blog.google/products/search/search-language-understanding-bert/", "anchor_text": "deployed BERT in its search engine"}, {"url": "https://aidungeon.io/", "anchor_text": "AI Dungeon"}, {"url": "https://en.wikipedia.org/wiki/Inductive_bias", "anchor_text": "inductive biases"}, {"url": "https://www.omnisci.com/technical-glossary/relational-database", "anchor_text": "link"}, {"url": "https://www.canva.com/", "anchor_text": "Canva"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://xgboost.readthedocs.io/en/latest/", "anchor_text": "XGBoost"}, {"url": "https://catboost.ai/", "anchor_text": "CatBoost"}, {"url": "https://lightgbm.readthedocs.io/en/latest/", "anchor_text": "LightGBM"}, {"url": "https://arxiv.org/pdf/1908.07442.pdf", "anchor_text": "TabNet paper"}, {"url": "https://arxiv.org/pdf/1908.07442.pdf", "anchor_text": "TabNet paper"}, {"url": "https://arxiv.org/pdf/1908.07442.pdf", "anchor_text": "TabNet paper"}, {"url": "https://arxiv.org/pdf/1908.07442.pdf", "anchor_text": "TabNet paper"}, {"url": "https://github.com/ptuls/tabnet-modified", "anchor_text": "here"}, {"url": "https://iclr.cc/", "anchor_text": "ICLR 2020"}, {"url": "https://towardsdatascience.com/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14", "anchor_text": "link"}, {"url": "https://arxiv.org/abs/1909.06312", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1909.06312", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "Deep Residual Learning for Image Recognition"}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "Generative Adversarial Networks"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://arxiv.org/abs/1908.07442", "anchor_text": "TabNet: Attentive Interpretable Tabular Learning"}, {"url": "http://proceedings.mlr.press/v48/martins16.pdf", "anchor_text": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification"}, {"url": "http://archive.ics.uci.edu/ml/datasets/Poker+Hand", "anchor_text": "Poker Hand Dataset"}, {"url": "https://arxiv.org/abs/1603.08983", "anchor_text": "Adaptive Computation Time for Recurrent Neural Networks"}, {"url": "https://arxiv.org/abs/1909.06312", "anchor_text": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data"}, {"url": "https://www.aclweb.org/anthology/P19-1146/", "anchor_text": "Sparse sequence-to-sequence models"}, {"url": "https://arxiv.org/abs/1608.06993", "anchor_text": "Densely Connected Convolutional Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fd784ea29c33---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----fd784ea29c33---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----fd784ea29c33---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/tabular-data?source=post_page-----fd784ea29c33---------------tabular_data-----------------", "anchor_text": "Tabular Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fd784ea29c33---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd784ea29c33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&user=Paul+Tune&userId=3f15d28c014e&source=-----fd784ea29c33---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd784ea29c33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&user=Paul+Tune&userId=3f15d28c014e&source=-----fd784ea29c33---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd784ea29c33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffd784ea29c33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fd784ea29c33---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fd784ea29c33--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fd784ea29c33--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fd784ea29c33--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anamolous.behaviour?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anamolous.behaviour?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Paul Tune"}, {"url": "https://medium.com/@anamolous.behaviour/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "491 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3f15d28c014e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&user=Paul+Tune&userId=3f15d28c014e&source=post_page-3f15d28c014e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1d01df761956&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unreasonable-ineffectiveness-of-deep-learning-on-tabular-data-fd784ea29c33&newsletterV3=3f15d28c014e&newsletterV3Id=1d01df761956&user=Paul+Tune&userId=3f15d28c014e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}