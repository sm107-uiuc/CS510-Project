{"url": "https://towardsdatascience.com/deep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58", "time": 1683014849.9058719, "path": "towardsdatascience.com/deep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58/", "webpage": {"metadata": {"title": "How to Improve your Supply Chain with Deep Reinforcement Learning | by Christian Hubbs | Towards Data Science", "h1": "How to Improve your Supply Chain with Deep Reinforcement Learning", "description": "What has set Amazon apart from the competition in online retail? Their supply chain. In fact, this has long been one of the greatest strengths of one of their chief competitors, Walmart. Supply\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=U3W2v7LN-88", "anchor_text": "single, simple product would be staggering", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c", "anchor_text": "Ray", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/2008.06319", "anchor_text": "or-gym", "paragraph_index": 3}, {"url": "https://www.datahubbs.com/how-to-use-deep-reinforcement-learning-to-improve-your-supply-chain/", "anchor_text": "benchmark it", "paragraph_index": 3}, {"url": "https://github.com/hubbs5/or-gym", "anchor_text": "OR-Gym", "paragraph_index": 5}, {"url": "https://www.datahubbs.com/action-masking-with-rllib/", "anchor_text": "an example is given here", "paragraph_index": 10}, {"url": "https://www.datahubbs.com/ray-and-rllib-fast-reinforcement-learning/", "anchor_text": "Ray library", "paragraph_index": 11}, {"url": "https://www.datahubbs.com/hyperparameter-tuning-with-tune/", "anchor_text": "tune", "paragraph_index": 14}, {"url": "https://www.datahubbs.com/hyperparameter-tuning-with-tune/", "anchor_text": "for hyperparameter tuning", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Powell%27s_method", "anchor_text": "Powell's Method", "paragraph_index": 24}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P", "paragraph_index": 40}], "all_paragraphs": ["What has set Amazon apart from the competition in online retail? Their supply chain. In fact, this has long been one of the greatest strengths of one of their chief competitors, Walmart.", "Supply chains are highly complex systems consisting of hundreds if not thousands of manufacturers and logistics carriers around the world who combine resources to create the products we use and consume every day. To track all of the inputs to a single, simple product would be staggering. Yet supply chain organizations inside vertically integrated corporations are tasked with managing inputs from raw materials, to manufacturing, warehousing, and distribution to customers. The companies that do this best cut down on waste from excess storage, to unneeded transportation costs, and lost time to get products and materials to later stages in the system. Optimizing these systems is a key component in businesses as dissimilar as Apple and Saudi Aramco.", "A lot of time and effort has been put into building effective supply chain optimization models, but due to their size and complexity, they can be difficult to build and manage. With advances in machine learning, particularly reinforcement learning, we can train a machine learning model to make these decisions for us, and in many cases, do so better than traditional approaches!", "We train a deep reinforcement learning model using Ray and or-gym to optimize a multi-echelon inventory management model and benchmark it against a derivative free optimization model using Powell\u2019s Method.", "In our example, we\u2019re going to work with a multi-echelon supply chain model with lead times. This means that we have different stages of our supply chain that we need to make decisions for, and each decision that we make at different levels are going to affect decisions downstream. In our case, we have M stages going back to the producer of our raw materials all the way to our customers. Each stage along the way has a different lead time, or time it takes for the output of one stage to arrive and become the input for the next stage in the chain. This may be 5 days, 10 days, whatever. The longer these lead times become, the earlier you need to anticipate customer orders and demand to ensure you don\u2019t stock out or lose sales!", "The OR-Gym library has a few multi-echelon supply chain models ready to go to simulate this structure. For this, we\u2019ll use the InvManagement-v1 environment, which has the structure shown above, but results in lost sales if you don't have sufficient inventory to meet customer demand.", "If you haven\u2019t already, go ahead and install the package with:", "Once installed, we can set up our environment with:", "This is a four-echelon supply chain by default. The actions determine how much material to order from the echelon above at each time step. The orders quantities are limited by the capacity of the supplier and their current inventory. So, if you order 150 widgets from a supplier that has a shipment capacity of 100 widgets and only has 90 widgets on hand, you\u2019re going to only get 90 sent.", "Each echelon has its own costs structure, pricing, and lead times. The last echelon (Stage 3 in this case) provides raw materials, and we don\u2019t have any inventory constraints on this stage, assuming that the mine, oil well, forest \u2014 or whatever produces your raw material inputs \u2014 is large enough that this isn\u2019t a constraint we need to concern ourselves with.", "As with all or-gym environments, if these settings don't suit you, simply pass an environment configuration dictionary to the make function to customize your supply chain accordingly (an example is given here).", "To train your environment, we\u2019re going to leverage the Ray library to speed up our training, so go ahead and import your packages.", "To get started, we\u2019re going to need a brief registration function to ensure that Ray knows about the environment we want to run. We can register that with the register_env function shown below.", "From here, we can set up our RL configuration and everything we need to train the model.", "The rl_config dictionary is where you can set all of the relevant hyperparameters or set your system to run on a GPU. Here, we're just going to use 2 workers for parallelization, and train a two-layer network with an ELU activation function. Additionally, if you're going to use tune for hyperparameter tuning, then you can use tools like tune.gridsearch() to systematically update learning rates, change the network, or whatever you like.", "Once your happy with that, go head and choose your algorithm and get to training! Below, I just use the PPO algorithm because I find it trains well on most environments.", "The code above will initialize ray, then build the agent according to the configuration you specified previously. If you're happy with that, then let it run for a bit and see how it does!", "One thing to note with this environment: if the learning rate is too high, the policy function will begin to diverge such that the loss becomes astronomically large. At that point, you\u2019ll wind up getting an error, typically stemming from Ray\u2019s default pre-processor with state showing bizarre values because the actions being given by the network are all nan. This is easy to fix by bringing the learning rate down a bit and trying again.", "Let\u2019s take a look at the performance.", "It looks like our agent learned a decent policy!", "One of the difficulties of deep reinforcement learning for these classic, operations research problems is the lack of optimality guarantees. In other words, we can look at that training curve above and see that it is learning a better and better policy \u2014 and it seems to be converging on a policy \u2014 but we don\u2019t know how good that policy is. Could we do better? Should we invest more time (and money) into hyperparameter tuning? To answer this, we need to turn to some different methods and develop a benchmark.", "A good way to benchmark an RL model is with derivative free optimization (DFO). Like RL, DFO treats the system as a black-box model providing inputs and getting some feedback in return to try again as it seeks the optimal value.", "Unlike RL, DFO has no concept of a state. This means that we will try to find a fixed re-order policy to bring inventory up to a certain level to balance holding costs and profit from sales. For example, if the policy at stage 0 is to re-order up to 10 widgets, and the currently, we have 4 widgets, then the policy states we\u2019re going to re-order 6. In the RL case, it would take into account the current pipeline and all of the other information that we provide into the state. So RL is more adaptive and ought to outperform a straightforward DFO implementation. If it doesn\u2019t, then we know we need to go back to the drawing board.", "While it may sound simplistic, this fixed re-order policy isn\u2019t unusual in industrial applications, partly because real supply chains consist of many more variables and interrelated decisions than we\u2019re modeling here. So a fixed policy is tractable and something that supply chain professionals can easily work with.", "There are a lot of different algorithms and solvers out there for DFO. For our purposes, we\u2019re going to leverage Scipy\u2019s optimize library to implement Powell's Method. We won't get into the details here, but this is a way to quickly find minima on functions and can be used for discrete optimization - like we have here.", "Because we\u2019re going to be working with a fixed re-order policy, we need a quick function to translate inventory levels into actions to evaluate.", "The base_stock_policy function takes the policy levels we supply and calculates the difference between the level and the inventory as described above. One thing to note, when we calculate the inventory level, we include all of the inventory in transit to the stage as well (given in env.T). For example, if the current inventory on hand for stage 0 is 100, and there is a lead time of 5 days between stage 0 and stage 1, then we take all of those orders for the past 5 days into account as well. So, if stage 0 ordered 10 units each day, then the inventory at this echelon would be 150. This makes policy levels greater than capacity meaningful because we're looking at more than just the inventory in our warehouse today, but looking at everything in transit too.", "Our DFO method needs to make function evaluation calls to see how the selected variables perform. In our case, we have an environment to evaluate, so we need a function that will run an episode of our environment and return the appropriate results.", "Rather than return the sum of the rewards, we\u2019re returning the negative expectation of our rewards. The reason for the negative is the Scipy function we\u2019re using seeks to minimize whereas our environment is designed to maximize the reward, so we invert this to ensure everything is pointing in the right direction. We calculate the expected rewards by multiplying by the probability of our demand based on the distribution. We could take more samples to estimate the distribution and calculate our expectation that way (and for many real-world applications, this would be required), but here, we have access to the true distribution so we can use that to reduce our computational burden.", "The following function will build an environment based on your configuration settings, take our dfo_func to evaluate, and apply Powell's Method to the problem. It will return our policy and ensure that our answer contains only positive integers (e.g. we can't order half a widget or a negative number of widgets).", "Now it\u2019s time to put it all together.", "Our DFO model found a fixed-stock policy with re-order levels at 540 for stage 0, 216 for stage 1, and 81 for stage 2. It did this with only 212 function evaluations, i.e. it simulated 212 episodes to find the optimal value.", "We can run then feed this policy into our environment, say 1,000 times, to generate some statistics and compare it to our RL solution.", "Before we get into the reward comparisons, note that these are not perfect, 1:1 comparisons. As mentioned before, DFO yields us a fixed policy whereas RL has a more flexible, dynamic policy that changes based on state information. Our DFO approach was also given some information in terms of probabilities of demand to calculate the expectation on, RL had to infer that from additional sampling. So while RL learned from nearly ~65k episodes and DFO only had to make 212 function calls, they aren\u2019t exactly comparable. Considering that to enumerate every meaningful fixed policy once would require ~200 million episodes, then RL doesn\u2019t look so sample inefficient given its task.", "So, how do these stack up?", "What we can see above is that RL does indeed outperform our DFO policy by 11% on average (460 to 414). The RL model overtook the DFO policy after ~15k episodes and improved steadily after that. There is some higher variance with the RL policy however, with a few terrible episodes thrown in to the mix. All things considered, we did get stronger results overall from the RL approach, as expected.", "In this case, neither method was very difficult to implement nor computationally intensive. I forgot to change my rl_config settings to run on my GPU and it still only took about 25 minutes to train on my laptop while the DFO model took ~2 seconds to run. More complex models may not be so friendly in either case.", "Another thing to note, both methods can be very sensitive to initial conditions and neither are guaranteed to find the optimum policy in every case. If you have a problem you\u2019d like to apply RL to, maybe use a simple DFO solver first, try a few initial conditions to get a feel for the problem, then spin up the full, RL model. You may find that the DFO policy is sufficient for your task.", "Hopefully this gave a good overview of how to use these methods and the or-gym library. Leave feedback or questions if you have any!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI/ML researcher writing about technology, economics, and business. Connect with me: https://bit.ly/2scbU1P"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3e4d99ad4b58&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b----3e4d99ad4b58---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e4d99ad4b58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e4d99ad4b58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=U3W2v7LN-88", "anchor_text": "single, simple product would be staggering"}, {"url": "https://unsplash.com/@shawnanggg?utm_source=medium&utm_medium=referral", "anchor_text": "Shawn Ang"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c", "anchor_text": "Ray"}, {"url": "https://arxiv.org/abs/2008.06319", "anchor_text": "or-gym"}, {"url": "https://www.datahubbs.com/how-to-use-deep-reinforcement-learning-to-improve-your-supply-chain/", "anchor_text": "benchmark it"}, {"url": "https://arxiv.org/abs/2008.06319", "anchor_text": "from Hubbs et al."}, {"url": "https://github.com/hubbs5/or-gym", "anchor_text": "OR-Gym"}, {"url": "https://www.datahubbs.com/action-masking-with-rllib/", "anchor_text": "an example is given here"}, {"url": "https://www.datahubbs.com/ray-and-rllib-fast-reinforcement-learning/", "anchor_text": "Ray library"}, {"url": "https://www.datahubbs.com/hyperparameter-tuning-with-tune/", "anchor_text": "tune"}, {"url": "https://www.datahubbs.com/hyperparameter-tuning-with-tune/", "anchor_text": "for hyperparameter tuning"}, {"url": "https://en.wikipedia.org/wiki/Powell%27s_method", "anchor_text": "Powell's Method"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----3e4d99ad4b58---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3e4d99ad4b58---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----3e4d99ad4b58---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3e4d99ad4b58---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/supply-chain?source=post_page-----3e4d99ad4b58---------------supply_chain-----------------", "anchor_text": "Supply Chain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e4d99ad4b58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----3e4d99ad4b58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e4d99ad4b58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----3e4d99ad4b58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e4d99ad4b58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3e4d99ad4b58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3e4d99ad4b58---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3e4d99ad4b58--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://christiandhubbs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "657 Followers"}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd00e5d4c3897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-supply-chain-optimization-3e4d99ad4b58&newsletterV3=8c5ed989fb1b&newsletterV3Id=d00e5d4c3897&user=Christian+Hubbs&userId=8c5ed989fb1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}