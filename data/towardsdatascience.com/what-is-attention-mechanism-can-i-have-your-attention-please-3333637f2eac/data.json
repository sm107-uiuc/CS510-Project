{"url": "https://towardsdatascience.com/what-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac", "time": 1683015986.915781, "path": "towardsdatascience.com/what-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac/", "webpage": {"metadata": {"title": "What is attention mechanism?. Evolution of the techniques to solve\u2026 | by Nechu BM | Towards Data Science", "h1": "What is attention mechanism?", "description": "Throughout this article we will first understand the limits of the techniques we were using to then introduce the new mechanisms to overcome the problems they were facing. We will be looking at the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630", "anchor_text": "built with RNN", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1502.03044", "anchor_text": "Show, Attend and Tell", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "\u2018Neural Machine Translation by jointly learning to align and translate\u2019", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Bahdanau et al 2015", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1508.04025", "anchor_text": "Luong 2015", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need\u2019", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "(BERT)", "paragraph_index": 18}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "following article.", "paragraph_index": 27}, {"url": "https://bit.ly/36vajnu", "anchor_text": "https://bit.ly/36vajnu", "paragraph_index": 38}], "all_paragraphs": ["Throughout this article we will first understand the limits of the techniques we were using to then introduce the new mechanisms to overcome the problems they were facing. We will be looking at the following sections:", "An encoder decoder architecture is built with RNN and it is widely used in neural machine translation (NMT) and sequence to sequence (Seq2Seq) prediction. Its main benefit is that we can detach the encoder and the decoder, so they have different lengths.", "Models with different sequences lengths are, for example, sentiment analysis that receives a sequence of words and outputs a number, or Image captioning models where the input is an image and the output is a sequence of words. This architecture is so powerful that even Google has adopted it as the core technology for Google Translate.", "In the image below we can see how the model looks like for a translation example:", "We have the encoder (blue rectangle) build with an Input Layer and a Recurrent Neural Network (RNN) more precisely a Long Short-Term Memory (LSTM). The encoder receives the Spanish sentence and outputs a single vector which is the hidden state of the last LSTM time step, the meaning of the whole sentence is captured in this vector. Then the decoder receives this hidden state as an input and return a sequence of words, the English translation.", "This architecture has demonstrated its great capacities in Seq2Seq problems, however it also has one important limitation. As we have said all the input sentence meaning is captured in one vector, so as the length of the sentence increase, the more difficult it gets for the model to capture the information in this vector. Consequently, its performance decreases with long sentences as it tends to forget parts of it, the hidden vector becomes a bottleneck.", "Attention mechanism is built upon the encoder decoder structure we have just analysed. There exist two major differences which we will analyse in the following sections", "In the previous structure we were just passing the hidden state from the last time step. With this new structure we are keeping all the hidden states for every time step.", "As we can see in the image below while previously the output of the encoder was one vector, we now have a matrix composed by each of the hidden states. This solve the problem of not having enough information with just one vector, but it also adds the exact opposite problem, too much information. For each word the decoder wants to translate, it does not need the full matrix because not all the words provide relevant information, so how can the decoder know which part of the matrix to focus on or to pay more attention to?", "When humans describe an image, we don\u2019t analyse it as a whole, but we inspect the relevant parts of the image and pay attention to what we are describing. How does a machine learning model pay attention?", "Exactly the same way. A good example of the attention mechanism can be found in the paper \u2018Show, Attend and Tell\u2019 for Image captioning. Here we have as input as an image and the objective is to generate a sentence describing it. In the figure below we have a heatmap of where the model was paying more attention to when generating a word, the whiter the area, the more attention it was paying, the darker the less.", "When it describes the bird, the attention is on the animal (whiter parts) while when it describes the water its attention shift from the animal to its surrounding.", "Another example can be found in the paper \u2018Neural Machine Translation by jointly learning to align and translate\u2019 by Bzmitry Bahdanau et al in 2015. Let\u2019s analyse the below heatmap which shows where the model is paying more attention to when translating a sentence:", "The x-axis corresponds to the source sentence in English and the y-axis to the generated translation in French. Each pixel shows where the model is shifting its attention, the whiter the area the more attention it was paying, the darker the less (like the previous example).", "For example, to generate the word \u2018accord\u2019 it mainly focuses on \u2018agreement\u2019. We can see the power of the attention mechanism when the model translates \u2018European Economic Area\u2019 to \u2018zone economique europ\u00e9enne\u2019. While in English, adjectives are used before the noun, in French they go after, so in this case the model is shifting the attention to the words that appear afterwards.", "The key question is, how does the model know where to focus? It calculates a score known as alignment score which quantifies how much attention we should give to each input. There exist multiple alignment scores, the most popular are additive (also known as concat, Bahdanau et al 2015), location-base, general and dot-product (Luong 2015). This distinction has led to broader categories like Global/Soft and Local/Hard attention.", "The way we shift our attention has an impact on our interpretation and consequently on the results, the alignment score function we choose has a similar impact.", "This attention mechanism can be applied only once in the model, it is the piece that connects the encoder with the decoder and allows to compare the input and the output sentence as in the previous image. It receives the matrix of hidden states from the encoder and calculates thanks to the alignment score where to pay attention to, a simplified representation looks like this:", "In 2017 in the paper \u2018Attention is all you need\u2019 from the Google team, they introduced a novel architecture known as Transformers which is also the seed for Bidirectional Encoder Representations from Transformers (BERT).", "In this paper they presented two major changes from the previous architecture we have analysed. First, they striped RNN from the picture. Second, they add self-attention mechanism. In the next section we will analyse both decisions and their implications.", "If RNN has shown to be so proficient in the NLP task, why would we want to remove it? RNN works sequentially, this means in order to compute the second word of a sentence (second time step) we need the first hidden vector to be calculated (first time step). Then in order to calculate the hidden state a time t you always have to wait the results from t-1, so we cannot parallelize. Moreover, RNN implies a huge number of calculations requiring a lot of resources.", "Also, RNN with attention has improved the extraction of temporal dependencies over longer sentences but still struggles with long sequences, so we haven\u2019t completely solved the problem.", "If we don\u2019t have RNN what is the model structure? To better understand the new model let\u2019s have a look to the image below:", "As we can see we keep the encoder decoder structure, but we no longer have the attention mechanism connecting to each other. Then where is the attention mechanism calculated? And if we don\u2019t have RNN what is inside the encoder and decoder?", "The encoding component is a stack of encoder, those encoders share the same internal structure. In the paper, the model proposed is built with 6 encoders, the same for the decoder.", "Let\u2019s focus first on the encoder, it is composed of two layers the self-attention mechanism (which we will explore later) and a feed-forward network. Each encoder has both of those layers, so if we previously said we stacked 6 encoders, we have 6 self-attention mechanism just in the encoding phase. This implies we are not limited to apply attention mechanism once, but we can apply it as many times as layers we have.", "The decoder shares the same layers and adds a layer called Encoder-Decoder Attention, we can think of this layer as the attention mechanism used in the second section (Attention Mechanism) to connect the encoder and the decoder.", "The objective of this article is to focus on the attention mechanism, if you would like to better understand transformers, I recommend the following article.", "Thanks to the new structure the model can be parallelized and the calculations required are less resource demanding thus increasing considerably training performance, key task in deep learning. Moreover it extracts temporal dependencies from the whole sentence no matter how long it is. Then, how does this new breakthrough technique work?", "While before we were using attention mechanism to connect both the encoder and the decoder, here we are using attention mechanism to compute dependency relationships between words of the same sentence, the input\u2019s sentence words interact with each other (self). In the image below we can see an example of the results for a translation, more precisely we are focusing on the encoder number 5:", "In this image we have the same sentence represented twice. When the model translates the word \u2018it\u2019 it needs to pay attention to the relevant words and extract the meaning, is this word referring to animal or street? In this example we select the word \u2018it\u2019 (right column) and it highlights the words it is paying more attention to (left column), in this case \u2018The\u2019 and \u2018animal\u2019. The more intense the colour, the more attention the model is paying to this specific word.", "While previously we calculated attention mechanism between input and output sentences (figure 1, figure 2) here we are calculating attention between a sentence and itself.", "The paper keeps fine-tuning the model and also adds the multi-headed attention.", "From this example we know that the word \u2018it\u2019 refers to the animal but what happens to this animal? With just one subspace we are not able to extract so much meaning, here is where it comes the multi-head attention:", "In this image we now have two heads or subspaces (orange and green), the stronger the colour the more attention it is paying. From the first subspace we understand that \u2018it\u2019 refers to animal and from the second subspace we know that the animal is \u2018tired\u2019. Multi-head is the concept of adding dimensions or subspaces to the self-attention mechanism to retrieve more meaning, in the paper they used 8 heads.", "Through this article we have analysed the evolution of attention mechanism. We started with the use of RNN and the encoder decoder structure to solve Seq2Seq problems. The problem with these models is the bottleneck, we expect to extract the whole meaning of the sentence in one hidden state.", "To solve this problem, we keep all the hidden states of the encoder but now have too much information, so we need to pay attention to the relevant parts. Here when we introduced attention mechanism to connect the encoder with the decoder.", "Finally, Transformers neglect the RNN and it mainly focus on self-attention mechanism. In this case attention is not just used once to connect the encoder with the decoder but we can apply it multiple times. Moreover, self-attention is used to compare a sentence with itself rather than input with output.", "Data Scientist & Entrepreneur! Learn Artificial Intelligence and Machine Learning to become a Linchpin \u279c https://bit.ly/36vajnu"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3333637f2eac&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----3333637f2eac--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Nechu BM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56da16d481ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&user=Nechu+BM&userId=56da16d481ba&source=post_page-56da16d481ba----3333637f2eac---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3333637f2eac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&user=Nechu+BM&userId=56da16d481ba&source=-----3333637f2eac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3333637f2eac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&source=-----3333637f2eac---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630", "anchor_text": "built with RNN"}, {"url": "https://arxiv.org/abs/1502.03044", "anchor_text": "Show, Attend and Tell"}, {"url": "https://arxiv.org/abs/1502.03044", "anchor_text": "\u2018Show, Attend and Tell\u2019"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "\u2018Neural Machine Translation by jointly learning to align and translate\u2019"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "\u2018Neural Machine Translation by jointly learning to align and translate\u2019"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Bahdanau et al 2015"}, {"url": "https://arxiv.org/abs/1508.04025", "anchor_text": "Luong 2015"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need\u2019"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "(BERT)"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "following article."}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "link"}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "link"}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb", "anchor_text": "link"}, {"url": "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b", "anchor_text": "How to build an encoder decoder translation model using LSTM with Python and Keras.Follow this step by step guide to build an encoder decoder model and create your own translation model.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/3-reasons-why-i-love-to-be-a-data-scientist-90696ac0d314", "anchor_text": "3 reasons why I love being a Data ScientistIt all comes down to the concept of \u2018meaningful work\u2019 explained by Malcolm Gladwell on his book Outliers.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3333637f2eac---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3333637f2eac---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3333637f2eac---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3333637f2eac---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----3333637f2eac---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3333637f2eac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&user=Nechu+BM&userId=56da16d481ba&source=-----3333637f2eac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3333637f2eac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&user=Nechu+BM&userId=56da16d481ba&source=-----3333637f2eac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3333637f2eac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----3333637f2eac--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56da16d481ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&user=Nechu+BM&userId=56da16d481ba&source=post_page-56da16d481ba----3333637f2eac---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4f6c9f72bf01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&newsletterV3=56da16d481ba&newsletterV3Id=4f6c9f72bf01&user=Nechu+BM&userId=56da16d481ba&source=-----3333637f2eac---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Written by Nechu BM"}, {"url": "https://medium.com/@dbenzaquenm/followers?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "154 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://bit.ly/36vajnu", "anchor_text": "https://bit.ly/36vajnu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56da16d481ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&user=Nechu+BM&userId=56da16d481ba&source=post_page-56da16d481ba----3333637f2eac---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4f6c9f72bf01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac&newsletterV3=56da16d481ba&newsletterV3Id=4f6c9f72bf01&user=Nechu+BM&userId=56da16d481ba&source=-----3333637f2eac---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a?source=author_recirc-----3333637f2eac----0---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=author_recirc-----3333637f2eac----0---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=author_recirc-----3333637f2eac----0---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Nechu BM"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3333637f2eac----0---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a?source=author_recirc-----3333637f2eac----0---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "What is an encoder decoder model?Encoder decoder is a widely used structure in deep learning and through this article, we will understand its architecture"}, {"url": "https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a?source=author_recirc-----3333637f2eac----0---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "\u00b74 min read\u00b7Oct 7, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86b3d57c5e1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-an-encoder-decoder-model-86b3d57c5e1a&user=Nechu+BM&userId=56da16d481ba&source=-----86b3d57c5e1a----0-----------------clap_footer----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a?source=author_recirc-----3333637f2eac----0---------------------39c75570_6303_46a2_af46_7acf84662c01-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86b3d57c5e1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-an-encoder-decoder-model-86b3d57c5e1a&source=-----3333637f2eac----0-----------------bookmark_preview----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3333637f2eac----1---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3333637f2eac----1---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3333637f2eac----1---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3333637f2eac----1---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3333637f2eac----1---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3333637f2eac----1---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3333637f2eac----1---------------------39c75570_6303_46a2_af46_7acf84662c01-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----3333637f2eac----1-----------------bookmark_preview----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3333637f2eac----2---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----3333637f2eac----2---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----3333637f2eac----2---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3333637f2eac----2---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3333637f2eac----2---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3333637f2eac----2---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3333637f2eac----2---------------------39c75570_6303_46a2_af46_7acf84662c01-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----3333637f2eac----2-----------------bookmark_preview----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b?source=author_recirc-----3333637f2eac----3---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=author_recirc-----3333637f2eac----3---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=author_recirc-----3333637f2eac----3---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Nechu BM"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3333637f2eac----3---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b?source=author_recirc-----3333637f2eac----3---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "How to build an encoder decoder translation model using LSTM with Python and Keras.Follow this step by step guide to build an encoder decoder model and create your own translation model."}, {"url": "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b?source=author_recirc-----3333637f2eac----3---------------------39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": "\u00b78 min read\u00b7Oct 20, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&user=Nechu+BM&userId=56da16d481ba&source=-----a31e9d864b9b----3-----------------clap_footer----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b?source=author_recirc-----3333637f2eac----3---------------------39c75570_6303_46a2_af46_7acf84662c01-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=-----3333637f2eac----3-----------------bookmark_preview----39c75570_6303_46a2_af46_7acf84662c01-------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "See all from Nechu BM"}, {"url": "https://towardsdatascience.com/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----0-----------------clap_footer----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----3333637f2eac----0-----------------bookmark_preview----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----1-----------------clap_footer----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----3333637f2eac----1-----------------bookmark_preview----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3333637f2eac----0---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----3333637f2eac----0-----------------bookmark_preview----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----3333637f2eac----1---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----3333637f2eac----1-----------------bookmark_preview----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3333637f2eac----2---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----3333637f2eac----2---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----3333637f2eac----2---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Aleid ter Weel"}, {"url": "https://medium.com/better-advice?source=read_next_recirc-----3333637f2eac----2---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Better Advice"}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3333637f2eac----2---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "10 Things To Do In The Evening Instead Of Watching NetflixDevice-free habits to increase your productivity and happiness."}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3333637f2eac----2---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "\u00b75 min read\u00b7Feb 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-advice%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&user=Aleid+ter+Weel&userId=6ffe087f07e5&source=-----4e270e9dd6b9----2-----------------clap_footer----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3333637f2eac----2---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "204"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&source=-----3333637f2eac----2-----------------bookmark_preview----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----3333637f2eac----3---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----3333637f2eac----3---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----3333637f2eac----3---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Steins"}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----3333637f2eac----3---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "Diffusion Model Clearly Explained!How does AI artwork work? Understanding the tech behind the rise of AI-generated art."}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----3333637f2eac----3---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": "\u00b77 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&user=Steins&userId=a36be384d77d&source=-----cd331bd41166----3-----------------clap_footer----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----3333637f2eac----3---------------------6a2958df_0976_4cad_9c34_9c223cc03665-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&source=-----3333637f2eac----3-----------------bookmark_preview----6a2958df_0976_4cad_9c34_9c223cc03665-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3333637f2eac--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----3333637f2eac--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}