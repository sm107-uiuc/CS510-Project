{"url": "https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f", "time": 1682995452.0008261, "path": "towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f/", "webpage": {"metadata": {"title": "Intuitive Understanding of Attention Mechanism in Deep Learning | by Harshall Lamba | Towards Data Science", "h1": "Intuitive Understanding of Attention Mechanism in Deep Learning", "description": "This is a slightly advanced tutorial and requires basic understanding of sequence to sequence models using RNNs. Please refer my earlier blog here wherein I have explained in detail the concept of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog here", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog", "paragraph_index": 47}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog", "paragraph_index": 54}, {"url": "https://www.tensorflow.org/guide/datasets", "anchor_text": "here", "paragraph_index": 57}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset", "anchor_text": "here", "paragraph_index": 57}, {"url": "https://www.tensorflow.org/guide/keras#model_subclassing", "anchor_text": "here", "paragraph_index": 58}, {"url": "https://www.tensorflow.org/guide/eager", "anchor_text": "here", "paragraph_index": 61}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog", "paragraph_index": 65}, {"url": "https://github.com/hlamba28/NMT-with-Attention-Mechanism", "anchor_text": "here", "paragraph_index": 70}], "all_paragraphs": ["This is a slightly advanced tutorial and requires basic understanding of sequence to sequence models using RNNs. Please refer my earlier blog here wherein I have explained in detail the concept of Seq2Seq models.", "Attention is one of the most influential ideas in the Deep Learning community. Even though this mechanism is now used in various problems like image captioning and others,it was initially designed in the context of Neural Machine Translation using Seq2Seq Models. In this blog post I will consider the same problem as the running example to illustrate the concept. We would be using attention to design a system which translates a given English sentence to Marathi, the exact same example I considered in my earlier blog.", "So what\u2019s wrong with seq2seq models?", "The seq2seq models is normally composed of an encoder-decoder architecture, where the encoder processes the input sequence and encodes/compresses/summarizes the information into a context vector (also called as the \u201cthought vector\u201d) of a fixed length. This representation is expected to be a good summary of the entire input sequence. The decoder is then initialized with this context vector, using which it starts generating the transformed output.", "A critical and apparent disadvantage of this fixed-length context vector design is the incapability of the system to remember longer sequences. Often is has forgotten the earlier parts of the sequence once it has processed the entire the sequence. The attention mechanism was born to resolve this problem.", "Let\u2019s break this down into finer details. Since I have already explained most of the basic concepts required to understand Attention in my previous blog, here I will directly jump into the meat of the issue without any further adieu.", "For the illustrative purposes, I will borrow the same example that I used to explain Seq2Seq models in my previous blog.", "Input (English) Sentence: \u201cRahul is a good boy\u201d", "The only change will be that instead of an LSTM layer that I used in my previous explanation, here I will use a GRU layer. The reason being that LSTM has two internal states (hidden state and cell state) and GRU has only one internal state (hidden state). This will help simplify the the concept and explanation.", "Recall the below diagram in which I summarized the entire process procedure of Seq2Seq modelling.", "In the traditional Seq2Seq model, we discard all the intermediate states of the encoder and use only its final states (vector) to initialize the decoder. This technique works good for smaller sequences, however as the length of the sequence increases, a single vector becomes a bottleneck and it gets very difficult to summarize long sequences into a single vector. This observation was made empirically as it was noted that the performance of the system decreases drastically as the size of the sequence increases.", "The central idea behind Attention is not to throw away those intermediate encoder states but to utilize all the states in order to construct the context vectors required by the decoder to generate the output sequence.", "Let\u2019s name each of the intermediate states of the encoder as below:", "Notice that since we are using a GRU instead of an LSTM, we only have a single state at each time step and not two states, which thus helps to simplify the illustration. Also note that attention is useful specially in case of longer sequences but for the sake of simplicity we will consider the same above example for illustration.", "Recall that these states (h1 to h5) are nothing but vectors of fixed length. To develop some intuition think of these states as vectors which store local information within the sequence. For example;", "h1 stores the information present in the start of the sequence (words like \u2018Rahul\u2019 and \u2018is\u2019) while h5 stores the information present in the later part of the sequence (words like \u2018good\u2019 and \u2018boy\u2019).", "Lets represent our Encoder GRU with the below simplified diagram:", "Now the idea is to utilize all of these local information collectively in order to decide the next sequence while decoding the target sentence.", "When you predict \u201c\u0930\u093e\u0939\u0941\u0932\u201d, its obvious that this name is the result of the word \u201cRahul\u201d present in the input English sentence regardless of the rest of the sentence. We say that while predicting \u201c\u0930\u093e\u0939\u0941\u0932\u201d, we pay more attention to the word \u201cRahul\u201d in the input sentence.", "Similarly while predicting the word \u201c\u091a\u093e\u0902\u0917\u0932\u093e\u201d, we pay more attention to the word \u201cgood\u201d in the input sentence.", "Similarly while predicting the word \u201c\u092e\u0941\u0932\u0917\u093e\u201d, we pay more attention to the word \u201cboy\u201d in the input sentence. And so on..", "As human beings we are quickly able to understand these mappings between different parts of the input sequence and corresponding parts of the output sequence. However its not that straight forward for artificial neural network to automatically detect these mappings.", "Thus the Attention mechanism is developed to \u201clearn\u201d these mappings through Gradient Descent and Back-propagation.", "Let\u2019s get technical and dive into the nitty gritty of Attention mechanism.", "Continuing the above example, let\u2019s say we now want our decoder to start predicting the first word of the target sequence i.e. \u201c\u0930\u093e\u0939\u0941\u0932\u201d", "At time step 1, we can break the entire process into five steps as below:", "Before we start decoding, we first need to encode the input sequence into a set of internal states (in our case h1, h2, h3, h4 and h5).", "Now the hypothesis is that, the next word in the output sequence is dependent on the current state of the decoder (decoder is also a GRU) as well as on the hidden states of the encoder. Thus at each time step, we consider these two things and follow the below steps:", "Step 1 \u2014 Compute a score each encoder state", "Since we are predicting the first word itself, the decoder does not have any current internal state. For this reason, we will consider the last state of the encoder (i.e. h5) as the previous decoder state.", "Now using these two components (all the encoder states and the current state of the decoder), we will train a simple feed forward neural network.", "Recall we are trying to predict the first word in the target sequence i.e. \u201c\u0930\u093e\u0939\u0941\u0932\u201d. As per the idea behind attention, we do not need all the encoder states to predict this word, but we need those encoder states which store information about the word \u201cRahul\u201d in the input sequence.", "As discussed previously these intermediate encoder states store the local information of the input sequence. So it is highly likely that the information of the word \u201cRahul\u201d will be present in the states, let\u2019s say, h1 and h2.", "Thus we want our decoder to pay more attention to the states h1 and h2 while paying less attention to the remaining states of the encoder.", "For this reason we train a feed forward neural network which will learn to identify relevant encoder states by generating a high score for the states for which attention is to be paid while low score for the states which are to be ignored.", "Let s1, s2, s3, s4 and s5 be the scores generated for the states h1, h2, h3, h4 and h5 correspondingly. Since we assumed that we need to pay more attention to the states h1 and h2 and ignore h3, h4 and h5 in order to predict \u201c\u0930\u093e\u0939\u0941\u0932\u201d, we expect the above neural to generate scores such that s1 and s2 are high while s3, s4 and s5 are relatively low.", "Step 2\u2014 Compute the attention weights", "Once these scores are generated, we apply a softmax on these scores to produce the attention weights e1, e2, e3 ,e4 and e5 as shown above. The advantage of applying softmax is as below:", "Thus we get a nice probabilistic interpretation of the attention weights.", "In our case we would expect values like below: (just for intuition)", "This means that while predicting the word \u201c\u0930\u093e\u0939\u0941\u0932\u201d, the decoder needs to put more attention on the states h1 and h2 (since values of e1 and e2 are high) while ignoring the states h3, h4 and h5 (since the values of e3, e4 and e5 are very small).", "Step 3\u2014 Compute the context vector", "Once we have computed the attention weights, we need to compute the context vector (thought vector) which will be used by the decoder in order to predict the next word in the sequence. Calculated as follows:", "Clearly if the values of e1 and e2 are high and those of e3, e4 and e5 are low then the context vector will contain more information from the states h1 and h2 and relatively less information from the states h3, h4 and h5.", "Step 4\u2014 Concatenate context vector with output of previous time step", "Finally the decoder uses the below two input vectors to generate the next word in the sequence", "b) The output word generated from the previous time step.", "We simply concatenate these two vectors and feed the merged vector to the decoder. Note that for the first time step, since there is no output from the previous time step, we use a special <START> token for this purpose. This concept is already discussed in detail in my previous blog.", "The decoder then generates the next word in the sequence (in this case, it is expected to generate \u201c\u0930\u093e\u0939\u0941\u0932\u201d) and along with the output, the decoder will also generate an internal hidden state, and lets call it as \u201cd1\u201d.", "Now in order to generate the next word \u201c\u091a\u093e\u0902\u0917\u0932\u093e\u201d, the decoder will repeat the same procedure which can be summarized in the below diagram:", "The changes are highlighted in green circles", "Once the decoder outputs the <END> token, we stop the generation process.", "Note that unlike the fixed context vector used for all the decoder time steps in case of the traditional Seq2Seq models, here in case of Attention, we compute a separate context vector for each time step by computing the attention weights every time.", "Thus using this mechanism our model is able to find interesting mappings between different parts of the input sequence and corresponding parts of the output sequence.", "Note that during the training of the network, we use teacher forcing in order to input the actual word rather than the predicted word from the previous time step. This concept also has been explained in my previous blog.", "As in case of any NLP task, after reading the input file, we perform the basic cleaning and preprocessing as follows:", "Create a class to map every word to an index and vice-versa for any given vocabulary:", "We use the tf.data input pipeline to create the dataset and then load it later in mini batches. To read more about the input pipeline in TensorFlow, go through the official documentations here and here.", "Now using the model sub-classing API of TensorFlow, we define the model as follows. To read more about model sub classing, read the official documentation here.", "Note: Please read the comments in the below section of the code to get better understanding using the concepts we discussed above. Most of the important lines of the code point to the corresponding section of the explanation given above.", "Define Optimizer, Loss Function and Checkpoints", "Using Eager Execution, we train the network for 10 epochs. To read more about Eager Execution, refer the official documentation here.", "If you are new to heat maps, this is how you can interpret the above plot:", "Notice that the cell at the intersection of \u201cfather\u201d and \u201c\u092c\u093e\u092c\u093e\u0902\u0928\u0940\u201d is pretty dark This means when the decoder predicts the word \u201c\u092c\u093e\u092c\u093e\u0902\u0928\u0940\u201d, it is paying more attention to the input word \u201cfather\u201d (which is what we wanted).", "Similarly while predicting the word \u201c\u0915\u0945\u092e\u0947\u0930\u093e\u201d, the decoder pays a lot of attention to the input word \u201ccamera\u201d. And so on.", "The first thing to be noted is that the translation results are much better than the results from my previous blog. Secondly the model is able to find the correct local mappings between the input and the output sequences which do match with our intuition.", "Given more data and with more hyper parameter tuning, the results and mappings will definitely improve by a good margin.", "Using LSTM layers in place of GRU and adding Bidirectional wrapper on the encoder will also help in improved performance.", "Deep Learning models are generally considered as black boxes, meaning that they do not have the ability to explain their outputs. However, Attention is one of the successful methods that helps to make our model interpretable and explain why it does what it does.", "The only disadvantage of the Attention mechanism is that it is a very time consuming and hard to parallelize system. To solve this problem, Google Brain came up with the \u201cTransformer Model\u201d which uses only Attention and gets rid of all the Convolutional and Recurrent Layers, thus making it highly parallelizable and compute efficient.", "PS: For complete implementation, refer my GitHub repository here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6c9482aecf4f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@harshall.lamba?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@harshall.lamba?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "Harshall Lamba"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa35dfc76e885&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&user=Harshall+Lamba&userId=a35dfc76e885&source=post_page-a35dfc76e885----6c9482aecf4f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c9482aecf4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c9482aecf4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog here"}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog"}, {"url": "https://www.tensorflow.org/guide/datasets", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/guide/keras#model_subclassing", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/guide/eager", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "anchor_text": "blog"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "https://arxiv.org/abs/1409.0473"}, {"url": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb", "anchor_text": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb"}, {"url": "https://www.coursera.org/lecture/nlp-sequence-models/attention-model-lSwVa", "anchor_text": "https://www.coursera.org/lecture/nlp-sequence-models/attention-model-lSwVa"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"}, {"url": "https://www.tensorflow.org/xla/broadcasting", "anchor_text": "https://www.tensorflow.org/xla/broadcasting"}, {"url": "http://www.manythings.org/anki/", "anchor_text": "http://www.manythings.org/anki/"}, {"url": "https://github.com/hlamba28/NMT-with-Attention-Mechanism", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6c9482aecf4f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/sequence-to-sequence?source=post_page-----6c9482aecf4f---------------sequence_to_sequence-----------------", "anchor_text": "Sequence To Sequence"}, {"url": "https://medium.com/tag/attention-mechanism?source=post_page-----6c9482aecf4f---------------attention_mechanism-----------------", "anchor_text": "Attention Mechanism"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----6c9482aecf4f---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/machine-translation?source=post_page-----6c9482aecf4f---------------machine_translation-----------------", "anchor_text": "Machine Translation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c9482aecf4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&user=Harshall+Lamba&userId=a35dfc76e885&source=-----6c9482aecf4f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c9482aecf4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&user=Harshall+Lamba&userId=a35dfc76e885&source=-----6c9482aecf4f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c9482aecf4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6c9482aecf4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6c9482aecf4f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6c9482aecf4f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@harshall.lamba?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@harshall.lamba?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Harshall Lamba"}, {"url": "https://medium.com/@harshall.lamba/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa35dfc76e885&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&user=Harshall+Lamba&userId=a35dfc76e885&source=post_page-a35dfc76e885--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbafcdf1b349d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&newsletterV3=a35dfc76e885&newsletterV3Id=bafcdf1b349d&user=Harshall+Lamba&userId=a35dfc76e885&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}