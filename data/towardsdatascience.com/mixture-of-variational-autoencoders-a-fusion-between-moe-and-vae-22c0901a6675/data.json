{"url": "https://towardsdatascience.com/mixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675", "time": 1682995593.713536, "path": "towardsdatascience.com/mixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675/", "webpage": {"metadata": {"title": "Mixture of Variational Autoencoders \u2014 a Fusion Between MoE and VAE | by Yoel Zeldes | Towards Data Science", "h1": "Mixture of Variational Autoencoders \u2014 a Fusion Between MoE and VAE", "description": "The Variational Autoencoder (VAE) is a paragon for neural networks that try to learn the shape of the input space. Once trained, the model can be used to generate new samples from the input space. If\u2026"}, "outgoing_paragraph_urls": [{"url": "http://anotherdatum.com/vae.html", "anchor_text": "Variational Autoencoder (VAE)", "paragraph_index": 0}, {"url": "http://anotherdatum.com/vae2.html", "anchor_text": "condition the generation process", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/MNIST_database", "anchor_text": "MNIST", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1701.06538", "anchor_text": "Mixture of Experts (MoE)", "paragraph_index": 11}, {"url": "https://www.youtube.com/watch?v=d_GVvIBlWtI", "anchor_text": "YouTube", "paragraph_index": 12}, {"url": "https://www.youtube.com/watch?v=d_GVvIBlWtI", "anchor_text": "video", "paragraph_index": 19}, {"url": "http://anotherdatum.com", "anchor_text": "anotherdatum.com", "paragraph_index": 30}], "all_paragraphs": ["The Variational Autoencoder (VAE) is a paragon for neural networks that try to learn the shape of the input space. Once trained, the model can be used to generate new samples from the input space.", "If we have labels for our input data, it\u2019s also possible to condition the generation process on the label. In the MNIST case, it means we can specify which digit we want to generate an image for.", "Let\u2019s take it one step further\u2026 Could we condition the generation process on the digit without using labels at all? Could we achieve the same results using an unsupervised approach?", "If we wanted to rely on labels, we could do something embarrassingly simple. We could train 10 independent VAE models, each using images of a single digit.", "That would obviously work, but you\u2019re using the labels. That\u2019s cheating!", "OK, let\u2019s not use them at all. Let\u2019s train our 10 models, and just, well, have a look with our eyes on each image before passing it to the appropriate model.", "Hey, you\u2019re cheating again! While you don\u2019t use the labels per se, you do look at the images in order to route them to the appropriate model.", "Fine\u2026 If instead of doing the routing ourselves we let another model learn the routing, that wouldn\u2019t be cheating at all, would it?", "We can use an architecture of 11 modules as follows:", "But how will the manager decide which expert to pass the image to? We could train it to predict the digit of the image, but again \u2014 we don\u2019t want to use the labels!", "Phew\u2026 I thought you\u2019re gonna cheat\u2026", "So how can we train the manager without using the labels? It reminds me of a different type of model \u2014 Mixture of Experts (MoE). Let me take a small detour to explain how MoE works. We\u2019ll need it, since it\u2019s going to be a key component of our solution.", "MoE is a supervised learning framework. You can find a great explanation by Geoffrey Hinton on Coursera and on YouTube. MoE relies on the possibility that the input might be segmented according to the \ud835\udc65\u2192\ud835\udc66 mapping. Have a look at this simple function:", "The ground truth is defined to be the purple parabola for \ud835\udc65<\ud835\udc65\u2019, and the green parabola for \ud835\udc65\u2265\ud835\udc65\u2019. If we were to specify by hand where the split point \ud835\udc65\u2019 is, we could learn the mapping in each input segment independently using two separate models.", "In complex datasets we might not know the split points. One (bad) solution is to segment the input space by clustering the \ud835\udc65\u2019s using K-means. In the two parabolas example, we\u2019ll end up with \ud835\udc65\u2019\u2019 as the split point between two clusters. Thus, when we\u2019ll train the model on the \ud835\udc65<\ud835\udc65\u2019\u2019 segment, it\u2019ll be inaccurate.", "So how can we train a model that learns the split points while at the same time learns the mapping that defines the split points?", "MoE does so using an architecture of multiple subnetworks \u2014 one manager and multiple experts:", "The manager maps the input into a soft decision over the experts, which is used in two contexts:", "First, the output of the network is a weighted average of the experts\u2019 outputs, where the weights are the manager\u2019s output.", "\ud835\udc66\u00a1 is the label, \ud835\udc66\u00af\u00a1 is the output of the i\u2019th expert, \ud835\udc5d\u00a1 is the i\u2019th entry of the manager\u2019s output. When you differentiate the loss, you get these results (I encourage you to watch the video for more details):", "This loss function encourages the experts to specialize in different kinds of inputs.", "Let\u2019s get back to our challenge! MoE is a framework for supervised learning. Surely we can change \ud835\udc66 to be \ud835\udc65 for the unsupervised case, right? MoE\u2019s power stems from the fact that each expert specializes in a different segment of the input space with a unique mapping \ud835\udc65\u2192\ud835\udc66. If we use the mapping \ud835\udc65\u2192\ud835\udc65, each expert will specialize in a different segment of the input space with unique patterns in the input itself.", "We\u2019ll use VAEs as the experts. Part of the VAE\u2019s loss is the reconstruction loss, where the VAE tries to reconstruct the original input image \ud835\udc65:", "A cool byproduct of this architecture is that the manager can classify the digit found in an image using its output vector!", "One thing we need to be careful about when training this model is that the manager could easily degenerate into outputting a constant vector \u2014 regardless of the input in hand. This results in one VAE specialized in all digits, and nine VAEs specialized in nothing. One way to mitigate it, which is described in the MoE paper, is to add a balancing term to the loss. It encourages the outputs of the manager over a batch of inputs to be balanced:", "Enough talking \u2014 It\u2019s training time!", "In the last figure we see what each expert has learned. After each epoch we used the experts to generate images from the distributions they specialized in. The i\u2019th column contains the images generated by the i\u2019th expert.", "We can see that some of the experts easily managed to specialize in a single digit, e.g. \u2014 1. Some got a bit confused by similar digits, such as the expert that specialized in both 3 and 5.", "Using a simple model without a lot of tuning and tweaking, we got reasonable results. Optimally, we would want each expert to specialize in exactly one digit, thus achieving a perfect unsupervised classification via the output of the manager.", "Another interesting experiment would be to turn each expert into a MoE of its own! It will allow us to learn hierarchical parameters by which VAEs should specialize. For instance, some of the digits have multiple ways to be drawn: 7 can be drawn with or without a strikethrough line. This source of variation could be modeled by the MoE in the second level of the hierarchy. But I\u2019ll leave something for a future post\u2026", "Originally published by me at anotherdatum.com.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F22c0901a6675&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----22c0901a6675--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----22c0901a6675--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://yoelzeldes.medium.com/?source=post_page-----22c0901a6675--------------------------------", "anchor_text": ""}, {"url": "https://yoelzeldes.medium.com/?source=post_page-----22c0901a6675--------------------------------", "anchor_text": "Yoel Zeldes"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa609ddb637b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&user=Yoel+Zeldes&userId=a609ddb637b2&source=post_page-a609ddb637b2----22c0901a6675---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22c0901a6675&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22c0901a6675&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://anotherdatum.com/vae.html", "anchor_text": "Variational Autoencoder (VAE)"}, {"url": "http://anotherdatum.com/vae2.html", "anchor_text": "condition the generation process"}, {"url": "https://en.wikipedia.org/wiki/MNIST_database", "anchor_text": "MNIST"}, {"url": "https://arxiv.org/abs/1701.06538", "anchor_text": "Mixture of Experts (MoE)"}, {"url": "https://www.youtube.com/watch?v=d_GVvIBlWtI", "anchor_text": "YouTube"}, {"url": "https://www.youtube.com/watch?v=d_GVvIBlWtI", "anchor_text": "video"}, {"url": "http://anotherdatum.com", "anchor_text": "anotherdatum.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----22c0901a6675---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----22c0901a6675---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----22c0901a6675---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----22c0901a6675---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----22c0901a6675---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22c0901a6675&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&user=Yoel+Zeldes&userId=a609ddb637b2&source=-----22c0901a6675---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22c0901a6675&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&user=Yoel+Zeldes&userId=a609ddb637b2&source=-----22c0901a6675---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22c0901a6675&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----22c0901a6675--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F22c0901a6675&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----22c0901a6675---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----22c0901a6675--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----22c0901a6675--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----22c0901a6675--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----22c0901a6675--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----22c0901a6675--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----22c0901a6675--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----22c0901a6675--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----22c0901a6675--------------------------------", "anchor_text": ""}, {"url": "https://yoelzeldes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://yoelzeldes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yoel Zeldes"}, {"url": "https://yoelzeldes.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "816 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa609ddb637b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&user=Yoel+Zeldes&userId=a609ddb637b2&source=post_page-a609ddb637b2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd4674272ff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675&newsletterV3=a609ddb637b2&newsletterV3Id=fd4674272ff5&user=Yoel+Zeldes&userId=a609ddb637b2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}