{"url": "https://towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0", "time": 1682997011.758569, "path": "towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0/", "webpage": {"metadata": {"title": "XLNet \u2014 a clever language modeling solution | by Ajit Rajasekharan | Towards Data Science", "h1": "XLNet \u2014 a clever language modeling solution", "description": "Unsupervised learning of probability distribution of word sequences in a language by predicting each word within its sentence context in a large corpus, has proven to be useful to create models and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example/answer/Ajit-Rajasekharan", "anchor_text": "bag of words model like word2vec, even though position is not considered", "paragraph_index": 6}, {"url": "https://colab.research.google.com/github/zihangdai/xlnet/blob/master/notebooks/colab_imdb_gpu.ipynb", "anchor_text": "notebook", "paragraph_index": 18}, {"url": "https://github.com/graykode/xlnet-Pytorch", "anchor_text": "pytorch version", "paragraph_index": 18}, {"url": "https://qr.ae/TWtVmo", "anchor_text": "https://qr.ae/TWtVmo", "paragraph_index": 28}], "all_paragraphs": ["Unsupervised learning of probability distribution of word sequences in a language by predicting each word within its sentence context in a large corpus, has proven to be useful to create models and word representations that can then be fine tuned for downstream NLP tasks. Two factors seem to play a key role to boost performance when fine tuning models for downstream syntactic and semantic tasks:", "Satisfying both these design factors poses a challenge (described in detail below) that XLNet addresses by a clever solution. This enables XLNet in part to perform better than previous state-of-art model BERT which satisfies the above two requirements but with some deficiencies. However, XLNet requires more compute power and memory (GPU/TPU memory) in comparison to BERT. It also remains to be seen if improvements to reduce compute/memory requirements etc, will make XLNet good enough from a practical sense to replace BERT in all the NLP tasks that BERT currently excels in (Q&A, text classification, sequence tagging tasks like NER etc.).", "The challenge to create deep bidirectional context representations", "Language modeling, which is essentially learning the probability distribution of word sequences in a language by predicting words within their sentence contexts in a large corpus, can be done in multiple ways as illustrated in figure below", "In either case above, order(or position) of words in the sentence is taken into account (ignoring position would degenerate it to a bag of words model, like word2vec, Glove etc.)", "Models that make use of all the words surrounding a word to predict it, tend do better than the one-sided context prediction models in downstream tasks as one would intuitively expect \u2014 bidirectional context has more information than context from just one side.", "Prediction of a word at a position in a sentence as shown in figure below, say for the word \u201ctimes\u201d involves (regardless of the language model type) , computing a vector for it as a function of its neighbors, but not including itself (prediction becomes trivial if we included the word itself) . The learning process, essentially \u201ctugs\u201d those neighboring vectors such that the computed vector(the prediction) gets closer to the actual word vector at that position. The word vector for \u201ctimes\u201d in turn is involved in the prediction of its neighbors \u2014 \u201cAlaska\u201d for instance. So in a general sense, the training process tugs words that occur together by different amounts, based in part on their position, and this process (magically) tends to bring semantically close words together in high dimensional space (this process is strikingly close to what happens in a bag of words model like word2vec, even though position is not considered in such models). Additionally, the model learns parameters during training that enables it to create context sensitive embeddings for words in sentences including words (in models like BERT and XLNet) never seen during training time.", "Training a model that uses context from both sides however, poses a challenge if we add multiple layers to the model to create deep representations of a word. This is apparent in the illustration of an attention based model below \u2014 the prediction of a word using its context from both sides will indirectly enable seeing the word itself from layer 2 onwards.", "For instance, in layer 1 of an attention based model, composition of the vector for \u201ctimes\u201d is a weighted sum of all its neighbors, excluding itself. That is the computation of output of layer 1 for \u201ctimes\u201d (shown in yellow) does not include \u201ctimes\u201d. However, in layer 2, the weighted sum of hidden states from layer 1, indirectly includes the vector for \u201ctimes\u201d through all hidden state vectors adjacent to the \u201cyellow colored\u201d vector (only one of the indirect paths is shown in red to avoid clutter)", "We can avoid this problem of a word prediction indirectly seeing itself in a multilayered model, by choosing just a subset of hidden states around the hidden state for \u201ctimes\u201d in layer 1, such that they are not dependent on the word vector for \u201ctimes\u201d. Such a handcrafted instance is shown below", "However in order for us to capture the entire bidirectional context in the prediction of the word \u201ctimes\u201d we need more subsets of the bidirectional context to train the model on, where the word does not see itself.", "XLNet addresses this by cleverly choosing such subset contexts like the handcrafted one above such that a word does not \u201csee itself\u201d when it is being predicted. In the example above we have 9 words (assume for now these words are not further split into subwords \u2014 that is they are present as is in vocabulary). We have 9! permutations \u2014 362,880. If we sample from this permutations, pick a permutation, say 612934578 and apply the following rule:-", "A key point to note is the permutation sequence is only used to decide what tokens are involved in the computation of a vector. The order of words in the sentence is untouched \u2014 every word\u2019s position is fixed by a relative position encoding scheme ( we will come back to this later).", "In summary, by predicting a word at a position in a sentence by using a subset of its bidirectional context, and taking into account the word is itself not seen using the permutation scan rule above, we can avoid a word from seeing itself in a multilayered context. This scheme, however requires that we sample enough permutations so that we make full use of the bidirectional context. This is still computationally expensive, so the prediction is done only for a subset of the words in a sentence (this is a hyperparameter K in the model) \u2014 in the paper about 1/6 (~16%) of the words in a sentence are predicted. Interestingly this is roughly the same number of words in a sentence that are also predicted in BERT.", "BERT, however, skirts this problem by masking/corrupting those predicted words. Computationally prediction of all words in a sentence at each layer is a few constant number of matrix multiplies in BERT. XLNet in comparison has to do multiplication operations for each permutation it samples and for each position in the permutation sequence.", "There are a couple of downsides with BERT\u2019s masking approach however, which is not present in XLNet.", "The permutation based dependency rule to capture bidirectional context is perhaps the unique aspect of XLNet. It also leverages off a prior work Transformer XL to handle long sentences, by transferring state across fixed segments (a minor modification is done to ensure the positional information is relative as opposed to absolute). XLNet caches layer states from segments for reuse in subsequent segments. This enables XLNet to handle arbitrarily long sentences. BERT in comparison can only handle fixed length segments. Even though one could choose an arbitrary long fixed length segment in BERT, it becomes impractical due to the GPU/TPU memory requirements.", "Lastly, this is an implementation detail of the permutation based dependency rule - in order predict a word at a position using terms preceding it in permutation sequence, positional information of the target position being predicted needs to be factored into the prediction to disambiguate predictions at two positions using the same set of neighbors. Also while the hidden state vector used for predicting a position cannot include the word vector at that position, we also need a hidden state vector that includes that position while predicting other vectors. This requires the model to learn two vectors for each position, h and g as shown below. h is a function of word vectors and other hidden state vectors in the layer below. g is a function of hidden state vectors and positional information of the predicted word.", "This may be a bit challenging for many of us given the computation and memory resources (GPU/TPUs with sufficient on board memory) required for fine tuning the model on the evaluation tasks, unlike BERT( we can fine tune BERT on a single GPU machine for an NER task in about an hour or two on average) . There is an notebook to test the model on a classification task (I could only run it with a batch size of 2 and got an accuracy of 90% \u2014 they report 92% with a batch size of 8). There is a recent pytorch version that would be useful to try to understand how the model works.", "Oddly there are no benchmark results for any sequence tagging tasks like NER (BERT is current state-of-art for NER) \u2014 perhaps we will see some results for tagging tasks soon.", "The section below can be skipped.", "Why cant a word see itself in the prediction?", "The maximum likelihood estimate for a language model that only uses the left side context to predict the next word (e.g. predicting \u201ctimes\u201d given the words \u201cAlaska is about 12\u201d ).", "Key point to note is the computation of h is a function of all words preceding \u201ctimes\u201d, but not itself. If we include \u201ctimes\u201d too in the computation of h, the model can trivially learn, given e(xt) is the embedding for \u201ctimes\u201d.", "The masked objective of BERT does not include the word itself, but only the replaced mask token. It then predicts the actual word \u201ctimes\u201d given the corrupted sentence \u201cAlaska is about 12 [mask] larger than New York\u201d (assuming only one token is masked)", "BERT in essence skirts the seeing itself problem by replacing the predicted words with masked token and reconstructing them during the training process. However the computation of H includes a mask token that is only present during pre-training.", "XLNet uses a subset of the bidirectional context each time it predicts a word, but avoids the \u201cseeing itself\u201d problem by making sure the computation of \u201cg\u201d only includes tokens that do not see the word being predicted (g is a function of a subset of tokens around it and the predicted words position) . Again it is easy to see the learning problem become trivial if g is a function of the word being predicted, given e(x) includes the embedding for the token to be predicted.", "g in the equation above is computed as follows", "Imported this article manually from Quora https://qr.ae/TWtVmo (automatic import failed for some reason)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fab41e87798b0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7----ab41e87798b0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab41e87798b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab41e87798b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example/answer/Ajit-Rajasekharan", "anchor_text": "bag of words model like word2vec, even though position is not considered"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet"}, {"url": "https://colab.research.google.com/github/zihangdai/xlnet/blob/master/notebooks/colab_imdb_gpu.ipynb", "anchor_text": "notebook"}, {"url": "https://github.com/graykode/xlnet-Pytorch", "anchor_text": "pytorch version"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang et al, June 2019"}, {"url": "https://arxiv.org/pdf/1901.02860.pdf", "anchor_text": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang Dai et al, Jan 2019"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, Oct 2018"}, {"url": "https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58", "anchor_text": "A review of BERT based models"}, {"url": "https://towardsdatascience.com/deconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1", "anchor_text": "Deconstructing BERT"}, {"url": "https://qr.ae/TWtVmo", "anchor_text": "https://qr.ae/TWtVmo"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ab41e87798b0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ab41e87798b0---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ab41e87798b0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----ab41e87798b0---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/ai?source=post_page-----ab41e87798b0---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab41e87798b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----ab41e87798b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab41e87798b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----ab41e87798b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab41e87798b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fab41e87798b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ab41e87798b0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ab41e87798b0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ab41e87798b0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ab41e87798b0--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://ajitrajasekharan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "779 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F974aed893170&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-a-clever-language-modeling-solution-ab41e87798b0&newsletterV3=fd04a90b4be7&newsletterV3Id=974aed893170&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}