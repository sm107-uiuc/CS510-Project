{"url": "https://towardsdatascience.com/recurrent-neural-networks-part-1-498230290534", "time": 1683011286.1488209, "path": "towardsdatascience.com/recurrent-neural-networks-part-1-498230290534/", "webpage": {"metadata": {"title": "Recurrent Neural Networks \u2014 Part 1 | by Andreas Maier | Towards Data Science", "h1": "Recurrent Neural Networks \u2014 Part 1", "description": "In this lecture, we present an introduction to recurrent neural network and highlight the ideas of the Elman cell."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/architectures-part-5-7224dd8fcf39", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/0ZErqh2kE4w", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-2-5f45c1c612c4", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 16}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 16}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 16}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 16}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 16}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 16}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 16}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 16}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 16}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 16}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Character RNNs", "paragraph_index": 17}, {"url": "https://engineering.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/", "anchor_text": "CNNs for Machine Translation", "paragraph_index": 17}, {"url": "http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/", "anchor_text": "Composing Music with RNNs", "paragraph_index": 17}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome everybody to a new session of deep learning. Today we want to look into sequential learning and in particular recurrent neural networks. So far, we only had simple feed-forward networks where we had essentially a fixed size input and would then generate a classification result like \u201ccat\u201d, \u201cdog\u201d, or \u201chamster\u201d.", "If we have sequences like audio, speech, language, or videos that have a temporal context, the techniques that we\u2019ve seen so far are not that very well suited. So, we\u2019re interested now is looking into methods that will be applicable to very long input sequences. Recurrent neural networks (RNNs) are exactly one method to actually do so. After a first review of the motivation, we\u2019ll go ahead and look into simple recurrent neural networks. Then, we\u2019ll introduce the famous long short-term memory units followed by gated recurrent units. After that, we will compare these different techniques and discuss a bit the pros and cons. Finally, we will talk about sampling strategies for our RNNs. Of course, this is way too much for a single video. So, we will talk about the different topics in individual short videos. So, let\u2019s look at the motivation. Well, we had one input for one single image but this is not so great for sequential or time-dependent signals such as speech, music, video, or other sensor data. You could even talk about very simple sensors that measure energy consumption. So snapshots with a fixed length are often not that informative. If you look at a single word you probably have trouble getting the right translation because the context matters. The temporal context is really important and it needs to be modeled appropriately.", "The question is now: \u201cHow can we integrate this context into the network?\u201d The simple approach would be to feed the whole sequence to a big network. This is potentially a bad idea because we have inefficient memory usage. It\u2019s difficult to train or even impossible to train and we would never figure out the difference between spatial and temporal dimensions. We would just handle all the same. Actually maybe it\u2019s not such a bad idea. For rather simple tasks, as you can see in [6] because they actually investigated this and found quite surprising results with CNNs. Well, one problem that you have of course is it won\u2019t be real-time because you need the entire sequence for the processing. So, the approach that we are suggesting in this and the next couple of videos is to model sequential behavior within the architecture and that gives rise to recurrent neural networks.", "So let\u2019s have a look at the simple recurrent neural networks. The main idea is that you introduce a hidden state h subscript t that is carried on over time. So this can be changed but it is essentially connecting back to the original cell A. So, A is our recurrent cell and it has this hidden state that is somehow allowing us to encode what the current temporal information has brought to us. Now, we have some input x subscript t and this will then generate some output y subscript t and by the way, the first models were from the 1970s and early 1980s like Hopfield networks. Here, we will stick with the simple recurrent neural network or Elman network as introduced in [5].", "Now, feed-forward networks only feed information forward. So with recurrent networks, in contrast, we can now model loops, we can model memory and experience, and we learn sequential relationships. So, we can provide continuous predictions as the data comes in. This enables us to process everything in real-time.", "Now, this is again our basic recurrent neural network where we have some input x that is multiplied with some weight. Then, we have the additional input, the hidden state from the previous configuration, and we have essentially a feedback loop where you use the information from the present and the recent past.", "To compute the output y subscript t, we essentially end up with an unfolded structure. So, if you want to evaluate the recurrent unit, what you do is you start with some x\u2080 that you process with your unit. This generates a new result y\u2080 and the new hidden state h\u2080. Now, h\u2080 is fed forward to the next instance of where essentially the weights are coupled. So we have exactly the same copy of the same unit in the next time state but h is, of course, different. So now, we feed in x\u2081 process generate y\u2081 and produce a new hidden state h\u2081 and so on. We can do that until we are at the end of the sequence so each unit passes on the hidden state as an additional input to the successor. This means that the previous input can have an influence on the current output because if we\u2019ve seen x\u2080 and x\u2081, they can have an influence on y subscript (t-1), just because we have encoded the information that we observed x\u2080 and x\u2081 in the hidden state. So, the hidden state allows us to store information and carry it through the entire network to a certain period of time where we then want to choose a specific action.", "So now, the basic question is \u201cHow do we update the hidden state?\u201d and the second question is \u201cHow do we combine input and hidden state to compute the output?\u201d So, we do that opening the cell and looking inside. What you see here is that we essentially concatenate the hidden state with the new input then feed it to a non-linearity, here, a hyperbolic tangent. This produces a new state and from the new state, we\u2019re generating the new output with a sigmoid function. Then, we hand the new hidden state over to the next instance of the same cell. So, we have two activation functions: the hyperbolic tangent for combining the previous state and the current state and the sigmoid non-linearity for generating the output.", "In order to do so, of course, we need weight matrices and those weight matrices are essentially depicted here in red. We can look at them in a little more detail if we want to update the hidden state. This is essentially the hidden state transition matrix W subscript hh times the last hidden state plus the input to hidden state conversion matrix W subscript xh times xsubscript t plus the bias. This is then fed to the non-linearity which then produces the new hidden state.", "Ok, so how can we compute the output? Well, we have produced a new hidden state which means that we now just have another transition matrix that produces a preliminary output from the hidden state. So we have this new W subscript hy that is taking h subscript t and some bias and feeds it to the sigmoid function to produce the final output.", "If we stick to this architecture, we can then essentially realize many different types of architectures. This is determined by the setup of the architecture. So, we can do one to one mapping where we have one input cell and essentially one output cell, but you can also do one too many, many to one, or you can even many do many. So example of one-to-one is image classification. It\u2019s essentially classic feedforward. One-to-many would be image captioning. Many-to-one would be sentiment analysis where you need to observe a certain sequence in order to figure out what sentiment is going on in this situation. Many-to-many is video classification.", "Of course, we can also think about deep RNNS. So far, we only have one hidden layer and we can just use our recurring model which is why we need to go deeper. In this case, it\u2019s more like \u201cYo, dawg I heard you like RNNs, so I put an RNN on your RNN on your RNN.\u201d", "Well, what emerges from this is architectures like this one here. We simply stack multiple hidden units for deep RNNs. So, we can, of course, stack Elman cells on our Elman cells. Then, we would be essentially decoding this using inputs over time, decode them with multiple RNN cells, and produce multiple outputs over time. So, this then gives us access to deep elements.", "Ok. So, this is the simple introduction to deep RNNs and recurrent neural networks. In the next video, we want to go into a little bit more detail and actually see how the training is done and the actual update equations in order to perform the training. So, I hope you like this video and see you in the next one!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced.", "Character RNNsCNNs for Machine TranslationComposing Music with RNNs", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F498230290534&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----498230290534--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----498230290534--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://akmaier.medium.com/?source=post_page-----498230290534--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----498230290534--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----498230290534---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F498230290534&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F498230290534&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "https://towardsdatascience.com/architectures-part-5-7224dd8fcf39", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/0ZErqh2kE4w", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-2-5f45c1c612c4", "anchor_text": "Next Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "https://folkrnn.org/competition/", "anchor_text": "FolkRNN.org"}, {"url": "https://themachinefolksession.org/tunes/", "anchor_text": "MachineFolkSession.com"}, {"url": "https://github.com/IraKorshunova/folk-rnn/blob/master/soundexamples/successes/The%20Glas%20Herry%20Comment%2014128.mp3", "anchor_text": "The Glass Herry Comment 14128"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Character RNNs"}, {"url": "https://engineering.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/", "anchor_text": "CNNs for Machine Translation"}, {"url": "http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/", "anchor_text": "Composing Music with RNNs"}, {"url": "http://www.pnas.org/content/79/8/2554.full.pdf.", "anchor_text": "http://www.pnas.org/content/79/8/2554.full.pdf."}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----498230290534---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----498230290534---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----498230290534---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----498230290534---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "https://medium.com/tag/data-science?source=post_page-----498230290534---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F498230290534&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&user=Andreas+Maier&userId=b1444918afee&source=-----498230290534---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F498230290534&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&user=Andreas+Maier&userId=b1444918afee&source=-----498230290534---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F498230290534&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----498230290534--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F498230290534&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----498230290534---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----498230290534--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----498230290534--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----498230290534--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----498230290534--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----498230290534--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----498230290534--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----498230290534--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----498230290534--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-1-498230290534&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}