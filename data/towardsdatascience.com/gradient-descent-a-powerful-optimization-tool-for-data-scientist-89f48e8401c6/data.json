{"url": "https://towardsdatascience.com/gradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6", "time": 1683018292.450769, "path": "towardsdatascience.com/gradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6/", "webpage": {"metadata": {"title": "Gradient Descent \u2014 A Powerful Optimization tool for Data Scientist | by Steven Loaiza | Towards Data Science", "h1": "Gradient Descent \u2014 A Powerful Optimization tool for Data Scientist", "description": "In Data Science there are machine learning models that make predictions about a particular topic, such as fraud, the price of a house, the cost of an insurance claim, etc. Some of the models that are\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/StevenLoaiza/Machine_Learning/tree/master/optimize", "anchor_text": "GitHub", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Root-mean-square_deviation", "anchor_text": "RMSE", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Residual_sum_of_squares", "anchor_text": "RSS", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "Cross- entropy", "paragraph_index": 6}, {"url": "https://www.linkedin.com/in/stevenloaiza", "anchor_text": "LinkedIn", "paragraph_index": 25}], "all_paragraphs": ["In Data Science there are machine learning models that make predictions about a particular topic, such as fraud, the price of a house, the cost of an insurance claim, etc. Some of the models that are developed to help answer the business question require the Data Scientist to minimize a cost function. Minimizing this cost function is not always a trivial task.", "All the images, code ,and animations can be found on GitHub.", "Below we will preface the article with a simple introduction of the structure of the model optimization problem (The notation is not robust).", "Suppose you have the following model:", "Where f(.) is a function that takes features and parameters as input values. The function then returns an output. The parameter value (\u03b2) is not known to us, therefore we will need to learn the values from the data.", "We want to find the best parameters (\u03b2) so that our predictions (y\u0302) are close to the truth (y). To accomplish the task, we would minimize a loss function. \u03b2^ is the argument that minimizes the loss function L(.).", "The structure of the loss function will depend on the problem you are trying to solve. Some popular loss functions are RMSE, RSS, and Cross- entropy.", "Note: Identifying the structure of the loss function is beyond the scope of the article.", "The direct method of minimizing a cost function is ideal because you can be certain that the answer you receive will be the optimal solution. Take for example the simple linear regression below.", "The objective of the model is to solve for the optimal solution to the \u03b2 parameters. Linear regression has a well-defined closed-form solution for the loss function. The parameters and can be solved directly.", "But, not every loss function has a direct solution. Therefore, we need other methods to solve the parameter values.", "There are complex formulas that are not easy to solve for the unknown parameters. On the other hand, even if a parameter can be solved, the time complexity of performing the calculations could take a long time. There are many approximation methods, but we will focus on gradient descent.", "We deem the method an approximation because we are trying to get close to the true parameter values that will minimize a loss function. There can be times when the algorithm converges on a local solution instead of a global solution (more on that later).", "It is an algorithm that uses the first-order derivative of a loss function to find a local minimum (through iteration). Recall from calculus that the first-order derivative of a function (gradient) will tell you the rate of change of the function at a particular point. Meaning that if we travel along the path of the curve (from left to right) the value of the function will change in the direction of the slope.", "Let us take a look at a simple function. You can easily visualize that the lowest point (minimum) of the curve is at X=0. We would like to find a systematic procedure to uncover this point when it cannot be done arithmetically.", "At this point, we know that the derivative is negative and that we should increase the parameter from -3 to 0. Since the derivative is negative we know that if we travel in the direction of the graph (left to right) the value of Y will decrease.", "Now if we were at this new point the derivative is positive, and we would like to decrease the parameter from +1 to 0. Since the derivative is positive we know that if we travel in the direction of the graph (left to right) the value of Y will increase (We do not want this to happen).", "From the examples above I hope you were able to identify the pattern. If the derivative is negative then we want to increase our parameter value, but if it is positive we want to decrease the parameter. Therefore, we can get closer to the parameter value that yields the lowest point on the curve.", "Let J(.) be the loss function of our model. The gradient of the function is the partial derivative of the function w.r.t the parameter value:", "Below is the pseudo-code for the gradient descent algorithm to update our parameters and get to a solution (see the Appendix for an example via Python). As we noted above when the gradient is negative(positive) the weight parameter should increase(decrease).", "The learning rate is the lambda (\u03bb) parameter in the algorithm. The parameter controls the extent of the change in the parameter value. If the lambda parameter is too high you risk overshooting the optimal parameter (not converging) and if it is too low the learning process could take a long time to get to the optimal parameter.", "This is also referred to as the step size.", "Our objective is to have the parameter converge to the optimal value(s). We know that the minimum point of a function (assuming convexity) is when the gradient is equal to zero. Therefore, our stopping criteria can be when the gradient is sufficiently close to zero. Alternatively, we can set an arbitrary high number of iterations.", "Two issues can occur when implementing the gradient descent algorithm.", "There are many more items to learn when it comes to Gradient Descent. This article serves as an introduction to the algorithm. Other related topics to consider learning are Stochastic Learning, Batch Learning, and Random Weight Initialization.", "Thank you for reading, for your comments and feedback. You can reach me on LinkedIn!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F89f48e8401c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://stevenloaiza.medium.com/?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": ""}, {"url": "https://stevenloaiza.medium.com/?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "Steven Loaiza"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda5ac302aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&user=Steven+Loaiza&userId=da5ac302aef&source=post_page-da5ac302aef----89f48e8401c6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89f48e8401c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89f48e8401c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lazycreekimages?utm_source=medium&utm_medium=referral", "anchor_text": "Michael Dziedzic"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/StevenLoaiza/Machine_Learning/tree/master/optimize", "anchor_text": "GitHub"}, {"url": "https://en.wikipedia.org/wiki/Root-mean-square_deviation", "anchor_text": "RMSE"}, {"url": "https://en.wikipedia.org/wiki/Residual_sum_of_squares", "anchor_text": "RSS"}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "Cross- entropy"}, {"url": "https://unsplash.com/@markkoenig?utm_source=medium&utm_medium=referral", "anchor_text": "Mark K\u00f6nig"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@jodaarba?utm_source=medium&utm_medium=referral", "anchor_text": "Jose Aragones"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.linkedin.com/in/stevenloaiza", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/optimization?source=post_page-----89f48e8401c6---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----89f48e8401c6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----89f48e8401c6---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----89f48e8401c6---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89f48e8401c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&user=Steven+Loaiza&userId=da5ac302aef&source=-----89f48e8401c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89f48e8401c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&user=Steven+Loaiza&userId=da5ac302aef&source=-----89f48e8401c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89f48e8401c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F89f48e8401c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----89f48e8401c6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----89f48e8401c6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----89f48e8401c6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----89f48e8401c6--------------------------------", "anchor_text": ""}, {"url": "https://stevenloaiza.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://stevenloaiza.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Steven Loaiza"}, {"url": "https://stevenloaiza.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "85 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda5ac302aef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&user=Steven+Loaiza&userId=da5ac302aef&source=post_page-da5ac302aef--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6e8471dc97ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-a-powerful-optimization-tool-for-data-scientist-89f48e8401c6&newsletterV3=da5ac302aef&newsletterV3Id=6e8471dc97ff&user=Steven+Loaiza&userId=da5ac302aef&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}