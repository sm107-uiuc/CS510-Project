{"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db", "time": 1683005978.7764962, "path": "towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db/", "webpage": {"metadata": {"title": "Creating Deep Neural Networks from Scratch, an Introduction to Reinforcement Learning | by Abhav Kedia | Towards Data Science", "h1": "Creating Deep Neural Networks from Scratch, an Introduction to Reinforcement Learning", "description": "By the end of the previous article, we had a simple program loop: on every time step, the agent observes the state of the environment and passes that through its neural network (that was randomly\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@abhavkedia/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2", "anchor_text": "first part", "paragraph_index": 1}, {"url": "https://thehappypuppysite.com/reinforcement-in-dog-training/", "anchor_text": "positive or negative reinforcement", "paragraph_index": 3}, {"url": "http://www.briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4", "anchor_text": "blog", "paragraph_index": 41}, {"url": "https://github.com/abhavk/dqn_from_scratch", "anchor_text": "Github", "paragraph_index": 44}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2", "anchor_text": "Part I", "paragraph_index": 51}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-95bcb493a0c9", "anchor_text": "Part III", "paragraph_index": 51}], "all_paragraphs": ["This post is the second of a three part series that will give a detailed walk-through of a solution to the Cartpole-v1 problem on OpenAI gym \u2014 using only numpy from the python libraries.", "The first part laid the foundations, creating an outline of the program and building the feed-forward functions to propagate the state of the environment to its action values. This part will focus on the theory behind cumulative reward and action values in reinforcement learning and on building the backpropagation mechanism. These are foundational pieces for our agent\u2019s learning process.", "By the end of the previous article, we had a simple program loop: on every time step, the agent observes the state of the environment and passes that through its neural network (that was randomly initialized) to obtain predicted values for each action. It then (with probability 1-epsilon) picks the action with the greatest predicted reward. This process is then repeated on the next timestep.", "At the moment, however, the agent does not know what happened after a particular action was taken because there is no feedback. When we are training a pet, for example, it is important that we create feedback loops of positive or negative reinforcement depending on whether their actions are desirable or undesirable. A dog remembers that fetching the ball earned him a treat (a valuable reward) in the past, and is more likely to prioritize fetching the next time a similar situation arises. Similarly, it is important that we implement memory in our program such that the agent can keep track of the rewards and the resulting state after taking actions.", "We\u2019ll do this by adding experiences to the agent\u2019s memory and using them to improve the agent in a process called experience replay. Modify the main program loop as such,", "We will also add the relevant code to the RLAgent, first in the init function,", "Note that this memory implementation uses deque, a simple data structure that allows us to \u2018append\u2019 memories and keeps track of the latest n entries, where n is the size of the deque. Deque is a collection so we also need to add its import statement,", "Finally, we add the experience_replay method that will help the agent learn from experiences to improve its gameplay,", "There\u2019s a lot to unpack in this method, but let\u2019s go through it step by step.", "First, there\u2019s a simple check to see if we have enough experiences to start learning. If not, we wait until we do.", "Next, we randomly sample from all our stored memories and get the indices of update_size memories. For each of these indices, we retrieve (line 7) the memory datum associated with it.", "We then calculate 3 things \u2014", "Once these values have been calculated, we feed the experimental values and the action value predictions to a self.backward function that will calculate the difference between these values and use that to make changes to the weight matrices. The backward function is examined and implemented in a later section on Backpropagation.", "Finally, we update the epsilon (rate of exploration) and learning rate variables for the system. The learning rate is a property used by the backpropagation algorithm that determines the size of the step it takes during learning. Note that we have moved the epsilon update to this method from its original place in the main loop.", "The code block pasted above has 3 calculations on lines 8\u201314. We\u2019ll now go through each of these.", "The first, action_values, is the agent\u2019s current estimate of the value of each action in a given state (prev_obs) \u2014 the same value calculated and used in the select_action function from Part I.", "The second calculation is next_action_values. This is the set of predicted values for both actions from the next state (new_obs), i.e. the state obtained after the agent took an action during the episode that created this memory.", "The next_action_values is only a temporary variable that is used in a subsequent calculation \u2014for experimental_values. This is the target value that our agent has learnt from this particular \u2018experience\u2019, and has two forms:", "Note that these two forms for the experimental value are only applied to the action that was selected during the episode. The other actions (only one in the cartpole problem) are not updated since we do not have any new experimental knowledge about those actions from the given state.", "The experimental_values quantity is crucial. It captures new empirical information that the agent has learned about the value of taking an action in a given state.", "This section will take a step back to formalize some reinforcement learning theory that is implicit in the code we have written so far. First, note that so far we have talked about values of actions in a particular state \u2014 but what does that really mean? How is that value obtained? Why is the variable experimental_values calculated in the way that it is?", "The rest of this section will discuss some Reinforcement Learning theory. They reference freely and borrow heavily from a great paper published in 2015 that showed the power of Deep Q-Networks and used a common architecture to play the Atari 2600 games. If this does not interest you, feel free to skip to the Backpropagation section where we continue with the implementation.", "We begin by defining the \u2018cumulative reward\u2019 or \u2018return\u2019 at a particular time step t as the sum of all future rewards after a time step until the end of the episode. Formally,", "where r\u209c is the immediate reward received at each time step, and \u03b3 is the discounting factor. We also define a quantity called the optimal action value function Q*(s,a) \u2014", "This is a maximum of the \u2018expected\u2019 cumulative reward for the state-action pair (s\u209c,a\u209c) over all policies, where a policy defines what action must be taken in a given state. It represents the \u2018true\u2019 value of an action in a given state \u2014 i.e. the maximum expected return starting from a particular state s, selecting action a, and then playing optimally thereafter with perfect knowledge of the environment. The expectation notation captures the fact that the environment may be stochastic in general.", "The optimal action value function above obeys an important identity called the Bellman equation,", "What this identity means is that if we know the optimal action values for all the actions from the resulting state s\u2019 after taking action a in state s, then the optimal action value function Q*(s,a) is the expected sum of the immediate reward observed after this action and the (discounted) maximum optimal action value over all actions a\u2019 that can be taken from state s\u2019. The expectation notation captures the fact that s\u2019 (and therefore also r) may be probabilistically determined by the initial state s and the action a.", "Most reinforcement learning agents learn by using the Bellman equation as an iterative update, which would in our case be \u2014", "a quantity that will converge to the optimal action value function Q*(s,a) as i tends to infinity. Despite its similarity to line 14 in the code block above, there is a difference between what we are doing and what this equation represents. This iterative update suggests we keep a table of all possible state action pairs and update each value in that table incrementally. This is practically impossible since we have a continuous state space! Instead, we are using a neural network based function approximator Q(s,a;W) to estimate the action-value function, where W represents the weights in our network. This function approximator is called a Q-network \u2014 a deep Q-network or DQN in our case. We have indeed been building a DQN all this time!", "Instead of updating all state-action pairs, it is more practical to iteratively minimize the expectation of the loss function, defined as:", "There are two points to note here,", "Armed with this knowledge, we can now go back and answer the question on the meaning of action values.", "The action-values for a particular state (computed by the forward function) at a given point in time in training are just the model\u2019s estimation of the optimal action value function at that time. In other words, it is the estimation of the total discounted reward the agent expects to receive from a given state for each action. In the specific case of the cartpole problem, this is a discounted sum of the number of timesteps that the agent expects to stay alive from a particular state. The experimental value, on the other hand, is the target value that the network attempts to get closer to on each iteration.", "With the points mentioned above, the last equation for the loss function empirically boils down to \u2014", "with the experimental_values and action_values as we have defined them in the definition of the experience_replay function in the previous section.", "This is the quantity that we will seek to minimize in every iteration (for a given sample). The difference between experimental_values and action_values will form the basis for updating the weights of our network, implemented in the backward function.", "Reinforcement Learning theory helped us define the loss function in the last section as the squared difference between experimental and action values. Now that we have these values, the optimization problem is the same as minimizing the error function in any other neural network, through backpropagation.", "We will now implement the last piece of the RLAgent.experience_replay code, the backward function:", "This function first calculates the difference between the calculated values (the predicted action_values) and the experimental value of taking an action in a state. This error is then \u2018propagated\u2019 backwards through each layer going from the last hidden layer to the input layer, and the weights are updated based on this error. Intuitively, each layer is told how much its estimate of its output differs from its expected output needed to generate the experimental values in the output layer.", "This function calls the NNLayer.backward function:", "First, (lines 4\u20135) if this layer has an associated activation function for its output, we pointwise-multiply the derivative of the activation function (the ReLU function in our case) with the error received from the layer above. The resulting value \u2014 adjusted_mul \u2014 is used for two further calculations in each layer:", "A full treatment of the derivation of the backpropagation algorithm and a step-by-step implementation is given in this instructive blog, but we will not go into the details here. Although my implementation is slightly different from the ones given in that blog, it might be a good exercise to check that the functions I have described actually implement backpropagation correctly.", "The two remaining pieces of the algorithm are the relu_derivative and the NNLayer.update_weights functions. Let\u2019s go through them now.", "The derivative of the ReLU function is pretty straightforward \u2014 the ReLU function returns the input value if the value is greater than 0 and 0 otherwise. So the derivative of this function is just the identity (i.e. 1) function for x>0, and 0 otherwise. Here\u2019s the implementation for a matrix input \u2014", "Great, this simple implementation should be enough to get a convergent algorithm working and train our agent! A working implementation of all the code from these two parts can be found on my Github.", "When we run this program (remember to call env.render() on every timestep!), we get a fairly well-trained agent after a few hundred episodes. The task is considered solved once the agent achieves an average reward above 195 over 100 consecutive episodes. This usually takes between 100-300 time steps to achieve, however, it may take longer on some runs because of unfortunate initialization. A more detailed analysis of convergence guarantees and performance will be done in the next post, but here is a sample full-length run from that agent,", "Great! Seems like our agent is able to balance the pole pretty well. Compare this with the random agent that we started out with in the previous article.", "Although the code that we have built is sufficient to train this agent, it can be optimized for faster convergence and higher stability. This will be the focus of the next post.", "Whew! We got quite a lot done in this part. Here\u2019s a summary:", "There are still a few more tasks to be done here, and we will pick these up in the next and final part of the series.", "See you once again next time!", "Links to Part I & Part III.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science, FinTech and the future of Technology. MA CompSci & Math, University of Oxford."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6bba874019db&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6bba874019db--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6bba874019db--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://abhavkedia.medium.com/?source=post_page-----6bba874019db--------------------------------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=post_page-----6bba874019db--------------------------------", "anchor_text": "Abhav Kedia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F634761ec89d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&user=Abhav+Kedia&userId=634761ec89d3&source=post_page-634761ec89d3----6bba874019db---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@good_citizen?utm_source=medium&utm_medium=referral", "anchor_text": "Humphrey Muleba"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@abhavkedia/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2", "anchor_text": "first part"}, {"url": "https://thehappypuppysite.com/reinforcement-in-dog-training/", "anchor_text": "positive or negative reinforcement"}, {"url": "https://arxiv.org/pdf/1901.00137.pdf", "anchor_text": "here"}, {"url": "http://www.briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4", "anchor_text": "blog"}, {"url": "https://github.com/abhavk/dqn_from_scratch", "anchor_text": "Github"}, {"url": "https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c", "anchor_text": "Adam"}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-part-i-549ef7b149d2", "anchor_text": "Part I"}, {"url": "https://towardsdatascience.com/creating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-95bcb493a0c9", "anchor_text": "Part III"}, {"url": "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf", "anchor_text": "Playing Atari with Deep Reinforcement Learning"}, {"url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf", "anchor_text": "Human-level control through deep reinforcement learning"}, {"url": "http://www.briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4", "anchor_text": "http://www.briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4"}, {"url": "https://arxiv.org/pdf/1901.00137.pdf", "anchor_text": "A Theoretical Analysis of Deep Q-Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----6bba874019db---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6bba874019db---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/backpropagation?source=post_page-----6bba874019db---------------backpropagation-----------------", "anchor_text": "Backpropagation"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----6bba874019db---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6bba874019db---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&user=Abhav+Kedia&userId=634761ec89d3&source=-----6bba874019db---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&user=Abhav+Kedia&userId=634761ec89d3&source=-----6bba874019db---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6bba874019db--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6bba874019db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6bba874019db---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6bba874019db--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6bba874019db--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6bba874019db--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6bba874019db--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6bba874019db--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6bba874019db--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6bba874019db--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6bba874019db--------------------------------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://abhavkedia.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhav Kedia"}, {"url": "https://abhavkedia.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "116 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F634761ec89d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&user=Abhav+Kedia&userId=634761ec89d3&source=post_page-634761ec89d3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5fb20815f9f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-deep-neural-networks-from-scratch-an-introduction-to-reinforcement-learning-6bba874019db&newsletterV3=634761ec89d3&newsletterV3Id=5fb20815f9f8&user=Abhav+Kedia&userId=634761ec89d3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}