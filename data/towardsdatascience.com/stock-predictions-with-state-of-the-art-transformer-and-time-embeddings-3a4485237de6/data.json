{"url": "https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6", "time": 1683010393.52898, "path": "towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6/", "webpage": {"metadata": {"title": "Stock predictions with Transformer and Time Embeddings | Towards Data Science", "h1": "Stock predictions with state-of-the-art Transformer and Time Embeddings", "description": "In my previous post, I have shared my first research results for predicting stock prices which will be subsequently used as input for a deep learning trading bot. While upscaling my datasets to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "[2]", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1907.05321.pdf", "anchor_text": "[2]", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "[4]", "paragraph_index": 31}, {"url": "https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb", "anchor_text": "GitHub", "paragraph_index": 44}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines", "paragraph_index": 45}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms", "paragraph_index": 45}, {"url": "http://www.pinklion.xyz", "anchor_text": "www.pinklion.xyz", "paragraph_index": 48}, {"url": "http://www.pinklion.xyz", "anchor_text": "www.pinklion.xyz", "paragraph_index": 51}, {"url": "http://www.pinklion.xyz", "anchor_text": "www.pinklion.xyz", "paragraph_index": 53}], "all_paragraphs": ["In my previous post, I have shared my first research results for predicting stock prices which will be subsequently used as input for a deep learning trading bot. While upscaling my datasets to thousands of equity tickers equating to almost 1 Terabyte of stock price histories and news articles, I have come to realize that my initial approach of working with neural networks that are comprised of LSTM(Long-Short Term Memory Models) and CNN (Convolutional Neural Network) components has its limitations. Thus, to overcome the limitations I had to implement a Transformer, specialized for stock time-series.", "In recent years Transformers have gained popularity due to their outstanding performance. Combining the self-attention mechanism, parallelization, and positional encoding under one hood provides usually an edge over classical LSTM and CNN models when working on tasks where semantic feature extraction and large datasets are required [1].", "Since I was not able to find a simple Transformer implementation which is customized for time-series sequences that have multiple features e.g. the (Open, High, Low, Close, Volume) features of our stock data I had to implement it myself. In this post, I\u2019ll be sharing my Transformer architecture for stock data as well as what Time Embeddings are and why it essential to use them in combination with time-series.", "For the explanatory purpose of this article, we will be using the IBM stock price history as a simplified version of the 1 Terabyte stock dataset. Nonetheless, you can easily apply to code in this article to a significantly larger dataset. The IBM dataset starts on the 1962\u201301\u201302, ends on the date 2020\u201305\u201324 and contains in total 14699 trading days. Additionally, for every training day, we have the Open, High, Low, and Close price as well as the trading Volume (OHLCV) of the IBM stock.", "The price and volume features are converted into daily stock returns and daily volume changes, a min-max normalized is applied and the time-series is split into a training, validation, and test set. Converting stock prices and volumes into daily change rates increases the stationarity of our dataset. Thus the learnings a model derives from our dataset have a higher validity for future predictions. Here an overview of how the transformed data looks like.", "Lastly, the training, validation, and test sets are separated into individual sequences with a length of 128 days each. For each sequence day, the 4 price features (Open, High, Low, Close) and the Volume feature are present, resulting in 5 features per day. During a single training step, our Transformer model will receive 32 sequences (batch_size = 32) that are 128 days long (seq_len=128) and have 5 features per day as input.", "As the first step of our Transformer implementation, we have to consider how to encode the notion of time which is hidden in our stock prices into our model.", "When processing time-series data, time is an essential feature. However, when processing time-series/sequential data with a Transformer, sequences are forwarded all at once through the Transformer architecture, making it difficult to extract temporal/sequential dependencies. Thus, Transformers that are used in combination with natural language data tend to utilize positional encoding to provide a notion of word order to the model. In detail, the positional encoding is a representation of a word\u2019s value and its position in a sentence, allowing the Transformer to obtain knowledge about a sentence structure and word interdependencies. An example of positional encoding can be found when looking under the hood of the BERT [2] model, which has achieved state-of-the-art performance for many language tasks.", "Similarly, a Transformer requires a notion of time when processing our stock prices. Without Time Embeddings, our Transformer would not receive any information about the temporal order of our stock prices. Hence, a stock price from 2020 can have the same influence on tomorrows\u2019 price prediction as a price from the year 1990. And of course, this would be ludicrous.", "In order to overcome a Transformer\u2019s temporal indifferences, we will implement the approach described in the paper Time2Vec: Learning a Vector Representation of Time [2]. The authors of the paper propose \u201ca model-agnostic vector representation for time, called Time2Vec\u201d. You can think of a vector representation just like a normal embedding layer that can be added to a neural network architecture to improve a model\u2019s performance.", "Boiling the paper down to its essentials, there are two main ideas to consider. Firstly, the authors identified that a meaningful representation of time has to include both periodic and non-periodic patterns. An example of a periodic pattern is the weather that varies over different seasons. In contrast, an example of a non-periodic pattern would be a disease, which occurs with a high probability, the older a patient.", "Secondly, a time representation should have an invariance to time rescaling, meaning that the time representation is not affected by different time increments e.g. (days, hours or seconds) and long time horizons.", "Combining the ideas of periodic and non-periodic patterns as well as the invariance to time rescaling we are presented by the following mathematical definition. No worries, it is easier than it looks and I\u2019ll explain it in detail. \ud83d\ude09", "The time vector/representation t2v is comprised of two components, where \u03c9\u1d62\u03c4 + \u03c6\u1d62 represents the non-periodic/linear and F(\u03c9\u1d62\u03c4 + \u03c6\u1d62) the periodic feature of the time vector.", "Rewriting t2v(\u03c4) = \u03c9\u1d62\u03c4 + \u03c6\u1d62 in a simpler way, the new version y = m\u1d62x + b\u1d62 should look familiar since it is the vanilla version of a linear function that you know from high school. \u03c9 in \u03c9\u1d62\u03c4 + \u03c6\u1d62 is a matrix that defines the slope of our time-series \u03c4 and \u03c6 in simple terms is a matrix that defines where our time-series \u03c4 intersects with the y-axis. Hence, \u03c9\u1d62\u03c4 + \u03c6\u1d62 is nothing more than a linear function.", "The second component F(\u03c9\u1d62\u03c4 + \u03c6\u1d62) represents the periodic feature of the time vector. Just like before we have the linear term \u03c9\u1d62\u03c4 + \u03c6\u1d62 again, however, this time the linear function is wrapped in an additional function F(). The authors experimented with different functions to best describe a periodic relationship (sigmoid, tanh, ReLU, mod, triangle, etc.). In the end, a sine-function achieved the best and most stable performance (cosine achieved similar results). When combining the linear function \u03c9\u1d62\u03c4 + \u03c6\u1d62 with a sine function the 2D representation looks as follows. \u03c6 shifts the sine function along the x-axis and \u03c9 determines the wavelength of the sine function.", "Let\u2019s have a look at an overview of how the accuracy of an LSTM network in combination with different non-linear functions of the time vector (Time2vec) changes. We can clearly see that the ReLU function performs the worst, in contrast, the sine function outperforms every other non-linear function. The reason, the ReLU function has such an unsatisfying results is that a ReLU function is not invariant to time rescaling. The higher the invariant of a function to time rescaling the better the performance.", "Before we start implementing the time embedding let\u2019s take look at the performance difference of a normal LSTM network (blue) and an LSTM+Time2Vec network (red). As you can see the proposed time vector leads across multiple datasets never to worse performance and almost always improves a model\u2019s performance. Equipped with these insights we move on to the implementation.", "Ok, we have discussed how the periodic and non-periodic components of our time vector work in theory, now we\u2019ll implement them in code. In order for the time vector to be easily integrated in any kind of neural network architecture, we\u2019ll define the vector as a Keras layer. Our custom Time2Vector Layer has two sub-functions def build(): and def call():. In def build(): we initiate 4 matrices, 2 for \u03c9 and 2 for\u03c6 since we need a\u03c9 and \u03c6 matrix for both non-periodical (linear) and the periodical (sin) features.", "After having initiated our 4 matrices we define the calculation steps that will be performed once the layer is called, hence the def call(): function.", "The input which will be received by the Time2Vector layer has the following shape (batch_size, seq_len, 5) \u2192 (32, 128, 5). The batch_size defines how many stock price sequences we want to feed into the model/layer at once. The seq_len parameter determines the length of a single stock price sequence. Lastly, the number 5 is derived from the fact that we have 5 features of the daily IBM stock recording (Open price, High price, Low price, Close price, Volume).", "The first calculation step excludes the Volume and takes an average across the Open, High, Low, and Close prices, resulting in the shape (batch_size, seq_len) .", "Next, we calculate the non-periodic (linear) time feature and expand the dimension by 1 again. (batch_size, seq_len, 1)", "The same process is repeated for the periodic time feature, also resulting in the same matrix shape. (batch_size, seq_len, 1)", "The last step that is needed to conclude the time vector calculation is concatenating the linear and periodic time feature. (batch_size, seq_len, 2)", "Combining all steps into one Layer function the code looks as follows.", "Now we know that it is important to provide a notion of time and how to implement a time vector, the next step will be the Transformer. A Transformer is a neural network architecture that uses a self-attention mechanism, allowing the model to focus on the relevant parts of the time-series to improve prediction qualities. The self-attention mechanism consists of a Single-Head Attention and Multi-Head Attention layer. The self-attention mechanism is able to connect all time-series steps with each other at once, leading to the creation of long-term dependency understandings. Finally, all these processes are parallelized within the Transformer architecture, allowing an acceleration of the learning process.", "After having implemented the Time Embeddings we will be using the time vector in combination with IBM\u2019s price and volume features as input for our Transformer. The Time2Vector layer receives the IBM price and volume features as input and calculates the non-periodic and periodic time features. In the subsequent model step, the calculated time features are concatenated with the price and volume features forming a matrix, with the shape (32, 128, 7).", "The IBM time-series plus the time features which we just calculated, form the initial input to the first single-head attention layer. The single-head attention layer takes 3 inputs (Query, Key, Value) in total. For us, each Query, Key, and Value input is representative of the IBM price, volume, and time features. Each Query, Key, and Value input receives a separate linear transformation by going through individual dense layers. Providing the dense layers with 96 output cells was a personal architectural choice.", "After the initial linear transformation, we will calculate the attention score/weights. The attention weights determine how much focus is placed on individual time-series steps when predicting a future stock price. Attention weights are calculated by taking the dot-product of the linearly transformed Query and Key inputs, whereas the transformed Key input has been transposed to make the dot-product multiplication feasible. Then the dot-product is divided by the dimension size of the previous dense layers (96), to avoid exploding gradients. The divided dot-product then goes through the softmax function to yield a set of weights that sum up 1. As the last step, the calculated softmax matrix which determines the focus of each time step is multiplied with the transformed v matrix which concludes the single-head attention mechanism.", "Since illustrations are great for first initial learnings but lack the implementation aspect, I have prepared a clean SingleAttention Keras layer function for you guys \ud83d\ude42.", "To further improve the self-attention mechanism the authors of the paper Attention Is All You Need [4] proposed the implementation of multi-head attention. The functionality of a multi-head attention layer is to concatenate the attention weights of n single-head attention layers and then apply a non-linear transformation with a Dense layer. The illustration below shows the concatenation of 3 single-head layers.", "Having the output of n single-head layers allows the encoding of multiple independent single-head layers transformation into the model. Hence, the model is able to focus on multiple time-series steps at once. Increasing the number of attention heads impacts a model\u2019s ability to capture long-distance dependencies positively. [1]", "The same as above, a clean implementation of the multi-head attention layer.", "The single- and multi-head attention mechanisms (self-attention) are now aggregated into a transformer encoder layer. Each encoder layer incorporates a self-attention sublayer and a feedforward sublayer. The feedforward sublayer consists of two dense layers with ReLU activation in between.", "On a side note, the dense layers can be replaced with 1-dimensional convolutional layers if the Conv-layers have a kernel size and stride of 1. The math of a dense layer and a convolutional layer with the described configuration is the same.", "Each sublayer is followed by a dropout layer, after the dropout, a residual connection is formed by adding the initial Query input to both sublayer outputs. Concluding each sublayer a normalization layer placed after the residual connection addition to stabilize and accelerate the training process.", "Now we have a ready to use Transformer layer, which can be easily stacked to improve a model\u2019s performance. Since we do not need any Transformer decoder layers our implemented Transformer architecture is very similar to the BERT [2] architecture. Although, the differences are the time embeddings and our transformer can handle a 3-dimensional time-series instead of a simple 2-dimensional sequence.", "If you want to dive into the code instead, here we go.", "In conclusion, we first initialize the time embedding layer as well as 3 Transformer encoder layers. After the initialization, we stack a regression head onto the last transformer layer and the training process begins.", "The training process has a total number of 35 epochs. After the training, we can see that our transform model is just predicting a flat line that is centered in between the daily stock price changes. When only using the IBM stock history even a transformer model is merely capable of predicting the linear trend of a stock\u2019s development. Concluding that the historical price and volume data of a stock does only contain enough explanatory value for a linear trend prediction. However, when upscaling the dataset to thousands of stock tickers (1 Terabyte dataset) the results, look quite different \ud83d\ude42.", "As shown above, even the most advanced model architectures are not able to extract non-linear stock predictions from historical stock prices and volumes. However, when applying a simple moving average smoothing effect on the data (window size=10), the model is able to provide significantly better predictions (green line). Instead of predicting the linear trend of the IBM stock, the model is able to predict the up and downs, too. However, when observing carefully you can still see that the model has a large prediction delta on days with extreme daily change rate, hence we can conclude that we still have issues with outliers.", "The disadvantage of applying a moving average effect is that the new dataset is not reflecting our original data anymore. Hence the predictions with the moving average effect cannot be used as input for our trading bot.", "The performance increases due to the smoothing of the moving average effect can be achieved however without applying a moving average though. My latest research has shown that when extending the dataset to a large magnitude of stocks the same performance can be obtained.", "All the code that has been presented in this article is part of a notebook that can be run end-to-end. The notebook can be found on GitHub.", "Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should not rely on an author\u2019s works without seeking professional advice. See our Reader Terms for details.", "In the past I\u2019ve received a tone of feedback and requests to make my financial asset prediction models available and easily usable. It would help people to answer questions such as:", "What are good assets to invest in? What do I put into my financial portfolio?", "Let me introduce you to PinkLion www.pinklion.xyz", "PinkLion is a product that lives on top of my code base to make daily asset data available for thousands of stocks, funds/ETFs, and crypto currencies. In addition, it allows asset analytics and portfolio optimisation on the fly by providing access to underlying prediction models.", "At the moment the number of signups is still limited due to the massive amount of server resources needed for the individual calculations.", "Feel free to give it a try and provide feedback. (It is still in a rough state) www.pinklion.xyz", "None of the content presented in this article constitutes a recommendation that any particular security, portfolio of securities, transaction, or investment strategy is suitable for any specific person. Futures, stocks, and options trading involves substantial risk of loss and is not suitable for every investor. The valuation of futures, stocks, and options may fluctuate, and, as a result, clients may lose more than their original investment.", "Data Scientist \u2014 Who is leveraging the sweet compounding interest rate effect \ud83d\ude42 \u2014 www.pinklion.xyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3a4485237de6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@janschmitz_80340?source=post_page-----3a4485237de6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Jan Schmitz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7214fdeacabf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&user=Jan+Schmitz&userId=7214fdeacabf&source=post_page-7214fdeacabf----3a4485237de6---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a4485237de6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&user=Jan+Schmitz&userId=7214fdeacabf&source=-----3a4485237de6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a4485237de6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&source=-----3a4485237de6---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@morningbrew?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Morning Brew"}, {"url": "https://towardsdatascience.com/t/technology?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "[2]"}, {"url": "https://arxiv.org/pdf/1907.05321.pdf", "anchor_text": "[2]"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "[4]"}, {"url": "https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb", "anchor_text": "GitHub"}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines"}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms"}, {"url": "http://www.pinklion.xyz", "anchor_text": "www.pinklion.xyz"}, {"url": "https://www.pinklion.xyz/", "anchor_text": "PinkLionWe relieve you from the tedious and frustrating parts of your job. No more cleaning data and model trials and\u2026www.pinklion.xyz"}, {"url": "http://www.pinklion.xyz", "anchor_text": "www.pinklion.xyz"}, {"url": "https://arxiv.org/abs/1808.08946", "anchor_text": "https://arxiv.org/abs/1808.08946"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "https://arxiv.org/abs/1810.04805"}, {"url": "https://arxiv.org/pdf/1907.05321.pdf", "anchor_text": "https://arxiv.org/abs/1907.05321"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "https://arxiv.org/abs/1706.03762"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3a4485237de6---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/finance?source=post_page-----3a4485237de6---------------finance-----------------", "anchor_text": "Finance"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3a4485237de6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/stock-market?source=post_page-----3a4485237de6---------------stock_market-----------------", "anchor_text": "Stock Market"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3a4485237de6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a4485237de6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&user=Jan+Schmitz&userId=7214fdeacabf&source=-----3a4485237de6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a4485237de6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&user=Jan+Schmitz&userId=7214fdeacabf&source=-----3a4485237de6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a4485237de6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=post_page-----3a4485237de6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7214fdeacabf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&user=Jan+Schmitz&userId=7214fdeacabf&source=post_page-7214fdeacabf----3a4485237de6---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd50f621b7ec5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&newsletterV3=7214fdeacabf&newsletterV3Id=d50f621b7ec5&user=Jan+Schmitz&userId=7214fdeacabf&source=-----3a4485237de6---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Written by Jan Schmitz"}, {"url": "https://medium.com/@janschmitz_80340/followers?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "1.7K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://www.pinklion.xyz", "anchor_text": "www.pinklion.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7214fdeacabf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&user=Jan+Schmitz&userId=7214fdeacabf&source=post_page-7214fdeacabf----3a4485237de6---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd50f621b7ec5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6&newsletterV3=7214fdeacabf&newsletterV3Id=d50f621b7ec5&user=Jan+Schmitz&userId=7214fdeacabf&source=-----3a4485237de6---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-beginning-of-a-deep-learning-trading-bot-part1-95-accuracy-is-not-enough-c338abc98fc2?source=author_recirc-----3a4485237de6----0---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=author_recirc-----3a4485237de6----0---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=author_recirc-----3a4485237de6----0---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Jan Schmitz"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3a4485237de6----0---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-beginning-of-a-deep-learning-trading-bot-part1-95-accuracy-is-not-enough-c338abc98fc2?source=author_recirc-----3a4485237de6----0---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "The beginning of a deep learning trading bot \u2014 95% accuracy is not enoughFollow me on my research journey where I develop a deep learning-based trading system."}, {"url": "https://towardsdatascience.com/the-beginning-of-a-deep-learning-trading-bot-part1-95-accuracy-is-not-enough-c338abc98fc2?source=author_recirc-----3a4485237de6----0---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "\u00b710 min read\u00b7May 17, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc338abc98fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-beginning-of-a-deep-learning-trading-bot-part1-95-accuracy-is-not-enough-c338abc98fc2&user=Jan+Schmitz&userId=7214fdeacabf&source=-----c338abc98fc2----0-----------------clap_footer----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-beginning-of-a-deep-learning-trading-bot-part1-95-accuracy-is-not-enough-c338abc98fc2?source=author_recirc-----3a4485237de6----0---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc338abc98fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-beginning-of-a-deep-learning-trading-bot-part1-95-accuracy-is-not-enough-c338abc98fc2&source=-----3a4485237de6----0-----------------bookmark_preview----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3a4485237de6----1---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3a4485237de6----1---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3a4485237de6----1---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3a4485237de6----1---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3a4485237de6----1---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3a4485237de6----1---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3a4485237de6----1---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----3a4485237de6----1-----------------bookmark_preview----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3a4485237de6----2---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----3a4485237de6----2---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----3a4485237de6----2---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3a4485237de6----2---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3a4485237de6----2---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3a4485237de6----2---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----3a4485237de6----2---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----3a4485237de6----2-----------------bookmark_preview----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/illumination/15-free-tools-to-bootstrap-your-business-end-to-end-8f85c0a08af?source=author_recirc-----3a4485237de6----3---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=author_recirc-----3a4485237de6----3---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=author_recirc-----3a4485237de6----3---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "Jan Schmitz"}, {"url": "https://medium.com/illumination?source=author_recirc-----3a4485237de6----3---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "ILLUMINATION"}, {"url": "https://medium.com/illumination/15-free-tools-to-bootstrap-your-business-end-to-end-8f85c0a08af?source=author_recirc-----3a4485237de6----3---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "15 Free Tools to Bootstrap Your Business End-to-EndSurprisingly, the best tools in life are free and you\u2019ve probably never heard of them."}, {"url": "https://medium.com/illumination/15-free-tools-to-bootstrap-your-business-end-to-end-8f85c0a08af?source=author_recirc-----3a4485237de6----3---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": "\u00b710 min read\u00b7Nov 14, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fillumination%2F8f85c0a08af&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fillumination%2F15-free-tools-to-bootstrap-your-business-end-to-end-8f85c0a08af&user=Jan+Schmitz&userId=7214fdeacabf&source=-----8f85c0a08af----3-----------------clap_footer----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/illumination/15-free-tools-to-bootstrap-your-business-end-to-end-8f85c0a08af?source=author_recirc-----3a4485237de6----3---------------------fa917bfa_879f_4fd5_b052_9e9e3487f887-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f85c0a08af&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fillumination%2F15-free-tools-to-bootstrap-your-business-end-to-end-8f85c0a08af&source=-----3a4485237de6----3-----------------bookmark_preview----fa917bfa_879f_4fd5_b052_9e9e3487f887-------", "anchor_text": ""}, {"url": "https://medium.com/@janschmitz_80340?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "See all from Jan Schmitz"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Temporal Fusion Transformer: Time Series Forecasting with Deep Learning \u2014 Complete TutorialCreate accurate & interpretable predictions"}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "\u00b712 min read\u00b7Nov 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd32c1e51cd91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftemporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----d32c1e51cd91----0-----------------clap_footer----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "15"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd32c1e51cd91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftemporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91&source=-----3a4485237de6----0-----------------bookmark_preview----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://trading-data-analysis.pro/the-trend-is-your-friend-for-your-trading-and-for-neural-prophet-how-to-find-it-part-2-e1415cf4d90c?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@peteramaral?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@peteramaral?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Peter Amaral"}, {"url": "https://trading-data-analysis.pro/?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Trading Data Analysis"}, {"url": "https://trading-data-analysis.pro/the-trend-is-your-friend-for-your-trading-and-for-neural-prophet-how-to-find-it-part-2-e1415cf4d90c?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "The Trend Is Your Friend. For Your Trading And For Neural Prophet. Tuning Changepoints(Part 2).HYPERPARAMETER TUNING"}, {"url": "https://trading-data-analysis.pro/the-trend-is-your-friend-for-your-trading-and-for-neural-prophet-how-to-find-it-part-2-e1415cf4d90c?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "\u00b75 min read\u00b7Nov 10, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftrading-data-analysis%2Fe1415cf4d90c&operation=register&redirect=https%3A%2F%2Ftrading-data-analysis.pro%2Fthe-trend-is-your-friend-for-your-trading-and-for-neural-prophet-how-to-find-it-part-2-e1415cf4d90c&user=Peter+Amaral&userId=7bb5b593cdd&source=-----e1415cf4d90c----1-----------------clap_footer----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://trading-data-analysis.pro/the-trend-is-your-friend-for-your-trading-and-for-neural-prophet-how-to-find-it-part-2-e1415cf4d90c?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1415cf4d90c&operation=register&redirect=https%3A%2F%2Ftrading-data-analysis.pro%2Fthe-trend-is-your-friend-for-your-trading-and-for-neural-prophet-how-to-find-it-part-2-e1415cf4d90c&source=-----3a4485237de6----1-----------------bookmark_preview----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@mrconnor/forecasting-the-stock-market-using-lstm-will-it-rise-tomorrow-94ff6b6a34b6?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@mrconnor?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@mrconnor?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Connor Roberts"}, {"url": "https://medium.com/@mrconnor/forecasting-the-stock-market-using-lstm-will-it-rise-tomorrow-94ff6b6a34b6?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Forecasting the stock market using LSTM; will it rise tomorrow.Using a machine learning model, this tutorial will predict a stock\u2019s future value in real time with high accuracy on a stock exchange. To\u2026"}, {"url": "https://medium.com/@mrconnor/forecasting-the-stock-market-using-lstm-will-it-rise-tomorrow-94ff6b6a34b6?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "\u00b76 min read\u00b7Jan 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F94ff6b6a34b6&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mrconnor%2Fforecasting-the-stock-market-using-lstm-will-it-rise-tomorrow-94ff6b6a34b6&user=Connor+Roberts&userId=14ca88ad1cb0&source=-----94ff6b6a34b6----0-----------------clap_footer----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@mrconnor/forecasting-the-stock-market-using-lstm-will-it-rise-tomorrow-94ff6b6a34b6?source=read_next_recirc-----3a4485237de6----0---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F94ff6b6a34b6&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mrconnor%2Fforecasting-the-stock-market-using-lstm-will-it-rise-tomorrow-94ff6b6a34b6&source=-----3a4485237de6----0-----------------bookmark_preview----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/unlocking-insights-from-multivariate-data-with-the-temporal-fusion-transformer-b711f0396f57?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@brent.larzalere?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@brent.larzalere?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Brent Larzalere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/unlocking-insights-from-multivariate-data-with-the-temporal-fusion-transformer-b711f0396f57?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Unlocking Insights from Multivariate Data with the Temporal Fusion TransformerProbabilistic Forecast of a Multivariate Time Series using the Temporal Fusion Transformer & PyTorch Lightning"}, {"url": "https://pub.towardsai.net/unlocking-insights-from-multivariate-data-with-the-temporal-fusion-transformer-b711f0396f57?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "\u00b712 min read\u00b7Jan 27"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Fb711f0396f57&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Funlocking-insights-from-multivariate-data-with-the-temporal-fusion-transformer-b711f0396f57&user=Brent+Larzalere&userId=c8d1e1ec90e5&source=-----b711f0396f57----1-----------------clap_footer----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/unlocking-insights-from-multivariate-data-with-the-temporal-fusion-transformer-b711f0396f57?source=read_next_recirc-----3a4485237de6----1---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb711f0396f57&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Funlocking-insights-from-multivariate-data-with-the-temporal-fusion-transformer-b711f0396f57&source=-----3a4485237de6----1-----------------bookmark_preview----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538?source=read_next_recirc-----3a4485237de6----2---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=read_next_recirc-----3a4485237de6----2---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=read_next_recirc-----3a4485237de6----2---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----3a4485237de6----2---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538?source=read_next_recirc-----3a4485237de6----2---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "N-BEATS : Time-Series Forecasting with Neural Basis ExpansionA Deep Learning model that provides accuracy and interpretability"}, {"url": "https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538?source=read_next_recirc-----3a4485237de6----2---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "\u00b712 min read\u00b7Nov 25, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf09ea39f538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fn-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----af09ea39f538----2-----------------clap_footer----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538?source=read_next_recirc-----3a4485237de6----2---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf09ea39f538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fn-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538&source=-----3a4485237de6----2-----------------bookmark_preview----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----3a4485237de6----3---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----3a4485237de6----3---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----3a4485237de6----3---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----3a4485237de6----3---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----3a4485237de6----3---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "Feature Importance with Deep Neural Network for CryptocurrenciesA FinRL-Meta Tutorial for NeurIPS 2022 Datasets and Benchmarks"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----3a4485237de6----3---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": "\u00b710 min read\u00b7Jan 2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----f06191e2d562----3-----------------clap_footer----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----3a4485237de6----3---------------------58fc44c5_fef0_4073_a85f_18a0a896b99b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&source=-----3a4485237de6----3-----------------bookmark_preview----58fc44c5_fef0_4073_a85f_18a0a896b99b-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3a4485237de6--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----3a4485237de6--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}