{"url": "https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2", "time": 1682994144.148834, "path": "towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2/", "webpage": {"metadata": {"title": "A Journey Into Big Data with Apache Spark: Part 1 | by Ash Broadley | Towards Data Science", "h1": "A Journey Into Big Data with Apache Spark: Part 1", "description": "Using Docker, we\u2019ll build a cluster that we can scale automatically using docker-compose."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Domain-specific_language", "anchor_text": "Domain Specific Language", "paragraph_index": 4}, {"url": "https://12factor.net", "anchor_text": "12factor", "paragraph_index": 5}, {"url": "https://www.docker.com/get-started", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://hub.docker.com/_/openjdk/", "anchor_text": "already available", "paragraph_index": 10}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080", "paragraph_index": 25}, {"url": "https://docs.docker.com/compose/", "anchor_text": "Docker Compose", "paragraph_index": 40}], "all_paragraphs": ["My journey into Big Data began in May 2018. I\u2019ve been a Software Engineer for over a decade, being both hands on and leading the development of some of Sky Betting & Gaming\u2019s biggest products and the services that underpin them. During this time I\u2019ve learned a lot about how to build and operate applications that scale, at scale \u2014 from the need for logs and metrics, to performance optimisation and maintainability. When I was asked to become the Engineering Manager for our Data Tribe, I was both excited and extremely nervous \u2014 I\u2019d lead small teams before, but knew nothing about Big Data. So I decided to learn!", "\u201cWhere do you start with something like this?\u201d is often the question that I would ask myself. The Big Data ecosystem is, well, big. So I asked one of the Principal Engineers within my team what they would recommend and Apache Spark came top of their list.", "Apache Spark\u2122 is a unified analytics engine for large-scale data processing.", "In Human terms, Spark is a distributed computing framework that has a common interface across multiple languages for things like SQL & MapReduce when querying in-memory datasets. Being distributed, Spark can work with extremely large datasets across a cluster of machines.", "From an Engineering perspective, Spark is available in multiple languages: Scala, Java, Python and R. This seemed like a pretty big win as it doesn\u2019t limit what we can hire for to a specific language. Spark has it\u2019s own DSL (Domain Specific Language) that\u2019s the same across all implementations, meaning there\u2019s a common language despite the choice of implementation language. Our Data Science Squad uses a combination of PySpark and SparkR and the wider Tribe uses Scala \u2014 we let Squads (mostly) choose their own tooling.", "Spark\u2019s use of Data Frames lends itself very well to the usual Software Engineering and application design principles such as Unit Testing, Data Modelling, Single Responsibility Principle and so on. Again, another big win. I can continue to build applications in the same way as I\u2019ve always done (I\u2019m a big fan of the 12factor principles).", "Spark also comes with an SQL interface, which is usually familiar to most Programmers/Engineers that have ever needed to store & query data somewhere.", "Spark even has a Streaming library available, if that\u2019s what you\u2019re into. Not that I will be immediately (although I\u2019m huge fan of Apache Kafka), but it does seem quite attractive for future learnings (and I suppose business use cases).", "Apache Spark seemed quite daunting (as anything new does) and again, I found myself asking \u201cWhere do I start?\u201d I\u2019m a big believer that understanding things from the ground up makes you a better Engineer. That, plus the need to actually practice using Spark somewhere lead me to build a small local Spark standalone cluster.", "Note: I\u2019m a big fan of Docker. It gives you a way to package things and distribute in a very portable way. The only dependency I need to install on my own machine is Docker itself. I\u2019ll be using Docker in all my examples, so you might want to get it from here. I\u2019m also running OSX, which has some minor limitations with Docker, but we can work around them.", "Our first task is to find a suitable Docker image that contains a version of Java. Being a fan of Open Source Software, I chose to use OpenJDK. I\u2019m not a fan of bloat and decided that Alpine Linux was the way to go. Luckily, there\u2019s an image we can use already available. Let\u2019s begin creating our Dockerfile.", "Create a new file in an empty directory called Dockerfile. Add the following line at the very top:", "Now we can build our image, which uses the OpenJDK image as a base. Build the image by running the following:", "This will pull the base image and create our own version of the image. You\u2019ll see something akin to the following output:", "Instead of noting the output hash each time we build the image, this time we\u2019ll tag the image, giving it name so it\u2019s easier to work with (replacing $MYNAME with your own name, of course). This is a slight tweak to the build command above:", "Now instead of using the generated hash to refer to the built image, we can refer to it using the tag we gave it. Test by running a container using the tag name like so:", "This image isn\u2019t of much use yet, as all it contains is Java. Let\u2019s install some utilities that we\u2019re going to need to download and install Spark. We\u2019re going to need wget to download the archive containing Spark and tar to extract files from that archive. We\u2019ll also need bash to run some things. On a new line in the Dockerfile, add the following (we need to include--update so that Alpine has a fresh list of repositories to download binaries from):", "Now rebuild the image and watch as wget, tar and bash are installed.", "We can now download and install Spark. We\u2019ll be using the latest version of Spark (2.4.0) based on Scala 2.11 and Hadoop 2.7 (Don\u2019t worry about Hadoop, we\u2019re not interested in that\u2026 yet). Add the following to a new line in your Dockerfile and build:", "Docker is pretty clever and will reuse the layers that we previously built, so all it has to do at this point is download the Spark archive. Once built, the image will contain the Spark archive ready for us to install. Installation is a matter of simply extracting Spark from the archive and placing it somewhere sensible. Add another line to the Dockerfile to do this like so (I break the command on to multiple lines for readability. The rm is to simply clean up the downloaded archive) and then build:", "Congratulations! You\u2019ve now downloaded and installed Spark on a Docker image. Time to test it! Let\u2019s start a container and get a shell. Take note of the ID output at the end of building the image, as we\u2019ll be using this now. In your shell run the following command:", "We\u2019ll then end up in a shell running inside the container, with which we use to start a Spark Master. Spark needs a couple of options when starting to startup successfully. These are the port for the Master to listen on, the port of the WebUI and the hostname of the Master:", "And hopefully you should see a bunch of log output! If so, CONGRATULATIONS! You\u2019ve successfully started running a Spark Master.", "The next step is to get some Workers added to the cluster, but first, we need to set some configuration on the Master so that the Worker can talk to it. To make things easy, we\u2019ll give the Master a proper name and expose the Master port (the --port option in the last command) and also make the WebUI available to us. Stop the Master and drop out of the container by using CTRL+C and then CTRL+D. You should now be in your local shell. Simply tweak the docker run command to add the --name, --hostname and -p options as per below and run:", "Run docker ps and you should see the container running with an output similar to this (I\u2019ve removed some output to make it fit in the code block):", "In the container, re-run the command to start the Spark Master and once it\u2019s up, you should be able to browse to http://localhost:8080 and see the WebUi for the cluster, as per the screenshot below.", "As I mentioned, I\u2019m using Docker for Mac, which makes DNS painful and accessing the container by IP nigh on impossible without running a local VPN server or something to work around the issue \u2014 that\u2019s beyond the scope of this post. Luckily, Docker has its own networking capability (the specifics of which are out of scope of this post, too) which we\u2019ll use to create a network for the local cluster to sit within. Creating a network is pretty simple and is done by running the following command:", "We don\u2019t need to specify any particular options as the defaults are fine for our use case. Now we need to recreate our Master to attach it to the new network. Run docker stop spark-master and docker rm spark-master to remove the current instance of the running Master. To recreate the Master on the new network we can simply add the --network option to docker run, as per the below:", "This is really no different to the first time we ran the Spark Master, except it uses a newly defined network that we can use to attach Workers to, to make the cluster work \ud83d\udc4d. Now the Master is up and running, let\u2019s add a Worker node to it. This is where the magic of Docker really shines through. To create a Worker and add it to the cluster, we can simply launch a new instance of the same docker image and run the command to start the Worker. We\u2019ll need to give the Worker a new name, but other than the command remains largely the same:", "And to start the Spark Worker on the container, we simply run:", "When it\u2019s started and connect to the master, you should see the last line of the output being:", "And the Master will output the following line:", "Congratulations! You\u2019ve setup a Spark cluster using Docker!", "To check it works, we can load the Master WebUI and we should see the Worker node listed under the \u201cWorkers\u201d section, but this only really confirms the log output from attaching the Worker to the Master.", "To be a true test, we need to actually run some Spark code across the cluster. Let\u2019s run a new instance of the docker image so we can run one of the examples provided when we installed Spark. Again, we can reuse the existing docker image and simply launch a new instance to use as the driver (the thing that submits the application to the cluster). This one doesn\u2019t need the--hostname,--name and -p options:", "In the container, we can then submit an application to the cluster by running the following command:", "The example provided works out what Pi is utilising the cluster.", "While the application is running, you should be able to see that in the Spark Master WebUI:", "Once complete, you\u2019ll see the value of Pi output in the logs:", "And you\u2019ll see the application reporting as complete in the WebUI too:", "Docker Compose is a neat utility provided with Docker that we can use as an orchestration tool so that we don\u2019t have to keep running commands ourselves in a number of terminal windows. We essentially stitch together the various commands and parameterise some things which means we can simply run docker-compose up and the cluster starts running.", "To enable this, we can create some scripts to copy to the image and run at container start. The first one we\u2019ll create will be to setup the Spark Master. Create a new file called start-master.sh and add the following command:", "Instead of specifying the IP, Master and WebUI ports directly in the script, we\u2019ve parameterised them, meaning we can provide them as environment variables. Thus giving greater flexibility around the configuration (ports) the Master listens on. To get the script onto the image, we need to copy it. Before we do that however, we need to ensure the script is executable. Simply run chmod +x start-worker.sh to make it so. Now to add the script into the image, in the Dockerfile, below the last RUN command, add the following:", "Before we rebuild the image, we\u2019ll create a similar script for the Worker too. Create a new file called start-worker.sh and add the following command:", "Again, we\u2019ve parameterised the configuration to give greater flexibility. Make the new script executable (chmod +x start-worker.sh) and add another COPY line to the Dockerfile to add the script to the image:", "Rebuild and run. If all is well, you\u2019ll land in a shell in the container that has the scripts located in the root of the filesystem:", "Come back out and we\u2019ll hook this up with docker-compose to orchestrate a cluster.", "Create a new file called docker-compose.yml and enter the following:", "All we\u2019ve done here is specify the image name to use, set a hostname and container name, expose the right ports and attach to the network we create at the bottom, set some environment configuration and the command to run upon starting. We\u2019ve also set the Worker to depend on the Master being up and running.", "To bring the cluster up, we simply run docker-compose up. One of the great things about Docker Compose is that we can scale the Workers by simply adding a --scale option to the compose command. Say we want 3 Worker nodes, we run:", "And et voila! We\u2019re done. Tune in next time when we start to learn the basics by building our own application in Scala.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interests include, but not limited to: Google Cloud (Certified Arch & Data Eng), Programming, Squash, Technology, Space, Food. Views are my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5dfcc2bccdd2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ls12styler?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ls12styler?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "Ash Broadley"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6dcc427e3e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&user=Ash+Broadley&userId=6dcc427e3e6a&source=post_page-6dcc427e3e6a----5dfcc2bccdd2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5dfcc2bccdd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5dfcc2bccdd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Domain-specific_language", "anchor_text": "Domain Specific Language"}, {"url": "https://12factor.net", "anchor_text": "12factor"}, {"url": "https://www.docker.com/get-started", "anchor_text": "here"}, {"url": "https://hub.docker.com/_/openjdk/", "anchor_text": "already available"}, {"url": "http://apache.mirror.anlx.net/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz", "anchor_text": "http://apache.mirror.anlx.net/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz"}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080"}, {"url": "https://docs.docker.com/compose/", "anchor_text": "Docker Compose"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----5dfcc2bccdd2---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/big-data?source=post_page-----5dfcc2bccdd2---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/docker?source=post_page-----5dfcc2bccdd2---------------docker-----------------", "anchor_text": "Docker"}, {"url": "https://medium.com/tag/docker-compose?source=post_page-----5dfcc2bccdd2---------------docker_compose-----------------", "anchor_text": "Docker Compose"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5dfcc2bccdd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&user=Ash+Broadley&userId=6dcc427e3e6a&source=-----5dfcc2bccdd2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5dfcc2bccdd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&user=Ash+Broadley&userId=6dcc427e3e6a&source=-----5dfcc2bccdd2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5dfcc2bccdd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5dfcc2bccdd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5dfcc2bccdd2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5dfcc2bccdd2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ls12styler?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ls12styler?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ash Broadley"}, {"url": "https://medium.com/@ls12styler/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "370 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6dcc427e3e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&user=Ash+Broadley&userId=6dcc427e3e6a&source=post_page-6dcc427e3e6a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa575d6e4f79a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2&newsletterV3=6dcc427e3e6a&newsletterV3Id=a575d6e4f79a&user=Ash+Broadley&userId=6dcc427e3e6a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}