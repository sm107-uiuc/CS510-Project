{"url": "https://towardsdatascience.com/xgboost-b736c2ab10ce", "time": 1683003863.363442, "path": "towardsdatascience.com/xgboost-b736c2ab10ce/", "webpage": {"metadata": {"title": "XGBoost. The GBM which shook the world | by Manu Joseph | Towards Data Science", "h1": "XGBoost", "description": "Now let\u2019s get the elephant out of the way \u2014 XGBoost. This is the most popular cousin in the Gradient Boosting Family. XGBoost with its blazing fast implementation stormed into the scene and almost\u2026"}, "outgoing_paragraph_urls": [{"url": "https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters", "anchor_text": "here", "paragraph_index": 14}, {"url": "http://netlib.org/linalg/html_templates/node92.html", "anchor_text": "Compressed column (CSC) format", "paragraph_index": 25}, {"url": "https://xgboost.readthedocs.io/en/latest/parameter.html", "anchor_text": "official documentation", "paragraph_index": 29}, {"url": "http://towardsdatascience.com/tagged/the-gradient-boosters", "anchor_text": "The Gradient Boosters", "paragraph_index": 34}], "all_paragraphs": ["Now let\u2019s get the elephant out of the way \u2014 XGBoost. This is the most popular cousin in the Gradient Boosting Family. XGBoost with its blazing fast implementation stormed into the scene and almost unanimously turned the tables in its favor. Soon enough, Gradient Boosting, via XGBoost, was the reigning king in Kaggle Competitions and pretty soon, it trickled down to the business world.", "There were a few key innovations that made XGBoost so effective:", "Similar to Regularized Greedy Forest, XGBoost also has an explicit regularization term in the objective function.", "\u03b3 is the regularization term which penalizes T, the number of leaves in the tree and \u03bb is the regularization term which penalizes w, the weights of different leaves.", "This is a much simpler regularization term than some of the ways we saw in Regularized Greedy Forest.", "One of the key ingredients of Gradient Boosting algorithms is the gradients or derivatives of the objective function. And all the implementations that we saw earlier used pre-calculated gradient formulae for specific loss functions, thereby, restricting the objectives which can be used in the algorithm to a set which is already implemented in the library.", "XGBoost uses the Newton-Raphson method we discussed in a previous part of the series to approximate the loss function.", "Now, the complex recursive function made up of tree structures can be approximated using Taylor\u2019s approximation into a differentiable form. In the case of Gradient Boosting, we take the second order approximation, meaning we use two terms \u2014 first order derivative and second order derivative- to approximate the function.", "The first term, the loss, is constant at a tree building stage, t, and because of that it doesn\u2019t add any value to the optimization objective. So removing it and simplifying we get,", "Substituting the \u03a9 with the regularization term, we get:", "The f(x) we are talking about is essentially a tree with leaf weights, w. So if we define I\u2c7c as the instance set in leaf j, we can substitute the tree structure directly into the equation and simplify as:", "Setting this equation to zero we can find the optimum value for w\u2c7c as :", "Putting this back into the loss function and simplifying we get:", "What all of this enables us to do is to separate out the objective function from the core working of the algorithm. And by adopting this formulation, the only requirement from an objective function/loss function is that it needs to be differentiable. To be very specific, the loss function should return the first and second order derivatives.", "See here for a list of all the objective functions that are pre-built into XGBoost.", "The tree building strategy lies somewhat in between classical Gradient Boosting and regularized Greedy Forests. While the classical Gradient Boosting takes the tree at each stage as a black box, Regularized Greedy Forest operates at a leaf level by updating any part of the forest at each step. XGBoost takes a middle ground and considers a tree that is made in a previous iteration sacrosanct, but while making a tree for any iteration, it doesn\u2019t use the standard impurity measures, but the gradient based Loss function we derived in the last section in the tree building process. While in classic Gradient Boosting the optimization of the loss function happens after the tree is built, XGBoost gets that optimization during the tree building process as well.", "Normally it's impossible to enumerate all possible tree structures. Therefore a greedy algorithm is implemented that starts from a single leaf and iteratively adds branches to the tree based on the simplified loss function", "One of the key problems in tree learning is to find the best split. Usually, we will have to enumerate all the possible splits over all the features and then use the impurity criteria to choose the best split. This is called exact greedy algorithm. It is computationally demanding, especially for continuous and high cardinality categorical features. And it also not feasible when the data doesn\u2019t fit into memory.", "To overcome these inefficiencies, the paper proposes an Approximate Algorithm. It first proposes candidate splitting points according to the percentiles of the features. On a high level, the algorithm is:", "One of the important steps in the algorithm discussed above is the proposal of candidate splits. Usually, percentiles of a feature are used to make candidates distribute evenly on the data. And a set of algorithms which does that in a distributed manner and with speed and efficiency are called Quantile sketching algorithms. But here, the problem is slightly complicated because the need is to have a weighted quantile sketching algorithm which weighs the instances based on the gradient(so that we can learn the most from instances with most error). So they proposed a new data structure which has provable theoretical guarantee.", "This is another key innovation in XGBoost and this came from the realization that real-world datasets are sparse. This sparsity can come from multiple causes,", "And for this reason, the authors decide to make the algorithm aware of the sparsity so that it can be dealt with intelligently. And the way they made that is deceivingly simple.", "They gave a default direction in each tree node. i.e. if a value is missing or zero, that instance flows down a default direction in the branch. And the optimal default direction is learned from the data", "This innovation has a two-fold benefit -", "One of the main drawbacks of all the implementations of the Gradient Boosting algorithm were that they were quite slow. While the forest creation in a Random Forest was parallel out of the box, Gradient Boosting was a sequential process which builds new base learners on old ones. One of XGBoost\u2019s claim to fame was how blazingly fast it was. It was at least 10 times faster than the existing implementations and it was able to work with large datasets because of the out-of-core learning capabilities. The key innovations in performance improvement were:", "The most time consuming part of tree learning is to get the data sorted into order. The authors of the paper proposed to store the data in in-memory units, called blocks. Data in each block is sorted in the Compressed column (CSC) format. This input data layout is computed once before training and reused thereafter. By handing the data in this sorted format, the tree split algorithm is reduced to a linear scan over the sorted columns", "While the proposed block structure helps optimize the computation complexity of split finding, it requires indirect fetches of gradient statistics by row index. To get over the slow write and read operations in the process, the authors implemented an internal buffer for each thread and accumulate the gradient statistics in minibatches. This helps in reducing the runtime overhead when the rows are large.", "One of the goals of the algorithm is to fully utilize the machine\u2019s resources. While the CPUs are utilized by parallelization of the process, the available disk space is utilized by the out-of-core learning. These blocks that we saw earlier are stored on disk and a separate prefetch thread keeps fetching the data into memory just in time for the computation to continue. They use two techniques to make the I/O operations from disk faster -", "XGBoost has so many articles and blogs about it covering the hyperparameters and how to tune them that I\u2019m not even going to attempt it.", "The single source of truth for any hyperparameter is the official documentation. It might be intimidating to look at the long list of hyperparameters there, but you won\u2019t end up touching the majority of them in a normal use case.", "After publishing this, I came to realize I haven\u2019t talked about some of the later developments in XGBoost like leaf-wise tree growth and how tuning the parameters are slightly different for the new faster implementation.", "LightGBM, about which we will be talking about in the next blog in the series, implemented leaf-wise tree growth and that led to a huge performance improvement. XGBoost also played catch-up and implemented the leaf-wise strategy in a histogram based tree splitting strategy.", "Leaf-wise growth policy, while faster, also overfits faster if the data is small. Therefore it is quite important to use regularization or cap tree complexity through hyperparameters. But if we just keep tuning max_depth as before to control complexity, it won\u2019t work. num_leaves (which controls the number of leaves in a tree) also need to be tuned together. This is because with the same depth, a leaf-wise tree growing algorithm can make a more complex tree.", "You can enable leaf-wise tree building in XGBoost by setting tree_method parameter to \u201chist\u201d and the grow_policy parameter to \u201clossguide\u201d", "Other Articles in The Gradient Boosters", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb736c2ab10ce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@manujosephv?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "Manu Joseph"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8dcc7fb5ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=post_page-c8dcc7fb5ce5----b736c2ab10ce---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb736c2ab10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb736c2ab10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://towardsdatascience.com/tagged/the-gradient-boosters", "anchor_text": "The Gradient Boosters"}, {"url": "https://unsplash.com/photos/-IZ2sgQKIhM", "anchor_text": "Unsplash"}, {"url": "https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters", "anchor_text": "here"}, {"url": "http://netlib.org/linalg/html_templates/node92.html", "anchor_text": "Compressed column (CSC) format"}, {"url": "https://xgboost.readthedocs.io/en/latest/parameter.html", "anchor_text": "official documentation"}, {"url": "http://towardsdatascience.com/tagged/the-gradient-boosters", "anchor_text": "The Gradient Boosters"}, {"url": "https://towardsdatascience.com/the-good-old-gradient-boosting-f4614b0e62b0", "anchor_text": "The Good Old Gradient Boosting"}, {"url": "https://towardsdatascience.com/regularized-greedy-forest-a17cd0c85f06", "anchor_text": "Regularized Greedy Forest"}, {"url": "https://towardsdatascience.com/xgboost-b736c2ab10ce", "anchor_text": "XGBoost"}, {"url": "https://towardsdatascience.com/lightgbm-800340f21415", "anchor_text": "LightGBM"}, {"url": "https://deep-and-shallow.com/2020/02/12/the-gradient-boosters-iii-xgboost/", "anchor_text": "http://deep-and-shallow.com"}, {"url": "https://medium.com/tag/the-gradient-boosters?source=post_page-----b736c2ab10ce---------------the_gradient_boosters-----------------", "anchor_text": "The Gradient Boosters"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b736c2ab10ce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b736c2ab10ce---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----b736c2ab10ce---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----b736c2ab10ce---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb736c2ab10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=-----b736c2ab10ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb736c2ab10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=-----b736c2ab10ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb736c2ab10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb736c2ab10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b736c2ab10ce---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b736c2ab10ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manujosephv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manu Joseph"}, {"url": "https://medium.com/@manujosephv/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "183 Followers"}, {"url": "https://www.linkedin.com/in/manujosephv/", "anchor_text": "https://www.linkedin.com/in/manujosephv/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8dcc7fb5ce5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=post_page-c8dcc7fb5ce5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7cea8b947fdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxgboost-b736c2ab10ce&newsletterV3=c8dcc7fb5ce5&newsletterV3Id=7cea8b947fdd&user=Manu+Joseph&userId=c8dcc7fb5ce5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}