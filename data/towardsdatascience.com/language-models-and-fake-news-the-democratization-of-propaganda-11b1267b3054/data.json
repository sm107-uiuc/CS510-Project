{"url": "https://towardsdatascience.com/language-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054", "time": 1683008793.6731288, "path": "towardsdatascience.com/language-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054/", "webpage": {"metadata": {"title": "Language Models and Fake News: the Democratization of Propaganda | by Adrian Yijie Xu | Towards Data Science", "h1": "Language Models and Fake News: the Democratization of Propaganda", "description": "A phrase published the press statement by OpenAI to accompany their announcement of the release of their GPT-2 language model in February 2019. With the recent release of the more advanced GPT-3 last\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.independent.co.uk/life-style/gadgets-and-tech/news/ai-artificial-intelligence-dangerous-text-gpt2-elon-musk-a9192121.html", "anchor_text": "press statement", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/gpt-3-the-new-mighty-language-model-from-openai-a74ff35346fc", "anchor_text": "recent release of the more advanced GPT-3 last week", "paragraph_index": 0}, {"url": "https://time.com/5565991/russia-influence-2016-election/", "anchor_text": "campaigns to influence global events", "paragraph_index": 1}, {"url": "https://www.latimes.com/world-nation/story/2020-04-21/senate-panel-backs-assessment-that-russia-interfered-in-2016", "anchor_text": "US Senate committee", "paragraph_index": 1}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "Brexit referendum and the Crimea crisis", "paragraph_index": 1}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "studies suggest", "paragraph_index": 2}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "proliferation of AI-generated", "paragraph_index": 2}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "concerning the COVID-19 crisis", "paragraph_index": 2}, {"url": "https://www.ft.com/content/55a39e92-8357-11ea-b872-8db45d5f6714", "anchor_text": "AI-generated fake accounts", "paragraph_index": 4}, {"url": "https://medium.com/gradientcrescent/applying-sentiment-analysis-to-e-commerce-classification-using-recurrent-neural-networks-in-keras-cd89b77baa60", "anchor_text": "RNNs", "paragraph_index": 6}, {"url": "https://medium.com/gradientcrescent/generating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3", "anchor_text": "STM", "paragraph_index": 6}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Alammar et. al,", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1801.10198.pdf", "anchor_text": "based on work at Google Brain", "paragraph_index": 14}, {"url": "https://human-memory.net/brain-neurons-synapses/#:~:text=Each%20individual%20neuron%20can%20form,as%20neural%20nets%20or%20assemblies", "anchor_text": "100 trillion synapses", "paragraph_index": 17}, {"url": "https://www.google.com/search?q=reinforcement+learning+adrian+yijie+xu&rlz=1C1EJFA_enSG791SG791&oq=reinforcement+learning+adrian+yijie+xu&aqs=chrome..69i57.6802j0j7&sourceid=chrome&ie=UTF-8", "anchor_text": "reinforcement-learning approach", "paragraph_index": 35}, {"url": "http://OpenAI's GPT family of models", "anchor_text": "domain of molecular design", "paragraph_index": 35}, {"url": "https://grover.allenai.org/", "anchor_text": "AllenAI", "paragraph_index": 36}, {"url": "https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/", "anchor_text": "IBM Watson", "paragraph_index": 36}, {"url": "https://medium.com/gradientcrescent/ai-truth-and-society-deepfakes-at-the-front-of-the-technological-cold-war-86c3b5103ce6", "anchor_text": "sparking fears of an arms race similar to that observed for deepfakes", "paragraph_index": 36}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent", "paragraph_index": 40}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github", "paragraph_index": 40}], "all_paragraphs": ["A phrase published the press statement by OpenAI to accompany their announcement of the release of their GPT-2 language model in February 2019. With the recent release of the more advanced GPT-3 last week, the possibility of AI-driven misinformation has become a significant risk that remains unaddressed in today\u2019s post-factual information landscape.", "Since 2016, the term \u201cfake news\u201d has gained popularity in both political leadership and general populations worldwide as a dismissive concept to use on reports that do not support one\u2019s own points of view. But the term has grown to encompass state and non-state operated misinformation campaigns. Efforts to utilize such campaigns to influence global events have accelerated significantly, with a bipartisan US Senate committee concluding that Russia\u2019s misinformation campaign during the 2016 US Presidential Election \u201cdistributed messages through Russian-controlled propaganda outlets to undermine public faith in the democratic process\u201d. Similar sentiments were echoed by Robert Mueller, who added that Russian interference had been \u201csweeping and systematic\u201d, and added the fear of continued influence in the upcoming 2020 Presidential Election. Researchers now believe that political bots and disinformation have played a role in other significant cases such as \u2014 the Brexit referendum and the Crimea crisis.", "Such concrete reports lend apprehension on the use of AI to support misinformation campaigns as a tool of cyber-warfare. However, studies suggest that the majority of \u201cAI-supported\u201d solutions amount to nothing more than bots designed to repeat certain phrases and links under particular hashtags, in order to change the context of discussions. Such approaches rely more on the ability of the human operators understand the psychology of online discourse more than on the ability of the bot itself. More recently, the proliferation of AI-generated content concerning the COVID-19 crisis on social media platforms has led to accelerated calls for improved vetting. However human vetting is slow and coarse, while AI models significant amounts of data to train. Facebook CTO Mike Schroepfer summarized the fundamental challenge of building solutions to new unforeseen threats:", "\u201cBuilding a novel classifier for something that understands content it\u2019s never seen before takes time and a lot of data.\u201d", "There have been no systematic studies on the prevalence of AI-generated misinformation, and it is probable that the majority of misinformation thus far has been created by human actors. But with the proliferation of AI-generated fake accounts, coupled with the increasing availability and capabilities aof language models, it is expected that AI-generated textual misinformation will be encountered in the future. This problem is compounded by the post-factual, opinion-dominated information landscape observed today.", "But why the apprehension over these models? What is it that makes them dangerous? Are we witnessing the death of truth? To answer this question, we need to delve into their principles and how they operate. In the interest of time and scope, we will focus our attention on OpenAI\u2019s GPT-series of language models in this article.", "We\u2019ve covered the the general theory behind language models such as recurrent neural networks (RNNs) and Long-Short Term Memory (LSTM) architectures in previous articles, and thus encourage the reader to consult these for a detailed understanding of those architectures.", "The GPT family of natural language models are based on Transformer models, which are characterized by repeated encoder-architecture coupled with an attention mechanism. The general architecture of Transformers has been explained in depth by Alammar et. al, but we will provide a high-level summary here.", "Consider the general structure of a Transformer model, as shown below:", "Encoder-decoder architectures work by by reducing the input data into a latent dimension (representing the meaning of a word) through the encoder, to be reconstructed by the decoder component in the target language. As such, they have traditionally shown strong performance in machine-translation. However, Transformer-models build upon this architecture by incorporating self-attention layers into each encoder and decoder block, with each component possessing its own set of weight parameters. It is the job of attention layers to relate the position of the input word within a sentence with its meaning, in order to improve upon its encoding. In particular, transformer models are more capable of capturing long-range dependencies in sequences compared to traditional pure RNN and LSTM architectures.", "We can best visualize the effect of attention on the encoding process with an example. Consider the following sentence:", "\u201cThe animal didn\u2019t cross the street because it was too tired\u201d", "We can view the relationships of the sentence components with the use of attention by examining the activations of the corresponding layers.", "You\u2019ll notice that the encoded representation of the word \u201cit\u201d possesses strong attention links to the concepts \u201canimal\u201d and \u201ctired\u201d, creating a form of meaning association between these pairs. The benefits of this association are particularly useful for machine translation applications, where the syntax of different languages may require a completely different sentence structure, but expand to conditional text generation and other applications too.", "OpenAI\u2019s GPT family of semantic language models, based on work at Google Brain, remove the encoder component completely, consisting of stacks of decoder blocks with attention layers.", "While the model was trained to predict the next word in a sequence given an input text sequence, their high parameter complexity has resulted in significantly expanded capabilities, acquired via meta-learning (or retraining with only a few examples of a new task), including:", "The original GPT-1 model consisted of a transformer model and a pretrained ELMO-language model, with hundreds of millions of trainable parameters. GPT-2 expanded beyond this with over 1.5 billion parameters, more than 10 times the parameters and trained on more than 10 times amount of data than its previous counterpart (derived from internet and literary sources).", "The more recent GPT-3 is built with over 175 billion parameters, improving its semantic performance, particularly in one-shot or zero-shot training applications. Achieving a high performance for such applications is necessary to approach a human-level responsiveness and performance for language models. For comparison, a typical typical human brain has over 100 trillion synapses, roughly three orders of magnitudes larger than the largest GPT-3 model. Given that it took OpenAI roughly a year or so to increase the parameter capacity of their model by two orders of magnitude, reaching this amount seems like feasible task given time and resources.", "GPT-2 and GPT-3 have also been tested in different parameter configurations, to evaluate their performance and to prevent potential misuse. The configurations and benchmark performance of GPT-3 configurations are shown below:", "We can best visualize the performance enhancements of GPT-3 by comparing the difference in their outputs. Below is a sample of text generated by GPT-2, as reported in the original OpenAI paper by Brown et. al.", "While at a glance, the text seems to be grammatically correct, a closer examination reveals many factual inconsistencies. For example, consider the sentence:", "\u201cThese four-horned, silver-white unicorns were previously unknown to science.\u201d", "This is a factual inconsistency, given that a unicorn is defined as possessing a single horn. Naturally, the model would not know the definition of a word explicitly, and has failed to infer it from its training data.", "The following section displays similar inconsistencies.", "\u201cWhile their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization.\u201d", "Given that the subject of the article are unicorns themselves, this sentence doesn\u2019t make sense. However, as the sentences are grammatically correct, these errors would take a significant amount of concentration and awareness to be recognized.", "In comparison, let\u2019s take a two reported examples of at GPT-3 outputs \u2014 the first had a higher level of realism, and was more capable of passing off as human- generated (88%) while the second was significantly less realistic (39%).", "In comparison to GPT-2, GPT-3 seems to have captured the subtleties of word definitions. However, analysis of these two outputs suggests that the model still struggles with extended dialogue snippets. Consider the extract:", "\u201cA year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, \u201cI am a shape-shifter. I can\u2019t change the world. I can only change myself.\u201d It was a promise to not change to fit into the Hollywood mold: \u201cI think that\u2019s a really special thing, to not change yourself. I think it\u2019s a really special thing to say\u201d", "In fact, the realism of the text could be significantly improved by simply fixing the punctuation surrounding individual lines, and joining them together. But again, this would need to be learned, not defined. On average, the mean human accuracy of detecting articles produced by the largest (175 Billion) and smallest (125 million) GPT-3 models was ca. 52% and 76% respectively across 200 articles.", "As previously mentioned, GPT-3 has performed impressively in a variety of other NLP tasks with few-shot, one-shot, or zero-shot training. A selection of these tasks are shown below for reference:", "It\u2019s the performance that the model displays in these new applications with minimal training that really makes it impressive, as it lowers the boundary to the development of commercial applications such as in chatbots or review sentiment analysis.", "So given the trend in performance improvements, would it be possible to increase the size and complexity of the model further by orders of magnitude, and fool humans beyond random chance?", "Maybe. While conditional text generation has shown significant improvement with model complexity, the researchers observed that GPT-3 performed poorly when it came to analyzing the relationships between sentences, and suggested that the problem here is independent of model complexity, and that the approach itself that may be incorrect. This is summarized in their conclusions:", "\u201cWith self-supervised objectives, task specification relies on forcing the desired task into a prediction problem. Whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions.\u201d", "The mentioning of goal-directed actions is of particular interest, as it would suggest that a reinforcement-learning approach would be a more ideal solution. OpenAI has previously explored using human preferences to fine-tune the GPT-2 model, but such approaches are too coarse and labour intensive for real-world applications. A true reinforcement learning model would require the careful definition of a reward function for its application, perhaps through the use of feed-forward discriminator models, as has been done in the domain of molecular design.", "So are we doomed to a future of misinformation? Perhaps not. Training these models is prohibitively resource intensive, taking the equivalent of 665 years to traing the largest GPT-3 model using a single NVIDIA RTX-8000 GPU-unit. Moreover, researchers at organisations such as HarvardNLP, AllenAI, and IBM Watson have leveraged the availability of generative language models such as GPT-2 to build discriminator counterparts capable of detecting counterfeit outputs. With the release of GPT-3 and other more complex models, discriminators must similarly evolve, sparking fears of an arms race similar to that observed for deepfakes. Furthermore, as the training of discriminator models will require the output of the generators, there will always be a window during which detection solutions will be insufficient.", "But the true risk of \u201cfake news\u201d lie not in capabilities of language models, but in our degrading skills in information literacy and critical thinking in society. Whether it be due to social media or otherwise, polarization has become a strong component of discourse, resulting in symptoms ranging from online echo chambers for extremist views, to a lack of bipartisanship at the highest levels of governance.", "Until we can consciously reduce polarization and adopt a culture oriented around factual information and respectful discourse, we will continue to remain vulnerable to misinformation, whether it be generated by AI or human actors.", "This wraps up our review on the theoretical advances of the OpenAI GPT models. In our upcoming article, we\u2019ll cover how to implement a GPT-model in producing some \u201cfake news\u201d and poetry.", "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F11b1267b3054&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----11b1267b3054--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11b1267b3054--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----11b1267b3054--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----11b1267b3054--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----11b1267b3054---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11b1267b3054&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11b1267b3054&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.independent.co.uk/life-style/gadgets-and-tech/news/ai-artificial-intelligence-dangerous-text-gpt2-elon-musk-a9192121.html", "anchor_text": "press statement"}, {"url": "https://towardsdatascience.com/gpt-3-the-new-mighty-language-model-from-openai-a74ff35346fc", "anchor_text": "recent release of the more advanced GPT-3 last week"}, {"url": "https://time.com/5565991/russia-influence-2016-election/", "anchor_text": "campaigns to influence global events"}, {"url": "https://www.latimes.com/world-nation/story/2020-04-21/senate-panel-backs-assessment-that-russia-interfered-in-2016", "anchor_text": "US Senate committee"}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "Brexit referendum and the Crimea crisis"}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "studies suggest"}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "proliferation of AI-generated"}, {"url": "https://www.technologyreview.com/2020/01/08/130983/were-fighting-fake-news-ai-bots-by-using-more-ai-thats-a-mistake/", "anchor_text": "concerning the COVID-19 crisis"}, {"url": "https://www.ft.com/content/55a39e92-8357-11ea-b872-8db45d5f6714", "anchor_text": "AI-generated fake accounts"}, {"url": "https://medium.com/gradientcrescent/applying-sentiment-analysis-to-e-commerce-classification-using-recurrent-neural-networks-in-keras-cd89b77baa60", "anchor_text": "RNNs"}, {"url": "https://medium.com/gradientcrescent/generating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3", "anchor_text": "STM"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Alammar et. al,"}, {"url": "https://arxiv.org/pdf/1801.10198.pdf", "anchor_text": "based on work at Google Brain"}, {"url": "https://human-memory.net/brain-neurons-synapses/#:~:text=Each%20individual%20neuron%20can%20form,as%20neural%20nets%20or%20assemblies", "anchor_text": "100 trillion synapses"}, {"url": "https://www.google.com/search?q=reinforcement+learning+adrian+yijie+xu&rlz=1C1EJFA_enSG791SG791&oq=reinforcement+learning+adrian+yijie+xu&aqs=chrome..69i57.6802j0j7&sourceid=chrome&ie=UTF-8", "anchor_text": "reinforcement-learning approach"}, {"url": "http://OpenAI's GPT family of models", "anchor_text": "domain of molecular design"}, {"url": "https://grover.allenai.org/", "anchor_text": "AllenAI"}, {"url": "https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/", "anchor_text": "IBM Watson"}, {"url": "https://medium.com/gradientcrescent/ai-truth-and-society-deepfakes-at-the-front-of-the-technological-cold-war-86c3b5103ce6", "anchor_text": "sparking fears of an arms race similar to that observed for deepfakes"}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github"}, {"url": "https://www.zdnet.com/article/openais-gigantic-gpt-3-hints-at-the-limits-of-language-models-for-ai/", "anchor_text": "OpenAI's gigantic GPT-3 hints at the limits of language models for AI | ZDNetThe California research outfit OpenAI is back with another gigantic deep learning model, GPT-3. While it shows that\u2026www.zdnet.com"}, {"url": "https://openai.com/blog/fine-tuning-gpt-2/", "anchor_text": "Fine-Tuning GPT-2 from Human PreferencesWe've fine-tuned the 774M parameter GPT-2 language model using human feedback for various tasks, successfully matching\u2026openai.com"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Better Language Models and Their ImplicationsWe've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves\u2026openai.com"}, {"url": "https://openai.com/blog/language-unsupervised/", "anchor_text": "Improving Language Understanding with Unsupervised LearningWe've obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system\u2026openai.com"}, {"url": "https://arxiv.org/abs/2005.14165", "anchor_text": "Language Models are Few-Shot LearnersRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of\u2026arxiv.org"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "The Illustrated GPT-2 (Visualizing Transformer Language Models)Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations\u2026jalammar.github.io"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated TransformerDiscussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations\u2026jalammar.github.io"}, {"url": "https://lambdalabs.com/blog/demystifying-gpt-3/", "anchor_text": "Demystifying GPT-3 - The Latest in Deep Learning Language ModelsLast Thursday, OpenAI's latest update to language modeling, GPT-3, appeared on arxiv. Given how influential its\u2026lambdalabs.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----11b1267b3054---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/fake-news?source=post_page-----11b1267b3054---------------fake_news-----------------", "anchor_text": "Fake News"}, {"url": "https://medium.com/tag/openai?source=post_page-----11b1267b3054---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/nlp?source=post_page-----11b1267b3054---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----11b1267b3054---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11b1267b3054&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----11b1267b3054---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F11b1267b3054&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----11b1267b3054---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F11b1267b3054&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----11b1267b3054--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F11b1267b3054&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----11b1267b3054---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----11b1267b3054--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----11b1267b3054--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----11b1267b3054--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----11b1267b3054--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----11b1267b3054--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----11b1267b3054--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----11b1267b3054--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----11b1267b3054--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}