{"url": "https://towardsdatascience.com/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d", "time": 1683005722.8064792, "path": "towardsdatascience.com/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d/", "webpage": {"metadata": {"title": "Understanding ELECTRA and Training an ELECTRA Language Model | by Thilina Rajapakse | Towards Data Science", "h1": "Understanding ELECTRA and Training an ELECTRA Language Model", "description": "The process of training a Transformer model for use in a particular Natural Language Processing task is fairly simple, although it might not be easy. Start with a randomly initialized Transformer\u2026"}, "outgoing_paragraph_urls": [{"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA", "paragraph_index": 3}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA paper", "paragraph_index": 5}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 23}, {"url": "https://huggingface.co/blog/how-to-train", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://wortschatz.uni-leipzig.de/en/download", "anchor_text": "Leipzig Corpora Collection", "paragraph_index": 25}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers#default-settings", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers#additional-attributes-for-language-modeling-tasks", "anchor_text": "here", "paragraph_index": 34}, {"url": "https://huggingface.co/transformers/model_doc/electra.html#electraconfig", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://app.wandb.ai/thilina/Esperanto%20-%20ELECTRA?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 42}, {"url": "https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/pos-train.txt", "anchor_text": "train", "paragraph_index": 49}, {"url": "https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/pos-labels.txt", "anchor_text": "labels", "paragraph_index": 49}, {"url": "https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0", "anchor_text": "here", "paragraph_index": 51}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 55}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 57}], "all_paragraphs": ["The process of training a Transformer model for use in a particular Natural Language Processing task is fairly simple, although it might not be easy. Start with a randomly initialized Transformer model, put together a huge (and I do mean huge) dataset containing text in the language or languages you are interested in, pre-train the Transformer on the huge dataset, and fine-tune the pre-trained Transformer on your particular task, using your task-specific dataset (which may be comparatively tiny).", "The advantage of this method is that you only need labelled data for the final step (fine-tuning). Personally, if there\u2019s anything I don\u2019t like about my work with Deep Learning, it\u2019s labelling the data! I\u2019m sure any Deep Learning practitioner would agree that labelling is time-consuming, tedious, and potentially error-prone.", "Pre-training is the process through which the Transformer model learns to model a language. In other words, the Transformer will learn good, context-dependent ways of representing text sequences. This knowledge can then be reused in downstream tasks, drastically cutting down the amount of required task-specific, labelled data as the model has already learned the language features and now only needs to fine-tune its representations to perform a specific task. For pre-training, the only requirement in terms of data is a lot of (hopefully) clean data. No labelling necessary! I shudder to think of the ordeal someone would have had to undergo if it had been necessary to label the gigabytes of text used to pre-train BERT.", "That\u2019s all well and good, but how exactly does a Transformer learn language representation from a huge dump of unlabelled text? Well, the original BERT model relied on two pre-training tasks, masked language modelling (MLM) and next sentence prediction. In next sentence prediction, the model is tasked with predicting whether two sequences of text naturally follow each other or not. This task was said to help with certain downstream tasks such as Question Answering and Natural Language Inference in the BERT paper although it was shown to be unnecessary in the later RoBERTa paper which only used masked language modelling. Regardless, we are more interested in the first approach, MLM, as this is what the ELECTRA pre-training method aims to improve upon.", "In Masked Language Modelling, a certain percentage of the tokens (a token is a term for a single unit of a text sequence. It may be a word or a piece of a word) of an input sequence is masked, and the model is tasked with predicting the original token for the masked tokens. The masked tokens may be replaced with an actual mask token (e.g. [MASK]) or replaced with another random token from the vocabulary (the set of all tokens known to the model).", "BERT, armed with this pre-training technique, was capable of shredding previously set benchmarks in many of the downstream NLP tasks. However, the authors of the ELECTRA paper note that MLM approaches only learns from the masked tokens (typically 15%) of any given example. This leads to a substantial increase in the computational resources needed to train a language model with MLM. Another downside of MLM is that the mask tokens only appear in the pre-training stage and never during fine-tuning or downstream usage. This discrepancy also contributes to a slight loss of performance in models trained using MLM.", "ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) is a new pre-training approach which aims to match or exceed the downstream performance of an MLM pre-trained model while using significantly less compute resources for the pre-training stage. The pre-training task in ELECTRA is based on detecting replaced tokens in the input sequence. This setup requires two Transformer models, a generator and a discriminator.", "Let\u2019s break down the pre-training process step-by-step.", "The generator model is trained to predict the original tokens for masked-out tokens while the discriminator model is trained to predict which tokens have been replaced given a corrupted sequence. This means that the discriminator loss can be calculated over all input tokens as it performs prediction on each token. With MLM, the model loss is only calculated over the masked tokens. This is shown to be a key difference between the two approaches and the primary reason behind ELECTRA\u2019s greater efficiency.", "This setup is similar to the training setup of a GAN (Generative Adversarial Network), except that the generator is not trained to try and deceive the discriminator (so it is not adversarial per se). Also, if the generator happens to predict the original token of a masked token correctly, that token is considered to be an original token (as the token has not been corrupted/changed).", "The discriminator model is used for downstream tasks and the generator is thrown away after pre-training.", "The ELECTRA paper highlights the major improvements of the ELECTRA pre-training approach and also shed light on why those improvements are seen.", "All scores are GLUE benchmark scores given in the ELECTRA paper unless stated otherwise.", "As mentioned earlier, the discriminator model\u2019s loss is defined over all the tokens in the sequence as it must predict whether each token is an original or a replacement. To demonstrate the significance of this difference, the authors compare the ELECTRA model to an identically trained model (ELECTRA 15%) except that ELECTRA 15% only calculates the discriminator loss over the masked tokens. The GLUE benchmark is used for the comparison.", "It is shown that the original ELECTRA approach yields a 85.0 score while ELECTRA 15% gets 82.4. (For comparison, BERT scored 82.2)", "The ELECTRA is also compared to another model (All-Tokens MLM) where the masked tokens are replaced with generator predictions AND the model is tasked with predicting the original identity of all tokens in the input. The loss, too, is calculated over all input tokens. The All-Tokens MLM model scores 84.3, outperforming BERT\u2019s 82.2, and nearly catching up to ELECTRA\u2019s 85.0.", "This clearly shows that the ability to calculate the loss over all input tokens significantly boosts the performance of a pre-trained model.", "For this, the original BERT model is compared to a model (Replace MLM) trained using the MLM pre-training objective except that the masked tokens are replaced with a token from a generator rather than with an actual [MASK] token.", "The replace MLM model scores 82.4, slightly outperforming BERT\u2019s 82.2. This difference indicates that the pre-train/fine-tuning discrepancy does slightly harm BERT\u2019s performance.", "Comparatively, learning from all input tokens seems to have a much bigger impact than resolving the pre-train/fine-tune mismatch of masked tokens.", "Summing up the comparisons in a table;", "The performance gains of ELECTRA over BERT is shown to be larger at smaller model sizes.", "One huge advantage of the ELECTRA pre-training approach is that it\u2019s possible to train your own language models on a single GPU!", "Below, I\u2019ll show you how you can train your own language model with the Simple Transformers library.", "We\u2019ll be training our language model on Esperanto (inspired by Hugging Face\u2019s tutorial here). Don\u2019t worry if you don\u2019t speak Esperanto, neither do I!", "For pre-training a model we are going to need a (preferably large) corpus of text in Esperanto. I\u2019ll be using the Esperanto text files from the Leipzig Corpora Collection. Specifically, I downloaded the following datasets;", "You should be able to improve the results by using a bigger dataset.", "Download the datasets and extract the archives into a directory data/ .", "Take all the \u201csentence\u201d files and move them into the data/ directory.", "Script for Linux/bash users. (Others: Why aren\u2019t you on Linux? \ud83d\ude09)", "If you open one of the files, you\u2019ll notice that they have two columns, with the first column containing the index and the second column containing the text. We just need the text, so we\u2019ll drop the indexes, combine all the text, and split the text into train and test files.", "Now we are ready to start training!", "In Simple Transformers, all language modelling tasks are handled with the LanguageModelingModel class. You have tons of configuration options that you can use when performing any NLP task in Simple Transformers, although you don\u2019t need to set each one (sensible defaults are used wherever possible).", "List of common configuration options and their usage here.", "Language modelling specific options and their usage here.", "The gist above sets up a LanguageModelingModel which can be used to train our new model.", "A single training epoch (with this configuration) takes a little under 2 hours on a Titan RTX GPU. To speed up training, you can increase evaluate_during_training_steps or turn off evaluate_during_training altogether.", "When training a language model from scratch with Simple Transformers, it will automatically create a tokenizer for us from the specified train_file. You can configure the size of the trained tokenizer by setting a vocab_size in the train_args. In my case, I\u2019m using a vocabulary of 52000 tokens.", "You can also configure the architecture of the generator and the discriminator models as required. The configuration options are set in the two dictionaries generator_config and discriminator_config found in train_args. The ELECTRA paper recommends using a generator model that is 0.25\u20130.5 of the size of the discriminator. They also recommend decreasing the number of hidden layers and keeping the other parameters constant between the generator and the discriminator. With that in mind, I went with a small (12 layers) architecture for the discriminator and a similar generator albeit with 1/4th the number of hidden layers.", "You can find all the architecture configuration options and their default values here.", "Now that we\u2019ve set up our models, all we need to do is initiate the training.", "Running the above script will start training our language model!", "You can take a look at all the training progress information (and a lot of charts!) here on Wandb.", "Amazingly, the ELECTRA pre-training approach lets us train a brand-new language model, in a matter of hours, on a single GPU! The chart below tells the full story.", "The flattish parts of the curve are where Simple Transformers did the evaluations. Sadly, I forgot to set a higher batch size for evaluation so the evaluations were a lot slower than they should have been.", "When using the ELECTRA pre-training approach, the generator is typically discarded after training and only the discriminator is used. To facilitate this, Simple Transformers will save the generator and the discriminator separately once training completes.", "However, if you terminate training before it completes, your model will be saved as a LanguageModelingModel which contains both the discriminator and the generator. In order to extract either the discriminator or the generator separately, you can use the methods save_discriminator() or save_generator().", "The script below demonstrates how you can load a LanguageModelingModel and extract the generator model separately.", "This will load the trained language model found in outputs/best_model into a LanguageModelingModel and then save only the discriminator into discriminator_trained/discriminator_model.", "Now that we have a model that can \u201cunderstand\u201d Esperanto, we can go ahead and fine-tune it on a specific NLP task. For this, we can use the Part-of-Speech tagging dataset in Esperanto provided by Hugging Face (train and labels).", "In my case, I downloaded the files and saved them to data/pos-tagging.", "Using this we can train our ELECTRA pre-trained model (the discriminator) as a NERModel on the Part-of-Speech dataset. (For more info on Named Entity Recognition with Transformers, you can check out my article here)", "As you can see, the model is capable of performing impressively in the downstream task after only a few hours of training!", "And that, folks, is how you can train a brand new language model from scratch with ELECTRA!", "I am excited by the possibilities this opens up as ELECTRA should significantly lower the compute resource barrier to training your own language models. Hopefully, we will be seeing more and more language models being developed for all the languages spoken around the world!", "As always, a huge shoutout to Hugging Face and their Transformers library, who\u2019s work make all of this possible!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3d33e3a9660d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----3d33e3a9660d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d33e3a9660d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d33e3a9660d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/photos/woman-studying-learning-books-1852907/#", "anchor_text": "https://pixabay.com/photos/woman-studying-learning-books-1852907/#"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA paper"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA paper"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA paper"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA paper"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "here"}, {"url": "https://github.com/NVIDIA/apex", "anchor_text": "here"}, {"url": "https://huggingface.co/blog/how-to-train", "anchor_text": "here"}, {"url": "https://wortschatz.uni-leipzig.de/en/download", "anchor_text": "Leipzig Corpora Collection"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers#default-settings", "anchor_text": "here"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers#additional-attributes-for-language-modeling-tasks", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/model_doc/electra.html#electraconfig", "anchor_text": "here"}, {"url": "https://app.wandb.ai/thilina/Esperanto%20-%20ELECTRA?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/pos-train.txt", "anchor_text": "train"}, {"url": "https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/pos-labels.txt", "anchor_text": "labels"}, {"url": "https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0", "anchor_text": "here"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3d33e3a9660d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3d33e3a9660d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3d33e3a9660d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----3d33e3a9660d---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3d33e3a9660d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d33e3a9660d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----3d33e3a9660d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d33e3a9660d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----3d33e3a9660d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d33e3a9660d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3d33e3a9660d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3d33e3a9660d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3d33e3a9660d--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-electra-and-training-an-electra-language-model-3d33e3a9660d&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}