{"url": "https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0", "time": 1683003702.340428, "path": "towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0/", "webpage": {"metadata": {"title": "Building a Sentiment Classifier using Scikit-Learn | by Dorian Lazar | Towards Data Science", "h1": "Building a Sentiment Classifier using Scikit-Learn", "description": "Here we will build a classifier that is able to distinguish movie reviews as being either positive or negative. For that, we will use Large Movie Review Dataset v1.0[2] of IMDB movie reviews. This\u2026"}, "outgoing_paragraph_urls": [{"url": "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", "anchor_text": "Large Movie Review Dataset v1.0", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Sentiment_analysis", "anchor_text": "Sentiment Analysis \u2014 Wikipedia", "paragraph_index": 27}, {"url": "http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf", "anchor_text": "Learning Word Vectors for Sentiment Analysis", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "Bag-of-words model \u2014 Wikipedia", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "Tf-idf \u2014 Wikipedia", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html", "anchor_text": "TfidfTransformer \u2014 Scikit-learn documentation", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Stop_words", "anchor_text": "Stop words \u2014 Wikipedia", "paragraph_index": 27}, {"url": "https://gist.github.com/sebleier/554280", "anchor_text": "A list of English stopwords", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "CountVectorizer \u2014 Scikit-learn documentation", "paragraph_index": 27}, {"url": "https://docs.scipy.org/doc/scipy/reference/sparse.html", "anchor_text": "Scipy sparse matrices", "paragraph_index": 27}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix", "anchor_text": "Compressed Sparse Row matrix", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "anchor_text": "SGDClassifier \u2014 Scikit-learn documentation", "paragraph_index": 27}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV \u2014 Scikit-learn documentation", "paragraph_index": 27}, {"url": "https://www.aclweb.org/anthology/P19-2057.pdf", "anchor_text": "Sentiment Classification using Document Embeddings trained with Cosine Similarity", "paragraph_index": 27}, {"url": "https://github.com/lazuxd/simple-imdb-sentiment-analysis/blob/master/sentiment-analysis.ipynb", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://www.nablasquared.com/building-a-sentiment-classifier-using-scikit-learn/", "anchor_text": "here", "paragraph_index": 30}, {"url": "https://www.nablasquared.com/", "anchor_text": "https://www.nablasquared.com/", "paragraph_index": 32}], "all_paragraphs": ["Sentiment analysis, an important area in Natural Language Processing, is the process of automatically detecting affective states of text. Sentiment analysis is widely applied to voice-of-customer materials such as product reviews in online shopping websites like Amazon, movie reviews or social media. It can be just a basic task of classifying the polarity of a text as being positive/negative or it can go beyond polarity, looking at emotional states such as \u201chappy\u201d, \u201cangry\u201d, etc.", "Here we will build a classifier that is able to distinguish movie reviews as being either positive or negative. For that, we will use Large Movie Review Dataset v1.0[2] of IMDB movie reviews. This dataset contains 50,000 movie reviews divided evenly into 25k train and 25k test. The labels are balanced between the two classes (positive and negative). Reviews with a score <= 4 out of 10 are labeled negative and those with score >= 7 out of 10 are labeled positive. Neutral reviews are not included in the labeled data. This dataset also contains unlabeled reviews for unsupervised learning; we will not use them here. There are no more than 30 reviews for a particular movie because the ratings of the same movie tend to be correlated. All reviews for a given movie are either in train or test set but not in both, in order to avoid test accuracy gain by memorizing movie-specific terms.", "After the dataset has been downloaded and extracted from archive we have to transform it into a more suitable form for feeding it into a machine learning model for training. We will start by combining all review data into 2 pandas Data Frames representing the train and test datasets, and then saving them as csv files: imdb_train.csv and imdb_test.csv.", "The Data Frames will have the following form:", "But machine learning algorithms work only with numerical values. We can\u2019t just input the text itself into a machine learning model and have it learn from that. We have to, somehow, represent the text by numbers or vectors of numbers. One way of doing this is by using the Bag-of-words model[3], in which a piece of text(often called a document) is represented by a vector of the counts of words from a vocabulary in that document. This model doesn\u2019t take into account grammar rules or word ordering; all it considers is the frequency of words. If we use the counts of each word independently we name this representation a unigram. In general, in a n-gram we take into account the counts of each combination of n words from the vocabulary that appears in a given document.", "For example, consider these two documents:", "The vocabulary of all words encountered in these two sentences is:", "The unigram representations of d1 and d2:", "And, the bigrams of d1 and d2 are:", "Often, we can achieve slightly better results if instead of counts of words we use something called term frequency times inverse document frequency (or tf-idf). Maybe it sounds complicated, but it is not. Bear with me, I will explain this. The intuition behind this is the following. So, what\u2019s the problem of using just the frequency of terms inside a document? Although some terms may have a high frequency inside documents they may not be so relevant for describing a given document in which they appear. That\u2019s because those terms may also have a high frequency across the collection of all documents. For example, a collection of movie reviews may have terms specific to movies/cinematography that are present in almost all documents(they have a high document frequency). So, when we encounter those terms in a document this doesn\u2019t tell much about whether it is a positive or negative review. We need a way of relating term frequency (how frequent a term is inside a document) to document frequency (how frequent a term is across the whole collection of documents). That is:", "Now, there are more ways used to describe both term frequency and inverse document frequency. But the most common way is by putting them on a logarithmic scale:", "We added 1 in the first logarithm to avoid getting -\u221e when the count is 0. In the second logarithm we added one fake document to avoid division by zero.", "Before we transform our data into vectors of counts or tf-idf values we should remove English stopwords[6][7]. Stopwords are words that are very common in a language and are usually removed in the preprocessing stage of natural text-related tasks like sentiment analysis or search.", "Note that we should construct our vocabulary only based on the training set. When we will process the test data in order to make predictions we should use only the vocabulary constructed in the training phase, the rest of the words will be ignored.", "Now, let\u2019s create the data frames and save them as csv files:", "Fortunately, for the text vectorization part all the hard work is already done in the Scikit-Learn classes CountVectorizer[8] and TfidfTransformer[5]. We will use these classes to transform our csv files into unigram and bigram matrices(using both counts and tf-idf values). (It turns out that if we only use a n-gram for a large n we don't get a good accuracy, we usually use all n-grams up to some n. So, when we say here bigrams we actually refer to uni+bigrams and when we say unigrams it's just unigrams.) Each row in those matrices will represent a document (review) in our dataset, and each column will represent values associated with each word in the vocabulary (in the case of unigrams) or values associated with each combination of maximum 2 words in the vocabulary (bigrams).", "CountVectorizer has a parameter ngram_range which expects a tuple of size 2 that controls what n-grams to include. After we constructed a CountVectorizer object we should call .fit() method with the actual text as a parameter, in order for it to learn the required statistics of our collection of documents. Then, by calling .transform() method with our collection of documents it returns the matrix for the n-gram range specified. As the class name suggests, this matrix will contain just the counts. To obtain the tf-idf values, the class TfidfTransformer should be used. It has the .fit() and .transform() methods that are used in a similar way with those of CountVectorizer, but they take as input the counts matrix obtained in the previous step and .transform() will return a matrix with tf-idf values. We should use .fit() only on training data and then store these objects. When we want to evaluate the test score or whenever we want to make a prediction we should use these objects to transform the data before feeding it into our classifier.", "Note that the matrices generated for our train or test data will be huge, and if we store them as normal numpy arrays they will not even fit into RAM. But most of the entries in these matrices will be zero. So, these Scikit-Learn classes are using Scipy sparse matrices[9] (csr_matrix[10] to be more exactly), which store just the non-zero entries and save a LOT of space.", "We will use a linear classifier with stochastic gradient descent, sklearn.linear_model.SGDClassifier[11], as our model. First we will generate and save our data in 4 forms: unigram and bigram matrix (with both counts and tf-idf values for each). Then we will train and evaluate our model for each these 4 data representations using SGDClassifier with the default parameters. After that, we choose the data representation which led to the best score and we will tune the hyper-parameters of our model with this data form using cross-validation in order to obtain the best results.", "Now, for each data form we split it into train & validation sets, train a SGDClassifier and output the score.", "The best data form seems to be bigram with tf-idf as it gets the highest validation accuracy: 0.9; we will use it next for hyper-parameter tuning.", "For this part we will use RandomizedSearchCV[12] which chooses the parameters randomly from the list that we give, or according to the distribution that we specify from scipy.stats (e.g. uniform); then is estimates the test error by doing cross-validation and after all iterations we can find the best estimator, the best parameters and the best score in the variables best_estimator_, best_params_ and best_score_.", "Because the search space for the parameters that we want to test is very big and it may need a huge number of iterations until it finds the best combination, we will split the set of parameters in 2 and do the hyper-parameter tuning process in two phases. First we will find the optimal combination of loss, learning_rate and eta0 (i.e. initial learning rate); and then for penalty and alpha.", "The output that we get is:", "Because we got \u201clearning_rate = optimal\u201d to be the best, then we will ignore the eta0 (initial learning rate) as it isn\u2019t used when learning_rate=\u2019optimal\u2019; we got this value of eta0 just because of the randomness involved in the process.", "So, the best parameters that I got are:", "And we got 90.18% test accuracy. That\u2019s not bad for our simple linear model. There are more advanced methods that give better results. The current state-of-the-art on this dataset is 97.42% [13]", "[1] Sentiment Analysis \u2014 Wikipedia[2] Learning Word Vectors for Sentiment Analysis[3] Bag-of-words model \u2014 Wikipedia[4] Tf-idf \u2014 Wikipedia[5] TfidfTransformer \u2014 Scikit-learn documentation[6] Stop words \u2014 Wikipedia[7] A list of English stopwords[8] CountVectorizer \u2014 Scikit-learn documentation[9] Scipy sparse matrices[10] Compressed Sparse Row matrix[11] SGDClassifier \u2014 Scikit-learn documentation[12] RandomizedSearchCV \u2014 Scikit-learn documentation[13] Sentiment Classification using Document Embeddings trained with Cosine Similarity", "The Jupyter notebook can be found here.", "I hope you found this information useful and thanks for reading!", "This article is also posted on my own website here. Feel free to have a look!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Passionate about Data Science, AI, Programming & Math | Owner of \u2207\u00b2 https://www.nablasquared.com/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F54c8e7c5d2f0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dorianlazar.medium.com/?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": ""}, {"url": "https://dorianlazar.medium.com/?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "Dorian Lazar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F79574042e17b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&user=Dorian+Lazar&userId=79574042e17b&source=post_page-79574042e17b----54c8e7c5d2f0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54c8e7c5d2f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54c8e7c5d2f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/ro/photos/smiley-emoticon-furie-sup%C4%83rat-2979107/", "anchor_text": "pixabay.com"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", "anchor_text": "Large Movie Review Dataset v1.0"}, {"url": "https://en.wikipedia.org/wiki/Sentiment_analysis", "anchor_text": "Sentiment Analysis \u2014 Wikipedia"}, {"url": "http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf", "anchor_text": "Learning Word Vectors for Sentiment Analysis"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "Bag-of-words model \u2014 Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "Tf-idf \u2014 Wikipedia"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html", "anchor_text": "TfidfTransformer \u2014 Scikit-learn documentation"}, {"url": "https://en.wikipedia.org/wiki/Stop_words", "anchor_text": "Stop words \u2014 Wikipedia"}, {"url": "https://gist.github.com/sebleier/554280", "anchor_text": "A list of English stopwords"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "CountVectorizer \u2014 Scikit-learn documentation"}, {"url": "https://docs.scipy.org/doc/scipy/reference/sparse.html", "anchor_text": "Scipy sparse matrices"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix", "anchor_text": "Compressed Sparse Row matrix"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "anchor_text": "SGDClassifier \u2014 Scikit-learn documentation"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html", "anchor_text": "RandomizedSearchCV \u2014 Scikit-learn documentation"}, {"url": "https://www.aclweb.org/anthology/P19-2057.pdf", "anchor_text": "Sentiment Classification using Document Embeddings trained with Cosine Similarity"}, {"url": "https://github.com/lazuxd/simple-imdb-sentiment-analysis/blob/master/sentiment-analysis.ipynb", "anchor_text": "here"}, {"url": "https://www.nablasquared.com/building-a-sentiment-classifier-using-scikit-learn/", "anchor_text": "here"}, {"url": "https://medium.com/tag/sentiment-analysis?source=post_page-----54c8e7c5d2f0---------------sentiment_analysis-----------------", "anchor_text": "Sentiment Analysis"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----54c8e7c5d2f0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----54c8e7c5d2f0---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/nlp?source=post_page-----54c8e7c5d2f0---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----54c8e7c5d2f0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54c8e7c5d2f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&user=Dorian+Lazar&userId=79574042e17b&source=-----54c8e7c5d2f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54c8e7c5d2f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&user=Dorian+Lazar&userId=79574042e17b&source=-----54c8e7c5d2f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54c8e7c5d2f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F54c8e7c5d2f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----54c8e7c5d2f0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----54c8e7c5d2f0--------------------------------", "anchor_text": ""}, {"url": "https://dorianlazar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dorianlazar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dorian Lazar"}, {"url": "https://dorianlazar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "593 Followers"}, {"url": "https://www.nablasquared.com/", "anchor_text": "https://www.nablasquared.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F79574042e17b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&user=Dorian+Lazar&userId=79574042e17b&source=post_page-79574042e17b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fdaa579c06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0&newsletterV3=79574042e17b&newsletterV3Id=9fdaa579c06&user=Dorian+Lazar&userId=79574042e17b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}