{"url": "https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35", "time": 1683005441.532469, "path": "towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35/", "webpage": {"metadata": {"title": "All about Feature Scaling. Scale data for better performance of\u2026 | by Baijayanta Roy | Towards Data Science", "h1": "All about Feature Scaling", "description": "Machine learning is like making a mixed fruit juice. If we want to get the best-mixed juice, we need to mix all fruit not by their size but based on their right proportion. We just need to remember\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.linkedin.com/in/baijayantaroy", "anchor_text": "LinkedIn", "paragraph_index": 42}, {"url": "https://baijayanta.medium.com/membership", "anchor_text": "https://baijayanta.medium.com/membership", "paragraph_index": 43}], "all_paragraphs": ["Machine learning is like making a mixed fruit juice. If we want to get the best-mixed juice, we need to mix all fruit not by their size but based on their right proportion. We just need to remember apple and strawberry are not the same unless we make them similar in some context to compare their attribute. Similarly, in many machine learning algorithms, to bring all features in the same standing, we need to do scaling so that one significant number doesn\u2019t impact the model just because of their large magnitude.", "Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one.", "The most common techniques of feature scaling are Normalization and Standardization.", "Normalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. While Standardization transforms the data to have zero mean and a variance of 1, they make our data unitless. Refer to the below diagram, which shows how data looks after scaling in the X-Y plane.", "Machine learning algorithm just sees number \u2014 if there is a vast difference in the range say few ranging in thousands and few ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant number starts playing a more decisive role while training the model.", "The machine learning algorithm works on numbers and does not know what that number represents. A weight of 10 grams and a price of 10 dollars represents completely two different things \u2014 which is a no brainer for humans, but for a model as a feature, it treats both as same.", "Suppose we have two features of weight and price, as in the below table. The \u201cWeight\u201d cannot have a meaningful comparison with the \u201cPrice.\u201d So the assumption algorithm makes that since \u201cWeight\u201d > \u201cPrice,\u201d thus \u201cWeight,\u201d is more important than \u201cPrice.\u201d", "So these more significant number starts playing a more decisive role while training the model. Thus feature scaling is needed to bring every feature in the same footing without any upfront importance. Interestingly, if we convert the weight to \u201cKg,\u201d then \u201cPrice\u201d becomes dominant.", "Another reason why feature scaling is applied is that few algorithms like Neural network gradient descent converge much faster with feature scaling than without it.", "One more reason is saturation, like in the case of sigmoid activation in Neural Network, scaling would help not to saturate too fast.", "Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scale, the feature with a higher value range starts dominating when calculating distances, as explained intuitively in the \u201cwhy?\u201d section.", "The ML algorithm is sensitive to the \u201crelative scales of features,\u201d which usually happens when it uses the numeric values of the features rather than say their rank.", "In many algorithms, when we desire faster convergence, scaling is a MUST like in Neural Network.", "Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions do not work correctly without normalization. For example, the majority of classifiers calculate the distance between two points by the distance. If one of the features has a broad range of values, the distance governs this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.", "Even when the conditions, as mentioned above, are not satisfied, you may still need to rescale your features if the ML algorithm expects some scale or a saturation phenomenon can happen. Again, a neural network with saturating activation functions (e.g., sigmoid) is a good example.", "Rule of thumb we may follow here is an algorithm that computes distance or assumes normality, scales your features.", "Some examples of algorithms where feature scaling matters are:", "Algorithms that do not require normalization/scaling are the ones that rely on rules. They would not be affected by any monotonic transformations of the variables. Scaling is a monotonic transformation. Examples of algorithms in this category are all the tree-based algorithms \u2014 CART, Random Forests, Gradient Boosted Decision Trees. These algorithms utilize rules (series of inequalities) and do not require normalization.", "Algorithms like Linear Discriminant Analysis(LDA), Naive Bayes is by design equipped to handle this and give weights to the features accordingly. Performing features scaling in these algorithms may not have much effect.", "Few key points to note :", "Below are the few ways we can do feature scaling.", "1) Min Max Scaler2) Standard Scaler3) Max Abs Scaler4) Robust Scaler5) Quantile Transformer Scaler6) Power Transformer Scaler7) Unit Vector Scaler", "For the explanation, we will use the table shown in the top and form the data frame to show different scaling methods.", "Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g., between zero and one. This Scaler shrinks the data within the range of -1 to 1 if there are negative values. We can set the range like [0,1] or [0,5] or [-1,1].", "This Scaler responds well if the standard deviation is small and when a distribution is not Gaussian. This Scaler is sensitive to outliers.", "The Standard Scaler assumes data is normally distributed within each feature and scales them such that the distribution centered around 0, with a standard deviation of 1.", "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. If data is not normally distributed, this is not the best Scaler to use.", "Scale each feature by its maximum absolute value. This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set is 1.0. It does not shift/center the data and thus does not destroy any sparsity.", "On positive-only data, this Scaler behaves similarly to Min Max Scaler and, therefore, also suffers from the presence of significant outliers.", "As the name suggests, this Scaler is robust to outliers. If our data contains many outliers, scaling using the mean and standard deviation of the data won\u2019t work well.", "This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). The centering and scaling statistics of this Scaler are based on percentiles and are therefore not influenced by a few numbers of huge marginal outliers. Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required.", "Let\u2019s now see what happens if we introduce an outlier and see the effect of scaling using Standard Scaler and Robust Scaler (a circle shows outlier).", "This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is, therefore, a robust pre-processing scheme.", "The cumulative distribution function of a feature is used to project the original values. Note that this transform is non-linear and may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable. This is also sometimes called as Rank scaler.", "The above example is just for illustration as Quantile transformer is useful when we have a large dataset with many data points usually more than 1000.", "The power transformer is a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to the variability of a variable that is unequal across the range (heteroscedasticity) or situations where normality is desired.", "The power transform finds the optimal scaling factor in stabilizing variance and minimizing skewness through maximum likelihood estimation. Currently, Sklearn implementation of PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.", "Scaling is done considering the whole feature vector to be of unit length. This usually means dividing each component by the Euclidean length of the vector (L2 Norm). In some applications (e.g., histogram features), it can be more practical to use the L1 norm of the feature vector.", "Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.", "If we plot, then it would look as below for L1 and L2 norm, respectively.", "The below diagram shows how data spread for all different scaling techniques, and as we can see, a few points are overlapping, thus not visible separately.", "Feature scaling is an essential step in Machine Learning pre-processing. Deep learning requires feature scaling for faster convergence, and thus it is vital to decide which feature scaling to use. There are many comparison surveys of scaling methods for various algorithms. Still, like most other machine learning steps, feature scaling too is a trial and error process, not a single silver bullet.", "I look forward to your comment and share if you have any unique experience related to feature scaling. Thanks for reading. You can connect me @LinkedIn.", "For only $5/month, get unlimited access to the most inspiring and uplifting content\u2026 Click on the link below to become a Medium member and support my writing. Thank you!https://baijayanta.medium.com/membership", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science | Machine Learning | Deep Learning | Artificial Intelligence | Quantum Computing"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbcc0ad75cb35&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://baijayanta.medium.com/?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": ""}, {"url": "https://baijayanta.medium.com/?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "Baijayanta Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F583a83b12a79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&user=Baijayanta+Roy&userId=583a83b12a79&source=post_page-583a83b12a79----bcc0ad75cb35---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbcc0ad75cb35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbcc0ad75cb35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://stackoverflow.com/questions/46686924/why-scaling-data-is-very-important-in-neural-networklstm/46688787#46688787", "anchor_text": "Photo Credit"}, {"url": "http://www.linkedin.com/in/baijayantaroy", "anchor_text": "LinkedIn"}, {"url": "https://baijayanta.medium.com/membership", "anchor_text": "https://baijayanta.medium.com/membership"}, {"url": "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html", "anchor_text": "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"}, {"url": "https://www.kdnuggets.com/2019/04/normalization-vs-standardization-quantitative-analysis.html", "anchor_text": "https://www.kdnuggets.com/2019/04/normalization-vs-standardization-quantitative-analysis.html"}, {"url": "https://scikit-learn.org/stable/modules/preprocessing.html", "anchor_text": "https://scikit-learn.org/stable/modules/preprocessing.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bcc0ad75cb35---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bcc0ad75cb35---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----bcc0ad75cb35---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/programming?source=post_page-----bcc0ad75cb35---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----bcc0ad75cb35---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbcc0ad75cb35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&user=Baijayanta+Roy&userId=583a83b12a79&source=-----bcc0ad75cb35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbcc0ad75cb35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&user=Baijayanta+Roy&userId=583a83b12a79&source=-----bcc0ad75cb35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbcc0ad75cb35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbcc0ad75cb35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bcc0ad75cb35---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bcc0ad75cb35--------------------------------", "anchor_text": ""}, {"url": "https://baijayanta.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://baijayanta.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Baijayanta Roy"}, {"url": "https://baijayanta.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F583a83b12a79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&user=Baijayanta+Roy&userId=583a83b12a79&source=post_page-583a83b12a79--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff205a028dace&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-about-feature-scaling-bcc0ad75cb35&newsletterV3=583a83b12a79&newsletterV3Id=f205a028dace&user=Baijayanta+Roy&userId=583a83b12a79&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}