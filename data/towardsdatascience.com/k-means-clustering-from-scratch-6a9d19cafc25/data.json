{"url": "https://towardsdatascience.com/k-means-clustering-from-scratch-6a9d19cafc25", "time": 1683005498.324235, "path": "towardsdatascience.com/k-means-clustering-from-scratch-6a9d19cafc25/", "webpage": {"metadata": {"title": "K-Means Clustering From Scratch. We Learn How K-Means Clustering Works\u2026 | by Tony Yiu | Towards Data Science", "h1": "K-Means Clustering From Scratch", "description": "K-means clustering (referred to as just k-means in this article) is a popular unsupervised machine learning algorithm (unsupervised means that no target variable, a.k.a. Y variable, is required to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/yiuhyuk/knn", "anchor_text": "If you would like to see the code in its entirety, you can grab it from GitHub here.", "paragraph_index": 2}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "https://tonester524.medium.com/membership", "paragraph_index": 27}], "all_paragraphs": ["K-means clustering (referred to as just k-means in this article) is a popular unsupervised machine learning algorithm (unsupervised means that no target variable, a.k.a. Y variable, is required to train the algorithm). When we are presented with data, especially data with lots of features, it\u2019s helpful to bucket them. By sorting similar observations together into a bucket (a.k.a. cluster), we can then compare and contrast the buckets. Understanding the features that drive cross-bucket differentiation gives us critical clues towards what features to focus on when creating an analysis or building a model (we may even want to use our clusters directly as a model feature).", "So that\u2019s clustering in a nutshell. Now let\u2019s see how k-means separates our observations into meaningful clusters.", "If you would like to see the code in its entirety, you can grab it from GitHub here.", "I already downloaded it for a previous post so we\u2019re going to use the Titanic dataset again today:", "Since our main focus is to build k-means and explore how it works, we will just work with 2 columns from the dataset: fare (price paid for the ticket) and age of the passenger. We will also drop nulls. I sort the data by fare and then age because I will eventually pick the top k observations to be the cluster centers (centroids). Sorting ensures that I will pick k observations that are all very similar to each other as my initial centroids. This way, the starting centroids will be suboptimal and we can more clearly see how the algorithm is able to converge to much better centroids (and clusters).", "Our data that we will cluster is stored in the array, cluster_array. It looks like the following (first column is fare, second is age):", "Before we code it up, let\u2019s first understand conceptually how the k-means algorithm works. K-means imagines each cluster as a solar system. The star that everything (all the observations) in the cluster revolves around is known as the cluster\u2019s centroid.", "So given a set of centroids and their coordinates (where the X coordinate is fare and the Y coordinate is age), we can easily figure out what cluster each observation belongs to by calculating which centroid it is closest to (in terms of Euclidean distance).", "But how do we decide where the centroids are? That\u2019s where the k-means algorithm comes in. First, we pick a value for k, the number of centroids (this is a hyperparameter that we must tune). Let\u2019s say we pick k to be 4. Then we can just pick 4 points at random and assign them to be our starting centroids. And using our randomly chosen starting centroids, we can create 4 clusters. Sounds kind of silly right? What\u2019s the point to picking random centroids and creating random clusters?", "Here\u2019s the trick: the means of our clusters become our new centroids (for each cluster, we calculate the mean fare and mean age and that\u2019s the coordinate of our new centroid). And as long as our starting randomly picked centroids were even slightly different from each other, the new centroids (the cluster means) will be more optimal than our initial clusters; where optimality is defined as maximizing similarity within cluster and difference across clusters.", "Once we have our new centroids, we can reassign each observations\u2019 cluster based on which of the new centroids it is closest to. Since the centroids became a bit more optimal, our clusters should improve too (in terms of homogeneity within cluster and variance across clusters). And now we can again calculate new centroids from the means of the coordinates of the reassigned clusters. These centroids will have again improved upon their predecessors, and we can keep rinsing and repeating this process until the algorithm has converged. Convergence is defined as when we are no longer able to decrease the sum of squared deviations from the centroid (a.k.a. cluster mean) for all clusters. The sum of squared deviations from the mean is a measure of how alike the members of a cluster are to each other \u2014 the lower the value, the more similar and better.", "An example might help you better understand why this algorithm works. Imagine a room full of people with different heights (image below). We want to sort them into two clusters based on the only observable parameter, height. So we choose two people at random (the ones in gray) and tell everyone else to stand next to whoever they are closest in height to (coin flip to break ties). Once they do so, the mean of the group (or person with heigh equal to the mean) becomes the new centroid (in green). Notice how this serves to push the centroids apart \u2014 the green centroids are further apart than the gray ones. This pushing apart of all the centroids (in reality we would have more than 1 feature and 2 centroids) is what tends to happen as we repeatedly take the mean and re-cluster (assuming we didn\u2019t take points at maximum and opposite extremes from each other as our initial centroids).", "Before we start coding it up, let\u2019s refresh ourselves on the steps:", "To make life easier, let\u2019s define a few helper functions. First let\u2019s write one to calculate Euclidean distance between 2 points(a.k.a. the straight line distance between 2 points):", "Next, we need a function that given a set of centroids, can tell us which cluster each observation belongs to. The following function uses nested for loops (not efficient I know) to calculate the distance between every observation and every centroid (using our calc_distance function). Then it assigns an observation to a cluster based on whichever centroid it is closest to. The output is the list of each observation\u2019s cluster label.", "Now we need a function for the updating step where we assign new centroids. The following function concatenates the data (fare and age of each observation), cluster_array, and the current cluster that it belongs to, clusters, together into a dataframe, cluster_df. We can then filter cluster_df by cluster to get just the observations that belong to a particular cluster and calculate the mean of those observations. These calculated means are our new centroids.", "The returned value of calc_centroids is an array where the first column is the mean fare of each cluster and the second column is the mean age of each cluster:", "The last helper function we need is more for reporting purposes. We want to know what the variance within the cluster is, or in other words how similar or dissimilar the observations within a cluster are to each other. So let\u2019s build a function to calculate the sum of squared deviations from the centroid for each cluster. This function filters cluster_df by cluster, calculates the mean, and then subtracts the cluster mean from each observation within the cluster. The function, repmat, takes a given array and replicates it \u2014 in our case we want to copy the mean for as many times as we have observations so that we can directly subtract the two arrays.", "Cool, we are ready to run k-means now.", "Let\u2019s go with 4 clusters still (k=4). Like I stated previously, we will purposefully choose bad starting centroids so that we can see the improvements made by the algorithm (I use observations 2, 3, 4, and 5 because they produce really bad starting clusters) \u2014 in reality, we don\u2019t want to do this as it slows things down. Then we use our helper function, assign_clusters, to assign each observation to a cluster based on the centroid that it\u2019s closest to.", "Following this, we run a for loop 20 times (20 is enough for convergence in this case) where we repeatedly calculate new centroids (using calc_centroids) and new clusters (using assign_clusters) so that we can obtain optimal clusters. Recall that by repeating this process of calculating cluster means (a.k.a. new centroids) and assigning new clusters based on these new centroids is how the algorithm converges to the final clusters.", "For each iteration in my loop, I stored the average of the clusters\u2019 sums of squared deviations from their centroids in the list cluster_vars. This average is an approximate measure of the level of variance within each cluster (for all clusters). Because we want the cluster members to be as alike as possible, we would like this average to be as low as possible. Let\u2019s check out how this average evolves as we iterate. It decreases drastically at the start and then levels off. By the sixth iteration of our loop, it\u2019s more or less converged.", "Let\u2019s take a look to see if our clusters got better. Recall that we started with arbitrarily chosen centroids. Here are the clusters based on those initial centroids (each color is a cluster). Seems like we got lucky with some age based separation, but that\u2019s about it.", "Now here are the converged clusters. These clusters seem to be more meaningfully differentiated:", "Another way we can check how we did is by seeing whether the survival probabilities of our clusters are different. Remember that k-means clustering is unsupervised; so we are not explicitly training the model to predict whether a passenger survived. Rather we are hoping that by producing meaningfully differentiated clusters, we can find groups that also happen to behave differently in terms of the things we care about (in this case survival). And it seems like our converged clusters do a better job of predicting survival relative to our initial ones.", "Obviously there is a lot more we can do including adding more features (versus just 2), tuning k (the number of clusters), and trying to better understand the identity and key characteristics of each cluster. But that\u2019s something for another day. I hope that by reading this post, you gained some insight into how clustering works, and how k-means uses a wonderfully simple algorithm to produce some pretty cool results. Cheers and stay safe everyone!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist. Founder Alpha Beta Blog. Doing my best to explain the complex in plain English. Support my writing: https://tonester524.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6a9d19cafc25&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tonester524.medium.com/?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "Tony Yiu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840a3210fbe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&user=Tony+Yiu&userId=840a3210fbe7&source=post_page-840a3210fbe7----6a9d19cafc25---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a9d19cafc25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a9d19cafc25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@melipoole?utm_source=medium&utm_medium=referral", "anchor_text": "Mel Poole"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/yiuhyuk/knn", "anchor_text": "If you would like to see the code in its entirety, you can grab it from GitHub here."}, {"url": "https://unsplash.com/@rosssneddon?utm_source=medium&utm_medium=referral", "anchor_text": "Ross Sneddon"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6a9d19cafc25---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6a9d19cafc25---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----6a9d19cafc25---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/technology?source=post_page-----6a9d19cafc25---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6a9d19cafc25---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a9d19cafc25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&user=Tony+Yiu&userId=840a3210fbe7&source=-----6a9d19cafc25---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a9d19cafc25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&user=Tony+Yiu&userId=840a3210fbe7&source=-----6a9d19cafc25---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a9d19cafc25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6a9d19cafc25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6a9d19cafc25---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6a9d19cafc25--------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tony Yiu"}, {"url": "https://tonester524.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "102K Followers"}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "https://tonester524.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840a3210fbe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&user=Tony+Yiu&userId=840a3210fbe7&source=post_page-840a3210fbe7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F78d3e392d884&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-means-clustering-from-scratch-6a9d19cafc25&newsletterV3=840a3210fbe7&newsletterV3Id=78d3e392d884&user=Tony+Yiu&userId=840a3210fbe7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}