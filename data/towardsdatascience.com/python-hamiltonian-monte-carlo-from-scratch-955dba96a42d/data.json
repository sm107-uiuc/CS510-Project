{"url": "https://towardsdatascience.com/python-hamiltonian-monte-carlo-from-scratch-955dba96a42d", "time": 1683012952.1242611, "path": "towardsdatascience.com/python-hamiltonian-monte-carlo-from-scratch-955dba96a42d/", "webpage": {"metadata": {"title": "Python: Hamiltonian Monte Carlo from scratch | by Jake | Towards Data Science", "h1": "Python: Hamiltonian Monte Carlo from scratch", "description": "If your experience with Markov Chain Monte Carlo (MCMC) methods has been anything like mine, it\u2019s been an endless search for answers. Every time you get an answer to one question, a dozen more pop\u2026"}, "outgoing_paragraph_urls": [{"url": "https://colab.research.google.com/drive/1YQBSfS1Nb8a9TAMsV1RjWsiErWqXLbrj#scrollTo=wKD7ltATG588", "anchor_text": "public Google Colab Notebook", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1111.4246", "anchor_text": "NUTS", "paragraph_index": 15}], "all_paragraphs": ["If your experience with Markov Chain Monte Carlo (MCMC) methods has been anything like mine, it\u2019s been an endless search for answers. Every time you get an answer to one question, a dozen more pop up! You\u2019re continuously learning how much you don\u2019t know. I recently accepted a full time role as data scientist at an organization that makes extensive use of Bayesian Statistics and by extension, MCMC methods. So I decided to go to war with this branch of mathematics once and for all! For a detailed review of Metropolis, Metropolis-Hastings, and Hamiltonian Monte Carlo, please visit my public Google Colab Notebook :)", "I\u2019ll revisit the key ideas behind Metropolis-Hastings (MH) before diving into the new material because Hamiltonian Monte Carlo (HMC) is an extension of MH.", "Metropolis-Hastings is a glorified random walk. You need four elements: A starting point, a target distribution, a proposal distribution, and an impartial judge (a random event.) The target distribution can be any distribution you\u2019d like to sample from; all you need is the probability density function (PDF). Likewise, the proposal distribution can be any PDF (although the math is simpler if the distribution is symmetric.) The whole idea is start at the random point using the proposal distribution. Append it to an array. Feed this point into the target\u2019s PDF to get the probability density (aka likelihood). Now, generate a disturbance via the proposal distribution, add it the the current location, assigning this variable a new name, and evaluate its likelihood, as well, via the target\u2019s PDF. Compare the likelihoods", "This number will fall anywhere in the range [0, inf). If it\u2019s less than 1, there\u2019s an x% chance that you move. Anything above 1 is guaranteed movement. This is where the \u201cimpartial judge\u201d comes in. Observe a random event (typically a number in [0,1] sampled from a uniform distribution.) If this number is less than or equal to acceptance, you move to the proposed point. Else, you stay at the previous location. Append the winner to the array. And that\u2019s it to the Metropolis algorithm.", "Things are slightly more complicated if the proposal distribution is asymmetric. If you sample movements from a normal distribution, uniform distribution, etc this step is unnecessary. However, if you use a distribution, such as gamma, poisson, exponential, lognormal, etc. then you\u2019ll need to account for the inherent bias. In the Colab Notebook I linked at the beginning, I use a Beta proposal distribution with a (-0.5 constant term), biased towards 1 (as opposed to symmetric or biased toward 0.) Two things to note: First, the Beta distribution is defined [0,1] meaning we will never observe a negative disturbance, which means that our proposals will increasingly move only one direction. I added the -0.5 bias such that now positive and negative samples can disturbances can be sampled. Second, however, the distribution is still biased towards 1, meaning that it will slowly creep in only one direction. We need to account for this. We do this by getting the likelihoods of the disturbance and the reverse disturbance.", "This correction term (as you\u2019ll see in the Colab notebook) is multiplied by the likelihood ratio; it accounts for the asymmetry of the proposal distribution. (Note, this ratio is called the Hastings-ratio as it\u2019s the only difference between Metropolis and MH algorithms.)", "You can sample from most distributions using MH algorithm. However, at the end of the day, it\u2019s fundamentally a random walk. There\u2019s no logic informing how large the jumps should be given the current position. When sampling 1 variable distributions, this isn\u2019t an issue. But as dimensionality increases, the region of high likelihood composes only a fraction of the total area, whereas the moderate and low likelihood regions compose (exponentially) more of the total area. This effect imposes only a mild inefficiency on MH when sampling low dimensional distributions. But as dimensionality increases, MH runs the risk of returning samples that aren\u2019t representative of the target distribution.", "The reason is that small jumps are appropriate near the peak(s), because over-exploration of this region, relative to its likelihood, is low risk; however, as we move towards the tails, over-exploration of these areas, relative to their likelihoods, becomes a sizable risk. (Many samples from one tail and none in the other isn\u2019t desirable, for context.) So\u2026how do we propose jumps with likelihood in mind?", "Physics has the answer \u2014 hurray! For the remainder of this post, we\u2019re not going to view locations as a random walk. Rather, we\u2019re going to view the absolute peak (or peaks) of a distribution as a planet. And a satellite will orbit this planet, collecting samples. Hamiltonian mechanics uses differential equations relating kinetic energy to potential energy. These differential equations have really tricky exact solutions but friendly approximate solutions using leapfrog integration. Integration of our differential equations will give us the path which our satellite will follow. These differential equations (or Hamiltonian equations) define the energy of a system in terms of kinetic and potential energy. When you throw a ball up into the air, its kinetic energy is replaced by potential energy. As the ball falls, its potential energy is replaced by kinetic energy. Hamiltonian equations define the relationship between position and momentum:", "Note the above equations are derived in the context of statistics. The physics world is necessarily, far more complicated.", "Of note, our differential equations are dQ/dT, the change in position wrt (with respect to) time and dP/dT, the change in momentum wrt time, which evaluates to the change in potential energy wrt position. We can define position in terms of the PDF, itself. And so, changes in momentum are a function of position in the PDF, namely distance from the peak. Note that the normal distribution\u2019s gradient is steepest at inflection points between the peak and the tail(s), respectively. Conversely, the negative log normal distribution\u2019s gradient is steepest at the tails and approaches (and reaches) 0 as it approaches the peak. As we discussed earlier: We want our jump size ro be proportional to the location in the distribution. Near the peak, we want to take small jumps. As we approach the tails, we want to take progressively bigger and bigger jumps. It turns out that the negative-log normal distribution gradient accomplishes this effect perfectly. (Whereas the normal distribution gradient would propose small jumps from both the peak and the tails, which we don\u2019t want!)", "The algorithm breaks down into four parts:", "You might have noticed in step-1 that we sampled momentum from a gaussian. What\u2019s that all about? Using our satellite metaphor from before, Hamiltonian motion will guide our orbital path around a specific likelihood (visiting positions associated with the same likelihood.) However, we\u2019re not interested in exploring just one likelihood, we\u2019d like to explore them all. In order to accomplish this effect, we sample momentum \u201ckicks,\u201d which can cause our satellite to jump or fall orbital distances (aka visit different likelihoods.)", "And let\u2019s take a look at performance!", "If you\u2019d like to run my code, please hop onto the Google Colab link from the beginning. You\u2019ll only have viewer access, not editor access \u2014 but you can copy to your own Google Drive for experimentation, download the iPython Notebook file, etc. If you play around with the code, you\u2019ll notice that path length and step size are very sensitive hyper-parameters. The slightest adjustment might result in nonsensical samples. This is the curse of gradient-based MCMC samplers. However, there\u2019s hope \u2014 newer methods, such as No-U-Turn-Sampler (NUTS) builds upon HMC by dynamically choosing path length and step size.", "But why is this a problem? Well, from our hypothetical satellite orbit example, Hamiltonian equations will guide the motion of the satellite, \u201corbiting\u201d a particular likelihood, meaning it proposes new locations near similar likelihoods. But the orbital path around any given likelihood varies in length. At higher likelihoods, the circumference is quite small but larger when orbiting lower likelihoods (closer to the tails.) We can visually confirm this by simply looking at the gaussian distribution. What happens when me make a 360 orbit around a given likelihood? We propose the same point we\u2019re currently at. This is called a \u201cU-Turn\u201d in HMC lingo, meaning that we come back to where we\u2019ve just been. Dynamic adjustments to path length and step size control the extent of integration (the orbital path.) And as you\u2019ll see if you play with the code a bit, these hyper-parameters are quite sensitive \u2014 hence you\u2019ll likely use NUTS over HMC in practice.", "I\u2019ve got to give credit where it\u2019s due. Colin Carroll, a software engineer at Google, wrote a more scalable implementation of HMC. (Mine can only be used to sample one-variable gaussians.) The scalability came at a price \u2014 interpretability. I found that to really \u201cget\u201d the concept, I had to dumb it down and build one from scratch. Hopefully, you\u2019ve enjoyed the read!", "Thank you for reading \u2014 If you think my content is alright, please subscribe! :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Amazon Scientist, author/blogger, and continual learner"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F955dba96a42d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----955dba96a42d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----955dba96a42d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jacob.d.moore1?source=post_page-----955dba96a42d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jacob.d.moore1?source=post_page-----955dba96a42d--------------------------------", "anchor_text": "Jake"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F58a07ece941f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&user=Jake&userId=58a07ece941f&source=post_page-58a07ece941f----955dba96a42d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F955dba96a42d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F955dba96a42d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://colab.research.google.com/drive/1YQBSfS1Nb8a9TAMsV1RjWsiErWqXLbrj#scrollTo=wKD7ltATG588", "anchor_text": "public Google Colab Notebook"}, {"url": "https://arxiv.org/abs/1111.4246", "anchor_text": "NUTS"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----955dba96a42d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----955dba96a42d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----955dba96a42d---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----955dba96a42d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----955dba96a42d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F955dba96a42d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&user=Jake&userId=58a07ece941f&source=-----955dba96a42d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F955dba96a42d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&user=Jake&userId=58a07ece941f&source=-----955dba96a42d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F955dba96a42d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----955dba96a42d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F955dba96a42d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----955dba96a42d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----955dba96a42d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----955dba96a42d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----955dba96a42d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----955dba96a42d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----955dba96a42d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----955dba96a42d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----955dba96a42d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----955dba96a42d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jacob.d.moore1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jacob.d.moore1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jake"}, {"url": "https://medium.com/@jacob.d.moore1/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F58a07ece941f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&user=Jake&userId=58a07ece941f&source=post_page-58a07ece941f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F22fa69989e9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpython-hamiltonian-monte-carlo-from-scratch-955dba96a42d&newsletterV3=58a07ece941f&newsletterV3Id=22fa69989e9a&user=Jake&userId=58a07ece941f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}