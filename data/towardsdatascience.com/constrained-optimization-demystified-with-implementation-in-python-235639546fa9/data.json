{"url": "https://towardsdatascience.com/constrained-optimization-demystified-with-implementation-in-python-235639546fa9", "time": 1683003427.710827, "path": "towardsdatascience.com/constrained-optimization-demystified-with-implementation-in-python-235639546fa9/", "webpage": {"metadata": {"title": "Constrained Optimization demystified, with implementation in Python. | by Aakash Agrawal | Towards Data Science", "h1": "Constrained Optimization demystified, with implementation in Python.", "description": "Nonlinear constrained optimization problems are an important class of problems with a broad range of engineering, and scientific applications. In this article, we will see how the refashioning of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Bisection_method", "anchor_text": "Bisection method", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method", "anchor_text": "Newton-Rapson method", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Lagrange_multiplier", "anchor_text": "Method of Multipliers", "paragraph_index": 25}, {"url": "https://github.com/aakash2016/Optimization_", "anchor_text": "repo", "paragraph_index": 29}, {"url": "https://github.com/aakash2016/Optimization_", "anchor_text": "repo", "paragraph_index": 38}, {"url": "https://www.linkedin.com/in/akash2016123/", "anchor_text": "Linkedin", "paragraph_index": 43}], "all_paragraphs": ["Nonlinear constrained optimization problems are an important class of problems with a broad range of engineering, and scientific applications. In this article, we will see how the refashioning of simple unconstrained Optimization techniques leads to a hybrid algorithm for constrained optimization problems. Later, we will observe the robustness of the algorithm through a detailed analysis of a problem set and monitor the performance of optima by comparing the results with some of the inbuilt functions in python.", "Many engineering design and decision making problems have an objective of optimizing a function and simultaneously have a requirement for satisfying some constraints arising due to space, strength, or stability considerations. So, Constrained optimization refers to the process of optimizing an objective function with respect to some variables in the presence of constraint of those variables.", "A constrained optimization problem with N variables is given by:", "-where g\u2c7c(x) are the J inequality constraints, h\u2096(x) are the K equality constraints, f(x) is the objective function to be optimized. Let us understand some of the frequently used terminologies in optimization.", "In the parlance of mathematical optimization, there are two routes by which one can find the optimum(Numerically):", "1. Using Direct Search methods: Here, we only use the function values at a given point to find the optimum. It works by comparing the function values in a neighborhood of a point and subsequently moving in the direction that leads to a decrease in the function value (for minimization problems). Direct Search methods are typically used when the function is discontinuous, and hence the derivate is not available at that point.", "2. Using gradient-based methods: Here, we use the first and second-order derivatives to locate the optima. These methods take the gradient information into account and thus have the advantage of converging faster to the optima.", "How to find the derivative at a particular point numerically? We use the central difference method, which is mathematically given as {limit h ->0}:", "Our proposed algorithm for constraint optimization hires two single variable optimization methods and one multi-variable optimization method. Our main intention is to convert this multivariable constraint optimization problem into an unconstraint multi-variable optimization problem, and this unconstraint problem then can be solved using the single variable optimization methods.", "Again, there are two routes for finding the optimum of a linear or non-linear function of a single variable, one is using direct search methods, and the other is through the gradient-based techniques. One can find the optima using solely either of the approaches. Our algorithm for constraint optimization uses both approaches. Using the direct search method, we will bracket the optima, and once we have a particular bound for the optima, we can find the exact optima using the gradient-based method (for single variable function).", "There are many direct search and gradient-based methods for obtaining the optimum of a single variable function. Our method uses the Bounding Phase Method and the Secant method.", "Note: all the optimization methods described are iterative. We converge to the optimum value gradually after a series of iterations.", "Bounding Phase Method: A direct search method to find lower and upper bounds for the minima in a single variable unconstrained optimization. The Algorithm is given as (f\u2019 refers to the 1\u02e2\u1d57 order derivative at a point):", "Secant Method: A very popular gradient-based method for a single variable optimization. The termination condition is when the gradient of a function is very small (~0) at a point. The method is as follows:", "Other popular gradient-based methods for single variable optimization are the Bisection method, Newton-Rapson method, etc.", "Unidirectional Search: Here, the goal is to find where the function value will be minimum in a particular direction. Mathematically, we need to find scalar \u03b1(alpha) such that, f(\u03b1) = f(x\u207d\u1d57\u207e+\u03b1.s\u207d\u1d57\u207e) is minimized, which is achieved using the single variable optimization methods. {s\u207d\u1d57\u207e = search direction}.", "Note: Many multi-variable optimization techniques are nothing but successive unidirectional search, to find the minimum point along a particular direction.", "The DFP method is a gradient-based multi-variable optimization algorithm. It results in a faster convergence to the optima by not taking into account the hessian for creating a search direction, thereby overcoming the limitations of several other multi-variable optimization algorithms. The search direction is given by:", "where the matrix A is given by:", "e(x\u207d\u1d4f\u207e) represents the gradient of the function at a point x\u207d\u1d4f\u207e. The uni-direction search involves the secant method and the bounding phase method to find the value of \u03b1 in the search space. The new point obtained after the unidirectional search is :", "A penalty(regularizer) is an additional term we add to our objective function, which helps in controlling the excessive fluctuation of that objective function. By adding these penalty terms, we transform our constrained problem to an unconstrained problem structured such that minimization favors satisfaction of the constraints, as shown in the figure below.", "Simply put, the technique is to add a term to the objective function such that it produces a high cost for violation of constraints. This is known as the Penalty function method. Mathematically,", "where R is a penalty parameter, P(x, R) is the penalty function, and \u03a9 is the penalty term. There are various types of penalty terms depending upon the feasibility and type of constraint. This one is known as the bracket operator penalty term. where,", "Note: all the optimization methods described are iterative. We converge to the optimum value gradually after a series of iterations. We update this R-value at each iteration.", "There are several limitations to using the penalty function method. Firstly, It results in a distortion of the contours, due to which the algorithm takes a greater time to converge. Also, this results in the presence of artificial local optimas.", "For the purpose of implementation, we will only stick to the penalty function method. There is another method known as the Method of Multipliers, used to overcome the limitations of distortion. It is basically a slight modification of the Penalty function method. This method does not distort the contours but instead has an effect of shifting the contours towards the constraint optimum point. So, here the presence of artificial local optima will be zero.", "The method of multiplier and penalty function method both will convert a constrained optimization problem to an unconstrained problem, that further can be solved by any multi-variable optimization method.", "Well, that\u2019s it!!! If you have come this far, great! Now, let us have a look at the flow chart of our method and then go for the implementation. For the sake of implementation, we will only cover the penalty function method.", "Let\u2019s solve the following constraint optimization problem using our proposed algorithm.", "DFP method is used for multi-variable optimization, and a combination of the bounding phase and the secant method is used to obtain a uni-directional search. The code is implemented in python and hosted in my GitHub repo. We now briefly demonstrate each of the functions used:", "multi_f: This function takes an input vector x (a point in search space) and returns the function value (penalized function value) at that point.", "grad_multi_f: This function uses the central difference method to calculate the gradient vector at a particular point in search space.", "bracketing_: This function implements the bounding phase method used to bracket the \u03b1*(minima obtained by performing the uni-directional search). It takes a vector x and vector s(search direction) and outputs an interval on the basis of which \u03b1 can be evaluated.", "f _dash: This function is used to get the first-order differential for a single variable function using the central difference method. (represents f\u2019).", "secant_minima: This function takes the bounds found from the bounding phase method, a point x, and a search direction s as input and evaluates the alphastar.", "compute_z: This function is used to compute the formula used in the secant method:", "DFP: It takes only the input vector x as an argument and returns the solution vector. This function is called inside the main function until the termination conditions for the penalty function method are met.", "It starts by finding a search direction s from the input vector x, then performs a unidirectional search by calling the bounding phase and the secant method to find the optimum \u03b1. It then finds new search directions by evaluating the equations (1) and (2). The process continues until the termination condition for DFP is met, and we have found the optimum solution of this unconstrained problem for this particular sequence.", "The full code is on my Github repo. Note that our method is generalized and applicable to any number of dimensions one wants to work on. The parameter setting for our algorithm is: * M=2 {specifies the total dimensions we are working with}, * R=0.1 {panalty parameter} , * c=1.55 {factor for updating R},* x_ip (initial guess)=(0.11, 0.1)\u1d40.", "I suggest the reader to try using different initial guesses and play with these parameter values. So, our algorithm converged after 14 sequences. And we get the optimum solution to the constrained optimization problem.", "Let\u2019s compare our results with those been found from the optimize module of the scipy library in python.{Working with the same initial guess}:", "The results are quite close enough, to get even closer results, we can try very small terminating factors. I also tried a range of different initial guesses. And yes, they all converged!!", "\u201cPremature optimization is the root of all evil\u201d \u2014 Donald Knuth.", "I hope you enjoyed the ride through constraint optimization along with me. I would love to know the feedback of anyone reading this article (A clap \ud83d\udc4f\ud83c\udffc will also be good feedback \ud83d\ude07). I would be happy to answer doubts/questions on any of the concepts mentioned above. You can reach me via Linkedin.", "Finally, a special thanks to Prof. Deepak Sharma, IIT Guwahati, who taught me the optimization course as a part of my curriculum.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @CRED | IIT Guwahati 2020"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F235639546fa9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----235639546fa9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----235639546fa9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aakashagrawal?source=post_page-----235639546fa9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakashagrawal?source=post_page-----235639546fa9--------------------------------", "anchor_text": "Aakash Agrawal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce827b6548&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&user=Aakash+Agrawal&userId=93ce827b6548&source=post_page-93ce827b6548----235639546fa9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F235639546fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F235639546fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://unsplash.com", "anchor_text": "src"}, {"url": "https://en.wikipedia.org/wiki/Bisection_method", "anchor_text": "Bisection method"}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method", "anchor_text": "Newton-Rapson method"}, {"url": "https://en.wikipedia.org/wiki/Lagrange_multiplier", "anchor_text": "Method of Multipliers"}, {"url": "http://Wikimedia.org", "anchor_text": "src"}, {"url": "https://github.com/aakash2016/Optimization_", "anchor_text": "repo"}, {"url": "https://github.com/aakash2016/Optimization_", "anchor_text": "repo"}, {"url": "https://www.linkedin.com/in/akash2016123/", "anchor_text": "Linkedin"}, {"url": "https://medium.com/tag/optimization?source=post_page-----235639546fa9---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----235639546fa9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----235639546fa9---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/programming?source=post_page-----235639546fa9---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----235639546fa9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F235639546fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&user=Aakash+Agrawal&userId=93ce827b6548&source=-----235639546fa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F235639546fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&user=Aakash+Agrawal&userId=93ce827b6548&source=-----235639546fa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F235639546fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----235639546fa9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F235639546fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----235639546fa9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----235639546fa9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----235639546fa9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----235639546fa9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----235639546fa9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----235639546fa9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----235639546fa9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----235639546fa9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----235639546fa9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakashagrawal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakashagrawal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aakash Agrawal"}, {"url": "https://medium.com/@aakashagrawal/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "157 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce827b6548&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&user=Aakash+Agrawal&userId=93ce827b6548&source=post_page-93ce827b6548--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F732974c607f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-demystified-with-implementation-in-python-235639546fa9&newsletterV3=93ce827b6548&newsletterV3Id=732974c607f4&user=Aakash+Agrawal&userId=93ce827b6548&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}