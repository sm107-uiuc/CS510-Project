{"url": "https://towardsdatascience.com/tsne-simplified-cf1837349302", "time": 1683011562.697747, "path": "towardsdatascience.com/tsne-simplified-cf1837349302/", "webpage": {"metadata": {"title": "tSNE simplified. Reducing dimensions with tSNE | by Dr. Saptarsi Goswami | Towards Data Science", "h1": "tSNE simplified", "description": "Guys, to tell you the truth when I heard the name tSNE and the full form being t-distributed stochastic neighborhood embedding, I was scared. Gradually, I could find my way through the papers\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Guys, to tell you the truth when I heard the name tSNE and the full form being t-distributed stochastic neighborhood embedding, I was scared. Gradually, I could find my way through the papers, various blogs, and realized it is not so ominous as the name suggests. I just thought I will try to share the intuitions that I gathered. Let me put it down in a question, answer format like it came to my mind.", "Less number of attributes are simpler to manage. It\u2019s easy for the models to train, store, and also easy for visualization. Technically, you can eliminate some of the original features or attributes. This is called feature selection. There is another family of techniques where new features are created from the original set of features. Let\u2019s say, A typical example, where there are a group of customers and there height and weight is collected. If we want to create one feature which will represent both these attributes, we can use the Body Mass Index (BMI), which is computed as weight in Kg and Height in Square Meter. This is the simple motivation of dimension reduction.", "From a technical standpoint, these second set of methods where we construct new features are called as dimension reduction techniques.", "Can this be done arbitrarily or will it always give meaningful real-world features?", "Of-course not arbitrarily. There has to be an objective function like any ML algorithm. Let\u2019s call the original dataset as D and the dataset with a reduced or new feature set as R. let there be a function \u2018f\u2019 that measures the information contained in any dataset. The loss function/Objective function will be at an abstract level, f(D) \u2014 f(R), let\u2019s call this as J(D, R). We should try to minimize this J. If there are several possibilities reductions namely R1, R2, Rn, we should pick that version of the newly constructed features which minimizes J.", "Most of the time, no real-world i.e.domain meaning like BMI can be attached to the newly constructed features, which is a weakness of these sets of methods.", "tSNE is the t-distributed stochastic neighborhood embedding", "Let\u2019s start with stochastic neighborhood embedding. Well, a City Agra can be represented by its\u2019 latitude, longitude, alternatively, it can be represented by its distance from New Delhi.", "It\u2019s similar to the concept of word embedding, where a word is represented by the surrounding words. A vector is known by the company it keeps.", "In the above figure, there are 9 points and there are two attributes x1 and x2. The point marked in green can be represented by these two values of x1 and x2.", "Alternatively, it can be represented by its distance or similarity with k nearest neighbors (k = 4), the ones marked by black. The similarity is inversely proportional to the distance. To standardize, we can scale these similarities between a number between 0 to 1.", "If we think, this in a slightly different manner, we can think of this as the probability of selecting the neighbor, given the green point is already chosen. Let\u2019s understand this with the simple example below.", "Any time, when we are dealing with probability, we are dealing with randomness, in more formal term randomness is mentioned as stochastic.", "So instead of representing the green point by its coordinates, we can represent it as (0.91,0.045,0.045,0.01). This is called a stochastic neighborhood embedding.", "This is still not done, your question will be of course, how the dimension is reduced? Now let\u2019s call the probability distribution in the original space as p and let's also now assume that the original vectors do not have two features but a total of fifty features.", "Let\u2019s further assume that by some magic, we have transformed these vectors into two-dimensional vectors, and then again we have represented the points by its stochastic neighborhood embedding, which is nothing but a probability distribution q. When we are calculating the probability, a standard probability density function like normal distribution is assumed.", "Let\u2019s recap, so the same point is represented by a probability distribution in the original dimension called p and by another probability distribution q in reduced space. Without applying much thought, we can conclude p and q should be as similar as possible.", "How do we measure, the similarity between two probability distribution? We do that by using KL Divergence.", "The J, that we were talking about is nothing but the above equation.", "Let\u2019s say the original distribution is given as the following", "Now let\u2019s use the concept of KL Divergence which is a function of the ratio of p and q. Let\u2019s evaluate the following two set of distributions in reduced space.", "Visually, we can see that the second one is more similar to the original distribution in figure 3 and when we calculate this p/q for each point and sum it up we do see the value is less for the second alternative (4.08 instead of 5).", "Again, we will not deal with maths here, the idea is we try to preserve the neighborhood (Often called as local structure) when we embed or convert to lower dimension. In the first paper, which was called as stochastic neighborhood embedding, this neighborhood preservation was good enough. In its improvement, the authors wanted the points which are far in original space to move further in reduced space.", "Let\u2019s say, we know that the probability of a particular neighbor is 0.05, using the standard normal table we can say it\u2019s away from the mean (reference point) by a distance of 1.65. If we convert the same probability of 0.05 using a t-distribution (with a degree of freedom 20), it is 1.725. The bottom line, for the same probability, t-distribution pushes the neighbor in a lower dimension further away from the reference point.", "That\u2019s the story of t-distributed stochastic neighborhood embedding.", "what are the Parameters of tSNE?", "The simplest parameter is how many features we want in the lower dimension space. This is typically two.", "Another important parameter is perplexity, which determines when we are doing this stochastic embedding, how many neighbors to consider. If we keep a high value of perplexity, it considers all the points (Global), if we keep a small value, it will consider a few neighbors (Local). This is demonstrated with the concentric circle example.", "Figure 5 is the original data. Here we are not reducing the dimension, but are illustrating the effect of perplexity", "It\u2019s evident that with lower perplexity the red points are still close to each other but the global structure is lost. In the papers, the author suggested a value between 5 to 50.", "When we talk about dimension reduction Principal component analysis has the most recall. PCA works by preserving the variability of the data. The points that contribute most to the variability are the ones furthest from the center. Hence PCA is more influenced by such points which gives kind of global structure of the data. Also, PCA has a linearity assumption, which tSNE does not have.", "Fig 7: 2D Representation of MNIST Dataset for both PCA and tSNE (Author\u2019s notebook)", "I am sure, you know about the MNIST dataset which has 10 classes and 784 features. In the above scatter plots 2 new features are constructed using both tSNE and PCA and each class is presented by a different color in Fig 7. It is very evident in the tSNE reduced space, the classes are much more well separated. Please remember both are unsupervised methods and hence do not use the class information.", "Please look at the notebook and the videos for further details", "Our Resources from Calcutta University (CU) Data Science Lab ( A.K.Chouhdury School of IT and Statistics Department)", "If original features are important, dimensionality reduction can not be used. PCA can be a good starting point to see the structure if we do not get a good result we can go for tSNE. If it\u2019s costly to run on the entire dataset, some sampling approach can be adopted.", "[1] Hinton GE, Roweis ST. Stochastic neighbor embedding. In Advances in neural information processing systems 2003 (pp. 857\u2013864).", "[2] G\u00e9ron A. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O\u2019Reilly Media; 2019 Sep 5.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Advisor - AchieveX Solutions Pvt Ltd ,Asst Prof \u2014 CS Bangabasi Morning Clg, Lead Researcher University of Calcutta Data Science Lab, ODSC Kolkata Chapter Lead"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcf1837349302&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf1837349302--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf1837349302--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://saptarsi007.medium.com/?source=post_page-----cf1837349302--------------------------------", "anchor_text": ""}, {"url": "https://saptarsi007.medium.com/?source=post_page-----cf1837349302--------------------------------", "anchor_text": "Dr. Saptarsi Goswami"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc9848cae784&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&user=Dr.+Saptarsi+Goswami&userId=dc9848cae784&source=post_page-dc9848cae784----cf1837349302---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf1837349302&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf1837349302&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=su6amJTXnto", "anchor_text": "https://www.youtube.com/watch?v=su6amJTXnto"}, {"url": "https://youtu.be/KvbuJ0daXd4", "anchor_text": "https://youtu.be/KvbuJ0daXd4"}, {"url": "https://www.kaggle.com/saptarsi/tsne-with-python", "anchor_text": "https://www.kaggle.com/saptarsi/tsne-with-python"}, {"url": "https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1", "anchor_text": "https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1"}, {"url": "https://www.youtube.com/watch?v=RJVL80Gg3lA", "anchor_text": "https://www.youtube.com/watch?v=RJVL80Gg3lA"}, {"url": "https://medium.com/tag/tsne?source=post_page-----cf1837349302---------------tsne-----------------", "anchor_text": "Tsne"}, {"url": "https://medium.com/tag/pca?source=post_page-----cf1837349302---------------pca-----------------", "anchor_text": "Pca"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----cf1837349302---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----cf1837349302---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/tag/principal-component?source=post_page-----cf1837349302---------------principal_component-----------------", "anchor_text": "Principal Component"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf1837349302&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&user=Dr.+Saptarsi+Goswami&userId=dc9848cae784&source=-----cf1837349302---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf1837349302&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&user=Dr.+Saptarsi+Goswami&userId=dc9848cae784&source=-----cf1837349302---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf1837349302&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf1837349302--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcf1837349302&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cf1837349302---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf1837349302--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cf1837349302--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cf1837349302--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cf1837349302--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cf1837349302--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cf1837349302--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cf1837349302--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cf1837349302--------------------------------", "anchor_text": ""}, {"url": "https://saptarsi007.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://saptarsi007.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dr. Saptarsi Goswami"}, {"url": "https://saptarsi007.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "157 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc9848cae784&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&user=Dr.+Saptarsi+Goswami&userId=dc9848cae784&source=post_page-dc9848cae784--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcd431b031b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsne-simplified-cf1837349302&newsletterV3=dc9848cae784&newsletterV3Id=cd431b031b8c&user=Dr.+Saptarsi+Goswami&userId=dc9848cae784&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}