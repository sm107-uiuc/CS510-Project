{"url": "https://towardsdatascience.com/going-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c", "time": 1683017829.8479722, "path": "towardsdatascience.com/going-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c/", "webpage": {"metadata": {"title": "Going Global \u2014How to Multi-Task in Multiple Languages with the mT5 Transformer | by Thilina Rajapakse | Towards Data Science", "h1": "Going Global \u2014How to Multi-Task in Multiple Languages with the mT5 Transformer", "description": "The original T5 (Text-To-Text Transfer Transformer) model achieved state-of-the-art performance on a variety of NLP benchmarks by leveraging a unified text-to-text format and a gigantic training\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "T5", "paragraph_index": 0}, {"url": "https://www.tensorflow.org/datasets/catalog/c4", "anchor_text": "C4", "paragraph_index": 0}, {"url": "https://super.gluebenchmark.com/leaderboard", "anchor_text": "SuperGLUE", "paragraph_index": 0}, {"url": "https://github.com/google-research/multilingual-t5#languages-covered", "anchor_text": "101 different languages", "paragraph_index": 3}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 7}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5/mt5", "anchor_text": "link", "paragraph_index": 12}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 22}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 22}, {"url": "https://gist.github.com/ThilinaRajapakse/dabd80228a208d2cc54064be94c32068", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://gist.github.com/ThilinaRajapakse/dabd80228a208d2cc54064be94c32068", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://huggingface.co/google/mt5-small", "anchor_text": "google/mt5-small", "paragraph_index": 30}, {"url": "https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b", "anchor_text": "original T5 experiment", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c", "anchor_text": "article", "paragraph_index": 39}, {"url": "https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb", "anchor_text": "notebook", "paragraph_index": 39}, {"url": "https://researchportal.helsinki.fi/en/persons/j%C3%B6rg-tiedemann", "anchor_text": "J\u00f6rg Tiedemann", "paragraph_index": 53}, {"url": "https://marian-nmt.github.io/", "anchor_text": "Marian", "paragraph_index": 53}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 70}], "all_paragraphs": ["The original T5 (Text-To-Text Transfer Transformer) model achieved state-of-the-art performance on a variety of NLP benchmarks by leveraging a unified text-to-text format and a gigantic training dataset (C4). With the unified text-to-text approach, all downstream tasks were reframed such that both the input and the output of the model are text sequences. At a whopping 750 GB, the C4 (Colossal Clean Crawled Corpus) dataset was orders of magnitude larger than most existing datasets. Released back in October 2019 by Google, T5 still sits pretty at the top of the SuperGLUE benchmark as a testament to its capabilities.", "More information regarding the original T5 implementation and how to use it can be found in my article below.", "As impressive as T5 was (and still is), it was trained entirely on English text and therefore, can only be used for English-language tasks. Sadly, this leaves out some 80% of the world population who don\u2019t speak English.", "The mT5 model is a multilingual variant of the original T5 model, aimed at remedying this problem. mT5 closely follows the architecture and the training procedure of T5 but is trained on mC4 (~26 Terabytes!), a multilingual variant of the C4 dataset. It retains all the advantages of T5, but it also supports a total of 101 different languages!", "MT5 reframes any NLP task as a text-to-text task, which means that both the input and the output are text sequences. Just like with T5, the task to be performed can be specified by adding a prefix (a small text sequence) to the start of the sequence as shown below.", "However, unlike T5, the mT5 model can also be trained on datasets in languages other than English.", "Even more impressively, mT5 is also capable of cross-lingual, zero-shot transfer learning, i.e., the model is trained on an English dataset for a specific task, but it also learns to perform that task in other languages. This is quite useful when a certain dataset is only available in English, but you need it to work in another language (or multiple other languages)!", "In this article, we will focus on the zero-shot transfer learning ability of mT5. We\u2019ll also do a quick comparison of its performance against T5 on English data. We\u2019ll be doing all of this with the Simple Transformers library.", "In a previous article, I have trained a T5 model on the 3 tasks below:", "We\u2019ll be using the same tasks and datasets to train our mT5 model so that we can easily make the comparison between mT5 and T5.", "To test the zero-shot transfer learning, we\u2019ll focus on the first task. We\u2019ll translate each testing example of the Yelp dataset to five different languages and see how well the model performs on the translated data.", "We\u2019ll be using MarianMT models to translate the text, which are also available through Simple Transformers!", "Note: You can find all the code in this article in the examples/t5/mt5 directory (link) of the Simple Transformers Github repo.", "Since we are going to be working with 3 datasets, we\u2019ll put them in 3 separate subdirectories inside the data directory.", "The task to be performed by an mT5 model is specified by the prefix prepended to the input. Because of this, the input data format for an mT5 model (or a T5 model) in Simple Transformers is a Pandas dataframe with the 3 columns \u2014 prefix, input_text, and target_text.", "This makes it easy to combine all 3 of our datasets into a single dataframe, as we can simply assign a prefix value to each task and use that to differentiate between the 3 tasks.", "In this case, the three prefixes are as follows:", "The notebook above loads each of the datasets; preprocesses them for mT5 and finally combines them into a unified dataframe.", "This gives us a dataframe with 3 unique prefixes, namely binary classification, multilabel classification, and similarity. Note that the prefixes themselves are fairly arbitrary, the important thing is to ensure that each task has its own unique prefix. The input to the model will take the following format:", "The \": \" is automatically added when training.", "A few other things to note:", "Running the notebook should give you two files, train.tsv and eval.tsv, which we will use to train and test our model!", "We will be using the Simple Transformers library (based on the Hugging Face Transformers) to train the mT5 model.", "The instructions given below will install all the requirements.", "Training a model is quite easy with Simple Transformers. First, we start by importing the necessary libraries and setting up logging.", "Next, we load the datasets that we generated earlier.", "Note that we are casting all the data in the Dataframe as strings. This is because mT5 is a sequence-to-sequence model which expects all inputs and outputs to be text sequences. If we have numeric values (or any other non-string values), we\u2019ll run into errors during training.", "Next, we set up our pre-trained mT5 model.", "Here, we are configuring our mT5 model through model_args and instantiating a model with the pre-trained mt5-base weights.", "For more information about the different model_args and what they do, please refer to the Simple Transformers docs here and here.", "A model with this configuration can be trained on a GPU with 24 GB of VRAM. If your GPU has less VRAM than that and you run into CUDA memory issues, you can try using smaller train_batch_size and eval_batch_size values. You can also try using the google/mt5-small model which requires less memory (replace \u201cgoogle/mt5-base\u201d with \"google/mt5-small\" in line 18 above).", "Now, we just have to train our model!", "The evaluation line is optional as we\u2019ll be doing some real testing in the next section. You can comment it out to save some time.", "In our first test, we\u2019ll follow the same evaluation procedure as in my original T5 experiment.", "Specifically, we\u2019ll use the following metrics.", "First, we will define the functions to calculate the metrics given above.", "Next, we read in the evaluation dataset and process it for testing.", "Then, we load our trained mT5 model and generate predictions for each input sequence.", "As a sequence-to-sequence model, the decoding algorithm used and the hyperparameters (num_beams, do_sample, top_k, top_p) used to control it has a substantial impact on the quality of the predictions.", "If you\u2019d like to learn more about the decoding process, please refer to the decoding algorithms section in this article and this excellent notebook by Huggingface.", "Now that we have our predictions (in df[\"predicted\"]), it\u2019s time to calculate the metrics.", "Running this snippet of code yields the results shown below.", "Note that there is a degree of randomness when using sampling decoding algorithms, so the scores will change when the test is repeated. However, the scores should be relatively close to these values.", "Before we dig into these scores, let\u2019s put them side-by-side (and round them off for clarity) with the T5 scores for comparison.", "The mT5 model does a pretty good job at the binary classification task, although it does fall short of the scores set by the T5 model.", "Similar story here with the multilabel classification task with the mT5 models doing well but not as well as the T5 model.", "Typically, a monolingual model will outperform a comparable multilingual model, so, these results are as expected.", "Unlike the two previous tasks, the mT5 model fails miserably at sentence similarity.", "While it\u2019s difficult to pinpoint the exact cause without more experimentation, the two reasons given below are likely to be largely responsible!", "There might be other reasons (e.g. similarity being a sentence-pair task) and it\u2019s likely that there are ways to get it to work for this task as well. Let me know if you find anything!", "However, mT5 was never designed to beat T5 in English language tasks! So, let\u2019s move on to the true test, cross-lingual zero-shot transfer.", "In this test, we want to see if an mT5 model trained on an English task can perform the same task in other languages, without further training.", "For this test, we\u2019ll focus on the first task, i.e. binary classification. We won\u2019t be testing on the other tasks as the second task involves a lot of toxic language (I don\u2019t expect the translation models to perform well here). The third task is, unfortunately, disqualified by default as it didn\u2019t do well even in English.", "Before we can test the cross-lingual capabilities of mT5, we\u2019ll first need to translate our evaluation data. For this, we\u2019ll use the pre-trained MarianMT models originally trained by J\u00f6rg Tiedemann using the Marian C++ library.", "First, we\u2019ll set up our translation models in translation_models.py", "Here, we are setting up functions to load four different translation models (english_to_romance_model will handle both French and Spanish).", "Next, we\u2019ll do use these models to translate our evaluation dataset. Note that, the translations may take some time depending on available computational resources. If you want to speed things up, you can test with one language only.", "To keep the code clean, we\u2019ll be doing this step in another file, translate_dataset.py.", "You can drop the languages you don\u2019t want from the languages list in line 51. You can also comment out the unused translation models defined in model_map to prevent them from being downloaded and loaded into memory.", "Running the script above will generate the translated datasets and save them to the data directory.", "The code to calculate the metrics can be easily adapted from our original evaluation code. The only differences are that we need to loop over the eval.csv files for each language and we\u2019ll only be evaluating the binary classification task.", "Very impressive, considering the mT5 model was only fine-tuned on an English dataset!", "Based on these results, it seems like mT5 favours more common languages, although that is to be expected. However, it\u2019s also possible that this difference is due to translation models for more common languages being relatively superior.", "Unfortunately, it doesn\u2019t perform well for all the supported languages. For example, I tried a few examples in my native language, Sinhalese, with fairly poor (although not completely useless) results.", "If you want to try some quick predictions yourself, use the command simple-viewerin the terminal (in the directory which contains the outputs directory).", "Overall, I\u2019m quite impressed with the performance of the mT5 model, particularly in the context of cross-lingual, zero-shot transfer learning.", "However, mT5 does seem to struggle with smaller datasets when compared to T5. Oversampling the similarity dataset to bring it in line with the others (or even training mT5 on this task alone) didn\u2019t solve the issue, so it\u2019s possible that the small size of the dataset isn\u2019t the sole factor here. This calls for more investigation!", "The ability to train a model on English data and have it automatically transfer that knowledge to a bunch of other languages can be incredibly useful, especially when you consider that most training datasets are only available in English.", "With such versatility, I\u2019m sure that people will come up with a ton of creative applications!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F892617cd890c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----892617cd890c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----892617cd890c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----892617cd890c--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----892617cd890c--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----892617cd890c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F892617cd890c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F892617cd890c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@d0cz?utm_source=medium&utm_medium=referral", "anchor_text": "Bruno Wolff"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "T5"}, {"url": "https://www.tensorflow.org/datasets/catalog/c4", "anchor_text": "C4"}, {"url": "https://super.gluebenchmark.com/leaderboard", "anchor_text": "SuperGLUE"}, {"url": "https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c", "anchor_text": "Asking the Right Questions: Training a T5 Transformer Model on a New taskThe T5 Transformer frames any NLP task as a text-to-text task enabling it to easily learn new tasks. Let\u2019s teach the\u2026towardsdatascience.com"}, {"url": "https://github.com/google-research/multilingual-t5#languages-covered", "anchor_text": "101 different languages"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz", "anchor_text": "Yelp Reviews Dataset"}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/", "anchor_text": "Toxic Comments dataset"}, {"url": "http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark", "anchor_text": "STS-B dataset"}, {"url": "https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b", "anchor_text": "The Guide to Multi-Tasking with the T5 TransformerThe T5 Transformer can perform any NLP task. And, it can perform multiple tasks, at the same time, with the same model\u2026towardsdatascience.com"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5/mt5", "anchor_text": "link"}, {"url": "https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz", "anchor_text": "Yelp Reviews Dataset"}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/", "anchor_text": "Toxic Comments dataset"}, {"url": "http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark", "anchor_text": "STS-B dataset"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/installation/#installation-steps", "anchor_text": "docs"}, {"url": "https://gist.github.com/ThilinaRajapakse/dabd80228a208d2cc54064be94c32068", "anchor_text": "here"}, {"url": "https://gist.github.com/ThilinaRajapakse/dabd80228a208d2cc54064be94c32068", "anchor_text": "here"}, {"url": "https://huggingface.co/google/mt5-small", "anchor_text": "google/mt5-small"}, {"url": "https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b", "anchor_text": "original T5 experiment"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html", "anchor_text": "F1 score"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html", "anchor_text": "Accuracy score"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html", "anchor_text": "Pearson correlation coefficient"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html", "anchor_text": "Spearman correlation"}, {"url": "https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c", "anchor_text": "article"}, {"url": "https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb", "anchor_text": "notebook"}, {"url": "https://researchportal.helsinki.fi/en/persons/j%C3%B6rg-tiedemann", "anchor_text": "J\u00f6rg Tiedemann"}, {"url": "https://marian-nmt.github.io/", "anchor_text": "Marian"}, {"url": "https://arxiv.org/pdf/2010.11934.pdf", "anchor_text": "https://arxiv.org/pdf/2010.11934.pdf"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "https://arxiv.org/abs/1910.10683"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "https://huggingface.co/transformers/"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----892617cd890c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----892617cd890c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----892617cd890c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----892617cd890c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F892617cd890c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----892617cd890c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F892617cd890c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----892617cd890c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F892617cd890c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----892617cd890c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F892617cd890c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----892617cd890c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----892617cd890c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----892617cd890c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----892617cd890c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----892617cd890c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----892617cd890c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----892617cd890c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----892617cd890c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----892617cd890c--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-global-how-to-multi-task-in-multiple-languages-with-the-mt5-transformer-892617cd890c&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}