{"url": "https://towardsdatascience.com/transformer-in-cv-bbdb58bf335e", "time": 1683017405.754407, "path": "towardsdatascience.com/transformer-in-cv-bbdb58bf335e/", "webpage": {"metadata": {"title": "Transformer in CV. The increasing convergence of computer\u2026 | by Cheng He | Towards Data Science", "h1": "Transformer in CV", "description": "Transformer architecture has achieved state-of-the-art results in many NLP (Natural Language Processing) tasks. One of the main breakthroughs with the Transformer model could be the powerful GPT-3\u2026"}, "outgoing_paragraph_urls": [{"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/gpt3-the-dream-machine-in-real-world-c99592d4842f?sk=f7cf265a9b5b3a36ca529fe5f691e8ec", "anchor_text": "GPT-3", "paragraph_index": 0}, {"url": "https://twitter.com/OpenAI/status/1336000843368210432", "anchor_text": "Best Paper at NeurIPS2020", "paragraph_index": 0}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformers", "paragraph_index": 2}, {"url": "https://github.com/google-research/vision_transformer", "anchor_text": "Vision Transformer", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR", "paragraph_index": 5}, {"url": "https://github.com/google-research/vision_transformer", "anchor_text": "ViT", "paragraph_index": 6}, {"url": "https://github.com/facebookresearch/detr", "anchor_text": "DETR", "paragraph_index": 6}, {"url": "https://openai.com/blog/image-gpt/#introduction", "anchor_text": "Image GPT", "paragraph_index": 6}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "illustrated transformer", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "attention is all you need", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "Vision Transformer", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "ViT", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "ViT", "paragraph_index": 19}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "Detection Transformer", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR", "paragraph_index": 26}, {"url": "https://openai.com/blog/image-gpt/", "anchor_text": "Image GPT", "paragraph_index": 29}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2", "paragraph_index": 29}], "all_paragraphs": ["Transformer architecture has achieved state-of-the-art results in many NLP (Natural Language Processing) tasks. One of the main breakthroughs with the Transformer model could be the powerful GPT-3 released in the middle of the year, which has been awarded Best Paper at NeurIPS2020.", "In Computer Vision, CNNs have become the dominant models for vision tasks since 2012. There is an increasing convergence of computer vision and NLP with much more efficient class of architectures.", "Using Transformers for vision tasks became a new research direction for the sake of reducing architecture complexity, and exploring scalability and training efficiency.", "The following are a couple of well known projects in the related work:", "Overall, there are 2 major model architectures in the related work of adopting transformer in CV. One is pure transformer architecture, the other is the hybrid architecture which combines the CNNs/backbone and the Transformer.", "Vision Transformer is the full self attention based Transformer architecture without CNNs and can be used out of the box, while DETR is an example of using the hybrid model architecture, which combines the convolutional neural network (CNNs) with Transformer.", "You will find the experiments and answers in the following deep dive of ViT (vision transformer), DETR (detection transformer), and Image GPT.", "If you are new to Transformer architecture, recommend you to check out this illustrated transformer and attention is all you need paper.", "Vision Transformer (ViT) can achieve excellent results with pure transformer architecture applied directly to a sequence of image patches for classification tasks.", "It also outperforms the state-of-the-art convolutional networks on many image classification tasks while requiring substantially fewer computational resources (at least 4 times fewer than SOTA CNN) to pre-train.", "How they feed an image into the Transformer is by splitting it into fixed-size patches, and feed the linear projections of these patches along with their image position into Transformer. Then the rest of the pipeline is a clean and standard encoder and decoder blocks of the transformer.", "Position embeddings are added to the image patch embeddings to retain spatial/positional information in a global scope with different strategies. In the paper, they tried different ways to encode the spatial information, including no positional information, 1D/2D positional embeddings, and relative positional embeddings.", "One of the interesting findings is 2D positional embeddings did not bring significant performance gains when compared with 1D positional embeddings.", "The model is pre-trained from multiple large scale datasets with deduplication to support fine tuning (smaller dataset) downstream tasks.", "Like other popular Transformer models (GPT, BERT, RoBERTa), the ViT (vision transformer) comes with different model sizes (Base, large, and huge) and different number of transformer layers and heads. For example, ViT-L/16, can be interpreted as a large (24 layers) ViT model with 16 x 16 input image patch size.", "Note, the smaller the input patch size yields larger computational model, simply because the input number of patches N = HW/P*P, where (H,W) is the resolution of the original image and P is the resolution of the patch image. Which means patch size 14 x 14 is more computationally expensive than the image patch of 16 x 16.", "The above results show the large Vision Transformer model beats previous SOTA on multiple popular benchmark datasets.", "The vision transformer (ViT-H/14, ViT-L/16) pre-trained on JFT-300M dataset outperforms the ResNet model (ResNet152x4, which is pre-trained on same JFT-300M dataset) on all testing dataset while taking substantially less computational resources (TPUv3 core days) during pre-training. Even the ViT pre-trained on ImageNet-21K outperform the baseline.", "The above graph shows the impact of dataset size on model performance. ViT did not perform well when the size of pre-training dataset is small, it outperforms previous SOTA with sufficient training data.", "Like mentioned in the beginning, there are different architecture designs of using transformer for computer vision, some totally replace CNNs with transformer (ViT), some partially replace, and some combine both CNNs and transformer (DETR). The following results show the performance of each model architecture under the same computational budget.", "Detection Transformer (DETR) is the first object detection framework that successfully used Transformer as the main building blocks in the pipeline.", "It matches the performance of the previous STOA methods(highly optimized Faster R-CNN) with a much simpler and flexible pipeline.", "The above shows DETR, a hybrid pipeline that uses CNN and Transformer as the main building blocks in the pipeline. Here is the flow:", "With the traditional object detection methods, like Faster R-CNN, there are multiple steps to do anchor generation and removing duplicates with non-maximum suppression (NMS) procedures. DETR dropped these hand-designed components to significantly streamline the object detection pipeline.", "In the paper, they further extended the DETR pipeline for panoptic segmentation task, a recently popular and challenging pixel level recognition task.", "To simply explain the panoptic segmentation task, it unifies 2 distinct tasks, one is traditional semantic segmentation (assign class label for each pixel), the other is instance segmentation (detect and segment each object instance). What a smart idea to use one model architecture to solve 2 tasks (classification and segmentation).", "The above graph shows an example of the panoptic segmentation. With the unified pipeline of DETR, it outperformed the competitive baselines.", "The following graph visualizes the Transformer\u2019s decoder attention for the predicted objects. Attention scores are represented with different colors for different objects.", "By looking at the colors/attention, you will be amazed by the model\u2019s capability of resolving those overlapping bounding box with global understanding of the image through self attention. Especially with the example of the orange color zebra legs, how they have been interpreted/classified even though they are heavily overlapped with the blue and green ones locally.", "Image GPT is a GPT-2 transformer based model that has been trained on pixel sequence to generate image completion and samples. Like a general pre-trained language model, it is designed to learn high-quality unsupervised image representations. It can predict the next pixel auto-regressively without any knowledge of the 2D structure of the input image.", "Features from the pre-trained image GPT achieved state-of-the-art performance on a number of classification benchmark and near state-of-the-art unsupervised accuracy on ImageNet.", "The following image shows the model generated completion with human provided half image as input, followed by the creative completions from the model.", "Transformer\u2019s great success in NLP has been explored in the computer vision domain and became a new research direction.", "This is just an early glimpse, looking forward to seeing the new emerging things with the increasing convergence of the NLP and CV.", "If you are new to Transformer or would like to dive deep to understand the bottleneck and evolution of it, feel free to checkout the following blog, which captures the details of the model and a summary of difference approaches to further improve it.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI evangelist, engineer, entrepreneur, YC alum"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbbdb58bf335e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chengh.medium.com/?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": ""}, {"url": "https://chengh.medium.com/?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "Cheng He"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7412a26d36ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&user=Cheng+He&userId=7412a26d36ce&source=post_page-7412a26d36ce----bbdb58bf335e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbbdb58bf335e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbbdb58bf335e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR"}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer"}, {"url": "https://towardsdatascience.com/gpt3-the-dream-machine-in-real-world-c99592d4842f?sk=f7cf265a9b5b3a36ca529fe5f691e8ec", "anchor_text": "GPT-3"}, {"url": "https://twitter.com/OpenAI/status/1336000843368210432", "anchor_text": "Best Paper at NeurIPS2020"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformers"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "End-to-End Object Detection with Transformers"}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"}, {"url": "https://openai.com/blog/image-gpt/", "anchor_text": "Image GPT"}, {"url": "https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf", "anchor_text": "Generative Pretraining from Pixels"}, {"url": "https://arxiv.org/pdf/2011.04233.pdf", "anchor_text": "End-to-end Lane Shape Prediction with Transformers"}, {"url": "https://github.com/google-research/vision_transformer", "anchor_text": "Vision Transformer"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR"}, {"url": "https://github.com/google-research/vision_transformer", "anchor_text": "ViT"}, {"url": "https://github.com/facebookresearch/detr", "anchor_text": "DETR"}, {"url": "https://openai.com/blog/image-gpt/#introduction", "anchor_text": "Image GPT"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "illustrated transformer"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "attention is all you need"}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "Vision Transformer"}, {"url": "https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html", "anchor_text": "Goole AI blog"}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "ViT"}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "ViT"}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "ViT"}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "ViT"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "Detection Transformer"}, {"url": "https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/", "anchor_text": "Facebook AI blog"}, {"url": "https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/", "anchor_text": "Facebook AI blog"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR"}, {"url": "https://arxiv.org/pdf/2005.12872.pdf", "anchor_text": "DETR"}, {"url": "https://openai.com/blog/image-gpt/", "anchor_text": "Image GPT"}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2"}, {"url": "https://openai.com/blog/image-gpt/#rf1", "anchor_text": "OpenAI"}, {"url": "https://arxiv.org/pdf/2010.11929.pdf", "anchor_text": "ViT"}, {"url": "https://chengh.medium.com/evolution-of-fast-and-efficient-transformers-ec0378257994", "anchor_text": "Evolution of Fast and Efficient TransformersAn overview of Transformer optimization and accelerationchengh.medium.com"}, {"url": "https://github.com/google-research/vision_transformer", "anchor_text": "code and models"}, {"url": "https://github.com/facebookresearch/detr", "anchor_text": "code and models"}, {"url": "https://arxiv.org/pdf/1802.05751.pdf", "anchor_text": "Image Transformer"}, {"url": "https://openai.com/blog/image-gpt/", "anchor_text": "Image GPT"}, {"url": "https://towardsdatascience.com/gpt3-the-dream-machine-in-real-world-c99592d4842f?sk=f7cf265a9b5b3a36ca529fe5f691e8ec", "anchor_text": "GPT-3"}, {"url": "https://openai.com/blog/openai-api/", "anchor_text": "OpenAI API"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Illustrated Transformer"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Attention is all you need"}, {"url": "http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "Transformer-XL: Unleashing the Potential of Attention Models"}, {"url": "http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html", "anchor_text": "Rethinking Attention with Performers"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer: The Long-Document Transformer"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer: The Efficient Transformer"}, {"url": "https://medium.com/tag/transformers?source=post_page-----bbdb58bf335e---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----bbdb58bf335e---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/ai?source=post_page-----bbdb58bf335e---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machie-learning?source=post_page-----bbdb58bf335e---------------machie_learning-----------------", "anchor_text": "Machie Learning"}, {"url": "https://medium.com/tag/startup?source=post_page-----bbdb58bf335e---------------startup-----------------", "anchor_text": "Startup"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbbdb58bf335e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&user=Cheng+He&userId=7412a26d36ce&source=-----bbdb58bf335e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbbdb58bf335e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&user=Cheng+He&userId=7412a26d36ce&source=-----bbdb58bf335e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbbdb58bf335e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbbdb58bf335e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bbdb58bf335e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bbdb58bf335e--------------------------------", "anchor_text": ""}, {"url": "https://chengh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chengh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Cheng He"}, {"url": "https://chengh.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "210 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7412a26d36ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&user=Cheng+He&userId=7412a26d36ce&source=post_page-7412a26d36ce--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffdd02d6a867b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-in-cv-bbdb58bf335e&newsletterV3=7412a26d36ce&newsletterV3Id=fdd02d6a867b&user=Cheng+He&userId=7412a26d36ce&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}