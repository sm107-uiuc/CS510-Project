{"url": "https://towardsdatascience.com/intuition-exploration-vs-exploitation-c645a1d37c7a", "time": 1683001145.030676, "path": "towardsdatascience.com/intuition-exploration-vs-exploitation-c645a1d37c7a/", "webpage": {"metadata": {"title": "Intuition: Exploration vs Exploitation | by Eugen Lindwurm | Towards Data Science", "h1": "Intuition: Exploration vs Exploitation", "description": "The exploration-exploitation trade-off is a well-known problem that occurs in scenarios where a learning system has to repeatedly make a choice with uncertain pay-offs. In essence, the dilemma for a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.sciencedirect.com/science/article/pii/S0893608002000564", "anchor_text": "reinforcement learning", "paragraph_index": 1}, {"url": "https://www.datacouncil.ai/talks/a-multi-armed-bandit-framework-for-recommendations-at-netflix", "anchor_text": "recommendation systems", "paragraph_index": 1}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.8373&rep=rep1&type=pdf", "anchor_text": "online advertising", "paragraph_index": 1}, {"url": "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf", "anchor_text": "multi-armed bandit problem", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)", "anchor_text": "uniform distribution", "paragraph_index": 10}, {"url": "http://jmlr.org/papers/volume3/auer02a/auer02a.pdf", "anchor_text": "upper confidence bound", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/1707.02038.pdf", "anchor_text": "Thompson sampling", "paragraph_index": 14}, {"url": "https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf", "anchor_text": "introduced in 1933", "paragraph_index": 14}, {"url": "https://www.semanticscholar.org/paper/Learning-to-Optimize-via-Posterior-Sampling-Russo-Roy/28cf1bd6110e734e20fc63f727d0d5bba612b921", "anchor_text": "close connections", "paragraph_index": 17}], "all_paragraphs": ["The exploration-exploitation trade-off is a well-known problem that occurs in scenarios where a learning system has to repeatedly make a choice with uncertain pay-offs. In essence, the dilemma for a decision-making system that only has incomplete knowledge of the world is whether to repeat decisions that have worked well so far (exploit) or to make novel decisions, hoping to gain even greater rewards (explore).", "This is highly relevant in reinforcement learning, but also for many other applications, such as recommendation systems and online advertising.", "In this article, I give an overview of three simple and proven strategies to tackle the exploration-exploitation trade-off for multi-armed bandits.", "The term one-armed bandit comes from slot machines that have one \u201carm\u201d to pull, and who would effectively \u201crob\u201d the gambler of their money.", "We call problems \u201cmulti-armed bandit problem\u201d if they consist of repeatedly choosing among multiple discrete options (arms) that each yield a probabilistic reward (money). In this setting, just like with slot machines, the reward for each choice is independent of what options have been taken earlier. Over time, the decision-maker builds an understanding of the reward distribution of each option. The goal is to maximize the expected reward, which requires finding out which option gives the highest mean reward and exploiting this option as much as possible.", "For example, imagine you\u2019re suffering from afternoon-fatigue and you want to figure out how to still be most productive. Every day, you can try one of several strategies: getting a coffee, taking a nap, trying to just power through, et cetera. For each of the strategies, the pay-off will seem probabilistic, as many other factors are influencing your productivity as well. You start choosing one option each day, sometimes trying something new, and sometimes repeating what has worked well in the past. As you observe how productive you are each day, you get a better understanding of the effectiveness of your strategies, enabling you to eventually consistently chose the most reliable one.", "As you can imagine, there are many ways to go about multi-armed bandit problems. I\u2019ll try to give you an intuitive overview over a couple of simple but effective (i.e. they\u2019re actually being used) approaches.", "This one is as simple as it gets, and if you have taken any reinforcement learning 101 course, I guarantee you that you have heard of it. The idea behind the purely greedy approach is to try every decision once and keep picking the one that resulted in the highest reward.", "To add some exploration into the mix, \u03f5-greedy lets you decide what fraction of your decisions you want to spend exploring (\u03f5) and what fraction you want to spend exploiting (1-\u03f5) the best option so far. If you set \u03f5 to 0.4, then you will take the option that gave you the best average reward in the past 60% of the time and chose any other option 40% of the time. Typically, you want \u03f5 to be small so that you mostly exploit your experience, but also go explore from time to time. Sometimes though, it\u2019s a good idea to start with a larger \u03f5 to encourage initial exploration and reduce it as you gather knowledge about the rewards.", "For many applications, \u03f5-greedy works remarkably well. In other scenarios, it loses some efficiency by indiscriminately exploring. This means that when you\u2019re exploring, the probability of choosing either of the seemingly suboptimal options is equal. Even if there are options that are consistently worse than others, they still get picked sometimes, wasting precious resources.", "Now we\u2019re treading Bayesian waters. Imagine that for each option, we maintain a distribution of the rewards that we have collected when picking this option. If we haven\u2019t collected any rewards yet, we might start with an assumption: a prior reward distribution for each option that would typically be quite broad (e.g. the uniform distribution). As we collect rewards, we constantly update this distribution.", "At every step, we don\u2019t just pick the option with the highest estimated mean reward \u03bc, as we would with the greedy approach, but we instead go for the option with the highest \u03bc plus one standard deviation \u03c3. The effect of this is that in the beginning, when we\u2019re uncertain about the rewards for any option and variances are high, we tend to explore the options with high uncertainty more as \u03c3 dominates. As we become more certain about the mean rewards of each option, the standard deviations shrink and we tend to favor the options with the highest estimated mean reward \u03bc and disregard the options offering little reward.", "We call \u03bc + \u03b2\u03c3 the upper confidence bound, where beta is a trade-off parameter, used to steer towards more or less exploration. As \u03b2 tends to zero, we get closer to pure exploitation of the option that provided the highest mean reward thus far. High \u03b2 values, on the other hand, favor exploring until we have little uncertainty left.", "Note that unlike with the \u03f5-greedy approach, our experience influences which options we\u2019ll explore. If we find out early on that one of the options is almost certainly worse than the other choices, then we will not invest in exploring it further.", "Remember that we are constantly re-estimating our reward distributions. When following the Thompson sampling scheme, introduced in 1933, we make use of the uncertainty provided by those distributions. At every time-step, we draw one sample from each distribution. We then rank the options based on the reward-value of these samples, just as we ranked the distributions based on mean plus \u03b2 times the standard deviation before. Finally, we pick the highest-ranked option.", "This way, options with a high estimated mean reward are likely to be picked. Still, options that we are uncertain about and that thus have a wide estimated reward distribution have a chance to get picked as well because they spread their samples over a large area of possible rewards values.", "With this strategy, we typically never give up on any option entirely, unlike we would do with the UCB. Instead, it just becomes increasingly unlikely to pick a seemingly bad option.", "Intuitively, we can say that both UCB and Thompson sampling are \u201coptimistic\u201d because they give options a chance even if those options haven\u2019t delivered the best mean reward so far. On a theoretical level, it can be shown that there are close connections between UCB and Tompson sampling.", "Thank you for reading. I hope you enjoyed this article and also learned something useful!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student in Machine Learning. Interested in social and environmental issues."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc645a1d37c7a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00----c645a1d37c7a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc645a1d37c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc645a1d37c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/fqkrXYMosT4", "anchor_text": "Berries"}, {"url": "https://unsplash.com/@gndclouds", "anchor_text": "William Felker"}, {"url": "https://unsplash.com/license", "anchor_text": "Unsplash"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0893608002000564", "anchor_text": "reinforcement learning"}, {"url": "https://www.datacouncil.ai/talks/a-multi-armed-bandit-framework-for-recommendations-at-netflix", "anchor_text": "recommendation systems"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.8373&rep=rep1&type=pdf", "anchor_text": "online advertising"}, {"url": "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf", "anchor_text": "multi-armed bandit problem"}, {"url": "https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)", "anchor_text": "uniform distribution"}, {"url": "http://jmlr.org/papers/volume3/auer02a/auer02a.pdf", "anchor_text": "upper confidence bound"}, {"url": "https://arxiv.org/pdf/1707.02038.pdf", "anchor_text": "Thompson sampling"}, {"url": "https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf", "anchor_text": "introduced in 1933"}, {"url": "https://www.semanticscholar.org/paper/Learning-to-Optimize-via-Posterior-Sampling-Russo-Roy/28cf1bd6110e734e20fc63f727d0d5bba612b921", "anchor_text": "close connections"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c645a1d37c7a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----c645a1d37c7a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c645a1d37c7a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c645a1d37c7a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c645a1d37c7a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc645a1d37c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----c645a1d37c7a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc645a1d37c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=-----c645a1d37c7a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc645a1d37c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc645a1d37c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c645a1d37c7a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c645a1d37c7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pflaenzchen?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eugen Lindwurm"}, {"url": "https://medium.com/@pflaenzchen/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "135 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9e52bc6a00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=post_page-cb9e52bc6a00--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56b4244ba55f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuition-exploration-vs-exploitation-c645a1d37c7a&newsletterV3=cb9e52bc6a00&newsletterV3Id=56b4244ba55f&user=Eugen+Lindwurm&userId=cb9e52bc6a00&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}