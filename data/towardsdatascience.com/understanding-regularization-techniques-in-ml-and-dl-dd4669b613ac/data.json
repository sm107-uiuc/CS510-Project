{"url": "https://towardsdatascience.com/understanding-regularization-techniques-in-ml-and-dl-dd4669b613ac", "time": 1683006874.5168, "path": "towardsdatascience.com/understanding-regularization-techniques-in-ml-and-dl-dd4669b613ac/", "webpage": {"metadata": {"title": "Regularisation in ML | Towards Data Science", "h1": "Understanding Regularization Techniques in ML and DL", "description": "Now, as industries start to accept \u201cArtificial Intelligence\u201d as an important part of predicting their company\u2019s success, the techniques of Machine Learning and Deep Learning are making their way into\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Now, as industries start to accept \u201cArtificial Intelligence\u201d as an important part of predicting their company\u2019s success, the techniques of Machine Learning and Deep Learning are making their way into the job profile list of companies. But it is often seen that the actual decision-makers in a company (the people calling the shots: CxOs) have a very misguided notion of what these techniques can do and how their company can be benefitted. ML is often seen as a technology that has the potential to solve any and all industrial problems as per the people who don\u2019t fully comprehend ML\u2019s truth. The following picture makes the current state of ML quite clear.", "This is not satire but a quite accurate understanding of ML. It is the un-hardcoded ability of computers to predict events while a set of precursor events is provided. I will try to keep this blog as non-math as possible but here I would like to include the fact that ML, in its essence, is the act of predicting the functional relation F between x and y given multiple such equations.", "But many a time, it is seen that even after training a model and achieving an acceptable training accuracy, when the model is employed to work on the test cases, it fails miserably.", "This happens due to the phenomenon of overfitting or making the function over-approximate the training data. This leads the model, instead of understanding the generic idea of how to solve the problem, to rote the training data. The following picture makes it clear.", "The real function is a sinusoid (green) and we are trying to predict it from the data given. Till the third figure, we see the model learning very well. It is an almost perfect approximation of the function even though all data points are not satisfied. But as training continues, we see the function molding itself to fit all data points and taking a form quite different from what is desired. This is overfitting. Where the training loss is taken to zero but the test loss rises", "Understanding Bias-Variance Tradeoff and the need for Regularization:", "Bias is mathematically, the difference between the expected value and the actual value of the of the function. We won\u2019t be going into the underlying statistics of bias but I will responsibly, leave you with a scary looking equation:", "To make things clear, the bias of a simple, linear model is high and that of a complex, multidimensional model, is low. This is because a complex model is better at fitting all the training data.", "Variance is the change in prediction accuracy of an ML model between training data and test data. Error due to variance is the amount by which the prediction, over one training set, differs from the expected value over all the training sets. In other words, how far are the values of the different predictions from each other as per the model. Another equation follows to scare you, folks, off.", "A simple model has a low variance whereas a complex one has a high variance.", "The following graph can be used to establish the concepts of Bias and Variance clearly. The left end of the graph is the zone with high bias as both the training and the testing error are high. This is the zone of underfitting or the zone where the model has not learned enough.", "The right end of the model is an area of high variance where the training error is low and but the testing error is high. This is the zone of overfitting, where we see that even though the model has achieved a high training accuracy, and it seems like the model is near perfect, it performs poorly on test data. This is a sheer waste of computational power and the engineer\u2019s time.", "The middle zone, where both the bias and variance are low, even though not the lowest possible is the best possible zone for a model. The act of achieving this state of model training is known as Bias-Variance Tradeoff.", "There are various methods using which we can achieve Bias Variance Tradeoff. These methods or techniques are known as Regularization Techniques.", "Keeping things as simple as possible, I would define L2 Regularization as \u201ca trick to not let the model drive the training error to zero\u201d. If only things were that simple\u2026", "During training a model, we have continuous updating of the various variables (weights and biases; w & b) which try to predict our original function. This update takes place based on an \u201cupdate rule\u201d like Gradient Descent (which we won\u2019t talk about). This update rule depends on the \u201closs function\u201d which is a function of these variables. If things are getting complicated, bear with me. Our aim is to minimize this \u201closs function\u201d. And that\u2019s quite intuitive isn\u2019t it? In any profitable industrial situation, you strive to minimize the loss. Simple, init?", "So, we minimize the loss function during training. What is special about the L2 technique is that instead of minimizing the training loss, we minimize a different version of it.", "The first term in the equation above us is the \u2018loss term\u2019 or the term that measures how well the model fits the data. The last term is the \u201clog of the likelihood of the Gaussian distribution of the weights\u201d by the math folks. This measures the model complexity. For us, laymen, it is the sum of squares of all the feature weights (w). Here again, responsibly, I leave you with:", "This model complexity is quantified by the L2 technique. In this, the feature weights close to zero aren\u2019t affected as much by the transformation, but the outliers have a huge impact. And the more the value of the above term, the more we see the increase in bias and decrease in variance. From the above graph, it is evident that at the end of the training, the bias is very low and the variance is high. So, if we were to increase in the bias and decrease the variance, we would effectively be reaching somewhere in the \u2018good model\u2019 zone of the graph. So now, we have a good model! Yay!", "This is, by far, the simplest regularization technique (well all of them are, but you wouldn\u2019t believe me, would you). This process involves recording the values of the variables (w & b) at the minimum loss value. While going through the training process, we record the values of w & b, at which we obtain the least validation error. We stop training when we see the validation error rising again. This is a very useful procedure but the downside to it is, during training very deep neural networks or very complex models, this utilizes a lot of processing power during the writing and rewriting the minimum values.", "Training a model to a good prediction state is only possible when we have a lot of data to train it on. In other words, it is quite easy to drive the training error to zero if the data is too less. Let\u2019s take the example of training a neural network on image classification. Say we have 1000 images to train the model on. Wouldn\u2019t it be better if we had say, 3000 images to train it on? Without procuring extra data, we can easily \u201caugment\u201d the current images and create \u201cnew\u201d ones. These are not in fact, new to us, but to the model, it is as new as they come.", "So what is augmentation? It is the act of artificially generating new data from the already available data by introducing certain variances in it while retaining the original labels. These variances depend on the type of data we are dealing with. For audio, speeding up the sample or introducing some background noise is an augmentation technique. This doesn\u2019t change the label value. For text, we can replace a word by its synonyms and not change its conveying message. For images, we can change the viewing angle, the zoom, the lighting, and other techniques that change the image but retain its label. Here\u2019s some cuteness to cancel out your boredom from reading this blog and make image augmentation clear.", "So, when we now have more data to feed our model, which makes it more difficult for it to memorize the entire thing and therefore, the training error isn\u2019t driven to zero. Kinda like your history test, init?", "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance, bias, or improve predictions.", "The above paragraph is Google\u2019s definition of Ensemble Methods, and I\u2019ll try to break it down for you. In this technique, we employ multiple model architectures to predict an output, be it classification or regression. Let\u2019s say models A, B and C are given the task of classifying a dog: Model A says it\u2019s a cat, but B and C say it\u2019s a dog. So if we are to believe what the majority says, we arrive at the correct output, but if we were to trust the output of the first model, we would have erred. Similarly with regression, or value prediction. We take the weighted average of the predictions given the 3 models to arrive at our final output. This decreases the chance of an error and improves accuracy.", "The interesting part about this is we needn\u2019t spend resources on 3 models as well. We could train on the same model 3 times with different batches of the data. That would serve the purpose as well. But you get the idea, don\u2019t you?", "Dropout is also classified in the category of an ensemble method. But I, for fun, think it to be the reverse of that. In ensemble methods you \u2018ask\u2019 the opinion of other models to arrive at a conclusion but here, it is basically silencing other contributors. Let me make it clear.", "This is a very simple neural network whose purpose is to be a True/False classifier. Look at the output layer (green). It has 2 blobs, one that gives the probability that the output is True and the other False. The sum of the two values: you guessed it: 1! Aren\u2019t you smart? XD.", "The idea here is to make you understand that these \u2018blobs\u2019 are called nodes. Each of these nodes has a ton of complex calculations happening inside them. Remember the stuff I was talking about in L2 Regularization? It all happens here. So these nodes are the actual contributors to the output.", "Dropout involves turning off certain nodes randomly. This changes the architecture of the model and the way information flows through the nodes. Doing this makes the model a more robust predictor. The model has to predict the same outputs with some of its contributors turned off. That\u2019s like saying you need to get through your quiz without your topper friends being around. You gotta learn. Get it? XD.", "So that sums up my blog on regularization techniques. I intentionally did not provide you with information on Batch Normalization as that would have required me to give you the entire process of training a neural network and that would have gone against the main idea behind this blog: keeping things simple.", "If you are itching to know how to code these on Python using PyTorch, refer to the following repository on GitHub. The batchnorm_dropout.ipynb file will be of interest. I will be uploading TensorFlow files on another repo as well, to have the code on both these frameworks.", "I\u2019ve had an amazing time writing this out for you folks and I hope you could take away something from this. If you liked it, leave a clap. If you didn\u2019t you probably would\u2019ve left the page long back. And if you have any queries, please feel free to comment down there. I\u2019ll be looking forward to clearing your doubts.", "I love making new friends, so here is my LinkedIn ID. Please connect if you wanna chat or even if you don\u2019t. XD.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd4669b613ac&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sreyan2000?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sreyan2000?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "Sreyan Ghosh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F61465f0747f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&user=Sreyan+Ghosh&userId=61465f0747f7&source=post_page-61465f0747f7----dd4669b613ac---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd4669b613ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd4669b613ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@josephgardnerphotography?utm_source=medium&utm_medium=referral", "anchor_text": "Joe Gardner"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/sreyan-ghosh/pytorch_files", "anchor_text": "https://github.com/sreyan-ghosh/pytorch_files"}, {"url": "https://www.linkedin.com/in/sreyan-ghosh-b0722a18b/", "anchor_text": "https://www.linkedin.com/in/sreyan-ghosh-b0722a18b/"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----dd4669b613ac---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/regularization?source=post_page-----dd4669b613ac---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dd4669b613ac---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----dd4669b613ac---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/simplicity?source=post_page-----dd4669b613ac---------------simplicity-----------------", "anchor_text": "Simplicity"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd4669b613ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&user=Sreyan+Ghosh&userId=61465f0747f7&source=-----dd4669b613ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd4669b613ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&user=Sreyan+Ghosh&userId=61465f0747f7&source=-----dd4669b613ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd4669b613ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdd4669b613ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dd4669b613ac---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dd4669b613ac--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dd4669b613ac--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dd4669b613ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sreyan2000?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sreyan2000?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sreyan Ghosh"}, {"url": "https://medium.com/@sreyan2000/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F61465f0747f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&user=Sreyan+Ghosh&userId=61465f0747f7&source=post_page-61465f0747f7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F61465f0747f7%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-regularization-techniques-in-ml-and-dl-dd4669b613ac&user=Sreyan+Ghosh&userId=61465f0747f7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}