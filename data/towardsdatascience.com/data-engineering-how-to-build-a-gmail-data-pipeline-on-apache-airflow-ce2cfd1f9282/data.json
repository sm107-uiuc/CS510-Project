{"url": "https://towardsdatascience.com/data-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282", "time": 1682997045.614741, "path": "towardsdatascience.com/data-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282/", "webpage": {"metadata": {"title": "Data Engineering \u2014 How to Build a Gmail Data Pipeline on Apache Airflow | by Nicholas Leong | Towards Data Science", "h1": "Data Engineering \u2014 How to Build a Gmail Data Pipeline on Apache Airflow", "description": "A definitive guide on how to build a data pipeline that extracts data from your Gmail Inbox using Apache Airflow"}, "outgoing_paragraph_urls": [{"url": "https://www.toptenreviews.com/best-free-email-services", "anchor_text": "one", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/data-engineering-basics-of-apache-airflow-build-your-first-pipeline-eefecb7f1bb9", "anchor_text": "Basics of Apache Airflow", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/data-engineering-basics-of-apache-airflow-build-your-first-pipeline-eefecb7f1bb9", "anchor_text": "article", "paragraph_index": 16}, {"url": "https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-usage-python", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://www.linkedin.com/in/nickefy/", "anchor_text": "linkedIn Profile", "paragraph_index": 41}, {"url": "https://github.com/nickefy", "anchor_text": "Github", "paragraph_index": 41}, {"url": "https://www.linkedin.com/in/nickefy/", "anchor_text": "https://www.linkedin.com/in/nickefy/", "paragraph_index": 43}], "all_paragraphs": ["Greetings my fellow readers.How are you doing today ? Good ? I\u2019m about to make it better by explaining to you how I made a Gmail Data Pipeline for my company.Gather around. You don\u2019t wanna miss this.", "What is that ? Why should you care about this you say ?Well, considering the fact that Google has pretty much dominated the playing field for email service providers by integrating pretty much a world of technologies with it like Google Drive, Google Photos, Google Calendar, Google Everything, it is also rated as the top email service providers by many review sites. Here\u2019s one example.", "It is also preferred by majority of companies and casual users.I\u2019m not gonna dive into why Gmail is the best.Who uses yahoo mail here ? Exactly my point.", "Not convinced yet ?Just take a look at the Google Trends for the comparisons between the Email Providers.", "The results speak for itself.There is also huge support for Gmail related code on Github. Github has over 16k repositories and 180mil codes regarding Gmail alone. Hence, anything you can think of implementing with Gmail, someone has probably done something similar to it.", "That being said. Are you onboard the Gmail Train yet ? Better get on. After you\u2019ve told your boss that you are able to extract, transform and load data from Gmail into your data warehouse automatically, he\u2019ll probably promote you to the data science lead immediately with a huge raise. Please do donate some dough to me then, my bank account number is I-M-K-I-D-D-I-N-G from the Sarcasm Bank. Let\u2019s get into business.", "I will be using Apache Airflow managed by Google Cloud Composer to execute the pipeline. I am also using Google Bigquery as my data warehouse here in my company. You may use any WMS and data warehouse you prefer, the Google API should be callable regardless.", "I wrote an article on how to operate Apache Airflow from zero to hero.You can find it here : Basics of Apache Airflow", "The first step for the pipeline is to set up a label inbox to be extracted.It is possible to extract all the attachments in your inbox, but think about it.Some of it will be pictures, text files, videos, pdfs etc. We only want csv files because data is usually sent that way. We also want to standardise the format for each pipeline so we don\u2019t run into any inconsistencies in the future. For eg, data from James who sends you the weekly financial report is different from the data sent by Monica who sends you the weekly campaign results.Hence, we need to differentiate the data by adding labels to it.", "Head into your gmail inbox. On the top right corner,", "Settings > Filters and Blocked Addresses > Create a New filter", "Set up the conditions for the label. For example, unknown@gmail.com sends me a weekly report on the performance of a product. The subject of the email is usually \u2018Weekly Report Product A\u2019", "Hence, I will set From : unknown@gmail.comsubject : Weekly Report Product Aand check the has attachment box.", "Select Create Filter and you will be faced with a bunch of checkboxes:The important ones are", "Do check both boxes and create a new label name for this specific batch of emails. I usually check \u2018Skip The Inbox\u2019 as well so that the emails get directed to the labeled inbox only. Let\u2019s create a label inbox for the example we are using named as \u2018Weekly Report Product A Inbox\u2019 .", "You should now see all the emails, including existing ones regarding the Weekly Report of Product A that contains attachments in the Weekly Report Product A Inbox. Congratulations, you have completed the easiest step in this Pipeline.", "If you are lost right now, give yourself two slaps and focus. We\u2019re about to get into the nitty gritty, code that is. I assume you know how to create DAGs and operators in Apache Airflow, if you don\u2019t, save yourself some time and read my article on how to do so. With that out of the way, open your favourite text editor and start coding.", "We are setting the DAG to run Weekly on Mondays 10am. This is assumed that the Weekly Report will be sent by 10am every Monday. If the email is received after 10am, there won\u2019t be any data to load into the data warehouse. Keep that in mind as you set this up, you always want to set a time that you are certain the data will be present in the inbox. There\u2019s also no particular reason I\u2019ve chosen the report to be weekly. If it\u2019s a daily report you\u2019re dealing with, just change the schedule_interval to timedelta(days=1).", "Let\u2019s design our tasks to be ran in this specific DAG. The way I do it is:", "Let\u2019s write the code for the operators to execute these tasks.Don\u2019t worry. I will run through all the important operators with you.Open a second window in your text editor and start coding your operators.", "In this block of code, we are:", "Remember that the Operator only takes into account emails that are Unseen/Unread only. Hence, if you somehow accidentally clicked on the email, it will be automatically marked as read, disregarding the email completely from the Pipeline. If you do not want that to happen, remember to mark it as Unseen/Unread from the Gmail UI after you had read it.", "Here\u2019s the full code for the operator itself:", "Thats the first operator done. Moving on.", "Since emails may be missed inevitably, the DAG will sometimes return an error since no data is loaded into BQ. For this section, we are checking if there are any files to be loaded into Google Bigquery using the ShortCircuit Operator. If there isn\u2019t a file to be loaded, all downstream tasks of this operator are skipped, and the DAG is successfully completed. Otherwise, the DAG proceeds as usual. There is no need to write a custom operator for this, we will use the default operator provided by Airflow. When the tasks are skipped, Airflow will display pink indicators. An example is shown below.", "Important : Since I\u2019m using Google Cloud Composer by default, I do not have to go through any authentication while calling the Google Bigquery API. If you\u2019re using vanilla Airflow, please find out how to call the Google Bigquery API here. You will also need to create the table in Google BQ before loading the data into it.", "In this block of code, we are :", "The cleaning with Pandas is to avoid any human errors. However, you may do some sort of transformation in that section and load that file instead. I also use it to add a column called Airflow Execution Date with the execution date of the DAG as the input. This is useful for removing duplicates later.Here\u2019s the full code for the operator:", "Thats another one down, hang in there.", "Whenever we load (append) data into BQ,there may be duplications in the data. This may happen due to several reasons :", "Hence, just to be sure, we always run a duplication check to ensure we only load the data once. We would not want to report a 2x Revenue just to find out it was a duplicated load now do we ?", "In this block of code we are:", "Considering the mechanics, we will need to have a date column in the table. If there isn\u2019t one by default, we have to create one explicitly. This is why I created the Airflow Execution Date column in the previous operator.Once again, here\u2019s the full code for the operator:", "Save all the operators you\u2019ve written in a folder accessible by Airflow. For me, it\u2019s a folder named Operators in the DAG folder. The final 2 Operators are just to notify you on the status of the job as well as writing logs in BQ itself. There is also a section in the writing logs operator that deletes the files in the file_path directory. I will not go into the last 2 operators since we can already achieve what we wanted to up until this stage. I will write an article for Writing Logs and Sending Emails since it caters for all of my DAGs. This article has been going for too long anyways. Stay with me.", "With all our operators done, what\u2019s left is to call them into our DAG file.Let\u2019s head back into our DAG file.", "The ShortCircuit Operator works by passing a python function that returns a True or False output. If the output is True, the DAG proceeds as usual. Otherwise, the DAG skips all downstream tasks and proceeds to completion. Hence, we write a function to check for files in the file_path directory and return True if files exists.", "Note that all the parameters to be passed into the operators are defined at the top of the DAG. This is done so that the DAG can be used as a template for the other labels in your Gmail Inbox. It is also easier to be explained to your colleagues that may want to use it.", "Relax. There\u2019s no more code to read.Pop the DAG file into your Airflow DAG folder and you\u2019re good.Congratulations, you have successfully built your personal Gmail Data Pipeline. What to do with the data is entirely up to you now. You can automate dashboards to show reports, make a webpage showing trends or even feed data to your Machine Learning Model.", "So go now my fellow data practitioners. Spread the word. Give yourself a pat in the back and.. continue working on the next project. At least thats what\u2019s next for me.", "To end, let me drop a quote :", "You can have data without information, but you cannot have information without data. \u2014 Daniel Keys Moran", "Thanks for reading ! If you want to get in touch with me, feel free to reach me on nickmydata@gmail.com or my linkedIn Profile. You can also view the code in my Github.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist turned Engineer \u2014 Crunching data and writing about it so you don\u2019t get headaches. https://www.linkedin.com/in/nickefy/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fce2cfd1f9282&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://nickefy.medium.com/?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": ""}, {"url": "https://nickefy.medium.com/?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "Nicholas Leong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4539c5f517b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&user=Nicholas+Leong&userId=c4539c5f517b&source=post_page-c4539c5f517b----ce2cfd1f9282---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce2cfd1f9282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce2cfd1f9282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.toptenreviews.com/best-free-email-services", "anchor_text": "one"}, {"url": "https://towardsdatascience.com/data-engineering-basics-of-apache-airflow-build-your-first-pipeline-eefecb7f1bb9", "anchor_text": "Basics of Apache Airflow"}, {"url": "https://towardsdatascience.com/data-engineering-basics-of-apache-airflow-build-your-first-pipeline-eefecb7f1bb9", "anchor_text": "article"}, {"url": "http://twitter.com/apply_defaults", "anchor_text": "@apply_defaults"}, {"url": "http://twitter.com/apply_defaults", "anchor_text": "@apply_defaults"}, {"url": "https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-usage-python", "anchor_text": "here"}, {"url": "http://twitter.com/apply_defaults", "anchor_text": "@apply_defaults"}, {"url": "https://www.nicholas-leong.com/sign-up-here", "anchor_text": "Subscribe to my newsletter to stay in touch."}, {"url": "https://www.linkedin.com/in/nickefy/", "anchor_text": "linkedIn Profile"}, {"url": "https://github.com/nickefy", "anchor_text": "Github"}, {"url": "https://medium.com/tag/python?source=post_page-----ce2cfd1f9282---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ce2cfd1f9282---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----ce2cfd1f9282---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data?source=post_page-----ce2cfd1f9282---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/google?source=post_page-----ce2cfd1f9282---------------google-----------------", "anchor_text": "Google"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce2cfd1f9282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&user=Nicholas+Leong&userId=c4539c5f517b&source=-----ce2cfd1f9282---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce2cfd1f9282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&user=Nicholas+Leong&userId=c4539c5f517b&source=-----ce2cfd1f9282---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce2cfd1f9282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fce2cfd1f9282&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ce2cfd1f9282---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ce2cfd1f9282--------------------------------", "anchor_text": ""}, {"url": "https://nickefy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://nickefy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nicholas Leong"}, {"url": "https://nickefy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2K Followers"}, {"url": "https://www.linkedin.com/in/nickefy/", "anchor_text": "https://www.linkedin.com/in/nickefy/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4539c5f517b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&user=Nicholas+Leong&userId=c4539c5f517b&source=post_page-c4539c5f517b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cfd2d291e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-engineering-how-to-build-a-gmail-data-pipeline-on-apache-airflow-ce2cfd1f9282&newsletterV3=c4539c5f517b&newsletterV3Id=2cfd2d291e3&user=Nicholas+Leong&userId=c4539c5f517b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}