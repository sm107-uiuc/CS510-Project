{"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e", "time": 1683016783.044937, "path": "towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e/", "webpage": {"metadata": {"title": "Mastering Word Embeddings in 10 Minutes with IMDB Reviews | by Orhan G. Yal\u00e7\u0131n | Towards Data Science", "h1": "Mastering Word Embeddings in 10 Minutes with IMDB Reviews", "description": "Learn the Basics of Text Vectorization, Create a Word Embedding Model trained with a Neural Network on IMDB Reviews Dataset, and Visualize it with TensorBoard Embedding Projector"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "Mastering Word Embeddings in 10 Minutes with TensorFlow", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "this tutorial", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "natural language processing", "paragraph_index": 0}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/tutorials/text/word_embeddings", "anchor_text": "tutorial", "paragraph_index": 1}, {"url": "http://keras.io", "anchor_text": "Keras", "paragraph_index": 1}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews", "paragraph_index": 1}, {"url": "http://projector.tensorflow.org/", "anchor_text": "Embedding Projector", "paragraph_index": 1}, {"url": "http://colab.research.google.com", "anchor_text": "create a new Colab notebook", "paragraph_index": 2}, {"url": "http://tensorflow.org", "anchor_text": "TensorFlow", "paragraph_index": 3}, {"url": "https://docs.python.org/3/library/os.html", "anchor_text": "os", "paragraph_index": 3}, {"url": "https://www.imdb.com/", "anchor_text": "IMDB", "paragraph_index": 4}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews", "paragraph_index": 4}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "directory", "paragraph_index": 5}, {"url": "http://projector.tensorflow.org/", "anchor_text": "Embedding Projector", "paragraph_index": 43}, {"url": "http://projector.tensorflow.org/", "anchor_text": "Embedding Projector", "paragraph_index": 49}, {"url": "http://eepurl.com/hd6Xfv", "anchor_text": "Newsletter", "paragraph_index": 53}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Linkedin", "paragraph_index": 55}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Orhan G. Yal\u00e7\u0131n \u2014 Linkedin", "paragraph_index": 55}, {"url": "http://Vizio.ai", "anchor_text": "Vizio.ai", "paragraph_index": 58}], "all_paragraphs": ["This is a follow-up tutorial prepared after Part I of the tutorial, Mastering Word Embeddings in 10 Minutes with TensorFlow, where we introduce several word vectorization concepts such as One Hot Encoding and Encoding with a Unique ID Value. I would highly recommend you to check this tutorial if you are new to natural language processing.", "In Part II of the tutorial, we will vectorize our words and trained their values using the IMDB Reviews dataset. This tutorial is our own take on TensorFlow\u2019s tutorial on word embedding. We will train a word embedding using a simple Keras model and the IMDB Reviews dataset. Then, we will visualize them using Embedding Projector.", "First of all, you need the environment to start coding. For the sake of simplicity, I recommend you work with Google Colab. It comes with all the libraries pre-installed, and you won\u2019t have to worry about them. All you need is a Google account, and I am sure you have one. So, create a new Colab notebook (see Figure 2) and start coding.", "We will start by importing TensorFlow and os libraries. We will use the os library for some directory level operations we will do below and the TensorFlow library for dataset loading, deep learning models, and text preprocessing.", "IMDB Reviews Dataset is a large movie review dataset collected and prepared by Andrew L. Maas from the popular movie rating service, IMDB. The IMDB Reviews dataset is used for binary sentiment classification, whether a review is positive or negative. It contains 25,000 movie reviews for training and 25,000 for testing. All these 50,000 reviews are labeled data that may be used for supervised deep learning. Besides, there is an additional 50,000 unlabeled reviews that we will not use in this case study. In this case study, we will only use the training dataset.", "We can download the dataset from Stanford\u2019s relevant directory with tf.keras.utils.get_file function, as shown below:", "We need a little bit of housekeeping to create a proper dataset. Let\u2019s start with viewing our main directory with the", "As you can see below, we have our train and test folders. For this study, we will only use the /train folder", "With the following lines, let\u2019s view what\u2019s under the /train subdirectory:", "We have reviews with negative sentiments and positive sentiments. Next step, we will remove theunsup folder, which contains unlabeled reviews. Since we are working on a supervised learning problem in this tutorial, we do not need it.", "As you can see in Figure X, we removed the unsup folder thanks to theshutil library:", "Now that we cleaned our directory, we can create our Dataset object. For this, we can use thetf.keras.preprocessing.text_dataset_from_directory function. As the name suggests, the text_dataset_from_directory function allows us to create text datasets directly from a directory. We selected an 80/20 train and validation split, but feel free to play around by adjusting the validation_split argument.", "As you can see in Figure 5, we have 20,000 reviews for training and 5,000 for validation.", "Let\u2019s check how our dataset looks by using the .take() function and run a for-loop. Note that our dataset is a TensorFlow Dataset object. It requires a little more effort to print out its elements. The following line does that:", "And here is the results in Figure 6:", "Now, since we are in the realms of deep learning, optimization is essential for a bearable training experience. TensorFlow has an experimental tool that we can use to optimize the workload and shorten the time needed for preprocessing, training, and other parallel operations. We can optimize our pipeline with the following lines:", "Now that we created our dataset, it is time to process its elements so that our model can understand them.", "We will create a custom string standardization function to make the best of standardization. Standardization can be described as a set of preprocessing operations for NLP studies, including lowercasing, tag removal, and punctuation stripping. In the below code, we are achieving exactly these:", "Now our data will be more standardized with our custom function.", "Since we created our custom standardization function, we can pass it in the TextVectorization layer we import from TensorFlow. TextVectorization is a layer that we use to map our strings to integers. We will pass in our custom standardization function, we will use up to 10,000 unique words (vocabulary), and we will keep a maximum of 100 words for each review. Check the below lines:", "We will remove the labels from the train dataset and call the .adapt() function to build the vocabulary to use later on. Note that we haven\u2019t vectorized our dataset yet. Just created the vocabulary with the lines below:", "We already processed our reviews, and it is time to create our model.", "We will make the inial imports, which include Sequential API for model building and Embedding, GlobalAveragePooling, and Dense layers we will use in the model.", "We set the embedding dimension to 16, so each word will have 16 representative values. We limit the vocabulary size to 10,000 in parallel with the code above.", "We add the following layers to our Keras model:", "1 \u2014 A TextVectorization layer for converting strings to integers;", "2 \u2014 AEmbedding layer to convert integer values with 16-dimensional vectors;", "3 \u2014 A Global Average Pooling 1D layer to resolve the issue of having reviews with different lengths;", "4 \u2014 A Dense layer with 16 neurons with a relu activation layer", "5 \u2014 A final Dense layer with 1 neuron to classify if the review has a positive or negative sentiment.", "The following lines do all these:", "Since we want to see how our model evolves and performs over time, we will configure our callback settings with the following lines:", "We will use these callbacks to visualize our model performance at each epoch using TensorBoard", "Then, we will configure our model with Adam as optimizer and Binary Crossentropy as loss function because it is a binary classification task and select accuracy as our performance metric.", "Now that our model is configured, we can use .fit() function to start the training. We will run for 15 epochs and record the callbacks for TensorBoard.", "Here is the screenshot of the training process, as shown in Figure :", "Now that we concluded our model training let\u2019s do some visualization to understand better what we built and trained.", "We can easily see the summary of our model with the .summary() function, as shown below:", "Figure 8 shows how our model looks and lists the number of parameters and output shape for each layer:", "Let\u2019s see how our mode evolved as it trained on the IMDB reviews dataset. We can use TensorFlow\u2019s visualization kit, TensorBoard. TensorBoard can be used for several machine learning visualization tasks such as:", "In this tutorial, we will use %load_ext to load TensorBoard and view the logs. The lines above will run a small server within our cell to visualize our metric values over time.", "As you can see on the left, our accuracy increases over time while our loss values decrease. Figure 9 shows that our model does what it is supposed to do because decreasing loss value means that our model is doing something to lower its mistakes: learning.", "Our model looks nice and it learned a lot in just 15 epochs. But, the main goal of this tutorial to create a word embedding. We will not predict review sentiments in this tutorial. Instead, we will visualize our word embedding cloud using Embedding Projector.", "Embedding Projector is a tool built on top of TensorBoard. It is a useful tool to analyze data and visualize the position of embedding values relative to one another. Using Embedding Projector, we can graphically represent high dimensional embedding by simplifying them using algorithms like PCA.", "We will start by getting our 16-dimensional embedding values for each word. Also, we will get a list of all these words we embedded. We can achieve these two tasks with the following code:", "Let\u2019s see how our word and its vector values look with a random example. We selected the word with index no. 500 and visualize it with the following code:", "The vector values and the corresponding word for index no. 500 is shown in Figure 10:", "Feel free to change the index value to view other words with their vector values.", "Now we have the entire list of words (vocabulary) with their corresponding 16-dimensional vector values. We will save word names to the metadata.tsv file and vector values to the vectors.tsv file. The following lines create new files, write our data to these new files, save the data, close the files, and download them to your local machine:", "Now we visit the Embedding Projector website:", "Then, we click the \u201cLoad\u201d button on the left to load our vectors.tsv and metadata.tsv files. Then, we can click anywhere outside of the popup window.", "Note that Embedding Projectors runs a PCA algorithm to reduce the 16-dimensional vector space into 3-dimensional since this is the only way to visualize it.", "You have successfully built a neural network to train a word embedding model, and it takes a lot of effort to achieve this. Pat yourself on the back and keep improving yourself in the field of natural language processing, as there are many unsolved problems.", "If you liked this post, consider subscribing to the Newsletter! \u2709\ufe0f", "Besides my latest content, I also share my Google Colab notebooks with my subscribers, containing full codes for every post I published.", "Since you are reading this article, I am sure that we share similar interests and are/will be in similar industries. So let\u2019s connect via Linkedin! Please do not hesitate to send a contact request! Orhan G. Yal\u00e7\u0131n \u2014 Linkedin", "Take a look at the guide to my content:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I write about AI and data apps here building them at Vizio.ai with my team. Feel free to get in touch!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc345f83e054e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c345f83e054e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c345f83e054e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://blog.orhangaziyalcin.com/?source=post_page-----c345f83e054e--------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=post_page-----c345f83e054e--------------------------------", "anchor_text": "Orhan G. Yal\u00e7\u0131n"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff47ab81282a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=post_page-ff47ab81282a----c345f83e054e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc345f83e054e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc345f83e054e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "\u2190 Part 1"}, {"url": "https://unsplash.com/@raphaelphotoch?utm_source=medium&utm_medium=referral", "anchor_text": "Raphael Schaller"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "Mastering Word Embeddings in 10 Minutes with TensorFlow"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "this tutorial"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "natural language processing"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "Mastering Word Embeddings in 10 Minutes with TensorFlowCovering the Basics of Word Embedding, One Hot Encoding, Text Vectorization, Embedding Layers, and an Example Neural\u2026towardsdatascience.com"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews"}, {"url": "https://www.tensorflow.org/tutorials/text/word_embeddings", "anchor_text": "tutorial"}, {"url": "http://keras.io", "anchor_text": "Keras"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews"}, {"url": "http://projector.tensorflow.org/", "anchor_text": "Embedding Projector"}, {"url": "http://colab.research.google.com", "anchor_text": "create a new Colab notebook"}, {"url": "http://colab.research.google.com", "anchor_text": "Create a New Google Colab Notebook"}, {"url": "http://tensorflow.org", "anchor_text": "TensorFlow"}, {"url": "https://docs.python.org/3/library/os.html", "anchor_text": "os"}, {"url": "https://www.imdb.com/", "anchor_text": "IMDB"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "directory"}, {"url": "http://projector.tensorflow.org/", "anchor_text": "Embedding Projector"}, {"url": "http://projector.tensorflow.org/", "anchor_text": "Embedding Projector"}, {"url": "http://projector.tensorflow.org/", "anchor_text": "Embedding projector - visualization of high-dimensional dataVisualize high dimensional data.projector.tensorflow.org"}, {"url": "http://eepurl.com/hd6Xfv", "anchor_text": "Newsletter"}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Linkedin"}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Orhan G. Yal\u00e7\u0131n \u2014 Linkedin"}, {"url": "https://oyalcin.medium.com/a-guide-to-my-content-on-artificial-intelligence-c70c9b4a3b17", "anchor_text": "A Guide to My Content on Artificial IntelligenceThe guide to help you navigate around my content with ease.oyalcin.medium.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c345f83e054e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c345f83e054e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c345f83e054e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----c345f83e054e---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/python?source=post_page-----c345f83e054e---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc345f83e054e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=-----c345f83e054e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc345f83e054e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=-----c345f83e054e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc345f83e054e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c345f83e054e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc345f83e054e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c345f83e054e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c345f83e054e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c345f83e054e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c345f83e054e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c345f83e054e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c345f83e054e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c345f83e054e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c345f83e054e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c345f83e054e--------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Orhan G. Yal\u00e7\u0131n"}, {"url": "https://blog.orhangaziyalcin.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://Vizio.ai", "anchor_text": "Vizio.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff47ab81282a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=post_page-ff47ab81282a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6340e0deb03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e&newsletterV3=ff47ab81282a&newsletterV3Id=6340e0deb03&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}