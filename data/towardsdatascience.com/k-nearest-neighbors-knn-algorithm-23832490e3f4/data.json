{"url": "https://towardsdatascience.com/k-nearest-neighbors-knn-algorithm-23832490e3f4", "time": 1683013017.024815, "path": "towardsdatascience.com/k-nearest-neighbors-knn-algorithm-23832490e3f4/", "webpage": {"metadata": {"title": "K-Nearest Neighbors (KNN) algorithm | by Vaibhav Jayaswal | Towards Data Science", "h1": "K-Nearest Neighbors (KNN) algorithm", "description": "K nearest neighbors (KNN) is a supervised machine learning algorithm. A supervised machine learning algorithm\u2019s goal is to learn a function such that f(X) = Y where X is the input, and Y is the\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["K nearest neighbors (KNN) is a supervised machine learning algorithm. A supervised machine learning algorithm\u2019s goal is to learn a function such that f(X) = Y where X is the input, and Y is the output. KNN can be used both for classification as well as regression. In this article, we will only talk about classification. Although for regression, there is just a minute change.", "The properties of KNN is that it is a lazy learning algorithm and a non-parametric method.", "Lazy learning means the algorithm takes almost zero time to learn because it only stores the data of the training part (no learning of a function). The stored data will then be used for the evaluation of a new query point.", "The non-parametric method refers to a method that does not assume any distribution. Therefore, KNN does not have to find any parameter for the distribution. While in the parametric method, the model finds new parameters, which in turn will be used for the prediction purpose. The only hyperparameter (provided by the user to the model) KNN has is K, which is the number of points that needs to be considered for comparison purpose.", "In the above image, yellow is the query point, and we want to know which class it belongs to (red or green).", "With K=3, the 3 nearest neighbors of the yellow point are considered, and the class is assigned to the query point based on the majority (e.g., 2 green and 1 red \u2014 then it is of green class). Similarly, for K=5, 5 nearest neighbors are considered for the comparison, and the majority will decide which class the query point belongs to. One thing to notice here, if the value of K is even, it might create problems when taking a majority vote because the data has an even number of classes (i.e., 2). Therefore, choose K as an odd number when the data has an even number of classes and even number when the data has an odd number of classes.", "In the training phase, the model will store the data points. In the testing phase, the distance from the query point to the points from the training phase is calculated to classify each point in the test dataset. Various distances can be calculated, but the most popular one is the Euclidean distance (for smaller dimension data).", "Euclidean distance between a query point (q) and a training data point (p) is defined as", "Other distance measures such as Manhattan, Hamming, and Chebyshev distance can also be used based on the data, which is out of the scope of this article.", "Let\u2019s learn it with an example:", "The procedure for calculating the class of query point is:", "K=1 means that it will take one nearest neighbor and classify the query point based on that. The surface that divides the classes will be very uneven (many vertices).", "The problem that arises here is if an outlier is present in the data, the decision surface considers that as a data point. Due to this, KNN will perform exceptionally well on the training dataset but will misclassify many points on the test dataset (unseen data). This is considered as overfitting, and therefore, KNN is sensitive to outliers.", "As the value of K increases, the surface becomes smooth and will not consider the outliers as data points. This will better generalize the model on the test dataset also.", "If K value is extremely large, the model will underfit and will be unable to classify the new data point. For example, if K is equal to the total number of data points, no matter where the query point lies, the model will always classify the query point based on the majority class of the whole dataset.", "Choosing a correct value K will give accurate results. But how to choose that?", "In real-world problems, the dataset is separated into three parts, namely, training, validation, and test data. In KNN, the training data points get stored, and no learning is performed. Validation data is to check the model performance, and the test data is used for prediction.", "To select optimal K, plot the error of model (error = 1 \u2014 accuracy) on training as well as on the validation dataset. The best K is where the validation error is lowest, and both training and validation errors are close to each other.", "Time complexity and space complexity is enormous, which is a major disadvantage of KNN. Time complexity refers to the time model takes to evaluate the class of the query point. Space complexity refers to the total memory used by the algorithm. If we have n data points in training and each point is of m dimension. Then time complexity is of order O(nm), which will be huge if we have higher dimension data. Therefore, KNN is not suitable for high dimensional data.", "Another disadvantage is if the data point is far away from the classes present (no similarity), KNN will classify the point even if it is an outlier. In order to overcome the problem of time complexity, algorithms such as KD-Tree and Locality Sensitive Hashing (LSH) can be used, which is not covered in this article.", "K- Nearest Neighbors (KNN) identifies the nearest neighbors given the value of K. It is lazy learning and non-parametric algorithm. KNN works on low dimension dataset while faces problems when dealing with high dimensional data.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A machine learning enthusiast. I have pursued my masters from Indian Institute of Technology, Bombay."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F23832490e3f4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----23832490e3f4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----23832490e3f4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jayaswalvaibhav.medium.com/?source=post_page-----23832490e3f4--------------------------------", "anchor_text": ""}, {"url": "https://jayaswalvaibhav.medium.com/?source=post_page-----23832490e3f4--------------------------------", "anchor_text": "Vaibhav Jayaswal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fd8456ab811&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&user=Vaibhav+Jayaswal&userId=8fd8456ab811&source=post_page-8fd8456ab811----23832490e3f4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23832490e3f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23832490e3f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/wtD7u1lFMwg", "anchor_text": "Joshua J. Cotten"}, {"url": "https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "Source"}, {"url": "http://sameersingh.org/courses/gml/fa17/sched.html", "anchor_text": "Source"}, {"url": "https://medium.com/tag/knn?source=post_page-----23832490e3f4---------------knn-----------------", "anchor_text": "Knn"}, {"url": "https://medium.com/tag/nearest-neighbors?source=post_page-----23832490e3f4---------------nearest_neighbors-----------------", "anchor_text": "Nearest Neighbors"}, {"url": "https://medium.com/tag/real-world-examples-knn?source=post_page-----23832490e3f4---------------real_world_examples_knn-----------------", "anchor_text": "Real World Examples Knn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23832490e3f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&user=Vaibhav+Jayaswal&userId=8fd8456ab811&source=-----23832490e3f4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23832490e3f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&user=Vaibhav+Jayaswal&userId=8fd8456ab811&source=-----23832490e3f4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23832490e3f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----23832490e3f4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F23832490e3f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----23832490e3f4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----23832490e3f4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----23832490e3f4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----23832490e3f4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----23832490e3f4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----23832490e3f4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----23832490e3f4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----23832490e3f4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----23832490e3f4--------------------------------", "anchor_text": ""}, {"url": "https://jayaswalvaibhav.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jayaswalvaibhav.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vaibhav Jayaswal"}, {"url": "https://jayaswalvaibhav.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "147 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fd8456ab811&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&user=Vaibhav+Jayaswal&userId=8fd8456ab811&source=post_page-8fd8456ab811--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4199defa080a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-knn-algorithm-23832490e3f4&newsletterV3=8fd8456ab811&newsletterV3Id=4199defa080a&user=Vaibhav+Jayaswal&userId=8fd8456ab811&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}