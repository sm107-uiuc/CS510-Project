{"url": "https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9", "time": 1682993775.095783, "path": "towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9/", "webpage": {"metadata": {"title": "Building a Recommendation System Using Neural Network Embeddings | by Will Koehrsen | Towards Data Science", "h1": "Building a Recommendation System Using Neural Network Embeddings", "description": "In this article, we will see how to use neural network embeddings to create a book recommendation system using all Wikipedia articles on books."}, "outgoing_paragraph_urls": [{"url": "https://blog.statsbot.co/deep-learning-achievements-4c563e034257", "anchor_text": "incredible things", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "obscured in academic papers", "paragraph_index": 0}, {"url": "https://dawn.cs.stanford.edu/benchmark/ImageNet/train.html", "anchor_text": "computing resources", "paragraph_index": 0}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "Jupyter Notebook on GitHub", "paragraph_index": 2}, {"url": "https://www.kaggle.com/willkoehrsen/neural-network-embedding-recommendation-system", "anchor_text": "notebook on Kaggle", "paragraph_index": 2}, {"url": "https://medium.com/p/4d028e6f0526?source=your_stories_page---------------------------", "anchor_text": "covered in an earlier article", "paragraph_index": 2}, {"url": "https://medium.com/p/c08efbac5f5c?source=your_stories_page---------------------------", "anchor_text": "this article", "paragraph_index": 2}, {"url": "http://shop.oreilly.com/product/0636920097471.do", "anchor_text": "Deep Learning Cookbook", "paragraph_index": 3}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "hands-on examples", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1705.03127", "anchor_text": "machine translation", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1604.06737", "anchor_text": "entity embeddings for categorical variables", "paragraph_index": 7}, {"url": "https://medium.com/p/c08efbac5f5c?source=your_stories_page---------------------------", "anchor_text": "In this article", "paragraph_index": 8}, {"url": "https://keras.io/", "anchor_text": "Keras deep learning framework", "paragraph_index": 24}, {"url": "https://keras.io/layers/embeddings/", "anchor_text": "embedding neural network,", "paragraph_index": 26}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "Adam", "paragraph_index": 26}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "optimizer", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Online_machine_learning#Batch_learning", "anchor_text": "batch learners", "paragraph_index": 32}, {"url": "https://wiki.python.org/moin/Generators", "anchor_text": "generator", "paragraph_index": 32}, {"url": "https://arxiv.org/abs/1609.04836", "anchor_text": "small batch size and increase it until performance starts to decline", "paragraph_index": 37}, {"url": "https://keras.io/callbacks/#earlystopping", "anchor_text": "implement early stopping", "paragraph_index": 37}, {"url": "https://www.deeplearningbook.org/", "anchor_text": "Deep Learning", "paragraph_index": 38}, {"url": "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "anchor_text": "cosine similarity", "paragraph_index": 41}, {"url": "https://en.wikipedia.org/wiki/War_and_Peace", "anchor_text": "War and Peace by Leo Tolstoy", "paragraph_index": 42}, {"url": "https://www.google.com/search?rlz=1C5CHFA_enUS805US805&ei=rR21W5iENcGIggf0lrCwAg&q=classic+russian+novels&oq=classic+russian+novels&gs_l=psy-ab.3..0j0i22i30.20451.23296..23425...0.0..0.207.2208.17j4j1......0....1..gws-wiz.......0i71j35i39j0i131j0i67j0i20i264j0i131i67j0i131i20i264.7Ft4zcztdIc", "anchor_text": "classic Russian novels", "paragraph_index": 43}, {"url": "https://www.goodreads.com/book/show/656.War_and_Peace?ac=1&from_search=true", "anchor_text": "GoodReads for these same recommendations", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Bully_for_Brontosaurus", "anchor_text": "Bully for Brontosaurus", "paragraph_index": 45}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "t-Distributed Stochastic Neighbor Embedding (TSNE)", "paragraph_index": 46}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "TSNE", "paragraph_index": 54}, {"url": "https://projector.tensorflow.org/", "anchor_text": "projector", "paragraph_index": 55}, {"url": "https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/WillKoehrsen/wikipedia-data-science/master/embeddings/metadata/metadata_sample.json", "anchor_text": "here", "paragraph_index": 56}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "Deep Learning Cookbook", "paragraph_index": 57}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "notebook", "paragraph_index": 62}, {"url": "https://towardsdatascience.com/diy-deep-learning-projects-c2e0fac3274f", "anchor_text": "many applications", "paragraph_index": 62}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 63}, {"url": "https://willk.online", "anchor_text": "willk.online", "paragraph_index": 63}], "all_paragraphs": ["Deep learning can do some incredible things, but often the uses are obscured in academic papers or require computing resources available only to large corporations. Nonetheless, there are applications of deep learning that can be done on a personal computer with no advanced degree required. In this article, we will see how to use neural network embeddings to create a book recommendation system using all Wikipedia articles on books.", "Our recommendation system will be built on the idea that books which link to similar Wikipedia pages are similar to one another. We can represent this similarity and hence make recommendations by learning embeddings of books and Wikipedia links using a neural network. The end result is an effective recommendation system and a practical application of deep learning.", "The complete code for this project is available as a Jupyter Notebook on GitHub. If you don\u2019t have a GPU, you can also find the notebook on Kaggle where you can train your neural network with a GPU for free.This article will focus on the implementation, with the concepts of neural network embeddings covered in an earlier article. (To see how to retrieve the data we\u2019ll use \u2014 all book articles on Wikipedia \u2014 take a look at this article.)", "This project was adapted from the Deep Learning Cookbook, an excellent book with hands-on examples for applying deep learning.", "Embeddings are a way to represent discrete \u2014 categorical \u2014 variables as continuous vectors. In contrast to an encoding method like one-hot encoding, neural network embeddings are low-dimensional and learned, which means they place similar entities closer to one another in the embedding space.", "In order to create embeddings, we need a neural network embedding model and a supervised machine learning task. The end outcome of our network will be a representation of each book as a vector of 50 continuous numbers.", "While the embeddings themselves are not that interesting \u2014 they are just vectors \u2014 they can be used for three primary purposes:", "This project covers primarily the first use case, but we\u2019ll also see how to create visualizations from the embeddings. Practical applications of neural network embeddings include word embeddings for machine translation and entity embeddings for categorical variables.", "As usual with a data science project, we need to start with a high-quality dataset. In this article, we saw how to download and process every single article on Wikipedia, searching for any pages about books. We saved the book title, basic information, links on the book\u2019s page that point to other Wikipedia pages (wikilinks), and links to external sites. To create the recommendation system, the only information we need are the title and wikilinks.", "Even when working with a neural network, it\u2019s important to explore and clean the data, and in the notebook I make several corrections to the raw data. For example, looking at the most linked pages:", "We see that the top four pages are generic and won\u2019t help in making recommendations. The format of a book tells us nothing about the content: knowing a book is paperback or hardcover does not allow us \u2014 or a neural network \u2014to figure out the other books it is similar to. Therefore, we can remove these links to help the neural network distinguish between books.", "Thinking about the end purpose can help in the data cleaning stage and this action alone significantly improves the recommendations.", "Out of pure curiosity, I wanted to find the books most linked to by other books on Wikipedia. These are the 10 \u201cmost connected\u201d Wikipedia books:", "This is a mix of reference works and classic books which makes sense.", "After data cleaning, we have a set of 41758 unique wikilinks and 37020 unique books. Hopefully there is a book in there for everyone!", "Once we\u2019re confident our data is clean, we need to develop a supervised machine learning task with labeled training examples.", "To learn meaningful embeddings, our neural network must be trained to accomplish an objective. Working from the guiding assumption of the project \u2014 that similar books link to similar Wikipedia pages \u2014 we can formulate the problem as follows: given a (book title, wikilink) pair, determine if the wikilink is present in the book\u2019s article.", "We won\u2019t actually need to give the network the book article. Instead, we\u2019ll feed in hundreds of thousands of training examples consisting of book title, wikilink, and label. We give the network some true examples \u2014 actually present in the dataset \u2014 and some false examples, and eventually it learns embeddings to distinguish when a wikilink is on a book\u2019s page.", "Expressing the supervised learning task is the most important part of this project. Embeddings are learned for a specific task and are relevant only to that problem. If our task was to determine which books were written by Jane Austen, then the embeddings would reflect that goal, placing books written by Austen closer together in embedding space. We hope that by training to tell if a book has a certain wikilink on its page, the network learns embeddings that places similar books \u2014 in terms of content \u2014 closer to one another.", "Once we\u2019ve outlined the learning task, we need to implement it in code. To get started, because the neural network can only accept integer inputs, we create a mapping from each unique book to an integer:", "We also do the same thing with the links. After this, to create a training set, we make a list of all (book, wikilink) pairs in the data. This requires iterating through each book and recording an example for each wikilink on its page:", "This gives us a total of 772798 true examples that we can sample from to train the model. To generate the false examples \u2014 done later \u2014 we\u2019ll simply pick a link index and book index at random, make sure it\u2019s not in the pairs, and then use it as a negative observation.", "Note about Training / Testing Sets", "While using a separate validation and testing set is a must for a normal supervised machine learning task, in this case, our primary objective is not to make the most accurate model, but to generate embeddings. The prediction task is just the means by which we train our network for those embeddings. At the end of training, we are not going to be testing our model on new data, so we don\u2019t need to evaluate the performance or use a validation set to prevent overfitting. To get the best embeddings, we\u2019ll use all examples for training.", "Although neural network embeddings sound technically complex, they are relatively easy to implement with the Keras deep learning framework. (I recommend starting with Keras if you are new to deep learning. TensorFlow may give you more control, but Keras cannot be beat for development).", "The embedding model has 5 layers:", "In an embedding neural network, the embeddings are the parameters \u2014 weights \u2014 of the neural network that are adjusted during training in order to minimize loss on the objective. The neural network takes in a book and a link as integers and outputs a prediction between 0 and 1 that is compared to the true value. The model is compiled with the Adam optimizer (a variant on Stochastic Gradient Descent) which, during training, alters the embeddings to minimize the binary_crossentropy for this binary classification problem.", "Below is the code for the complete model:", "This same framework can be used for many embedding models. The important point to understand is that the embeddings are the model parameters (weights) and also the final result we want. We don\u2019t really care if the model is accurate, what we want is relevant embeddings.", "We\u2019re used to the weights in a model being a means to make accurate predictions, but in an embedding model, the weights are the objective and the predictions are a means to learn an embedding.", "There are almost 4 million weights as shown by the model summary:", "With this approach, we\u2019ll get embeddings not only for books, but also for links which means we can compare all Wikipedia pages that are linked to by books.", "Neural networks are batch learners because they are trained on a small set of samples \u2014 observations \u2014 at a time over many rounds called epochs. A common approach for training neural networks is to use a generator. This is a function that yields (not returns) batches of samples so the entire result is not held in memory. Although it\u2019s not an issue in this problem, the benefit of a generator is that large training sets do not need to all be loaded into memory.", "Our generator takes in the training pairs, the number of positive samples per batch ( n_positive ) , and the ratio of negative:positive samples per batch ( negative_ratio ). The generator yields a new batch of positive and negative samples each time it is called. To get positive examples, we randomly sample true pairs. For the negative examples, we randomly sample a book and link, make sure this pairing is not in the true pairs, and then add it to the batch.", "The code below shows the generator in its entirety.", "Each time we call next on the generator, we get a new training batch.", "With a supervised task, a training generator, and an embedding model, we\u2019re ready to learn book embeddings.", "There are a few training parameters to select. The first is the number of positive examples in each batch. Generally, I try to start with a small batch size and increase it until performance starts to decline. Also, we need to choose the number of negative examples trained for each positive example. I\u2019d recommend experimenting with a few options to see what works best. Since we\u2019re not using a validation set to implement early stopping, I choose a number of epochs beyond which the training loss does not decrease.", "(If the training parameters seem arbitrary, in a sense they are, but based on best practices outlined in Deep Learning. Like most aspects of machine learning, training a neural network is largely empirical.)", "Once the network is done training, we can extract the embeddings.", "The embeddings themselves are fairly uninteresting: they are just 50-number vectors for each book and each link:", "However, we can use these vectors for two different purposes, the first of which is to make our book recommendation system. To find the closest book to a query book in the embedding space, we take the vector for that book and find its dot product with the vectors for all other books. If our embeddings are normalized, then the dot product between the vectors represents the cosine similarity, ranging from -1, most dissimilar, to +1, most similar.", "Querying the embeddings for the classic War and Peace by Leo Tolstoy yields:", "The recommendations make sense! These are all classic Russian novels. Sure we could have gone to GoodReads for these same recommendations, but why not build the system ourselves? I encourage you to work with the notebook and explore the embeddings yourself.", "In addition to embedding the books, we also embedded the links which means we can find the most similar links to a given Wikipedia page:", "Currently, I\u2019m reading a fantastic collection of essays by Stephen Jay Gould called Bully for Brontosaurus. What should I read next?", "One of the most intriguing aspects of embeddings are that they can be used to visualize concepts such as novel or nonfiction relative to one another. This requires a further dimension reduction technique to get the dimensions to 2 or 3. The most popular technique for reduction is another embedding method: t-Distributed Stochastic Neighbor Embedding (TSNE).", "Starting with the 37,000-dimensional space of all books on Wikipedia, we map it to 50 dimensions using embeddings, and then to just 2 dimensions with TSNE. This results in the following image:", "By itself this image is not that illuminating, but once we start coloring it by book characteristics, we start to see clusters emerge:", "There are some definite clumps (only the top 10 genres are highlighted) with non-fiction and science fiction having distinct sections. The novels seem to be all over the place which makes sense given the diversity in novel content.", "We can also do embeddings with the country:", "I was a little surprised at how distinctive the countries are! Evidently Australian books are quite unique.", "Furthermore, we can highlight certain books in the Wikipedia map:", "There are a lot more visualizations in the notebook and you make your own. I\u2019ll leave you with one more showing the 10 \u201cmost connected\u201d books:", "One thing to note about TSNE is that it tries to preserve distances between vectors in the original space, but because it reduces the number of dimensions, it may distort the original separation. Therefore, books that are close to one another in the 50-dimensional embedding space may not be closest neighbors in the 2-dimensional TSNE embedding.", "These visualizations are pretty interesting, but we can make stunning interactive figures with TensorFlow\u2019s projector tool specifically designed for visualizing neural network embeddings. I plan on writing an article on how to use this tool, but for now here are some of the results:", "To explore a sample of the books interactively, head here.", "Data science projects aren\u2019t usually invented entirely on their own. A lot of the projects I work on are ideas from other data scientists that I adapt, improve, and build on to make a unique project. (This project was inspired by a similar project for movie recommendations in the Deep Learning Cookbook.)", "With that attitude in mind, here are a few ways to build on this work:", "This is by no means a homework assignment, just some project ideas if you want to put what you\u2019ve read into practice. If you do decide to take on a project, I\u2019d enjoy hearing about it!", "Neural network embeddings are a method to represent discrete categorical variables as continuous vectors. As a learned low-dimensional representation, they are useful for finding similar categories, as input into a machine learning model, or to visualize maps of concepts. In this project, we used neural network embeddings to create an effective book recommendation system built on the idea that books which link to similar pages are similar to each other.", "The steps for creating neural network embeddings are:", "The details can be found in the notebook and I\u2019d encourage anyone to build on this project. While deep learning may seem overwhelming because of technical complexity or computational resources, this is one of many applications that can be done on a personal computer with a limited amount of studying. Deep learning is a constantly evolving field, and this project is a good way to get started by building a useful system. And, when you\u2019re not studying deep learning, now you know what you should be reading!", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or on my personal website at willk.online.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1ef92e5c80c9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----1ef92e5c80c9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef92e5c80c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef92e5c80c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/photo/body-of-water-during-golden-hour-1118945/", "anchor_text": "Source"}, {"url": "https://blog.statsbot.co/deep-learning-achievements-4c563e034257", "anchor_text": "incredible things"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "obscured in academic papers"}, {"url": "https://dawn.cs.stanford.edu/benchmark/ImageNet/train.html", "anchor_text": "computing resources"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "Jupyter Notebook on GitHub"}, {"url": "https://www.kaggle.com/willkoehrsen/neural-network-embedding-recommendation-system", "anchor_text": "notebook on Kaggle"}, {"url": "https://medium.com/p/4d028e6f0526?source=your_stories_page---------------------------", "anchor_text": "covered in an earlier article"}, {"url": "https://medium.com/p/c08efbac5f5c?source=your_stories_page---------------------------", "anchor_text": "this article"}, {"url": "http://shop.oreilly.com/product/0636920097471.do", "anchor_text": "Deep Learning Cookbook"}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "hands-on examples"}, {"url": "https://arxiv.org/abs/1705.03127", "anchor_text": "machine translation"}, {"url": "https://arxiv.org/abs/1604.06737", "anchor_text": "entity embeddings for categorical variables"}, {"url": "https://medium.com/p/c08efbac5f5c?source=your_stories_page---------------------------", "anchor_text": "In this article"}, {"url": "https://keras.io/", "anchor_text": "Keras deep learning framework"}, {"url": "https://keras.io/layers/embeddings/", "anchor_text": "embedding neural network,"}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "Adam"}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "optimizer"}, {"url": "https://en.wikipedia.org/wiki/Online_machine_learning#Batch_learning", "anchor_text": "batch learners"}, {"url": "https://wiki.python.org/moin/Generators", "anchor_text": "generator"}, {"url": "https://arxiv.org/abs/1609.04836", "anchor_text": "small batch size and increase it until performance starts to decline"}, {"url": "https://keras.io/callbacks/#earlystopping", "anchor_text": "implement early stopping"}, {"url": "https://www.deeplearningbook.org/", "anchor_text": "Deep Learning"}, {"url": "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "anchor_text": "cosine similarity"}, {"url": "https://en.wikipedia.org/wiki/War_and_Peace", "anchor_text": "War and Peace by Leo Tolstoy"}, {"url": "https://www.google.com/search?rlz=1C5CHFA_enUS805US805&ei=rR21W5iENcGIggf0lrCwAg&q=classic+russian+novels&oq=classic+russian+novels&gs_l=psy-ab.3..0j0i22i30.20451.23296..23425...0.0..0.207.2208.17j4j1......0....1..gws-wiz.......0i71j35i39j0i131j0i67j0i20i264j0i131i67j0i131i20i264.7Ft4zcztdIc", "anchor_text": "classic Russian novels"}, {"url": "https://www.goodreads.com/book/show/656.War_and_Peace?ac=1&from_search=true", "anchor_text": "GoodReads for these same recommendations"}, {"url": "https://en.wikipedia.org/wiki/Bully_for_Brontosaurus", "anchor_text": "Bully for Brontosaurus"}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "t-Distributed Stochastic Neighbor Embedding (TSNE)"}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "TSNE"}, {"url": "https://projector.tensorflow.org/", "anchor_text": "projector"}, {"url": "https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/WillKoehrsen/wikipedia-data-science/master/embeddings/metadata/metadata_sample.json", "anchor_text": "here"}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "Deep Learning Cookbook"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb", "anchor_text": "this notebook"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "this notebook"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "notebook"}, {"url": "https://towardsdatascience.com/diy-deep-learning-projects-c2e0fac3274f", "anchor_text": "many applications"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://willk.online", "anchor_text": "willk.online"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1ef92e5c80c9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1ef92e5c80c9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/education?source=post_page-----1ef92e5c80c9---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1ef92e5c80c9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1ef92e5c80c9---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ef92e5c80c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----1ef92e5c80c9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ef92e5c80c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----1ef92e5c80c9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef92e5c80c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1ef92e5c80c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1ef92e5c80c9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1ef92e5c80c9--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}