{"url": "https://towardsdatascience.com/multi-gpu-training-in-pytorch-dbdb3389fd4a", "time": 1683004468.761765, "path": "towardsdatascience.com/multi-gpu-training-in-pytorch-dbdb3389fd4a/", "webpage": {"metadata": {"title": "Multi-GPU Training in Pytorch. Data and Model Parallelism | by Rachel Draelos, MD, PhD | Towards Data Science", "h1": "Multi-GPU Training in Pytorch", "description": "Let\u2019s say you have 3 GPUs available and you want to train a model on one of them. You can tell Pytorch which GPU to use by specifying the device: There are a few different ways to use multiple GPUs\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html", "anchor_text": "this article", "paragraph_index": 8}, {"url": "https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html", "anchor_text": "Pytorch documentation", "paragraph_index": 10}, {"url": "https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html", "anchor_text": "this article", "paragraph_index": 13}, {"url": "https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://pytorch.org/docs/stable/data.html", "anchor_text": "this page", "paragraph_index": 25}, {"url": "https://pytorch.org/tutorials/intermediate/ddp_tutorial.html", "anchor_text": "\u201cGetting Started with Distributed Data Parallel.\u201d", "paragraph_index": 26}, {"url": "https://glassboxmedicine.com/2020/03/04/multi-gpu-training-in-pytorch-data-and-model-parallelism/", "anchor_text": "http://glassboxmedicine.com", "paragraph_index": 30}], "all_paragraphs": ["This post will provide an overview of multi-GPU training in Pytorch, including:", "Let\u2019s say you have 3 GPUs available and you want to train a model on one of them. You can tell Pytorch which GPU to use by specifying the device:", "To allow Pytorch to \u201csee\u201d all available GPUs, use:", "There are a few different ways to use multiple GPUs, including data parallelism and model parallelism.", "Data parallelism refers to using multiple GPUs to increase the number of examples processed simultaneously. For example, if a batch size of 256 fits on one GPU, you can use data parallelism to increase the batch size to 512 by using two GPUs, and Pytorch will automatically assign ~256 examples to one GPU and ~256 examples to the other GPU.", "Using data parallelism can be accomplished easily through DataParallel. For example, let\u2019s say you have a model called \u201ccustom_net\u201d that is currently initialized as follows:", "Now, all you have to do to use data parallelism is wrap the custom_net in DataParallel:", "You\u2019ll also want to increase the batch size to make use of all your available devices to their fullest extent.", "For more information on data parallelism, see this article.", "You can use model parallelism to train a model that requires more memory than is available on one GPU. Model parallelism allows you to distribute different parts of the model across different devices.", "There are two steps to using model parallelism. The first step is to specify in your model definition which parts of the model should go on which device. Here\u2019s an example from the Pytorch documentation:", "The second step is to ensure that the labels are on the same device as the model\u2019s outputs when you call the loss function.", "For example, you may want to start out by moving your labels to device \u2018cuda:1\u2019 and your data to device \u2018cuda:0\u2019. Then you can process your data with a part of the model on \u2018cuda:0\u2019, then move the intermediate representation to \u2018cuda:1\u2019 and produce the final predictions on \u2018cuda:1\u2019. Because your labels are already on \u2018cuda:1\u2019 Pytorch will be able to calculate the loss and perform backpropagation without any further modifications.", "For more information on model parallelism, see this article.", "Pytorch\u2019s DataLoader provides an efficient way to automatically load and batch your data. You can use it for any data set, no matter how complicated. All you need to do is first define your own Dataset that inherits from Pytorch\u2019s Dataset class:", "The only requirements on your Dataset are that it defines the methods __len__ and __getitem__.", "The __len__ method must return the total number of examples in your dataset.", "The __getitem__ method must return a single example based on an integer index.", "How you actually prepare the examples and what the examples are is entirely up to you.", "Once you\u2019ve created a Dataset, you need to wrap that Dataset in Pytorch\u2019s Dataloader as follows:", "In order to get batches all you have to do is iterate through the DataLoader:", "If you want to accelerate data loading, you can use more than one worker. Notice in the call to DataLoader you specify a number of workers:", "By default, num_workers is set to 0. Setting num_workers to a positive integer turns on multi-process data loading in which data will be loaded using the specified number of loader worker processes. (Note that this isn\u2019t really multi-GPU, as these loader worker processes are different processes on the CPU, but since it\u2019s related to accelerating model training I decided to put it in the same article).", "Note that more worker processes is not always better. If you set num_workers too high, it can actually slow down your data loading. There are also no great rules about how to choose the optimal number of workers. There are numerous online discussions about it (e.g. here) but no conclusive answers. The reason there aren\u2019t any great rules about how to choose the number of workers is that the optimal number of workers depends on what kind of machine you are using, what kind of data set you are using, and how much on-the-fly pre-processing your data requires.", "A good way to choose a number of workers is to run some small experiments on your data set in which you time how long it takes to load a fixed number of examples using different numbers of workers. As you increase num_workers up from 0, you will first see an increase in data loading speed, followed by a decrease in data loading speed once you hit \u201ctoo many workers.\u201d", "For more information see \u201cMulti-process data loading\u201d on this page.", "If you want to use both model parallelism and data parallelism at the same time, then the data parallelism will have to be implemented in a slightly different way, using DistributedDataParallel instead of DataParallel. For more information, see \u201cGetting Started with Distributed Data Parallel.\u201d", "What if you want to use model parallelism or data parallelism but you don\u2019t want to take up all available devices for a single model? In that case, you can restrict which devices Pytorch can see for each model. Within your code, you\u2019ll set the device as if you want to use all GPUs (i.e. using device = torch.device(\u2018cuda\u2019)) but when you run the code you\u2019ll restrict which GPUs can be seen.", "Let\u2019s say you have 6 GPUs and you want to train Model A on 2 of them and Model B on 4 of them. You can do that as follows:", "Or, if you have 3 GPUs and you want to train Model A on 1 of them and Model B on 2 of them, you could do this:", "Originally published at http://glassboxmedicine.com on March 4, 2020.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a physician with a PhD in Computer Science. My research focuses on machine learning methods development for medical data. I am the CEO of Cydoc."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdbdb3389fd4a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rachel-draelos.medium.com/?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": ""}, {"url": "https://rachel-draelos.medium.com/?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "Rachel Draelos, MD, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F209c0f742bcf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=post_page-209c0f742bcf----dbdb3389fd4a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbdb3389fd4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbdb3389fd4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/File:Anna_Ancher_-_Harvesters_-_Google_Art_Project.jpg", "anchor_text": "Wikipedia"}, {"url": "https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html", "anchor_text": "this article"}, {"url": "https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html", "anchor_text": "Pytorch documentation"}, {"url": "https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html", "anchor_text": "this article"}, {"url": "https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813", "anchor_text": "here"}, {"url": "https://pytorch.org/docs/stable/data.html", "anchor_text": "this page"}, {"url": "https://pytorch.org/tutorials/intermediate/ddp_tutorial.html", "anchor_text": "\u201cGetting Started with Distributed Data Parallel.\u201d"}, {"url": "https://glassboxmedicine.com/2020/03/04/multi-gpu-training-in-pytorch-data-and-model-parallelism/", "anchor_text": "http://glassboxmedicine.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----dbdb3389fd4a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-net?source=post_page-----dbdb3389fd4a---------------neural_net-----------------", "anchor_text": "Neural Net"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dbdb3389fd4a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----dbdb3389fd4a---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/data-science?source=post_page-----dbdb3389fd4a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbdb3389fd4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=-----dbdb3389fd4a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbdb3389fd4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=-----dbdb3389fd4a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbdb3389fd4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdbdb3389fd4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dbdb3389fd4a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dbdb3389fd4a--------------------------------", "anchor_text": ""}, {"url": "https://rachel-draelos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rachel-draelos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rachel Draelos, MD, PhD"}, {"url": "https://rachel-draelos.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "576 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F209c0f742bcf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=post_page-209c0f742bcf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa0377bd1bf3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-gpu-training-in-pytorch-dbdb3389fd4a&newsletterV3=209c0f742bcf&newsletterV3Id=a0377bd1bf3d&user=Rachel+Draelos%2C+MD%2C+PhD&userId=209c0f742bcf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}