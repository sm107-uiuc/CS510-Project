{"url": "https://towardsdatascience.com/web-scraping-with-scrapy-8071fd627051", "time": 1683007219.8129542, "path": "towardsdatascience.com/web-scraping-with-scrapy-8071fd627051/", "webpage": {"metadata": {"title": "Web Scraping with Scrapy. Build your first web crawler | by Siphu Langeni, MS | Towards Data Science", "h1": "Web Scraping with Scrapy", "description": "Scrapy is one of the popular Python frameworks used for web scraping. For the purpose of this tutorial, I wanted to use a website I am familiar with. I previously did a project on classifying hit\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.billboard.com/charts/hot-100", "anchor_text": "https://billboard.com/charts/hot-100", "paragraph_index": 4}, {"url": "https://stackoverflow.com/questions/43630434/how-to-handle-a-429-too-many-requests-response-in-scrapy", "anchor_text": "Stack Overflow", "paragraph_index": 16}, {"url": "https://github.com/SiphuLangeni/Billboard-Scrapy", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://www.linkedin.com/in/SiphuLangeni", "anchor_text": "Linked In", "paragraph_index": 17}], "all_paragraphs": ["Scrapy is one of the popular Python frameworks used for web scraping. For the purpose of this tutorial, I wanted to use a website I am familiar with. I previously did a project on classifying hit records using entries on the Billboard Hot 100 charts as ground truth. I used a python wrapper at the time which was effective in obtaining my dataset. However, I wanted to show that this could have easily been done using Scrapy.", "DISCLAIMER: This is for training and educational purposes only. Please make yourself familiar with ethical practices as they relate to scraping content from the web.", "The first step to any successful web scraping project is to review the website to be scraped. Try to understand what\u2019s happening \u201cunder the hood\u201d. Your browser\u2019s web development tools will be essential in helping you with this step. Identify the information you would like to extract for inclusion in your dataset.", "I previously used Billboard Hot 100 data in a project I worked on. To establish ground truth hits for my Hit Song Classifier, I used billboard.py to extract weekly chart data. This package is a python wrapper that uses Beautiful Soup to parse the html data from the Billboard site. Since I am familiar with this dataset, I thought it would be a good choice to demonstrate how I could use Scrapy to build your first web crawler.", "Navigate to https://billboard.com/charts/hot-100/. Open the web dev tools of your browser by right clicking and selecting inspect or pressing option-command-I. I disable JavaScript at this point by pressing shift-command-P, entering javascript and selecting the Disable JavaScript option. Remember to refresh the page by clicking the refresh button or pressing command-R. This step is crucial for making decisions about creating the web crawler as this allows me to see the page as Scrapy will see it.", "I have decided that I want to scrape the following data from the website:", "I discovered that charts occur weekly and catalogued every Monday from August 4, 1958 until December 25, 1961. Charts were published on Saturday beyond that. The url for previous charts followed the format of base_url/date_string. For example, the charts from the week of December 25, 1999 will be found at https://billboard.com/charts/hot-100/1999\u201312\u201325. We will need this later when creating pagination for our web crawler.", "If you haven\u2019t already done so, be sure to install scrapy.", "While still in the command line choose a directory you want to work in, create a new project and create a basic spider.", "Scrapy creates a new project with all the appropriate hierarchy for your web crawler.", "Between scrapy shell commands and web dev tools, I can discover how best to extract each of my required data from the html. There are 100 songs that appear in each weekly chart. They can be found in the ordered list element. By putting these hundred elements in a variable, I can iterate over each of them to extract relevant information from each one. I have chosen to extract my data using xpaths. You may use css selectors if you prefer. This short tutorial assumes you have some working knowledge of either of these so I will not go into that level of detail.", "Now that I have a good idea about each item I want to scrape, I will open up my spider in a text editor of my choice. I am using Visual Studio Code in this example but any will suffice. I open the project folder, then open the spider that I have created named hot100.py.", "I make a slight change to allowed_domains from \u2018www.billboard.com/charts\u2019 to just \u2018billboard.com\u2019. I also have included the start url in the start_requests function so I can delete the start_urls variable as it is no longer necessary. Also note that I have included the User Agent instead of allowing Scrapy to use the default which may sometimes impede your spider.", "Next, I want to instruct the spider how to crawl the site to get the info. I create a variable, hits, that has all 100 of the hits on the page. Then I create a loop to find each of my variables for each hit.", "Finally, I instruct the spider exactly how to advance to the next page. I created a date object from the date of the chart to easily calculate the date of the previous chart. That date was then turned into a string object in the format YYYY-mm-dd. If you remember from earlier, this was the format we needed to append to the base url to get the get the previous week chart url. A callback is used to inform the spider to go back to the parse method.", "Open up a terminal window inside VS Code by pressing command-J. Ensure you are in the directory you expect to be in. If not, be sure to change the directory accordingly. Then send your spider out to crawl the site! I chose to save my records in a .csv file you could store yours in any structured format that you desire such as .json, .xml, etc. In total, my spider crawled for about 4 hours and more than 300,000 records!", "One of the challenges I encountered was timing out errors. With the help of Aminah Nuraini\u2019s solution on Stack Overflow, I made changes to my settings.py and middleware.py files that made my crawler run smoothly. Please feel free to check out her solution if it pertains to your individual situation. I will not go into the details as they are outside of the scope of this short tutorial.", "There you have it! You are more than equipped to take a crack at creating your first web crawler for the task of web scraping. Complete source code can be found here. If you found this article helpful, please feel to connect with me here and/or on Linked In.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Machine Learning | Deep Learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8071fd627051&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8071fd627051--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8071fd627051--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@SiphuLangeni?source=post_page-----8071fd627051--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@SiphuLangeni?source=post_page-----8071fd627051--------------------------------", "anchor_text": "Siphu Langeni, MS"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fcaa7871337&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&user=Siphu+Langeni%2C+MS&userId=8fcaa7871337&source=post_page-8fcaa7871337----8071fd627051---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8071fd627051&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8071fd627051&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@artnok?utm_source=medium&utm_medium=referral", "anchor_text": "Nicolas Picard"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.billboard.com/charts/hot-100", "anchor_text": "https://billboard.com/charts/hot-100"}, {"url": "https://stackoverflow.com/questions/43630434/how-to-handle-a-429-too-many-requests-response-in-scrapy", "anchor_text": "Stack Overflow"}, {"url": "https://github.com/SiphuLangeni/Billboard-Scrapy", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/SiphuLangeni", "anchor_text": "Linked In"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----8071fd627051---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8071fd627051---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/scrapy?source=post_page-----8071fd627051---------------scrapy-----------------", "anchor_text": "Scrapy"}, {"url": "https://medium.com/tag/python3?source=post_page-----8071fd627051---------------python3-----------------", "anchor_text": "Python3"}, {"url": "https://medium.com/tag/web-crawler?source=post_page-----8071fd627051---------------web_crawler-----------------", "anchor_text": "Web Crawler"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8071fd627051&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&user=Siphu+Langeni%2C+MS&userId=8fcaa7871337&source=-----8071fd627051---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8071fd627051&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&user=Siphu+Langeni%2C+MS&userId=8fcaa7871337&source=-----8071fd627051---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8071fd627051&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8071fd627051--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8071fd627051&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8071fd627051---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8071fd627051--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8071fd627051--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8071fd627051--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8071fd627051--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8071fd627051--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8071fd627051--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8071fd627051--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8071fd627051--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@SiphuLangeni?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@SiphuLangeni?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Siphu Langeni, MS"}, {"url": "https://medium.com/@SiphuLangeni/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "131 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fcaa7871337&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&user=Siphu+Langeni%2C+MS&userId=8fcaa7871337&source=post_page-8fcaa7871337--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F284b4821a92b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-scrapy-8071fd627051&newsletterV3=8fcaa7871337&newsletterV3Id=284b4821a92b&user=Siphu+Langeni%2C+MS&userId=8fcaa7871337&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}