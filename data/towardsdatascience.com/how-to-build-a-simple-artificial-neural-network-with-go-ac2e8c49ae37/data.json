{"url": "https://towardsdatascience.com/how-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37", "time": 1682993297.794584, "path": "towardsdatascience.com/how-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37/", "webpage": {"metadata": {"title": "How to build a simple artificial neural network with Go | by Sau Sheong | Towards Data Science", "h1": "How to build a simple artificial neural network with Go", "description": "I have written a lot of computer programs in my career, most of the time to solve various problems or perform some tasks (or sometimes just for fun). For most part, other than bugs, as long as I tell\u2026"}, "outgoing_paragraph_urls": [{"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[A%20Beginner%E2%80%99s%20Guide%20to%20AI/ML%20%F0%9F%A4%96%F0%9F%91%B6%20%E2%80%93%20Machine%20Learning%20for%20Humans%20%E2%80%93%20Medium](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)", "anchor_text": "machine learning", "paragraph_index": 15}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Understanding%20Neurons%E2%80%99%20Role%20in%20the%20Nervous%20System](https://www.verywellmind.com/what-is-a-neuron-2794890)", "anchor_text": "neuron", "paragraph_index": 16}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Understanding%20Activation%20Functions%20in%20Neural%20Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)", "anchor_text": "activation function", "paragraph_index": 24}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Introduction%20to%20Linear%20Regression](http://onlinestatbook.com/2/regression/intro.html)", "anchor_text": "linear regression", "paragraph_index": 25}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Sigmoid%20function](https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Sigmoid_function.html)", "anchor_text": "sigmoid function", "paragraph_index": 26}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[How%20to%20Multiply%20Matrices](https://www.mathsisfun.com/algebra/matrix-multiplying.html)", "anchor_text": "matrix dot product", "paragraph_index": 33}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[An%20Introduction%20to%20Gradient%20Descent%20and%20Linear%20Regression](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)", "anchor_text": "gradient descent", "paragraph_index": 42}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Introduction%20to%20Derivatives](https://www.mathsisfun.com/calculus/derivatives-introduction.html)", "anchor_text": "differentiation", "paragraph_index": 43}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Chain%20rule%20-%20Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)", "anchor_text": "chain rule", "paragraph_index": 45}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[How%20to%20Compute%20the%20Derivative%20of%20a%20Sigmoid%20Function%20(fully%20worked%20example)%20-%20kawahara.ca](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/)", "anchor_text": "here", "paragraph_index": 53}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Neural%20networks%20and%20deep%20learning](http://neuralnetworksanddeeplearning.com/chap2.html)", "anchor_text": "back propagation", "paragraph_index": 58}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Setting%20the%20learning%20rate%20of%20your%20neural%20network.](https://www.jeremyjordan.me/nn-learning-rate/)", "anchor_text": "best learning rate", "paragraph_index": 65}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Make%20Your%20Own%20Neural%20Network:%20Bias%20Nodes%20in%20Neural%20Networks](http://makeyourownneuralnetwork.blogspot.sg/2016/06/bias-nodes-in-neural-networks.html)", "anchor_text": "bias", "paragraph_index": 67}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Gonum](https://www.gonum.org/)", "anchor_text": "Gonum", "paragraph_index": 72}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[MNIST%20in%20CSV](https://pjreddie.com/projects/mnist-in-csv/)", "anchor_text": "simpler CSV formatted datasets", "paragraph_index": 110}], "all_paragraphs": ["I have written a lot of computer programs in my career, most of the time to solve various problems or perform some tasks (or sometimes just for fun). For most part, other than bugs, as long as I tell the computer what to do very clearly (in whichever the programming language I use) it will obediently follow my instructions.", "This is because computer programs are really good at executing algorithms \u2014 instructions that follow defined steps and patterns that are precise and often repetitious. And in most cases they work well for us for tasks like number crunching or repetitious boring work.", "What computer programs are not so good at doing though, are tasks that are not so well defined, and doesn\u2019t follow precise patterns.", "So how can we use computers to do such tasks? Just think about how you do this task. You probably learned about birds when you\u2019re young, and you\u2019ve been told certain animals are birds and certain animals are not, mostly through seeing them either in real life or through picture books. When you get it wrong, you\u2019ll be told and you remember that. Over time you have a mental model of what\u2019s a bird and what\u2019s not. Every time you see something parts of a bird (clawed feet, feathered wings, sharp beak) you don\u2019t even need to see the whole animal anymore, you\u2019ll automatically identify it correctly by comparing it with your mental model.", "So how do we do this with computer programs? Basically we do the same thing. We try to create a model that we can use to compare inputs with, through a trial and error process. And since computer programs are all mathematics, you can guess that it\u2019s going to be a mathematical modelthat we\u2019re going to be talking about.", "Let\u2019s take a simple example create a black box that accepts an input and tries to predict the output.", "We feed it with an input and get the output from this predictor. Since we know what the actual output should be, we can tell how different the predicted output is from the actual output. This difference between the actual and the predicted output becomes the error.", "Of course, if the predictor is static and can\u2019t be changed, it\u2019s all pretty much moot. When we feed our predictor with an input, an output is produced with an error and that\u2019s the end of the story. Not very useful.", "To make our predictor more useful let\u2019s give it a configurable parameter that we can use to influence the output. Since it only predicts correctly if there is no error, we want to change the parameter such that the error shrinks as we keep on feeding the predictor with data. The aim is to get a predictor that predicts the correct output most of the time, without actually needing to give clear instructions to the predictor.", "In other words, this is very much like a numbers guessing game.", "Let\u2019s see this in a more practical way. Let\u2019s say we have a predictor with the simple mathematical formula o = i x c where o is the output, i is the input and c is configurable parameter.", "We are also given a confirmed valid output with a given input, that is, we know if i is 10, o is 26. How do we find c using the predictor?", "First, we need take a random prediction, let\u2019s say c is 2. Let\u2019s put in the input 10, and crank up the predictor. The output o is 20. Since the error e = t - o where t is the truth (or target), this means e = 26 - 20 = 6. Our error e is 6 and we want to achieve 0, so let\u2019s try again.", "Let\u2019s make c to be 3. The output is then 30 and is e is now -4. Oops, we overshot! Let\u2019s go back a bit and make c to be 2.5. That makes o to be 25, and e to be 1. Finally we try c to be 2.6 and we get the error e to be 0!", "Once we know what c is, we can use the predictor to predict the output for other inputs. Let\u2019s say the input i is now 20, then we can predict o to be 52.", "As you can see, this method tries to find answers iteratively and improving itself as it goes along, until we get the best fit. This in essence is what machine learning is. The computer program tries to find answers iteratively and \u2018learns\u2019 through its mistakes until it achieves a model that can produce the best answer. Once it has the correct model, we can use the model to correctly guess the answers. This is very similar to what we humans do (by learning from past mistakes and correcting ourselves) but how exactly do we do it?", "Let\u2019s step out a bit. We talked a bit about how a machine can possible learn using mathematical functions. How humans do the same thing (as research over the years have shown) is using something called a neuron.", "A neuron, or a nerve cell, is a cell that receives information, processes them and transmits through electrical and chemical signals. Our brain and spinal cord (part of what is called our central nervous system) consists of neurons.", "A neuron consists of a cell body, dendrites and an axon and can connected to each other to form neural networks. In a neural network, a neuron\u2019s axon is connected to the next neuron\u2019s dendrites and synaptic signals are transmitted from a neuron through its axon, and received by the next neuron through its dendrites. The connections between the axon and the dendrites is the synapse.", "The incoming signals through the dendrite are strengthened or weakened based on how often the synaptic connections are being used and these strengthened or weakened signals are pooled together in the cell body.", "If the pooled signals that are received are strong enough, it will trigger a new signal that is sent down the line through the axon to other neurons.", "As you can see, how a neuron work is somewhat analogous to our predictor earlier. It has a bunch of inputs through its dendrites which it processes and an output through its axon. Instead of a configurable parameter, each input is paired with the strength (or weight) of the synaptic connection.", "With this information, let\u2019s get back into our predictor and make some changes.", "We start off with building an artificial neuron that emulates the actual biological neuron. This artificial neuron is our upgraded predictor.", "Instead of a single input we have a bunch of inputs, each of the modified with a weight (in place of a configurable parameter). These modified inputs are summed up and passed through a triggering or activation function which determines if an output should be sent.", "So why an activation function (beyond the fact that a biological neuron behaves the same way)? There are a few good reasons but one the most important is that activation functions introduce non-linearity into the network. A neural network without activation functions (or a linear activation function) is basically just a linear regression model and is not able to do more complicated tasks such as language translations and image classifications. You will see later how non-linear activation functions enable back propagation.", "For now, we will assume the use of a common activation function, the sigmoid function.", "The interesting thing to note about this function is that the output is always within the range between 0 and 1 but never reaches either.", "Just like we had neurons forming neural networks, we can also connect our artificial neurons to form artificial neural networks.", "It seems quite a bit more complicated now!", "However we\u2019re simply stacking the neurons up in different layers. All the inputs come in through the input layer, which sends its output to the hidden layer, which in turn sends its outputs to the final output layer. While the output from each node is the same (there is only 1 output) but the connections to the neurons in the next layer are weighted differently. For example, the inputs to the first node in the hidden layer would be (w11 x i1) + (w21 x i2).", "Calculating the final outputs in this network can be a bit tedious if we have to do it one at a time, especially if we have a lot of neurons. Fortunately there\u2019s an easier way. If we represent the inputs as well as the weights as matrices, we can use the matrix operations to make the calculations easier. In fact we don\u2019t need to do individual neuron input summation and output activation anymore, we simply do it layer by layer.", "This will help a lot later in the code, as you will see.", "We used the matrix dot product to handle the multiplication and summation of the input and weights, but for the activation function we will need to apply the sigmoid function to each of the matrix elements. We\u2019ll have to do the same for every hidden layer, and also the output layer.", "You might realise at this point in time, our neural network is (conceptually) simply a larger version of the neuron, and therefore is very much like our predictor earlier. And just like our predictor we want to train our neural network to learn from its mistakes by passing it input with known output. Then using the difference (error) between the known and actual outputs, we change the weights to minimise the error.", "However you\u2019d probably realise that the neural network is quite a bit more complicated than our predictor. First, we have multiple neurons arranged in layers. As a result, while we know the final target output, we don\u2019t know the intermediate target outputs of the different layers in between. Secondly, we while our predictor is linear, our neurons are passed through a non-linear activation function so the output is not linear. So how do we change the weights of the different connections?", "We know from our predictor earlier, we are looking to minimise the final output error Ek by changing the various output weights that connect between the hidden layer to the output layer, wjk.", "That\u2019s very well and good but how do we minimise a value of a function by changing its input variable?", "Let\u2019s look at this from a different perspective. We know the final output error Ek is:", "However just subtracting ok from tk isn\u2019t a great idea because it will often result in negative numbers. If we are trying to find the final output error of the network, we\u2019re actually adding up all the errors, so if some of them are negative numbers it will result in the wrong final output error. A common solution is to use the squared error, which the name suggests is:", "At the same time we know:", "So we know (roughly speaking) if we map Ek with wjk we will get a range of values (blue line) plotted on a chart (actually this is a multi- dimensional chart but in order to keep our collective sanity, I\u2019m going to use a 2 dimensional chart):", "As you can see, to reach the minimal value of Ek we follow the gradient downwards or the negative gradient. In other words, we try to find the negative gradient, change the weight accordingly, then find negative gradient again, until we reach the minimal point of Ek. This algorithm is called gradient descent.", "You might remember from secondary school calculus, in order to find the gradient of a point in a function we use differentiation to get the derivative of the function. This allows us to find out how much we need to tweak wjk. To find the minimum value of Ek, we subtract this amount from wjkand we do this repeatedly.", "In order to calculate the change that we need for the output weights wjk we should compute the derivative of the final output error Ek with respect to the output weights wjk. This means:", "That\u2019s neat but how do we get our results using the other variables we have? To do that we need to use the chain rule:", "That looks slightly better, but we can go even one step further:", "Let\u2019s get to work. First, we need to find the derivative of Ek with respect to the final output ok.", "From earlier, we know Ek is the squared error:", "But to differentiate nicely we scale it down by half (I know it\u2019s a bit like cheating but it makes our lives easier):", "That\u2019s easy enough! Let\u2019s look at the derivative of the final output ok with respect to the summation of the product of the intermediate outputs and the weights, sumk. We know the summation is passed through a sigmoid function sig in order to get the final output ok:", "Therefore the derivative of the final output ok with respect to the summation sumkis:", "This is because we know the derivative of a sigmoid is:", "I mentioned earlier above that there are good reasons why we use a sigmoid function \u2014 easy differentiation is one of them! The proof of this can be found here. Now since:", "we can further simplify the equation to:", "Finally we want to find the derivative of the summation sumk with respect to the output weight wjk. We know the summation is the sum of the product of the output weight wjk and the previous output oj:", "So the derivative of the summation sumk with respect to the output weight wjk is:", "Now that we have all the 3 derivatives, let\u2019s put them together. Earlier, we said:", "With that we have the equation to change the weights for the output layer. What about the weights for the hidden layer(s)? We simply use the same equation but going backwards one layer. This algorithm is called back propagation because it calculates the weights backwards from the final output.", "But wait. We don\u2019t have the target output for the hidden layer. So how are we going to get the error for the hidden layer? We have to find another way.", "If you think about it, the error at the output layer is contributed by the errors from the hidden layer according to the connections from the previous hidden layer. In other words, the combination of the errors for the hidden layer forms the errors for the output layer. And since the weights represent the importance of the input, it also represents the contribution of the error.", "As as result, we can use the ratio of the weights to calculate the change to make for each weight. And because the denominator is constant, we can simplify this further by simply drop the denominators.", "Now let\u2019s see how we can back propagate the errors from the output layer using matrices.", "Once we have the errors from the hidden layer, we can use the same equation as before, but substituting the final output error with the hidden output error.", "So an artificial neural network learns through back propagation using gradient descent. During gradient descent iterations it\u2019s often easy to overshoot, which results in the moving too quickly and stepping over the minimal wjk. To prevent that we use a learning rate l to scale down the amount we want to change for the weights. This results in the change of our earlier equation:", "l is generally a small value so that we are more cautious about overshooting the minimum but we can\u2019t make it too small either, or else it will take too long to train. There is quite a bit of research literature on setting the best learning rate.", "With our current neural network, the activation function is a sigmoid that cuts though y at 0.5. Any changes to the weights simply changes the the steepness of the sigmoid. As a result there is a limitation to how the neuron is being triggered. For example, to make the sigmoid return a low value of 0.1 when x is 2 is going to be impossible.", "However if we add a bias value to x this changes things altogether.", "How we do this is by adding something called a bias neuron to the neural network. This bias neuron always outputs 1.0 and is added to a layer but doesn\u2019t have any have any input.", "Not all neural networks need bias neurons. In the simple neural network we\u2019re going to write later, we\u2019ll not be using any bias neurons (and it works pretty ok).", "So we\u2019re finally here! After all the concepts and maths, we\u2019re now going to start some implementation!", "The code snippets in this post is not complete so don\u2019t simply cut and paste from here to run it. All the code here can be found in this Github repository:", "At this point in time Go doesn\u2019t have a lot of support in terms of libraries for machine learning, unlike Python. However there is a very useful library called Gonum that provides what we need most \u2014 matrix manipulation.", "Also, while Gonum has perfectly fine packages, I thought some of the quirks in Gonum makes it unnecessarily verbose so I created my own helper functions to overcome it.", "We\u2019ll start with the helper functions first. Gonum\u2019s main package for matrix manipulation is called mat. What we\u2019ll be using are primarily going to be the mat.Matrix interface and its implementation mat.Dense.", "The mat package has a quirk that it requires us to create a new matrix with the exact correct rows and columns first, before we can execute the operations on the matrices. Doing so for multiple operations would be rather annoying so I wrapped around each function with my own.", "For example, the Gonum Product function allows us to perform the dot product operation on two matrices, and I created a helper function that finds out the size of the matrix, creates it and perform the operation before returning the resultant matrix.", "This helps to save around 1\u20133 lines of code or so, depending on the operation.", "The apply function allows us to apply a function to the matrix.", "The scale function allows us to scale a matrix i.e. multiply a matrix by a scalar.", "The multiply function multiplies 2 functions together (this is different from dot product`.", "The add and subtract functions allow to add or subtract a function to/from another.", "Finally the addScalar function allows us to add a scalar value to each element in the matrix.", "We\u2019ll be creating a very simple 3 layer feedforward neural network (also known as multi-layer perceptron). We start off with defining the network:", "The fields inputs, hiddens and output define the number of neurons in each of the input, hidden and output layers (remember, this is a 3 layer network). The hiddenWeights and outputWeights fields are matrices that represent the weights from the input to hidden layers, and the hidden to output layers respectively. Finally, the learningRate is, well, the learning rate for the network.", "Next we have a simple method to actually create the neural network.", "The number of input, hidden and output neurons as well as the learning rate is passed in from the caller to create the network. However the hidden and output weights are randomly created.", "If you remember from above, the weights we are creating is a matrix with the number of columns represented by from layer, and the number of rows represented by the to layer. This is because the number of rows in the weight must the same as the number of neurons in the to layer and the number of columns must be the same number of neurons as the from layer (in order to multiply with the outputs of the from layer). Take a while to look at the diagrams below again \u2014 it will make more sense.", "Initializing the weights with a random set of numbers is one of the important parameters. For this we\u2019re going to use a function randomArray to create this random array of float64.", "The randomArray function uses the distuv package in Gonum to create a uniformly distributed set of values between the range of -1/sqrt(v) and 1/sqrt(v) where v is the size of the fromlayer. This is quite a commonly used distribution.", "Now that we have our neural network, the two main functions we can ask it to do is to either train itself with a set of training data, or predict values given a set of test data.", "From our hard work earlier on, we know that prediction means forward propagation through the network while training means forward propagation first, then back propagation later on to change the weights using some training data.", "Since both training and prediction requires forward propagation, let\u2019s start with that first. We define a function called Predict to predict the values using the trained neural network.", "We start off with the inputs first, by creating a matrix called inputs to represent the input values. Next we find the inputs to hidden layer by applying the dot product between the hidden weights and the inputs, creating a matrix called hiddenInputs. In other words, given a 2 neuron input layer and a 3 neuron hidden layer, this is what we get:", "Next, we apply our activation function, sigmoid on the hidden inputs to produce hiddenOutputs.", "We repeat these 2 actions for final inputs and final outputs to produce finalInputs and finalOutputs respectively and the prediction is the final outputs.", "So that\u2019s how we predict using the forward propagation algorithm. Let\u2019s see how we do forward and back propagation in training.", "The forward propagation part is exactly the same as in the Predict function. We are not calling Predict here though because we still need the other intermediary values.", "The first thing we need to do after getting the final outputs is to determine the output errors. This is relatively simple, we simply subtract our target data from the final outputs to get outputErrors:", "The hidden errors from the hidden layer is a bit trickier. Remember this?", "We use back propagation to calculate the hidden errors by applying the dot product on the transpose of the output weights and the output errors. This will give us hiddenErrors.", "Now that we have the errors, we simply use the formula we derived earlier (including the learning rate) for changes to the weights we need to do:", "Remember that we are subtracting this number from the weights. Since this is a negative number, we end up adding this to the weights, which is what we did.", "To simplify the calculations we use a sigmoidPrime function, which is nothing more than doing sigP = sig(1 - sig):", "Also you might see that we\u2019re doing the dot product of the transpose of the previous output \u2014 this is because we\u2019re multiplying across layers.", "Finally we do this twice to get the new hidden and output weights for our neural network.", "And that\u2019s a wrap for the Train function.", "Before we move on to using the neural network, we\u2019ll see how we can save our training results and load it up for use later. We certainly don\u2019t want to train from scratch each time we want to do the prediction \u2014 training the network can often take quite a while.", "The save and load functions are mirror images of each other and we use a convenient function from the Gonum mat package to marshal the weight matrices into binary form and unmarshal the same form back to matrices. This is pretty mundane \u2014 the only thing of note is that when we unmarshal from the binary data back to the weight matrices, we need to first reset the matrices back to zero-value so that it can be reused.", "We\u2019re finally here \u2014 using the neural network!", "Let\u2019s start with a \u2018hello world\u2019 of machine learning \u2014 using the MNIST dataset to recognise handwritten numeric digits. The MNIST dataset is a set of 60,000 scanned handwritten digit images used for training and 10,000 similar images used for testing. It\u2019s a subset of a larger set from NIST (National Institute of Standards and Technology) that has been size-normalised and centered. The images are in black and white and are 28 x 28 pixels. The original dataset are stored in a format is that more difficult to work with, so people have come up with simpler CSV formatted datasets, which is what we\u2019re using.", "In the CSV format every line is an image, and each column except the first represents a pixel. The first column is the label, which is the actual digit that the image is supposed to represent. In other words, this is the target output. Since there are 28 x 28 pixels, this means there are 785 columns in every row.", "Let\u2019s start with the training. We create a function called mnistTrain that takes in a neural network and use it for training the MNIST dataset:", "We open up the CSV file and read each record, then process each record. For every record we read in we create an array that represents the inputs and an array that represents the targets.", "For the inputs array we take each pixel from the record, and convert it to a value between 0.0 and 1.0 with 0.0 meaning a pixel with no value and 1.0 meaning a full pixel.", "For the targets array, each element of the array represents the probability of the index being the target digit. For example, if the target digit is 3, then the 4th element targets[3] would have a probability of 0.99 while the rest would have a probability of 0.01.", "Once we have the inputs and targets, we call the Train function of the network and pass it the inputs and targets.", "You might notice that we ran this in \u2018epochs\u2019. Basically what we did was to run this multiple times because the more times we run through the training the better trained the neural network will be. However if we over-train it, the network will overfit, meaning it will adapt too well with the training data and will ultimately perform badly with data that it hasn\u2019t seen before.", "Predicting the hand-written images is basically the same thing, except that we call the Predictfunction with only the inputs.", "The results that we get is an array of probabilities. We figure out the element with the highest probability and the digit should be the index of that element. If it is, we count that as a win. The final count of the wins is our final score.", "Because we have 10,000 test images, if we manage to detect all of them accurately then we have will 100% accuracy. Let\u2019s look at the main function:", "This is pretty straightforward, we first create a neural network with 784 neurons in the input layer (each pixel is one input), 200 neurons in the hidden layer and 10 neurons in the output layer, one for each digit.", "Then we train the network with the MNIST training dataset, and the predict the images with the testing dataset. This is what I have when I test it:", "Now that we have tested our network, let\u2019s see how to use it on individual images.", "First we get the data from the PNG file. To do this, we create a dataFromImage function.", "Each pixel in the image represents an value but we can\u2019t use the normal RGBA, instead we need an image.Gray . From the image.Gray struct we get the Pix value and translate it into a float64value instead. The MNIST image is white on black, so we need to subtract each pixel value from 255.", "Once we have the pixel array, it\u2019s quite straightforward. We use a predictFromImage function that takes in the neural network and predicts the digit from an image file. The results are an array of probabilities where the index is the digit. What we need to do is to find the index and return it.", "Finally from the main function we print the image and predict the digit from the image.", "Assuming the network has already been trained, this is what we get.", "And that\u2019s it, we have written a simple 3-layer feedforward neural network from scratch using Go!", "Here are some of the references for I took when writing this post and the code.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac2e8c49ae37&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@sausheong", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sausheong.com/?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": ""}, {"url": "https://sausheong.com/?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "Sau Sheong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa358058b2410&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&user=Sau+Sheong&userId=a358058b2410&source=post_page-a358058b2410----ac2e8c49ae37---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac2e8c49ae37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac2e8c49ae37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://commons.wikimedia.org/wiki/File:Eniac.jpg", "anchor_text": "https://commons.wikimedia.org/wiki/File:Eniac.jpg"}, {"url": "https://www.explainxkcd.com/wiki/index.php/1425:_Tasks", "anchor_text": "Explain XKCD \u2014 1425: Tasks"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[A%20Beginner%E2%80%99s%20Guide%20to%20AI/ML%20%F0%9F%A4%96%F0%9F%91%B6%20%E2%80%93%20Machine%20Learning%20for%20Humans%20%E2%80%93%20Medium](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)", "anchor_text": "machine learning"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Understanding%20Neurons%E2%80%99%20Role%20in%20the%20Nervous%20System](https://www.verywellmind.com/what-is-a-neuron-2794890)", "anchor_text": "neuron"}, {"url": "https://commons.wikimedia.org/wiki/File:PurkinjeCell.jpg", "anchor_text": "https://commons.wikimedia.org/wiki/File:PurkinjeCell.jpg"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Understanding%20Activation%20Functions%20in%20Neural%20Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)", "anchor_text": "activation function"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Introduction%20to%20Linear%20Regression](http://onlinestatbook.com/2/regression/intro.html)", "anchor_text": "linear regression"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Sigmoid%20function](https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Sigmoid_function.html)", "anchor_text": "sigmoid function"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[How%20to%20Multiply%20Matrices](https://www.mathsisfun.com/algebra/matrix-multiplying.html)", "anchor_text": "matrix dot product"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[An%20Introduction%20to%20Gradient%20Descent%20and%20Linear%20Regression](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)", "anchor_text": "gradient descent"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Introduction%20to%20Derivatives](https://www.mathsisfun.com/calculus/derivatives-introduction.html)", "anchor_text": "differentiation"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Chain%20rule%20-%20Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)", "anchor_text": "chain rule"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[How%20to%20Compute%20the%20Derivative%20of%20a%20Sigmoid%20Function%20(fully%20worked%20example)%20-%20kawahara.ca](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/)", "anchor_text": "here"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Neural%20networks%20and%20deep%20learning](http://neuralnetworksanddeeplearning.com/chap2.html)", "anchor_text": "back propagation"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Setting%20the%20learning%20rate%20of%20your%20neural%20network.](https://www.jeremyjordan.me/nn-learning-rate/)", "anchor_text": "best learning rate"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Make%20Your%20Own%20Neural%20Network:%20Bias%20Nodes%20in%20Neural%20Networks](http://makeyourownneuralnetwork.blogspot.sg/2016/06/bias-nodes-in-neural-networks.html)", "anchor_text": "bias"}, {"url": "https://github.com/sausheong/gonn", "anchor_text": "https://github.com/sausheong/gonn"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[Gonum](https://www.gonum.org/)", "anchor_text": "Gonum"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/[MNIST%20in%20CSV](https://pjreddie.com/projects/mnist-in-csv/)", "anchor_text": "simpler CSV formatted datasets"}, {"url": "https://www.amazon.com/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G", "anchor_text": "Make Your Own Neural Network"}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "Neural Networks and Deep Learning"}, {"url": "http://www.datadan.io/building-a-neural-net-from-scratch-in-go/", "anchor_text": "Building a Neural Net from Scratch in Go"}, {"url": "https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/", "anchor_text": "introductory post on neural networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ac2e8c49ae37---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----ac2e8c49ae37---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/go?source=post_page-----ac2e8c49ae37---------------go-----------------", "anchor_text": "Go"}, {"url": "https://medium.com/tag/golang?source=post_page-----ac2e8c49ae37---------------golang-----------------", "anchor_text": "Golang"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ac2e8c49ae37---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac2e8c49ae37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&user=Sau+Sheong&userId=a358058b2410&source=-----ac2e8c49ae37---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac2e8c49ae37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&user=Sau+Sheong&userId=a358058b2410&source=-----ac2e8c49ae37---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac2e8c49ae37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fac2e8c49ae37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ac2e8c49ae37---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ac2e8c49ae37--------------------------------", "anchor_text": ""}, {"url": "https://sausheong.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sausheong.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sau Sheong"}, {"url": "https://sausheong.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa358058b2410&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&user=Sau+Sheong&userId=a358058b2410&source=post_page-a358058b2410--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb7ba9640486a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37&newsletterV3=a358058b2410&newsletterV3Id=b7ba9640486a&user=Sau+Sheong&userId=a358058b2410&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.manning.com/books/go-web-programming", "anchor_text": "Go Web Programming2016"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}