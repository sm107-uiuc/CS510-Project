{"url": "https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f", "time": 1683016656.9843352, "path": "towardsdatascience.com/deep-learning-optimizers-436171c9e23f/", "webpage": {"metadata": {"title": "Deep Learning Optimizers. SGD with momentum, Adagrad, Adadelta\u2026 | by Gunand Mayanglambam | Towards Data Science", "h1": "Deep Learning Optimizers", "description": "This blog post explores how the advanced optimization technique works. We will be learning the mathematical intuition behind the optimizer like SGD with momentum, Adagrad, Adadelta, and Adam\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/analytics-vidhya/implementing-gradient-descent-for-multi-linear-regression-from-scratch-3e31c114ae12", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://github.com/GUNAND12/multi_linear-Gradient-descent", "anchor_text": "Github", "paragraph_index": 4}, {"url": "https://golden.com/wiki/Adadelta", "anchor_text": "Adadelta", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?v=lAq96T8FkTw&t=257s", "anchor_text": "video", "paragraph_index": 22}, {"url": "https://ruder.io/optimizing-gradient-descent/", "anchor_text": "An overview of gradient descent optimization algorithms", "paragraph_index": 27}], "all_paragraphs": ["This blog post explores how the advanced optimization technique works. We will be learning the mathematical intuition behind the optimizer like SGD with momentum, Adagrad, Adadelta, and Adam optimizer.", "In this post, I am assuming that you have prior knowledge of how the base optimizer like Gradient Descent, Stochastic Gradient Descent, and mini-batch GD works. If not, you can check out my previous article here.", "Drawbacks of base optimizer:(GD, SGD, mini-batch GD)", "The above figure is the plot between the number of epoch on the x-axis and the loss on the y-axis. We can clearly see that in Gradient Descent the loss is reduced smoothly whereas in SGD there is a high oscillation in loss value.", "You can see the code implementation for the above plot on my Github.", "Before learning some of the faster optimization algorithms. Let\u2019s first try to learn \u201cExponentially Weighted Averages\u201d.", "Exponentially Weighted Averages is used in sequential noisy data to reduce the noise and smoothen the data. To denoise the data, we can use the following equation to generate a new sequence of data with less noise.", "Now, let\u2019s see how the new sequence is generated using the above equation: For our example to make it simple, let\u2019s consider a sequence of size 3.", "From the above equation, at time step t=3 more weightage is given to a3(which is the latest generated data) then followed by a2 previously generated data, and so on. This is how the sequence of noisy data is smoothened. It works better in a long sequence because, in the initial period, the averaging effect is less due to fewer data points.", "It always works better than the normal Stochastic Gradient Descent Algorithm. The problem with SGD is that while it tries to reach minima because of the high oscillation we can\u2019t increase the learning rate. So it takes time to converge. In this algorithm, we will be using Exponentially Weighted Averages to compute Gradient and used this Gradient to update parameter.", "In SGD with momentum, we have added momentum in a gradient function. By this I mean the present Gradient is dependent on its previous Gradient and so on. This accelerates SGD to converge faster and reduce the oscillation.", "The above picture shows how the convergence happens in SGD with momentum vs SGD without momentum.", "Whatever the optimizer we learned till SGD with momentum, the learning rate remains constant. In Adagrad optimizer, there is no momentum concept so, it is much simpler compared to SGD with momentum.", "The idea behind Adagrad is to use different learning rates for each parameter base on iteration. The reason behind the need for different learning rates is that the learning rate for sparse features parameters needs to be higher compare to the dense features parameter because the frequency of occurrence of sparse features is lower.", "In the above Adagrad optimizer equation, the learning rate has been modified in such a way that it will automatically decrease because the summation of the previous gradient square will always keep on increasing after every time step. Now, let\u2019s take a simple example to check how the learning rate is different for every parameter in a single time step. For this example, we will consider a single neuron with 2 inputs and 1 output. So, the total number of parameters will be 3 including bias.", "The above computation is done at a single time step, where all the three parameters learning rate \u201c\u03b7\u201d is divided by the square root of \u201c\u03b1\u201d which is different for all parameters. So, we can see that the learning rate is different for all three parameters.", "Now, let\u2019s see how weights and bias are updated in Stochastic Gradient Descent.", "Similarly, the above computation is done at a single time step, and here the learning rate \u201c\u03b7\u201d remains the same for all parameters.", "Lastly, despite not having to manually tune the learning rate there is one huge disadvantage i.e due to monotonically decreasing learning rates, at some point in time step, the model will stop learning as the learning rate is almost close to 0.", "Adadelta is an extension of Adagrad that attempts to solve its radically diminishing learning rates. The idea behind Adadelta is that instead of summing up all the past squared gradients from 1 to \u201ct\u201d time steps, what if we could restrict the window size. For example, computing the squared gradient of the past 10 gradients and average out. This can be achieved using Exponentially Weighted Averages over Gradient.", "The above equation shows that as the time steps \u201ct\u201d increase the summation of squared gradients \u201c\u03b1\u201d increases which led to a decrease in learning rate \u201c\u03b7\u201d. In order to resolve the exponential increase in the summation of squared gradients \u201c\u03b1\u201d, we replaced the \u201c\u03b1\u201d with exponentially weighted averages of squared gradients.", "So, here unlike the alpha \u201c\u03b1\u201d in Adagrad, where it increases exponentially after every time step. In Adadelda, using the exponentially weighted averages over the past Gradient, an increase in \u201cSdw\u201d is under control. The calculation for \u201cSdw\u201d is similar to the example I did in the Exponentially Weighted Averages section.", "The typical \u201c\u03b2\u201d value is 0.9 or 0.95. To learn more about the \u201c\u03b2\u201d value, you can watch this video.", "Adam optimizer is by far one of the most preferred optimizers. The idea behind Adam optimizer is to utilize the momentum concept from \u201cSGD with momentum\u201d and adaptive learning rate from \u201cAda delta\u201d.", "Using the above equation, now the weight and bias updation formula looks like:", "There is something called bias correction while using Exponential Weighted Averages. Bias correction is used to get a better estimate in the initial time steps. But often most people don\u2019t bother to implement bias correction because most people would rather wait for an initial period and then after some time steps the bias error will become insignificant.", "With that, I hope you have got some basic understanding of all the optimizers we have discussed so far. If any suggestions or feedback, please leave a comment down below. Thank you for reading my blog!", "1.Sebastian Ruder: An overview of gradient descent optimization algorithms", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F436171c9e23f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----436171c9e23f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----436171c9e23f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://gndjel3043.medium.com/?source=post_page-----436171c9e23f--------------------------------", "anchor_text": ""}, {"url": "https://gndjel3043.medium.com/?source=post_page-----436171c9e23f--------------------------------", "anchor_text": "Gunand Mayanglambam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1ff0b04c4430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&user=Gunand+Mayanglambam&userId=1ff0b04c4430&source=post_page-1ff0b04c4430----436171c9e23f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F436171c9e23f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F436171c9e23f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@varunnambiar?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Varun Nambiar"}, {"url": "https://unsplash.com/s/photos/hills-and-valley?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/analytics-vidhya/implementing-gradient-descent-for-multi-linear-regression-from-scratch-3e31c114ae12", "anchor_text": "here"}, {"url": "https://github.com/GUNAND12/multi_linear-Gradient-descent", "anchor_text": "Github"}, {"url": "https://ruder.io/optimizing-gradient-descent/index.html#momentum", "anchor_text": "Sebastian Ruder"}, {"url": "https://golden.com/wiki/Adadelta", "anchor_text": "Adadelta"}, {"url": "https://www.youtube.com/watch?v=lAq96T8FkTw&t=257s", "anchor_text": "video"}, {"url": "https://arxiv.org/pdf/1412.6980.pdf", "anchor_text": "Advantage of using ADAM optimizer:"}, {"url": "https://www.linkedin.com/in/gunand-mayanglambam-98727b141/", "anchor_text": "LinkedIn"}, {"url": "https://ruder.io/optimizing-gradient-descent/", "anchor_text": "An overview of gradient descent optimization algorithms"}, {"url": "https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w", "anchor_text": "Deeplearning.ai"}, {"url": "https://www.youtube.com/watch?v=TudQZtgpoHk", "anchor_text": "Youtube channel"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----436171c9e23f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimizer?source=post_page-----436171c9e23f---------------optimizer-----------------", "anchor_text": "Optimizer"}, {"url": "https://medium.com/tag/adagrad?source=post_page-----436171c9e23f---------------adagrad-----------------", "anchor_text": "Adagrad"}, {"url": "https://medium.com/tag/adam?source=post_page-----436171c9e23f---------------adam-----------------", "anchor_text": "Adam"}, {"url": "https://medium.com/tag/stochastic-gradient?source=post_page-----436171c9e23f---------------stochastic_gradient-----------------", "anchor_text": "Stochastic Gradient"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F436171c9e23f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&user=Gunand+Mayanglambam&userId=1ff0b04c4430&source=-----436171c9e23f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F436171c9e23f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&user=Gunand+Mayanglambam&userId=1ff0b04c4430&source=-----436171c9e23f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F436171c9e23f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----436171c9e23f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F436171c9e23f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----436171c9e23f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----436171c9e23f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----436171c9e23f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----436171c9e23f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----436171c9e23f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----436171c9e23f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----436171c9e23f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----436171c9e23f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----436171c9e23f--------------------------------", "anchor_text": ""}, {"url": "https://gndjel3043.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://gndjel3043.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gunand Mayanglambam"}, {"url": "https://gndjel3043.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "51 Followers"}, {"url": "http://Openstream.ai", "anchor_text": "Openstream.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1ff0b04c4430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&user=Gunand+Mayanglambam&userId=1ff0b04c4430&source=post_page-1ff0b04c4430--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa05026476d83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-optimizers-436171c9e23f&newsletterV3=1ff0b04c4430&newsletterV3Id=a05026476d83&user=Gunand+Mayanglambam&userId=1ff0b04c4430&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}