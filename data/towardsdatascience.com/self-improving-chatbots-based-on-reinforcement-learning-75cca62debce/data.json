{"url": "https://towardsdatascience.com/self-improving-chatbots-based-on-reinforcement-learning-75cca62debce", "time": 1683013861.024632, "path": "towardsdatascience.com/self-improving-chatbots-based-on-reinforcement-learning-75cca62debce/", "webpage": {"metadata": {"title": "Self-improving Chatbots based on Deep Reinforcement Learning | by Debmalya Biswas | Towards Data Science", "h1": "Self-improving Chatbots based on Deep Reinforcement Learning", "description": "Reinforcement Learning (RL) model for self-improving chatbots, esp. FAQ-type chatbots. The dialog system leverages user conversations to improve chatbot performance autonomously."}, "outgoing_paragraph_urls": [{"url": "http://rldm.org/", "anchor_text": "RLDM", "paragraph_index": 1}, {"url": "https://www.researchgate.net/publication/333203489_Self-improving_Chatbots_based_on_Reinforcement_Learning", "anchor_text": "Paper", "paragraph_index": 1}, {"url": "https://github.com/debmalyabiswas1/ReinforcementLearning_Chatbots", "anchor_text": "Code", "paragraph_index": 1}, {"url": "https://rasa.com/", "anchor_text": "Rasa", "paragraph_index": 8}, {"url": "https://www.tensorflow.org/hub", "anchor_text": "TensorFlow", "paragraph_index": 8}], "all_paragraphs": ["Abstract. We present a Reinforcement Learning (RL) model for self-improving chatbots, specifically targeting FAQ-type chatbots. The model is not aimed at building a dialog system from scratch, but to leverage data from user conversations to improve chatbot performance. At the core of our approach is a score model, which is trained to score chatbot utterance-response tuples based on user feedback. The scores predicted by this model are used as rewards for the RL agent. Policy learning takes place offline, thanks to an user simulator which is fed with utterances from the FAQ-database. Policy learning is implemented using a Deep Q-Network (DQN) agent with epsilon-greedy exploration, which is tailored to effectively include fallback answers for out-of-scope questions. The potential of our approach is shown on a small case extracted from an enterprise chatbot. It shows an increase in performance from an initial 50% success rate to 75% in 20\u201330 training epochs.", "The published version of the paper is available below, in proceedings of the 4th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM), Montreal, 2019 (Paper) (Code)", "The majority of dialog agents in an enterprise setting are domain specific, consisting of a Natural Language Understanding (NLU) unit trained to recognize the user\u2019s goal in a supervised manner. However, collecting a good training set for a production system is a time-consuming and cumbersome process. Chatbots covering a wide range of intents often face poor performance due to intent overlap and confusion. Furthermore, it is difficult to autonomously retrain a chatbot taking into account the user feedback from live usage or testing phase. Self-improving chatbots are challenging to achieve, primarily because of the difficulty in choosing and prioritizing metrics for chatbot performance evaluation. Ideally, one wants a dialog agent to be capable to learn from the user\u2019s experience and improve autonomously.", "In this work, we present a reinforcement learning approach for self-improving chatbots, specifically targeting FAQ-type chatbots. The core of such chatbots is an intent recognition NLU, which is trained with hard-coded examples of question variations. When no intent is matched with a confidence level above 30%, the chatbot returns a fallback answer. For all others, the NLU engine returns the corresponding confidence level along with the response.", "Several research papers [2, 3, 7, 8] have shown the effectiveness of a RL approach in developing dialog systems. Critical to this approach is the choice of a good reward model. A typical reward model is the implementation of a penalty term for each dialog turn. However, such rewards only apply to task completion chatbots where the purpose of the agent is to satisfy user\u2019s request in the shortest time, but it is not suitable for FAQ-type chatbots where the chatbot is expected to provide a good answer in one turn. The user\u2019s feedback can also be used as a reward model in an online reinforcement learning. However, applying RL on live conversations can be challenging and it may incur a significant cost in case of RL failure.", "A better approach for deployed systems is to perform the RL training offline and then update the NLU policy once satisfactory levels of performance have been reached.", "The RL model architecture is illustrated in Figure 1. The various components of the model are: the NLU unit, which is used to initially train the RL agent in a warm-up phase; the user simulator, which randomly extracts the user utterances from the database of user experiences; the score model trained on the user\u2019s conversation with feedback and the RL agent based on a Deep Q-Network (DQN) network.", "We apply the reinforcement learning approach on a FAQ-type chatbot.", "At the core of the chatbot, there is an intent recognition NLU, which is trained with hard-coded examples of question variations. An intent is defined as the user\u2019s intention, which is formulated through the utterance. For this work, we have chosen the open-source NLU from Rasa, using the TensorFlow pipeline. However the RL approach is independent from the NLU chosen and for systems in production it can easily be extended to NLU engines such as IBM Watson or Amazon LEX.", "We used user feedback obtained during the development an actual internal chatbot for our work.", "The scope of the chatbot was to answer employee queries related to office building facilities, HR policies and benefits, etc.", "All the 10 users participating in the test phase were informed that their feedback would be used to improve the chatbot performance. The testers provided a (binary) feedback after each conversation turn, thus rating the utterance-response tuples. The historical data thus contains quadruples of the following format: (utterance, response, NLU confidence level and feedback). By removing non valid conversations (i.e. those lacking or with invalid feedback) we end up with 550 user conversations, triggering about 120 intents. Although we have tested the score model on all the conversations, the RL model has been applied only on a subsample of 155 conversations, triggering the top 5 intents. On this subset, the user\u2019s satisfaction is 50%.", "Evaluating chatbot performance is a long-standing issue in computational linguistic. Automatic metrics borrowed from machine translations (e.g. [6]) do not perform well on short sentences (e.g. [4]), such as the chatbot utterance-response tuples. On the other hand, human rating of chatbots is by now the de-facto standard to evaluate the success of a chatbot, although those ratings are often difficult and expensive to gather.", "To evaluate the correctness of chatbot responses, we propose a new approach which makes use of the user conversation logs, gathered during the development and testing phases of the chatbot. Each user had been asked to provide a binary feedback (positive/negative) at each chatbot turn.", "In order to use the user feedback in an offline reinforcement learning, we have developed a score model, capable of modeling the binary feedback for unseen utterance-response tuples. In a supervised fashion, the score model learns how to project the vector representations of utterance and response in a linearly transformed space, such that similar vector representations give high score. As for the vector representation of sentences, we compute sentence embedding through the universal sentence encoder [1], available through TensorFlow Hub. To train the model, the optimization is done on a squared error (between model prediction and human feedback) loss with L2 regulation. To evaluate the model, the predicted scores are then converted into a binary outcome and compared with the targets (the user feedbacks). For those couples of utterances having a recognized intent with both a positive feedback and a NLU confidence level close to 1, we perform data augmentation, assigning low scores to the combination of utterance and fallback intent.", "A similar approach for chatbot evaluation has been suggested by [4]. The authors model the scores by using a labelled set of conversations, that also include model and human-generated responses, collected through crowdsourcing.", "Our approach differs from the above authors in that it just requires a labelled set of utterance-response tuples, which are relatively straightforward to gather during the chatbot development and user testing phases.", "To learn the policy, the RL agent uses a Q-learning algorithm with DQN architecture [5]. In DQN, a neural network is trained to approximate the state-action function Q(s_t, a_t, \u03b8), which represents the quality of an action a_t provided at state s_t, and \u03b8 are the trainable parameters. As for the DQN network, we have followed the approach proposed by [3], using a fully-connected network, fed by an experience replay pool buffer, that contains the one-hot representation of utterance and response and the corresponding reward. A one-hot representation is possible in this case as we have a finite possible values for utterances (given by the number of real users\u2019s question in the logs) and responses (equal to the number of intents used on out test-case, 5). In a warm-up phase, the DQN is trained on the NLU, using as a reward the NLU confidence level. The DQN training set is augmented whenever a state-action pair has a confidence above a threshold, by assigning zero weight to the given state and all the other available actions. Thus, at the starting of the RL training, the agent performs similar to the NLU unit.", "During RL training, we use an epsilon \u03b5-greedy exploration, where random actions are explored according to a probability \u03b5. We use a time-varying \u03b5 which facilitates the exploration at the beginning of the training with e_t0 = 0.2 and \u03b5_t = 0.05 during the last epoch. To speed-up the learning when picking random actions, we also force higher probability to get a \u201cNo intent detected\u201d, as several questions are actually out of the chatbot scope, but they are erroneously matched to a wrong intent by the NLU. During an epoch we simulate a batch of conversations of size n-episodes (ranging from 10 to 30 in our experiments) and fill an experience replay buffer with the tuple (s_t, a_t, r_t). The buffer has fixed size and it is flushed the first time when the agent performance increases above a specified threshold. In those episodes where the state-action tuple gets a reward greater than 50%, we perform data augmentation by assigning zero reward to the assignment of any other action to the current state.", "To evaluate the model, we select subsets of conversations, triggering the top N intents, with N between 5 and 50. The results of the score model are summarized in Figure 2, showing the cross-validated (5-fold CV) accuracy on the test set and the ROC curve as a function of the number of intents.", "For the whole sample of conversations, we obtain a cross-validated accuracy of 75% and an AUC of 0.84.", "However, by selecting only those conversations triggering the top 5 intents, thus including more examples per intent, we obtain an accuracy of 86% and an AUC of 0.94. For the RL model evaluation, we have focussed on the 5 intents subsets; which ensures that we have the most reliable rewards.", "The learning curve for the RL training is shown in Figure 3. In the left-hand panel, we compare the RL training with the reward model with a test done with a direct reward (in interactive way), showing that the score model is giving similar performances to the reference case, where the reward is known. Large fluctuations in the average score are due to a limited batch size (n-episodes = 10) and a relatively large \u03b5. We also show the success rate on a test set of 20 conversations, extracted from the full sample, where a \u201cgolden response\u201d is manually provided for all the utterances.", "The agent success rate increases from an initial 50% to 75% in only 30 epochs, showing the potential of this approach.", "In the right-hand panel, we show the results using n-episodes = 30, showing similar performances but with a smoother learning curve.", "In this work, we have shown the potential of a reinforcement learning approach in improving the performance of FAQ-type chatbots, based on the feedback from a user testing phase. To achieve this, we have developed a score model, which is able to predict the user\u2019s satisfaction on utterance-response tuples, and implemented a DQN reinforcement model, using the score model predictions as rewards. We have evaluated the model on a small, but real, test case, demonstrating promising results. Further training on more epochs and including more data, as well as extensive tests on the model hyper-parameters are in progress. The value of our approach is in providing a practical tool to improve large-scale chatbots (with a large set of diverse intents), in an automated fashion based on user feedback.", "Finally, we notice that although the reinforcement learning model presented in this work is suitable for FAQ-type chatbots, it can be generalised to include the sequential nature of conversation by incorporating a more complex score model.", "[2] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.", "[3] Xiujun Li, Yun-Nung Chen, Jianfeng Gao, and Asli Celikyilmaz. End-to-end task-completion neural dialogue systems. In 8th International Joint Conference on Natural Language Processing, 2017.", "[4] Ryan Lowe, et. al. Towards an automatic turing test: Learning to evaluate dialogue responses. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1116\u20131126. Association for Computational Linguistics, 2017.", "[6] Kishore Papineni, Salim Roukos, ToddWard, andWei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, 2002.", "[7] Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, and Kam-Fai Wong. Deep dyna-q: Integrating planning for task-completion dialogue policy learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2182\u20132192. Association for Computational Linguistics, 2018."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F75cca62debce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@debmalyabiswas", "anchor_text": "Mastodon"}, {"url": "https://debmalyabiswas.medium.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Debmalya Biswas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad84805121fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&user=Debmalya+Biswas&userId=ad84805121fe&source=post_page-ad84805121fe----75cca62debce---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75cca62debce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&user=Debmalya+Biswas&userId=ad84805121fe&source=-----75cca62debce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75cca62debce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&source=-----75cca62debce---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "http://rldm.org/", "anchor_text": "RLDM"}, {"url": "https://www.researchgate.net/publication/333203489_Self-improving_Chatbots_based_on_Reinforcement_Learning", "anchor_text": "Paper"}, {"url": "https://github.com/debmalyabiswas1/ReinforcementLearning_Chatbots", "anchor_text": "Code"}, {"url": "https://rasa.com/", "anchor_text": "Rasa"}, {"url": "https://www.tensorflow.org/hub", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/chatbots?source=post_page-----75cca62debce---------------chatbots-----------------", "anchor_text": "Chatbots"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----75cca62debce---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----75cca62debce---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----75cca62debce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----75cca62debce---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75cca62debce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&user=Debmalya+Biswas&userId=ad84805121fe&source=-----75cca62debce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75cca62debce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&user=Debmalya+Biswas&userId=ad84805121fe&source=-----75cca62debce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75cca62debce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad84805121fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&user=Debmalya+Biswas&userId=ad84805121fe&source=post_page-ad84805121fe----75cca62debce---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1a52f34e1d70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&newsletterV3=ad84805121fe&newsletterV3Id=1a52f34e1d70&user=Debmalya+Biswas&userId=ad84805121fe&source=-----75cca62debce---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Written by Debmalya Biswas"}, {"url": "https://debmalyabiswas.medium.com/followers?source=post_page-----75cca62debce--------------------------------", "anchor_text": "237 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://www.linkedin.com/in/debmalya-biswas-3975261/", "anchor_text": "https://www.linkedin.com/in/debmalya-biswas-3975261/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad84805121fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&user=Debmalya+Biswas&userId=ad84805121fe&source=post_page-ad84805121fe----75cca62debce---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1a52f34e1d70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-improving-chatbots-based-on-reinforcement-learning-75cca62debce&newsletterV3=ad84805121fe&newsletterV3Id=1a52f34e1d70&user=Debmalya+Biswas&userId=ad84805121fe&source=-----75cca62debce---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/conversational-bi-text-to-sql-c9f52a89acc5?source=author_recirc-----75cca62debce----0---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=author_recirc-----75cca62debce----0---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=author_recirc-----75cca62debce----0---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Debmalya Biswas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----75cca62debce----0---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/conversational-bi-text-to-sql-c9f52a89acc5?source=author_recirc-----75cca62debce----0---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Conversational BI: Text to SQLThe art of querying SQL Databases in Natural Language"}, {"url": "https://towardsdatascience.com/conversational-bi-text-to-sql-c9f52a89acc5?source=author_recirc-----75cca62debce----0---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "\u00b78 min read\u00b7Aug 8, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9f52a89acc5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconversational-bi-text-to-sql-c9f52a89acc5&user=Debmalya+Biswas&userId=ad84805121fe&source=-----c9f52a89acc5----0-----------------clap_footer----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/conversational-bi-text-to-sql-c9f52a89acc5?source=author_recirc-----75cca62debce----0---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9f52a89acc5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconversational-bi-text-to-sql-c9f52a89acc5&source=-----75cca62debce----0-----------------bookmark_preview----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----75cca62debce----1---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----75cca62debce----1---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----75cca62debce----1---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----75cca62debce----1---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----75cca62debce----1---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----75cca62debce----1---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----75cca62debce----1---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----75cca62debce----1-----------------bookmark_preview----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----75cca62debce----2---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----75cca62debce----2---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----75cca62debce----2---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----75cca62debce----2---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----75cca62debce----2---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----75cca62debce----2---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----75cca62debce----2---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----75cca62debce----2-----------------bookmark_preview----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/contextualizing-large-language-models-llms-with-enterprise-data-419e252fcbb7?source=author_recirc-----75cca62debce----3---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=author_recirc-----75cca62debce----3---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=author_recirc-----75cca62debce----3---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Debmalya Biswas"}, {"url": "https://medium.datadriveninvestor.com/?source=author_recirc-----75cca62debce----3---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/contextualizing-large-language-models-llms-with-enterprise-data-419e252fcbb7?source=author_recirc-----75cca62debce----3---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "Contextualizing Large Language Models (LLMs) with Enterprise DataPrompt Engineering, Fine-tuning, Reinforcement Learning from Human Feedback (RLHF)"}, {"url": "https://medium.datadriveninvestor.com/contextualizing-large-language-models-llms-with-enterprise-data-419e252fcbb7?source=author_recirc-----75cca62debce----3---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": "\u00b76 min read\u00b7Mar 21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F419e252fcbb7&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Fcontextualizing-large-language-models-llms-with-enterprise-data-419e252fcbb7&user=Debmalya+Biswas&userId=ad84805121fe&source=-----419e252fcbb7----3-----------------clap_footer----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/contextualizing-large-language-models-llms-with-enterprise-data-419e252fcbb7?source=author_recirc-----75cca62debce----3---------------------cc8debae_dfca_416f_9ae1_87fe00819459-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F419e252fcbb7&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Fcontextualizing-large-language-models-llms-with-enterprise-data-419e252fcbb7&source=-----75cca62debce----3-----------------bookmark_preview----cc8debae_dfca_416f_9ae1_87fe00819459-------", "anchor_text": ""}, {"url": "https://debmalyabiswas.medium.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "See all from Debmalya Biswas"}, {"url": "https://towardsdatascience.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----0-----------------clap_footer----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----75cca62debce----0-----------------bookmark_preview----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://kargarisaac.medium.com/?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://kargarisaac.medium.com/?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Isaac Kargar"}, {"url": "https://medium.com/aiguys?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "AIGuys"}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Reinforcement Learning from Human Feedback, InstructGPT, and ChatGPTNote: some parts of this blog post are generated by ChatGPT! :)"}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "\u00b79 min read\u00b7Jan 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Faiguys%2F693d00cb9c58&operation=register&redirect=https%3A%2F%2Fmedium.com%2Faiguys%2Freinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58&user=Isaac+Kargar&userId=bf5ea8e11f80&source=-----693d00cb9c58----1-----------------clap_footer----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F693d00cb9c58&operation=register&redirect=https%3A%2F%2Fmedium.com%2Faiguys%2Freinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58&source=-----75cca62debce----1-----------------bookmark_preview----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----75cca62debce----0---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----75cca62debce----0-----------------bookmark_preview----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Piotr Krosniak"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Vaccine Supply Chain Optimization with AI-Powered Capacitated Vehicle Routing Problem(CVRP)- Part 1The world is facing a global health crisis, and one of the most important challenges is to ensure an efficient and timely distribution of\u2026"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "6 min read\u00b7Jan 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&user=Piotr+Krosniak&userId=b791abcfafd5&source=-----ca79519e9ad7----1-----------------clap_footer----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----75cca62debce----1---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&source=-----75cca62debce----1-----------------bookmark_preview----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----75cca62debce----2---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----75cca62debce----2---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----75cca62debce----2---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----75cca62debce----2---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----75cca62debce----2---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----75cca62debce----2---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----2-----------------clap_footer----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----75cca62debce----2---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----75cca62debce----2-----------------bookmark_preview----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----75cca62debce----3---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----75cca62debce----3---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----75cca62debce----3---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----75cca62debce----3---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----75cca62debce----3---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----75cca62debce----3---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----3-----------------clap_footer----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----75cca62debce----3---------------------8322e228_3966_47ea_a8e7_65d155dda6dc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----75cca62debce----3-----------------bookmark_preview----8322e228_3966_47ea_a8e7_65d155dda6dc-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----75cca62debce--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----75cca62debce--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}