{"url": "https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011", "time": 1682999766.158528, "path": "towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011/", "webpage": {"metadata": {"title": "Introduction to Reinforcement Learning (DDPG and TD3) for News Recommendation | by Mikhail Scherbina | Towards Data Science", "h1": "Introduction to Reinforcement Learning (DDPG and TD3) for News Recommendation", "description": "Reinforcement learning as-is is a pretty hard topic. When I started to dig deeper, I realized the need for a good explanation. This article, coupled with the code is my school project. I am currently\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/awarebayes/RecNN", "anchor_text": "check it out.", "paragraph_index": 3}, {"url": "https://github.com/awarebayes/RecNN", "anchor_text": "https://github.com/awarebayes/RecNN", "paragraph_index": 3}, {"url": "https://github.com/awarebayes/RecNN", "anchor_text": "GitHub page", "paragraph_index": 6}, {"url": "https://github.com/awarebayes/RecNN/blob/master/notes/1.%20Generating%20the%20static%20dataset.ipynb", "anchor_text": "1. Generating the static dataset.ipynb", "paragraph_index": 8}, {"url": "https://code.fb.com/data-infrastructure/faiss-a-library-for-efficient-similarity-search/", "anchor_text": "Facebook\u2019s Faiss", "paragraph_index": 13}, {"url": "https://github.com/facebookresearch/faiss", "anchor_text": "GitHub link.", "paragraph_index": 13}, {"url": "https://medium.com/explorations-in-language-and-learning/online-learning-of-word-embeddings-7c2889c99704", "anchor_text": "link", "paragraph_index": 15}, {"url": "https://www.youtube.com/channel/UCgBncpylJ1kiVaPyP-PZauQ", "anchor_text": "Luis Serrano", "paragraph_index": 15}, {"url": "http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf", "anchor_text": "Yann LeCun\u2019s notes", "paragraph_index": 18}, {"url": "https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/", "anchor_text": "link", "paragraph_index": 22}, {"url": "https://www.youtube.com/watch?v=XL07WEc2TRI", "anchor_text": "lectures by Naftali Tishby", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d", "anchor_text": "short article", "paragraph_index": 31}, {"url": "https://github.com/awarebayes/RecNN/blob/master/readme.md#how-to-use-static-movielens-dataset-in-your-project", "anchor_text": "example", "paragraph_index": 33}, {"url": "https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/", "anchor_text": "here", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1908.03265", "anchor_text": "Axriv paper", "paragraph_index": 68}, {"url": "https://github.com/awarebayes/RecNN/blob/master/notes/_%20Results/1.%20Ranking.ipynb", "anchor_text": "notes/results/1. Ranking/", "paragraph_index": 91}, {"url": "https://github.com/awarebayes/RecNN/blob/master/notes/5.%20Predictions.ipynb", "anchor_text": "here", "paragraph_index": 93}, {"url": "https://github.com/milvus-io/milvus", "anchor_text": "Milvus library", "paragraph_index": 95}, {"url": "https://github.com/navy-xie", "anchor_text": "navy-xie", "paragraph_index": 103}, {"url": "https://github.com/KnightofK9", "anchor_text": "KnightofK9", "paragraph_index": 103}, {"url": "https://github.com/tomatatto", "anchor_text": "tomatatto", "paragraph_index": 103}, {"url": "https://github.com/lystahi", "anchor_text": "lystahi", "paragraph_index": 103}, {"url": "https://github.com/jungi21cc", "anchor_text": "jungi21cc", "paragraph_index": 103}, {"url": "https://github.com/nutorbit", "anchor_text": "nutorbit", "paragraph_index": 103}, {"url": "https://github.com/davidjiangt", "anchor_text": "davidjiangt", "paragraph_index": 103}, {"url": "https://github.com/kiminh", "anchor_text": "kiminh", "paragraph_index": 103}, {"url": "https://github.com/hb1500", "anchor_text": "hb1500", "paragraph_index": 103}, {"url": "https://github.com/YerinMin", "anchor_text": "YerinMin", "paragraph_index": 103}, {"url": "https://github.com/Saleh-Hassan", "anchor_text": "Saleh-Hassan", "paragraph_index": 103}, {"url": "https://github.com/ImranRolo", "anchor_text": "ImranRolo", "paragraph_index": 103}, {"url": "https://github.com/limtete", "anchor_text": "limtete", "paragraph_index": 103}, {"url": "https://github.com/erikson92", "anchor_text": "erikson92", "paragraph_index": 103}, {"url": "https://github.com/miladfa7", "anchor_text": "miladfa7", "paragraph_index": 103}, {"url": "https://github.com/sudongtan", "anchor_text": "sudongtan", "paragraph_index": 103}, {"url": "https://github.com/Belobobr", "anchor_text": "Belobobr", "paragraph_index": 103}, {"url": "https://github.com/noaRricky", "anchor_text": "noaRricky", "paragraph_index": 103}, {"url": "https://github.com/q710245300", "anchor_text": "q710245300", "paragraph_index": 103}, {"url": "https://github.com/andrebola", "anchor_text": "andrebola", "paragraph_index": 103}, {"url": "https://github.com/chenmingxia", "anchor_text": "chenmingxia", "paragraph_index": 103}, {"url": "https://github.com/ashdtu", "anchor_text": "ashdtu", "paragraph_index": 103}, {"url": "https://github.com/dindanovitasari", "anchor_text": "dindanovitasari", "paragraph_index": 103}, {"url": "https://github.com/kk-hainq", "anchor_text": "kk-hainq", "paragraph_index": 103}, {"url": "https://github.com/bazitur", "anchor_text": "bazitur", "paragraph_index": 103}, {"url": "https://github.com/youyiandyou", "anchor_text": "youyiandyou", "paragraph_index": 103}, {"url": "https://github.com/Vi-Sri", "anchor_text": "Vi-Sri", "paragraph_index": 103}, {"url": "https://github.com/sanjmen", "anchor_text": "sanjmen", "paragraph_index": 103}, {"url": "https://github.com/YvetteLi", "anchor_text": "YvetteLi", "paragraph_index": 103}, {"url": "https://www.dlschool.org/?lang=en", "anchor_text": "www.dlschool.org", "paragraph_index": 104}], "all_paragraphs": ["TL;DR: Reinforcement Learning is the ideal framework for a recommendation system because it has Markov Property. The state is movies rated by a user. Action is the movie chosen to watch next and the reward is its rating. I made a DDPG/TD3 implementation of the idea. The main section of the article covers implementation details, discusses parameter choice for RL, introduces novel concepts of action evaluation, addresses the optimizer choice (Radam for life), and analyzes the results.", "I also had released an ml20m dataset version specifically adopted for Markov decision process and to use with RL.", "Reinforcement learning as-is is a pretty hard topic. When I started to dig deeper, I realized the need for a good explanation. This article, coupled with the code is my school project. I am currently in a sophomore year of high school, and I understand the hard mathematical concepts in a more \u2018social studies\u2019 kind of way. I hope this article proves to be helpful for newcomers like me.", "I created a GitHub project you can clone and follow along! Make sure to check it out. You can download everything that I processed on my PC in the downloads section. As well as FAQ, dataset description, some docs, how-tos and more. It is frequently updated. I haven\u2019t pushed for a week because I was writing this article. I hope you like it! https://github.com/awarebayes/RecNN", "This article provides a comparison of the most popular recommendation methods as well as an in-depth look at various reinforcement learning algorithms, overviewing each tweakable parameter and the aftermath of changing it. I also propose alternative ways to evaluate deep learning-powered recommenders and discuss different optimizers for that application. It is also my legacy because my final year of high school is coming, so I won\u2019t have enough time to work on it. If you are interested in supporting the project, your contributions are welcome!", "Firstly, because of the frustration with the dataset being dynamic. When I started prototyping, it used to take more than 40 hours for just one iteration. With basic pandas and its optimization done, it shrinks down to 1.5. When I implemented dynamic dataset, the thing takes 10 minutes. If you encode the states with state representation, it comes down to 3. Also, I couldn\u2019t get the DDPG working at all, and it added quite some impact. Thus, I ended up using a static time series dataset + TD3. However, more about it later.", "However, above all, most of the articles on TDS are paid. Thus there are no premium articles, no Patreon, no money begging. You can clap to this article multiple times (please do so with the button up left) and go to the GitHub page and star the repo.", "It is my school project, and starring, it is essential to me. It would also give me better chances at winning the project competition, maybe even university payment cut-offs.", "As you can see, pandas can be optimized, but in the end, it is still quite costly to run because even my best optimization does not scale well. The x-axis represents the power of 10. The y-axis is the time it took (in seconds). Also, the thing with deep learning is that we often run the model on the same dataset over and over. So it would make total sense to make our dataset completely static, eliminating all pandas interaction whatsoever. Let\u2019s just run our dataset generator and save the results. If you have forked my repo and following along, the notebook is located under notes/1. Vanilla RL/1. Generating the static dataset.ipynb. Note: It is entirely mandatory; you can download the dataset generated by me.", "Sometimes the time-series cannot be entirely fed into your RAM. Also, the HDF5 format was developed specifically for that purpose. Use whenever possible because it works way faster than PyTorch and natively comes with numpy support. The only limit is your solid-state disk so you might want to buy a PCI Express one with fast reading.", "If you are using a static size time series (also called \u2018rolling\u2019 ts), make sure you encode the data into lower dimensions. For the classic ML approach, we have Principal Component Analysis or PCA for short. Here is a video if this is a new word for you.", "You can also use LSTM Autoencoders for dynamic length Time Series. From my experiments, I noticed that linear AEs perform poorly for rolling ts. However, I use state representation as the authors of the paper proposed. Rule #1337 of DL states that 90% of the actual learning happens in the first 10 minutes. So I ran the TD3 model and used its state representation module to encode the TS.", "When I first started to dig into the stuff, it realized that there is no comprehensive guide to even basic techniques of recommendation. I recently had found out about Restricted Boltzmann Machines. This section aims to fix it. I attempt to overview some of the most popular ones and make a quick comparison. For more analytical results, look at the memes below.", "SS is the most straightforward concept to understand. Just look for similar films liked or disliked among the users. State (being the films rated) is often represented as a metric space. There are a couple of ways to encode it from raw movies indexes. The first one is to use the embedding layer, which is often the case in modern DL applications. A similarity metric such as cosine or Euclidean distance is then used to rank them nicely. However, looking back to a more classical ML approach, we have the Locality Sensitive Hashing. LSH is an algorithmic technique that hashes similar input items into the same \u201cbuckets\u201d with high probability. Either way, we end up with a bunch of ranked states that are similar to the one we are predicting for. Then we look at the films the users liked/disliked and recommend them. If you want to use this method, I suggest you check out Facebook\u2019s Faiss library: GitHub link.", "The idea of factorizing matrices, i.e., breaking a big matrix into a product of smaller ones, further extends similarity search. The big matrix can be expressed as a table with rows being the movies, columns being the users, and the values are the ratings. We extend that idea by assuming that the big matrix can be expressed as a dot product of two smaller matrices. They represent the hidden (embedding) representation. The process can is easily implemented using PyTorch:", "\u2018Users\u2019 is an integer vector of userId. \u2018Films\u2019 is an integer vector of film_id. User and Film matrices are 2D embeddings for corresponding indexes. We calculate the dot product because we want to know the rating. As you might have noticed, the method is pretty limited due to the usage of embeddings. You cannot add new films/users to the existing ones unless you are using something like Incremental SGNS or Reservoir Computing. Just a good overview article of the methods above: link. Also, if you want to get an in-depth understanding of MF, I highly recommend this video by Luis Serrano.", "RBS is an early variant of an autoencoder. It falls under the energy-based methods. As an autoencoder, it is used for dimensionality reduction. The restricted part of the naming means that there is no interlayer propagation. The architecture looks like a usual two-layered linear network. The forward pass looks precisely like the feedforward net.", "The critical difference is that RBMs are probabilistic. They use Bayes stuff to work. Whenever you try to calculate the state of the network, i.e., the sample from these weights and biases distributions, you are met with the Boltzmann equation. It is an equation from particle physics. The learning of such model consists of two main steps: Gibbs Sampling and Contrastive divergence.", "I found out about these machines from Andrew Ng\u2019s interviews with Geoffrey Hinton. When asked about his greatest achievement, the latter acknowledged his contributions to training algorithms of RBMs. Just a reminder: G.H. is a man behind backpropagation. Indeed RBMs achieve state-of-the-art performance in the Netflix competition. If you want to learn more about the energy-based models: here are Yann LeCun\u2019s notes.", "Factorization Machines had proven to be super useful for click-through rate prediction. Their speed allows them to be highly scalable, but they are only applicable to data with categorical features. Nevertheless, they are worth a shout out. We need to incorporate feature data into our factorization process somehow. Of course, we can consider a single feature to be resourceful enough:", "As you can see, they cannot be used for a personalized recommendation!", "However, it would be cool to take label-label cross-correlations of a feature into consideration. We just learned about the concept of order. Order is the number of features calculating the cross-correlation for. Assuming the order to be 2, we need to calculate the CC for two features. Nevertheless, the feature is a categorical variable, so how does one calculate the dot product for two cats? More latent variables to the god of the latent variables! Feature labels can be described using vectors, and those vectors can be regressed using the same idea of embeddings we utilized for matrix factorization.", "Here is an article that helped me to understand this concept better: link.", "The key advantages of using RL for news recommendation are Markov Property and State Representation. Because we do not rely on any embeddings, we can recommend any movies to any user. Movie embeddings generated for this application do not rely on the embedding layer. I used simple statistics, such as average rating, revenue, TF-IDF for texts, genres, etc.\u2026 + PCA. Thus, you can add a new movie for a recommendation without re-training the network. Alternatively, you can use these new embeddings for state representation. Markov property ensures that we can use static-length time series. More about it later.", "To sum it up: RL allows learning on minibatches of any size, input of static length time series, does not depend on static embeddings, works on the client-side, can be used for transfer learning, has an adjustable adversary rate (in TD3), supports ensembling, works way faster than MF, and retains Markov Property. The most significant trade-off is the accuracy: big corporations such as Netflix/Amazon still rely on MF/RBM.", "This particular application, unlike Q-Learning, aims to solve the continuous control problem. In Q-Learning state is often continuous, but the action itself is discrete. Whereas in our case, the action (=movie) is not discrete, but it is a vector instead.", "But how do we get this vector we will be trying to somehow regress later on? Last time I checked the ML20M dataset, there were no vectors to be found. The answer is simple: I generated these vector representation of numerically indexed movies myself. Most of the stuff is trivial: I parsed the IMDB/TMDB data and applied basic statistics to make a vector of the gathered data (by encoding categories, using TF-IDF, applying PCA) But one of the most important things I utilized is Google\u2019s BERT for text data embeddings. I know, embeddings are technically a different thing, and these are called \u2018bottleneck features\u2019, but I will stick to that word.", "However, why does some middle layer of a neural network make sense? Why can you use that data as contextual information? Those questions are answered by the field of studies called Information Theory. It is not widely popular in the context of DL, but there are fascinating lectures by Naftali Tishby about the `maximization of preserved information` and other hints of why the nets learn. I recommend you to check these out!", "Now that we got our dataset working, understood how I transformed movie IDs into contextual vectors, it\u2019s time to recap some things about reinforcement learning and game theory. If you read Arxiv papers on DL/RL, is is a common thing to see the basics sorted out.", "News Recommendation can be thought of as a game that we are trying to win. We act based on the state, and the state is what we know about the user: ratings and movies watched combined. The action is produced based on the state and describes a point in space.", "Everything from now on strictly obeys the Markov Property. Quoting Wikipedia: \u2018 A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it.\u2019 Why should I care, you might ask. We assume that we can act only based on the current state, ignoring anything that happened before. Having that in mind, the problem becomes way more natural to solve because we don\u2019t have to worry about the past. Markov methods provide a framework that allows you to focus on the things that happen at the moment.", "You also have heard that name, because DeepMind\u2019s AlphaGo uses Markov Chain Monte Carlo. Monte Carlo part is only used for finite-state games such as chess or go. However, Markov Chains are everywhere! For simplicity purposes, let\u2019s consider a discrete state. We are also assuming that recommendation is a stochastic process meaning that we randomly walk over the dataset. Like any other physical chain, Markov Chain consists of \u2018loops\u2019 called nodes. Each node has a conditional transition probability. Think about is as a random graph walk, and each node that is currently visited has probabilities that determine which adjacent node goes next. Here is a short article with 19k likes that goes into details. The cool thing is that it preserves the Markov property: transition is only dependent on the current state, being the node visited.", "The idea of Markov Chains can be further applied to our \u2018game\u2019. We want to utilize the Markov Framework for our application because it is super handy. However, how do you apply an abstract chain to state-action-reward-state-action\u2026 process? Remember the graph example I introduced? Markov decision process can also be interpreted as a graph. We assume our current State to be some node in the graph. Because it retains that exact property, we don\u2019t need to know anything that happened before we arrived at that State (node). From that node, there is a multitude of actions that can be taken with assigned probabilities. These actions can be interpreted as adjacent edges that bring us to the New_State. When we arrive at the new State, we immediately receive a Reward. There is also one thing in the dataset that is used for TD \u2014 Done because we don\u2019t want to propagate temporal difference beyond the last step. Although, more about it later.", "The dataset I built consists of State-Action-Reward-Next_State-Done values. There is an example on GitHub", "One last thing to fully understand what is happening is that action is not discrete. I already have mentioned a couple of times that the action is a vector. I can understand how to move in the graph based on a discrete number, but where do you go with a vector? So instead of graphs think about the MDP as an N-dimensional plane. For simplicity purposes, I will use a 2D plane as an example. Let\u2019s consider an ant agent moving on the playground. At the moment he knows that he probably needs to bring some leaves to its house. The only problem is there is no such discrete action \u2018hard coded\u2019 into its brain. So it needs to take continuous steps in the environment: move his limbs, clitch his jaws, and avoid the evil antlion. With each step it takes, the reward is added. It might be some realization that the leaves are somewhere nearby, and it is going in the right direction. One other important distinction is that we assume that time steps are discrete. The recommendation is only possible if we know a certain number of films and the ratings assigned by the user. It would be weird to try to suggest something taking 5.6 films and 1.3 ratings into consideration.", "And if you look at the \u2018continuous vs. discrete action \u2019 example above, there is a MuJoCo Ant agent. It takes limb positions and angular velocities as an input to supply the environment. Actually, there are a lot more funny agents to fiddle with such as cheetah and Boston Dynamics robot models.", "This part is essential for understanding the Temporal Difference Loss, which will be covered a little bit later. In the context of Markov Games, we have such thing as the Value Function. So the Value Function does not imply a function that estimates the Reward. Value can only mean how good the action is for the current state. Although, as you will see, it does not necessarily mean \u2018give me the reward\u2019 for that action and state. It is a more abstract measure of \u2018goodness\u2019 and can be expressed as a continuous real-valued function. The range of the function can be any real number. Thus, the value is not an integer number in [-5, 5].", "Now let\u2019s forget for a brief moment about all the Markov stuff that we learned and try to get started with a basic DL approach. So you decided to build a deep-learning-powered recommender system, already know about Markov Decision process and the dataset structure, overall very eager to jump straight into the action. Let\u2019s try a more fundamental approach, without reinforcement learning just yet. All you have is a simple linear perceptron and the dataset: a bunch of states, corresponding actions, and rewards to those actions. Translated to more human language: films watched, the movie chosen to see next and its rating by the user. I want you to go ahead and think about the things you would do with these tools.", "We want to suggest good movies; hence, we train the network to generate movies like the action:", "This approach is called \u2018Policy Learning\u2019 because we learn the policy, being the action. It has its applications, but in the endpoint, PL is very limited. If you train a network like this, it will work fine and be somewhat usable. Still, did you notice the \u2018is_good\u2019 function? We learn only using \u2018good\u2019 actions, disregarding anything else.", "So far we\u2019d been looking at the reward as our \u2018learning\u2019 criteria. However, wouldn\u2019t it be wiser to analyze the action instead? That is the main idea behind the Actor-Critic methods. For clarity\u2019s sake, let me introduce the namings for the networks. The network we already contemplated is called Actor, and it acts based on the state. The network that tries to predict the reward based on the state and the Actor\u2019s action is called Critic. As we all know, from watching Bob Ross: everyone needs a friend. So let\u2019s add to our lonely Actor someone to mock his actions.", "Eventually it the actor will do better actions (maybe maybe maybe) and the loss will converge to zero or something. However, since that, we are working in the chad Pytorch; we can do wacky things with losses without worrying about the backpropagation! What if we want to directly use the critic-produced reward for the actor-generated action as a loss metric? With Pytorch it has never been easier!", "What we did here is that we used the critic as our loss function! As it was intended since the beginning. Note the minus before the criterion. We want to maximize the rewards. Still, there is no such thing as \u2018maximize\u2019 in machine learning. We often do the opposite: minimizing the negatives.", "That is the DDPG algorithm. Although, if you look in the paper, the code for the critic training will be different. That\u2019s what we are covering next.", "There is something wrong with our Critic. We don\u2019t need to estimate the rewards. Let\u2019s bootstrap the Value for the action instead. Why might you ask? The answer is simple: future rewards may be dependant on the current action. Considering the actions as individual Markov chain steps, we want to maximize the Value. What is Value?", "Value in the context of a Markov Game is a real-valued function: V(state, action) that indicates (not in terms of integer rewards -5 to 5, it can be any real number) how proper the particular action for the corresponding state.", "So that\u2019s where we need to bring back our Markov Chains. In this particular application, I use TD(1) version of the algorithm, meaning that I bootstrap the value 1 step in advance. You can implement TD(n), but the number of value iterations increases linearly.", "To learn the value function, we bootstrap the reward and the value of the next state and text action. You can read more about TD here.", "There is only one last thing you need to understand the DDPG completely. It uses the concept of Target and Learning networks. Target network is more stable rather than the learning one because it is updated using learning parameters through the soft update. It shows less tendency to overfit and overall performs better. Also, the TD loss is slightly tweaked. We don\u2019t need to bootstrap the value for endgame actions. Gamma parameter serves for stability, and I set it to around 0.9.", "Here is the actual screenshot of the update function I use", "That is basically what DDPG is about.", "In the next section, we will try to compare and, primarily, evaluate different reinforcement learning algorithms. But how do we tell if the results are good or not? The critic network assigns the values to our actions; however, are you sure whether the value is meaningful. Well, they are based on critic loss. If critic loss is small and the policy loss makes sense, we taught the actor. But those metrics are not enough. I also consider euclidean and cosine distances for actions. A matrix often represents this. You can see the distances described as individual grid pieces varying by color. The warmer it is, the bigger the distance. For instance here is how real actions (those I produced with statistics embeddings) look like.", "You will see similar matrices for training and testing actions later on. Another method to evaluate the \u2018artificiality\u2019 of actions is to use the autoencoder reconstruction error. I train the autoencoder model to reconstruct the embedding actions. The reconstruction error is used as artificiality metric. This technique is widely used for anomaly detection because it is unsupervised. Also, I perform Kernel Density Estimation on the error distribution for pretty plotting and visual comparison. Wasserstein distance or KL Divergence can also be implied.", "The goal is to make the generated test distribution close to the true (real) one. Generated train (or just generated) may be of different forms.", "All I did at this point is copypasting Higgsfield\u2019s RL adventure code with minor tweaks to work with torch 1.1", "It looks like we have something! Matrices look cool? Some correlation is showing up\u2026 The loss is descending\u2026 Wait, say what is on the scale bar? Why do I have next to no cosine distances? Why is the policy loss falling into the nether realm? Why don\u2019t my pet AIs work as supposed?", "And that\u2019s when it comes to the true joy of RL!", "Countless things can go wrong! Maybe I should play around wit the learning rate? Admittedly, that\u2019s the first thing that comes in mind for a person familiar with DL. However, why my error is growing in the first place? I haven\u2019t encountered these things in the DL\u2026 WTF is happening with the v-net optimization, PyTorch? It doesn\u2019t seem to have much of an impact as the process starts, but then the gradients explode, and everything goes into oblivion\u2026 So, likely, the learning rate is excellent. Maybe that\u2019s because of the Temporal Difference bootstrapping? But not only does the TDB use the target value network, which is soft updated via soft tau, the TD it also utilizes a parameter called gamma for weighting the future expectation.", "What am I supposed to tweak? Maybe I set the wrong parameter for clipping the value:", "One thing that came as a bamboozler to me is that the weight inits trick works slightly differently than one would expect. If we initialize the last layer\u2019s weights of the net with smaller numbers, it will generate smaller vectors. Wrong! It works exactly the opposite. So it is the main reason for the cosine distance being so short in comparison to the distribution of the real action. However, it doesn\u2019t seem to have much of an effect for Euclidean.", "Also, have you noticed the min and max parameters for the value clipping are min and max rewards, respectively? That also needs to be changed, as I mentioned it back in the Markov property stuff. I set it to reward * n_td_steps. That works fine, and at the end, the loss rarely goes below 6\u20137 at most.", "You may not realize it, but a smaller learning rate for critic leads to actor overfitting. If the policy loss falls weirdly fast, change the LR, because the actor is overfitting. Keep in mind: not the most intuitive thing!", "Always debug and keep track of the following parameters: value, target_value (mean, std), expected_value (mean, std), generated_action (cosine distances, Gramian, variance, std, means, samples KL distance to the original distribution). Always plot those metrics at the end of the testing.", "We solved the RL part of a not working network, now the reasons for the loss to grow are purely DL. Here are some bits of advice for you:", "This is supposed to bring back to life the DDPG on your/mine data.", "Always look at the scale of the loss function. The value should be somewhere between [0, 10], whereas policy within min/max values for TD clipping. If it goes up to 10e7, you are wasting your time debugging!", "I already paid attention to this, but I cannot stress it enough. Always use the state of the art optimizer. Check Arxiv daily, subscribe to the twitter bot and be on a lookout, because the Machine Learning community is now at its peak of productiveness.", "Adam is often the default choice for optimization. Moreover, this should be changed. Adam is one of the worst options you have. Why? It\u2019s very very learning rate sensitive. Thinking back about the learning rate schedulers: carefully memorizing which step to put the milestone at, using CosineAnnealingLR warm restarts SGD, CyclicLR. Everything of the techniques above is now in the past \u2014 no more fiddling with the learning rate. Just use RAdam! On the Variance of the Adaptive Learning Rate and Beyond!", "RAdam is an ideological extension of Adam, and it uses a smart trick called Learning Rate Warmups. Unfortunately, the authors of Radam were not welcome enough to make the Medium article free, so there will be no detailed explanation. You can read their Axriv paper.", "Nevertheless, I claim that just by changing the optimizer, I was able to achieve TD3 like performance with DDPG algorithm. And with way less overfitting in both cases", "Here is how the Adam loss looks like. One does not have to be a Ph.D. to spot clear value overfitting. Also, do you notice the Policy Loss? It seems strangely small. The max reward is 5, and max TD clipping is 10.", "Train actions seem to overfit, not much of Euclidean Distance is showing up, but alright\u2026", "Loss seems to be reasonable at 7.5. Or does it? Alright, let\u2019s look at the autoencoder reconstruction error.", "You can spot that something is wrong just by looking at the ae rec error distributions and/or comparing various metrics between them. The dumbest choice would be only to calculate KL/WDistance. You can also look at more meaningful statistical metrics such as mean/std/kurtosis/skewness instead.", "CosDist < 0.4, Euc < 3. No words needed. Also, an observation by me: Cosine matrix defines an angle distribution of the direction of vectors. You can use basic math to visualize it.", "Woah, that\u2019s some good memory! I wish I could memorize that much. Unfortunately, that is not leaning. You can jump straight inro TD3, and it will undoubtedly help with the overfitting, but let\u2019s try to use the RAdam instead. Just change optim.Adam to RAdam when you define the optimizers.", "So I ran the same code with RAdam, and that\u2019s what happened.", "First Off \u2014 no nasty Value Overfitting. Moreover, the value loss is less. The convergence is way more delayed in comparison to Adam. There you can see a clear drop trend at around 400. Here it takes twice as many iterations to notice the pattern. Also, value loss is much smoother. The train actions are way more apart in terms of the Euclidean distance. The cosine dist seems to be preserved across all the experiments. All right, let\u2019s see the test actions:", "The results speak for themselves! Test distribution is approximated way better with RAdam, but the training one is more obscure. Nevertheless, we only care about testing.", "However, RAdam comes with a drawback. It is hard to finetune models with it. If you leave the model running with RAdam, the training distribution will quickly converge to the true one. Also, the testing distro will be something like a small pine cone at the very right. Need to look out for the Policy loss approaching -5, or manually stop the learning. This is what happens if you do not stop it:", "The policy network overfits: values are quickly dropping. It is not the case with Adam.", "My end solution was to combine the two optimizers. First, I learn with RAdam, by creating checkpoints before overfitting I ensure that I have an excellent warmed up model. Then I load these models and train them with Adam to achieve even better performance. Unfortunately, there is no room for automatization: you find the smallest policy loss and start from the nearest checkpoint.", "It results in better representation of the training distro, but the testing one is way worse. But this method of fine-tuning will be more useful for the TD3 algorithm. Which we are switching onto\u2026 Now!", "P.S. all models are available for downloading on the github page(including raw Radam and Adam fine-tuned)", "TD3 stands for Twin Delayed DDPG. Three is the number of improvements the authors of the paper propose. As I already stated, it is an extension of Deep Deterministic Policy Gradients. The differences are the following:", "2. Delayed Policy Updates. We update policy less frequently in comparison to values. For each three value update, we update the actor just once. This allows for better value estimation and also prevents actor fooling. P.S. it is not a hard concept and is super easy to implement. I used delayed policy updates with DDPG.", "3. Action smoothing. TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along with changes in action. In my case, the noise is drawn from ~Normal(0, 0.1) and clipped to fit [-.3, .3].", "The algorithm has pretty much the same parameters as the same as the DDPG (excluding min/max clip, adding noise params). All the code/dataset/pretrained models are on GitHub. TD3 is located under notes/1. Vanilla RL/2.ddpg", "Phew! We\u2019ve made it! What you see is my final result for the On-Policy methods. Now let\u2019s discuss the losses and other stuff. First off: value loss is growing. This phenomenon is reasonable because if we look at the values, they are not overfitting. In ddpg, they were increasing and then decreasing again.", "The distances cosine distances in real and generated actions differ. It is something around ~0.6 is generated whereas the real ones are ~1. It can be increased by setting the noise std to a higher value. However, I consider it cheating because we add more noise. Alternatively, you can use another loss function in couple with policy loss to add more cosine diversity", "In one of my older commits, I implemented cosine and euclidian distance loss penalties with the new Pytorch JIT compiler. P.S. it returns the pairwise distance matrix, like the ones shown above. You can do whatever you want with this thing (i.e., compare variance, std, mean, KL) to make it look like one of the real actions.", "It\u2019s time to test our algorithms. You can download all the pre-trained models and test them yourself. The file is under notes/results/1. Ranking/ (clickable).", "Below you will see examples of distance ranking. It supports scipy. spatial distances or your ones. In the notebook, I included the following: euclidean, cosine, correlation, Canberra, Minkowski, Chebyshev, Bray-Curtis, and city block (Manhatten). Cosine ranking allows for better language and genres diversity and looks very similar to correlation.", "That is it! You can see all the ranking examples for both of the algorithms here.", "So here we are. Congratulations if you have finally made it. It might seem like a simple project: just using the existing Higgsfield\u2019s algorithm implementation for new data, but I had been working 6 hours a day for the last month to get it working, to figure out the parameters in DDPG, and to understand the depths of PyTorch although the article was the hardest part of it. I am already writing this conclusion, but I haven\u2019t finished Restricted Boltzmann Machines in the comparison section yet. The repo is unpushed. I wonder how it had turned out\u2026 Did you understand it?", "TD3 implementation is promising. Moreover, the ranking works fine, although there is room for improvement. I am using O(n) algorithm. Highly recommend you check out the Milvus library if you want to use embeddings in production.", "I am also yet to implement a web app. Already got a commit on my other repo with react and basic layout. Hopefully, it will be published soon. If not, you can do it yourself: all the models are released.", "I honestly don\u2019t have much else to say. I am passing 7131 words already.", "Anyway here are some ideas from me:", "2. Add yet another cosine/euclidean loss to the generated actions. The scripts are published above.", "3. Implement a ranking network. It is an extension of the policy. It takes action as input and learns to generate real actions based on top k ranking.", "If you happen to implement these, feel free to commit.", "Medium: Ping Guo, \u200d\uc784\ud55c\ub3d9[ \ud559\ubd80\uc7ac\ud559 / \uae30\uacc4\uacf5\ud559\ubd80 ], Jacky Noah, Shady Hassab, Alexander Makeev, Shivam Akhauri, Diksha Garg, Siddharth Prabhu, Zohar Komarovsky, Hary Prasad, Panagiotis Kapros, Vishal Shrinivas, Yvette Li, Lxs Lxs, Nils Schluter, Ayush Kumar, Dean Soe, Cody Bushnell, Marcus Au, \u0628\u0627\u0631\u0628\u0631\u06cc \u062a\u0647\u0631\u0627\u0646 \u0627\u062a\u0648\u0628\u0627\u0631 \u062a\u0647\u0631\u0627\u0646, Navi Xie, Sang Huynh, Simon Yu, Yuzhou Zhang, Hoglan Huang, Lambjed Ben, Axel Schwanke, Anirban Saha, Baris Can Tauris, Mingju He, Jean-Philippe Corbeil, Shoaib Hafiz \u2014 It was your follow that made me feel somewhat important for the community.", "GitHub: navy-xie, KnightofK9, tomatatto, lystahi, jungi21cc, nutorbit, davidjiangt, kiminh, hb1500, YerinMin, Saleh-Hassan, ImranRolo, limtete, erikson92, miladfa7, sudongtan, Belobobr, noaRricky, q710245300, andrebola, chenmingxia, ashdtu, dindanovitasari, kk-hainq, bazitur, youyiandyou, Vi-Sri, sanjmen, YvetteLi \u2014 You believed in my project back before it was implemented. Those starts begiventh by thee motivated me to keep on working.", "DeepLearningSchool (www.dlschool.org) I wouldn\u2019t be writing this article if it wasn\u2019t for your excellent lessons and mentoring. Hope I make it to MIPT in the next year.", "Feel free to tell about yourself. I started deep learning when I went to high school. Although, back then, I created countless GitHub projects of other thematics, that were by no means less ambitious. The only problem was that no one knew about it \u2014 not a single soul except for a couple of friends. If you are working hard on something, be sure to tell you about it. It\u2019s other\u2019s appreciation that keeps you moving forward. P.S. don\u2019t write yet another Keras for preschoolers tutorial ;)", "Recently I have published a new article covering reinforce recommender systems:", "Next article will address some problems with the fact that we have no exploration. It surely will take some time to make, but as I see it BCQ easily integrates with the DDPG. If you want to know more about RL applications with a static dataset here are these links:", "Have some questions? The comments are open\u2026", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning Researcher at GosNIIAS (Computer Vision, Reinforcement Learning), BMSTU Software Engineering student"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd3cddec26011&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d3cddec26011--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d3cddec26011--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@awarebayes?source=post_page-----d3cddec26011--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@awarebayes?source=post_page-----d3cddec26011--------------------------------", "anchor_text": "Mikhail Scherbina"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe76971d8fea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&user=Mikhail+Scherbina&userId=e76971d8fea0&source=post_page-e76971d8fea0----d3cddec26011---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd3cddec26011&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd3cddec26011&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@julianamalta?utm_source=medium&utm_medium=referral", "anchor_text": "Juliana Malta"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/awarebayes/RecNN", "anchor_text": "check it out."}, {"url": "https://github.com/awarebayes/RecNN", "anchor_text": "https://github.com/awarebayes/RecNN"}, {"url": "https://github.com/awarebayes/RecNN", "anchor_text": "GitHub page"}, {"url": "https://github.com/awarebayes/RecNN/blob/master/notes/1.%20Generating%20the%20static%20dataset.ipynb", "anchor_text": "1. Generating the static dataset.ipynb"}, {"url": "https://code.fb.com/data-infrastructure/faiss-a-library-for-efficient-similarity-search/", "anchor_text": "Facebook\u2019s Faiss"}, {"url": "https://github.com/facebookresearch/faiss", "anchor_text": "GitHub link."}, {"url": "https://medium.com/explorations-in-language-and-learning/online-learning-of-word-embeddings-7c2889c99704", "anchor_text": "link"}, {"url": "https://www.youtube.com/channel/UCgBncpylJ1kiVaPyP-PZauQ", "anchor_text": "Luis Serrano"}, {"url": "http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf", "anchor_text": "Yann LeCun\u2019s notes"}, {"url": "https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/", "anchor_text": "link"}, {"url": "https://www.youtube.com/watch?v=XL07WEc2TRI", "anchor_text": "lectures by Naftali Tishby"}, {"url": "https://www.youtube.com/watch?v=XL07WEc2TRI&t=1140s", "anchor_text": "00:19:00"}, {"url": "https://www.youtube.com/watch?v=pFWiauHOFpY", "anchor_text": "https://www.youtube.com/watch?v=pFWiauHOFpY"}, {"url": "https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d", "anchor_text": "short article"}, {"url": "https://github.com/awarebayes/RecNN/blob/master/readme.md#how-to-use-static-movielens-dataset-in-your-project", "anchor_text": "example"}, {"url": "https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/", "anchor_text": "here"}, {"url": "https://github.com/LiyuanLucasLiu/RAdam", "anchor_text": "GitHub link"}, {"url": "https://arxiv.org/abs/1908.03265", "anchor_text": "Axriv paper"}, {"url": "https://github.com/awarebayes/RecNN/blob/master/notes/_%20Results/1.%20Ranking.ipynb", "anchor_text": "notes/results/1. Ranking/"}, {"url": "https://github.com/awarebayes/RecNN/blob/master/notes/5.%20Predictions.ipynb", "anchor_text": "here"}, {"url": "https://github.com/milvus-io/milvus", "anchor_text": "Milvus library"}, {"url": "https://github.com/navy-xie", "anchor_text": "navy-xie"}, {"url": "https://github.com/KnightofK9", "anchor_text": "KnightofK9"}, {"url": "https://github.com/tomatatto", "anchor_text": "tomatatto"}, {"url": "https://github.com/lystahi", "anchor_text": "lystahi"}, {"url": "https://github.com/jungi21cc", "anchor_text": "jungi21cc"}, {"url": "https://github.com/nutorbit", "anchor_text": "nutorbit"}, {"url": "https://github.com/davidjiangt", "anchor_text": "davidjiangt"}, {"url": "https://github.com/kiminh", "anchor_text": "kiminh"}, {"url": "https://github.com/hb1500", "anchor_text": "hb1500"}, {"url": "https://github.com/YerinMin", "anchor_text": "YerinMin"}, {"url": "https://github.com/Saleh-Hassan", "anchor_text": "Saleh-Hassan"}, {"url": "https://github.com/ImranRolo", "anchor_text": "ImranRolo"}, {"url": "https://github.com/limtete", "anchor_text": "limtete"}, {"url": "https://github.com/erikson92", "anchor_text": "erikson92"}, {"url": "https://github.com/miladfa7", "anchor_text": "miladfa7"}, {"url": "https://github.com/sudongtan", "anchor_text": "sudongtan"}, {"url": "https://github.com/Belobobr", "anchor_text": "Belobobr"}, {"url": "https://github.com/noaRricky", "anchor_text": "noaRricky"}, {"url": "https://github.com/q710245300", "anchor_text": "q710245300"}, {"url": "https://github.com/andrebola", "anchor_text": "andrebola"}, {"url": "https://github.com/chenmingxia", "anchor_text": "chenmingxia"}, {"url": "https://github.com/ashdtu", "anchor_text": "ashdtu"}, {"url": "https://github.com/dindanovitasari", "anchor_text": "dindanovitasari"}, {"url": "https://github.com/kk-hainq", "anchor_text": "kk-hainq"}, {"url": "https://github.com/bazitur", "anchor_text": "bazitur"}, {"url": "https://github.com/youyiandyou", "anchor_text": "youyiandyou"}, {"url": "https://github.com/Vi-Sri", "anchor_text": "Vi-Sri"}, {"url": "https://github.com/sanjmen", "anchor_text": "sanjmen"}, {"url": "https://github.com/YvetteLi", "anchor_text": "YvetteLi"}, {"url": "https://www.dlschool.org/?lang=en", "anchor_text": "www.dlschool.org"}, {"url": "https://towardsdatascience.com/top-k-off-policy-correction-for-a-reinforce-recommender-system-e34381dceef8", "anchor_text": "Top-K Off-Policy Correction for a REINFORCE Recommender SystemOffKTopPolicy is now available for your usage out of the box with no prerequisites in my Reinforced Recommendation\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/abs/1812.02900", "anchor_text": "https://arxiv.org/abs/1812.02900"}, {"url": "https://github.com/sfujim/BCQ", "anchor_text": "https://github.com/sfujim/BCQ"}, {"url": "https://github.com/awarebayes/RecNN", "anchor_text": "https://github.com/awarebayes/RecNN"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d3cddec26011---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----d3cddec26011---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/recommendation-system?source=post_page-----d3cddec26011---------------recommendation_system-----------------", "anchor_text": "Recommendation System"}, {"url": "https://medium.com/tag/code?source=post_page-----d3cddec26011---------------code-----------------", "anchor_text": "Code"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----d3cddec26011---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd3cddec26011&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&user=Mikhail+Scherbina&userId=e76971d8fea0&source=-----d3cddec26011---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd3cddec26011&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&user=Mikhail+Scherbina&userId=e76971d8fea0&source=-----d3cddec26011---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd3cddec26011&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d3cddec26011--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd3cddec26011&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d3cddec26011---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d3cddec26011--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d3cddec26011--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d3cddec26011--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d3cddec26011--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d3cddec26011--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d3cddec26011--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d3cddec26011--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d3cddec26011--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@awarebayes?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@awarebayes?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mikhail Scherbina"}, {"url": "https://medium.com/@awarebayes/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "187 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe76971d8fea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&user=Mikhail+Scherbina&userId=e76971d8fea0&source=post_page-e76971d8fea0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe76971d8fea0%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011&user=Mikhail+Scherbina&userId=e76971d8fea0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}