{"url": "https://towardsdatascience.com/ngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61", "time": 1683010319.421712, "path": "towardsdatascience.com/ngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61/", "webpage": {"metadata": {"title": "NGBoost algorithm: solving probabilistic prediction problems | by Michael Larionov, PhD | Towards Data Science", "h1": "NGBoost algorithm: solving probabilistic prediction problems", "description": "You may ask, how many more papers we need on Gradient Boosting? But in fact, the GBT family of algorithms works really well on tabular data, consistently taking the first places in the Kaggle\u2026"}, "outgoing_paragraph_urls": [{"url": "https://icml.cc/Conferences/2020/Schedule?type=Poster", "anchor_text": "ICML 2020 accepted papers", "paragraph_index": 0}, {"url": "https://catboost.ai/docs/features/text-features.html", "anchor_text": "NLP", "paragraph_index": 1}, {"url": "https://scikit-learn.org/", "anchor_text": "scikit-learn", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Fisher_information", "anchor_text": "Fisher Information matrix", "paragraph_index": 15}, {"url": "https://github.com/stanfordmlgroup/ngboost", "anchor_text": "https://github.com/stanfordmlgroup/ngboost", "paragraph_index": 21}], "all_paragraphs": ["While looking through the ICML 2020 accepted papers, I found an interesting paper:", "You may ask, how many more papers we need on Gradient Boosting? But in fact, the GBT family of algorithms works really well on tabular data, consistently taking the first places in the Kaggle leaderboards. The technique is so successful, that it is now being extended to the areas beyond tabular data, for example, to NLP.", "The problem we are tackling here is that almost all regression algorithms do not return the distribution of the target variable given predictors P(y|X), but an expectation of the target variable E(y|X), i.e. a point estimate. This is in contrast with most classification algorithms that do return the probability of the class. For example, in scikit-learn the classifiers have method predict_proba(), that returns the probability of the class. Regressors lack this method and only return probability.", "Why this is a problem? Suppose you are trying to forecast weather for the next week. What if you need to provide a range of your predictions, i.e. predict 15\u201320 degrees instead of predicting 17.25? Also if you want to know the probability that the temperature will not go below freezing? Can you do all these predictions with one single model? Yes, if your model returns a conditional probability distribution of the target variable given the predictors. While this is not a new problem, most of the solutions were problem-specific and we lacked the general algorithm we could use out of the box.", "This gap is addressed by the NGBoost algorithm:", "How does NGBoost finds the target variable distribution?", "So first of all, this is not a non-parametric model (If you are interested in non-parametric modeling of the continuous variable, you can try techniques such as discretizing or quantile-transforming). You have to make an assumption about the conditional target variable distribution. For example, it could be a Normal distribution (but with both \u03bc and \u03c3 depending on X), or a Gaussian Mixture (more realistic case for many regression problems), or a skewed normal distribution. For survival analysis it will be an Exponential distribution, for predicting positive values a variant of Gamma Distribution, etc. Once you picked your conditional distribution, the problem is simplified to learning parameter vector \u03b8 of the distribution given the input variables.", "When working on a regular regression problem, we often minimize mean squared error. How do we pick a loss function if what we are trying to predict is a probability distribution? In this case we use a scoring rule:", "For many problems, we can use negative log-likelihood as a scoring function", "In this case, what we are trying to get is how likely is a target value y given the distribution parameterized with \u03b8. The scoring rule for the best choice of the parameters \u03b8 is the smallest of the scoring rules. The difference between the scoring rules is called a divergence and is a measure of the \u201cdistance\u201d between probability distributions. If Q is the true distribution and P is a predicted distribution the divergence is:", "If we use MLE, then the divergence becomes Kullback\u2013Leibler divergence, widely used to measure the difference between probability distributions.", "\u201cNG\u201d in NGBoost stands for \u201cNatural Gradient\u201d. (Or could it be because it is the last name of one of the co-author of the paper Andrew Ng?) Why did it have to be introduced?", "Once you start using gradient-based methods in order to learn parameters \u03b8, you quickly realize that the distance between two distribution is not defined by the difference of their parameters, and so direction of the gradient by the parameters does not necessarily indicate the direction of the best improvement of the model", "The Natural Gradient corrects this issue by using the divergence instead of parameter difference:", "So, instead of drawing a unit sphere in the parameter space around the point and finding the direction of maximum improvement, we draw a surface on which the divergence takes the same value \u03b5. The natural gradient is connected with the regular gradient through the Riemann metric of the statistical manifold:", "If we use MLE, the matrix I is the Fisher Information matrix.", "Using the introduced techniques it is possible to create a GBT-based algorithm that predicts the parameters of the conditional distribution:", "While this looks like a regular GBT algorithm, it has a few important differences:", "This algorithm shows good performance comparing with other methods. It is very interesting to compare the experimental results with Natural Gradient vs. regular gradient:", "We can see that using Natural Gradient significantly improves learning, and the results describe the conditional variance more correctly.", "What is also interesting that the algorithm is also good at predicting point estimate, even though it was trained on a different objective.", "The Github repository https://github.com/stanfordmlgroup/ngboost has a good example as you would use it on a simple data set:", "Predicting a conditional distribution of a continuous variable is hard, it is much easier to predict a point estimate. It is different in classification problems, where more algorithms are able to predict the class probability. The NGBoost algorithm allows to easily get prediction of the parameters of conditional distribution of the target variable given the predictors. There are few assumptions on the type of distribution that can be handled by NGBoost. Indeed, if Normal distribution is not fit for a given problem, another distribution can be chosen (with potentially more parameters) to fit the model better. As indicated in the Conclusion of the paper, there are many possibilities of improvement of the algorithm, exploring theoretical questions and more complex problems. But even at this stage, the NGBoost is available for probabilistic regression in most of the practical problems.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffdbe1858ca61&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@michaellarionov", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaellarionov.medium.com/?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "Michael Larionov, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9158ca11a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=post_page-f9158ca11a43----fdbe1858ca61---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdbe1858ca61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdbe1858ca61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/icml-2020", "anchor_text": "ICML 2020"}, {"url": "https://unsplash.com/@mohamadaz?utm_source=medium&utm_medium=referral", "anchor_text": "mohammad alizade"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://icml.cc/Conferences/2020/Schedule?type=Poster", "anchor_text": "ICML 2020 accepted papers"}, {"url": "https://arxiv.org/abs/1910.03225", "anchor_text": "NGBoost: Natural Gradient Boosting for Probabilistic PredictionWe present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic prediction via gradient\u2026arxiv.org"}, {"url": "https://catboost.ai/docs/features/text-features.html", "anchor_text": "NLP"}, {"url": "https://scikit-learn.org/", "anchor_text": "scikit-learn"}, {"url": "https://en.wikipedia.org/wiki/Fisher_information", "anchor_text": "Fisher Information matrix"}, {"url": "https://github.com/stanfordmlgroup/ngboost", "anchor_text": "https://github.com/stanfordmlgroup/ngboost"}, {"url": "https://medium.com/tag/icml-2020?source=post_page-----fdbe1858ca61---------------icml_2020-----------------", "anchor_text": "Icml 2020"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fdbe1858ca61---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fdbe1858ca61---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----fdbe1858ca61---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/probabilistic?source=post_page-----fdbe1858ca61---------------probabilistic-----------------", "anchor_text": "Probabilistic"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffdbe1858ca61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=-----fdbe1858ca61---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffdbe1858ca61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=-----fdbe1858ca61---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdbe1858ca61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffdbe1858ca61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fdbe1858ca61---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fdbe1858ca61--------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Larionov, PhD"}, {"url": "https://michaellarionov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "611 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9158ca11a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=post_page-f9158ca11a43--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcf000f0c5fdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fngboost-algorithm-solving-probabilistic-prediction-problems-fdbe1858ca61&newsletterV3=f9158ca11a43&newsletterV3Id=cf000f0c5fdd&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}