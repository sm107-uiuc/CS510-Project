{"url": "https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad", "time": 1682993938.175979, "path": "towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad/", "webpage": {"metadata": {"title": "Over 1.5 TB\u2019s of Labeled Audio Datasets | by Christopher Dossman | Towards Data Science", "h1": "Over 1.5 TB\u2019s of Labeled Audio Datasets", "description": "At Wonder Technologies, we have spent a lot of time building Deep learning systems that understand the world through audio. From deep learning based voice extraction to teaching computers how to read\u2026"}, "outgoing_paragraph_urls": [{"url": "https://calendly.com/cdossman/cdossman-consultation", "anchor_text": "Sign up for a time slot", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Music_information_retrieval", "anchor_text": "Music Information Retrieval (MIR)", "paragraph_index": 2}, {"url": "https://medium.com/ai\u00b3-theory-practice-business/top-6-cheat-sheets-novice-machine-engineers-need-5ea43d1be3de", "anchor_text": "Top 6 Cheat Sheets Novice Machine Learning Engineers Need", "paragraph_index": 4}, {"url": "http://voice.mozilla.org/", "anchor_text": "Common Voice website", "paragraph_index": 13}, {"url": "https://scholar.google.com/citations?user=AVMa3t0AAAAJ&hl=en#d=gs_md_cita-d&p=&u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DAVMa3t0AAAAJ%26citation_for_view%3DAVMa3t0AAAAJ%3AIjCSPb-OGe4C%26tzom%3D-270", "anchor_text": "Saber MalekzadeH, Mohammad Hossein Gholizadeh, Seyed Naser Razavi \u201cFull Persian Vowel recognition with MFCC and ANN on PCVC speech dataset\u201d 5th International conference of electrical engineering, computer science and information technology, Iran, Tehran, 2018.", "paragraph_index": 16}, {"url": "http://bayanbox.ir/download/2723849504007807268/Full-Persian-Vowel-recognition-with-MFCC-and-ANN-on-PCVC-speech-dataset.pdf", "anchor_text": "PDF", "paragraph_index": 16}, {"url": "https://archive.org/details/chime-home", "anchor_text": "here.", "paragraph_index": 19}, {"url": "https://github.com/SenticNet/MELD", "anchor_text": "Download here", "paragraph_index": 26}, {"url": "https://towardsdatascience.com/deep-learning-performance-cheat-sheet-21374b9c4f45", "anchor_text": "Deep Learning Performance Cheat Sheet", "paragraph_index": 27}, {"url": "https://github.com/audioset/ontology", "anchor_text": "GitHub link.", "paragraph_index": 28}, {"url": "http://mivia.unisa.it/datasets-request/", "anchor_text": "Mivia website.", "paragraph_index": 30}, {"url": "https://twitter.com/cdossman", "anchor_text": "Twitter", "paragraph_index": 38}, {"url": "https://www.linkedin.com/in/christopherdossman/", "anchor_text": "LinkedIn", "paragraph_index": 38}], "all_paragraphs": ["At Wonder Technologies, we have spent a lot of time building Deep learning systems that understand the world through audio. From deep learning based voice extraction to teaching computers how to read our emotions, we needed to use a wide set of data to deliver APIs that worked even in the craziest sound environments. Here is a list of datasets that I found pretty useful for our research and that I've personally used to make my audio related models perform much better in real-world environments.", "Trying to build a custom dataset? Not sure where to start? Join me for a 30-minute one on one to talk about your project. Sign up for a time slot", "FMA is a dataset for music analysis. The dataset consists of full-length and HQ audio, pre-computed features, and track and user-level meta-data. It is an open dataset created for evaluating several tasks in Music Information Retrieval (MIR).", "The Million Song Dataset is a freely-available collection of audio features and meta-data for a million contemporary popular music tracks. The core of the dataset is the feature analysis and meta-data for one million songs. The dataset does not include any audio, only the derived features. The sample audio can be fetched from services like 7digital, using the code provided by Columbia University. The size of this dataset is about 280 GB.", "Top 6 Cheat Sheets Novice Machine Learning Engineers Need", "This one was created to solve the task of identifying spoken digits in audio samples. It\u2019s an open dataset so the hope is that it will keep growing as people keep contributing more samples. Currently, it contains the below characteristics: 1) 3 speakers 2) 1,500 recordings (50 of each digit per speaker) 3) English pronunciations. This is a really small set- about 10 MB in size.", "This dataset is a large-scale corpus of around 1000 hours of English speech. The data has been sourced from audio books from the LibriVox project and is 60 GB in size.", "VoxCeleb is a large-scale speaker identification dataset. It contains around 100,000 utterances by 1,251 celebrities, extracted from You Tube videos. The data is mostly gender balanced (males comprise of 55%). The celebrities span a diverse range of accents, professions, and age. There is no overlap between the development and test sets. It\u2019s an intriguing use case for isolating and identifying which superstar the voice belongs to.", "This set is 150 MB in size and has about 2000 hours of speech.", "This is a corpus of aligned spoken Wikipedia articles from the English, German, and Dutch Wikipedia. Hundreds of hours of aligned audio and annotations can be mapped back to the original HTML. The entire set is about 38 GB in size available in both audio and without audio format.", "40,000 spoken captions of 8,000 natural images, 4.2 GB in size. This corpus was collected in 2015 to investigate multi-modal learning schemes for unsupervised speech pattern discovery.", "Audio transcription of TED talks. 1495 TED talks audio recordings along with full-text transcriptions of those recordings, created by Laboratoire d\u2019Informatique de l\u2019Universit\u00e9 du Maine (LIUM).", "The dataset (1.4 GB) has 65,000 one-second long utterances of 30 short words, by thousands of different people, contributed by members of the public through the AIY website. It\u2019s released under a Creative Commons-BY 4.0 license and will continue to grow in future releases as more contributions are received. The dataset is designed to let you build basic but useful voice interfaces for applications, with common words like \u201cYes\u201d, \u201cNo\u201d, digits and directions included. The infrastructure used to create the data has been open sourced too, and we hope to see it used by the wider community to create their own versions, especially to cover under served languages and applications.", "Common Voice (12 GB is size) is a corpus of speech data read by users on the Common Voice website, and based on text from a number of public domain sources like user-submitted blog posts, old books, movies, and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems.", "The Persian Consonant Vowel Combination (PCVC) Speech Dataset is a Modern Persian speech corpus for speech recognition and also speaker recognition. The dataset contains sound samples of Modern Persian combination of vowel and consonant phonemes from different speakers. Every sound sample contains just one consonant and one vowel So it is somehow labeled in phoneme level. This dataset contains 23 Persian consonants and 6 vowels. The sound samples are all possible combinations of vowels and consonants (138 samples for each speaker) with a length of 30000 data samples.", "If you want to use this dataset, reference to this paper:", "Saber MalekzadeH, Mohammad Hossein Gholizadeh, Seyed Naser Razavi \u201cFull Persian Vowel recognition with MFCC and ANN on PCVC speech dataset\u201d 5th International conference of electrical engineering, computer science and information technology, Iran, Tehran, 2018. (PDF)", "Clean speech dataset of accented English. Useful for instances in which you expect to need robustness to different accents or intonations.", "This is a noisy speech recognition challenge dataset (~4GB in size). The dataset contains real simulated and clean voice recordings. Real being actual recordings of 4 speakers in nearly 9000 recordings over 4 noisy locations, simulated is generated by combining multiple environments over speech utterances and clean being non-noisy recordings.", "You can download the dataset from here.", "English-only speech data used most recently in the Deep Speech paper from Baidu.", "The training data belongs to 20 Parkinson\u2019s Disease (PD) patients and 20 healthy subjects. From all subjects, multiple types of sound recordings (26) are taken for this 20 MB set.", "The ultimate goal of the Zero Resource Speech Challenge is to construct a system that learns an end-to-end Spoken Dialog (SD) system, in an unknown language, from scratch, using only information available to a language learning infant. \u201cZero resource\u201d refers to zero linguistic expertise (e.g., orthographic/linguistic transcriptions), not zero information besides audio (visual, limited human feedback, etc). The fact that 4-year-olds spontaneously learn a language without supervision from language experts show that this goal is theoretically reachable.", "This 38.7 GB dataset helps predict which letter-name was spoken \u2014 a simple classification task.", "The Arabic Speech Corpus (1.5 GB) is a Modern Standard Arabic (MSA) speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of more than 3.7 hours of MSA speech aligned with recorded speech on the phoneme level. The annotations include word stress marks on the individual phonemes. This Speech corpus has been developed as part of PhD work carried out by Nawar Halabiat the University of Southampton. The corpus was recorded in South Levantine Arabic (Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.", "The TIMIT corpus (440 MB) of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. It includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16 kHz speech waveform file for each utterance.", "Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Each utterance in a dialogue has been labeled with\u2014 Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. Download here", "Getting more data for your algorithms is one way to increase accuracy. Explore a few more cheats in Deep Learning Performance Cheat Sheet", "An expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. To download this set, click on this GitHub link.", "6,000 events of surveillance applications, namely glass breaking, gunshots, and screams. The events are divided into a training set composed of 4,200 events and a test set composed of 1,800 events.", "To download this dataset, you must register yourself on the Mivia website.", "This page tries to maintain a list of datasets suitable for environmental audio research. In addition to the freely available dataset, also proprietary and commercial datasets are listed here for completeness. In addition to the datasets, also some of the on-line sound services are listed at the end of the page. These services can be used to form new datasets for special research needs.", "The datasets are divided into two tables:", "The AudioSet Ontology is a hierarchical collection of over 600 sound classes and we have filled them with 297,159 audio samples from Freesound. This process generated 678,511 candidate annotations that express the potential presence of sound sources in audio clips. FSD includes a variety of everyday sounds, from human and animal, sounds to music and sounds made by things, all under Creative Commons licenses. By creating this dataset, we seek to promote research that will enable machines to hear and interpret sound similarly to humans.", "Freesound is a platform for the collaborative creation of audio collections labeled by humans and based on Freesound content.", "The dataset (6 GB) is called UrbanSound and contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes namely: Air Conditioner, Car Horn, Children Playing, Dog bark, Drilling Engine, Idling, Gun Shot, Jackhammer, Siren and Street Music The attributes of data are as follows: ID \u2014 Unique ID of sound excerpt Class \u2014 type of sound.", "This dataset contains 1302 labeled sound recordings. Each recording is labeled with the start and end times of sound events from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. Each recording may contain multiple sound events, but for each file, only events from a single class are labeled. The classes are drawn from the urban sound taxonomy.", "In collaboration with the IEEE Signal Processing Society, a research data challenge was introduced to create a robust and scalable bird detection algorithm. This challenge contained new datasets (5.4 GB) collected in real live bio-acoustics monitoring projects, and an objective, standardized evaluation framework.", "Let\u2019s also connect on Twitter or LinkedIn", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning Engineer, Teacher, and Entrepreneur"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb45b88cd4ad&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://cdossman.medium.com/?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": ""}, {"url": "https://cdossman.medium.com/?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "Christopher Dossman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbc670b9a1aca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&user=Christopher+Dossman&userId=bc670b9a1aca&source=post_page-bc670b9a1aca----b45b88cd4ad---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb45b88cd4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb45b88cd4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://calendly.com/cdossman/cdossman-consultation", "anchor_text": "Sign up for a time slot"}, {"url": "https://github.com/mdeff/fma", "anchor_text": "Free Music Archive"}, {"url": "https://en.wikipedia.org/wiki/Music_information_retrieval", "anchor_text": "Music Information Retrieval (MIR)"}, {"url": "https://labrosa.ee.columbia.edu/millionsong/", "anchor_text": "Million Song Dataset"}, {"url": "https://medium.com/ai\u00b3-theory-practice-business/top-6-cheat-sheets-novice-machine-engineers-need-5ea43d1be3de", "anchor_text": "Top 6 Cheat Sheets Novice Machine Learning Engineers Need"}, {"url": "https://github.com/Jakobovski/free-spoken-digit-dataset", "anchor_text": "Free Spoken Digit Dataset"}, {"url": "http://www.openslr.org/12/", "anchor_text": "LibriSpeech"}, {"url": "http://www.robots.ox.ac.uk/~vgg/data/voxceleb/", "anchor_text": "VoxCeleb"}, {"url": "https://nats.gitlab.io/swc/", "anchor_text": "The Spoken Wikipedia Corpora"}, {"url": "https://groups.csail.mit.edu/sls/downloads/flickraudio/", "anchor_text": "Flickr Audio Caption Corpus"}, {"url": "http://www.openslr.org/51/", "anchor_text": "TED-LIUM"}, {"url": "http://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html", "anchor_text": "Speech Commands Dataset"}, {"url": "https://www.kaggle.com/mozillaorg/common-voice/home", "anchor_text": "Common Voice"}, {"url": "http://voice.mozilla.org/", "anchor_text": "Common Voice website"}, {"url": "https://github.com/S-Malek/PCVC", "anchor_text": "Persian Consonant Vowel Combination (PCVC) Speech Dataset"}, {"url": "https://scholar.google.com/citations?user=AVMa3t0AAAAJ&hl=en#d=gs_md_cita-d&p=&u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DAVMa3t0AAAAJ%26citation_for_view%3DAVMa3t0AAAAJ%3AIjCSPb-OGe4C%26tzom%3D-270", "anchor_text": "Saber MalekzadeH, Mohammad Hossein Gholizadeh, Seyed Naser Razavi \u201cFull Persian Vowel recognition with MFCC and ANN on PCVC speech dataset\u201d 5th International conference of electrical engineering, computer science and information technology, Iran, Tehran, 2018."}, {"url": "http://bayanbox.ir/download/2723849504007807268/Full-Persian-Vowel-recognition-with-MFCC-and-ANN-on-PCVC-speech-dataset.pdf", "anchor_text": "PDF"}, {"url": "http://www.voxforge.org/", "anchor_text": "VoxForge"}, {"url": "http://spandh.dcs.shef.ac.uk/chime_challenge/data.html", "anchor_text": "CHIME"}, {"url": "https://archive.org/details/chime-home", "anchor_text": "here."}, {"url": "https://catalog.ldc.upenn.edu/LDC2002T43", "anchor_text": "2000 HUB5 English"}, {"url": "https://archive.ics.uci.edu/ml/datasets/Parkinson+Speech+Dataset+with++Multiple+Types+of+Sound+Recordings", "anchor_text": "Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set"}, {"url": "https://github.com/bootphon/zerospeech2017", "anchor_text": "Zero Resource Speech Challenge"}, {"url": "https://data.world/uci/isolet", "anchor_text": "ISOLET Data Set"}, {"url": "http://en.arabicspeechcorpus.com/", "anchor_text": "Arabic Speech Corpus"}, {"url": "https://github.com/philipperemy/timit/blob/master/README.md", "anchor_text": "TIMIT Corpus"}, {"url": "https://github.com/SenticNet/MELD", "anchor_text": "Multimodal EmotionLines Dataset (MELD)"}, {"url": "https://github.com/SenticNet/MELD", "anchor_text": "Download here"}, {"url": "https://towardsdatascience.com/deep-learning-performance-cheat-sheet-21374b9c4f45", "anchor_text": "Deep Learning Performance Cheat Sheet"}, {"url": "https://research.google.com/audioset/", "anchor_text": "AudioSet"}, {"url": "https://github.com/audioset/ontology", "anchor_text": "GitHub link."}, {"url": "http://mivia.unisa.it/datasets/audio-analysis/mivia-audio-events/", "anchor_text": "Mivia Audio Events Dataset"}, {"url": "http://mivia.unisa.it/datasets-request/", "anchor_text": "Mivia website."}, {"url": "http://www.cs.tut.fi/~heittolt/datasets", "anchor_text": "Environmental Audio Datasets"}, {"url": "https://datasets.freesound.org/fsd/", "anchor_text": "FSD: a dataset of everyday sounds"}, {"url": "https://datasets.freesound.org/", "anchor_text": "Freesound"}, {"url": "https://www.kaggle.com/pavansanagapati/urban-sound-classification", "anchor_text": "Urban Sound Classification"}, {"url": "https://urbansounddataset.weebly.com/urbansound.html", "anchor_text": "Urban Sound Dataset"}, {"url": "http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/", "anchor_text": "Bird Audio Detection challenge"}, {"url": "https://twitter.com/cdossman", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/christopherdossman/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b45b88cd4ad---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b45b88cd4ad---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b45b88cd4ad---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----b45b88cd4ad---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/programming?source=post_page-----b45b88cd4ad---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb45b88cd4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&user=Christopher+Dossman&userId=bc670b9a1aca&source=-----b45b88cd4ad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb45b88cd4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&user=Christopher+Dossman&userId=bc670b9a1aca&source=-----b45b88cd4ad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb45b88cd4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb45b88cd4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b45b88cd4ad---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b45b88cd4ad--------------------------------", "anchor_text": ""}, {"url": "https://cdossman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://cdossman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Christopher Dossman"}, {"url": "https://cdossman.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbc670b9a1aca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&user=Christopher+Dossman&userId=bc670b9a1aca&source=post_page-bc670b9a1aca--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffe6b86ac765b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-lakes-worth-of-audio-datasets-b45b88cd4ad&newsletterV3=bc670b9a1aca&newsletterV3Id=fe6b86ac765b&user=Christopher+Dossman&userId=bc670b9a1aca&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}