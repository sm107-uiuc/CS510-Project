{"url": "https://towardsdatascience.com/evolution-of-natural-language-processing-8e4532211cfe", "time": 1683015572.6994271, "path": "towardsdatascience.com/evolution-of-natural-language-processing-8e4532211cfe/", "webpage": {"metadata": {"title": "The Evolution of Natural Language Processing | Towards Data Science", "h1": "Evolution of Natural Language Processing", "description": "Understand how NLP has evolved to the point of the huge and awe-inspiring Transformer models such as BERT and GPT-3 in use today with easy to grasp visuals."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw", "anchor_text": "YouTube here", "paragraph_index": 46}, {"url": "https://twitter.com/jamescalam", "anchor_text": "Twitter", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Efficient Estimation of Word Representations in Vector Space", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Neural Machine Translation by Jointly Learning to Align and Translate", "paragraph_index": 49}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs", "paragraph_index": 52}], "all_paragraphs": ["Attention is all you need. That is the name of the 2017 paper that introduced attention as an independent learning model \u2014 the herald of our now transformer dominant world in natural language processing (NLP).", "Transformers are the new cutting-edge in NLP, and they may seem somewhat abstract \u2014 but when we look at the past decade of developments in NLP they begin to make sense.", "We will cover these developments, and look at how they have led to the Transformers being used today. This article makes no assumptions in you already understanding these concepts \u2014 we will build an intuitive understanding without getting overly technical.", "NLP really blew up with the 2013 paper introducing word2vec by Mikolov et al [2]. This introduced a way to represent similarity and relationships between words through the use of word vectors.", "These initial word vectors contained a dimensionality of 50\u2013100 values. The encoding mechanism of these vectors meant that similar words would be grouped together (Monday, Tuesday, etc) \u2014 and calculations on the vector space could produce genuinely insightful relationships.", "A well-known example is that of taking the vector for King, subtracting the vector Man, and adding the vector Woman resulting in the nearest datapoint being Queen.", "During this boom in NLP, the recurrent neural network (RNN) quickly became the favorite for most language applications. RNNs suited language well thanks to their recurrence.", "This recurrence allowed the neural nets to consider the order of words and their effect on preceding and subsequent words \u2014 allow the nuances of human language to be better represented.", "Although we didn\u2019t see their popular usage until 2013 \u2014 the concept and methodology of RNNs were being discussed across several papers in the 80s [2], [3].", "RNNs came with their problems, primarily the vanishing gradient problem. The recurrence of these networks means that they are by nature very deep networks with many points containing an operation between the incoming data and the neuron weight.", "When calculating the error of the network, and using that to update the network weights, we walk back through the network updating weight after weight.", "If the update gradient is a small number, we are multiplying an increasingly smaller and smaller number \u2014 meaning the full network either takes a very long time to train or simply does not work.", "On the other hand, if our weight recurring value is too high \u2014 we suffer from the exploding gradient problem. Here, the network weights will oscillate without learning any meaningful representation.", "The solution to the vanishing gradients problem came with the introduction of long short-term memory (LSTM) units.", "LSTMs introduced an additional stream of information down the chain of time-states with a minimal number of transformations controlled by \u2018gates\u2019.", "This allowed long-term dependencies to be learned by allowing information from much earlier in a sequence to be retained and applied to states much later in the sequence.", "Very quickly recurrent encoder-decoder models were complemented with additional hidden states and neural network layers \u2014 these produced the attention mechanism.", "Adding encoder-decoder networks allowed the output layers of a model to not only receive the final-state of the RNN units \u2014 but to also receive information from every state of the input layer, creating an \u2018attention\u2019 mechanism.", "Using this approach, we find that similarity between the encoder and decoder states will result in a higher weight \u2014 producing outcomes like that of the French translation image above.", "With this encoder-decoder implementation three tensors, the Query, Key, and Value are used in the attention operation. The Query is pulled from the hidden state of the decoder at every time-step \u2014 alignment between this and the Key-Value tensors is assessed to produce the context vector.", "The context vector is then passed back into the decoder where is used to produce a prediction for that time-step. This process is repeated for every time-step in the decoder space.", "As we said in the introduction, it all started with the 2017 \u2018Attention Is All You Need\u2019 paper [5]. You may have guessed it already, this paper introduced the idea that we don\u2019t need to use these complex convolutional or recurrent neural networks alongside attention \u2014 attention is in fact, all you need.", "Self-attention was a key factor for this to function. It meant that whereas before the Query came from the output decoder, it is now generated directly from the input values alongside Key and Value.", "Because the Query, Key, and Value are all produced by the input, we are able to encode the alignment between different parts of the same input sequence. If we take the image above, we can see that changing the final word from tired to wide shifts the attention focus from animal to street.", "This allows the attention mechanism to encode relationships between all of the words in the input data.", "The next big change to the attention mechanism was the addition of multiple attention heads \u2014 essentially many self-attention operations performed in parallel and initialized with different weights.", "Without multi-head attention, the A. Vaswani et al. transformer model actually performed worse than many of its predecessors [5].", "The parallel mechanism allowed the model to represent several subspaces of the same sequence. These different levels of attention were then concatenated and processed by a linear unit.", "The input to a Transformer model is not sequential like RNNs. In the past, it was this sequential operation allowed us to consider the position and order of words.", "To maintain the positional information of words a positional encoding is added to the word embedding before entering the attention mechanism.", "The approach taken in the Attention Is All You Need paper was to produce a different sinusoidal function for every dimension in the embedding dimension.", "Remember before we said word2vec introduced to the concept of representing a word as many numbers in a 50 to 100-dimensional vector? Here, in the Vaswani et al. paper, they use the same idea but to represent the position of a word.", "However, this time \u2014 rather than calculating the vector values using an ML model, the values are calculated using modified sinusoidal functions.", "Each index of the vector is assigned an alternating sine-cosine-sine function (index 0 is sine, index 1 is cosine). Next, as the index value increases away from zero towards d (the embedding dimensionality) the frequency of the sinusoidal function decreases.", "We can take the same unruly sinusoidal plot from above, add in the 512 embedding dimensionality used in the A. Vaswani at al. paper and map these onto an easier to understand heatmap:", "We can see the higher frequency in the lower embedding dimensions (left) \u2014 which decreases as the embedding dimension increases. Around dimension 24, the frequency has decreased so much that we no longer see any change in the remaining (alternating) sine-cosine waves.", "These positional encodings are then added to the word embeddings.", "As a side note, this means that both the word embedding dimensionality and positional encoding dimensionality must match.", "The resultant architecture of these changes to the attention model produced the world\u2019s first Transformer.", "Beyond the word embedding, positional encodings, and multi-head self-attention operations already discussed \u2014 the model is reasonably easy to comprehend.", "We have the addition and normalization layers where we simply add two matrices together and then normalize them. And there are the vanilla feed-forward neural nets.", "Finally, we output our tensors into the linear layer. This is a fully connect neural net which maps onto a logits vector \u2014 a big vector where each index maps to a specific word and the values contained inside are the probabilities of each respective word.", "The softmax function then outputs the highest probability index, which maps to our highest probability word.", "And that is all there is to it (which I admit, is a lot).", "The current state-of-the-art in NLP still uses transformers, albeit with some weird and wonderful modifications and additions. Nonetheless, the core concept is still the same even with models such as GPT-3 and BERT.", "I\u2019m confident that the future of NLP will be every bit as diverse as it\u2019s past and we will see some truly fascinating, and quite frankly world-changing advances in the coming years \u2014 it\u2019s a very exciting space.", "I hope this article has helped you better understand the basics of transformer models and why they\u2019re so powerful. If you\u2019d like more, I post programming/ML videos on YouTube here!", "If you have any questions, ideas, or suggestions \u2014 get in touch via Twitter or in the comments below.", "[1] T. Mikolov et al., Efficient Estimation of Word Representations in Vector Space (2013), ICLR", "[4] D. Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate (2015), ICLR", "*All images are by the author except where stated otherwise", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Freelance ML engineer learning and writing about everything. I post a lot on YT https://www.youtube.com/c/jamesbriggs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8e4532211cfe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jamescalam.medium.com/?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "James Briggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----8e4532211cfe---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e4532211cfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e4532211cfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://unsplash.com/@owlixir?utm_source=medium&utm_medium=referral", "anchor_text": "Thyla Jane"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw", "anchor_text": "YouTube here"}, {"url": "https://twitter.com/jamescalam", "anchor_text": "Twitter"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Efficient Estimation of Word Representations in Vector Space"}, {"url": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf", "anchor_text": "Learning Internal Representations by Error Propagation"}, {"url": "https://cseweb.ucsd.edu/~gary/258/jordan-tr.pdf", "anchor_text": "Serial Order: A Parallel Distributed Processing Approach"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need"}, {"url": "https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM", "anchor_text": "\ud83e\udd16 NLP With Transformers Course"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8e4532211cfe---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/technology?source=post_page-----8e4532211cfe---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8e4532211cfe---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----8e4532211cfe---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----8e4532211cfe---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e4532211cfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&user=James+Briggs&userId=b9d77a4ca1d1&source=-----8e4532211cfe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e4532211cfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&user=James+Briggs&userId=b9d77a4ca1d1&source=-----8e4532211cfe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e4532211cfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8e4532211cfe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8e4532211cfe---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8e4532211cfe--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8e4532211cfe--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8e4532211cfe--------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Briggs"}, {"url": "https://jamescalam.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.6K Followers"}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F75e31c56d187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolution-of-natural-language-processing-8e4532211cfe&newsletterV3=b9d77a4ca1d1&newsletterV3Id=75e31c56d187&user=James+Briggs&userId=b9d77a4ca1d1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}