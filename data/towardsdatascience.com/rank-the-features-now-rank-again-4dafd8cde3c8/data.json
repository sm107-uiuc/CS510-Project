{"url": "https://towardsdatascience.com/rank-the-features-now-rank-again-4dafd8cde3c8", "time": 1682999791.251107, "path": "towardsdatascience.com/rank-the-features-now-rank-again-4dafd8cde3c8/", "webpage": {"metadata": {"title": "Rank the Features, now rank again | by Henry Heberle, PhD | Towards Data Science", "h1": "Rank the Features, now rank again", "description": "Here I discuss methods to rank features in 40 ways and a difficult case with an unstable model, caused by the data having many more variables than samples. As a bonus, you will know a bit of a field\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/", "anchor_text": "Scikit-learn", "paragraph_index": 33}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif", "anchor_text": "information gain", "paragraph_index": 34}, {"url": "https://github.com/heberleh/proteins-stability", "anchor_text": "https://github.com/heberleh/proteins-stability", "paragraph_index": 37}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3536906/", "anchor_text": "This", "paragraph_index": 55}, {"url": "https://heberleh.github.io/", "anchor_text": "https://heberleh.github.io/", "paragraph_index": 97}], "all_paragraphs": ["Here I discuss methods to rank features in 40 ways and a difficult case with an unstable model, caused by the data having many more variables than samples.", "As a bonus, you will know a bit of a field applied to Cancer Biology and learn about the characteristic of data sets in this area. I explain a Proteomics data set, which we used to validate an approach to find Biomarker candidates in Discovery Proteomics.", "You may read the entire article or jump to the following sections:", "The discovery of a biomarker is of great importance to Cancer Research. We can use them to, for instance, guide experiments for drug discovery, new treatment approaches and tests that can identify if there is a tumour or its type.", "Collecting saliva to identify a cancer is much less invasive than a biopsy.", "A non-invasive test is generally preferred and can result in faster diagnosis because it is easy to perform it more frequently. For instance, if they have a history of oral cancer in their family or have habits that increase their chances to have cancer, they could test their saliva every year. People try to avoid or delay invasive or complicated tests.", "Different quantities of proteins found in our saliva can predict an outcome.", "Imagine if a small change in the data leads to a very different rank of features.", "With a large sample, you would expect that at least the ranks would be similar. If the sample is small and has many features, you can\u2019t expect that.", "We cannot trust results here. Our sample is too small. Yet, we need to decide what are the best features for our problem.", "If we run one ranker with all observations, then remove one observation and run the ranker again, we would end up with two different ranks. Which one do we choose?", "If I use a T test, Wilcoxon test or Kruskal Wallis test on all observations, remove one observation and run again, we would end up with different p-values. Which one do we trust?", "Look at Figure 1 and read the legend to understand it better.", "Because our sample is too small, there is no right answer!", "The way we embraced the problem was by using what we had, visualizing the results and taking action. We couldn\u2019t have any statistical confidence, but we had power to grasp and understand our data.", "In the current biomarkers discovery pipeline, we start with many features and a few observations (small sample) because of technologies\u2019 limitations.", "We choose a small set of features first, then increase the sample size in follow-up experiments. The figure below illustrates the steps. \u201cAnalytes\u201d are our features, \u201cSamples\u201d are our observations. In the initial phases, we have many features and a few observations, which is a terrible scenario to run any machine learning or statistical method.", "After choosing the best features, we can increase the number of observations in further phases, such as in the Verification and Validation illustrated in the figure.", "Bad decisions here come with a price:", "If we initially chose a set of bad features, it means we wasted thousands of dollars on months or years of research! We at least prove those are not good candidates.", "Now add to this challenge the fact that the features are by nature correlated.", "Proteins\u2019 quantities in our body are dependent. They interact with each other to execute processes, like in a company, and influence the production of other proteins; similarly, employees can influence the company\u2019s success and other factors and, as a result, how many new employees it will hire.", "As you will see along the article, methods like SVM-RFE may give highly correlated features different weights, meaning similar proteins would appear in distant positions in the rank, leading to wrong conclusions and decisions.", "To address this problem, I ranked the features over and over, using different methods and cross-validation, and compared their scores. I end up with a list of high-scored features and their stability score computed by tracking the features\u2019 weights and position in ranks.", "There were many differences between ranks. While one method would say protein A is the best, another would say it's the worse.", "Having the average weight and knowing how the features behave when the data changes and among different rankers, I could decide what are the most important with some level of confidence.", "By adding true markers in the biological samples, we validated the relevance of our method to find potential biomarker candidates in Proteomics.", "The works I cite here are part of bigger projects made during my Ph.D. at the University of S\u00e3o Paulo, in collaboration with the Brazilian Biosciences National Laboratory, UNICAMP and others institutions.", "There are many ways to find the features\u2019 relevance as a number and to select a set of candidate features. I explain here the top-n selection approach and popular ways to weight and rank features.", "I ended up with 40 different perspectives (ranks) about my features.", "Product of statistical methods and combination of different classifiers to 6 ranking approaches.", "One common approach to select features to create a model is to select the top-N from a rank. Scikit-Learn, for instance, has SelectPercentile and SelectKBest that select features according to a percentile of the highest scores and according to the k highest scores, respectively.", "Scores, or weights, is a measure of importance. Examples are p-values from statistical tests and slopes (coefficients) from linear regression equations. To create a rank, we sort the features by using those numbers.", "I implemented six categories of ranking approaches combined with univariate and multivariate algorithms defined in the Scikit-learn.", "The first category, named Type 1\u2014 Univariate, is composed by univariate methods, such as t test and information gain.", "The other categories are based on multivariate models:", "Each of the multivariate-based ranking methods mentioned above (Types 2 to 7) was used in combination with different classifiers.", "I implemented the scripts (https://github.com/heberleh/proteins-stability) in python and selected classical machine learning tools from scikit-learn. Only classifiers that implement the features weights were considered:", "A total of 40 rankers were defined, they are the combinations of the above ranking approaches and classifiers.", "In the next section I explain how these 40 rankers generated hundreds of ranks to be interpreted and help the decision on which features we should consider in the next steps of the research.", "While here we have a small data set. You may think of ways to apply these ranking methods to bigger data sets, for instance, by using sampling and parallelism. In case you have thousands of observations and features, you could:", "In this project, we defined an analytical pipeline combining approaches for ranking features, identifying candidate signatures evaluated by simple and double cross validation.", "The general idea of the pipeline is illustrated and explained as follows:", "Speaking it more generally, it creates sets of features with the ones that were considered relevant by some classifier/ranker algorithm. Then, it tests these set of features by building classifiers and estimating their prediction power with cross-validation.", "The more a feature is selected, the greater is its importance.", "This is like a Bayesian approach, where we would think of the probability of a feature to be in a high accuracy model.", "Now, there is an outer-loop in the pipeline, a k-fold sampling. This will create different datasets and for each loop we can check how those frequencies are behaving with the changes in the data set.", "If, for instance, one feature appears in many good classification models only in the first loop, this means this feature is actually not relevant.", "Classifiers are built here with each set of feature (signature) and their accuracy is measured to tell if the respective signature is good or not.", "Rankers are used to select the features that are used to create signatures. For instance, only top-20 features that appear in ranks are considered. This heuristic reduces the computational time.", "Rankers are also used to compute the frequency of proteins in top-10 positions. Selecting top-10 features is a rule of thumb in Discovery Proteomics \u2014 those 10 proteins are then analysed in what we call Targeted Proteomics, they are our targets and because it is a small set, we can increase precision and the number of observations.", "In the following images you can see how frequent is each of the proteins in the set of top-10 positioned proteins, and good, even better and best sets of signatures.", "The good, even better and best sets were built with flexibility in mind. It would not be fair to select only models with maximum accuracy, for instance, 95% (there were hundreds with maximum accuracy). Because the number of samples is small, we should not me too strict.", "Therefore, the \u201cgood\u201d and \u201ceven better\u201d sets represent this flexibility; for instance, selecting models that have accuracy > 90%, and then selecting >93%, and finally 95%.", "Tracking the behaviour of our features on all those sets of candidate signatures gives us more information and confidence about the data. In our study, we found interesting patterns in all the analytical approaches.", "Finally, in the outer-loop, the best models are tested again with independent test sets \u2014 resulting in what we call a Double Cross Validation (DCV). This is an article on DCV.", "The main problem with a cross-validation (CV) when we have a small set is that the estimation is too over-optimistic. Performing a DCV helps us to understand how optimistic is the CV validation.", "In my experience, we are good if we get a DCV error < 40%. It means that your results have a higher chance to not be random than if you get a > 50% error.", "In fact, even with known true markers in our data set, we got DCV error > 60% when considering the best signatures. It means that, despite the signatures were working well when cross-validated, they performed poorly when tested with new observations.", "Therefore, if we aim to reduce over-fitting, we should not select the signatures with the maximum scores. This is one of the motivation to, instead, analyse the frequency of features in many models with different CV scores.", "In this section I explain how we validated our approach with one data set in Discovery Proteomics and how the true markers (features) were successfully prioritized by our stability pipeline.", "If we selected the signatures with the maximum CV score, we would end up with a terrible DCV score \u2014 as we did. For this reason, we implemented the analytical pipeline where we track features\u2019 scores under different perspectives.", "The general idea that I demonstrated in this project is that when you have a small number of observations, you need to grasp the data and analyse it under different conditions. Then, you need to choose the heuristic to prioritize your features having in mind that you should not be strict; that you should define different thresholds and compare them.", "True positive features (markers) are prioritized features that in fact are important to our problem. False positives would be features that were prioritized but in reality they do not contribute to discriminate our classes.", "Our goal was to analyse the behavior of the computational methods and the mass-spectrometry technology quantification in Discovery Proteomics (small sample & many features).", "Can we identify the true markers from our biological samples by quantifying their proteins and ranking them with computational methods?", "The answer to this question is YES and NO.", "NO, because if you chose the wrong computational methods they may not identify the true markers.", "In our experiments ranking the same data set with different rankers resulted in many disagreements. Proteins that were in top positions in some ranks appeared in the last positions in other ranks. If you chose the \u201cwrong group\u201d of rankers, you end up extremely biased.", "YES, because as I show in our approach, the true markers can be identified if you run many methods and combine information from different perspectives to prioritize your proteins.", "Here I show you how the frequencies calculated in our pipeline revealed the true markers as good candidates.", "Our true markers were defined in the biological samples before quantification.", "We added proteins in biological samples with the same concentration in each class; different concentrations in different classes.", "There are three classes: C (control), TR (tumor removed) and T (tumor).", "They refer to saliva samples of patients with no cancer, from patients that had a tumor removed, and patients with tumor. The quantification result for these proteins are shown below.", "As we can see, despite the proteins were added with the same concentration in each sample of a class, their quantified intensities vary. The average in each class is different: around -1 in class C, around 0 in class TR and around 1 in class T.", "By computing ranking scores and varying the data set we found clear patterns of stability that are shown and described in the images below.", "In summary, we found a group of proteins that were selected as good most of the time and that are shown in red in the heat map.", "We also found a group of proteins that appeared frequently in the top-10 positions of ranks. These proteins and their weights are represented by the box-plots. The box-plots also show how the rankers/data disagree. In B) we have a better plot of weights because we are considering only the best scores of each protein. In A), we see that all proteins had bad scores at some point.", "After scoring the proteins and ranking them, the pipeline creates and tests the protein signatures. The following images show us how the proteins were selected by the pipeline. The more a protein is selected (the bigger is the bar), the more stable and important it is.", "We found that our marker proteins, true positives, are among the proteins with the highest frequencies in top-10 positions and in good/even better/best signatures.", "The image below shows how frequently each protein was selected among the first 10 positions in each computed rank, in each fold (color). We can see that the proteins that appear in the middle of the plot were not so frequently positioned among the top-10 and that most of time they appeared among the top-10 only in some folds. This means that depending on each observation you remove from the training set, the protein does not appear among the top-10 anymore, revealing instability and higher risk if we chose this protein to use in follow-up experiments.", "The image below is similar the one above. Here, instead of calculating the frequency in the top-10 positions, we calculate the frequency in signatures that were selected to form the groups good, even better and best. Again, the true markers were found among the best proteins according to the frequency criteria.", "Yes. The more I grasp this small data set the more I overfit my models. On the other hand, I have to. That is the only option I can think of to understand how small changes in the data set will change the prediction power. We are assuming here that we have a great set of observations, but that can contain one or two observations that are outliers.", "Performing a simple k-fold sampling shows us the variance. Here, we use an outer k-fold with many k-fold inside it, simulating new samples that were never considered selecting the features. DCV is necessary if you want to estimate the power of selected features.", "In the end, you need to show ALL the results. Use the DCV error to explain to the experts the risk of false positives.", "The DCV scheme also brought some problems to the pipeline as I explain below \u2014 skip it if you do not plan to use this specific pipeline as it is.", "Limitation: because correlated proteins were separated in the beginning of the pipeline in each execution of the k-fold sampling, there was a problem with the frequency of these proteins. One marker protein could be removed from some training sets and not removed from others, resulting in lower frequencies than they should be. In the current plots you can see that even with these proteins appearing fewer times, they got high frequencies. This happened because I added them back to create signatures IF its reference protein was selected in a good signature \u2014 replacing the reference protein with the removed protein in that signature specifically. This is not ideal because despite they are highly correlated, they may have different intensities. Similar behavior happened with the filtering steps, such as the filter by p-value and false discovery rate. One solution would be to remove the correlated proteins and filter before the outer k-fold sampling, but this is not ideal because in a DCV we do not want to apply any method before the k-fold sampling, to avoid biasing the feature selection process. If you remove features based on class-information before the DCV pipeline, you are biasing your results, once the independent test sets should not be part of any processing. Therefore, a better algorithm that could track the pairs, for instance, to always use the same protein as the reference, is necessary. Note that the current version runs in parallel, making it difficult to define a deterministic solution. Yet, the current approach that adds back the proteins in the middle of the pipeline worked well for the presented case.", "Our pipeline is not a perfect solution for the problem of prioritizing proteins. The perfect solution would be to increase the number of observations to thousands.", "Because we cannot increase the sample size, understanding the behavior of our data is important to make decisions.", "Visualization, re-sampling, averaging and the calculation of frequency of proteins to measure their stability and power helped to identify and understand how proteins and methods behave.", "By adding true markers to biological samples and quantifying all proteins using our Discovery Proteomics protocol, we locally validated our pipeline. It was capable of highlight the true markers as shown by the plots presented in this article.", "Here I named it a \u201clocal validation\u201d because many other studies with different data sets would be required to actual validate the approach to use with any Discovery Proteomics data.", "Despite that, our approach gives control and highlight information about the behavior of Machine Learning and Statistical methods, and on how each protein behave regarding the importance given by those methods.", "The comprehension of over-fitting is important, and so it is showing to all researchers involved in the project how we should not trust numbers and p-values when our sample size is smaller than the number of factors/variables.", "After understanding all the risks of false-positives and false-negatives, and how the methods and proteins are behaving in our analysis pipeline, we may proceed to the decision making with more confidence.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist & Visualization Engineer. Info: https://heberleh.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4dafd8cde3c8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@heberleh?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@heberleh?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "Henry Heberle, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f904f61e54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&user=Henry+Heberle%2C+PhD&userId=4f904f61e54b&source=post_page-4f904f61e54b----4dafd8cde3c8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dafd8cde3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dafd8cde3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://scholar.google.com/citations?user=dgLjTXYAAAAJ&hl=en#d=gs_md_cita-d&u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DdgLjTXYAAAAJ%26citation_for_view%3DdgLjTXYAAAAJ%3AmaZDTaKrznsC%26tzom%3D240", "anchor_text": "<here>"}, {"url": "http://www.teses.usp.br/teses/disponiveis/55/55134/tde-15102019-145225/", "anchor_text": "http://www.teses.usp.br/teses/disponiveis/55/55134/tde-15102019-145225/"}, {"url": "https://github.com/heberleh/proteins-stability", "anchor_text": "https://github.com/heberleh/proteins-stability"}, {"url": "https://scikit-learn.org/stable/", "anchor_text": "Scikit-learn"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif", "anchor_text": "information gain"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE", "anchor_text": "RFE"}, {"url": "https://github.com/scikit-learn-contrib/stability-selection", "anchor_text": "https://github.com/scikit-learn-contrib/stability-selection"}, {"url": "https://github.com/heberleh/proteins-stability", "anchor_text": "https://github.com/heberleh/proteins-stability"}, {"url": "https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf", "anchor_text": "here"}, {"url": "https://ieeexplore.ieee.org/document/709601", "anchor_text": "Random Subspaces"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3536906/", "anchor_text": "This"}, {"url": "https://doi.org/10.18632/oncotarget.6018", "anchor_text": "https://doi.org/10.18632/oncotarget.6018"}, {"url": "https://doi.org/10.1038/s41467-018-05696-2", "anchor_text": "https://doi.org/10.1038/s41467-018-05696-2"}, {"url": "https://doi.org/10.1186/1471-2105-13-69", "anchor_text": "https://doi.org/10.1186/1471-2105-13-69"}, {"url": "https://doi.org/10.1074/mcp.M112.022566", "anchor_text": "https://doi.org/10.1074/mcp.M112.022566"}, {"url": "https://doi.org/10.1038/nbt1235", "anchor_text": "https://doi.org/10.1038/nbt1235"}, {"url": "https://doi.org/10.1186/1471-2105-13-S4-S22", "anchor_text": "https://doi.org/10.1186/1471-2105-13-S4-S22"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4dafd8cde3c8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4dafd8cde3c8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/feature-selection?source=post_page-----4dafd8cde3c8---------------feature_selection-----------------", "anchor_text": "Feature Selection"}, {"url": "https://medium.com/tag/biomarker?source=post_page-----4dafd8cde3c8---------------biomarker-----------------", "anchor_text": "Biomarker"}, {"url": "https://medium.com/tag/biology?source=post_page-----4dafd8cde3c8---------------biology-----------------", "anchor_text": "Biology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4dafd8cde3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&user=Henry+Heberle%2C+PhD&userId=4f904f61e54b&source=-----4dafd8cde3c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4dafd8cde3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&user=Henry+Heberle%2C+PhD&userId=4f904f61e54b&source=-----4dafd8cde3c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dafd8cde3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4dafd8cde3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4dafd8cde3c8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4dafd8cde3c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@heberleh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@heberleh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Henry Heberle, PhD"}, {"url": "https://medium.com/@heberleh/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "47 Followers"}, {"url": "https://heberleh.github.io/", "anchor_text": "https://heberleh.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f904f61e54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&user=Henry+Heberle%2C+PhD&userId=4f904f61e54b&source=post_page-4f904f61e54b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4f904f61e54b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frank-the-features-now-rank-again-4dafd8cde3c8&user=Henry+Heberle%2C+PhD&userId=4f904f61e54b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}