{"url": "https://towardsdatascience.com/train-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db", "time": 1683004357.3957498, "path": "towardsdatascience.com/train-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db/", "webpage": {"metadata": {"title": "Train a GPT-2 Transformer to write Harry Potter Books! | by Priya Dwivedi | Towards Data Science", "h1": "Train a GPT-2 Transformer to write Harry Potter Books!", "description": "Natural Language Processing is a field widely growing in popularity these days. A large number of companies worldwide are leveraging the power of Natural language processing and the innovations in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is All You Need", "paragraph_index": 0}, {"url": "https://talktotransformer.com/", "anchor_text": "Talk to Transformers", "paragraph_index": 1}, {"url": "https://github.com/priya-dwivedi/Deep-Learning/tree/master/GPT2-HarryPotter-Training", "anchor_text": "Github", "paragraph_index": 1}, {"url": "https://deeplearninganalytics.org/train-a-gpt-2-transformer-to-write-harry-potter-books/", "anchor_text": "my website here.", "paragraph_index": 3}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is All You Need", "paragraph_index": 4}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "\u201cOpenAI\u2019s GPT-2\u201d.", "paragraph_index": 5}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace\u2019s transformer repository", "paragraph_index": 14}, {"url": "https://github.com/priya-dwivedi/Deep-Learning/tree/master/GPT2-HarryPotter-Training", "anchor_text": "Github repo", "paragraph_index": 15}, {"url": "https://github.com/priya-dwivedi/Deep-Learning/blob/master/GPT2-HarryPotter-Training/data_cleanup/clean_harry_book.ipynb", "anchor_text": "here", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3", "anchor_text": "blog", "paragraph_index": 20}, {"url": "http://deeplearninganalytics.org/", "anchor_text": "http://deeplearninganalytics.org/.", "paragraph_index": 27}, {"url": "https://medium.com/@priya.dwivedi", "anchor_text": "https://medium.com/@priya.dwivedi", "paragraph_index": 28}], "all_paragraphs": ["Natural Language Processing is a field widely growing in popularity these days. A large number of companies worldwide are leveraging the power of Natural language processing and the innovations in the field to extract meaningful insights from text and to generate text. In the past couple of years, with Google Brain\u2019s Attention is All You Need paper, the transformers architecture has revolutionized this field even further. In almost all the classic NLP tasks like Machine translation, Question Answering, Reading Comprehension, Common Sense Reasoning and Summarization, a Transformer-based architecture beat the State-of-the-art. Since then, all tech giants like Google, Facebook, OpenAI, Microsoft have been experimenting with transformers in various applications.", "One such application that made headlines was the Language Generation task, wherein Transformers were able to generate meaningful text given a prompt. One of the first headliners was HuggingFace with their Talk to Transformers web page, where anyone could generate their own AI-generated text by giving a prompt. Here, we will explore how transformers are used in Language generation. Also later in the blog, we will share code for how to train a transformer language model on your own corpus. We trained a GPT-2 model on Harry Potter books. The trained model is able to generate text like Harry Potter books when presented with an input. See the example below. Full code is available on my Github.", "Interesting observations: 1. Model has learnt that Hagrid has large feet!, 2. Gilderoy Lockhart writes books, 3. New books can appear in Hogwarts bookshelves.", "Original full story published on my website here.", "A language model is a model which learns to predict the probability of a sequence of words. In simpler words, language models essentially predict the next word given some text. By training language models on specific texts, it is possible to make the model learn the writing style of that text. Although various kinds of language models existed in the past, they became much more powerful after the introduction of Transformers by the Google Brain team(\u201cAttention is All You Need\u201d).", "With the advent of transformers, multiple groups were able to create and train custom architectures for language models. One such group was the Open AI community who introduced GPT(short for Generative Pre-training Transformer). The GPT model was released in 2018, but unfortunately, soon after it\u2019s release, it was knocked off the GLUE leaderboard by BERT. But, in February, 2019, OpenAI scaled up their model by training on a whopping 1.5 billion parameters which in turn gave it human-like writing capabilities. It was named \u201cOpenAI\u2019s GPT-2\u201d.", "Transformers is the basic architecture behind the language models. A transformer mainly consists of two basic components: encoders and decoders.", "As seen in the diagram above, both Encoder and Decoders have modules that can be stacked together, as represented by Nx. Mainly, both Encoders and Decoders have the Feed-forward and Multi-Head attention components.", "The inputs and outputs are embedded into an n-dimensional space before passing them on to the components. One important step in the input and output components is the Positional Encoding wherein we provide information regarding the position of the word to the transformer. These encodings are added to the embeddings of each word and the resulting embeddings are passed to the transformer.", "An encoder block has multiple encoder blocks and the decoder block has the same number of decoder blocks. The number of blocks is a hyperparameter which can be tuned while training.", "The working of the encoder-decoder stack is described as follows:", "The self-attention layer in the encoder and decoders play a very important role in the processing of the words. It enables the model to look at other words in the input sequence to get a better understanding of the context of the current word.", "Apart from the Feed-forward and Attention component, decoders also have another attention layer (Masked multi-head attention) which helps the decoder focus on specific parts of the input sequence.", "OpenAI extended the Transformers concept for the Language generation task in two iterations: GPT and GPT-2. GPT architecture used 12-layer decoders with masked self-attention heads and trained for 100 epochs. With GPT-2 model, the vocabulary was expanded to 50,257 words. There was also an increase in the context size from 512 to 1024 tokens and a larger batchsize of 512 was used.", "In this blog, we will leverage the awesome HuggingFace\u2019s transformer repository to train our own GPT-2 model on text from Harry Potter books. We will provide a sentence prompt to the model and the model will complete the text. In order to train the model, we will feed all Harry Potter books for the model to learn from them.", "We have cloned the huggingface repo and updated the code to correctly perform language model training and inference. Please follow along on my Github repo.", "The first step is downloading all the harry potter books and preprocessing the text. We scraped the text from the first 4books and merged it together. Then we wrote a short piece of code to remove unnecessary text like the page numbers from the merged text. Finally the GPT-2 model needs both train and validation text. So we take first 90% of the data as training sample and the remaining as validation sample. The preprocessing code is here.", "To train the model we use the script \u2014 run_lm_finetuning.py. The script takes as input the model type and its size, as well as the preprocessed text. The script also provides a bunch of hyperparameters that can be tweaked in order to customize the training process. The code snippet for training is:", "The parameters used in the code is as follows:", "The parameters used here are explained as follows:", "We trained a medium GPT-2 model on the text of 4harry potter books. This model took only 10 min to train on a GTX 1080 Ti. The perplexity score of the trained model was 12.71. Read this blog to learn more about Perplexity score. But remember, lower the score, the better the model is.", "Once the model is trained, we can run inference using it. The inference script is run_generation.py", "For doing inference, the input text is first encoded through the tokenizer , then the result is passed through a generate function where the generation of text happens based on parameters like temperature, top-p and k values.", "The code snippet for doing inference is:", "Some additional parameters that can be tweaked are:", "One more example of model output is below. Very interesting to see the story around the cloaked figure that this model is creating.", "The advent of transformers has truly revolutionized many Natural language processing tasks, and language generation is one of them. The potential of a language generation model is huge and can be leveraged in many applications like chatbots, long answer generation, writing automated reports and many more. In this blog, we understood the working of transformers, how they are used in language generation and some examples of how anyone can leverage these architectures to train their own language model and generate text.", "I am extremely passionate about NLP, Transformers and deep learning in general. I have my own deep learning consultancy and love to work on interesting problems. I have helped many startups deploy innovative AI based solutions. Check us out at \u2014 http://deeplearninganalytics.org/.", "You can also see my other writings at: https://medium.com/@priya.dwivedi", "If you have a project that we can collaborate on, then please contact me through my website or at info@deeplearninganalytics.org", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fedf8b2e3f3db&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://priya-dwivedi.medium.com/?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": ""}, {"url": "https://priya-dwivedi.medium.com/?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "Priya Dwivedi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb040ce924438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&user=Priya+Dwivedi&userId=b040ce924438&source=post_page-b040ce924438----edf8b2e3f3db---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fedf8b2e3f3db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fedf8b2e3f3db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/WE7YfTGpXlg", "anchor_text": "https://unsplash.com/photos/WE7YfTGpXlg"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is All You Need"}, {"url": "https://talktotransformer.com/", "anchor_text": "Talk to Transformers"}, {"url": "https://github.com/priya-dwivedi/Deep-Learning/tree/master/GPT2-HarryPotter-Training", "anchor_text": "Github"}, {"url": "https://deeplearninganalytics.org/train-a-gpt-2-transformer-to-write-harry-potter-books/", "anchor_text": "my website here."}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is All You Need"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "\u201cOpenAI\u2019s GPT-2\u201d."}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "https://blog.openai.com/better-language-models/"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace\u2019s transformer repository"}, {"url": "https://github.com/priya-dwivedi/Deep-Learning/tree/master/GPT2-HarryPotter-Training", "anchor_text": "Github repo"}, {"url": "https://github.com/priya-dwivedi/Deep-Learning/blob/master/GPT2-HarryPotter-Training/data_cleanup/clean_harry_book.ipynb", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3", "anchor_text": "blog"}, {"url": "https://medium.com/huggingface/how-to-write-with-transformer-5ee58d6f51fa", "anchor_text": "https://medium.com/huggingface/how-to-write-with-transformer-5ee58d6f51fa"}, {"url": "http://deeplearninganalytics.org/", "anchor_text": "http://deeplearninganalytics.org/."}, {"url": "https://medium.com/@priya.dwivedi", "anchor_text": "https://medium.com/@priya.dwivedi"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers \u2014 Attention is all you need"}, {"url": "https://github.com/google-research/bert", "anchor_text": "BERT"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2 model"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face Repo"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----edf8b2e3f3db---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----edf8b2e3f3db---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----edf8b2e3f3db---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----edf8b2e3f3db---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/transformers?source=post_page-----edf8b2e3f3db---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fedf8b2e3f3db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&user=Priya+Dwivedi&userId=b040ce924438&source=-----edf8b2e3f3db---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fedf8b2e3f3db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&user=Priya+Dwivedi&userId=b040ce924438&source=-----edf8b2e3f3db---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fedf8b2e3f3db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fedf8b2e3f3db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----edf8b2e3f3db---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----edf8b2e3f3db--------------------------------", "anchor_text": ""}, {"url": "https://priya-dwivedi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://priya-dwivedi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Priya Dwivedi"}, {"url": "https://priya-dwivedi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.1K Followers"}, {"url": "http://www.deeplearninganalytics.org/", "anchor_text": "http://www.deeplearninganalytics.org/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb040ce924438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&user=Priya+Dwivedi&userId=b040ce924438&source=post_page-b040ce924438--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F373349a3bfb7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db&newsletterV3=b040ce924438&newsletterV3Id=373349a3bfb7&user=Priya+Dwivedi&userId=b040ce924438&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}