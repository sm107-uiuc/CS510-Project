{"url": "https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a", "time": 1683018357.321075, "path": "towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a/", "webpage": {"metadata": {"title": "Natural Language Generation Part 2: GPT2 and Huggingface | by George Dittmar | Towards Data Science", "h1": "Natural Language Generation Part 2: GPT2 and Huggingface", "description": "Tutorial on using huggingface and tensorflow for gpt2 based training and text generation."}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 0}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface", "paragraph_index": 0}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow", "paragraph_index": 0}, {"url": "https://pytorch.org/", "anchor_text": "Pytorch", "paragraph_index": 0}, {"url": "https://gist.github.com/GeorgeDittmar/5c57a35332b2b5818e51618af7953351", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/natural-language-generation-part-1-back-to-basics-2f0b2654624f?sk=8700385e4ff325da52a28597c44851a1", "anchor_text": "my last tutorial", "paragraph_index": 1}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)", "anchor_text": "Transformer", "paragraph_index": 1}, {"url": "https://jalammar.github.io/illustrated-gpt2/", "anchor_text": "Here", "paragraph_index": 1}, {"url": "https://www.reddit.com/r/investing/", "anchor_text": "/r/investing subreddit", "paragraph_index": 3}, {"url": "https://www.kaggle.com/jeet2016/us-financial-news-articles", "anchor_text": "US Financial News Articles", "paragraph_index": 3}, {"url": "https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Perplexity", "anchor_text": "perplexity", "paragraph_index": 11}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "here", "paragraph_index": 15}], "all_paragraphs": ["So it\u2019s been a while since my last article, apologies for that. Work and then the pandemic threw a wrench in a lot of things so I thought I would come back with a little tutorial on text generation with GPT-2 using the Huggingface framework. This will be a Tensorflow focused tutorial since most I have found on google tend to be Pytorch focused, or light on details around using it with Tensorflow. If you don\u2019t want to read my whole post and just see how it works, I have the following Colab notebook as an outline for people to reference here. This post will be basically going over whats in the notebook so should be easy to reference back and forth.", "In my last tutorial, I used Markov chains to learn n-gram probabilities from presidential speeches and used those probabilities to generate similar text output given new starting input. Now we will go a step further and utilize a more state of the art architecture to create text output that should be more accurate and realistic. If you haven't already heard about GPT-2, its a language model from OpenAI trained on a mass amount of data from the web using an architecture called the Transformer. Here is a good visual overview of the transformer architecture used by GPT-2 that should help give you intuition on how it works. GPT-2 is not the most advanced version of the language model from Open AI, but its one that has many reference implementations and frameworks to use compared to the newer GPT-3 model. As well its a version of the model that can run on Colab and is fairly straight forward to setup and hopefully even easier after this tutorial :)", "For our task we will create a model to generate financial article titles. If we started the task of training the language model from scratch we would need lots and lots of examples (GPT-2 was trained on 8 million web pages). Fine tuning from the pre-trained model means we don\u2019t need to use nearly the same amount to get decent results on our specific task.", "The plan is to get a decent amount of examples, couple hundred thousand, and then split them into train and eval sets. I decided to grab data from reddit titles in the /r/investing subreddit and titles extracted from US Financial News Articles dataset from Kaggle. Some of the examples from the joined dataset are not just finance related, since many financial news sites also report on non-financial events and the subreddit data has a mix of investing advice and questions.", "The titles pulled from reddit submissions are about 100k and the titles extracted from the Kaggle dataset are about another 179k. That should be enough examples so as to not over fit on our task and give us a rich set of possible text to generate from within the \u201cfinancial\u201d domain.", "The format of the data seems to make or break the training and output of these models I have found. For GPT-2 if you want to just generate a whole bunch of text, say a book or articles, you can throw all the examples into a single document with no special tokens between examples. However if you want to generate output that follows a certain pattern or prompt, you should add special tokens into the dataset to make it more clear what pattern GPT-2 should attempt to learn to output. Below is the basic format for an example in the dataset for our title generation task.", "Each example is then concatenated together as one long string. We don\u2019t have to add a start token for training since GPT-2 only needs the \u2018<|endoftext|>\u2019 token to split examples, but with this leading token we can then have the model generate new random output on each run when we prompt it with \u201c<|title|>\u201d first. You can set the start token to be whatever you want really, or have none at all, but I have found that setting these tokens to something that wont be likely to show up in the vocab of the data makes it easier to generate coherent text and you won\u2019t be as likely to fall into a repetitive cycle.", "The gist above shows the cell step that is used to create our train and eval sets. As you can see when we read in the dataset line by line, then append the <|title|> token to the input then rejoin with <|endoftext|> and write back out to their respective file. Now that we have these two files written back out to the Colab environment, we can use the Huggingface training script to fine tune the model for our task.", "For fine tuning GPT-2 we will be using Huggingface and will use the provided script run_clm.py found here. I tried to find a way to fine tune the model via TF model calls directly, but had trouble getting it to work easily so defaulted to using the scripts provided. Some things like classifiers can be trained directly via standard TF api calls, but the language models seem to not be fully supported when I started this work. Its possible newer versions of Huggingface will support this.", "The script above will run the fine tuning process using the medium sized GPT-2 model, though if you are using standard Colab you might only be able to run the small GPT-2 model due to resource limits on the vm. For myself I am using Colab Pro which gives me access to more powerful base machines and GPU\u2019s. Depending on your use case regular Colab may be sufficient or you can use GCP if you really need access to more powerful GPU instances for longer times. Transformer models are very computationally expensive due to their architecture, so when training on a GPU it can easily take hours or days with a large enough dataset.", "For the investing title dataset, 5 epochs on a p100 took over 3\u20134 hours while on a v100 it only took 1.5 to 2 hours depending on the settings I used. Its up to some luck it seems on which GPU you get when starting up your Colab instance. I found I was usually able to get a v100 every other day after a multi hour training session. One thing to call out in the above script call is that I am using mixed precision in the model training with the \u2014 fp16 argument. Using mixed precision shaved off about 30 mins of training time with no noticeable drop in model performance when compared to a single precision trained model on our data.", "At the end of the model training there is an eval step that happens which tells us our models perplexity. As you can see our title generation GPT-2 model gets us a perplexity score of around 10.6 which isn't bad considering it only ran for 5 epochs.", "So now that we have trained our new language model to generate financial news titles, lets give it a try! We will want to use the path to the directory that the script outputs the model file to, and load it up to see if it will output some great new finance article / reddit titles for us!", "To load into TF we will want to import the TFGPT2LMHeadModel and then call from_pretrained, making sure to set the from_pt flag to True. This way it will load the Pytorch model into TF compatible tensors. We will also use the pre-trained GPT-2 tokenizer for creating our input sequence to the model.", "The pre-trained tokenizer will take the input string and encode it for our model. When using the tokenizer also be sure to set return_tensors=\u201dtf\u201d. If we were using the default Pytorch we would not need to set this. With these two things loaded up we can set up our input to the model and start getting text output.", "After creating the input we call the models generate function. Huggingface has a great blog that goes over the different parameters for generating text and how they work together here. I suggest reading through that for a more in depth understanding. The below parameters are ones that I found to work well given the dataset, and from trial and error on many rounds of generating output. The one thing with language models is that you have to try a number of different parameter options to start to see some good output, and even then sometimes it takes many runs to get output that fits your task so do not be surprised if initial results are less than stellar.", "Below is some of the output that was generated by our investing title model given the \u201c<|title|>\u201d token as the prompt.", "From the generated examples above they look like believable article and reddit titles. Still sometimes when running you can get some funny output like the one below.", "Well, that was maybe a bit long of a post, but hopefully, you found it useful for learning how to use Huggingface to fine tune a language model and generate some text using a Tensorflow back end. Now with these techniques you can start to come up with different tasks and models for your own work/interests. For instance, after building this title model I decided to see if I can generate a title and use that title to generate some sort of article, to varying degrees of success. Try experimenting for your self and see what you can come up with!", "ML Engineer. Trying to work on some cool stuff."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff3acb35bc86a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://georgedittmar.medium.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": ""}, {"url": "https://georgedittmar.medium.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "George Dittmar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43dcb64ca1bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=post_page-43dcb64ca1bc----f3acb35bc86a---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3acb35bc86a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=-----f3acb35bc86a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3acb35bc86a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&source=-----f3acb35bc86a---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@aliissinisalu?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Aliis Sinisalu"}, {"url": "https://unsplash.com/s/photos/tech?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface"}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow"}, {"url": "https://pytorch.org/", "anchor_text": "Pytorch"}, {"url": "https://gist.github.com/GeorgeDittmar/5c57a35332b2b5818e51618af7953351", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/natural-language-generation-part-1-back-to-basics-2f0b2654624f?sk=8700385e4ff325da52a28597c44851a1", "anchor_text": "my last tutorial"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)", "anchor_text": "Transformer"}, {"url": "https://jalammar.github.io/illustrated-gpt2/", "anchor_text": "Here"}, {"url": "https://www.reddit.com/r/investing/", "anchor_text": "/r/investing subreddit"}, {"url": "https://www.kaggle.com/jeet2016/us-financial-news-articles", "anchor_text": "US Financial News Articles"}, {"url": "https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Perplexity", "anchor_text": "perplexity"}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "here"}, {"url": "https://gist.github.com/GeorgeDittmar/5c57a35332b2b5818e51618af7953351", "anchor_text": "https://gist.github.com/GeorgeDittmar/5c57a35332b2b5818e51618af7953351"}, {"url": "https://medium.com/tag/nlp?source=post_page-----f3acb35bc86a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/gpt-2?source=post_page-----f3acb35bc86a---------------gpt_2-----------------", "anchor_text": "Gpt 2"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f3acb35bc86a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----f3acb35bc86a---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f3acb35bc86a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3acb35bc86a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=-----f3acb35bc86a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3acb35bc86a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=-----f3acb35bc86a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3acb35bc86a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://georgedittmar.medium.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43dcb64ca1bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=post_page-43dcb64ca1bc----f3acb35bc86a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F43dcb64ca1bc%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=-----f3acb35bc86a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://georgedittmar.medium.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Written by George Dittmar"}, {"url": "https://georgedittmar.medium.com/followers?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "31 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43dcb64ca1bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=post_page-43dcb64ca1bc----f3acb35bc86a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F43dcb64ca1bc%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a&user=George+Dittmar&userId=43dcb64ca1bc&source=-----f3acb35bc86a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/natural-language-generation-part-1-back-to-basics-2f0b2654624f?source=author_recirc-----f3acb35bc86a----0---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://georgedittmar.medium.com/?source=author_recirc-----f3acb35bc86a----0---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://georgedittmar.medium.com/?source=author_recirc-----f3acb35bc86a----0---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "George Dittmar"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3acb35bc86a----0---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/natural-language-generation-part-1-back-to-basics-2f0b2654624f?source=author_recirc-----f3acb35bc86a----0---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Natural Language Generation Part 1: Back to BasicsUsing ML to generate text, images and video is becoming more widespread as research and hardware advances. With recent advancements in\u2026"}, {"url": "https://towardsdatascience.com/natural-language-generation-part-1-back-to-basics-2f0b2654624f?source=author_recirc-----f3acb35bc86a----0---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "\u00b79 min read\u00b7Jul 19, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f0b2654624f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-1-back-to-basics-2f0b2654624f&user=George+Dittmar&userId=43dcb64ca1bc&source=-----2f0b2654624f----0-----------------clap_footer----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/natural-language-generation-part-1-back-to-basics-2f0b2654624f?source=author_recirc-----f3acb35bc86a----0---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f0b2654624f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-generation-part-1-back-to-basics-2f0b2654624f&source=-----f3acb35bc86a----0-----------------bookmark_preview----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3acb35bc86a----1---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----f3acb35bc86a----1---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----f3acb35bc86a----1---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3acb35bc86a----1---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3acb35bc86a----1---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3acb35bc86a----1---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3acb35bc86a----1---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----f3acb35bc86a----1-----------------bookmark_preview----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f3acb35bc86a----2---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----f3acb35bc86a----2---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----f3acb35bc86a----2---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3acb35bc86a----2---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f3acb35bc86a----2---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f3acb35bc86a----2---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----f3acb35bc86a----2---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----f3acb35bc86a----2-----------------bookmark_preview----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3acb35bc86a----3---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----f3acb35bc86a----3---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----f3acb35bc86a----3---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3acb35bc86a----3---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3acb35bc86a----3---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3acb35bc86a----3---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3acb35bc86a----3---------------------29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----f3acb35bc86a----3-----------------bookmark_preview----29e23e42_76d3_4cb8_85cf_a9f08ab35d67-------", "anchor_text": ""}, {"url": "https://georgedittmar.medium.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "See all from George Dittmar"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----0-----------------clap_footer----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----f3acb35bc86a----0-----------------bookmark_preview----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----1-----------------clap_footer----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----f3acb35bc86a----1-----------------bookmark_preview----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----0-----------------clap_footer----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----f3acb35bc86a----0---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----f3acb35bc86a----0-----------------bookmark_preview----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Arslan Mirza"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "How To Build Your Own Custom ChatGPT BotA step-by-step guide to building and fine-tuning custom ChatGPT models"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "\u00b79 min read\u00b7Mar 29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&user=Arslan+Mirza&userId=35aaa5742af7&source=-----cf4af959adcc----1-----------------clap_footer----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----f3acb35bc86a----1---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&source=-----f3acb35bc86a----1-----------------bookmark_preview----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----f3acb35bc86a----2---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----f3acb35bc86a----2---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----f3acb35bc86a----2---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Jay Peterman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----f3acb35bc86a----2---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----f3acb35bc86a----2---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Make a Text Summarizer with GPT-3Quick tutorial using Python, OpenAI\u2019s GPT-3, and Streamlit"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----f3acb35bc86a----2---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "\u00b711 min read\u00b7Jan 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&user=Jay+Peterman&userId=9731dc608e6c&source=-----f0917a07189e----2-----------------clap_footer----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----f3acb35bc86a----2---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&source=-----f3acb35bc86a----2-----------------bookmark_preview----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----f3acb35bc86a----3---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----f3acb35bc86a----3---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----f3acb35bc86a----3---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----f3acb35bc86a----3---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----f3acb35bc86a----3---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----f3acb35bc86a----3---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----3-----------------clap_footer----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----f3acb35bc86a----3---------------------f426fe78_10c7_4630_b7dd_45af5fbb464c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----f3acb35bc86a----3-----------------bookmark_preview----f426fe78_10c7_4630_b7dd_45af5fbb464c-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----f3acb35bc86a--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}