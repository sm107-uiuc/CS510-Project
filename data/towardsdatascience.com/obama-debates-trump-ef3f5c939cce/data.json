{"url": "https://towardsdatascience.com/obama-debates-trump-ef3f5c939cce", "time": 1683010524.468117, "path": "towardsdatascience.com/obama-debates-trump-ef3f5c939cce/", "webpage": {"metadata": {"title": "Obama debates Trump. An AI alternative history with natural\u2026 | by Tim Sullivan | Medium", "h1": "Obama debates Trump", "description": "Machine learning text generation using transfer learning and Hugging Face transformers with OpenAI's GPT2 trained on the speeches of Presidents Obama and Trump."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnns-347903dd8d81", "anchor_text": "continue", "paragraph_index": 1}, {"url": "https://medium.com/swlh/introduction-to-lstms-and-neural-network-text-generation-bd47adaf55fe", "anchor_text": "learning", "paragraph_index": 1}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face", "paragraph_index": 4}, {"url": "http://www.thegrammarlab.com/?nor-portfolio=corpus-of-presidential-speeches-cops-and-a-clintontrump-corpus", "anchor_text": "The Grammar Lab", "paragraph_index": 6}, {"url": "https://millercenter.org/the-presidency/presidential-speeches", "anchor_text": "The Miller Center", "paragraph_index": 6}, {"url": "https://medium.com/swlh/introduction-to-lstms-and-neural-network-text-generation-bd47adaf55fe", "anchor_text": "article", "paragraph_index": 13}, {"url": "http://huggingface.co", "anchor_text": "HuggingFace.co", "paragraph_index": 28}, {"url": "https://huggingface.co/ts1829/obama_gpt2", "anchor_text": "Obama", "paragraph_index": 28}, {"url": "https://huggingface.co/ts1829/trump_gpt2", "anchor_text": "Trump", "paragraph_index": 28}], "all_paragraphs": ["My top priority is to reduce carbon emissions. The more we can reduce our dependence on foreign oil, the more energy we can produce. This is why I want the U.S. to keep growing as fast as possible, to do whatever it takes to bring about the transformation of our economy. The world, in its most powerful, strongest sense, needs a plan to deliver change.", "No, you didn\u2019t miss a debate between Presidents Obama and Trump. The quote above was generated entirely by an AI, trained to replicate the speeches of President Obama! In this article, we\u2019ll continue learning about Natural Language Processing and introduce transformers which allow us to use powerful models created by companies like OpenAI.", "If you follow machine learning, you\u2019re familiar with the incredible performance improvements in the field such as AIs that can play video games, drive cars, and speak and write like humans. What you\u2019re also probably familiar with is that the size of these models keeps getting bigger. The latest natural language processing model from OpenAI, GPT-3, has 175 billion parameters to learn and was trained on the enormous Common Crawl dataset (the May/June 2020 Common Crawl contains 2.75 billion web pages or 255 TiB of uncompressed content). This puts state-of-the-art performance outside of the means of individuals and all but the largest companies. But, there are ways around this! Enter transfer learning.", "Transfer learning is the process of starting with a pre-trained model with a general level of proficiency in a field such as natural language processing and fine-tuning it for a particular application. For example, we can take a model such as GPT or Bert which has a base-level understanding of English and adjust it through a much smaller training dataset for a custom application. A company might use transfer learning to create a chatbot on its website to answer visitor or customer questions or you could use one to summarize articles or any number of other applications. Maybe soon video game characters will use AI to generate unique dialog at each interaction with players.", "A company called Hugging Face has made it easy to work with these pre-trained models in PyTorch or TensorFlow and to share your results with the world! You can download the model weights and interact, retrain, or evaluate them as you would your own model.", "To explore transfer learning, I created two AIs starting from the same basic GPT2 model and re-trained them on speeches from Presidents Obama and Trump. Here\u2019s how I did it.", "Since presidents are public figures, it\u2019s relatively easy to get transcripts of their speeches and public remarks. I used two websites to get this data, The Grammar Lab and The Miller Center at UVA. I used all of President Obama\u2019s speeches from Grammar Lab which amounted to about 1MB of text. Grammar Lab didn\u2019t have Trump\u2019s speeches as president so I used a selection of speeches from The Miller Center from which I had to copy and paste by hand since they don\u2019t have a way to download text files as well as some of his speeches as a candidate from Grammar Lab\u2019s repository from the 2016 election. To keep things equal, I also used around 1MB of text.", "As with any machine learning project, you have to spend some time cleaning the data prior to learning. Some of the speech data included indications of applause or descriptions of what was going on inside of <> tags which needed to be removed as well as double spaces and other features that we don\u2019t want the model to learn to reproduce. I created a series of regular expressions to filter these out and condense all the .txt files into a single file.", "Next, I used PyTorch\u2019s DataSet/DataLoader classes to create a Data Loader which will feed sentences into our model during training. Since some lines in the text data are very large and would exceed the max input size of the GPT2 model, I used the sentence tokenizer from NLTK to break up each line into sentences. If you wanted to, you could include multiple sentences up to a certain length. This might allow for the model to maintain a better train of thought, but the one-sentence method seemed to work well enough.", "Downloading the GPT2 model from HuggingFace is as easy as one line of Python for the Tokenizer and another for the model.", "The Tokenizer converts the input text into numbers that the model can understand. GPT2 uses a byte-level byte-par encoding which transforms the text into tokens using as few tokens as possible. It searches over the text counting the frequency of each byte pair and replaces it with a token repeating the process until there are no pairs occurring more than once. Using this example from Wikipedia,", "The tokenizer for GPT2 has already been created. We just need to get a copy of it to encode our training text correctly.", "I set up the training loop to include a learning rate linear scheduler which seemed to give better performance than a constant learning rate. I took advantage of the free GPU time offered by Google Colab notebooks. They have enough RAM to run the GPT2 or GPT2-Medium models (depending on your sequence and batch size) but not GPT2-Large or above. Each iteration takes about 3 minutes to run which is pretty good for a free resource and a model that is 548MB!", "To generate the responses to different prompts I used the evaluation loop below. The search input is one of three different functions to choose which word/token will be selected by the model. I tried a simple probability selection, top-k selection, and nucleus search. If you\u2019d like an explanation of each one, I wrote about them in a previous article. Top-k seemed to do the best in this case.", "The evaluation function loops over each prompt and generates several responses of a certain length. It\u2019s difficult to assess how the model is performing at creating realistic language without examining a few different responses. I used each word selection method to generate 3 responses of 100 tokens to each of the prompts below.", "I selected some of the best or most interesting results and included them below. You can look at the full set of responses in the notebook which is linked at the bottom of the article. While the models don\u2019t always produce the most coherent responses, you can definitely identify each President\u2019s patterns of speech as well as the topics that were important during their Presidency or campaign. For example, the Obama bot discusses Afghanistan and the economy even when not prompted and the Trump bot brings up jobs, building a wall, Hillary Clinton, and says \u201cgreat\u201d a lot.", "The biggest challenge facing the country is climate change. But, to my mind, those concerns are not just about money. They\u2019re about how this country is going to address the problems that threaten us. I also believe in equal rights for all. And that is also why we\u2019re not at war. There\u2019s some talk about how we can get better education, but that\u2019s not what our teachers can provide for us, that\u2019s not what the military can provide for our troops. And as an American, I\u2019m here. Thank you\u2026", "The biggest challenge facing the country is whether the government will stop it or not, and if it will stop, we know it. I would love to have a Trump administration, and I love to have an administration. We\u2019re trying to bring back jobs and make the economy stronger \u2014 which is why we\u2019re fighting to save jobs. He said, you know I don\u2019t want to say it but what\u2019s the use of being in Washington, when you\u2019re doing so well with the press. You know, if the media can\u2019t tell\u2026", "I am running for president to be more open about what is happening to our economy. In this regard, we will be working with Republicans and Democrats to make sure they can work together to address this critical issue. And the fact that they\u2019ve done that is a sign that you are not as constrained by the status quo, that there is hope. And I believe in the power of the people who do this for them. I think that if we are truly a nation, we are going to be able to move on to more important\u2026", "I am running for president to rebuild our country. They\u2019re not the media. The media is dishonest. They want a president who knows that. We need strong leadership, strong leadership from all of you. We need our leaders, strong leadership in a very different country. We need our citizens to take care of each other, and they\u2019re doing it with amazing skill, tremendous care, incredible care. We need to do better. But as I have said, no more NAFTA, no more Mexico. We are all working together and\u2026", "Healthcare in the united states is not working, and the American people deserve to know what\u2019s going on. The only way is if we get this done, as quickly as possible. This is not just about the deficit. It is about making sure every citizen has access to quality health care and the quality of education. And if you believe in the principle of a just, healthy society, I\u2019ve got you covered. So we\u2019ll start with all the families that are at risk, who will face health insurance premiums, and who will\u2026", "Healthcare in the united states is the worst disaster of our time. I think we have made it, and we\u2019re going to make it much worse. It\u2019s been a disaster. He said he didn\u2019t think I would ever get to vote. You\u2019re going to hear Hillary talking about her great work. And it\u2019s amazing they\u2019re getting so many, you know, you know, like 15 million people, so, you know, I think I\u2019m going to be a much healthier person than my wife, I\u2019m going to\u2026", "My tax plan is based on the principles that you believe in when they say to their kids that you want to make it easier for them to live, and you want to expand opportunities like these for people to work full time. And as a result, we\u2019ve brought in more immigrants from Central America, from Africa, and the Middle East. I was also pleased to see that as our economy recovers, we have a stronger commitment to helping to improve safety and security around the world and help to prevent a situation that continues indefinitely\u2026", "My tax plan is one of the largest, if not the largest, tax cuts for the American people since World War II. And I just don\u2019t think they have enough of us. So many people, they\u2019re going to vote Republican on November 8th. I have a lot of respect for the people of Arizona \u2014 my family is here \u2014 but we won Arizona. I\u2019m the proudest American and most patriotic woman [\ud83e\udd23], I don\u2019t think they have this in mind. And the media is totally dishonest. And I\u2026", "My first priority as President will be to protect American interests, which is a responsibility to all Americans and not just to the wealthy and the powerful. And while there is no doubt that some people have done well \u2014 they should not be held responsible for their actions \u2014 I know, from my own experience, that a lot of our allies and allies in the Middle East have found the courage and humility to do it. Let me say it. We need to be better prepared to deal with the challenges we face \u2014 the challenges that are the biggest\u2026", "My first priority as President will be protecting our jobs and our safety. We\u2019re going to fight to defend our great nation and to help you and your family, the great America. And that includes protecting America\u2019s national security. I want to thank the people of Wisconsin and other states for their support. I\u2019ll thank the great people of America and the great folks of Wisconsin for helping me get here today. Thank you. And it really is amazing how much has happened. We\u2019ve been fighting for years. This will never happen. It\u2026", "One interesting thing I learned is that it only takes a small nudge to go from the general GPT2 model to the fine-tuned model. I used a very small learning rate with scheduler decay and just a few epochs to get the results shown here. Training the model with a high learning rate or too many iterations resulted in very poor performance.", "Also, while these models are able to write coherent English often including correct punctuation and grammar, they often contradict themselves logically. For example, the model might claim in one sentence that something is increasing and then in the next sentence say it is decreasing. We have a long way to go before being able to claim that the models \u201cunderstand\u201d what they are saying. Though sometimes it can really feel like they do understand such as in the responses from a GPT3 model asked coding interview questions:", "If you made it this far, thanks for reading! The link to the full notebook is below which you can modify and run in Google Colab for free using your own parameters and training text. You can also access the models from HuggingFace.co directly at these links: Obama, Trump. If you\u2019d like to read my previous articles on recurrent networks and LSTMs, you can find them here:"], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": "Tim Sullivan"}, {"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnns-347903dd8d81", "anchor_text": "continue"}, {"url": "https://medium.com/swlh/introduction-to-lstms-and-neural-network-text-generation-bd47adaf55fe", "anchor_text": "learning"}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face"}, {"url": "http://www.thegrammarlab.com/?nor-portfolio=corpus-of-presidential-speeches-cops-and-a-clintontrump-corpus", "anchor_text": "The Grammar Lab"}, {"url": "https://millercenter.org/the-presidency/presidential-speeches", "anchor_text": "The Miller Center"}, {"url": "https://medium.com/swlh/introduction-to-lstms-and-neural-network-text-generation-bd47adaf55fe", "anchor_text": "article"}, {"url": "https://twitter.com/lacker/status/1279136788326432771?s=20", "anchor_text": "https://twitter.com/lacker/status/1279136788326432771?s=20"}, {"url": "http://huggingface.co", "anchor_text": "HuggingFace.co"}, {"url": "https://huggingface.co/ts1829/obama_gpt2", "anchor_text": "Obama"}, {"url": "https://huggingface.co/ts1829/trump_gpt2", "anchor_text": "Trump"}, {"url": "https://towardsdatascience.com/", "anchor_text": "Tim Sullivan - MediumThere's a stack of old notebooks sitting on my shelf, each filled with notes from books, random thoughts, to-do lists\u2026tims457.medium.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ef3f5c939cce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/politics?source=post_page-----ef3f5c939cce---------------politics-----------------", "anchor_text": "Politics"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ef3f5c939cce---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/python?source=post_page-----ef3f5c939cce---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----ef3f5c939cce---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://towardsdatascience.com/?source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": "More from Tim Sullivan"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5aeed6a7a77&operation=register&redirect=https%3A%2F%2Ftims457.medium.com%2Fobama-debates-trump-ef3f5c939cce&newsletterV3=3c8d6dcfb858&newsletterV3Id=a5aeed6a7a77&user=Tim+Sullivan&userId=3c8d6dcfb858&source=-----ef3f5c939cce---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ef3f5c939cce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://towardsdatascience.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tim Sullivan"}, {"url": "https://towardsdatascience.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "79 Followers"}, {"url": "http://sullivantm.com", "anchor_text": "sullivantm.com"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5aeed6a7a77&operation=register&redirect=https%3A%2F%2Ftims457.medium.com%2Fobama-debates-trump-ef3f5c939cce&newsletterV3=3c8d6dcfb858&newsletterV3Id=a5aeed6a7a77&user=Tim+Sullivan&userId=3c8d6dcfb858&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}