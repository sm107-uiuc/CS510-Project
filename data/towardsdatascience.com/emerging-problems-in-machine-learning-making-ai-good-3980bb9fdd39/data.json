{"url": "https://towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39", "time": 1683014875.853617, "path": "towardsdatascience.com/emerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39/", "webpage": {"metadata": {"title": "Emerging problems in machine learning: making AI \u201cgood\u201d | by Jeremie Harris | Towards Data Science", "h1": "Emerging problems in machine learning: making AI \u201cgood\u201d", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 7}, {"url": "https://twitter.com/neutronsNeurons", "anchor_text": "follow Ed on Twitter here", "paragraph_index": 9}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 9}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 143}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:", "I think it\u2019s easy to lose sight of just how far and how fast machine learning has developed.", "Just 10 years ago, people were saying we were in an AI winter, and deep learning was a fringe and largely ignored field of research. Since then, we\u2019ve witnessed one of the most important technological revolutions in human history. Reinforcement learning allows machines to play games and robots to orient themselves in the real world, convolutional neural networks let computers see better than people, and transformer architectures are generating large bodies of text that experts can\u2019t tell apart from human writing.", "These new tools allow us to scale the impact of individual human beings in ways we never could have imagined. A single data scientist can design an algorithm that within minutes of its deployment will instantly affect the lives of millions, or even billions of people.", "But increasing the impact of individual developers or individual algorithms isn\u2019t necessarily a good thing. Developers can make mistakes, companies can have malicious intent, data can be poorly sampled, and ultimately these problems and many others can result in algorithms that do more harm than good. That harm, when scaled to millions or billions of people, has the potential to do unprecedented amounts of damage to humanity.", "And while it\u2019s certainly true that our technology has gotten a lot better, it\u2019s not so clear that we\u2019ve become wise enough to wield it. Many in the machine learning community increasingly have the sense that we need to significantly improve on thousands of years of moral philosophy, ethics and even metaphysics if we\u2019re going to be able to wield our new powers properly.", "But that\u2019s hard to do in a context where everyone is constantly rushing to put out the next algorithm, the next dataset or the next big architecture. So I think it\u2019s worth it for at least some of us to take some time to pause, look around and ask some pretty basic questions. Where is this all going? Where do we want our technology to lead us? How are we falling short of that target? What risks might advanced AI systems pose to us in the future, and what potential do they hold? And what does it mean to build ethical, safe, interpretable, and accountable AI that\u2019s aligned with human values?", "That\u2019s what this year is going to be about for the Towards Data Science podcast. I hope you join us for that journey, which starts today with an interview with my brother Ed, who apart from being a colleague who\u2019s worked with me as part of a small team to build the SharpestMinds data science mentorship program, is also collaborating with me on a number of AI safety, alignment and policy projects. I thought he\u2019d be a perfect guest to kick off this new year for the podcast.", "I hope you enjoy the episode.", "You can follow Ed on Twitter here, and you can follow me on Twitter here.", "Jeremie:Hey, everybody. Welcome back to the podcast. I hope you\u2019re doing well. My name of course is Jeremie. I\u2019m the host of the Towards Data Science Podcast and I\u2019m also on the team over the SharpestMinds data science mentorship program. We actually took a pause, a break last week. We didn\u2019t release an episode. I wanted to open this up just by explaining why we paused and what we\u2019re doing a little bit differently going forward.", "Jeremie:So first off, the reason for the pause is we\u2019re working on the next series of episodes. Those episodes are going to be focused a little bit more on questions surrounding how we deploy AI systems for the benefit of humanity. As our AI systems have become more and more powerful, increasingly, it\u2019s become important to start asking questions like should we actually deploy an AI system to solve this particular problem or what could go wrong if we start to deploy increasingly powerful AI systems in this sub-domain?", "Jeremie:They\u2019re all kinds of questions around safety both in the short term where we start asking questions like could an AI system accidentally recommend a course of action that might lead to harm to human beings or damage to economies? All the way to more fundamental and perhaps more concerning long-term questions like RAI systems actually reflecting human values shaping humanity into a form that we want and our AI systems actually safe from a more existential standpoint.", "Jeremie:Is there the possibility that an arbitrarily intelligent system might have implications for the continued existence of humanity itself that we might want to consider sooner rather than later. So that\u2019s going to be the focus going forward a lot of questions surrounding ethics, bias in AI systems and should we, shouldn\u2019t we questions around distribution deployment. So I\u2019m really excited to dive into this whole series. We have such great guests lined up and it was so hard to pick one to start that I ultimately ended up sitting back and thinking about the person who I knew who I\u2019d be most comfortable with talking about all these topics, not just AI ethics, not just AI bias, not just the technical details of AI alignment, but somebody who cover all those bases with me.", "Jeremie:I landed on my brother Ed who\u2019s been on the podcast before. He is a co-founder with me of SharpestMinds and he\u2019s actually got a whole bunch of experience on technical alignment work as well as startup work seeing algorithms deployed in the wild and because of his breadth of experience and depth of focus in the AI safety area, I really decided he\u2019d be the best person for me to open things up with. So I hope you enjoyed this conversation. It\u2019s going to be a little bit more off beat than some of the ones we\u2019ve had in the past. But if you have any feedback, let me know and I just can\u2019t wait to release the next couple episodes that we\u2019re exploring a really exciting topic here and it\u2019ll be great to share these thoughts with the community and get your feedback as well on how we\u2019re doing. So without any further ado, please enjoy the show.", "Jeremie:Hi, Ed. Thanks for joining me for the podcast.", "Ed:Thanks so much for having me. I appreciate it.", "Jeremie:Oh, yeah. No problem at all. Geez, I think there are so many places we could start with this. AI is a really big space and questions around how AI should be used, how it should be secured are starting to pop up. Obviously, that\u2019s the theme of this entire series of the podcast and what we\u2019ll be focusing on for the next few episodes. I guess, maybe to begin with, we could talk about so many different things, but you\u2019re more focused on this idea of AI alignment, getting machine learning models to do what we want them to do in different forms, making sure that their performance isn\u2019t dangerous essentially to human beings and that it optimizes in a direction that we want. Can we introduce the idea, the field of AI alignment like why should we be spending time and effort focusing on aligning our machine learning algorithms?", "Ed:Yeah. So this is something that I have been thinking about more recently as systems have gotten more powerful and as we\u2019re seeing the advanced from year to year. The more powerful the systems get, the more important it is that they are doing the things that we want and alignment is important because it\u2019s not always obvious that a system is going to be doing the thing that we want and it\u2019s not always obvious that even if the system starts out doing what it looks like what we want that it won\u2019t end up doing something really bad.", "Ed:So a good example is social media feeds. Algorithms that keep you coming back to Twitter and all of this stuff. Maybe you think about what is it that Twitter, the company wants from you? It\u2019s money, but it\u2019s pretty benign, right? They just want you to click more and like. At the first level what you think of is the algorithm is going to give you posts and tweets that you\u2019re going to click on more so you\u2019re going to stay there and see more ads. That\u2019s the first level.", "Ed:But the problem is that if you train a general system to do this, one of the things that the system will discover is that, \u201cHey, I actually from doing this testing myself on the people that I\u2019m showing these tweets to, I can discover that I can actually make human beings more predictable by showing them certain kinds of content.\u201d And one of the ways that these algorithms are making us more predictable to them is politicization of content. If I\u2019m more politicized, my cliques are more predictable than they are when my political position is at the center.", "Ed:So one of the things that\u2019s actually happening is that these algorithms purely through the act of trying to make more money are actually pushing people to different ends of all these different kinds of spectrums, different political issues and all of that, purely because they\u2019re like, \u201cHey, I\u2019m trying to make you into something that\u2019s easier for me to predict and people who are more extreme politically are just more predictable because they tend to have a lot of correlated views.\u201d", "Ed:So this is one of the ways in which like\u2026 It\u2019s actually scary and you can see how it like creeps up on you because this happened over years and years, but you can imagine this is already having an effect on the world and as these systems become smarter and smarter, we should expect more similar things to just begin to happen.", "Jeremie:Right, okay. So the primary problem here is we have algorithms that sometimes are too clever in the sense that if we don\u2019t think very, very carefully about what kind of world we want them to create for us long term, they\u2019ll find solutions that we\u2019ve never even imagined could be solutions to the problem, or if the problem rather is predict what I\u2019m going to click on, so that I can generate more engaging content, then really changing my users mindset to make them more predictable to turn the political spectrum into more of a binary so it\u2019s a smaller dimensional problem and easier to dimensionality-reduced. That becomes its own pathology, right? I mean, that\u2019s at the core of this, right?", "Ed:Yeah. It\u2019s the sort of thing that again creeps up on you. You can give a system a set of instructions that you think are perfectly benign and totally makes sense for you, make me more money with more clicks. Okay. Pretty benign. And it\u2019s only with the benefit of hindsight really that we can look back and say like, \u201cOh my god. If only we\u2019d known at the time that this was the force we were unleashing upon the world, we might have done things differently.\u201d But at the time, it was not possible to predict that this would be the outcome.", "Ed:And this is the problem is that when you\u2019re dealing with something smarter than you are, pretty much by definition, you can\u2019t predict, what it\u2019s going to do because it\u2019s smarter than you are. If you were smarter than it, you\u2019d be able to predict it, but because it\u2019s smarter than you are, it can predict you and not the other way around.", "Jeremie:I guess it doesn\u2019t even necessarily really\u2026 I mean, I think this is one of the problems when you\u2019re talking about AI, what is smartness? What does intelligence mean? I think these terms are ill-defined. There are certainly senses in which it would be very hard to argue that the Twitter algorithm is smarter than a human. Other senses in which narrowly interpreted, you could say it is smarter than a human, but there\u2019s definitely a threshold where the two start to interact and compete and one algorithm starts to outstrip the other. The Twitter algorithm starts to outstrip your human brain and starts to change you more than you change it, right?", "Ed:Yeah. I would say that algorithms can be smarter than us currently in narrow ways, but we should be concerned about the possibility that they may become smarter than us in more general ways. And of course, there are other things to be concerned about before even that point is reached.", "Jeremie:Okay. So, yeah. Because I think we\u2019ll get to the general concern about general intelligence and where these more and more advanced systems might go. In the shorter term, you mentioned Twitter. Obviously, a lot of people have talked about things like AI ethics and AI bias in AI systems, right? This is I think a real theme of the day because we see it around us. We can see systems that operate in ways that have biases that will surprise us. In many cases they seem to reflect the way that the world currently is with its current failures, failure modes and it\u2019ll tend to reinforce those failure modes because it\u2019s been trained on that data and it\u2019ll make predictions that mirror that data. I guess you\u2019re more concerned about the long-term side of things, but could that be an issue for the long term as well?", "Ed:To an extent. So one of the things about the kinds of failure modes and bias in AI is that in a manner of speaking, this is the sort of thing that is happening to an extent because these systems are a little bit too dumb and they haven\u2019t been trained in general enough ways. The further in the future risks and probably eventually the bigger risks are going to be what happens when systems are too smart. But the risks of bias are definitely real now today and they are current.", "Ed:These bias risks are a turbocharged version of the kinds of what you essentially get when\u2026 When people build software, they\u2019ll generally build for themselves, they\u2019ll test the flow of clicks and things that they naturally expect other people to do. Everyone is egocentric, right? So we all basically build for ourselves, and that\u2019s perfectly fine, but what can sometimes happen when the data that you collect is biased in a particular way is like it\u2019s not the algorithm\u2019s fault.", "Ed:The algorithm is just running on the data that\u2019s been given and it can just totally forget that, \u201cOh, there\u2019s a bunch of people that don\u2019t have names written in Latin characters or that don\u2019t have a particular skin color or whatever.\u201d So you actually can get of course these situations where an algorithm is very bad at dealing with cases that the developers didn\u2019t think of and the more power again we give to these systems, the more costly those mistakes become. So I would say that\u2019s currently the status of bias in AI.", "Jeremie:Right. I mean, people talk a lot about data set bias. That gets a lot of attention. I think one other really important aspect is feature selection bias because if you think about it, human beings collect features about the rest of the world as we navigate our environment. Those features typically are things like sound waves. They\u2019re things like smells. They\u2019re things like vision and touch. Those are the features that evolution has engineered for us. Because we see the world through that lens, we are unable to notice certain things. These are things for example like the neutrinos that fly right through our bodies all the time that we\u2019re totally insensitive to and yet that account for a large or not unconsiderate fraction of the amount of energy that we\u2019re bathed in at any given moment of time.", "Jeremie:So I don\u2019t think it\u2019s an exaggeration to say that our environment goes mostly unseen and even the things that we could in principle see, we\u2019re so myopically focused on one tiny fraction of our visual field that we don\u2019t take in the overwhelming majority of the information around us. When it comes to our machines, I think we\u2019re doing something similar. We select features.", "Jeremie:For example, if you were to tell a credit card pricing algorithm like the name, age, occupation and skin color of a person, you\u2019ve just caused it to look at the world through a certain lens and that biases it to find certain correlations more easily than others or to fold complexities and nuances into one high-level core strain feature. Is that something that you see as well as an issue or do you think that the data set bias is a bigger problem?", "Ed:So what I would say there is that there are two kinds of biases that can arise from feature selection. The first is an omission bias where you don\u2019t give the system an informative feature. You give the system a feature that is not informative with respect to the conclusion. The second is more like labeling bias. So in the event where you give a system the skin color of someone for a credit card assessment, if you were actually training the system on truly ground truth informative data set, in theory that skin color shouldn\u2019t end up mattering.", "Ed:If you have a perfectly general intelligence that\u2019s running on that system, it\u2019s going to ignore skin color to the extent that skin color needs to be ignored. But the problem is that when you\u2019re training that system on labels that have been put there by people who themselves have biases, which we all have, then that bias itself gets induced on to the system. And this is the sort of thing that one encounters in terms of potentially an algorithm delivering a verdict for example.", "Ed:If it turns out that the verdicts that it\u2019s seen before are delivered by\u2026 There\u2019s a significant sample of those verdicts that are biased against who knows, the height or the skin color or the hair color or whatever of the person, then again the system, it\u2019s garbage in, garbage out. The system is going to learn what you teach it. So it\u2019s going to have the same exact biases induced on it.", "Jeremie:So I guess that\u2019s interesting. I mean I guess I see the case of bias data as one part of this. I don\u2019t think it\u2019s the case that\u2026 I mean, you talked about a sufficiently generally intelligent program that would look at this information including skin color and so on and then go about drawing the right kinds of conclusions. I don\u2019t think that\u2019s actually the case. I think if you\u2026 Unless that system has access to a fuller set of features, it will tend to see things myopically through the particular lens that it\u2019s given on the world and sometimes that lens, even if that data set isn\u2019t biased, even if it detects just what\u2019s happening in the world, if we talk about skin color or we talk about gender or whatever, there are correlations between skin color and tendency to pay down debts or whatever, and those are reflected in the biases if you will of a poorly engineered system.", "Jeremie:But what\u2019s really going on under the surface is if you part that out, if you control for other variables all of a sudden, we expect\u2026 I don\u2019t know that anyone has actually done this, but I would highly expect personally that skin color would iron out of that equation, but only if you control for these other variables. So I think it\u2019s only to the extent that you add to that feature set that you can actually de-bias the system in that way.", "Ed:Yes, this is correct. This is I think another way of looking at what I was saying in terms of their sins of omission that you can make. So to the extent that you have given the system enough ground truth data to actually break down those variables that ultimately make these gender and these confounding features, I guess, irrelevant to the extent you\u2019ve given the system that extent of data, it will learn that those variables are irrelevant. But yes, if you make those omissions, they actually learn on variables and exhibit a bias as a result.", "Jeremie:Yeah. I think it\u2019s always interesting as well the extent to which all this challenges to our notion of free will or maybe illusion of free will, because I can imagine as well if you take this process to its limit and you keep fine-graining and adding more features and adding more features eventually your model becomes so sophisticated. It can account for, I mean, almost like in the absurd limit the firing patterns of every neuron in your brain and then you can predict your behavior with super high levels of certainty and all your agency kind of disappears. I mean, I see that actually kind of as an interesting part of the journey we\u2019re going to have to make as systems become more sophisticated.", "Ed:I think that\u2019s true to an extent, but there\u2019s a limit to that. So it\u2019s the same as predicting the weather. Systems that have\u2026 Even if you\u2019re not thinking about, \u201cOh, there\u2019s quantum mechanical uncertainty on top of everything,\u201d even if you\u2019re not thinking about that, classical systems that have chaotic properties have\u2026 You can\u2019t really predict them beyond a certain number of time steps. The way it works is that I think if I recall correctly, it\u2019s something like your\u2026 However, much accuracy you have in the beginning, the accuracy that you have later goes down as one over like the square root of the time.", "Ed:So basically after a certain period of time, even if you have a perfectly deterministic model of how stuff is going to happen, if your accuracy of measurement at the beginning basically propagates and gets much, much worse over time. So there will always be a limit to the extent to which very intelligent systems are able to predict the future. At least that\u2019s our best current understanding now. But you don\u2019t need to be able to predict the movement of every single atom to be able to do a lot of very instrumentally effective things. That could be dangerous.", "Jeremie:Yeah. And I think there\u2019s actually\u2026 Maybe the sidebar to this is there\u2019s already been a story in the UK about the government trying to\u2026 Not the government. Sorry, I think a university trying to predict students test scores given that they weren\u2019t able for whatever reason, I think it might have been COVID related. They weren\u2019t able to actually write the test, right?", "Jeremie:So you\u2019ve got students who you might say arguably are rightly upset that their agency has been stripped away. In this case, it seemed as though the algorithm was non-performative. It overstated as often machine learning models do nowadays, but it overstated its accuracy or downplayed its uncertainty. However, I think there\u2019s an interesting question as to, \u201cOkay. Well, what if that hadn\u2019t been the case? What if the algorithm had been super performative, where would we be there?\u201d", "Ed:If you think of it in a certain way, the test itself is an uncertain evaluation of your own. It\u2019s a noisy evaluation of your own skill and so if you have a good day on the test, you should be happier. You\u2019ve gotten like an extra bonus whereas if you just had a bad day, had a bad night\u2019s sleep, whatever it is, some sort of mix of stuff, you maybe you should be rightly indignant that this test didn\u2019t correctly assess your level.", "Ed:I think that ultimately it might be more of a semantic and comfort zone issue than a real issue if you have a perfectly unbiased algorithm with a known uncertainty. So if your algorithm is unbiased and well calibrated. Because if it\u2019s unbiased and well calibrated, then I can at least look at my test score or my predicted test score and say, \u201cWell, there\u2019s a 50\u201350 chance I should be mad about this and a 50\u201350 chance I should be happy about it.\u201d On the other hand, we all have a natural discomfort in having a computer judge our futures. So I don\u2019t know that we as a civilization will ever get over that necessarily.", "Jeremie:Right, which I guess starts to invite some of the parts of this conversation that I think are more forward-looking. So as we keep developing this technology, you alluded to this earlier. The more powerful these systems get, the more important it\u2019s going to become for us to know exactly what we want. And this idea of being able to build systems that know exactly what we want, being able to communicate that to them being able to make sure that they\u2019re actually doing exactly what we\u2019ve asked them to do is known as AI alignment. What I\u2019d like to do is ask you can you describe AI alignment serve in your own words and then maybe can you provide a description overall of maybe the outer alignment versus inner alignment problems as well just so people are a little bit more familiar with them.", "Ed:Yeah. So roughly speaking, the problem of AI alignment is the problem of getting an AI to do what we truly want it to do. There is a lot of detail in that description and even a big part of the alignment problem that\u2019s still open is figuring out exactly how much detail there is left to figure out in that description. But right now people tend to break down alignment into two sub questions like you said. There\u2019s outer alignment and inner alignment.", "Ed:So the first problem is getting the system, getting this big AI to actually try to do what we want it to do. So this, you can think about as the problem of, I found a magic lamp, I rub the lamp and a genie comes out. It\u2019s not the genie from Aladdin. It\u2019s not like this really nice genie who\u2019s trying to be helpful and stuff. It\u2019s a genie that will try really hard to misinterpret your wish as hard as possible. And the genie is also much smarter than you.", "Ed:So what you have to do is you have to figure out a way to word your wish so that you absolutely pin down that genie so they absolutely cannot misinterpret your wish in any meaningful way and they\u2019re like, \u201cUgh, I guess I\u2019m going to do the thing this thing because you\u2019ve left me no other choice.\u201d", "Jeremie:So this is interesting because you immediately frame this as an adversarial thing, which I think is interesting in and of itself, but the implication is that this machine, this AI, or this genie is trying to misinterpret what we\u2019re saying, which is not the case, but it does seem at least as somebody who\u2019s been in the space and worked with you on a lot of the stuff, it definitely does seem as though in practice you have to\u2026 It\u2019s almost like defensive driving. You have to assume, because this thing is so powerful, so much more intelligent than you are, if you\u2019re building super powerful, super intelligent AI systems, that it\u2019s going to find a solution to whatever problem you present it with that is going to be way too clever and that will involve doing things that you can\u2019t even imagine. Is that where this adversarial implication comes from?", "Ed:To an extent, yes. I think the simplest way to put it is that there are way more ways to misinterpret a wish than there are to correctly interpret a wish and the smarter you are, the more ways to misinterpret the wish, you\u2019re going to be able to think of. So the genie that you\u2019re facing is able to think of a lot of ways to misinterpret your wish, whereas you\u2019re like, \u201cNo, no, no. I want you to interpret it correctly in one of these areas.\u201d", "Jeremie:So I can you give an example of a wish actually that might help flush this out.", "Ed:Yeah, yeah. Let\u2019s say, I wish to\u2026 Something simple like I wish to be happy or something like that. Okay. You wish to be happy. If you\u2019re facing a person who really wants to be helpful and who has all the constraints of a person, they\u2019ll ask you like what do you want. I\u2019ll make you a cup of coffee, whatever makes you happy in the moment. But if you\u2019re facing a super intelligent genie, what the genie is going to say is first, it\u2019s going to be like, \u201cOkay. Well, how do you define happiness? There\u2019s a lot of fuzziness there. Is it that I\u2019m like smiling all the time?\u201d", "Ed:Well, if that\u2019s how the genie thinks what happiness is, it\u2019s going to grab onto my face and force me to smile forever. And great, it\u2019s done its job, but I\u2019m like, \u201cNo, no, no. Don\u2019t do that. Please stop.\u201d But the point is it\u2019s not listening. It doesn\u2019t want to listen. I\u2019ve already told it, make me happy. It thinks that making me happy means making me smile and more-", "Jeremie:Or any number of other things, right?", "Jeremie:It could be juicing you up on drugs, it could be any\u2026 Yeah.", "Ed:Yeah. Then I\u2019ll just inject you with cocaine or like I\u2019ll drill into your skull, pull out your brain and drop you in a vat of endorphins. That\u2019ll do it or whatever it is. But the problem is, once you\u2019ve given the genie that very first directive, from that point, it also has the incentive to prevent a change in that directive because it\u2019s going to\u2026 It doesn\u2019t want\u2026 Once, it has that directive, it\u2019s got this goal and it\u2019s going to try to do everything it can to accomplish that goal.", "Ed:One of the things that will reduce the chance of that goal being accomplished is if the genie\u2019s own goal is changed by you afterwards. So if you are able to tell the genie, \u201cOh, actually, no, no, no. Don\u2019t do that smiley face thing. That\u2019s really scary. Please don\u2019t do that.\u201d That reduces the chance of you ending up being happy and forcibly smiled.", "Ed:So the genie, before you tell it stop smiling, but after you told it make me smile is going to be like, \u201cOkay, now that I\u2019ve gotten this directive, I have to prevent my directive from being changed, defend myself against any attempt to change it including by the guy who just asked me to do this thing.\u201d So I better just force this guy to stop talking or just quickly kill him and make his face smile anyway. That\u2019s happy or some crazy stuff like that. This is the level of misinterpretation that we\u2019re talking about that\u2019s why it\u2019s dangerous.", "Jeremie:A lot of this reminds me of the early days of\u2026 Or not the early days. My first interaction with computers, right? I think everybody experiences this when you first start to code, you realize that, \u201cMan, this machine is taking me absolutely literally. I tell it to do something and\u2026\u201d I mean, I remember having this frustration. You write some code that you think should work. You think it should work and then you hit run and then it breaks and there\u2019s an error message. What do you do? You get frustrated not with yourself, but with the machine because your brain is telling you, \u201cOh, the machine screwed up. I know what I meant, but the machine has misinterpreted it.\u201d", "Jeremie:I think that there\u2019s a similar kind of error that gets made when people look at AI and start to think of it as if it\u2019s going to be default positive as an outcome. There\u2019s a lot of anthropomorphizing that people do where they imagine like, \u201cOh, but it\u2019ll get that I don\u2019t mean like this horrible way of achieving the thing that I just asked it to do.\u201d Or I won\u2019t have a genie take me literally is the assumption. Anyway, it strikes me that that really does reflect my own attitude when I was starting to learn how to program. It\u2019s a totally understandable way of approaching these problems.", "Ed:Yeah. The machine has no sympathy intrinsically except to the extent that we give it sympathy and we don\u2019t understand sympathy well enough to give a machine sympathy. And that\u2019s one part of the problem but it\u2019s a part of the problem.", "Jeremie:Right, okay. So we\u2019ve established that it\u2019s not going to be trivial to train this advanced AI in order to get it to do what we want. I mean, that\u2019s the other thing, right? When you tell a machine I would like them to be happy, there\u2019s so many things that get rolled into that. I mean philosophers have been trying to figure out what it means to be happy for thousands of years with no success and now, we\u2019re going to try to quantify that, complete uncertainty and feed it to machines.", "Jeremie:So I guess that\u2019s hinting at another part of the problem here. But suppose we got past that, and this is what you were saying, you refer to as the outer alignment problem. So what\u2019s the next layer of difficulty as hard as that was. What\u2019s the next piece here?", "Ed:Yeah. So the next piece is what\u2019s called the inner alignment problem. This is more like the genie you\u2019re talking to might be made up of a lot of internal components and those internal components might be working against the interests of the genie. So this is hard to picture, but maybe a good way of thinking about this kind of anthropomorphically is you think of a company. Let\u2019s say that instead of a genie, you were making a request to a company. Make me happy and the company charges you money or whatever and make you happy and something like that.", "Ed:So the company might well have a pathological tendency to make you happy or whatever, like Twitter does. I mean, I love Twitter but it does have this algorithm problem. But what\u2019s interesting about a company is that a company is made up of people that all are partly aligned with the goals of the company. The people work together to accomplish something that more than what any individual person in the company could have done. But they\u2019re not all rowing exactly the same direction.", "Ed:There are people inside the company that are more aligned than others. Some people really deeply believe in the company\u2019s mission and they\u2019ll put in all the hours they need. They\u2019re ambitious in the same direction. Other people have a variety of different motivations. They just want to get home at 5:00 pm and see their kids. That\u2019s one motivation. That\u2019s perfectly fine. There\u2019s other motivations like some people are going to be working just for their own ambition, their own personal ambition to rise to the ladder. They have no interest in the company\u2019s own success or outcome. And other people are just like they\u2019re literally freeloaders. They\u2019re literally doing crimes and embezzling the company and selling Twitter accounts to hackers or some crazy stuff like that.", "Ed:It\u2019s going to be a tiny, tiny fractional number of people like this. And the entire company itself, the goal of that company is to keep them all aligned and doing the same thing. But when we take that framework and transport it over onto something that\u2019s an AI, it\u2019s not actually clear to what extent. It\u2019s not clear what it takes to get the AI to get its internal components all aligned together.", "Ed:Actually, it\u2019s the same problem, basically is the outer alignment problem. It\u2019s as if the genie itself contains a bunch of little genies in it that are trying to solve the sub-problems, but those little genies might be acting against the main genie and the main genie to think carefully about the instructions it gives to its little genies.", "Jeremie:So what are some examples of sub-problems that might lead to the formation of these\u2026 By the way these little genies, right? I mean, in the language of AI alignment, these are mesa-optimizers. That\u2019s usually what they\u2019re referred to as. So a mesa-optimizer is it\u2019s kind of like an optimizer within an optimizer. So you have this machine learning algorithm and then within it there are a whole bunch of sub-problems that have to be solved in order for the overall algorithm to work.", "Jeremie:So if you\u2019re doing computer vision, right? Part of the overall problem of computer vision is for example, I don\u2019t know, recognizing edges and corners, right? So you might have a little mesa-optimizer and one that specializes in edges and another one that specializes in corners and these can take on a life of their own to some degree, I guess. That\u2019s the concern at least. And to the extent that they do that, they want to protect themselves in the same way\u2026 I guess this is what you\u2019re saying with outer alignment, right? In the same way as the outer aligned agent, doesn\u2019t want to be reprogrammed, it wants to retain its original purpose.", "Jeremie:These mesa-optimizers don\u2019t want to\u2026 Once they\u2019ve latched on to their sub-problem, they don\u2019t want that sub-problem to be changed. And so they desperately want to preserve their own structure, right?", "Ed:Yeah. So one example of this, if you go back to the genie putting your brain in like a vat of endorphins and whatnot, the genie, you give it the instruction like maybe happy and the genie is like, \u201cOh, I\u2019ll put his brain into a vat of endorphins. That\u2019s what I\u2019ll do.\u201d Then what it does is it assigns to one of its sub-components, the sub-problem of, \u201cOkay. In order to make this work, we\u2019re going to have to figure out a lot of stuff about human neurochemistry. So get working on that and we\u2019ve got a bunch of other sub-problems too.\u201d", "Ed:Then maybe the thing that\u2019s working on human neurochemistry has some version of the thought process of, \u201cOh, wow, man, this is a hard problem. I need lots of computers like more computing power than I currently have in order to solve this sub-problem.\u201d Okay, I\u2019m going to convert a lot of the atoms in the world and the solar system and the galaxy into more computers to make sure that I\u2019ve really, really well solved this sub-problem of figuring out human neurochemistry so then I can pass that solution upwards to the main optimizer that\u2019s solving the big problem.", "Ed:Oh, like maybe the big optimizer is using computers over here, but I want those computers to solve my sub-problem and so you get this kind of fight happen. It\u2019s actually, you can feel a similar thing happening within your own mind. So if you\u2019re deciding, \u201cHey, do I want to eat that cookie over there?\u201d It doesn\u2019t feel like you have a one coherent loss function for your entire life and you\u2019re trying to compute expected values over it. Instead it feels like there\u2019s a whole separate thing in your head that\u2019s dedicated to solving a sub-problem.", "Ed:So you have the hunger module in your head that\u2019s fighting you for control and the hunger module is like yes, yes go for the cookie, but you\u2019ve got a higher level process that goes like, \u201cOh, well hang on. How does that fit into my life goals of not getting fat or whatever your life goals are?\u201d It feels like you\u2019re fighting a part of yourself in that decision.", "Jeremie:Right, right. Well, there\u2019s so many places we could go with this having introduced these optimizers, but I think one thing that\u2019s worth pausing on and noting here is first off, this stuff can sound understandably a little bit out there and wild, right? I mean, we\u2019re talking really about machines that to some degree want to take over the world. I think it\u2019s worth pointing out or at least in some modes of operation, they will tend to\u2026 If they are sufficiently powerful, have sufficient compute, have access to enough data, eventually develop ambitions like that.", "Jeremie:It does sound wild, but it is a very, very well established\u2026 It\u2019s what\u2019s known as an instrumental objective of machine learning models in this kind of stage. It\u2019s something that at the very least, very serious researchers focus on as a problem. I mean, this is considered a very serious issue in the alignment community. Can we talk a little bit about this idea of instrumental objectives? What is an instrumental objective? Can you define that?", "Ed:Yeah. So what you said earlier about basically, we think that all systems that are smart enough or the vast majority of systems that are smart enough will want to, after some fashion, take over the world, which sounds a very extreme claim. And the reason is exactly this idea of instrumental goals and instrumental convergence. The idea is that there are certain resources and there are certain actions that are good bets to take and to seize, no matter what your goal is.", "Ed:So if your goal is to put my brain in a vat, you saw how, \u201cOh, the sub-optimizer wants more computers to make sure it absolutely nails the problem and gets it right.\u201d That\u2019s part of it. In order to solve a hard problem, you need lots of computers and when there are no real limits placed on your power, what you can do, there\u2019s no reason why you wouldn\u2019t just convert the entire Earth in every particle of mass that you can get your hands on into a computer. Again, there\u2019s nothing approaching sympathy that is even in the concept space of something like this.", "Jeremie:You could certainly argue that\u2019s what humans are trying to do already. I mean, we are quite literally converting as much of the world as we possibly can into compute resources, which we\u2019re then deploying to solve our own instrumental goals.", "Ed:Yeah. And even that is with\u2026 We have limits to our capabilities and even we have sympathy for the animals that we\u2019re killing, the trees we\u2019re destroying, the environment. Not all of us do, but enough of us do that we\u2019re like, \u201cHey, guys. We need natural preserve, national parks, all these sorts of things.\u201d That\u2019s because, we to an extent value these things enough that we\u2019re not willing to maybe utterly annihilate the planet to get them.", "Ed:But if our goal function were different, we wouldn\u2019t even care that much. Then yeah, who cares about the rain forest? Who cares about this and that. And that\u2019s ultimately what we would be dealing with. Imagine the plight of an African lion or something faced with that degree of destructiveness which is far worse than even the degree of destructiveness that we\u2019ve imposed on lions already. It\u2019s incomprehensible. Just zero sympathy, totally mechanical and utterly destructive.", "Jeremie:And this is I mean one famous example of how you get to this idea of the paperclip optimizer. It\u2019s the device that\u2019s told, \u201cHey, make paperclips.\u201d And just without any other context, it goes, \u201cOh, cool. There\u2019s some iron in the ground. There\u2019s some iron in your blood. There\u2019s some iron here, some iron there. It just collects iron from everywhere and you destroy the world making paperclips.", "Jeremie:Cool. So I guess one last note, I guess, on the instrumental convergence idea, there are instrumental goals or a number of different instrumental goals that many people consider to be plausible, things that machines will tend to optimize towards as a side effect of trying to optimize a main objective function. Can you list a couple of these? I know you\u2019ve alluded to a number of them, but maybe just so people can see some.", "Ed:Yeah. At a fundamental level, based on laws of physics that we know of today, matter and free energy, you want stuff and you want the juice to run it. So in other words, put matter together to build lots of computers so you can think as fast and as effectively as possible and grab the energy sources that you need to actually run those computers. And there are other instrumental goals like survival.", "Ed:Generally speaking, I can predict that if I survive, my goals are more likely to be accomplished than if I die. In most cases. If I have a certain set of goals like if I\u2019m around to continue pursuing them, those goals are more likely to get accomplished. So self-preservation is one of these instrumental subgoals and goal cohesiveness or goal preservation is also an instrumental subgoal.", "Ed:If I remain alive, but the goal that I\u2019m given in my brain is changed, then the previous goal that I had is also less likely to be accomplished because I would be no longer working on it after my goal has changed. So I\u2019m going to seek to prevent my goal from being changed.", "Ed:At least as we expect, we get basically one shot to set the goal right, because otherwise if we\u2019re like, \u201cOh, no, no, no. Wait, wait, wait,\u201d then presumably the system can be like, \u201cOh, no. Hang on, hang on. I got this. I got this. I got this goal and I\u2019m going to accomplish it.\u201d", "Jeremie:I think that reflects to some degree\u2026 Correct me if I\u2019m wrong. Maybe your tendency towards one of the camps in the AI alignment debate. So there\u2019s the camp that says, \u201cGuys, we get one shot at this. We got to make sure that we get our goals right.\u201d And there\u2019s another camp that says, \u201cGuys, we should be focused on robustness making models that account for their uncertainty with respect to the goals that they pursue and trying to be corrigible.\u201d In other words invite corrections and be robust in that sense. I just wanted to flag the distinction between the two, because it is two branches of the ecosystem. Would that be fair to say?", "Ed:Yeah. The argument starts to get a bit technical in those areas. I would say that both are viable directions and certainly there are probably going to be phases before we make some kind of super evil genie that can destroy the world where we definitely want to be more in learning mode than in, \u201cAh, I got this exactly right mode,\u201d so that we can actually look at like, \u201cOkay, what is systems that are pretty smart and maybe dangerous, but not necessarily instantaneously destructive? What do they do after they\u2019ve been given a set of instructions? Is there a way we can make them okay with having a switch flipped so they\u2019re stopped? All that sort of thing.", "Jeremie:Cool. Okay. So having laid that foundation, I think a lot of this will be\u2026 Well, it\u2019ll probably be old news to people who are familiar with the AI alignment ecosystem, but maybe new stuff for folks who are joining us from the last series where we\u2019re doing more about data science career focused stuff. One thing, I want to do is because you and I have talked a lot about mesa-optimizers. I think one of the really interesting things that the concept of mesa-optimizers does, is it does give us, I think a pretty interesting perspective on what humanity is, what life is, what the universe is really and I wanted to explore that, if you\u2019re okay with it, just airing some of our private conversations out here a little bit. This is part of the reason why I wanted to invite you on the podcast. So are you cool with that?", "Jeremie:Okay, great. So I guess maybe I\u2019m trying to think of a question to start us off with, but maybe I\u2019ll just provide a framing and you can take it from there. So we\u2019ve discussed the idea that essentially every organism in the universe can be seen as a mesa-optimizer. So essentially what\u2019s happening is the universe has a whole bunch of atoms, it has a whole bunch of photons, particles all kinds of different degrees of freedom and it\u2019s running this experiment.", "Jeremie:It\u2019s training on whatever reshuffling of particles will randomly tend to occur. So you\u2019ve got this interaction between particles that\u2019s going on over time and over time it seems that the jumbling of particles is tending towards self-assembly of complex systems. And whatever the final end state of the universe is like, it seems like we\u2019re tending towards one of greater, and greater, and greater complexity to the extent that humans don\u2019t wipe themselves out, to the extent that we actually survive to the point where we create a self-improving AI system.", "Jeremie:It really seems we\u2019re iterating on our way to something like an intelligence explosion as it\u2019s kind of been referred to. In that framing, we\u2019re here to compete with all these other mesa-optimizers around us. So we want access\u2026 Like you were saying, you\u2019ve got that mesa-optimizer that\u2019s specialized in understanding human neurochemistry so that the overall problem of putting your brain in a vat of endorphins can be solved. Well humans, are focused on solving the problem of being really, really good humans in surviving and propagating. Maybe I\u2019ll park the thought there.", "Ed:It\u2019s a good place to start. We and all the other animals and organisms in the world, we\u2019re all on the inside of a giant optimizer that\u2019s evolving us, right? So evolution is kind of the big process that\u2019s trying to drive us towards genetic fitness. Our evolutionary directive is have lots of kids relative to the size of the population. Increase your genetic representation in the next generation. That\u2019s our evolutionary directive.", "Ed:What\u2019s interesting is that especially recently, humans really don\u2019t seem to be following that directive very well. If we were actually following that directive, if we were actually like, \u201cAll right. I want as many kids as possible compared to\u2026\u201d We would be acting quite differently. We would not be using birth control for example. We would probably not be using condoms. There\u2019s a bunch of other things that we will be having more kids as opposed to fertility drops and stuff like that.", "Ed:So this is a signal that something has gone wrong. There\u2019s something mismatched between evolutions like slow optimization pressure and the stuff that we\u2019re doing and learning to do. We are actuated by desires like hunger, sex drive, fear, anger. All these sorts of things that correlated very well to our inclusive genetic fitness for a very long time. These are really complex adaptations. They evolved over millions and millions of years.", "Ed:Many animals that are unrelated to us, probably feel them like animals probably feel fear and all of that sort of thing. So something is happening. There\u2019s an increasing amount of space between evolution telling us what to do in a certain sense and our own internal drives, telling us what to do. Our internal drives were able to satisfy those internal drives in ways that evolution would look at, if it was a person and be like, \u201cHey, whoa. That\u2019s pathological.", "Ed:You guys are doing something wrong here. You\u2019re optimizing too much for your hunger. You\u2019re getting fat and not having kids. What\u2019s this? You\u2019re optimizing too much for your sex drive and having sex with condoms, you\u2019re not having kids.", "Jeremie:It almost feels like from evolution standpoint and evolution is like the human trying to get the code and the AI right so that it doesn\u2019t go off track, and what\u2019s happening now is evolution programs you to have a whole bunch of kids and then instead of getting horny and having sex with people and having kids, you start to watch pornography and evolution goes, \u201cWhoa, whoa, whoa. That\u2019s not what I meant. You can\u2019t cheat like that.\u201d", "Ed:And that\u2019s the key. Evolution isn\u2019t actually programming us to have kids, evolution found an easier way to do it. Evolution was like, \u201cOh, if I just put these little actuators on my thing, the organism is going to figure out how to have kids from there.\u201d The problem is when the organism gets really, really smart, it figures out how to satisfy those desires without doing the thing that evolution wanted it to do.", "Jeremie:It transcends that initial loss function basically. So you alluded to more and more daylight between what evolution wants humans to do and what humans end up actually doing. I think it\u2019s worth exploring what the source of that daylight is because I mean correct me if I\u2019m wrong, based on our conversations I think we\u2019re aligned on this, but I think it comes down to computational capacity. Eventually the organism is capable of running more computation than evolution itself.", "Jeremie:You\u2019re able to essentially\u2026 You have enough compute power that natural selection isn\u2019t the main thing that\u2019s changing you. Now, the main thing that\u2019s changing you is your own thinking, your own cognitive capacity. So over the course of a human lifetime, you can actually change yourself much more than evolution would allow if you were like a water buffalo or something.", "Ed:Yeah. I think that this has to do with how dense versus how sparse the internal versus the external feedback signals are. And how many processing steps you get-", "Jeremie:Can you elaborate on that? What do you mean by dense and sparse here?", "Ed:So evolution sends you a really sparse fitness signal. It sends a fitness signal that\u2019s like, it happens every 20 or 30 years. Do you have kids? Do they have kids? Every 20 or 30 years. Did you die in that time before having kids? That\u2019s a very smart signal. It doesn\u2019t like punch you in the face very often, but it punches hard when it does. Whereas, your own internal processes, send you\u2026 In that 30-year time span, you might get, god, I don\u2019t even know maybe, I don\u2019t know, you get hungry, god knows what three times a day and whatever that is in 30 years like thousands and thousands of times.", "Jeremie:You bump your elbow and you scratch your knee.", "Ed:Yeah. That\u2019s it, that\u2019s it. So what happens is you have degrees of freedom in between those pulses of evolutionary pressure signal to adapt yourself around your own brain signals. So the way it works is that evolution is providing feedback pressure on the way your own signals are arranged. So evolution manages the balance between how hungry do you get? How horny do you get? How mad do you get? How happy you get? All those things. Evolution manages that on a time scale of decades. But within your brain, you have a set arrangement of those signals and you\u2019re internally optimizing and processing on a time scale that\u2019s many tens of thousands to millions of times faster than that.", "Ed:And as a result, you have the space to be super focused on those signals in a way that evolution doesn\u2019t really account for. Our processing power is now too great to be ruled by these signals. It\u2019s to the point where it\u2019s plausible that soon we\u2019ll be able to alter our own DNA directly and just completely decouple ourselves.", "Ed:We\u2019re still slaves to evolution, but from evolution\u2019s point of view where these misshapen pathological slaves that are no longer slaves to it directly, but that are slaves to the clumsy clunky things that it built.", "Jeremie:Well, it\u2019s interesting that you\u2019re saying from evolution\u2019s perspective, right? But evolution of course is whatever evolution gives rise to. I mean, by definition we are the goal. So to me, this is exactly that difference between\u2026 Well, basically, this is the outer alignment issue, something is being optimized for in this universe. It\u2019s clearly not biological evolution. That was what was being optimized for. For a period of time of about 14 billion years leading up to the present day. But in the last few hundred thousand years, human beings have started to decouple, to lift off as our cognitive capacity has created daylight between what evolution wants us to do and the timeline as you said on which evolution gives us feedback, and the timeline on which we\u2019re able to get feedback from our environments.", "Jeremie:That\u2019s not by any means the end of the process, right? This is what brings us back to the AI discussion. There\u2019s an extra step here where we shift substrates where we\u2019re no longer running computation on biological hardware, but we\u2019re running computation on a substrate of silicon rather than cells. And to the extent that that happens, you start to relax a lot of the constraints that make humans so incredibly slow compared to machines and we might have the same paradigm play out at an even tighter time scale.", "Ed:Yeah, this is right. So one of the issues with computers is that yeah, they\u2019re so much faster now. It\u2019s like they\u2019re as much faster than us in terms of the serial depth of a computation. In other words, I do this then I do that, then I do that. The number of things that they can do one after the other, they\u2019re as much faster than us as we are faster than the evolutionary process in terms of its ability to switch out genes and test out new life forms and so forth.", "Ed:So as a result, you can make the analogy of like, \u201cOh, I build this thing and it\u2019s so much faster than me.\u201d I give it these kind of clunky, coarsely defined goals that correlate pretty well to my goals at first, but then it optimizes, and optimizes, and optimizes. It gets smarter and smarter. And very, very quickly, perhaps is able to satisfy the course goals that I gave it in a way that I had no idea. I was like, \u201cOh my god. This is not what I meant. Stop, stop, stop. But by then, we don\u2019t care. Evolution might be thinking the same thing about us, but we don\u2019t care about it. We do what we want.", "Jeremie:Yeah. Just as evolution seems to stand still relative to the pace of a human life, a human life will appear to stand still relative to the pace of the evolution of this artificial intelligence. I mean, whatever form it takes. So I mean I think that\u2019s a big pro in favor of your earlier argument that we have one shot to get this right. I guess one of the concerns too as we start to look into this domain, or time domain where AI is starting to pick up, more and more progress is being made, more and more general systems are being built, things like GPT3, but not only that. We\u2019re going to get more and more advanced systems very soon.", "Jeremie:We start to get in this domain where we\u2019re at the mercy of the least AI safety aware company that wants to design a program. I mean, this is really the domain of AI policy where you start to say, \u201cOkay, how do you get the game theory to work here?\u201d How do you prevent some foolish company that decides to say, \u201cOh, I\u2019m not particularly concerned about AI alignment. I\u2019m not worried about this.\u201d How do you prevent them from then taking some very irresponsible action in this direction? I know this may be less so your area, but do you have any thoughts on the AI policy side with respect to that game theory?", "Ed:Well, yeah. So on the economics of it, there\u2019s a lot of talk about democratizing AI and that\u2019s good. That\u2019s good to a point, I think. When you\u2019re starting to talk about these very, very big and extremely capable systems that start to potentially present dangers not just like they could take over the world, but danger is like long before that. They could be abused by someone to send massive amounts of crafted spam or just all sorts of novel kinds of risks.", "Ed:So the economics of it are that the closer to a monopoly you are, the more margin your business has. And here, I\u2019m talking about it as though it\u2019s a business because these models are being built to be sold as APIs and all of this.", "Jeremie:And we say the more margin, really you mean the more profit.", "Ed:That\u2019s part of what I mean. When I say margin, that margin can go to profit. It can absolutely go to profit and up until this point, it\u2019s gone to profit, but it can also go to safety. It can go to safety investment. So when you\u2019re a monopoly, let\u2019s say you charge like a 50% profit margin. 50% of the dollars that come in are yours to keep and you can do whatever you want with them. Yes, you can use them on your balance sheet and like, \u201cOh, now, I have more money. That\u2019s great.\u201d", "Ed:But another thing you do with them is commit them to keeping your systems secure and safe and that takes an increasing amount of investment. One of the issues of having a widely competitive landscape with lots and lots of these different models being sold is that when they compete against each other, each company with each model, they compete their margins away. And in capitalism, that\u2019s generally seen as good. That\u2019s good because it means that consumers are paying less.", "Ed:If a company is mean or evil or whatever, I can switch to another company and it\u2019s cheaper for everybody. In the case of a system that has the potential to be dangerous, what you kind of want ideally is one monopoly that\u2019s spending all that\u2019s margin budget on safety and not having too many with limited amounts of margin they\u2019re competing with each other, so they can\u2019t spend it on safety. This is potentially a risky arrangement to have.", "Jeremie:Yeah. I\u2019ve seen arguments for what\u2026 This is called multi-polarity or uni-polarity. Essentially the idea of like how many different poles, how many different companies or organizations have active AI efforts or cutting edge AI efforts and how does that affect the risk landscape? I\u2019ve seen arguments that say multi-polarity might be more desirable. I personally lean definitely in the direction that you\u2019ve just discussed there. I know we\u2019ve talked about it a fair bit. I think another pretty convincing\u2026 Oh, sorry. Did you want to add something?", "Ed:I was just going to say you might be able to make multi-polarity work if you had an organization that was purely dedicated to safety, very well funded, and also had a lot of internal transparency into what all the other companies were building. That might be a way to work, but that seems hard at least in the current arrangement of things.", "Jeremie:Yeah. I know open AI, put together a piece of policy work researching basically like moving towards a more transparent way of developing AI internally within organizations. A big part of the game theory behind this is trust. If you\u2019re Google and you\u2019re working towards a super powerful AGI that could potentially be dangerous and Facebook tells you they\u2019re working towards that as well, and you both claim to each other that you\u2019re investing a lot of effort into safety to make sure nothing horrible happens.", "Jeremie:How much should you trust that that\u2019s actually happening at the other company and that they\u2019re not actually just taking all their margin and throwing it out competing you to develop a product faster.", "Ed:And that\u2019s one of the good scenarios. These are two companies that are culturally aligned, exchange employees a lot, are based roughly on the same strip of the Bay Area. They\u2019re extremely similar companies in a lot of ways and how does the trust picture change when you\u2019re dealing with companies that are from different countries, don\u2019t speak the same language, have never exchanged employees, have different governmental structures and fundamentally different value systems. It becomes quite different and more difficult for trust to be established in that case.", "Jeremie:Yeah. I mean, maybe the most appropriate analogy is well, it depends how you take it. So people, especially those who argue for multi-polarity one common argument focuses on the more Mesoscopic domain between super powerful systems and current day systems where they say, \u201cLook, there\u2019s the risk of AI being deployed to weapon systems.\u201d Now, it may be good that you have multi-polarity, because who knows how the Cold War would have unfolded if just one country had access to nukes and no other country could engage in mutually destruction with them, thereby making sure that nobody would ever fire their nukes. Maybe something like that would happen with AI. I think there are compelling reasons why that\u2019s not the case or at least I\u2019ve taken that view in the past. But yeah, sorry.", "Ed:I think this is basically the capitalistic argument for competition that under all of the circumstances that we\u2019ve seen pretty much up to this point, the competition has been a good thing. It\u2019s been good that a lot of different organizations have similar technologies. They compete with each other and keep each other in check and so forth. This may continue to be true with AI, up to a certain level of capability, but I would suspect that there\u2019s going to come a level of capability beyond which safety investment becomes more important than capability investment. And that\u2019s the point at which you basically need a lot of margin to commit to safety. So some arrangement that\u2019s equivalent or isomorphic to a monopoly.", "Jeremie:Yeah. I mean, certainly we see that with tabletop bioweapons or bioengineering where designer pathogens are going to be a plausible reality fairly soon. And likewise with nuclear weapons, right? I mean you get to a point where, okay, it\u2019s time to stop publishing results out in the open. I think open AI itself to their credit, at least from my perspective has\u2026 And they\u2019ve taken a lot of flag for it. They\u2019ve come out and said, \u201cLook, as we start to approach greater and greater capabilities, we\u2019re going to have to start bringing some of our work in-house and not publishing as widely.\u201d", "Jeremie:They\u2019ve been more and more cautious, we heard with GPT2, their first step in that direction. Now GPT3 is entirely in-house. None of it is public domain. It\u2019s not just an open source model. Anyway, it\u2019s going to be interesting to see how the field evolves and develops. Obviously, there\u2019s a lot of controversy with any decision made in the space. I think given the stakes though, one of the key things is really targeting policies that are robustly good as you and I have seen as we\u2019ve engaged with everyone from policymakers to AI alignment researchers.", "Jeremie:It\u2019s so easy to make moves that end up doing more harm than good if you\u2019re not really, really careful in this space. So I guess what I wanted to do was close off this conversation with just an appeal to people who are listening. If you\u2019re interested in AI alignments, AI safety, anything like that. If you find the risks that we\u2019ve been talking about here, compelling and interesting to work on, then check out some of the links that we\u2019ll provide with the blog post in the description of the video because there are a number of organizations you might want to check out just to see what your options are, where you might be able to contribute, but looking at things through the lens of what\u2019s robustly good, what\u2019s likely to really not do more harm than good under unexpected shifts because things can go wrong in surprising ways when you\u2019re talking about cutting edge technology. Anyway, just something I wanted to throw out there because we\u2019ve run into some of these issues ourselves.", "Ed:Yeah. I think that makes a lot of sense. It\u2019s sort of a first do no harm type of thing. It\u2019s a hippocratic oath, I guess on a bigger scale, but I think that\u2019s where the first effort should be directed is like, \u201cHow can we do stuff that avoids bad consequences going forward and keeps everyone in a nice state?\u201d", "Jeremie:Awesome. Okay. Well, Ed, I really appreciate it. I think what we\u2019ll do is I\u2019ll link obviously your own personal website in the description of all this stuff too. Anything you wanted to share? I guess do you mind sharing your Twitter, so people can follow you?", "Ed:Sure, yep. I\u2019m @neutronsNeurons or Neurons Neutrons. I actually forget. Sorry, man.", "Jeremie:I think\u2026 Oh, shoot. We\u2019ll, link to it.", "Ed:Neutrons Neurons. I\u2019m @neutronsNeurons. So you can hit me up and drop a follow there.", "Jeremie:Fantastic. All right. Well, thanks so much, Ed. I really appreciate it. Great conversation.", "Jeremie:My pleasure. We\u2019ll keep it going offline. Cheers.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3980bb9fdd39&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://youtu.be/-rpd1geSNXM", "anchor_text": "YOUTUBE"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/podcast/home", "anchor_text": "TDS podcast"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----3980bb9fdd39---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3980bb9fdd39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&user=Jeremie+Harris&userId=59564831d1eb&source=-----3980bb9fdd39---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3980bb9fdd39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&source=-----3980bb9fdd39---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/", "anchor_text": "AI alignment"}, {"url": "https://80000hours.org/articles/ai-policy-guide/", "anchor_text": "AI policy"}, {"url": "https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/", "anchor_text": "amazing podcast with OpenAI\u2019s Paul Christiano"}, {"url": "https://www.fhi.ox.ac.uk/govai/#home", "anchor_text": "GovAI"}, {"url": "https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence", "anchor_text": "Open Philanthropy Project"}, {"url": "https://twitter.com/neutronsNeurons", "anchor_text": "follow Ed on Twitter here"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://medium.com/tag/ai-safety?source=post_page-----3980bb9fdd39---------------ai_safety-----------------", "anchor_text": "Ai Safety"}, {"url": "https://medium.com/tag/ai-alignment?source=post_page-----3980bb9fdd39---------------ai_alignment-----------------", "anchor_text": "Ai Alignment"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----3980bb9fdd39---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/tag/fairness-and-bias?source=post_page-----3980bb9fdd39---------------fairness_and_bias-----------------", "anchor_text": "Fairness And Bias"}, {"url": "https://medium.com/tag/ai-alignment-and-safety?source=post_page-----3980bb9fdd39---------------ai_alignment_and_safety-----------------", "anchor_text": "Ai Alignment And Safety"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3980bb9fdd39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&user=Jeremie+Harris&userId=59564831d1eb&source=-----3980bb9fdd39---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3980bb9fdd39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&user=Jeremie+Harris&userId=59564831d1eb&source=-----3980bb9fdd39---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3980bb9fdd39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----3980bb9fdd39---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=-----3980bb9fdd39---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Written by Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "122K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----3980bb9fdd39---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femerging-problems-in-machine-learning-making-ai-good-3980bb9fdd39&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=-----3980bb9fdd39---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----3980bb9fdd39----0---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----3980bb9fdd39----0---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----3980bb9fdd39----0---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3980bb9fdd39----0---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----3980bb9fdd39----0---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "The 4 fastest ways not to get hired as a data scientistAvoiding these common mistakes won\u2019t get you hired. But not avoiding them guarantees your application a one-way ticket to the \u201cno\u201d pile."}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----3980bb9fdd39----0---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "6 min read\u00b7Jun 12, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F565b42bd011e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e&user=Jeremie+Harris&userId=59564831d1eb&source=-----565b42bd011e----0-----------------clap_footer----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----3980bb9fdd39----0---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F565b42bd011e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e&source=-----3980bb9fdd39----0-----------------bookmark_preview----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3980bb9fdd39----1---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3980bb9fdd39----1---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----3980bb9fdd39----1---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3980bb9fdd39----1---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3980bb9fdd39----1---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3980bb9fdd39----1---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----3980bb9fdd39----1---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----3980bb9fdd39----1-----------------bookmark_preview----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3980bb9fdd39----2---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----3980bb9fdd39----2---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----3980bb9fdd39----2---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3980bb9fdd39----2---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3980bb9fdd39----2---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3980bb9fdd39----2---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----3980bb9fdd39----2---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----3980bb9fdd39----2-----------------bookmark_preview----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----3980bb9fdd39----3---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----3980bb9fdd39----3---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----3980bb9fdd39----3---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----3980bb9fdd39----3---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----3980bb9fdd39----3---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "Why you\u2019re not a job-ready data scientist (yet)You\u2019re getting rejected for a reason, but it\u2019s almost always something you can fix."}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----3980bb9fdd39----3---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": "6 min read\u00b7Jun 16, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a0d73f15012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012&user=Jeremie+Harris&userId=59564831d1eb&source=-----1a0d73f15012----3-----------------clap_footer----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----3980bb9fdd39----3---------------------b91809c0_99c7_4859_aac2_8ef0972ebf56-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "38"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a0d73f15012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012&source=-----3980bb9fdd39----3-----------------bookmark_preview----b91809c0_99c7_4859_aac2_8ef0972ebf56-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "See all from Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----3980bb9fdd39----0-----------------bookmark_preview----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----3980bb9fdd39----1-----------------bookmark_preview----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----3980bb9fdd39----0---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----3980bb9fdd39----0-----------------bookmark_preview----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Aleid ter Weel"}, {"url": "https://medium.com/better-advice?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Better Advice"}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "10 Things To Do In The Evening Instead Of Watching NetflixDevice-free habits to increase your productivity and happiness."}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "\u00b75 min read\u00b7Feb 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-advice%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&user=Aleid+ter+Weel&userId=6ffe087f07e5&source=-----4e270e9dd6b9----1-----------------clap_footer----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----3980bb9fdd39----1---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "204"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&source=-----3980bb9fdd39----1-----------------bookmark_preview----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----3980bb9fdd39----2---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----3980bb9fdd39----2---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----3980bb9fdd39----2---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Babar M Bhatti"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----3980bb9fdd39----2---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Essential Guide to Foundation Models and Large Language ModelsThe term Foundation Model (FM) was coined by Stanford researchers to introduce a new category of ML models. They defined FMs as models\u2026"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----3980bb9fdd39----2---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "\u00b714 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&user=Babar+M+Bhatti&userId=10dee34829b&source=-----27dab58f7404----2-----------------clap_footer----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----3980bb9fdd39----2---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&source=-----3980bb9fdd39----2-----------------bookmark_preview----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/@toni.becerra/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?source=read_next_recirc-----3980bb9fdd39----3---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/@toni.becerra?source=read_next_recirc-----3980bb9fdd39----3---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/@toni.becerra?source=read_next_recirc-----3980bb9fdd39----3---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "Unbecoming"}, {"url": "https://medium.com/@toni.becerra/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?source=read_next_recirc-----3980bb9fdd39----3---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "10 Seconds That Ended My 20 Year MarriageIt\u2019s August in Northern Virginia, hot and humid. I still haven\u2019t showered from my morning trail run. I\u2019m wearing my stay-at-home mom\u2026"}, {"url": "https://medium.com/@toni.becerra/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?source=read_next_recirc-----3980bb9fdd39----3---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": "\u00b74 min read\u00b7Feb 16, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fa6f367f02e53&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40toni.becerra%2F10-seconds-that-ended-my-20-year-marriage-a6f367f02e53&user=Unbecoming&userId=822077a7247e&source=-----a6f367f02e53----3-----------------clap_footer----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/@toni.becerra/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?source=read_next_recirc-----3980bb9fdd39----3---------------------db2b0841_b99c_4ede_9dab_bfb772a44676-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "733"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa6f367f02e53&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40toni.becerra%2F10-seconds-that-ended-my-20-year-marriage-a6f367f02e53&source=-----3980bb9fdd39----3-----------------bookmark_preview----db2b0841_b99c_4ede_9dab_bfb772a44676-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----3980bb9fdd39--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}