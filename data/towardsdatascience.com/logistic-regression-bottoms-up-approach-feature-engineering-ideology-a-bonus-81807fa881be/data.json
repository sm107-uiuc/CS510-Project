{"url": "https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be", "time": 1683001575.549355, "path": "towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be/", "webpage": {"metadata": {"title": "Logistic Regression: Bottoms-up Approach. | by Hemanth Devarapati | Towards Data Science", "h1": "Logistic Regression: Bottoms-up Approach.", "description": "Can we use RMSE or MSE as an evaluation metric for classification? If yes, when can we use it? If not why can\u2019t we use it? This article will walk you through the nuances of logistic regression and I\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/bhavikbb/password-strength-classifier-dataset", "anchor_text": "data set", "paragraph_index": 5}, {"url": "https://www.youtube.com/watch?v=o4QmoNfW3bI", "anchor_text": "probability", "paragraph_index": 6}, {"url": "https://www.youtube.com/watch?v=GxbXQjX7fC0", "anchor_text": "odds", "paragraph_index": 6}, {"url": "http://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/", "anchor_text": "An Intuitive Guide To Exponential Functions & e", "paragraph_index": 6}, {"url": "http://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/", "anchor_text": "Demystifying the Natural Logarithm (ln)", "paragraph_index": 6}, {"url": "http://nbviewer.jupyter.org/github/justmarkham/DAT8/blob/master/notebooks/12_e_log_examples.ipynb", "anchor_text": "brief summary", "paragraph_index": 6}, {"url": "https://nbviewer.jupyter.org/github/hdev7/medium-article-logistic-regression-follow-up-notebook/blob/master/logistic%20regression%20bottoms%20up%20approach.%20%28Feature%20engineering%20ideology%20-%20a%20bonus%29.ipynb", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://medium.com/@hemanthsaid7/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323", "anchor_text": "Linear Regression", "paragraph_index": 15}, {"url": "http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "anchor_text": "independent and identically distributed", "paragraph_index": 20}, {"url": "http://en.wikipedia.org/wiki/Convex_optimization", "anchor_text": "convex optimization", "paragraph_index": 24}, {"url": "http://en.wikipedia.org/wiki/Convex_function", "anchor_text": "convex", "paragraph_index": 24}, {"url": "http://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent", "paragraph_index": 24}, {"url": "https://github.com/Shathra/gradient-descent-demonstration/blob/master/gradient_descent_1d.gif", "anchor_text": "Here", "paragraph_index": 26}, {"url": "http://www.mathwords.com/l/logarithm_rules.htm", "anchor_text": "log rules", "paragraph_index": 28}, {"url": "https://www.math.hmc.edu/calculus/tutorials/quotient_rule/", "anchor_text": "quotient rule", "paragraph_index": 29}, {"url": "https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/", "anchor_text": "confusion matrix", "paragraph_index": 47}, {"url": "https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/", "anchor_text": "post", "paragraph_index": 62}, {"url": "https://www.linkedin.com/in/hemanthsaid/", "anchor_text": "LinkedIn", "paragraph_index": 65}], "all_paragraphs": ["Can we use RMSE or MSE as an evaluation metric for classification? If yes, when can we use it? If not why can\u2019t we use it?", "I will disclose the answer in the end.", "What is in it for you?", "This article will walk you through the nuances of logistic regression and I will make you familiar with the feature engineering ideology. The first thing I want to clarify is logistic regression uses regression in its core but is a classification algorithm.", "This is my second favorite algorithm after decision trees merely for the simplicity and robustness it offers. Despite having a ton of classification algorithms out there this age out algorithm survived the test of time and you could see its seamless integration with the neural networks is what makes it very special.", "To meet the objectives I have picked up a data set that will allow me to discuss feature engineering as well. It is a password strength classification dataset. It has 3 classes in it, 0- weak, 1- medium and 2- strong. It has only two columns which are password and it\u2019s strength class. The dataset has around 700K records so be assured it is not a toy dataset.", "This article expects a few probability concepts from your end. Please refresh probability (5 minutes) and odds (8 minutes). Read these excellent articles from BetterExplained: An Intuitive Guide To Exponential Functions & e and Demystifying the Natural Logarithm (ln). Then, review this brief summary of exponential functions and logarithms.", "OMG! I don\u2019t have the right data.", "You can follow the code here.", "Pitfall 1: In the password dataset it is possible to have commas(,) and you are reading CSV file which is a common separated value file. You might read more commas than you need and might land in an exception. So, you can sidewalk from this by setting error_bad_lines = False.", "The data we have is a list of passwords, which pandas identify as \u2018object type\u2019. But, in order to work with an ML model, we need numerical features. Let\u2019s generate new features out of the text we have. This is where feature engineering saves us. Feature engineering often works on intuition. There is no formulated right way of doing it. Usually, in industry people tend to rely on domain expertise in identifying new features. As per our data you are the domain expert. You know what are the key elements in an ideal password and we will start from there.", "So we want to return a value between 0 and 1 to make sure we are actually representing a probability. To do this we will make use of the logistic function. The logistic function mathematically looks like this:", "Let\u2019s take a look at the plot", "You can see why this is a great function for a probability measure. The y-value represents the probability and only ranges between 0 and 1. Also, for an x value of zero, you get a .5 probability and as you get more positive x values you get a higher probability and more negative x values a lower probability.", "Okay \u2014 so this is nice, but how the heck do we use it? Well we know we have five attributes \u2014 char_count, numerics, alpha, vowels, consonants\u2014 that we need to somehow use in our logistic function. One pretty obvious thing we could do is:", "Where CC is our value for char_count, N is our value for numerics, A is our value for alpha, V is our value for vowels, CO is our value for consonants. For those of you familiar with Linear Regression this looks very familiar. Basically, we are assuming that x is a linear combination of our data plus an intercept. For example, say we have a password with a char_count of 8, numerics of 4, alpha of 4, vowels of 1 and consonants of 3. Some oracle tells us that \ud835\udefd0=1, \ud835\udefd1=2, \ud835\udefd2=4, \ud835\udefd3=6, \ud835\udefd4=8 and \ud835\udefd5=10. This would imply:", "Plugging this into our logistic function gives:", "So we would give a 100% probability to a password with those features is medium (I took first row data)", "Okay \u2014 makes sense. But who is this oracle giving us our \ud835\udefd values? Good question! This is where the learning in machine learning comes in :). We will learn about our \ud835\udefd values.", "For the sake of simplicity, we will remove one class and make it a binary classification problem. Later in the article, we will see how to do multi-classifications as well. For now, I\u2019m mixing medium and weak as one class. If you have been around machine learning, you probably hear the phrase \u201ccost function\u201d thrown around. Before we get to that, though, let\u2019s do some thinking. We are trying to choose \ud835\udefd values to maximize the probability of correctly classifying our passwords. That is just the definition of our problem. Let\u2019s say someone did give us some \ud835\udefd values, how would we determine if they were good values or not? We saw above how to get the probability for one example. Now imagine we did this for all our password observations \u2014 all 700k. We would now have 700k probability scores. What we would hope is that for the strong passwords, the probability values are close to 1 and for the weak passwords the probability is close to 0.", "But we don\u2019t care about getting the correct probability for just one observation, we want to correctly classify all our observations. If we assume our data are independent and identically distributed, we can just take the product of all our individually calculated probabilities and that is the value we want to maximize. So in math, If we define the logistic function and x as:", "The \u220f symbol means to take the product for the observations classified as that password. Here we are making use of the fact that our data is labeled, so this is called supervised learning. Also, you will notice that for weak observations we are taking 1 minus the logistic function. That is because we are trying to find a value to maximize, and since weak observations should have a probability close to zero, 1 minus the probability should be close to 1. So we now have a value we are trying to maximize. Typically people switch this to minimization by making it negative:", "Note: minimizing the negative is the same as maximizing the positive. The above formula would be called our cost function.", "So now we have a value to minimize, but how do we actually find the \ud835\udefd\u03b2 values that minimize our cost function? Do we just try a bunch? That doesn\u2019t seem like a good idea\u2026", "This is where convex optimization comes into play. We know that the logistic cost function is convex \u2014 just trust me on this. And since it is convex, it has a single global minimum which we can converge to using gradient descent.", "Here is an image of a convex function:", "Now you can imagine, that this curve is our cost function defined above and that if we just pick a point on the curve, and then follow it down to the minimum we would eventually reach the minimum, which is our goal. Here is an animation of that. That is the idea behind gradient descent.", "So the way we follow the curve is by calculating the gradients or the first derivatives of the cost function with respect to each \ud835\udefd. So let's do some math. First realize that we can also define the cost function as:", "This is because when we take the log our product becomes a sum. See log rules. And if we define \ud835\udc66\ud835\udc56 to be 1 when the observation is strong and 0 when weak, then we only do h(x) for strong and 1 \u2014 h(x) for weak. So let's take the derivative of this new version of our cost function with respect to \ud835\udefd0. Remember that our \ud835\udefd0 is in our x value. So remember that the derivative of log(x) is 1/\ud835\udc65, so we get (for each observation):", "And using the quotient rule we see that the derivative of h(x) is:", "And the derivative of x with respect to \ud835\udefd0 is just 1. Putting it all together we get:", "Bring in the negative and sum and we get the partial derivative with respect to \ud835\udefd0 to be:", "Now the other partial derivatives are easy. The only change is now the derivative for \ud835\udc65\ud835\udc56 is no longer 1. For \ud835\udefd1 it is CC\ud835\udc56 and for \ud835\udefd2 it is Ni. So the partial derivative for \ud835\udefd1 is:", "So now that we have our gradients, we can use the gradient descent algorithm to find the values for our \ud835\udefds that minimize our cost function. The gradient descent algorithm is very simple:", "Here \ud835\udefc is our learning rate. Basically how large of steps to take on our cost curve. What we are doing is taking our current \ud835\udefd value and then subtracting some fraction of the gradient. We subtract because the gradient is the direction of greatest increase, but we want the direction of the greatest decrease, so we subtract. In other words, we pick a random point on our cost curve, check to see which direction we need to go to get closer to the minimum by using the negative of the gradient, and then update our \ud835\udefd values to move closer to the minimum. Repeat until converge means keep updating our \ud835\udefd values until our cost value converges \u2014 or stops decreasing \u2014 meaning we have reached the minimum. Also, it is important to update all the \ud835\udefd values at the same time. Meaning that you use the same previous \ud835\udefd values to update all the next \ud835\udefd values.", "Follow the code for the implementation. After building the model from scratch we got an accuracy of 76% and I have used the sklearn package and got an accuracy of 99% this is pretty good.", "So gradient descent is one way to learn our \ud835\udefd values, but there are some other ways too. Basically these are more advanced algorithms that I won\u2019t explain, but that can be easily run in Python once you have defined your cost function and your gradients. These algorithms are:", "I will leave it here for you to explore. I would suggest not many know about these advanced optimization techniques rather they work well with gradient descent and its variants as they do a pretty decent job.", "Now that we have spent some time understanding the math behind logistic regression, let\u2019s look more at how to evaluation classification problems. We can now fit a logistic regression model. Logistic regression can also leverage the regularization techniques we used in the linear regression post. They are used in exactly the same way \u2014 we penalize large coefficients by adding and L1 or L2 norm to our cost function.", "Note we won't be demonstrating CV for hyper-parameter optimization here as we did with linear regression, but that is still applicable. In fact, almost all of the techniques we discussed in the previous post can also be used here. We will just be discussing new topics. We already know about the train and test data from the previous post. Let\u2019s see how does the model do on test data.", "One of the easiest metrics to understand when it comes to classification is accuracy: what fraction did you correctly predict.", "One of the downfalls of accuracy is that it is a pretty terrible metric when you have imbalanced classes. Let\u2019s look at our class balance:", "So about 88% of our data are of the negative class \u2014 so if we just predicted everything to be negative, our accuracy would be about 88%! Not bad. Imagine other data where the negative class is only represented 1% of the time \u2014 you would get 99% accuracy by just predicting everything to be positive. That accuracy might feel good, but it is pretty worthless if that is your model.", "To deal with some of these issues, often you will see precision, recall, and F1 used to evaluate classification tasks.", "Precision is the number of true positives divided by all of the positive predictions (true positives plus false positives): Basically, how precise are our predictions when we predict something to be positive.", "Recall is the number of true positives divided by all of the actual positives (true positives plus false negatives): Basically, what fraction of all the positives do we actually predict.", "These two metrics trade-off between each other. For the same model, you can increase precision at the expense of recall and vice versa. Often, you have to determine which is more important to the problem at hand. Do we want our predictions to be precise or have a high recall?", "What are true/false positives/negatives? Let\u2019s take a look at a confusion matrix.", "Here is our confusion matrix for our test predictions. The rows are the truth. So, row 0 are actual 0 labels and row 1 are actual 1 labels. The columns are the predictions. Thus, cell 0,0 counts the number of the 0 class that we got right. This is called true negatives (assuming we consider the label 0 the negative label). And cell 1,1 counts the number of the 1 class that we got right \u2014 true positives. Cell 0,1 are our false positives and cell 1,0 false negatives.", "As you can probably tell, confusion matrices are extremely useful tools for error analysis. In this example, we have pretty symmetric errors \u2014 four missed within both classes. This is not always the case, and often there are more than two classes, so understanding which classes get confused for others can be very useful in making the model better.", "Sometimes you just want a balance between precision and recall. If that is your goal, F1 is a very common metric. It is the harmonic mean of precision and recall:", "The harmonic mean is used because it tends strongly towards the smallest element being averaged. If either precision or recall is 0 then your F1 is zero. The best F1 score is one.", "With all of the previous metrics, we have been using binary predictions. But really logistic regression returns probabilities, which are extremely useful.", "Now instead of a single 1/0 number for each prediction, we have two numbers: the probability of being class 0 and the probability of being class 1. These two numbers must sum to one so 1-probability of 1 = probability of 0. A useful thing to do is to make sure these probabilities actually seem to correlate with the classes:", "What this plot is showing us is that for our training data that indeed our positive class tends to have a high probability of being positive and our negative class has a high probability of being negative. This is basically what we designed our algorithm to do, so this is somewhat to be expected. It would also be good to check that this is true for validation sets.", "Sklearn\u2019s predict function just takes the class with the highest probability as the predicted class, which isn\u2019t a bad method. One might want, though, a very precise model for positive predictions. One way we could do this is only call something positive if the probability is over 90%. To understand the trade-offs between precision and recall with different cut-off values, we can plot them:", "Nice! We can clearly see the trade-offs between P and R as we adjust our thresholds. And in fact, we can find the cut-off that maximizes F1 (NOTE: we are using testing data here, which would not be good in practice. We would want to use a validation set to choose a cut-off value)", "ROC curves are another popular method for evaluating classification tasks. It is a plot with the recall value (or true positive rate) on the y-axis and the false positive rate (false positives divided by all the actual negatives) on the x-axis. Thus, it shows how your model trades off between these two values. The closer your value is to the top-left corner of the graph, the better. Let\u2019s take a look for our test data:", "Clearly, our model has done very well. The AUC (area under the curve) score you see in the bottom right is the area under the ROC curve, a value of 1 being the best one can do. We got to one. This curve can also be used to pick a threshold value if you know the trade-off you want to make between TPR and FPR.", "Logistic regression can also handle more than 2 classes. There are two ways we can do this:", "This loss takes the negative average of the predicted probabilities for the correct class. Thus, optimizing it attempts to get the predicted probabilities for the correct class as high as possible.", "Interpreting the coefficients of logistic regression is a little trickier than that of linear regression. Let\u2019s take a look at our coefficients:", "Our most positive coefficient is on char_counts with a value of 5 (roughly). Since we used a log-loss, we need to convert this value by doing e^5 which is 147.52. That is for every 1 unit increase in char_counts the odds of the password being a strong one increases by 14752% because we now have converted to an odds ratio. For more details on this, check out this post.", "I would like to show it using an example. Assume a 2 class classification problem and 6 instances.", "Although Case 1 is more correct in predicting the class for the instance if 0.5 is considered the threshold, the loss in Case 1 is higher than the loss in Case 2. Why is this? MSE is a metric that measures the standard deviation of the predictions from the ground truth. And also it\u2019s application might be limited only to binary classification. It still might be used in specific instances. The RMSE is one way to measure the performance of a classifier. Error-rate (or No. of misclassification) is another one. If our goal is a classifier with low error-rate, RMSE is inappropriate and vice versa. This explains why we don\u2019t use MSE/RMSE as an evaluation metric for classification problems. Other metrics provide more flexibility and understanding of the classifier performance than an MSE.", "If you need any help you can get in touch: LinkedIn", "Hands-on machine learning by Geron Aurelien", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I love data and writing is my first love. This blend of both worlds brings out a purpose."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F81807fa881be&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----81807fa881be--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81807fa881be--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----81807fa881be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----81807fa881be--------------------------------", "anchor_text": "Hemanth Devarapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd994668b1ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&user=Hemanth+Devarapati&userId=fd994668b1ec&source=post_page-fd994668b1ec----81807fa881be---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/bhavikbb/password-strength-classifier-dataset", "anchor_text": "data set"}, {"url": "https://www.youtube.com/watch?v=o4QmoNfW3bI", "anchor_text": "probability"}, {"url": "https://www.youtube.com/watch?v=GxbXQjX7fC0", "anchor_text": "odds"}, {"url": "http://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/", "anchor_text": "An Intuitive Guide To Exponential Functions & e"}, {"url": "http://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/", "anchor_text": "Demystifying the Natural Logarithm (ln)"}, {"url": "http://nbviewer.jupyter.org/github/justmarkham/DAT8/blob/master/notebooks/12_e_log_examples.ipynb", "anchor_text": "brief summary"}, {"url": "https://nbviewer.jupyter.org/github/hdev7/medium-article-logistic-regression-follow-up-notebook/blob/master/logistic%20regression%20bottoms%20up%20approach.%20%28Feature%20engineering%20ideology%20-%20a%20bonus%29.ipynb", "anchor_text": "here"}, {"url": "https://medium.com/@hemanthsaid7/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323", "anchor_text": "Linear Regression"}, {"url": "http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "anchor_text": "independent and identically distributed"}, {"url": "http://en.wikipedia.org/wiki/Convex_optimization", "anchor_text": "convex optimization"}, {"url": "http://en.wikipedia.org/wiki/Convex_function", "anchor_text": "convex"}, {"url": "http://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent"}, {"url": "https://github.com/Shathra/gradient-descent-demonstration/blob/master/gradient_descent_1d.gif", "anchor_text": "Here"}, {"url": "http://www.mathwords.com/l/logarithm_rules.htm", "anchor_text": "log rules"}, {"url": "https://www.math.hmc.edu/calculus/tutorials/quotient_rule/", "anchor_text": "quotient rule"}, {"url": "http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_bfgs.html", "anchor_text": "BFGS"}, {"url": "http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html", "anchor_text": "L-BFGS"}, {"url": "http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html", "anchor_text": "Conjugate Gradient"}, {"url": "https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/", "anchor_text": "confusion matrix"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier", "anchor_text": "One-vs-rest"}, {"url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "anchor_text": "Cross-entropy"}, {"url": "https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/", "anchor_text": "post"}, {"url": "https://nbviewer.jupyter.org/github/hdev7/medium-article-logistic-regression-follow-up-notebook/blob/master/logistic%20regression%20bottoms%20up%20approach.%20%28Feature%20engineering%20ideology%20-%20a%20bonus%29.ipynb", "anchor_text": "Github"}, {"url": "https://medium.com/@hemanthsaid7/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323", "anchor_text": "linear regression"}, {"url": "https://www.linkedin.com/in/hemanthsaid/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----81807fa881be---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----81807fa881be---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----81807fa881be---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----81807fa881be---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/metrics?source=post_page-----81807fa881be---------------metrics-----------------", "anchor_text": "Metrics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----81807fa881be---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----81807fa881be---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81807fa881be--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----81807fa881be---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----81807fa881be--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----81807fa881be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----81807fa881be--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----81807fa881be--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----81807fa881be--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----81807fa881be--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----81807fa881be--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----81807fa881be--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hemanth Devarapati"}, {"url": "https://medium.com/@hemanthsaid7/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "33 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd994668b1ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&user=Hemanth+Devarapati&userId=fd994668b1ec&source=post_page-fd994668b1ec--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffd994668b1ec%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&user=Hemanth+Devarapati&userId=fd994668b1ec&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}