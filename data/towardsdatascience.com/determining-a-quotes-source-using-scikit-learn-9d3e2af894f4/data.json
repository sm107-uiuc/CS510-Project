{"url": "https://towardsdatascience.com/determining-a-quotes-source-using-scikit-learn-9d3e2af894f4", "time": 1683007883.967823, "path": "towardsdatascience.com/determining-a-quotes-source-using-scikit-learn-9d3e2af894f4/", "webpage": {"metadata": {"title": "Determining a Quote's Source Using Scikit-Learn | Towards Data Science", "h1": "Determining a Quote\u2019s Source Using Scikit-Learn", "description": "Ensemble averaging using logistic regression and naive Bayes to determine if a message is more likely to be tweeted by Bernie Sanders or Donald Trump."}, "outgoing_paragraph_urls": [{"url": "https://github.com/Shawn-Chahal/who-said-it", "anchor_text": "GitHub", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Overfitting", "anchor_text": "overfitting", "paragraph_index": 11}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "CountVectorizer", "paragraph_index": 13}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html", "anchor_text": "TfidfTransformer", "paragraph_index": 16}, {"url": "https://docs.python.org/3.7/library/re.html", "anchor_text": "re", "paragraph_index": 22}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html", "anchor_text": "pandas.DataFrame", "paragraph_index": 23}, {"url": "https://scikit-learn.org/stable/glossary.html#term-n-jobs", "anchor_text": "n_jobs", "paragraph_index": 25}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TfidifVectorizer", "paragraph_index": 26}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html", "anchor_text": "LabelEncoder", "paragraph_index": 30}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html", "anchor_text": "train_test_split", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Stratified_sampling", "anchor_text": "stratify", "paragraph_index": 32}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "LogisticRegression", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1407.0202", "anchor_text": "saga", "paragraph_index": 38}, {"url": "https://en.wikipedia.org/wiki/Likelihood_function", "anchor_text": "likelihood", "paragraph_index": 40}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html", "anchor_text": "BernoulliNB", "paragraph_index": 41}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "GridSearchCV", "paragraph_index": 47}, {"url": "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Discussion", "anchor_text": "Wikipedia entry", "paragraph_index": 48}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB.predict_proba", "anchor_text": "predict_proba", "paragraph_index": 48}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html", "anchor_text": "VotingClassifier", "paragraph_index": 49}, {"url": "https://www.shawnchahal.com/who-said-it", "anchor_text": "app", "paragraph_index": 54}, {"url": "https://www.shawnchahal.com/", "anchor_text": "https://www.shawnchahal.com/", "paragraph_index": 56}], "all_paragraphs": ["People can use their senses such as vision and hearing to learn. Machine learning follows a similar concept, but there\u2019s one key difference. A computer can\u2019t see or hear the way we do. It only understands one thing \u2014 numbers. Therefore, before a machine can learn, it will first need to convert information that humans can perceive, into numbers. An example of this is the way that digital images store numerical information in the form of pixels.", "In this article we will develop a machine learning model which can receive a typed message and determine whether Bernie Sanders or Donald Trump is more likely to tweet it. A repository including the data set, Python code, and Jupyter Notebooks to be discussed can be found on my GitHub. Now let\u2019s get started!", "To minimize bias, we will need to train our model on text from both of our politicians that has been used in a similar context. For example, if we extracted text written from one politician only when they issued a response to a national tragedy, and from the other in response to positive events, our model would then be learning to differentiate between positive and negative text, and not necessarily on their writing style. Therefore, we will train the model on both politicians tweets since 2015 (i.e., when they launched their campaign for the 2016 US presidential election). We won\u2019t include retweets, since although they may reflect the socioeconomic views of the politicians, they will not necessarily reflect their language patterns.", "Tweets can contain links to other websites, images, or videos which will come with their own URL including terms such as (https://\u2026). Hashtags ( # ) and mentions ( @ ) also contain symbols that are not necessarily part of these politicians\u2019 writings, but are a necessity to use certain twitter features. Therefore, we will remove these items and any other non-alphanumeric characters and convert what remains to lower case. Essentially, we will be left with text containing only lower case letters, numbers, and spaces. For example:", "The message still makes sense when read and will now be easier to implement in our model.", "Let\u2019s consider the following three documents:", "Next, we will tokenize each document into terms:", "Note that the \u201c|\u201d symbol does not actually exist in our model, it is only used here to help visualize the tokens. Each document can now be thought of as a bag containing terms for each word. Hence the name bag-of-words. Let\u2019s look at the impact of tokenization on our documents:", "The first two terms (i.e., \u201cWe are\u201d and \u201cwe\u2019re\u201d) have the same meaning, but will be registered as different terms. Moreover, the words \u201cwe\u201d and \u201care\u201d have been separated from each other. This may seem undesirable, but remember that whether someone uses contractions or not is a distinguishing feature of how they write. Conversely, \u201cwe\u2019re\u201d and \u201cwere\u201d have different meanings, but have now merged into the same term.", "In the previous example, we tokenized our documents into one word terms known as unigrams (i.e., 1-grams). To improve our model, we can also extract bigrams (i.e., 2-grams) consisting of terms containing two words:", "Using bigrams, we can now include new terms such as \u201cwe are\u201d. We also have more information to distinguish \u201cwe\u2019re\u201d from \u201cwere\u201d since the term \u201cwere you\u201d might be more common than \u201cwe\u2019re you\u201d. We can also extract new terms like \u201cmachine learning\u201d which was previously only available as \u201cmachine\u201d and \u201clearning\u201d.", "Adding bigrams to our model increases the number of terms in each document, making our model more complex, but we must be careful not to get carried away. Including more features than necessary (e.g., 3-grams, 4-grams, 5-grams, etc.) to describe our model can make it prone to overfitting, as well as increasing its size and computational time. While we won\u2019t have a detailed discussion on overfitting in this article, the general idea is that we want to make the most accurate model we can, with as few features as necessary.", "Consider the following corpus (i.e., a collection of documents):", "Starting off, our model doesn\u2019t know any terms. Therefore, we will need to create a dictionary of all the terms we want our model to know. These terms make up the vocabulary of the model and can be extracted from a corpus using scikit-learn\u2019s CountVectorizer in Python:", "The output is our vocabulary containing all the unique terms (i.e., unigrams and bigrams) in our corpus and the term frequency (tf) (i.e., the number of times each term appears in a document) table.", "There are many common words that are used by everyone and won\u2019t be distinguishing features in someone\u2019s writing. For example, \u201cis\u201d and \u201cthe\u201d both have a document frequency (df) equal to 3 (i.e., they appear in all three documents in our corpus) and therefore, won\u2019t give us as much distinguishing information as the term \u201cgrowing\u201d which has df = 1 (i.e., it only appears in one document). We will want to assign more weight to terms which have a low document frequency. Note that it doesn\u2019t matter how many times a term appears in a document when calculating df, so long as it appears at least once. For example, the term \u201cthe\u201d still has df = 3, despite appearing a total of 4 times in the corpus. We can apply this weight by multiplying the term frequency by the inverse document frequency (idf). This gives us the term frequency \u2014 inverse document frequency (tf-idf or tfidf):", "where n is the total number of documents in the corpus (in our last example, n=3). Note that the use of \u201c+ 1\u201d throughout the equation varies with implementation. Their purpose is to avoid the creation of zeros, especially in the denominator. The equation as it is shown here is the default scikit-learn implementation for the TfidfTransformer which we will soon use.", "Finally, we want to make sure that the length of our document does not impact our model. For example, if one politician typically writes shorter tweets than the other, we don\u2019t want the user to be able to trick our model by simply writing a longer message. Therefore, we will normalize each document vector so that its sum of squares is equal to 1. In other words, after calculating the tfidf values, we will scale every document to a unit vector.", "Which gives us the following tfidf table:", "Notice that even though some terms have the same tf throughout different documents, they can have varying tfidf.", "Now that we know how to extract features from a corpus into a format that our model can understand, we can move forward and begin training the actual model.", "In the previous example, we looked at a small corpus with 3 documents and a vocabulary of 24 terms. In practice, we will need many more documents and a much larger vocabulary if we want to develop a model which can determine if a quote belongs to Bernie Sanders or Donald Trump. As mentioned earlier we will use all their tweets since 2015.", "Using a new Jupyter Notebook, let\u2019s start by cleaning the tweets using re from the Python Standard Library:", "We define clean_text which takes a string and cleans it according to the requirements we discussed previously. We then load our data set into a pandas.DataFrame and apply clean_text to each tweet. Finally, we drop any tweets that are now blank after our cleaning process. Before we train our model, let\u2019s define two constants:", "Note that by setting random_state = 0, we are not actually initializing our random seed, but will be passing an integer to our various scikit-learn implementations which will allow us to keep the same random state with each run of our code. While tuning hyperparameters, we want to ensure that we generate the same random sequence each time we test our model so that we can be sure that any improvement in performance that we see is due to the changes we made and not due to randomness created by the random number generator.", "Scikit-learn supports multithreading. Most computers today use a multi-core CPU. We can set n_jobs = -1 to use all the threads on a CPU for faster model training.", "Now we will compute the tfidf vectorization of our tweets. The corpus can be converted directly into tfidf vectors in scikit-learn using the TfidifVectorizer, which is essentially a pipeline of the CountVectorizer followed by the TfidfTransformer, both of which we used previously.", "Which outputs some stats for our data set:", "To stay consistent throughout the article, we\u2019ll assign them as:", "We set ngram_range=(1, 2) and min_df=2 which states that our vocabulary will consist of unigrams and bigrams with a document frequency of at least 2. By eliminating terms with a document frequency of only 1, we can reduce the number of features in our model, and reduce overfitting. Note that this method works well here because our corpus contains tens of thousands of documents. This may not be appropriate for a smaller corpus, and these parameters need to be tuned specifically for each project.", "Just like our model needs to receive tweets in the form of numbers, it will need to receive the politicians\u2019 names in the form of numbers. This can be done using scikit-learn\u2019s LabelEncoder:", "Before training our model, it is extremely important that we split our data set into a train and test set. The training set will be used to train our model, but it is important to have a separate testing data set to get a true measure of how accurate our model is on data that it has never seen before. Using scikit-learn\u2019s train_test_split function:", "Here we specified that we will keep 50% of the data set as our testing set with test_size=0.5. We also set stratify=y to ensure that our test and training data set have the same ratio of Bernie Sanders to Donald Trump tweets.", "Now that we have set up our data, let\u2019s look at our first algorithm.", "Logistic regression is one of the most well-known machine learning models and is often one of the first algorithms that most people will learn when starting in machine learning. Here\u2019s what the standard logistic function looks like:", "The standard logistic function receives a value z and outputs a value between 0 and 1. We are working with a binary classification problem, meaning that the result must be Bernie Sanders or Donald Trump, with no other options. This is why we previously assigned each politician a value of 0 or 1.", "Now let\u2019s take a look at how we can convert a tweet into a value z:", "From this equation, we see that z is equal to the dot product of our tfidf vector (x) with the weight vector (w) to which a bias term (b_0) is added. Note that although Python lists begin indexing at 0, it is more common to begin indexing at 1 when describing the mathematical model and reserving the index 0 for our bias term (which can sometimes also appear as w_0). For each tweet, we can calculate a value z, pass it through the logistic function, and round to the nearest whole number to get 0=Bernie Sanders or 1=Donald Trump. To do that, our machine learning algorithm needs to figure out which values for the weight vector (w) will result in the highest percentage of tweets being attributed to the correct politician.", "We won\u2019t look into the various methods of solving for w as that would be an entire article in itself. Scikit-learn implements various solvers for training a LogisticRegression classifier. In this app we set solver='saga' as our optimization method and C=20 as our inverse regularization term to reduce overfitting and make it easier to train our model:", "Which returns our accuracy on the test data set:", "Naive Bayes classifiers are relatively popular for text classification tasks. We will implement a specific event model known as Bernoulli naive Bayes. The likelihood of a specific tweet having been written by each politician can be calculated as follows:", "where p_ki, is the probability that politician C_k, will use the term i in a tweet. Let\u2019s train our model using scikit-learn\u2019s BernoulliNB classifier and then look at what some of these parameters represent:", "Now let\u2019s consider the following Bernie Sanders quote:", "\u201cMedicare for all\u201d - Bernie Sanders", "This quote will contain the term \u201cmedicare for\u201d. Let\u2019s see how this translates into the parameters in the above equation:", "Here, p_ki = 0.0289 can be interpreted as 2.89% of tweets from Bernie Sanders in our training set contain the term \u201cmedicare for\u201d. What\u2019s interesting about Bernoulli naive Bayes is that the p_ki values can be solved for directly. It is simply equal to the document frequency of term i in politician C_k\u2019s corpus divided by the total number of documents in politician C_k\u2019s corpus:", "Which both give us the same value for p_ki to 4 significant figures:", "In logistic regression, x contained tfidf values. In Bernoulli naive Bayes, x_i must equal to 0 or 1. In its simplest form, x_i would equal to 1 if the term i was present in the tweet, and 0 if it was absent. However, we can extract a bit more information by setting a threshold on our tfidf values instead. An optimal threshold is typically found through trial and error or exhaustive search methods such as GridSearchCV in scikit-learn. For this app, an optimal threshold was binarize=0.09. Therefore, any tfidf value above 0.09 was set to 1, and any below was set to 0. We also set alpha=0.01 which is a smoothing parameter.", "Going back to our equation, we can see that if we receive a document from the user whereby x_i = 1, then we are multiplying the probability (p_ki) of the term i appearing in politician C_k\u2019s tweet. Conversely, if x_i = 0, then we are multiplying the probability that the term i would not appear in politician C_k\u2019s tweet. This multiplication is done for each term in the vocabulary and for each politician. Then the politician with the highest joint log-likelihood is output as the result. Remember that while the logistic function outputs a probability, Bernoulli naive Bayes outputs a likelihood. These two terms are related, but do not mean the same thing. The method to convert likelihoods to probabilities will not be discussed here, but the following Wikipedia entry contains a concise discussion on the topic and its relation to logistic regression. Moreover, BernoulliNB can convert the likelihood into a probability using its predict_proba method. Now let\u2019s calculate our accuracy on the test data set:", "We trained two models, logistic regression and Bernoulli naive Bayes, both with a relatively high accuracy on the test data set. Using scikit-learn\u2019s VotingClassifier, we can take the weighted average of the probabilities they each calculate by selecting our classifiers with estimators=[('log', clf_log), ('bnb', clf_bnb)] and setting voting='soft' to obtain a more accurate result. Adding 60% of the logistic regression probability with 40% of the Bernoulli naive Bayes probability by setting weights=(0.6, 0.4) had the highest accuracy. An interesting result, considering that our Bernoulli naive Bayes had a higher accuracy, but we are assigning it less weight. This is likely because the underlying estimation of the probabilities from our Bernoulli naive Bayes classifier is not necessarily as reliable as the probability calculated from our logistic regression classifier. Now let\u2019s train our ensemble model:", "Which returns our accuracy on the test data set:", "Although this is a marginal increase in accuracy, we should keep in mind that people will likely input messages that are not necessarily something that either candidate might say. In these cases ensemble averaging can be beneficial since it\u2019s the equivalent to getting a second opinion before making a decision on which politician was more likely to tweet what the user had written.", "Once we are sure that we are not going to change any more hyperparameters (e.g., the threshold on naive Bayes, the weighted average for voting, etc.) then we can retrain our model on the entire data set to get twice the training examples.", "I hope that you now have a better understanding of how scikit-learn can be applied towards text classification and if you\u2019re new to machine learning, that you are now interested in learning more about it.", "Now you can try out the app for yourself to see the model we went over in action!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning enthusiast. I also write about socioeconomic topics in Canada. https://www.shawnchahal.com/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9d3e2af894f4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shawn.chahal?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shawn.chahal?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "Shawn Chahal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F475fafae9d0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&user=Shawn+Chahal&userId=475fafae9d0e&source=post_page-475fafae9d0e----9d3e2af894f4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d3e2af894f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d3e2af894f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/Shawn-Chahal/who-said-it", "anchor_text": "GitHub"}, {"url": "https://en.wikipedia.org/wiki/Overfitting", "anchor_text": "overfitting"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "CountVectorizer"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html", "anchor_text": "TfidfTransformer"}, {"url": "https://docs.python.org/3.7/library/re.html", "anchor_text": "re"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html", "anchor_text": "pandas.DataFrame"}, {"url": "https://scikit-learn.org/stable/glossary.html#term-n-jobs", "anchor_text": "n_jobs"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TfidifVectorizer"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html", "anchor_text": "LabelEncoder"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html", "anchor_text": "train_test_split"}, {"url": "https://en.wikipedia.org/wiki/Stratified_sampling", "anchor_text": "stratify"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "LogisticRegression"}, {"url": "https://arxiv.org/abs/1407.0202", "anchor_text": "saga"}, {"url": "https://en.wikipedia.org/wiki/Likelihood_function", "anchor_text": "likelihood"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html", "anchor_text": "BernoulliNB"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "GridSearchCV"}, {"url": "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Discussion", "anchor_text": "Wikipedia entry"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB.predict_proba", "anchor_text": "predict_proba"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html", "anchor_text": "VotingClassifier"}, {"url": "https://www.shawnchahal.com/who-said-it", "anchor_text": "app"}, {"url": "https://www.shawnchahal.com/who-said-it", "anchor_text": "Who said it?Write a message and find out whether Bernie Sanders or Donald Trump is more likely to say it.www.shawnchahal.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9d3e2af894f4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----9d3e2af894f4---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----9d3e2af894f4---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----9d3e2af894f4---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/naive-bayes?source=post_page-----9d3e2af894f4---------------naive_bayes-----------------", "anchor_text": "Naive Bayes"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d3e2af894f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&user=Shawn+Chahal&userId=475fafae9d0e&source=-----9d3e2af894f4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d3e2af894f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&user=Shawn+Chahal&userId=475fafae9d0e&source=-----9d3e2af894f4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d3e2af894f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9d3e2af894f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9d3e2af894f4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9d3e2af894f4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shawn.chahal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shawn.chahal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shawn Chahal"}, {"url": "https://medium.com/@shawn.chahal/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "15 Followers"}, {"url": "https://www.shawnchahal.com/", "anchor_text": "https://www.shawnchahal.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F475fafae9d0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&user=Shawn+Chahal&userId=475fafae9d0e&source=post_page-475fafae9d0e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fba6b1d43c53a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdetermining-a-quotes-source-using-scikit-learn-9d3e2af894f4&newsletterV3=475fafae9d0e&newsletterV3Id=ba6b1d43c53a&user=Shawn+Chahal&userId=475fafae9d0e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}