{"url": "https://towardsdatascience.com/cracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525", "time": 1683004026.6324239, "path": "towardsdatascience.com/cracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525/", "webpage": {"metadata": {"title": "Cracking Cause and Effect with Reinforcement Learning | by Aaron Krumins | Towards Data Science", "h1": "Cracking Cause and Effect with Reinforcement Learning", "description": "The difficulty of inferring cause and effect is omnipresent. Humans face this challenge every day and do a admirable job of surmounting the problem, at least compared with our animal cousins. Machine\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["The difficulty of inferring cause and effect is omnipresent. Humans face this challenge every day and do a admirable job of surmounting the problem, at least compared with our animal cousins. Machine Learning pioneers such as Yoshua Bengio even suggest that creating algorithms that can infer cause and effect is the key to avoiding another AI winter and unlocking new frontiers in machine intelligence. In this article I explain how the reinforcement learning can be reframed to infer causality, paralleling our human ability to do so and perhaps someday far exceeding it.", "For those unfamiliar with reinforcement learning, it refers to a subset of algorithms used to determine optimality in sequential decision making tasks. It is typically presented in terms of an agent learning to take a series of actions within an environment to receive a reward. These algorithms are readily applicable to problems that can be \u201cgamified\u201d such that there are clearly defined actions and rewards. Recently reinforcement learning algorithms have received much acclaim for besting humans in GO, Starcraft and a variety of video games.", "The basic means for such reinforcement algorithms to achieve their success is the concept of a prediction error minimized through successive episodes of trial and error training. This method fairly accurately corresponds with the human dopamine learning system which also makes use of a prediction error.", "There are two primary hurdles that make inferring causation a difficult problem, one is complexity and the other is correlation. Next I will explain how reinforcement learning can be reframed to tackle these.", "As mentioned, reinforcement learning is typically presented in terms of an agent making sequential decisions within an environment in pursuit of some reward, such as winning a game. The agent can take actions, and depending on the state of the environment, those actions are either rewarded or not. When they are rewarded, the agent propagates this reward back across all the actions and environment states that led to it eventually receiving the reward. Over many trials, it can thereby determine which combination of actions and environment states were instrumental to receiving the reward and which were superfluous. Looked at another way, the agent determines which combination of actions and environment states were causally related to it receiving a reward and which were not. In effect, the agent is inferring cause and effect in regard to its own actions and a specific reward state of the environment.", "The important point is that at its root, RL algorithms are agnostic towards which component in an environment is the agent and which are merely objects. Instead of thinking in terms of an agent, its actions, and an environment, we can think of all objects in the environment as possible agents, and all the possible states an object can exist in as it\u2019s action set. In this way, any causal relationship between a given set of components in an environment and a specific end state can be explored by simply changing which object is the agent and what is the end state that is being rewarded.", "Let\u2019s unpack this with an example. A fairly common demonstration of RL is a match to sample task, in which the agent(often a small furry mammal) must learn to take a series of actions to receive a food reward, for instance, by pressing a lever when a light turns on. With training, many animals as well as AI bots can solve such tasks. (See the accompanying video of a reinforcement learning agent solving a match to sample task)", "Now imagine we are a na\u00efve social scientist with no knowledge of how this experiment was designed and wish to know what causes the food reward to be unlocked. In other words, we want to know the cause and effect story behind the match to sample task. A potential way to answer this is to employ an RL algorithm that moves sequentially through objects in the environment, treats each as an\u201cagent\u201d, and attempts to see if there is a series of actions this agent/object can take to minimize a prediction error in regard to the unlocking of the food reward. An object which is causally connected to the reward state of unlocking the food bowl will have some actions that allow it minimize a prediction error in regard to this reward state. One that is not causally related will have no such state/action combinations, no matter how well its action set correlates with the reward state.", "Remember, in our reframing, actions simply refer to the different states that an object can exist in. For instance, we could pose the problem in such a way the the light that turns on and off is the agent and its actions are turning on and off. If there exists a causal relation between the actions of whatever object we have chosen as the agent, and the reward state we want to understand, than there also exists a prediction error in regard to its actions space that can be minimized through successive training episodes. In other words, there is a way this object/agent can causally influence the reward state.", "A potential way to unlock the causality story in the environment than is to iteratively move across objects in the environment, treat them as an agent taking actions, and examine which result in a prediction error that can be minimized in regard to the reward state. Again, an object that is not causally connected with the reward state or merely correlated with the reward state, will have a prediction error over its action space that is essentially random and will not diminish with more training episodes. In such a manner RL provides a way to algorithmically probe the elements in a given environment for causal relationships, treating each one in turn as an agent who is attempting to change its behavior through some actions in order to influence the reward state.", "Moreover, the rate at which the prediction error diminishes, what is frequently referred to as that agents learning curve, will give some indication as to how causally distant the object/agent is from the reward state. While it may be true that a butterfly flapping its wings can cause a hurricane on the other side of the globe, the number of training episodes necessary to predict this occurrence would be astronomical compared with the number of training episodes necessary to minimize a prediction error in regard to a much more proximate cause. Learning curves can therefore be useful in gauging relative causal proximity, all other things being equal.", "There are number of assumptions and limitation that apply to this methodology, namely all those applicable to reinforcement learning itself. It also assumes we have a means assessing the action space for each agent/object in the environment and that this satisfies the Markov property. In the example of the match to sample task, every object in the environment has a small clearly defined action space. For objects with a large or continuous action space, exploring causation may become computationally intractable. It\u2019s also worth noting that this system for inferring causality is only suitable to situations where a large number of sample trials can be generated, just as RL itself is only applicable where \u201cself-play\u201d allows for an enormous number of training episodes. While this may appear a computationally intensive means of exploring causality, with the diminishing cost of compute, an increasing set of environments could become approachable by such methods.", "It remains that one of the enduring challenges of science is determining causality in complex systems. Examples include the human genome, where myriad genes and environmental factors contribute to a specific phenotype. In such instances causality can be difficult to infer and methods such as this could potentially shed light on which \u201cactors\u201d or states are instrumental to achieving a specific outcome. It also opens up the field of AI to agents that can \u201cexplain\u201d the causal inference used in determining a particular course of action. Another potential application is generating prior knowledge by which an AI could store a list of previously inferred causal relationships and apply them to new action spaces when presented with a challenging problem. Such prior knowledge could vastly diminish the training episodes necessary to succeed at a task and possibly lead to the kind of one shot learning demonstrated by humans.", "Aaron Krumins is the author of \u201cOutsmarted \u2014 The Promise and Peril of Reinforcement Learning\u201d and currently works as a freelance machine learning consultant"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff3df8dfcd525&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://aaron-krumins.medium.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Aaron Krumins"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F700bb1fd579c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&user=Aaron+Krumins&userId=700bb1fd579c&source=post_page-700bb1fd579c----f3df8dfcd525---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3df8dfcd525&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&user=Aaron+Krumins&userId=700bb1fd579c&source=-----f3df8dfcd525---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3df8dfcd525&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&source=-----f3df8dfcd525---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@umby?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Umberto"}, {"url": "https://unsplash.com/s/photos/ice-cracking?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f3df8dfcd525---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f3df8dfcd525---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----f3df8dfcd525---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/psychology?source=post_page-----f3df8dfcd525---------------psychology-----------------", "anchor_text": "Psychology"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f3df8dfcd525---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3df8dfcd525&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&user=Aaron+Krumins&userId=700bb1fd579c&source=-----f3df8dfcd525---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3df8dfcd525&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&user=Aaron+Krumins&userId=700bb1fd579c&source=-----f3df8dfcd525---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3df8dfcd525&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F700bb1fd579c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&user=Aaron+Krumins&userId=700bb1fd579c&source=post_page-700bb1fd579c----f3df8dfcd525---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2b84a10369ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&newsletterV3=700bb1fd579c&newsletterV3Id=2b84a10369ed&user=Aaron+Krumins&userId=700bb1fd579c&source=-----f3df8dfcd525---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Written by Aaron Krumins"}, {"url": "https://aaron-krumins.medium.com/followers?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "176 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F700bb1fd579c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&user=Aaron+Krumins&userId=700bb1fd579c&source=post_page-700bb1fd579c----f3df8dfcd525---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2b84a10369ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525&newsletterV3=700bb1fd579c&newsletterV3Id=2b84a10369ed&user=Aaron+Krumins&userId=700bb1fd579c&source=-----f3df8dfcd525---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/creating-emergent-behaviors-with-reinforcement-learning-and-unreal-engine-4cd89c923b7f?source=author_recirc-----f3df8dfcd525----0---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=author_recirc-----f3df8dfcd525----0---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=author_recirc-----f3df8dfcd525----0---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Aaron Krumins"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3df8dfcd525----0---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/creating-emergent-behaviors-with-reinforcement-learning-and-unreal-engine-4cd89c923b7f?source=author_recirc-----f3df8dfcd525----0---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Creating Emergent Behaviors with Reinforcement Learning and Unreal EngineGenerate emergent behavior in AI characters using Unreal Engine and the free MindMaker machine learning plugin"}, {"url": "https://towardsdatascience.com/creating-emergent-behaviors-with-reinforcement-learning-and-unreal-engine-4cd89c923b7f?source=author_recirc-----f3df8dfcd525----0---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "\u00b79 min read\u00b7Oct 3, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cd89c923b7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-emergent-behaviors-with-reinforcement-learning-and-unreal-engine-4cd89c923b7f&user=Aaron+Krumins&userId=700bb1fd579c&source=-----4cd89c923b7f----0-----------------clap_footer----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/creating-emergent-behaviors-with-reinforcement-learning-and-unreal-engine-4cd89c923b7f?source=author_recirc-----f3df8dfcd525----0---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cd89c923b7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-emergent-behaviors-with-reinforcement-learning-and-unreal-engine-4cd89c923b7f&source=-----f3df8dfcd525----0-----------------bookmark_preview----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3df8dfcd525----1---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----f3df8dfcd525----1---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----f3df8dfcd525----1---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3df8dfcd525----1---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3df8dfcd525----1---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3df8dfcd525----1---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----f3df8dfcd525----1---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----f3df8dfcd525----1-----------------bookmark_preview----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3df8dfcd525----2---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----f3df8dfcd525----2---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----f3df8dfcd525----2---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3df8dfcd525----2---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3df8dfcd525----2---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3df8dfcd525----2---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----f3df8dfcd525----2---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----f3df8dfcd525----2-----------------bookmark_preview----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/create-a-custom-deep-reinforcement-learning-environment-in-ue4-cf7055aebb3e?source=author_recirc-----f3df8dfcd525----3---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=author_recirc-----f3df8dfcd525----3---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=author_recirc-----f3df8dfcd525----3---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Aaron Krumins"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----f3df8dfcd525----3---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/create-a-custom-deep-reinforcement-learning-environment-in-ue4-cf7055aebb3e?source=author_recirc-----f3df8dfcd525----3---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "Create a Custom Deep Reinforcement Learning Environment in UE4Design a custom RL environment using modern game development software and bleeding-edge DRL algorithms(A2C, PPO, TD3, ACER)"}, {"url": "https://towardsdatascience.com/create-a-custom-deep-reinforcement-learning-environment-in-ue4-cf7055aebb3e?source=author_recirc-----f3df8dfcd525----3---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": "14 min read\u00b7Nov 17, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf7055aebb3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-a-custom-deep-reinforcement-learning-environment-in-ue4-cf7055aebb3e&user=Aaron+Krumins&userId=700bb1fd579c&source=-----cf7055aebb3e----3-----------------clap_footer----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/create-a-custom-deep-reinforcement-learning-environment-in-ue4-cf7055aebb3e?source=author_recirc-----f3df8dfcd525----3---------------------e26d8333_1717_4940_9d80_2e3a9a9291ed-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf7055aebb3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-a-custom-deep-reinforcement-learning-environment-in-ue4-cf7055aebb3e&source=-----f3df8dfcd525----3-----------------bookmark_preview----e26d8333_1717_4940_9d80_2e3a9a9291ed-------", "anchor_text": ""}, {"url": "https://aaron-krumins.medium.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "See all from Aaron Krumins"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----f3df8dfcd525----0-----------------bookmark_preview----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----1-----------------clap_footer----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----f3df8dfcd525----1-----------------bookmark_preview----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----0-----------------clap_footer----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----f3df8dfcd525----0---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----f3df8dfcd525----0-----------------bookmark_preview----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----f3df8dfcd525----1---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----f3df8dfcd525----1-----------------bookmark_preview----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f3df8dfcd525----2---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----f3df8dfcd525----2---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----f3df8dfcd525----2---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----f3df8dfcd525----2---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f3df8dfcd525----2---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f3df8dfcd525----2---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----f3df8dfcd525----2---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----f3df8dfcd525----2-----------------bookmark_preview----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f3df8dfcd525----3---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----f3df8dfcd525----3---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----f3df8dfcd525----3---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----f3df8dfcd525----3---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f3df8dfcd525----3---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f3df8dfcd525----3---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----f3df8dfcd525----3---------------------829199f6_5860_46e2_9348_c7548d67c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----f3df8dfcd525----3-----------------bookmark_preview----829199f6_5860_46e2_9348_c7548d67c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----f3df8dfcd525--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}