{"url": "https://towardsdatascience.com/deep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0", "time": 1682995098.896569, "path": "towardsdatascience.com/deep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0/", "webpage": {"metadata": {"title": "Deep RL Case Study: Chaotic Gradients | by Nathan Lambert | Towards Data Science", "h1": "Deep RL Case Study: Chaotic Gradients", "description": "Why do some RL algorithms have notoriously unreliable gradients, what is recent research doing about it, and how effective are alternatives?"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Process", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Importance_sampling", "anchor_text": "importance sampling", "paragraph_index": 0}, {"url": "http://www.scholarpedia.org/article/Policy_gradient_methods", "anchor_text": "scholarpedia", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1902.01240", "anchor_text": "the paper", "paragraph_index": 16}, {"url": "https://github.com/natolambert/dynamics-learn/blob/master/pipps_psuedocode.py", "anchor_text": "https://github.com/natolambert/dynamics-learn/blob/master/pipps_psuedocode.py", "paragraph_index": 21}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com", "paragraph_index": 23}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com", "paragraph_index": 23}], "all_paragraphs": ["Policy gradient is a classic algorithm in RL. Vanilla policy gradient (VPG) comes down to attempting to increase the likelihood of actions that yield a high reward. There are a few assumptions required for estimating the update rule, but it has been shown to work in practice. This article will assume a brief background in RL, so if words like Markov Decision Process, importance sampling, gradients, likelihood, etc. aren\u2019t familiar I recommend going back to some machine learning basics. There are some resources at the bottom of this article that could be helpful. This article reviews the standard policy gradient, introduces a modified policy update rule, and explains why this change could mean a potentially more useful learning algorithm.", "Ultimately, VPG uses a policy estimator based only on the action probabilities and rewards through a trajectory, but a new paper in RL, PIPPS, suggests estimating the gradient by propagating particles through a learned dynamics model to significantly reduce the variance, potentially enabling the convergence of new model-based methods for RL policy generation.", "Here are some symbols you will see throughout this article.", "The big picture of an RL problem is to observe the actions taken by an agent employing the RL policy in a partially observed world. These feedback loops are the actions the agent takes and how it updates its policy parameters.", "In VPG the gradients are estimated without any assumption of knowledge of the dynamics model. To start, we have the overarching formulation of an RL problem \u2014 optimize parameters to maximize reward subject to the evolution of some agent.", "An item to note here is how the state transitions and action probabilities compound to generate a probability of the trajectory. An exercise to the reader is to consider how you could express the probability of an action at current time T given the initial state and transition probabilities.", "In policy gradient, we want to estimate the gradient of the value function and step our policy towards optimal. Below is the expansion of the expectation of reward with respect to the trajectory evolution (more on the omission of the state transitions later). In the blue is a relation called importance sampling \u2014 a common trick in ML for changing the form of integrals and expectations.", "Specifically, let\u2019s look at the terms that make up the log probability of a policy. If we expand the log probability to get a sum with an initial term (this equation is the result of the exercise I mentioned earlier), we see there are two terms of interest 1) the policy probabilities and the 2) the transition probabilities. In this case, the transition probabilities do not depend on the model parameters and go to 0. What if we could use this evolution to inform our gradients?", "With the presented equations and a couple more steps, the following policy gradient update emerges. Here, we include the estimated advantage of a given state and action, which is the difference between the current value of a state and the estimated return of the current state given current action. This is the standard policy gradient you will find in many tutorials.", "Policy gradient uses a stochastic policy sampled in a deterministic state transition space for its trajectories. This requires no understanding of the system evolution but assumes a deterministic model. This is classic model-free reinforcement learning.", "Here is an excerpt from scholarpedia that I found particularly useful to summarize the estimation assumption in vanilla policy gradient:", "Note, when using a cost function in RL, we are generally mirroring the setup of propagating gradients of a reward, but now minimizing with respect to the negative of some function -c(s). This negative sign is often omitted, but it\u2019s good to wrap your head around why maximizing a negative cost function mirrors a reward. Without this dual, the cost function formulations would be different.", "Here are some causes of divergence in policy gradient methods that need to be addressed.", "PIPPs is a recent paper in my area of research, named for Probabilistic Inference for Particle-Based Policy Search, addressing regularizing gradients in policy search for model-based RL. This paper uses model-based RL to calculate the policy gradient with the context of known system dynamics, building a model-based framework on-top of the more typical model-free variants discussed above. In section 3.2 of PIPPS the authors use a deterministic policy with a stochastic dynamics model to directly propagate particle-based gradients through the dynamics model rollouts into the policy. Recall the estimate for the gradient of a policy in the VPG framework, but now we have different terms go to 0 due to the deterministic policy and stochastic model.", "This new result for the gradient of the policy parameters gives us a new total expression for the policy gradient update rule! This update rule samples a set of particles running the policy through the dynamics model. This is a particle-based, dynamics model conditioned gradient.", "The main item to note above is the probability of a state transition replacing the probability for action in the gradient estimate. In PIPPS, the expectation is across the simulated particles, while in VPG the expectation is across the experimental trajectories. PIPPS is designed to be used with a learned dynamics model, rather than only learning via experiment. This gradient equation is using known model derivatives with a likelihood ratio to reduce variance. Their results on basic experiments show the order of magnitude reductions in the gradient variance. The core is that dynamics model derivatives are more reliable than neural network policy derivatives.", "If you read the paper in more detail, you will see much more in-depth equations explaining the combination of this estimator (likelihood ratio gradient) with the re-parameterization gradient and batch mode parallel computation. The exciting part of this direct gradient calculation is in the results, section 4.1, Figure 3, where this LR gradient has the same order of magnitude of variance as their Total Propagation Algorithm.", "PIPPS gives us a tool to use learned dynamics models when propagating gradients and doing a \u2018model-free RL like\u2019 policy update (I will likely add a link to an article summarizing recent progress in experimental, model-based RL). The standard policy gradient estimates based only on a stochastic policy regularly diverge in experiment due to the high variance of the estimator. Coupled with the high variance of composing a neural network dynamics model, vanilla policy gradient needs some enhancement to converge. PIPPS allows us to regularize the variance of the VPG trajectory-based gradients with that of a particle-based method leveraging the underlying dynamics model.", "The culminating issue as to if this regularization technique will work in our case is if the PIPPS gradient and trajectory estimation can handle the high epistemic uncertainty of experimentally learned dynamics models (epistemic variance is caused by noise in experiments that could be captured by a model, but is not, versus randomness or variance in a repeated process which is aleatoric uncertainty). Our simulations use a dynamics model trained to match the change in state with a regularization term on the variance as a ground truth. Policy gradient is generally considered an on-policy algorithm, so we are spoofing this criterion by passing our learned dynamics as a ground truth \u2014 this has a clear risk in policies learning to exploit inaccuracy in the model.", "The hypothesis is that the variance introduced from the dynamics model is significantly less than the reduction gained in changing how the gradient is propagated. We are using experimental data to learn a dynamics model and generate different types of policies. The hope is that PIPPS will enable more sample efficient, offline policy generation by relying on a trained dynamics model rather than in-experiment rollouts. Other methods that we will baseline PIPPS-based policy gradient to are: MBRL+MPC, imitative MPC, and ME-SAC (model-ensemble soft actor critic). PIPPS is one part of the answer to the question of: what is the best way to generate a control policy, given an initial set of experimental data from a robot that we do not have an underlying understanding of system dynamics.", "This subtle change in gradient architecture to leverage knowledge we have or can learn is why I am excited about the practical use of model-based RL. By using the context we have, we should be able to make more reliable gradient estimates and policy improvements. I hope to see more and more applications of model-based RL in coming years, but I do see the overarching goal of model-free learning to be more compelling and interesting in trying to understand how the human mind could function.", "I\u2019m working on experiments in using PIPPS to train policy on top of learned dynamics models, you can follow and experiment with the code here: https://github.com/natolambert/dynamics-learn/blob/master/pipps_psuedocode.py. I likely will create a follow-up post on experiments, but will not post complete details until published. Reach out with questions to nol@berkeley.edu.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trying to think freely and create equitable & impactful automation @ UCBerkeley EECS. Subscribe directly at robotic.substack.com. More at natolambert.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4546434c84b0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4546434c84b0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4546434c84b0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://natolambert.medium.com/?source=post_page-----4546434c84b0--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----4546434c84b0--------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----4546434c84b0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4546434c84b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4546434c84b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Process"}, {"url": "https://en.wikipedia.org/wiki/Importance_sampling", "anchor_text": "importance sampling"}, {"url": "https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f", "anchor_text": "source"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf", "anchor_text": "Berkeley\u2019s Deep RL Course"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "Source: Berkeley Deep RL Course"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "Source: Berkeley Deep RL Course"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "Source: Berkeley Deep RL Course"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "Source: Berkeley Deep RL Course"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "Source: Berkeley Deep RL Course"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "Source: Berkeley Deep RL Course"}, {"url": "https://spinningup.openai.com/en/latest/algorithms/vpg.html", "anchor_text": "Source: Spinning Up in RL"}, {"url": "https://spinningup.openai.com/en/latest/algorithms/vpg.html", "anchor_text": "Source: Spinning Up in RL"}, {"url": "http://www.scholarpedia.org/article/Policy_gradient_methods", "anchor_text": "scholarpedia"}, {"url": "http://www.scholarpedia.org/article/Policy_gradient_methods", "anchor_text": "http://www.scholarpedia.org/article/Policy_gradient_methods"}, {"url": "https://arxiv.org/abs/1902.01240", "anchor_text": "PIPPS"}, {"url": "https://arxiv.org/abs/1902.01240", "anchor_text": "the paper"}, {"url": "https://github.com/natolambert/dynamics-learn/blob/master/pipps_psuedocode.py", "anchor_text": "https://github.com/natolambert/dynamics-learn/blob/master/pipps_psuedocode.py"}, {"url": "https://spinningup.openai.com/en/latest/algorithms/vpg.html", "anchor_text": "OpenAI\u2019s spinning up in RL"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/\\", "anchor_text": "Berkeley\u2019s Deep RL Course."}, {"url": "http://www.scholarpedia.org/article/Policy_gradient_methods", "anchor_text": "Scholarpedia\u2019s notes on policy gradient."}, {"url": "http://proceedings.mlr.press/v80/parmas18a.html", "anchor_text": "Parmas, P., Rasmussen, C.E., Peters, J. & Doya, K.. (2018). PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:4065\u20134074 ."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4546434c84b0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4546434c84b0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/research?source=post_page-----4546434c84b0---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/tag/engineering?source=post_page-----4546434c84b0---------------engineering-----------------", "anchor_text": "Engineering"}, {"url": "https://medium.com/tag/theory?source=post_page-----4546434c84b0---------------theory-----------------", "anchor_text": "Theory"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4546434c84b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&user=Nathan+Lambert&userId=890b1765e6d&source=-----4546434c84b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4546434c84b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&user=Nathan+Lambert&userId=890b1765e6d&source=-----4546434c84b0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4546434c84b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4546434c84b0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4546434c84b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4546434c84b0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4546434c84b0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4546434c84b0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4546434c84b0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4546434c84b0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4546434c84b0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4546434c84b0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4546434c84b0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4546434c84b0--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://natolambert.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "653 Followers"}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com"}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}