{"url": "https://towardsdatascience.com/autoencoder-neural-networks-what-and-how-354cba12bf86", "time": 1683002062.0999172, "path": "towardsdatascience.com/autoencoder-neural-networks-what-and-how-354cba12bf86/", "webpage": {"metadata": {"title": "Autoencoder neural networks: what and how? | by Jake Krajewski | Towards Data Science", "h1": "Autoencoder neural networks: what and how?", "description": "I\u2019ll be walking through the creation of an autoencoder using Keras and Python. First, I\u2019ll address what an autoencoder is and how would we possibly implement one. Then I\u2019ll go through steps of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://keras.io/", "anchor_text": "Keras", "paragraph_index": 0}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST", "paragraph_index": 0}, {"url": "https://blog.keras.io/building-autoencoders-in-keras.html", "anchor_text": "Chollet", "paragraph_index": 2}, {"url": "https://medium.com/u/7462d2319de7?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Fran\u00e7ois Chollet", "paragraph_index": 20}, {"url": "https://blog.keras.io/building-autoencoders-in-keras.html", "anchor_text": "article", "paragraph_index": 20}], "all_paragraphs": ["I\u2019ll be walking through the creation of an autoencoder using Keras and Python. First, I\u2019ll address what an autoencoder is and how would we possibly implement one. Then I\u2019ll go through steps of actually creating one. Probably going to use MNIST because it\u2019s generic and simple.", "Autoencoders take any input, chop it into some compressed version, and use that to reconstruct what the input was. So basically, input x goes into hidden layer h, h = f(x), and comes out as reconstruction r, r = g(h). The autoencoder is good when r is close to x, or when the output looks like the input.", "So, is it a good thing to have a neural network that outputs exactly what the input was? In many cases, not really, but they\u2019re often used for other purposes. One common objective is that the hidden layer h should have some limitations imposed on it such that it pulls out important details about x, without actually needing to keep all the information that x provided, thereby acting as a sort of lossy compression, and it should do this automatically from examples rather than being engineered by a human to recognize the salient features (Chollet, 2016).", "Often when people write autoencoders, the hope is that the middle layer h will take on useful properties in some compressed format. Here we\u2019ll be building something called an undercomplete autoencoder, which basically means we shrink the hidden layer so that there\u2019s no way it can store the same amount of info that the input gives us, and from that, we try to get a nice reconstruction. Building a model like this forces the autoencoder to find the most salient features of the data you feed it. In real life, it can be used in reducing dimensionality of datasets, which can help for data visualization, or for potentially denoising noisy data.", "Here\u2019s the basic list of things we\u2019ll need to create.", "The simplest autoencoder looks something like this: x \u2192 h \u2192 r, where the function f(x) results in h, and the function g(h) results in r. We\u2019ll be using neural networks so we don\u2019t need to calculate the actual functions.", "Logically, step 1 will be to get some data. We\u2019ll grab MNIST from the Keras dataset library. It\u2019s comprised of 60,000 training examples and 10,000 test examples of handwritten digits 0\u20139. Next, we\u2019ll do some basic data preparation so that we can feed it into our neural network as our input set, x.", "Then in step 2, we\u2019ll build the basic neural network model that gives us hidden layer h from x.", "Once we have a model, we\u2019ll be able to train it in step 3, and then in step 4, we\u2019ll visualize the output.", "First, let\u2019s not forget the necessary imports to help us create our neural network (keras), do standard matrix mathematics (numpy), and plot our data (matplotlib). We\u2019ll call this step 0.", "Step 1. Import our data, and do some basic data preparation. Since we\u2019re not going to use labels here, we only care about the x values.", "Next, we\u2019ll normalize them between 0 and 1. Since they\u2019re greyscale images, with values between 0 and 255, we\u2019ll represent the input as float32's and divide by 255. This means if the value is 255, it\u2019ll be normalized to 255.0/255.0 or 1.0, and so on and so forth.", "Now think about this, we have images that are 28 x 28, with values between 0 and 1, and we want to pass them into a neural network layer as an input vector. What should we do? We could use a convolutional neural network, but in this simple case, we\u2019ll just use a dense layer. So how do we feed it in? We\u2019ll flatten each image into a single dimensional vector of 784 x 1 values (28 x 28 = 784).", "Step 2. Let\u2019s put together a basic network. We\u2019re simply going to create an encoding layer, and a decoding layer. We\u2019ll put them together into a model called the autoencoder below. We\u2019ll also decrease the size of the encoding so we can get some of that data compression. Here we\u2019ll use 36 to keep it simple.", "Now create a model that accepts input_img as inputs and outputs the decoder layer. Then compile the model, in this case with adadelta as the optimizer and binary_crossentropy as the loss.", "Step 3. Our model is ready to train. You\u2019ll be able to run this without a GPU, it doesn\u2019t take long. We\u2019ll call fit on the autoencoder model we created, passing in the x values for both the inputs and outputs, for 15 epochs, with a relatively large batch size (256). This will help it train somewhat quickly. We\u2019ll enable shuffle to prevent homogeneous data in each batch and then we\u2019ll use the test values as validation data. Note: if you want to train longer without over-fitting, sparseness and regularization may be added to your model.", "That\u2019s it. Autoencoder done. You\u2019ll see it should have a loss of about 0.69 meaning that the reconstruction we\u2019ve created generally represents the input fairly well from the highly compressed version. But can\u2019t we take a look at it for ourselves?", "Step 4. For this, we\u2019ll do some inference to grab our reconstructions from our input data, and then we\u2019ll display them with matplotlib. For this we want to use the predict method.", "Here\u2019s the thought process: take our test inputs, run them through autoencoder.predict, then show the originals and the reconstructions.", "Here\u2019s how you get that image above:", "That\u2019s it. That\u2019s the most basic autoencoder. Many thanks to Fran\u00e7ois Chollet, whose article I learned this from and inspired the basics of this tutorial, and Goodfellow, et al. for writing Deep Learning as an invaluable reference.", "If you liked this tutorial or have suggestions, leave a comment below.", "Edit: I\u2019ve added the ability to view the hidden layer here which is definitely interesting. You can see that from these 6 x 6 images, the neural network is able to reconstruct the original input. Are you starting to see why this might be useful?", "How would we get that middle row? Well that\u2019s easy, we create a model out of the first hidden layer. Why? We do this so we can run the predict functionality and add its results to a list in python.", "Then we modify the matplotlib instructions a little bit to include the new images:", "That\u2019s all for now! I hope this tutorial helped you understand a little about the thought processes behind autoencoders and how to use them in your neural networks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\u2627 Cognitive Science Master, Experienced digital product designer. Formerly @GoPro . Exploring the intersection of tech, startups, and a.i./deep learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F354cba12bf86&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----354cba12bf86--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jakekrajewski.medium.com/?source=post_page-----354cba12bf86--------------------------------", "anchor_text": ""}, {"url": "https://jakekrajewski.medium.com/?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Jake Krajewski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd08d60aa08c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&user=Jake+Krajewski&userId=d08d60aa08c&source=post_page-d08d60aa08c----354cba12bf86---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F354cba12bf86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F354cba12bf86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST"}, {"url": "https://blog.keras.io/building-autoencoders-in-keras.html", "anchor_text": "Chollet"}, {"url": "https://medium.com/u/7462d2319de7?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Fran\u00e7ois Chollet"}, {"url": "https://blog.keras.io/building-autoencoders-in-keras.html", "anchor_text": "article"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----354cba12bf86---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----354cba12bf86---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----354cba12bf86---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----354cba12bf86---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----354cba12bf86---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F354cba12bf86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&user=Jake+Krajewski&userId=d08d60aa08c&source=-----354cba12bf86---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F354cba12bf86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&user=Jake+Krajewski&userId=d08d60aa08c&source=-----354cba12bf86---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F354cba12bf86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F354cba12bf86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----354cba12bf86---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----354cba12bf86--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----354cba12bf86--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----354cba12bf86--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----354cba12bf86--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----354cba12bf86--------------------------------", "anchor_text": ""}, {"url": "https://jakekrajewski.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jakekrajewski.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jake Krajewski"}, {"url": "https://jakekrajewski.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "210 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd08d60aa08c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&user=Jake+Krajewski&userId=d08d60aa08c&source=post_page-d08d60aa08c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe248356cbb48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoder-neural-networks-what-and-how-354cba12bf86&newsletterV3=d08d60aa08c&newsletterV3Id=e248356cbb48&user=Jake+Krajewski&userId=d08d60aa08c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}