{"url": "https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a", "time": 1683006296.694168, "path": "towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a/", "webpage": {"metadata": {"title": "Working with Hugging Face Transformers and TF 2.0 | by Akash Desarda | Towards Data Science", "h1": "Working with Hugging Face Transformers and TF 2.0", "description": "I am assuming that you are aware of Transformers and its attention mechanism. The primary aim of this blog is to show how to use Hugging Face\u2019s transformer library with TF 2.0, i.e. it will be more\u2026"}, "outgoing_paragraph_urls": [{"url": "https://huggingface.co/transformers/", "anchor_text": "Docs", "paragraph_index": 3}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge", "anchor_text": "Kaggle\u2019s Toxic Comment Classification Challenge", "paragraph_index": 4}, {"url": "http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/", "anchor_text": "article", "paragraph_index": 23}, {"url": "https://medium.com/u/34369d020458?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Jay Alammar", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/akashdesarda", "anchor_text": "LinkedIn", "paragraph_index": 41}, {"url": "https://www.linkedin.com/in/akashdesarda/", "anchor_text": "https://www.linkedin.com/in/akashdesarda/", "paragraph_index": 43}], "all_paragraphs": ["I am assuming that you are aware of Transformers and its attention mechanism. The primary aim of this blog is to show how to use Hugging Face\u2019s transformer library with TF 2.0, i.e. it will be more code-focused blog.", "Hugging Face initially supported only PyTorch, but now TF 2.0 is also well supported. You can find a good number of quality tutorials for using the transformer library with PyTorch, but same is not true with TF 2.0 (primary motivation for this blog).", "To use BERT or even AlBERT is quite easy and the standard process in TF 2.0 courtesy to tensorflow_hub, but the same is not the case with GPT2, RoBERTa, DistilBERT, etc. Here comes Hugging Face\u2019s transformer library to rescue. They provide intuitive APIs to build a custom model from scratch or fine-tune a pre-trained model for a wide list of the transformer-based models.", "It supports a wide range of NLP application like Text classification, Question-Answer system, Text summarization, Token classification, etc. Head over to their Docs for more detail.", "This tutorial will be based on a Multi-Label Text classification of Kaggle\u2019s Toxic Comment Classification Challenge.", "Following is a general pipeline for any transformer model:", "Tokenizer definition \u2192Tokenization of Documents \u2192Model Definition \u2192Model Training \u2192Inference", "Let us now go over them one by one, I will also try to cover multiple possible use cases.", "Every transformer based model has a unique tokenization technique, unique use of special tokens. The transformer library takes care of this for us. It supports tokenization for every model which is associated with it.", "\u2192Every transformer model has a similar token definition API", "\u2192Here I am using a tokenizer from a Pretrained model.", "Next step is now to perform tokenization on documents. It can be performed either by encode() or encode_plus() method.", "\u2192Any transformer model generally needs three input:", "\u2192Though it is not compulsory to provide all these three ids and only input ids will also do, but attention mask help model to focus on only valid words. So at least for classification task both this should be provided.", "Now comes the most crucial part, the \u2018Training\u2019. The method which I am going to discuss is by no means \u2018the only possible way\u2019 to train. Though after a lot of experimenting I found this method to be most workable. I will discuss three possible ways to train the model:", "2.3.1 Use Pretrained model directly as a classifier", "This is the simplest but also with the least application. Hugging Face\u2019s transformers library provide some models with sequence classification ability. These model have two heads, one is a pre-trained model architecture as the base & a classifier as the top head.", "Tokenizer definition \u2192Tokenization of Documents \u2192Model Definition", "\u2192Note: Models which are SequenceClassification are only applicable here.", "\u2192Defining the proper config is crucial here. As you can see on line 6, I am defining the config. \u2018num_labels\u2019 is the number of classes to use when the model is a classification model. It also supports a variety of configs so go ahead & see their docs.", "\u2192 Some key things to note here are:", "2.3.2 Transformer model to extract embedding and use it as input to another classifier", "This approach needs two level or two separate models. We use any transformer model to extract word embedding & then use this word embedding as input to any classifier (eg Logistic classifier, Random forest, Neural nets, etc).", "I would suggest you read this article by Jay Alammar which discusses this approach with great detail and clarity.", "As this blog is all about neural nets, let me give you an example of this approach with NN.", "\u2192Line #11 is key here. We are only interested in <cls> or classification token of the model which can be extracted using the slice operation. Now we have 2D data and build the network as one desired.", "\u2192This approach works generally better every time compared to 2.3.1 approach. But it also has some drawbacks, like:", "The transformers library provide a great utility if you want to just extract word embedding.", "2.3.3 Fine-tuning a Pretrained transformer model", "This is my favourite approach as here we are making use of the full potential of any transformer model. Here we\u2019ll be using weights of pre-trained transformer model and then fine-tune on our data i.e transfer learning.", "\u2192Look at line #17 as 3D data is generated earlier embedding layer, we can use LSTM to extract great details.", "\u2192Next thing is to transform the 3D data into 2D so that we can use a FC layer. You can use any Pooling layer to perform this.", "\u2192 Also, note on line #18 & #19. We should always freeze the pre-trained weights of transformer model & never update them and update only remaining weights.", "\u2192Every approach has two things in common:", "\u2192config is a dictionary. So to see all available configuration, just simply print it.", "\u2192Choose base model carefully as TF 2.0 support is new, so there might be bugs.", "As the model is based on tf.keras model API, we can use Keras\u2019 same commonly used method of model.predict()", "We can even use the transformer library\u2019s pipeline utility (please refer to the example shown in 2.3.2). This utility is quite effective as it unifies tokenization and prediction under one common simple API.", "Hugging Face has really made it quite easy to use any of their models now with tf.keras. It has open wide possibilities.", "They have also made it quite easy to use their model in the cross library (from PyTorch to TF or vice versa).", "I would suggest visiting their docs, as they have very intuitive & to-the-point docs.", "You can contact me via LinkedIn", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer @ Duff and Phelps | I am a person who always tries to find an alternative. LinkedIn: https://www.linkedin.com/in/akashdesarda/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F89bf35e3555a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@desardaakash?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@desardaakash?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Akash Desarda"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd17e5f5c398e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&user=Akash+Desarda&userId=d17e5f5c398e&source=post_page-d17e5f5c398e----89bf35e3555a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89bf35e3555a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89bf35e3555a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Docs"}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge", "anchor_text": "Kaggle\u2019s Toxic Comment Classification Challenge"}, {"url": "http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/", "anchor_text": "article"}, {"url": "https://medium.com/u/34369d020458?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Jay Alammar"}, {"url": "https://www.linkedin.com/in/akashdesarda", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----89bf35e3555a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----89bf35e3555a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----89bf35e3555a---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----89bf35e3555a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/hugging-face?source=post_page-----89bf35e3555a---------------hugging_face-----------------", "anchor_text": "Hugging Face"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89bf35e3555a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&user=Akash+Desarda&userId=d17e5f5c398e&source=-----89bf35e3555a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89bf35e3555a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&user=Akash+Desarda&userId=d17e5f5c398e&source=-----89bf35e3555a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89bf35e3555a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F89bf35e3555a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----89bf35e3555a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----89bf35e3555a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----89bf35e3555a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----89bf35e3555a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@desardaakash?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@desardaakash?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akash Desarda"}, {"url": "https://medium.com/@desardaakash/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "504 Followers"}, {"url": "https://www.linkedin.com/in/akashdesarda/", "anchor_text": "https://www.linkedin.com/in/akashdesarda/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd17e5f5c398e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&user=Akash+Desarda&userId=d17e5f5c398e&source=post_page-d17e5f5c398e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe623bbd4584f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fworking-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a&newsletterV3=d17e5f5c398e&newsletterV3Id=e623bbd4584f&user=Akash+Desarda&userId=d17e5f5c398e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}