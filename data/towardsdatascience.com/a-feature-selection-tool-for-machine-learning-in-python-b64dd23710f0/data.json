{"url": "https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0", "time": 1682993458.8464582, "path": "towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0/", "webpage": {"metadata": {"title": "A Feature Selection Tool for Machine Learning in Python | by Will Koehrsen | Towards Data Science", "h1": "A Feature Selection Tool for Machine Learning in Python", "description": "Feature selection, the process of finding and selecting the most useful features in a dataset, is a crucial step of the machine learning pipeline. Unnecessary features decrease training speed\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/WillKoehrsen/feature-selector", "anchor_text": "available on GitHub", "paragraph_index": 1}, {"url": "https://github.com/WillKoehrsen/feature-selector", "anchor_text": "available on GitHub", "paragraph_index": 3}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit Default Risk machine learning competition", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426", "anchor_text": "this article", "paragraph_index": 4}, {"url": "https://www.kaggle.com/c/home-credit-default-risk/data", "anchor_text": "available for download", "paragraph_index": 4}, {"url": "https://www.quora.com/Why-is-multicollinearity-bad-in-laymans-terms-In-feature-selection-for-a-regression-model-intended-for-use-in-prediction-why-is-it-a-bad-thing-to-have-multicollinearity-or-highly-correlated-independent-variables", "anchor_text": "Collinear features", "paragraph_index": 14}, {"url": "http://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/", "anchor_text": "correlation coefficient", "paragraph_index": 15}, {"url": "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/", "anchor_text": "such as a boosting ensemble, we can find feature importances.", "paragraph_index": 20}, {"url": "https://www.salford-systems.com/blog/dan-steinberg/what-is-the-variable-importance-measure", "anchor_text": "features with zero importance are not used to split any nodes", "paragraph_index": 20}, {"url": "http://lightgbm.readthedocs.io", "anchor_text": "LightGBM library", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60", "anchor_text": "using Principal Components Analysis (PCA)", "paragraph_index": 33}, {"url": "https://github.com/Featuretools/featuretools/blob/master/featuretools/selection/selection.py", "anchor_text": "find any columns that have a single unique value.", "paragraph_index": 35}, {"url": "https://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/", "anchor_text": "feature has zero variance", "paragraph_index": 35}, {"url": "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html", "anchor_text": "calculating unique values in Pandas by default.", "paragraph_index": 38}, {"url": "https://machinelearningmastery.com/an-introduction-to-feature-selection/", "anchor_text": "operations for removing features", "paragraph_index": 45}, {"url": "https://hips.seas.harvard.edu/blog/2012/12/24/the-empirical-science-of-machine-learning-evaluating-rbms/", "anchor_text": "field of machine learning, is largely empirical", "paragraph_index": 46}, {"url": "https://github.com/WillKoehrsen/feature-selector", "anchor_text": "contribute on GitHub", "paragraph_index": 47}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 47}], "all_paragraphs": ["Feature selection, the process of finding and selecting the most useful features in a dataset, is a crucial step of the machine learning pipeline. Unnecessary features decrease training speed, decrease model interpretability, and, most importantly, decrease generalization performance on the test set.", "Frustrated by the ad-hoc feature selection methods I found myself applying over and over again for machine learning problems, I built a class for feature selection in Python available on GitHub. The FeatureSelector includes some of the most common feature selection methods:", "In this article we will walk through using the FeatureSelector on an example machine learning dataset. We\u2019ll see how it allows us to rapidly implement these methods, allowing for a more efficient workflow.", "The complete code is available on GitHub and I encourage any contributions. The Feature Selector is a work in progress and will continue to improve based on the community needs!", "For this example, we will use a sample of data from the Home Credit Default Risk machine learning competition on Kaggle. (To get started with the competition, see this article). The entire dataset is available for download and here we will use a sample for illustration purposes.", "The competition is a supervised classification problem and this is a good dataset to use because it has many missing values, numerous highly correlated (collinear) features, and a number of irrelevant features that do not help a machine learning model.", "To create an instance of the FeatureSelector class, we need to pass in a structured dataset with observations in the rows and features in the columns. We can use some of the methods with only features, but the importance-based methods also require training labels. Since we have a supervised classification task, we will use a set of features and a set of labels.", "(Make sure to run this in the same directory as feature_selector.py )", "The feature selector has five methods for finding features to remove. We can access any of the identified features and remove them from the data manually, or use the remove function in the Feature Selector.", "Here we will go through each of the identification methods and also show how all 5 can be run at once. The FeatureSelector additionally has several plotting capabilities because visually inspecting data is a crucial component of machine learning.", "The first method for finding features to remove is straightforward: find features with a fraction of missing values above a specified threshold. The call below identifies features with more than 60% missing values (bold is output).", "We can see the fraction of missing values in every column in a dataframe:", "To see the features identified for removal, we access the ops attribute of the FeatureSelector , a Python dict with features as lists in the values.", "Finally, we have a plot of the distribution of missing values in all feature:", "Collinear features are features that are highly correlated with one another. In machine learning, these lead to decreased generalization performance on the test set due to high variance and less model interpretability.", "The identify_collinear method finds collinear features based on a specified correlation coefficient value. For each pair of correlated features, it identifies one of the features for removal (since we only need to remove one):", "A neat visualization we can make with correlations is a heatmap. This shows all the features that have at least one correlation above the threshold:", "As before, we can access the entire list of correlated features that will be removed, or see the highly correlated pairs of features in a dataframe.", "If we want to investigate our dataset, we can also make a plot of all the correlations in the data by passing in plot_all = True to the call:", "The previous two methods can be applied to any structured dataset and are deterministic \u2014 the results will be the same every time for a given threshold. The next method is designed only for supervised machine learning problems where we have labels for training a model and is non-deterministic. The identify_zero_importance function finds features that have zero importance according to a gradient boosting machine (GBM) learning model.", "With tree-based machine learning models, such as a boosting ensemble, we can find feature importances. The absolute value of the importance is not as important as the relative values, which we can use to determine the most relevant features for a task. We can also use feature importances for feature selection by removing zero importance features. In a tree-based model, the features with zero importance are not used to split any nodes, and so we can remove them without affecting model performance.", "The FeatureSelector finds feature importances using the gradient boosting machine from the LightGBM library. The feature importances are averaged over 10 training runs of the GBM in order to reduce variance. Also, the model is trained using early stopping with a validation set (there is an option to turn this off) to prevent overfitting to the training data.", "The code below calls the method and extracts the zero importance features:", "The parameters we pass in are as follows:", "This time we get two plots with plot_feature_importances:", "On the left we have the plot_n most important features (plotted in terms of normalized importance where the total sums to 1). On the right we have the cumulative importance versus the number of features. The vertical line is drawn at threshold of the cumulative importance, in this case 99%.", "Two notes are good to remember for the importance-based methods:", "This should not have a major impact (the most important features will not suddenly become the least) but it will change the ordering of some of the features. It also can affect the number of zero importance features identified. Don\u2019t be surprised if the feature importances change every time!", "When we get to the feature removal stage, there is an option to remove any added one-hot encoded features. However, if we are doing machine learning after feature selection, we will have to one-hot encode the features anyway!", "The next method builds on zero importance function, using the feature importances from the model for further selection. The function identify_low_importance finds the lowest importance features that do not contribute to a specified total importance.", "For example, the call below finds the least important features that are not required for achieving 99% of the total importance:", "Based on the plot of cumulative importance and this information, the gradient boosting machine considers many of the features to be irrelevant for learning. Again, the results of this method will change on each training run.", "To view all the feature importances in a dataframe:", "The low_importance method borrows from one of the methods of using Principal Components Analysis (PCA) where it is common to keep only the PC needed to retain a certain percentage of the variance (such as 95%). The percentage of total importance accounted for is based on the same idea.", "The feature importance based methods are really only applicable if we are going to use a tree-based model for making predictions. Besides being stochastic, the importance-based methods are a black-box approach in that we don\u2019t really know why the model considers the features to be irrelevant. If using these methods, run them several times to see how the results change, and perhaps create multiple datasets with different parameters to test!", "The final method is fairly basic: find any columns that have a single unique value. A feature with only one unique value cannot be useful for machine learning because this feature has zero variance. For example, a tree-based model can never make a split on a feature with only one value (since there are no groups to divide the observations into).", "There are no parameters here to select, unlike the other methods:", "We can plot a histogram of the number of unique values in each category:", "One point to remember is NaNs are dropped before calculating unique values in Pandas by default.", "Once we\u2019ve identified the features to discard, we have two options for removing them. All of the features to remove are stored in the ops dict of the FeatureSelector and we can use the lists to remove features manually. Another option is to use the remove built-in function.", "For this method, we pass in the methods to use to remove features. If we want to use all the methods implemented, we just pass in methods = 'all'.", "This method returns a dataframe with the features removed. To also remove the one-hot encoded features that are created during machine learning:", "It might be a good idea to check the features that will be removed before going ahead with the operation! The original dataset is stored in the data attribute of the FeatureSelector as a back-up!", "Rather than using the methods individually, we can use all of them with identify_all. This takes a dictionary of the parameters for each method:", "Notice that the number of total features will change because we re-ran the model. The remove function can then be called to discard these features.", "The Feature Selector class implements several common operations for removing features before training a machine learning model. It offers functions for identifying features for removal as well as visualizations. Methods can be run individually or all at once for efficient workflows.", "The missing, collinear, and single_unique methods are deterministic while the feature importance-based methods will change with each run. Feature selection, much like the field of machine learning, is largely empirical and requires testing multiple combinations to find the optimal answer. It\u2019s best practice to try several configurations in a pipeline, and the Feature Selector offers a way to rapidly evaluate parameters for feature selection.", "As always, I welcome feedback and constructive criticism. I want to emphasis that I\u2019m looking for help on the FeatureSelector. Anyone can contribute on GitHub and I appreciate advice from those who just uses the tool! I can also be reached on Twitter @koehrsen_will.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb64dd23710f0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----b64dd23710f0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb64dd23710f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb64dd23710f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/WillKoehrsen/feature-selector", "anchor_text": "available on GitHub"}, {"url": "https://github.com/WillKoehrsen/feature-selector", "anchor_text": "available on GitHub"}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit Default Risk machine learning competition"}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426", "anchor_text": "this article"}, {"url": "https://www.kaggle.com/c/home-credit-default-risk/data", "anchor_text": "available for download"}, {"url": "https://www.quora.com/Why-is-multicollinearity-bad-in-laymans-terms-In-feature-selection-for-a-regression-model-intended-for-use-in-prediction-why-is-it-a-bad-thing-to-have-multicollinearity-or-highly-correlated-independent-variables", "anchor_text": "Collinear features"}, {"url": "http://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/", "anchor_text": "correlation coefficient"}, {"url": "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/", "anchor_text": "such as a boosting ensemble, we can find feature importances."}, {"url": "https://www.salford-systems.com/blog/dan-steinberg/what-is-the-variable-importance-measure", "anchor_text": "features with zero importance are not used to split any nodes"}, {"url": "http://lightgbm.readthedocs.io", "anchor_text": "LightGBM library"}, {"url": "https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60", "anchor_text": "using Principal Components Analysis (PCA)"}, {"url": "https://github.com/Featuretools/featuretools/blob/master/featuretools/selection/selection.py", "anchor_text": "find any columns that have a single unique value."}, {"url": "https://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/", "anchor_text": "feature has zero variance"}, {"url": "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html", "anchor_text": "calculating unique values in Pandas by default."}, {"url": "https://machinelearningmastery.com/an-introduction-to-feature-selection/", "anchor_text": "operations for removing features"}, {"url": "https://hips.seas.harvard.edu/blog/2012/12/24/the-empirical-science-of-machine-learning-evaluating-rbms/", "anchor_text": "field of machine learning, is largely empirical"}, {"url": "https://github.com/WillKoehrsen/feature-selector", "anchor_text": "contribute on GitHub"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b64dd23710f0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----b64dd23710f0---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/education?source=post_page-----b64dd23710f0---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/programming?source=post_page-----b64dd23710f0---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b64dd23710f0---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb64dd23710f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----b64dd23710f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb64dd23710f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----b64dd23710f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb64dd23710f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb64dd23710f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b64dd23710f0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b64dd23710f0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b64dd23710f0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b64dd23710f0--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}