{"url": "https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2", "time": 1683003698.365175, "path": "towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2/", "webpage": {"metadata": {"title": "SVM From Scratch \u2014 Python. Important Concepts Summarized | by Qandeel Abbassi | Towards Data Science", "h1": "SVM From Scratch \u2014 Python", "description": "1. SVM Introduction 2. Reading the Dataset 3. Feature Engineering 4. Splitting the Dataset 5. Cost Function 6. The Gradient of the Cost Function 7. Train Model Using SGD 8. Stoppage Criterion for SGD\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/", "anchor_text": "scikit-learn", "paragraph_index": 1}, {"url": "https://cvxopt.org/", "anchor_text": "cvxopt", "paragraph_index": 1}, {"url": "https://www.kaggle.com/uciml/breast-cancer-wisconsin-data", "anchor_text": "Kaggle", "paragraph_index": 6}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html", "anchor_text": "read_csv()", "paragraph_index": 8}, {"url": "https://pandas.pydata.org/", "anchor_text": "Pandas", "paragraph_index": 8}, {"url": "https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#features", "anchor_text": "features", "paragraph_index": 10}, {"url": "https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#labels", "anchor_text": "label", "paragraph_index": 10}, {"url": "https://www.javatpoint.com/backward-elimination-in-machine-learning", "anchor_text": "backward elimination", "paragraph_index": 41}, {"url": "https://Traverous.com", "anchor_text": "https://Traverous.com", "paragraph_index": 47}, {"url": "https://qandeelabbassi.github.io/", "anchor_text": "https://qandeelabbassi.github.io/", "paragraph_index": 47}], "all_paragraphs": ["1. SVM Introduction2. Reading the Dataset3. Feature Engineering4. Splitting the Dataset5. Cost Function6. The Gradient of the Cost Function7. Train Model Using SGD8. Stoppage Criterion for SGD9. Testing the Model10. Feature Selection With Correlation & P-values11. Give Me the Code", "Before diving right into the code or technical details, I would like to mention that while there are many libraries/frameworks available to implement SVM (Support Vector Machine) algorithm without writing a bunch of code, I decided to write the code with as few high-level libraries as possible so that you and I can get a good grasp of important components involved in training an SVM model (with 99% accuracy, 0.98 recall, and precision). If you are looking for a quick implementation of SVM, then you are better off using packages like scikit-learn, cvxopt, etc. Otherwise, let\u2019s get started!", "The SVM (Support Vector Machine) is a supervised machine learning algorithm typically used for binary classification problems. It\u2019s trained by feeding a dataset with labeled examples (x\u1d62, y\u1d62). For instance, if your examples are email messages and your problem is spam detection, then:", "Using this dataset the algorithm finds a hyperplane (or decision boundary) which should ideally have the following properties:", "How does it find this hyperplane? By finding the optimal values w* (weights/normal) and b* (intercept) which define this hyperplane. The optimal values are found by minimizing a cost function. Once the algorithm identifies these optimal values, the SVM model f(x) is then defined as shown below:", "Before we move any further let\u2019s import the required packages for this tutorial and create a skeleton of our program svm.py:", "We\u2019ll be working with a breast cancer dataset available on Kaggle. The features in the dataset are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe the characteristics of the cell nuclei present in the image. Based on these features we will train our SVM model to detect if the mass is benign B (generally harmless) or malignant M (cancerous).", "Download the dataset and place the data.csv file in the same folder as svm.py. Then add this code inside init() function:", "read_csv() function of the Pandas package reads data from the .csv file and stores it in a DataFrame. Think of DataFrame as an implementation of a data structure that looks like a table with labeled columns and rows. Here\u2019s how the data read from data.csv looks like inside DataFrame:", "As you can see the labels and structure of our dataset are preserved and that\u2019s what makes DataFrame intuitive.", "Machine learning algorithms operate on a dataset that is a collection of labeled examples which consist of features and a label i.e. in our case diagnosis is a label, [radius_mean, structure_mean, texture_mean\u2026] features, and each row is an example.", "In most of the cases, the data you collect at first might be raw; its either incompatible with your model or hinders its performance. That\u2019s when feature engineering comes to rescue. It encompasses preprocessing techniques to compile a dataset by extracting features from raw data. These techniques have two characteristics in common:", "Normalization is one of the many feature engineering techniques that we are going to use. Normalization is the process of converting a range of values, into a standard range of values, typically in the interval [\u22121, 1] or [0, 1]. It\u2019s not a strict requirement but it improves the speed of learning (e.g. faster convergence in gradient descent) and prevents numerical overflow. Add following code in init() functions to normalize all of your features:", "We\u2019ll split the dataset into train and test set using the train_test_split() function from sklearn.model_selection. We need a separate dataset for testing because we need to see how our model will perform on unseen observations. Add this code in init():", "If you are confused about why we added 1 in every row then don\u2019t worry. You will get the answer in the next section.", "Also known as the Objective Function. One of the building blocks of every machine learning algorithm, it\u2019s the function we try to minimize or maximize to achieve our objective.", "What\u2019s our objective in SVM? Our objective is to find a hyperplane that separates +ve and -ve examples with the largest margin while keeping the misclassification as low as possible (see Figure 3).", "How do we achieve this objective? We will minimize the cost/objective function shown below:", "You might have seen another version of a cost function that looks like this:", "In this cost function, \u03bb is essentially equal to 1/C and has the opposite effect i.e larger \u03bb gives a wider margin and vice versa. We can use any of the above cost functions keeping in mind what each regularization parameter (C and \u03bb) does and then tuning them accordingly. Let\u2019s see how can we calculate the total cost as given in (1) and then we will move on to its gradient which will be used in the training phase to minimize it:", "As you might have noticed that the intercept term b is missing. That\u2019s because we have pushed it into the weight vector like this:", "That\u2019s why we added an extra column with all 1s before splitting our dataset. Keep this in mind for the rest of the tutorial.", "As you would have noticed there are some changes in cost function in equation (4). Don\u2019t worry, if you solve it analytically it\u2019s the same. Now let\u2019s implement the calculate_cost_gradient() function using equation (5):", "Remember I said above, \u201cTo achieve our objective we try to minimize or maximize the cost function\u201d. In the SVM algorithm, we minimize the cost function.", "Why do we minimize the cost function? Because the cost function is essentially a measure of how bad our model is doing at achieving the objective. If you look closely at J(w), to find it\u2019s minimum, we have to:", "Because both of our SVM objectives are achieved by minimizing the cost function, that\u2019s why we minimize it.", "How do we minimize it? Well, there are multiple ways but the one we will use is called Stochastic Gradient Descent or SGD. Before diving into SGD, I will briefly explain how Gradient Descent works in the first place.", "Gradient descent algorithm works as follows:", "Why does it move opposite to the direction of the gradient? Because gradient is the direction of the fastest increase of the function. We need to move opposite to that direction to minimize our function J(w). Hence, the word \u201cdescent\u201d in Gradient Descent is used.", "In typical gradient descent (a.k.a vanilla gradient descent) the step 1 above is calculated using all the examples (1\u2026N). In SGD, however, only 1 example is used at a time. I won\u2019t discuss the benefits of SGD here but you can find some useful links at the end of this blog. Here\u2019s how to implement the SGD in code:", "Let\u2019s call it inside init() function by adding this code:", "In the above implementation of sgd() we are running the loop 5000 times (could have been any number). Each iteration costs us time and extra computations. We don\u2019t need to complete all the iterations. We can terminate the loop when our stoppage criterion is met.", "What should be the stoppage criterion? There are multiple options, but we will use the simplest one. We will stop the training when the current cost hasn\u2019t decreased much as compared to the previous cost. Here is how we will define sgd() with stoppage criterion:", "After training the model using SGD we finally got the optimal weights w* which defines the best possible hyperplane separating two classes. Let\u2019s test our model using this hyperplane. Add this code in init() function:", "Note the accuracy, recall, and precision score of the model and epochs it took to complete. Now let\u2019s try to improve it using feature selection.", "Feature selection encompasses statistical techniques that help in filtering irrelevant or redundant features. Correlation & p-values are among these statistical techniques. Using them we will select a subset of relevant and important features from our original set of features", "What is correlation? Correlation is a degree of linear dependence (or linear relationship) between two variables. Two features are said to be correlated if the values of one feature can be explained by some linear relationship of the second feature. The degree of this relationship is given by the correlation coefficient (or \u201cr\u201d). It ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related.", "Why do we remove one of the correlated features? There are multiple reasons but the simplest of them is that correlated features almost have the same effect on the dependent variable. Moreover, correlated features won\u2019t improve our model and would most probably worsen it, therefore we are better off using only one of them. After all, fewer features result in improved learning speed and a simpler model (model with fewer features).", "What are p-values? It\u2019s too broad to cover in this blog. But, in the context of feature selection, p-values help us find the features which are most significant in explaining variation in the dependent variable (y).", "A feature with a low p-value has more significance and a feature with a high p-value has less significance in explaining the variation. Usually, we set a Significance Level SL (threshold) and if a feature has a p-value above this level it is discarded. I will leave some links at the end of this blog for an in-depth study on p-values.", "Why do we remove features with high p-values? Because they don\u2019t tell much about the behavior of the dependent variable. So, why keep them and unnecessarily increase the complexity of our model when they are not helping us in predicting the result.", "We have two functions named remove_correlated_features() remove_less_significant_features() for removing highly correlated features and less significant features (using p-values and backward elimination) respectively:", "Let\u2019s call these functions inside init() before we apply normalization:", "Rerun the code and check the output:", "As you can see the accuracy (99%), precision (0.98), and recall (0.98) scores have improved. Moreover, SGD has converged faster; The training ended within 2048 epochs which is way less as compared to the previous one (5000 epochs)", "You can get the complete code in this Github repository:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-founder Traverous (https://Traverous.com). Full Stack Developer. New experiences and challenges are my thing. https://qandeelabbassi.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2db2fc52e5c2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@qandeelabbassi?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@qandeelabbassi?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "Qandeel Abbassi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1dc7f442b61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&user=Qandeel+Abbassi&userId=a1dc7f442b61&source=post_page-a1dc7f442b61----2db2fc52e5c2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2db2fc52e5c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2db2fc52e5c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://scikit-learn.org/", "anchor_text": "scikit-learn"}, {"url": "https://cvxopt.org/", "anchor_text": "cvxopt"}, {"url": "http://ireneli.eu", "anchor_text": "Ireneli.eu"}, {"url": "https://www.kaggle.com/uciml/breast-cancer-wisconsin-data", "anchor_text": "Kaggle"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html", "anchor_text": "read_csv()"}, {"url": "https://pandas.pydata.org/", "anchor_text": "Pandas"}, {"url": "https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#features", "anchor_text": "features"}, {"url": "https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#labels", "anchor_text": "label"}, {"url": "https://en.wikipedia.org/wiki/Support-vector_machine", "anchor_text": "Wikipedia"}, {"url": "https://www.javatpoint.com/backward-elimination-in-machine-learning", "anchor_text": "backward elimination"}, {"url": "https://github.com/qandeelabbassi/python-svm-sgd", "anchor_text": "qandeelabbassi/python-svm-sgdPython implementation of stochastic sub-gradient descent algorithm for SVM from scratch - qandeelabbassi/python-svm-sgdgithub.com"}, {"url": "https://towardsdatascience.com/https-towardsdatascience-com-why-stochastic-gradient-descent-works-9af5b9de09b8", "anchor_text": "Why SGD works?"}, {"url": "https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent", "anchor_text": "Benefits of SGD"}, {"url": "https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8", "anchor_text": "P-values explained by a data scientist"}, {"url": "https://www.youtube.com/watch?v=nk2CQITm_eo", "anchor_text": "Linear regression and p-values"}, {"url": "https://www.javatpoint.com/backward-elimination-in-machine-learning", "anchor_text": "Backward elimination"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2db2fc52e5c2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----2db2fc52e5c2---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/support-vector-machine?source=post_page-----2db2fc52e5c2---------------support_vector_machine-----------------", "anchor_text": "Support Vector Machine"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----2db2fc52e5c2---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/learning-to-code?source=post_page-----2db2fc52e5c2---------------learning_to_code-----------------", "anchor_text": "Learning To Code"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2db2fc52e5c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&user=Qandeel+Abbassi&userId=a1dc7f442b61&source=-----2db2fc52e5c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2db2fc52e5c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&user=Qandeel+Abbassi&userId=a1dc7f442b61&source=-----2db2fc52e5c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2db2fc52e5c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2db2fc52e5c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2db2fc52e5c2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2db2fc52e5c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@qandeelabbassi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@qandeelabbassi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Qandeel Abbassi"}, {"url": "https://medium.com/@qandeelabbassi/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "56 Followers"}, {"url": "https://Traverous.com", "anchor_text": "https://Traverous.com"}, {"url": "https://qandeelabbassi.github.io/", "anchor_text": "https://qandeelabbassi.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1dc7f442b61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&user=Qandeel+Abbassi&userId=a1dc7f442b61&source=post_page-a1dc7f442b61--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fa1dc7f442b61%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsvm-implementation-from-scratch-python-2db2fc52e5c2&user=Qandeel+Abbassi&userId=a1dc7f442b61&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}