{"url": "https://towardsdatascience.com/deconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1", "time": 1682996592.295234, "path": "towardsdatascience.com/deconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1/", "webpage": {"metadata": {"title": "Deconstructing BERT. Transformer architecture models\u2026 | by Ajit Rajasekharan | Towards Data Science", "h1": "Deconstructing BERT", "description": "Transformer architecture models, particularly BERT, has demonstrated to be quite effective in many NLP tasks by just fine tuning a model that was pretrained in an unsupervised manner on a large\u2026"}, "outgoing_paragraph_urls": [{"url": "https://nlp.stanford.edu/pubs/hewitt2019structural.pdf", "anchor_text": "1", "paragraph_index": 1}, {"url": "https://nlp.stanford.edu//~johnhew//structural-probe.html?utm_source=quora&utm_medium=referral#the-structural-probe", "anchor_text": "blog", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1906.02715.pdf", "anchor_text": "2", "paragraph_index": 1}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "blog", "paragraph_index": 1}, {"url": "https://nlp.stanford.edu//~johnhew//structural-probe.html?utm_source=quora&utm_medium=referral#the-structural-probe", "anchor_text": "recent discovery", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Minimum_spanning_tree", "anchor_text": "minimum spanning tree", "paragraph_index": 4}, {"url": "https://www.encyclopediaofmath.org/index.php/Isometric_mapping", "anchor_text": "isometrically", "paragraph_index": 6}, {"url": "https://github.com/john-hewitt/structural-probes/", "anchor_text": "Github repository", "paragraph_index": 20}], "all_paragraphs": ["Transformer architecture models, particularly BERT, has demonstrated to be quite effective in many NLP tasks by just fine tuning a model that was pretrained in an unsupervised manner on a large corpus. BERT model takes as input a sequence of words and produces a series of word embeddings across layers. These word embeddings take into account the context in which a word occurs.", "Two recently (April 2019 and 6 June 2019) published papers (1 and its blog, 2 and its blog) offer some geometric insights into to why BERT may be performing so well.", "Some of the key findings from these papers", "Language is made of discrete structures \u2014 sequence of symbols(words) with the syntactic structure of sentences captured in a tree. Neural net models, in contrast operate on continuous data \u2014 they translate symbolic information into vectors in high dimensional space. These vectors (word embeddings) have been known to capture semantic similarity in their lengths/orientation (e.g. word2vec, fasttext etc.) .", "As mentioned above, recent discovery shows BERT word vector output encodes rich linguistic structure. Encoded in the vector output for a sentence is geometrically approximate copies of syntax trees. Words in a sentence are given locations in a high-dimensional space and if we subject these word vectors to a specific transformation, the Euclidean distance between these locations maps to syntax tree distance. In essence we can recover syntax tree (and the dependency tree too with directional edges) for a sentence by transforming the word vectors using a specific linear transformation and then finding a minimum spanning tree on the distances between the word vectors.", "The mapping between the tree distance in the syntax tree (tree distance between two nodes is the number of edges in the path between them) and Euclidean distance (between nodes in the minimum spanning tree derived from word embeddings) is not linear. The syntax tree distance between two words corresponds to the square of the Euclidean distance between the corresponding nodes in the extracted minimum spanning tree. The second paper offers a reason as to why it is the square of the Euclidean distance as opposed to just the Euclidean distance (the remaining portion of this section is not core to understanding BERT model \u2014 can be skipped).", "It is not possible to isometrically (mapping preserving distances ) map a tree to Euclidean space, because of tree branches.", "For example, The tree distance between nodes A and B on the left is 2 \u2014 d(A,X) + d(X,B)", "Since d(A,B) = d(A,X) + d(X,B), in an isometric mapping to Euclidean space, A, X, and B have to be collinear ( X has to be on the line connecting A and B in order for the above condition to hold)", "Applying the same argument to point A, X, and C, d(A,C) = d(A,X) + d(X,C), A, X, and C have to be collinear too. But this implies B = C, which is a contradiction.", "However there is an isometric mapping from tree to squared distance (Pythagorean embeddings) as illustrated below", "The blog article has more examples. In general they illustrate", "The blog has additional interesting results. For instance, randomly branched trees, if mapped to a sufficiently high dimensional space, where each child is offset from its parent by a random unit Gaussian vector, will be approximately a Pythagorean embedding. A practical implication of this is that context embeddings approximate Pythagorean embeddings of a sentence\u2019s dependency parse tree. From the squared distance property, we have an overall shape of the embedded tree recovered from the vectors.", "The shape of the recovered tree ( influenced by the edge lengths between nodes) is only approximately similar to ideal tree \u2014 the discrepancy has some patterns. The average embedding distance between dependency relations vary widely. It is not known what these differences mean. Perhaps BERTs representations, have additional information beyond just dependency parse trees.", "As an aside it would be interesting to see kind of anomalous edges that are close (part/of same/as in figure above) if we do the same exercise on gene/protein sequences to examine if there is information contained in them \u2014 not necessarily corroborating actual 3-D geometric configurations.", "Syntax tree is also captured in attention matrix for a sentence", "The second paper also shows attention matrix for a sentence captures syntax tree. Consider the sentence \u201cthe old fox saw the new dog\u201d. Consider the word pair \u201cold, fox\u201d. We can construct a vector by picking up the scalar value for this pair from each attention head from across all layers (12 layers for BERT base with 12 attention heads in each layer) If we train linear classifiers that take as input such model-wide attention vector for each word pair, and classify if there is a relation between the two words, and the type of relationship, these classifiers perform reasonably well (even if not state of art results) suggesting that syntactic information is encoded in attention matrix for a sentence.", "By simply visualizing the embedding for words like die in different sentence contexts, we can see how word sense affects embeddings.", "In general, the word embeddings show", "The second paper claims word senses are captured in a lower dimensional space, though it is not clear how that is. Given this observation, it appears a vector encodes both syntactic and semantic information in complementary subspaces", "The Github repository for the first paper has code to recover syntax trees from sentences as described above", "For example, for the input sentence", "The duckling that fell through a drain was rescued", "The recovered parse tree from the word embeddings for the sentence above as well as a dependency parser output for the same sentence is shown below", "Deconstruction of models along the lines above in future", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F76a7e828c0f1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7----76a7e828c0f1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76a7e828c0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76a7e828c0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://nlp.stanford.edu//~johnhew//structural-probe.html?utm_source=quora&utm_medium=referral#the-structural-probe", "anchor_text": "blog"}, {"url": "https://nlp.stanford.edu/pubs/hewitt2019structural.pdf", "anchor_text": "a structural probe for finding syntax in word representations"}, {"url": "https://nlp.stanford.edu/pubs/hewitt2019structural.pdf", "anchor_text": "1"}, {"url": "https://nlp.stanford.edu//~johnhew//structural-probe.html?utm_source=quora&utm_medium=referral#the-structural-probe", "anchor_text": "blog"}, {"url": "https://arxiv.org/pdf/1906.02715.pdf", "anchor_text": "2"}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "blog"}, {"url": "http://Recovered Parse tree from BER vectors", "anchor_text": "syntax trees"}, {"url": "https://nlp.stanford.edu/pubs/hewitt2019structural.pdf", "anchor_text": "1"}, {"url": "https://nlp.stanford.edu//~johnhew//structural-probe.html?utm_source=quora&utm_medium=referral#the-structural-probe", "anchor_text": "blog"}, {"url": "https://arxiv.org/pdf/1906.02715.pdf", "anchor_text": "2"}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "blog"}, {"url": "https://nlp.stanford.edu/pubs/hewitt2019structural.pdf", "anchor_text": "2"}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "blog"}, {"url": "https://nlp.stanford.edu//~johnhew//structural-probe.html?utm_source=quora&utm_medium=referral#the-structural-probe", "anchor_text": "recent discovery"}, {"url": "https://en.wikipedia.org/wiki/Minimum_spanning_tree", "anchor_text": "minimum spanning tree"}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "blog on second paper"}, {"url": "https://www.encyclopediaofmath.org/index.php/Isometric_mapping", "anchor_text": "isometrically"}, {"url": "https://pair-code.github.io/interpretability/bert-tree/", "anchor_text": "Figure from blog on second paper"}, {"url": "https://arxiv.org/pdf/1906.02715.pdf", "anchor_text": "from second paper"}, {"url": "https://arxiv.org/pdf/1906.02715.pdf", "anchor_text": "from second paper"}, {"url": "https://arxiv.org/pdf/1906.02715.pdf", "anchor_text": "from second pape"}, {"url": "https://arxiv.org/pdf/1906.02715.pdf", "anchor_text": "second paper"}, {"url": "https://github.com/john-hewitt/structural-probes/", "anchor_text": "Github repository"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----76a7e828c0f1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----76a7e828c0f1---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----76a7e828c0f1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/linguistics?source=post_page-----76a7e828c0f1---------------linguistics-----------------", "anchor_text": "Linguistics"}, {"url": "https://medium.com/tag/ai?source=post_page-----76a7e828c0f1---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76a7e828c0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----76a7e828c0f1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76a7e828c0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----76a7e828c0f1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76a7e828c0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F76a7e828c0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----76a7e828c0f1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----76a7e828c0f1--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://ajitrajasekharan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "779 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F974aed893170&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1&newsletterV3=fd04a90b4be7&newsletterV3Id=974aed893170&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}