{"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "time": 1683009011.376621, "path": "towardsdatascience.com/the-bellman-equation-59258a0d3fa7/", "webpage": {"metadata": {"title": "The Bellman Equation. V-function and Q-function Explained | by Jordi TORRES.AI | Towards Data Science", "h1": "The Bellman Equation", "description": "In this post, we will present the so-called Value-based Agents and the Bellman equation."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc", "anchor_text": "the previous post", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman equation", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post", "paragraph_index": 34}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post", "paragraph_index": 48}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post", "paragraph_index": 49}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning, Second Edition", "paragraph_index": 50}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 51}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 51}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 52}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 53}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 56}], "all_paragraphs": ["In the previous post, we have been able to check with the Frozen-Lake Environment example the limitations of the Cross-Entropy Method. Continuing with the style of this series of gradually adding knowledge, in this post, we will present another type of Agent that allows us to solve the previous problem. These are the so-called Value-based Agents that store the value function and base their decisions on it. For this purpose, we will present the Bellman equation, one of the central elements of many Reinforcement Learning algorithms, and required for calculating the value functions in this post.", "Remember that the Agent\u2019s goal is to find a sequence of actions that will maximize the return: the sum of rewards (discounted or undiscounted \u2014 depending on the value of gamma) during an episode or the entire life of the Agent, depending on the task.", "In a continuous task, this is infinity. We can solve this with the help of the discount factor already introduced in post 2, which lies in the [0:1] range. The formula for the discounted Return G at time step t is as follows:", "Although the sum is still infinite, if \u03b3<1, then Gt will have a finite value. If \u03b3=0, the Agent is only interested in the immediate reward and discards the long-term return. Conversely, if \u03b3=1, the Agent will consider all future rewards equal to the immediate reward.", "We can rewrite this equation with a recursive relationship:", "In short, the Agent must be able to exploit this information that we have been able to express with this Return G to make their decisions. We also refer to this expression as a Discounted Return.", "Almost all Reinforcement Learning algorithms executed by the Agents involve estimating value functions \u2014 functions of states or of state-action pairs. These are the so-called Value-based Agents.", "A value function estimates how good it is for the Agent to be in a given state (or how good it is to perform a given action in a given state) in terms of return G. Note that the return G of an Agent may depend on the actions it will take. Accordingly, value functions introduced in post 2 are defined with respect to particular ways of acting, called policies, and usually denoted by \ud835\udf0b.", "The first value function we will introduce is V-function. Generally speaking, we can say that V-function answers the basic question of \u201cWhat to expect from here?\u201d.", "More formally, the V-function, also referred to as the state-value function, or even the value function, or simply V, measures the goodness of each state. In other words, how good or bad it is to be in a particular state according to the return G when following a policy \ud835\udf0b.", "That is, we can define the V-function as an expected total reward (discounted or undiscounted \u2014 depending on the value of gamma) that is obtainable from the state. In a formal way, the value of V\ud835\udf0b(\ud835\udc60) is:", "that describes the expected value of the total return G, at time step t starting from the state s at time t and then following policy \ud835\udf0b. It is used expectation \ud835\udd3c[.] in this definition because the Environment transition function might act in a stochastic way.", "To clarify the concept a little more, for beginners, let\u2019s consider a straightforward Environment with three states:", "We can represent this with the following Environment\u2019s states transition with rewards:", "The Environment is always deterministic \u2014 every action succeeds, and we always start from state 0. Once we reach either state 1 or state 2, the episode ends.", "Now, the question is, what is the value of state 0? (indicated with V(0)). An important detail is that the value of one state is always calculated (dependent) in terms of some policy that our Agent follows. Even in a simple Environment, our Agent can have different behaviors, each of which will have its own value for state 0. Consider some example of policy:", "The value of state 0, V(0), in each of the above policies is:", "Due to the goal of the Agent is to get as much total reward as possible, the optimal policy for this Agent in this simple one-step Environment is policy 2, the policy \u201calways right\u201d.", "But the preceding example may give a false impression that we should \u201cbeing greedy\u201d and always take action with the highest reward. Unfortunately, it\u2019s not that simple. For example, let\u2019s extend our preceding Environment with yet another state that is reachable from state 2. State 2 is no longer a terminal state but a transition to state 3, with a (very) bad reward of \u201310:", "The new Environment\u2019s states transition with rewards can be represented as:", "With that addition, for each policy, the V(0) will be:", "So, the best policy for this new Environment is now policy 1: \u201calways go left\u201d. Notice that once the Agent has chosen the \u201cright\u201d action in state 0, the bad reward is unavoidable, as from state 2, we have only one exit.", "This example based on a na\u00efve Environment pursues that the reader realises the complexity of this optimality problem and prepares him or her to see the importance of the Bellman equation. What is the Bellman equation?", "In post 2 we extended the definition of state-value function to state-action pairs, defining a value for each state-action pair, which is called the action-value function, also known as Q-function or simply Q. It defines the value of taking action a in state s under a policy \u03c0, denoted by Q\ud835\udf0b(\ud835\udc60,\ud835\udc4e), as the expected Return G starting from s, taking the action \ud835\udc4e, and thereafter following policy \u03c0:", "In this equation again it is used expectation \ud835\udd3c[.] because the Environment transition function might act in a stochastic way.", "Now that we have both Q and V defined, let\u2019s formalize their relationship. We denote with \u03c0(\ud835\udc4e|\ud835\udc60) the probability that a policy, \u03c0, selects an action, \ud835\udc4e, given a current state, \ud835\udc60. Note that the sum of probabilities of all outbound actions from s is equal to 1:", "We can assert that the state-value function is equivalent to the sum of the action-value functions of all outgoing (from s) actions a, multiplied by the policy probability of selecting each action:", "The Bellman equation shows up everywhere in the Reinforcement Learning literature, being one of the central elements of many Reinforcement Learning algorithms. In summary, we can say that the Bellman equation decomposes the value function into two parts, the immediate reward plus the discounted future values.", "This equation simplifies the computation of the value function, such that rather than summing over multiple time steps, we can find the optimal solution of a complex problem by breaking it down into simpler, recursive subproblems and finding their optimal solutions.", "To facilitate understanding of the formulation in the following sections, the next two backup diagrams summarize a convention of names given to the variables and their relationship:", "In these diagrams, P means the probability of action a, issued in state s, ending up in state s\u2019 (with reward r).", "We already saw that we could define the discounted return, G, in recursive terms. Let\u2019s now see how to recursively the Bellman equation is defined for the state-value function:", "This equation tells us how to find the value of a state s following a policy \ud835\udf0b. We can intuitively see that it recursively breaks down the value computation into an immediate expected reward from the next state, r(s,a), plus the value of a successor state, V\ud835\udf0b(s\u2019), with a discount factor \ud835\udefe. The above equation also expresses the stochasticity of the Environment with the sum over the policy probabilities.", "The Bellman equation is important because it gives us the ability to describe the value of a state s, V\ud835\udf0b(s), with the value of the s\u2019 state, V\ud835\udf0b(s\u2019), and with an iterative approach that we will present in the next post, we can calculate the values of all states.", "Unfortunately, in most scenarios, we do not know the probability P and the reward r, so we cannot solve MDPs by directly applying the Bellman equation. But do not worry, in the next post we present some alternatives to find them from experience.", "We also have the Bellman equation for the action-value function:", "In the same way as the state-value function, this equation tells us how to find recursively the value of a state-action pair following a policy \ud835\udf0b.", "And due we shown that the state-value function V(s\u2019) is equivalent to the sum of the action-value functions Q(s\u2019,a\u2019) of all outgoing actions a\u2019, multiplied by the policy probability of selecting each action, \ud835\udf0b(a\u2019|s\u2019), the previous formula can be expressed as follows:", "Remember that the goal of the Agent is to maximize the total cumulative reward in the long run. The policy, which maximizes the total cumulative reward is called the optimal policy.", "A policy \u03c0\u2032 is defined to be better than or equal to a policy \u03c0 if and only if V\u03c0\u2032 (s)\u2265V\u03c0\u200b(s) for all states s. An optimal policy \u03c0\u2217\u200b satisfies \u03c0\u2217\u200b\u2265\u03c0 for all policies \u03c0. An optimal policy is guaranteed to exist but may not be unique. That means that there could be different optimal policies, but they all share the same value functions, the \u201coptimal\u201d value functions.", "The optimal value function is one which yields maximum value compared to all other value function (following using other policies).", "When we say we are solving an MDP it actually means we are finding the optimal value function. So, mathematically optimal state-value function can be expressed as :", "In the above formula, v\u2217(s) tells us what is the maximum reward for state s we can get from the system.", "Similarly, optimal state-action value function indicates the maximum reward we are going to get if we are in state s and taking action a from there on-wards:", "We also can define V(s) via Q(s,a) so the value of some state equals the value of the maximum action we can execute from this state:", "Bellman proved that the optimal state value function in a state s is equal to the action a, which gives us the maximum possible expected immediate reward, plus the discounted long-term reward for the next state s\u2019:", "Bellman also proved that the optimal state-action value function in state s and taking action a is:", "In the future posts of this series, we will show examples of how to use the Bellman equation for optimality. As we will see along with this series, the Bellman equation is a keystone to find the optimal values of the value functions to obtain an optimal policy for an Agent.", "In future posts, you will see these formulas in practice by solving the Frozen-Lake Environment. However, to be able to do this, we have one important thing still missing: a general way to calculate those V-values and Q-values. In the next post, we will present the Value Iteration method for it.", "See you in the next post!.", "For more detail of the content of this post, the reader can review the excellent book Reinforcement Learning, Second Edition, by Richard S. Sutton and Andrew G. Barto.", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F59258a0d3fa7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----59258a0d3fa7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 08"}, {"url": "https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc", "anchor_text": "the previous post"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci%C3%B3n-de-bellman-7b0ebfac2be1", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci%C3%B3n-de-bellman-7b0ebfac2be1", "anchor_text": "3. Funciones de valor y la ecuaci\u00f3n de BellmanAcceso abierto al cap\u00edtulo 3 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2"}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman equation"}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post"}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post"}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post"}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "the next post"}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning, Second Edition"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----59258a0d3fa7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----59258a0d3fa7---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----59258a0d3fa7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----59258a0d3fa7---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----59258a0d3fa7---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----59258a0d3fa7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----59258a0d3fa7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----59258a0d3fa7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----59258a0d3fa7--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}