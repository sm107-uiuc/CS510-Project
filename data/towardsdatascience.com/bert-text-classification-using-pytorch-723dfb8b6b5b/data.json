{"url": "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b", "time": 1683009072.4841669, "path": "towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b/", "webpage": {"metadata": {"title": "BERT Text Classification Using Pytorch | by Raymond Cheng | Towards Data Science", "h1": "BERT Text Classification Using Pytorch", "description": "Text classification is a common task in Natural Language Processing (NLP). We apply BERT, a popular Transformer model, on fake news detection using Pytorch."}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Transformer", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0", "anchor_text": "LSTM for the same task in a later tutorial", "paragraph_index": 1}, {"url": "https://colab.research.google.com/drive/1P4Hq0btDUDOTGkCHGzZbAx1lb0bTzMMa?usp=sharing", "anchor_text": "this Google Colab Notebook", "paragraph_index": 2}, {"url": "https://colab.research.google.com/drive/1xqkvuNDg0Opk-aZpicTVPNY4wUdC7Y7v?usp=sharing", "anchor_text": "this Google Colab Notebook", "paragraph_index": 3}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Huggingface", "paragraph_index": 4}, {"url": "https://pytorch.org/", "anchor_text": "Pytorch", "paragraph_index": 4}, {"url": "https://www.kaggle.com/nopdev/real-and-fake-news-dataset", "anchor_text": "the REAL and FAKE News Dataset", "paragraph_index": 5}, {"url": "https://pytorch.org/text/", "anchor_text": "TorchText", "paragraph_index": 8}, {"url": "https://huggingface.co/transformers/pretrained_models.html", "anchor_text": "Huggingface\u2019s documentation", "paragraph_index": 10}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 21}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/", "paragraph_index": 23}], "all_paragraphs": ["Text classification is one of the most common tasks in NLP. It is applied in a wide variety of applications, including sentiment analysis, spam filtering, news categorization, etc. Here, we show you how you can detect fake news (classifying an article as REAL or FAKE) using the state-of-the-art models, a tutorial that can be extended to really any text classification task.", "The Transformer is the basic building block of most current state-of-the-art architectures of NLP. Its primary advantage is its multi-head attention mechanisms which allow for an increase in performance and significantly more parallelization than previous competing models such as recurrent neural networks. In this tutorial, we will use pre-trained BERT, one of the most popular transformer models, and fine-tune it on fake news detection. I have also used an LSTM for the same task in a later tutorial, please check it out if interested!", "The main source code of this article is available in this Google Colab Notebook.", "The preprocessing code is also available in this Google Colab Notebook.", "Huggingface is the most well-known library for implementing state-of-the-art transformers in Python. It offers clear documentation and tutorials on implementing dozens of different transformers for a wide variety of different tasks. We will be using Pytorch so make sure Pytorch is installed. After ensuring relevant libraries are installed, you can install the transformers library by:", "For the dataset, we will be using the REAL and FAKE News Dataset from Kaggle.", "The most important library to note here is that we imported BERTokenizer and BERTSequenceClassification to construct the tokenizer and model later on.", "In the original dataset, we added an additional TitleText column which is the concatenation of title and text. We want to test whether an article is fake using both the title and the text.", "For the tokenizer, we use the \u201cbert-base-uncased\u201d version of BertTokenizer. Using TorchText, we first create the Text Field and the Label Field. The Text Field will be used for containing the news articles and the Label is the true target. We limit each article to the first 128 tokens for BERT input. Then, we create a TabularDataset from our dataset csv files using the two Fields to produce the train, validation, and test sets. Then we create Iterators to prepare them in batches.", "Note: In order to use BERT tokenizer with TorchText, we have to set use_vocab=False and tokenize=tokenizer.encode. This will let TorchText know that we will not be building our own vocabulary using our dataset from scratch, but instead, use the pre-trained BERT tokenizer and its corresponding word-to-index mapping.", "We are using the \u201cbert-base-uncased\u201d version of BERT, which is the smaller model trained on lower-cased English text (with 12-layer, 768-hidden, 12-heads, 110M parameters). Check out Huggingface\u2019s documentation for other versions of BERT or other transformer models.", "We write save and load functions for model checkpoints and training metrics, respectively. Note that the save function for model checkpoint does not save the optimizer. We do not save the optimizer because the optimizer normally takes very large storage space and we assume no training from a previous checkpoint is needed. The training metric stores the training loss, validation loss, and global steps so that visualizations regarding the training process can be made later.", "We use Adam optimizer and a suitable learning rate to tune BERT for 5 epochs.", "We use BinaryCrossEntropy as the loss function since fake news detection is a two-class problem. Make sure the output is passed through Sigmoid before calculating the loss between the target and itself.", "During training, we evaluate our model parameters against the validation set. We save the model each time the validation loss decreases so that we end up with the model with the lowest validation loss, which can be considered as the best model. Here are the outputs during training:", "After training, we can plot a diagram using the code below:", "For evaluation, we predict the articles using our trained model and evaluate it against the true label. We print out classification report which includes test accuracy, precision, recall, F1-score. We also print out the confusion matrix to see how much data our model predicts correctly and incorrectly for each class.", "After evaluating our model, we find that our model achieves an impressive accuracy of 96.99%!", "We find that fine-tuning BERT performs extremely well on our dataset and is really simple to implement thanks to the open-source Huggingface Transformers library. This can be extended to any text classification dataset without any hassle.", "Here are other articles I wrote, if interested \ud83d\ude0a:", "[1] A. Vaswani, N. Shazeer, N. Parmar, etc., Attention Is All You Need (2017), 31st Conference on Neural Information Processing Systems", "[2] J. Devlin, M. Chang, K. Lee and K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019), 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Master\u2019s Student at Carnegie Mellon, Top Writer in AI, Top 1000 Writer, Blogging on ML | Data Science | NLP. Linkedin: https://www.linkedin.com/in/itsuncheng/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F723dfb8b6b5b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://itsuncheng.medium.com/?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840----723dfb8b6b5b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F723dfb8b6b5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F723dfb8b6b5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@clemhlrdt?utm_source=medium&utm_medium=referral", "anchor_text": "Cl\u00e9ment H"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Transformer"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0", "anchor_text": "LSTM for the same task in a later tutorial"}, {"url": "https://colab.research.google.com/drive/1P4Hq0btDUDOTGkCHGzZbAx1lb0bTzMMa?usp=sharing", "anchor_text": "this Google Colab Notebook"}, {"url": "https://colab.research.google.com/drive/1xqkvuNDg0Opk-aZpicTVPNY4wUdC7Y7v?usp=sharing", "anchor_text": "this Google Colab Notebook"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Huggingface"}, {"url": "https://pytorch.org/", "anchor_text": "Pytorch"}, {"url": "https://www.kaggle.com/nopdev/real-and-fake-news-dataset", "anchor_text": "the REAL and FAKE News Dataset"}, {"url": "https://pytorch.org/text/", "anchor_text": "TorchText"}, {"url": "https://huggingface.co/transformers/pretrained_models.html", "anchor_text": "Huggingface\u2019s documentation"}, {"url": "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0", "anchor_text": "LSTM Text Classification Using PytorchA step-by-step guide teaching you how to build a bidirectional LSTM in Pytorch!towardsdatascience.com"}, {"url": "https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7", "anchor_text": "Fine-tuning GPT2 for Text Generation Using PytorchFine-tune GPT2 for text generation using Pytorch and Huggingface. We train on the CMU Book Summary Dataset to generate\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/controlling-text-generation-from-language-models-6334935e80cf", "anchor_text": "Controlling Text Generation for Language ModelsHands-on approach to control style and content of machine-generated texttowardsdatascience.com"}, {"url": "https://medium.com/@itsuncheng/best-free-resources-that-computer-science-students-should-definitely-know-d148c51b956e", "anchor_text": "Best free resources that Computer Science students should definitely knowOne of the most important things to learn something effectively is to have the right resources and it is not an\u2026medium.com"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----723dfb8b6b5b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----723dfb8b6b5b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----723dfb8b6b5b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----723dfb8b6b5b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----723dfb8b6b5b---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F723dfb8b6b5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&user=Raymond+Cheng&userId=4c697cd55840&source=-----723dfb8b6b5b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F723dfb8b6b5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&user=Raymond+Cheng&userId=4c697cd55840&source=-----723dfb8b6b5b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F723dfb8b6b5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F723dfb8b6b5b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----723dfb8b6b5b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----723dfb8b6b5b--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://itsuncheng.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "731 Followers"}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa637a6d3749b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-text-classification-using-pytorch-723dfb8b6b5b&newsletterV3=4c697cd55840&newsletterV3Id=a637a6d3749b&user=Raymond+Cheng&userId=4c697cd55840&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}