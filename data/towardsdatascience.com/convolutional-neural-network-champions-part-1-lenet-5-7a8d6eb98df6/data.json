{"url": "https://towardsdatascience.com/convolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6", "time": 1683008403.106111, "path": "towardsdatascience.com/convolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6/", "webpage": {"metadata": {"title": "Convolutional Neural Network Champions \u2014Part 1: LeNet-5 (TensorFlow 2.x) | by Amir Nejad | Towards Data Science", "h1": "Convolutional Neural Network Champions \u2014Part 1: LeNet-5 (TensorFlow 2.x)", "description": "Convolutional Neural Network Architectures, LeNet-5 on MNIST database, #Python #DeepLearning #DataScience #MachineLearning #MNIST"}, "outgoing_paragraph_urls": [{"url": "https://github.com/anejad/Convolutional-Neural-Network-Champions", "anchor_text": "Github", "paragraph_index": 3}, {"url": "https://keras.io/api/preprocessing/image/", "anchor_text": "tf.keras.preprocessing", "paragraph_index": 27}, {"url": "https://github.com/slundberg/shap/blob/master/README.md", "anchor_text": "SHAP", "paragraph_index": 29}, {"url": "http://yann.lecun.com/", "anchor_text": "Yann Andr\u00e9 LeCun", "paragraph_index": 35}, {"url": "https://medium.com/quantjam", "anchor_text": "QuantJam", "paragraph_index": 36}, {"url": "https://github.com/anejad", "anchor_text": "Github", "paragraph_index": 36}, {"url": "https://twitter.com/Dr_Nejad", "anchor_text": "Twitter", "paragraph_index": 36}, {"url": "https://www.linkedin.com/in/amir-nejad-phd-8690a44b/", "anchor_text": "LinkedIn", "paragraph_index": 36}], "all_paragraphs": ["Convolutional neural networks are a special type of neural network that is used for modeling data with strong spatial correlations such as images, multivariate time-series, earth science studies (seismic classification and regression) among many other applications. Convolutional networks have gone under significant changes since 1998 and in this series of articles, I aim to reproduce the famous model architecture champions i.e. LeNet, AlexNet, Resnet. My aim is to share my findings and studies with wider audiences and deliver reproducible Python notebooks.", "Part 2: AlexNet classification on ImageNet and Tensorflow:", "Part 3: VGGnet classification on ImageNet and Tensorflow:", "The Python notebook for this study is located on my Github page.", "The Tensorflow version used in this study is 1.15.0.", "The machine used in this study has an Intel core I7 @ 1.8 GHZ with 16 GB memory.", "Applying convolutional layers (aka \u201cConvNet\u201d) on images and extracting key features of the image for analysis is the main premise of ConvNets. Each time \u201cConv-layers\u201d is applied to an image and divides this image into small slices known as receptive fields, hence extracting important features from the image and neglecting less important features. The kernel convolves with the images using a specific set of weights by multiplying its elements with the corresponding elements of the receptive field. It is common to use \u201cpooling layers\u201d in conjunction with Conv-layers to downsample the convolved features and to reduce the sensitivity of the models to the locations of the features in the input.", "Finally, adding dense blocks to the model and formulating the problem (classification and/or regression), one can train such models using the typical gradient descent algorithms such as SGD, ADAM, and RMSprop.", "Of course, prior to 1998, the use of convolutional neural networks were limited and typically support vector machine was the preferred method of choice in the field of image classification. However, this narrative changed when LeChun et al. [98] published their work on the use of gradient-based learning for handwritten digits recognition.", "LeNet models are developed based on MNIST data. This data-set consists of the hand written digits 0\u20139; sixty thousand images is used for training/validation of the model and then a thousand images are used to test the model. The images in this data-set have a size of 28\u00d728 pixels. An example can be seen in the following figure. The challenge of using a MNIST data-set is that the digits often have slight changes in shape and appearance (for example, the number 7 is written different way).", "Looking at the labels in the MNIST data set, we can see the number of labels are balanced, meaning there is not too much disparity.", "The proposed model structure of LeNet-5 has 7 layers, excluding input layers. As described in the Data section, images used in this model are MNIST handwritten images. The proposed structure can be seen in the image above, taken from the LeChun et al. [98] paper. The details of each layer are as follows:", "The following code snippet demonstrates how to build a LeNet model in Python using Tensorflow/Keras library. Keras sequential model is a linear stack of layers. Then, we need to define each layer as seen below. Finally, the model needs to be compiled and the choices of optimizer, loss function, and metrics need to be explicitly defined. The optimizer used in this work is sgd or Stochastic Gradient Descent. The loss function is optimized to train the machine learning model. The loss function used here is cross-entropy, or log loss, and measures the performance of a classification model whose output is a probability value between 0 and 1. An accuracy metric is used to evaluate the performance of training. Loss function is a continuous probability function while accuracy is a discrete function of the number of correctly predicted labels divided by total number of predictions (refer to appendix 3).", "Note that in the above code snippet, we have not specified anything about how the weight of neural network is initialized. By default, Keras uses aglorot_uniforminitializer. Weight values are chosen randomly in a way to make sure that information passed through the network can be processed and extracted. If the weight is too small, the information shrinks as a result. If the weight is too large, the information grows as a result and becomes too big to process. The Glorot uniform algorithm (also known as Xavier algorithm) chooses appropriate random weight values from a multivariate random normal scaled by the size of the neural network [refer Glorot 2010].", "The summary of LeNet-5 network constructed with Tensorflow is given below (Using model.summary()) :", "Now that we have constructed the LeNet model using Tensorflow and Keras, we need to train the model. Using model.fit() and feeding training and validation sets, the model is trained. Additional parameters needed for training are the number of epochs, batch size and verbose:", "Once the model is trained, we can use a testing set that we have set aside to evaluate the performance of model training using model.evaluate() command:", "The result of training the model for 10 epochs can be seen in the following figure. Initially, the weights of neural network are chosen randomly, but after 2 epochs of presenting 48,000 pictures to the model, the model loss reduced from 0.38 to 0.1. After 10 epochs of model training, the model accuracy surpassed 95% on testing set. This is a substantially improved accuracy compared to previous models at the time (mainly support vector machines), and as a result LeNet-5 cemented its legacy as one of the earliest champions of computer vision.", "Using model.optimizer.get_config() we can interrogate the optimizer parameters. Note that we only specified the type of optimizer, the loss function, and accuracy metrics. As it can be seen from the following snippet, the optimizer used to train LeNet model is a Stochastic Gradient Descent (SGD) optimizer. The learning rate by default is set at 0.01. The learning rate controls the change in the model parameters in response to observed error measured by loss function. Imagine the model training process as traversing from a hill to valley, and the learning rate defines step sizes. The larger step size traverses to the solution faster but it might result in jumping over the solution. On the other hand, smaller step sizes take too long to converge. In the more complex problems, step size decay is commonly used. In this problem, however, no decay is sufficient to get good results. Another parameter using SGD is momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. Momentum, therefore, can be used to improve converge the speed of the SGD. Another parameter is nesterov. If set to true (boolean), SGD enables the Nesterov accelerated gradient (NAG) algorithm. NAG is also a closely related algorithm to the momentum in which step sizes are modified using velocity of the learning rate change (refer to Nesterov [1983]).", "There are multiple ways to assess the performance of a classifier. Accuracy performance on the test data set that held back from any of the model development tasks is the obvious choice. However, the confusion matrix can provide a detailed report of classifier and better assess the classification performance. Furthermore, the Classification accuracy can be misleading if an unequal number of observations in each class are present.", "A confusion matrix is a detailed report of the number of samples classified correctly/incorrectly. The number of samples along the diagonal of the confusion matrix are correctly predicted samples. All other samples are miss-classified. The higher the number of samples on the diagonal, the higher the model accuracy. As it can be seen from the confusion matrix of LeNet-5 on MNIST data set, most of the classes are classified correctly. However, there are a few cases the classifier had trouble correctly classifying the label such as label 5,4, and 8. For example, there were 16 cases the classifier incorrectly classified number 2 as number 7. Those aforementioned cases are depicted in the following image.", "In the previous section, it is mentioned the SGD optimizer is used for this optimized neural network model. However, because of the slow convergence of SGD and problems of getting stuck at local minima, this method is not popular. Since its introduction, Adaptive Moment Estimation aka Adam (refer to Kingma et al. [2014] for more details) enjoys significant popularity in the field of deep learning. The Adam optimization algorithm is an extension to stochastic gradient descent in which momentum by default is applied to gradient calculation, and separate learning rates for each parameter. Using Adam optimizer and retraining the LeNet-5 from scratch, model accuracy can be increased to 98% as seen in the following learning curves:", "The batch size is one of the most important hyper-parameters in neural network training. As discussed in the previous section, the neural network optimizer during each training epoch randomly selects data and feeds it to the optimizer. The size of the selected data is called the batch size. Setting the batch size to the entire size of the training data may cause the model to be unable to generalize well on data it hasn\u2019t seen before (refer to Takase et al. [2018]). On the other hand, setting batch size to 1 results in higher computational training time. The proper choice of the batch size is particularity important as it leads to model stability and increases in accuracy. The following two bar charts demonstrate testing accuracy and training time of various batch sizes from 4 to 2,048. Testing accuracy of the model for batch sizes 4 to 512 is above 98%. However, the training time of batch size 4 is more than four times the training time of batch size 512. This effect can be more severe on more complex problems with a large number of classes, and large numbers of training samples.", "As discussed before, the pooling layer is required to down sample the detection of features in feature maps. There are two most commonly used pooling operators: average pooling and max pooling layers. Average pooling layer operates by calculating average values of the selected patch in the feature map, whereas max pooling layer calculates maximum value of the feature map.", "Max pooling operation as it can bee seen in the following figure, works by selecting the maximum feature value from the feature map. Max pooling layers discriminate against features with less dominant activation functions and only select the highest values. This way only the most important features are fed through pooling layer. The major drawback of max pooling is that the pooling operator in the regions with features of high magnitude, only the highest value feature is elected and the rest of features are ignored; the discerning features disappeared after performing max pooling operations and results in loss of information (the purple region in the following figure).", "Average pooling, on the other hand, works by computing average values of the features in the selected region of the feature map. All parts of the selected region in the feature map are fed through using average pooling. If the magnitude of all the activations is low, the computed mean would also be low and rise due to reduced contrast. The situation will be worst when the most of the activations in the pooling area come with a zero value. In that case, feature map characteristics would reduce by a large amount.", "As indicated earlier, the original LeNet-5 model uses average pooling strategy. Changing average pooling to the max pooling strategy resulted in approximately the same testing accuracy on the MNIST data set. One can argue the point of different pooling layers. However it should be noted that the MNIST data set is rather simple compared to other complex data sets such as CIFAR-10 or Imagenet, hence the performance benefits of max pooling in such data sets can be by far more beneficial.", "Thus far we have explored different aspects of LeNet-5 model including the choice of optimizer, effect of batch size, and choice of pooling layer. LeNet-5 model is designed based on MNIST data. As we have seen so far, the digits are centered in each image. However, more often than not, the location of the digits in the image in real life is shifted, rotated, and sometimes flipped. In the following few sections we will explore the effects of image augmentation and sensitivity of the LeNet-5 model to image flipping, rotation, and shifting. Image augmentation is done with the help of the Tensorflow image processing module, tf.keras.preprocessing.", "In this exercise, images are flipped along the horizontal axis using ImageDataGenerator (horizontal_flip=True). Applying ImageDataGenerator tests image results in the new data set with the images horizontally flipped as seen in the following image. As it can be seen, it is expected the model have low accuracy on the flipped images data set. As seen from the testing accuracy table, the accuracy of the LeNet-5 model dropped from 98% to 70%.", "A closer look at the confusion matrix of the flipped images data set reveals few interesting takeaways. The highest accuracy labels are 0,1,8, and 4. The first three labels are symmetrical (0,1,8) and as a result model has good accuracy of prediction on such classes. But it is interesting that the LeNet-5 model has good classification accuracy on label 4. Another interesting aspect of this test is how the model identifies digits. For example one of the labels that the model suffers from accuracy in the flipped data set is 3. The model almost half the time misclassified it as the number 8. It is very useful to understand how the model identifies each digit in order to build a robust classifier. Packages like SHAP can provide means of understanding input and output mapping of any deep neural network model (look for DeepExplainer module in SHAP library).", "Image rotation is another possible scenario in real life. Digits can be written in an angle with respect to the image boundaries. With the Tensorflow image augmentation module, one can produce randomly rotated images using the following line of code: ImageDataGenerator(rotation_range=angle). The testing result of LeNet-5 model with various randomly rotated images can be seen in the following figure. The more rotation, the worse the prediction of the model. It is interesting to note that model predictions are rather satisfactory for up to 20 degrees of rotation, then the predictions degrade rapidly.", "One final image augmentation effect is shifting digits along the horizontal or vertical axis within the image. This effect can be easily applied to MNIST test dataset using ImageDataGenerator(width_shift_range=shift). Note that, in this section I am demonstrating the result of width_shift generator. The sensitivity of LeNet-5 network to width shift is much higher than image flipping and image rotation. As can be seen from the following figure, the accuracy degrades much faster than other discussed image augmentation processes. Only a 10-degree width shift results in accuracy drop from over 95% to about 48%. This effect might be attributed to the filter size and kernel dimensions of the model.", "As we have seen from all the previous sections, LeNet-5 model achieved a significant milestone in the hand written digit recognition. Due to its superior performance on classification problems, LeNet-5 model used in banks and ATM machines for automatic digit classification in mid-1990s. However, the next frontier for this model was addressing image recognition problem to identify various objects in the image.", "In this final section, we aim to train the LeNet-5 on CIFAR-10 dataset. CIFAR-10 (Canadian Institute For Advanced Research) is an established computer vision data set with 60,000 color images with the size 32\u00d732 containing 10 object classes as it can be seen from the following picture. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. As can be seen from the images below, the complexity of images is much higher than that of MNIST.", "Applying LeNet-5 structure to this data set and training the model for 10 epochs, results in an accuracy of 73%. The testing accuracy of this model is 66%. Considering human level accuracy on this data set is about 94% (according to Ho-Phuoc [2018]), LeNet-5 type structure is not efficient enough to achieve high recognition capability.", "The LeNet-5 provided important framework for training and classifying handwritten digits. As we saw in previous sections, the LeNet-5 structure even though achieved many successful milestones for optical character recognition tasks, it did not perform well in the task of image recognition. However, Yann Andr\u00e9 LeCun\u2019s and many contemporaries paved the way for more sophisticated neural network model structures and optimizing techniques. In the next article, I am going to explore and discuss another convolutional neural network structure champion, ALexNet.", "Thanks for reading! My name is Amir Nejad,PhD. I\u2019m a data scientist and editor of QuantJam , and I love share my ideas and to collaborate with other fellow data scientists. You can connect with me on Github, Twitter, and LinkedIn.", "You can see my other writings at:", "Formula to calculate output dimension of a convolutional neural networks:", "Formula to calculate number of parameters:", "Using the two formulas above, one can calculate the output dimensions and number of parameters of LeNet5 model as follows:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD. Engineer | Data Scientist | Problem Solver | Solution Oriented (twitter: @Dr_Nejad)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a8d6eb98df6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://amirnejad.medium.com/?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": ""}, {"url": "https://amirnejad.medium.com/?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "Amir Nejad"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb0004f07a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&user=Amir+Nejad&userId=fb0004f07a1a&source=post_page-fb0004f07a1a----7a8d6eb98df6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8d6eb98df6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8d6eb98df6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@charlesdeluvio?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Charles Deluvio"}, {"url": "https://unsplash.com/s/photos/fingers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/convolutional-neural-network-champions-part-2-alexnet-tensorflow-2-x-de7e0076f3ff", "anchor_text": "Convolutional Neural Network Champions \u2014 Part 2: AlexNet (TensorFlow 2.x)Part 2 of the multiple-part series on the most popular convolutional neural network (CNN) architectures with\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/convolutional-neural-network-champions-part-3-vggnet-tensorflow-2-x-ddad77492d96", "anchor_text": "Convolutional Neural Network Champions \u2014 Part 3: VGGNet (TensorFlow 2.x)Part 3 of this multiple-part series on the most popular convolutional neural network (CNN) architectures with\u2026towardsdatascience.com"}, {"url": "https://github.com/anejad/Convolutional-Neural-Network-Champions", "anchor_text": "Github"}, {"url": "https://keras.io/api/preprocessing/image/", "anchor_text": "tf.keras.preprocessing"}, {"url": "https://github.com/slundberg/shap/blob/master/README.md", "anchor_text": "SHAP"}, {"url": "http://yann.lecun.com/", "anchor_text": "Yann Andr\u00e9 LeCun"}, {"url": "https://medium.com/quantjam", "anchor_text": "QuantJam"}, {"url": "https://github.com/anejad", "anchor_text": "Github"}, {"url": "https://twitter.com/Dr_Nejad", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/amir-nejad-phd-8690a44b/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/quantjam", "anchor_text": "QuantJamQuantjam is a Medium publishing platform providing ideas with codes in the field of finance, algorithmic trading, and\u2026medium.com"}, {"url": "http://amirnejad.medium.com/", "anchor_text": "Amir Nejad - MediumMultipart series on time series analysis with Python applied to financial datasets A time series is a series of data\u2026amirnejad.medium.com"}, {"url": "https://dataconomy.com/2017/04/history-neural-networks/", "anchor_text": "https://dataconomy.com/2017/04/history-neural-networks/"}, {"url": "https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/", "anchor_text": "https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html", "anchor_text": "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"}, {"url": "https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization", "anchor_text": "https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization"}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "anchor_text": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"}, {"url": "https://deepai.org/machine-learning-glossary-and-terms/confusion-matrix", "anchor_text": "https://deepai.org/machine-learning-glossary-and-terms/confusion-matrix"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "http://yann.lecun.com/exdb/mnist/"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "https://www.cs.toronto.edu/~kriz/cifar.html"}, {"url": "https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/", "anchor_text": "https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/"}, {"url": "https://github.com/slundberg/shap/blob/master/README.md", "anchor_text": "https://github.com/slundberg/shap/blob/master/README.md"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7a8d6eb98df6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----7a8d6eb98df6---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----7a8d6eb98df6---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/python?source=post_page-----7a8d6eb98df6---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7a8d6eb98df6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a8d6eb98df6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&user=Amir+Nejad&userId=fb0004f07a1a&source=-----7a8d6eb98df6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a8d6eb98df6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&user=Amir+Nejad&userId=fb0004f07a1a&source=-----7a8d6eb98df6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8d6eb98df6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7a8d6eb98df6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7a8d6eb98df6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7a8d6eb98df6--------------------------------", "anchor_text": ""}, {"url": "https://amirnejad.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://amirnejad.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Amir Nejad"}, {"url": "https://amirnejad.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "75 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb0004f07a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&user=Amir+Nejad&userId=fb0004f07a1a&source=post_page-fb0004f07a1a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa07399551bab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6&newsletterV3=fb0004f07a1a&newsletterV3Id=a07399551bab&user=Amir+Nejad&userId=fb0004f07a1a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}