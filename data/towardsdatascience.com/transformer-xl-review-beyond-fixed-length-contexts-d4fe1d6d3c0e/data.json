{"url": "https://towardsdatascience.com/transformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e", "time": 1683015062.5981681, "path": "towardsdatascience.com/transformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e/", "webpage": {"metadata": {"title": "Transformer-XL Review: Beyond Fixed-Length Contexts | by Jiajin Li | Towards Data Science", "h1": "Transformer-XL Review: Beyond Fixed-Length Contexts", "description": "Transformer-XL, a new architecture that enables natural language understanding beyond a fixed-length context without disrupting temporal coherence."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["This paper (\u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length Context\u201d) was published in ACL 2019, one of the top NLP conferences, by researchers at Google AI. It proposes Transformer-XL, a new architecture that enables natural language understanding beyond a fixed-length context without disrupting temporal coherence. Its key innovations are a segment-level recurrence mechanism and a novel positional encoding scheme. Unlike the traditional Transformer model, it can capture longer-term dependency and solve the context fragmentation problem, which are the main limitations of the vanilla Transformer. The experiments show that Transformer-XL learns dependency that is much longer than RNNs and vanilla Transformer. Transformer-XL also achieves state-of-the-art results in the evaluation with large benchmark datasets.", "Language modeling is an important topic in natural language processing. People have proposed many unsupervised pre-training methods like BERT and ELMo. However, modeling long-term dependency remains a challenge. Recurrent neural networks (RNNs), especially Long Short-term Memory networks (LSTM) have been a standard solution to modeling long-term dependency. The introduction of gating in LSTMs and the gradient clipping technique improve the ability of modeling long-term dependency, but it is insufficient to address this challenge. Also, it is difficult to optimize RNNs for modeling long-term dependency due to gradient vanishing and explosion.", "Transformers were proposed to solve this issue, which allows direct connections between word pairs and better captures long-term dependency than LSTMs. The author defines the original Transformers and vanilla Transformers. However, Transformers were implemented with a fixed-length context. It splits the input into segments and trains within each segment (Figure 1). Therefore, Transformers fail to capture longer-term dependency beyond the predefined context length. And the fixed-length segments do not respect the sentence boundaries, leading to context fragmentation and thus inefficient optimization and performance loss. During evaluation, it makes one prediction at one position at a time by shifting the input by one position in each step, where segments are processed from scratch. So the evaluation procedure is expensive.", "To address these limitations, the authors proposed Transformer-XL. It reuses hidden states in previous segments to support long-term dependency and resolve context fragmentation. And it employs a relative positional encoding scheme to avoid temporal confusion.", "During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context (Figure 2). In each segment, each hidden layer receives the output of the previous hidden layer and the output of the previous segment. It increases the largest possible dependency by using contextual information from several previous segments. Despite resolving the context fragmentation issue, this segment-level recurrence mechanism improves the evaluation speed because it can advance by an entire long segment and use the representations from the previous segments without recomputation.", "Naively applying recurrence introduces another technical challenge. That is, the positional information is incoherent, and tokens from different segments have the same positional encoding, which is referred to as temporal confusion. To address this challenge, Transformer-XL employs novel relative positional encodings. Positional information bias is encoded in the hidden states, which is different from other approaches that incorporate bias in the initial embedding. The use of fixed embeddings with learnable transformations makes it more intuitive and more generalizable to longer sequences. The relative positional encodings make segment-level recurrence possible so that Transformer-XL can model much longer-term dependency than a vanilla Transformer model.", "The authors apply Transformer-XL on word-level and character-level datasets, including WikiText-103, text8, enwik8, One Billion Word, and Penn Treebank, and compare it with other models.", "On WikiText-103 dataset, Transformer-XL reaches a perplexity of 18.3, in comparison to the previous state-of-the-art (SoTA) result (Baevski & Auli) which reaches a perplexity of 20.5 (Table 1).", "On enwik8 dataset, the 12-layer Transformer-XL achieves 1.06 bits per character (bpc), which is similar to the previous SoTA result by Al-Rfou et al.. The 24-layer Transformer-XL improves the SoTA bpc from 1.06 to 0.99 (Table 2).", "With the same hyper-parameters on enwik8, Transformer-XL reduces the SoTA bpc from 1.13 to 1.08 (Table 3).", "One Billion Word dataset has only short-term dependencies, but Transformer-XL also achieves a new SoTA result, decreasing SoTA perplexity from 23.7 to 21.8 (Table 4).", "On word-level Penn Treebank dataset with only 1 million training tokens, Transformer-XL improves SoTA perplexity from 55.3 to 54.52, when compared with other models without two-step finetuning (Table 5). It indicates that Transformer-XL can generalize well on small datasets.", "The authors propose a new metric, Relative Effective Context Length (RECL), which is defined on a model group, and the gain of a long context is measured by the relative improvement over the best short context model. The parameter r in RECL constrains the comparison on top-r hard examples. As shown in Table 6, Transformer-XL can model 80% to 133% longer dependency than RNN, and 291% to 447% longer dependency than the vanilla Transformer. It shows that both segment-level recurrence and relative positional encoding contribute to the longer RECL of Transformer-XL. The ablation studies on WikiText-103 and One Billion Word dataset also show that Transformer-XL outperforms other models because it can model longer-term dependency with the recurrence and the new encoding.", "Besides, because no recomputation is needed, Transformer-XL is up to 1,874 times faster than a vanilla Transformer during evaluation.", "Transformer-XL obtains new SoTA perplexity or bpc results on multiple datasets. Combining recurrence and relative positional encoding, it can model longer-term dependency than RNNs and vanilla Transformers, and reduce computational cost substantially during evaluation. Transformer-XL could be effective in other fields, such as generating long articles and improving language model pretraining methods like BERT and ALBERT.", "(1) Attention is all you need", "This paper proposes the Transformer, a novel model architecture relying entirely on the attention mechanism to model global dependencies between input and output. The Transformer model allows significantly more parallelization and therefore requires less time to train. The Transformer reaches a new SoTA result on the WMT 2014 English-to-French translation task. The original Transformer is the basis of Transfomer-XL presented in this paper.", "Citation: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008.", "(2) Character-level language modeling with deeper self-attention", "This paper proposes a deep, non-recurrent transformer model for character-level modeling. The transformer self-attention layers with causal attention are used to process fixed-length inputs and predict upcoming characters. Al-Rfou et al. design three auxiliary losses to train deep Transformer networks, which outperforms LSTMs and achieve new SoTA results on text8 and enwik8 datasets. However, it uses fixed-length segments so it fails to capture any longer-term dependency beyond the predefined context length. This limitation motivates the authors to design Transfomer-XL to model long-term dependency.", "Citation: Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3159\u20133166.", "(3) Bert: Pre-training of deep bidirectional transformers for language understanding", "This paper introduces a novel language representation model, Bidirectional Encoder Representations from Transformers (BERT). It is designed to pre-train bidirectional language representations with unlabeled text. Then the pre-trained BERT can be fine-tuned to various tasks with one additional output layer, and achieve SoTA results. In practice, BERT simply chunks long text into fixed-length shorter segments, leading to context fragmentation problems. Transformer-XL resolves the context fragmentation issue, so it can be used to improve BERT and then achieve new SoTA results in different kinds of tasks.", "Citation: Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171\u20134186.", "(4) Self-attention with relative position representations", "This paper presents an approach of incorporating relative position representations or distances between sequence elements in the self-attention mechanism of the Transformer. It is demonstrated that relative positional encodings can improve the translation quality on the WMT 2014 English-to-German dataset, in comparison with absolute position representations. It inspires the authors of Transformer-XL to derive a new form of relative positional encodings. The new relative positional encodings of Transformer-XL resolve the temporal confusion problem and has better generalization empirically.", "Citation: Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 464\u2013468.", "(5) ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "This paper presents two new techniques to reduce the parameters in BERT, contributing to lower costs of memory and training time. It also introduces a self-supervised loss for sentence-order prediction that focuses on modeling inter-sentence coherence. It establishes new SoTA results on different benchmark datasets with fewer parameters than BERT-large. Like BERT, it also splits the long text into fixed-length segments, leading to potential context fragmentation problems. Transformer-XL can be used to solve context fragmentation in ALBERT, and therefore further improve its performance.", "Citation: Zhenzhong Lan, Mingda Chen, Sabastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In International Conference on Learning Representations.", "Machine learning researcher | PhD at UCLA"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd4fe1d6d3c0e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@lijj36?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lijj36?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Jiajin Li"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1176ab5f0cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=post_page-1176ab5f0cdc----d4fe1d6d3c0e---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4fe1d6d3c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=-----d4fe1d6d3c0e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4fe1d6d3c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&source=-----d4fe1d6d3c0e---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.aclweb.org/anthology/P19-1285.pdf", "anchor_text": "https://www.aclweb.org/anthology/P19-1285.pdf"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d4fe1d6d3c0e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d4fe1d6d3c0e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d4fe1d6d3c0e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----d4fe1d6d3c0e---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d4fe1d6d3c0e---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4fe1d6d3c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=-----d4fe1d6d3c0e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4fe1d6d3c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=-----d4fe1d6d3c0e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4fe1d6d3c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@lijj36?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1176ab5f0cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=post_page-1176ab5f0cdc----d4fe1d6d3c0e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1176ab5f0cdc%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=-----d4fe1d6d3c0e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@lijj36?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Written by Jiajin Li"}, {"url": "https://medium.com/@lijj36/followers?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "8 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1176ab5f0cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=post_page-1176ab5f0cdc----d4fe1d6d3c0e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1176ab5f0cdc%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-xl-review-beyond-fixed-length-contexts-d4fe1d6d3c0e&user=Jiajin+Li&userId=1176ab5f0cdc&source=-----d4fe1d6d3c0e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d4fe1d6d3c0e----0---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d4fe1d6d3c0e----0---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d4fe1d6d3c0e----0---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d4fe1d6d3c0e----0---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d4fe1d6d3c0e----0---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d4fe1d6d3c0e----0---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----0-----------------clap_footer----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d4fe1d6d3c0e----0---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----d4fe1d6d3c0e----0-----------------bookmark_preview----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----d4fe1d6d3c0e----1---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----d4fe1d6d3c0e----1---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----d4fe1d6d3c0e----1---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d4fe1d6d3c0e----1---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----d4fe1d6d3c0e----1---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----d4fe1d6d3c0e----1---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----1-----------------clap_footer----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----d4fe1d6d3c0e----1---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----d4fe1d6d3c0e----1-----------------bookmark_preview----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d4fe1d6d3c0e----2---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d4fe1d6d3c0e----2---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d4fe1d6d3c0e----2---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d4fe1d6d3c0e----2---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d4fe1d6d3c0e----2---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d4fe1d6d3c0e----2---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d4fe1d6d3c0e----2---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----d4fe1d6d3c0e----2-----------------bookmark_preview----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----d4fe1d6d3c0e----3---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----d4fe1d6d3c0e----3---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----d4fe1d6d3c0e----3---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d4fe1d6d3c0e----3---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----d4fe1d6d3c0e----3---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "Time-Series Forecasting: Deep Learning vs Statistics \u2014 Who Wins?A comprehensive guide on the ultimate dilemma"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----d4fe1d6d3c0e----3---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": "\u00b714 min read\u00b7Apr 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----c568389d02df----3-----------------clap_footer----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----d4fe1d6d3c0e----3---------------------531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&source=-----d4fe1d6d3c0e----3-----------------bookmark_preview----531f7027_788b_4b35_b5c1_dfc4fe7f3d33-------", "anchor_text": ""}, {"url": "https://medium.com/@lijj36?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "See all from Jiajin Li"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Babar M Bhatti"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Essential Guide to Foundation Models and Large Language ModelsThe term Foundation Model (FM) was coined by Stanford researchers to introduce a new category of ML models. They defined FMs as models\u2026"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "\u00b714 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&user=Babar+M+Bhatti&userId=10dee34829b&source=-----27dab58f7404----0-----------------clap_footer----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&source=-----d4fe1d6d3c0e----0-----------------bookmark_preview----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----1-----------------clap_footer----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----d4fe1d6d3c0e----1-----------------bookmark_preview----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----0-----------------clap_footer----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d4fe1d6d3c0e----0---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----d4fe1d6d3c0e----0-----------------bookmark_preview----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d4fe1d6d3c0e----1---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----d4fe1d6d3c0e----1-----------------bookmark_preview----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----d4fe1d6d3c0e----2---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----d4fe1d6d3c0e----2---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----d4fe1d6d3c0e----2---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----d4fe1d6d3c0e----2---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----d4fe1d6d3c0e----2---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----d4fe1d6d3c0e----2---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----2-----------------clap_footer----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----d4fe1d6d3c0e----2---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----d4fe1d6d3c0e----2-----------------bookmark_preview----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://storiusmag.com/transformers-are-here-gpt-explained-7385db908432?source=read_next_recirc-----d4fe1d6d3c0e----3---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://medium.com/@storiusmag?source=read_next_recirc-----d4fe1d6d3c0e----3---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://medium.com/@storiusmag?source=read_next_recirc-----d4fe1d6d3c0e----3---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Storius Magazine"}, {"url": "https://storiusmag.com/?source=read_next_recirc-----d4fe1d6d3c0e----3---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Storius Magazine"}, {"url": "https://storiusmag.com/transformers-are-here-gpt-explained-7385db908432?source=read_next_recirc-----d4fe1d6d3c0e----3---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "Transformers Are Here: GPT ExplainedGoing beyond buzzwords"}, {"url": "https://storiusmag.com/transformers-are-here-gpt-explained-7385db908432?source=read_next_recirc-----d4fe1d6d3c0e----3---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": "\u00b711 min read\u00b7Jan 22"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fstoriusmag%2F7385db908432&operation=register&redirect=https%3A%2F%2Fstoriusmag.com%2Ftransformers-are-here-gpt-explained-7385db908432&user=Storius+Magazine&userId=52842e393f1e&source=-----7385db908432----3-----------------clap_footer----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://storiusmag.com/transformers-are-here-gpt-explained-7385db908432?source=read_next_recirc-----d4fe1d6d3c0e----3---------------------9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7385db908432&operation=register&redirect=https%3A%2F%2Fstoriusmag.com%2Ftransformers-are-here-gpt-explained-7385db908432&source=-----d4fe1d6d3c0e----3-----------------bookmark_preview----9f9a083a_3e5c_4a72_baf3_bc068ea4a7f5-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----d4fe1d6d3c0e--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}