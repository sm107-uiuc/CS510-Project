{"url": "https://towardsdatascience.com/build-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9", "time": 1683003971.062915, "path": "towardsdatascience.com/build-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9/", "webpage": {"metadata": {"title": "Build Powerful Lightweight Models Using Knowledge Distillation | by Youness Mansar | Towards Data Science", "h1": "Build Powerful Lightweight Models Using Knowledge Distillation", "description": "The trend in the Machine learning world in the last few years was to get the biggest models, train them on huge amounts of data and then ensemble them in order to get the last few percents of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1704.04861.pdf", "anchor_text": "MobileNets", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1609.07061", "anchor_text": "weight quantization", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1710.09412", "anchor_text": "mixup: Beyond Empirical Risk Minimization", "paragraph_index": 1}, {"url": "https://physionet.org/content/mitdb/1.0.0/", "anchor_text": "MIT-BIH dataset", "paragraph_index": 2}, {"url": "https://www.kaggle.com/shayanfazeli/heartbeat", "anchor_text": "https://www.kaggle.com/mondejar/mitbih-database", "paragraph_index": 2}], "all_paragraphs": ["The trend in the Machine learning world in the last few years was to get the biggest models, train them on huge amounts of data and then ensemble them in order to get the last few percents of accuracy. One drawback of this approach is that big models or ensembles can be challenging to deploy in a real life application. Their weights size can be too big or their inference time may be too long for any practical use, especially if you are trying to use them on an embedded device or on the client-side of a web application.There is also active research on how to make models that achieve decent performance while being small and fast, by building custom architectures for mobile like in MobileNets or by weight quantization.", "In this post, we will demonstrate how we can boost the performance of a tiny neural network by using Knowledge Distillation (From Distilling the Knowledge in a Neural Network) and MixUp (From mixup: Beyond Empirical Risk Minimization). The basic idea behind Knowledge Distillation is that you define a Teacher (which can be a single model or an ensemble) and a Student ( Which is the lightweight model you want to use in production), then you train the Teacher on the target task and have the Student try to mimic the Teacher.", "We will use the MIT-BIH dataset that is available in a pre-processed state on Kaggle Datasets : https://www.kaggle.com/mondejar/mitbih-database. This dataset contains individual heartbeats that are classified into five classes related to arrhythmia abnormalities.", "The teacher model is a 1D CNN that has Convolution layers with 64 filters each and two fully connected Layers. It comes to a total of 17,221 trainable parameters.", "The student Model has the same structure as the Teacher but with smaller convolution layers. It comes to a total of 3,909 trainable parameters, so 4x smaller than the teacher model.", "We train the Teacher model using the categorical cross-entropy applied to the one-hot labels. When applying knowledge distillation the Student model is trained using a mix between the Kullback Leibler divergence and the MAE loss on the soft labels predicted by the Teacher model as target.", "The Kullback Leibler divergence measures the difference between two probability distributions, so the objective here is to make the distribution {over the classes} predicted by the Student as close as possible from the Teacher.", "Without using any Knowledge distillation the tiny model achieved an F1 score of 0.67 +- 0.02, after using knowledge distillation the performance of the tiny model was boosted to 0.78 +- 0.02. We were able to gain 11 performance points in term of F1 score using the same architecture when using knowledge distillation.", "In this post, we were able to implement a simple Knowledge Distillation training scheme that was able to boost the performance of a very small model from 0.67 F1 to 0.78 F1 using the exact same architecture. This can be useful when having the smallest model possible with decent performance is important for deployment purposes for example.There is much to be explored when using this approach, like using an ensemble as a teacher or how the size difference between the Teacher and Student influences the quality of the Knowledge distillation. This will be done in a future post \ud83d\ude03.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F618f69b569d9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----618f69b569d9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----618f69b569d9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@CVxTz?source=post_page-----618f69b569d9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@CVxTz?source=post_page-----618f69b569d9--------------------------------", "anchor_text": "Youness Mansar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F830ca2245343&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&user=Youness+Mansar&userId=830ca2245343&source=post_page-830ca2245343----618f69b569d9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F618f69b569d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F618f69b569d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@thutra0803?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Tra Nguyen"}, {"url": "https://unsplash.com/s/photos/teacher?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1704.04861.pdf", "anchor_text": "MobileNets"}, {"url": "https://arxiv.org/abs/1609.07061", "anchor_text": "weight quantization"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://arxiv.org/abs/1710.09412", "anchor_text": "mixup: Beyond Empirical Risk Minimization"}, {"url": "https://physionet.org/content/mitdb/1.0.0/", "anchor_text": "MIT-BIH dataset"}, {"url": "https://www.kaggle.com/shayanfazeli/heartbeat", "anchor_text": "https://www.kaggle.com/mondejar/mitbih-database"}, {"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://github.com/CVxTz/knowledge_distillation", "anchor_text": "https://github.com/CVxTz/knowledge_distillation"}, {"url": "https://medium.com/tag/knowledge-distillation?source=post_page-----618f69b569d9---------------knowledge_distillation-----------------", "anchor_text": "Knowledge Distillation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----618f69b569d9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deployment?source=post_page-----618f69b569d9---------------deployment-----------------", "anchor_text": "Deployment"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----618f69b569d9---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----618f69b569d9---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F618f69b569d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&user=Youness+Mansar&userId=830ca2245343&source=-----618f69b569d9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F618f69b569d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&user=Youness+Mansar&userId=830ca2245343&source=-----618f69b569d9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F618f69b569d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----618f69b569d9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F618f69b569d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----618f69b569d9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----618f69b569d9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----618f69b569d9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----618f69b569d9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----618f69b569d9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----618f69b569d9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----618f69b569d9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----618f69b569d9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----618f69b569d9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@CVxTz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@CVxTz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Youness Mansar"}, {"url": "https://medium.com/@CVxTz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "425 Followers"}, {"url": "https://www.linkedin.com/in/mansar/", "anchor_text": "https://www.linkedin.com/in/mansar/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F830ca2245343&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&user=Youness+Mansar&userId=830ca2245343&source=post_page-830ca2245343--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e379bdcb386&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9&newsletterV3=830ca2245343&newsletterV3Id=7e379bdcb386&user=Youness+Mansar&userId=830ca2245343&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}