{"url": "https://towardsdatascience.com/lines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364", "time": 1683016626.2232268, "path": "towardsdatascience.com/lines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364/", "webpage": {"metadata": {"title": "\u201cLines First, Color Next\u201d An Inspirational Deep Image Inpainting Approach | by Chu-Tak Li | Towards Data Science", "h1": "\u201cLines First, Color Next\u201d An Inspirational Deep Image Inpainting Approach", "description": "Hello \ud83d\udc4b :). Today, we are going to dive into an inspirational deep image inpainting paper named EdgeConnect. Simply speaking, this paper adopts a very straightforward approach to handle image\u2026"}, "outgoing_paragraph_urls": [{"url": "https://lichutak.medium.com/", "anchor_text": "my previous posts", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0", "anchor_text": "coarse-to-fine", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "2", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "last post", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "Partial Convolution", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0", "anchor_text": "Contextual Attention", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "GLCIC", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1706.08500#:~:text=version%2C%20v6)%5D-,GANs%20Trained%20by%20a%20Two%20Time%2DScale%20Update%20Rule,to%20a%20Local%20Nash%20Equilibrium&text=In%20experiments%2C%20TTUR%20improves%20learning,the%20One%20Billion%20Word%20Benchmark.", "anchor_text": "FID", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1706.08500#:~:text=version%2C%20v6)%5D-,GANs%20Trained%20by%20a%20Two%20Time%2DScale%20Update%20Rule,to%20a%20Local%20Nash%20Equilibrium&text=In%20experiments%2C%20TTUR%20improves%20learning,the%20One%20Billion%20Word%20Benchmark.", "anchor_text": "FID", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0", "anchor_text": "Contextual Attention (DeepFillv1)", "paragraph_index": 30}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "Partial Convolution", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning", "paragraph_index": 31}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "Spectral Normalization for Generative Adversarial Networks", "paragraph_index": 32}, {"url": "https://chutakcode.wixsite.com/website", "anchor_text": "https://chutakcode.wixsite.com/website", "paragraph_index": 35}], "all_paragraphs": ["Hello \ud83d\udc4b :). Today, we are going to dive into an inspirational deep image inpainting paper named EdgeConnect. Simply speaking, this paper adopts a very straightforward approach to handle image inpainting, from easy to difficult. They first predict the skeleton (i.e. edges/lines) of the missing regions, then they fill in colors according to the generated skeleton. This is the \u201cLines first, Color next\u201d approach. Figure 1 shows some of the inpainting results and the predicted edge maps using the proposed method. Are they look realistic? Let\u2019s dive deeper into this paper and grasp its core ideas!", "As mentioned in my previous posts, existing deep image inpainting approaches usually produce blurry regions. One of the possible reasons is the employment of the L1 loss (i.e. pixel-wise reconstruction accuracy). Note that blurry results tend to offer smaller L1 loss (minimize the average pixel-wise difference). To further improve the quality of the inpainting results, we can provide prior information of the missing regions to the generator network to generate better local fine texture details. An edge map of the missing region perhaps is a good choice of the prior information as it contains the overall structure of an image.", "Let\u2019s imagine that direct filling in the missing regions in an image is difficult and we can simplify it into two easier steps, namely edge prediction and color filling. According to this paper, the authors assume that edge prediction of the missing regions is much easier than direct completion of the missing regions. So, they employ the \u201clines first, color next\u201d strategy. They first generate the edges of the missing regions as prior information for the second generator. Then, the second generator uses the generated edges to fill in the missing regions with local fine texture details.", "Same as before, I assume that readers have a basic understanding of deep image inpainting. Actually, the network architecture and loss functions used in this paper have been introduced previously. Again, if you need or want, please feel free to have a quick recall of my previous posts. In this section, I will briefly go through those I think are less important and/or interested readers can refer to the paper for more details, hence we can have more time to discuss about the main idea of this paper in the following sections.", "Actually, the main idea of this paper is to employ two generators. One for edge prediction and another for completing an image with the predicted edge map as prior.", "Network architecture. Same as several papers we have covered before, the authors of this paper use a two-stage network (i.e. two generates and two discriminators). Similar to the coarse-to-fine network we have introduced, you may think that we have an edge-to-image two-stage network in this paper.", "Loss functions. Again, most of the loss terms used in this paper have been introduced previously. For the first edge generator, there are two loss terms, namely adversarial loss and feature matching loss. The adversarial loss is the most basic one and the feature matching loss is similar to the VGG perceptual loss. For the second image generator, there are four loss terms, namely style loss, perceptual loss, L1 reconstruction loss, and adversarial loss. For the style and the perceptual losses, you may check it out in my last post [here].", "The authors simplify the task of image inpainting into two easier steps, namely edge prediction and image completion. The first generator is just responsible for predicting the edges of the missing regions so as to obtain the overall image structure. The predicted edge map is also a binary map to show the skeleton of an image. The second generator is conditioned on the predicted edge map and is responsible for filling in the missing regions with better texture details.", "Figure 2 shows the overall network structure of the proposed approach, EdgeConnect. As you can see that, we are familiar with this kind of network architecture for the task of image inpainting. The first generator G_1 takes the mask image, masked edge image and the masked grayscale image as input and gives a predicted edge map. This generator is trained using the standard adversarial loss and the feature matching loss.", "The second generator G_2 takes the predicted edge map and the masked RGB image as input and outputs a completed RGB image. This generator is trained using the style loss, perceptual loss, L1 reconstruction loss and the standard adversarial loss.", "As I mentioned before, the feature matching loss is very similar to the VGG perceptual loss. The feature matching loss computes the L1 distance between the feature maps of the edge discriminator D_1 instead of a pre-trained VGG-19 network. In the equation above, L is the number of layers of the edge discriminator, N_i is the number of elements at the i-th layer of the edge discriminaotr, C_gt and C_pred are the ground truth edge map and the predicted edge map respectively. Similarly, we would like to force the edge generator to generate edge maps with similar feature representations of the ground truth edge maps computed by the edge discriminator. Simply speaking, feature matching loss wants similar feature representations inside the discriminator.", "One training technique used in this first stage is the employment of Spectral Normalization (SN) [2]. The authors claim that the use of SN can further stabilize their training process and they apply SN to all the layers of the edge generator and edge discriminator. Interested readers can refer to [2] for more details about the SN and this paper for more details about the network architecture.", "Here, I briefly mention the perceptual loss to show its similarity to the feature matching loss. \u03a6_i(I) represents the feature maps at the i-th layer of \u03a6 given the input I. In this work, the authors use relu1_1, relu2_1, relu3_1, relu4_1, and relu5_1 of a pre-trained VGG-19 network to calculate the perceptual loss and the style loss. For readers who want to know more about the perceptual loss and the style loss, you may refer to my last post.", "This is the overall loss function to train the image completion network (i.e. the second generator). \u03bb_l1, \u03bb_adv,2, \u03bb_p and \u03bb_s are the weights to balance these four loss terms. In their experiments, these 4 weights are set to 1, 0.1, 0.1, and 250 respectively. Different from the first edge prediction network, SN is only applied to the discriminator for this image completion network to speed up the training process.", "For training the edge generator G_1, the authors employ a very famous conventional edge detection algorithm named Canny edge detector. I will not go through the Canny edge detector here, interested readers may google it for more details. Anyway, for Canny edge detector, we have to input a parameter \u03c3 which would affect the quality of the detected edge image. The authors found that \u03c3=2 gives the best edge image for the task of image inpainting. I will talk about this ablation study later on.", "For the types of the masks, two types of masks are used in this paper. The first type is a regular rectangular missing region with fixed size (25% of the pixels are masked) at a random location in an image. The second type is the irregular mask introduced in the work of Partial Convolution we have introduced last time.", "The authors evaluated their proposed approach on CelebA, Places2, and Paris StreetView datasets. As you can see, these are some commonly used datasets for evaluating deep image inpainting approaches.", "Figure 3 shows some inpainting results using the proposed method, EdgeConnect. Please zoom in for a better view of the quality of the completed images. Figure 4 shows the visual comparison of different inpainting approaches. Note that Figure 4(c) and (d) represent the Contextual Attention and the GLCIC we have covered before. For Ours (Canny), the authors try to provide the ground truth edge information (not available in practice) to the image generator, hence the generator only needs to fill in the local details with full edge information. Again, please zoom in for a better view. You can see that the proposed method, Figure 4(e), offers the best inpainting results. If the ground truth edge information is available, Figure 4(f), even better inpainting results can be produced.", "Table 1 lists out some commonly used objective numerical metrics for the task of image inpainting. Note that for SSIM and PSNR, the higher the better; while for l1(%) and FID, the smaller the better. FID can be regarded as a metric to estimate the visual quality. The last column (Canny) means that the full edge information is available for the second image generator. We can see that in most of the situations, the proposed EdgeConnect (i.e. edge prediction first then image completion) offers the best numerical metrics.", "Figure 5 shows the effect of various mask sizes on PSNR (right) and FID (left) on the Places2 dataset. As you can see, when the mask ratio becomes larger, the task becomes much more difficult.", "The authors also did visual turing tests to evaluate their inpainting results. If you are interested on it, please refer to their paper for details.", "First of all, the authors provide evidence that the use of edge information is useful for the task of image inpainting. Table 2 lists out the related numerical metrics and shows that the use of the edge information is helpful to enhance the inpainting results.", "Then, they have tried different values of the parameter \u03c3 to obtain different edge images and see the effect of using different edge images on the final inpainting results. Figure 6 shows the effect of different \u03c3 on PSNR and FID on three different datasets. As you can observe, \u03c3=2 (or 2.xx) offers the best final inpainting results (best PSNR and FID).", "Figure 7 shows the effect of different \u03c3 on the edge images and the inpainting results. Simply speaking, as you can see, when \u03c3 is too small, we will have too many small edges and this may be too complicated for image completion. On the other hand, when \u03c3 is too large, it is difficult to detect edges and hence the edge maps may not be useful for image completion. Therefore, suitable \u03c3 is crucial for producing good inpainting results.", "Apart from the Canny edge detector, the authors also tried to employ better edge detection algorithm to see any further improvements can be brought. They found that there is no any obvious improvements, hence they chose to use the Canny edge detector. Of course, the choice of the edge detection algorithm has not been fully explored. Again, for readers who want to know more about this experiment, please refer to the paper.", "Figure 8 shows some failure cases in which the edge generator cannot produce satisfactory edge information. As you can see, when the missing region is too large to be filled and the scene is too complex, it is still difficult to complete the image with good visual quality.", "Figure 9 shows a possible extension of the proposed EdgeConnect to an interactive image editing tool. You can see that it is possible to combine the characteristics of two images in the form of image inpainting. Please zoom in for a better view of the results.", "Figure 10 shows some examples in which unwanted object is removed from the original images. Note that edge guidance is optional and you may zoom in for a better view of the results.", "Clearly, the main idea of this paper is to separate the task of image inpainting into two simpler steps, namely edge prediction and image completion based on the predicted edge map. They first predict the edges in the missing regions, then complete the image based on the predicted edge information. By doing this, sharper inpainting results are obtained instead of blurry results. In fact, most of the techniques used in this paper have been introduced in my previous posts. If you have read my previous posts, you may find that it is easy to understand the idea of this paper. For readers who are interested in the idea of this paper, I highly recommend you to read their paper for further details such as the detailed network architecture and the training procedure.", "Actually, we have gone through most of the commonly used technqiues for deep image inpainting. Now, we can handle different types of masks such as regular rectangular masks and irregular masks. Image inpainting has various possible applications such as unwanted object removal and other image editing tasks (combine two images half-half as shown in Figure 9). However, same as before, it is still very difficult to complete images with complicated structures and large mask ratio. Anyway, we still have a long way to go for deep image inpainting. Let\u2019s learn and read more related papers together! :)", "Next time, we will dive into another very popular deep image inpainting paper which can be regarded as an enhanced version of Contextual Attention (DeepFillv1), Partial Convolution, and EdgeConnect. Looking forward to learning together! :)", "[1] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, Mehran Ebrahimi, \u201cEdgeConnect: Generative Image Inpainting with Adversarial Edge Learning,\u201d Proc. International Conference on Computer Vision (ICCV), 2019.", "[2] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida, \u201cSpectral Normalization for Generative Adversarial Networks,\u201d Proc. International Conference on Learning Representations (ICLR), 2018.", "Thanks again for reading my post. If you have any questions, please feel free to send my an email or leave comments here. I am happy to hear from you and any suggestions are welcome. Hope to see you next time! :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "DO IT FIRST. ONLY U CAN DEFINE YOURSELF. I have started my PhD journey accidentally. To know more about me at: https://chutakcode.wixsite.com/website"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb2d980efb364&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b2d980efb364--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b2d980efb364--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://lichutak.medium.com/?source=post_page-----b2d980efb364--------------------------------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=post_page-----b2d980efb364--------------------------------", "anchor_text": "Chu-Tak Li"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff4c07943b642&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&user=Chu-Tak+Li&userId=f4c07943b642&source=post_page-f4c07943b642----b2d980efb364---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2d980efb364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2d980efb364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://lichutak.medium.com/", "anchor_text": "my previous posts"}, {"url": "https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0", "anchor_text": "coarse-to-fine"}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "2"}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "last post"}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "Partial Convolution"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0", "anchor_text": "Contextual Attention"}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "GLCIC"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1706.08500#:~:text=version%2C%20v6)%5D-,GANs%20Trained%20by%20a%20Two%20Time%2DScale%20Update%20Rule,to%20a%20Local%20Nash%20Equilibrium&text=In%20experiments%2C%20TTUR%20improves%20learning,the%20One%20Billion%20Word%20Benchmark.", "anchor_text": "FID"}, {"url": "https://arxiv.org/abs/1706.08500#:~:text=version%2C%20v6)%5D-,GANs%20Trained%20by%20a%20Two%20Time%2DScale%20Update%20Rule,to%20a%20Local%20Nash%20Equilibrium&text=In%20experiments%2C%20TTUR%20improves%20learning,the%20One%20Billion%20Word%20Benchmark.", "anchor_text": "FID"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0", "anchor_text": "Contextual Attention (DeepFillv1)"}, {"url": "https://towardsdatascience.com/pushing-the-limits-of-deep-image-inpainting-using-partial-convolutions-ed5520775ab4", "anchor_text": "Partial Convolution"}, {"url": "https://arxiv.org/pdf/1901.00212.pdf", "anchor_text": "EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning"}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "Spectral Normalization for Generative Adversarial Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b2d980efb364---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/image-processing?source=post_page-----b2d980efb364---------------image_processing-----------------", "anchor_text": "Image Processing"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----b2d980efb364---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/tag/image-inpainting?source=post_page-----b2d980efb364---------------image_inpainting-----------------", "anchor_text": "Image Inpainting"}, {"url": "https://medium.com/tag/generative-adversarial?source=post_page-----b2d980efb364---------------generative_adversarial-----------------", "anchor_text": "Generative Adversarial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2d980efb364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&user=Chu-Tak+Li&userId=f4c07943b642&source=-----b2d980efb364---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2d980efb364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&user=Chu-Tak+Li&userId=f4c07943b642&source=-----b2d980efb364---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2d980efb364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b2d980efb364--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb2d980efb364&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b2d980efb364---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b2d980efb364--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b2d980efb364--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b2d980efb364--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b2d980efb364--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b2d980efb364--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b2d980efb364--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b2d980efb364--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b2d980efb364--------------------------------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chu-Tak Li"}, {"url": "https://lichutak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "122 Followers"}, {"url": "https://chutakcode.wixsite.com/website", "anchor_text": "https://chutakcode.wixsite.com/website"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff4c07943b642&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&user=Chu-Tak+Li&userId=f4c07943b642&source=post_page-f4c07943b642--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd59b7e8b1294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flines-first-color-next-an-inspirational-deep-image-inpainting-approach-b2d980efb364&newsletterV3=f4c07943b642&newsletterV3Id=d59b7e8b1294&user=Chu-Tak+Li&userId=f4c07943b642&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}