{"url": "https://towardsdatascience.com/from-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3", "time": 1683016551.827256, "path": "towardsdatascience.com/from-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3/", "webpage": {"metadata": {"title": "From Decision Trees and Random Forests to Gradient Boosting | by Robby Sneiderman | Towards Data Science", "h1": "From Decision Trees and Random Forests to Gradient Boosting", "description": "Suppose we wish to perform supervised learning on a classification problem to determine if an incoming email is spam or not spam. The spam dataset consists of 4601 emails, each labelled as real (or\u2026"}, "outgoing_paragraph_urls": [{"url": "https://archive.ics.uci.edu/ml/datasets/spambase", "anchor_text": "spam dataset", "paragraph_index": 0}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.info.txt", "anchor_text": "The Elements of Statistical Learning, II edition", "paragraph_index": 1}, {"url": "https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/", "anchor_text": "Gini Impurity", "paragraph_index": 5}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "Hastie, Tibshirani, Friedman (2009). The Elements of Statistical Learning II.", "paragraph_index": 22}], "all_paragraphs": ["Suppose we wish to perform supervised learning on a classification problem to determine if an incoming email is spam or not spam. The spam dataset consists of 4601 emails, each labelled as real (or not spam) (0) or spam (1). The data also contains a large number of predictors (57), each of which is either a character count, or a frequency of occurrence of a certain word or symbol. In this short article, we will briefly cover the main concepts in tree based classification and compare and contrast the most popular methods.", "This dataset and several worked examples are covered in detail in The Elements of Statistical Learning, II edition.", "The spam dataset consists of 4601 observations, with the response, Y, being not spam (0) or spam (1). The data also consists of 57 predictors x. Each predictor reflects the frequency of a certain word or letter in the email or a certain symbol. For example the total number of capital letters, the frequency of the \u2018$\u2019 sign, etc. Take a look at a sample of the original data.", "Notice that the predictors are quite small and there appear to be many zero data points. We use a log transformation on our predictors before fitting models to help deal with this issue. We add an epsilon of 0.1 to our log to ensure we don\u2019t try to take the log of 0.", "We also randomly split our dataset into a training set and a test set. We will use the test set later to evaluate our model performance.", "Decision trees for classification are one of the oldest and most simple machine learning algorithms. They are popular due to the easy visual interpretation. However, in practice, they tend to overfit the training data which leads to poor performance when attempting to make predictions (also known as the high variance problem). R allows us to easily fit decision trees with the rpart function. There are several ways to decide what makes a \u2018good\u2019 or \u2018best\u2019 tree, but typically we use the Gini Impurity. Ideally, a terminal node should contain only one class. The more mixed it is, the less \u2018pure\u2019 it is and the worse the tree will perform on new data.", "The function builds several trees, and at each point we either go left or right depending on our predictors value. We can see that \u2018$\u2019 appears to be the most important feature for determining if an email is spam or not in our training data. Depending on \u2018$\u2019 we either go left, at which point we check for \u2018remove\u2019 or go right at which point we look at \u2018hp\u2019. Regardless of our path, we eventually end up at a terminal node, where our email is classified as either spam (1) or not spam(0). At our root node, we have 100% of our training examples. Depending on the value of log($+0.1) we either go to the left (which happens 76% of the time, or to the right, which happens 24% of the time). The number in the middle represents the \u2018impurity\u2019 of that node. Our root nodes has an impurity of around 40%, because our training data consists of approximately 40% spam emails.", "Once we have fit a model to our training data, we can see how it performs on our test data. This is done by feeding our new examples into the tree and following the path indicated. To evaluate the performance, we examine the confusion matrix. We obtain a test accuracy of 91% which is quite high.", "In decision trees, our algorithm built several trees and chose one that we found to be the \u2018best\u2019. We then used this single tree to produce predictions on new emails. Brieman improved on these simple models greatly by introducing two key concepts. Firstly, Bagging or Bootstrap Aggregation and finally, the Random Forest. These combine many (hundreds or thousands) of trees, where we take random samples of our observations and predictors to form new trees.", "Random Forests greatly reduce the possibility of overfitting (i.e it reduces variance) by averaging over multiple trees, this is sometimes referred to as the wisdom of crowds.", "Hastie described the following in a lecture (linked in sources):", "Essentially, Random Forests utilise bagging to form hundreds of trees. Since bagging by definition excludes certain observations from some our models, we can use trees that don\u2019t contain a certain observation to obtain \u2018out-of-bag error\u2019 estimates. We can also obtain estimates of the most important features by seeing which predictors consistently are important in many trees. Note that bootstrap aggregation in general will result in a decorrelation of predictors. Hence, if we have some predictors that are essentially identical, the random forest should be able to tease these out and only keep one copy.", "We use the randomForest package in R to fit a model. We can also retain the variables that are most included in our trees, which allows us to visualise the \u2018variable importance\u2019. We have greatly improved our accuracy from just under 91% to 94.6%.", "One of the great aspects of Random Forests as they also allow us to retain \u2018relative importance\u2019 of our predictors. Figure 7 summarises which symbols/words are important in determining spam, and those which are not as important.", "Gradient Boosting is an improvement on Random Forests ( if you tune the hyperparameters well). Now, instead of just fitting bootstrap aggregated trees, we take into account how intermediate trees are performing. Using these intermediate trees, we adjust our weights of future trees (we weight certain trees more than others). This can be thought of as repeatedly fitting the residuals of our model. We start with a basic model and see how it preforms at predicting new observations. The residuals are where we did not do as well at predicting, so we fit our next tree to these residuals and so forth to account for where we did poorly (we also don\u2019t use bagging, but instead sample with replacement). Gradient Boosting also tends to create many \u2018shallow\u2019 trees. That is, they contain a root node and only a small number of subsequent splits. Often, they only contain a single split, these are known as \u2018stumps\u2019 or \u2018weak learners\u2019.", "Figure 8 illustrates bagging which was discussed above. Bagging can be viewed as an algorithm that works in parallel. That is, bagging is done by selecting multiple sets with replacement at the same time.", "Bagging and Boosting are not the same. Bagging is done in parallel as seen in Figure 8 and is used to form several sets of observations to build models with. Boosting, actually uses intermediate trees to adjust the weights that are used when computing the final prediction. As seen in Figure 9, boosting is not done in parallel, it updates weights as it progresses.", "We use the R package, \u2018gbm\u2019 to apply gradient boosting to our dataset. Fitting with gradient boosting requires", "The hyperparmaters n.trees, interaction.depth and shrinkage can be tuned using a grid search and cross validation. We set the interaction depth to six, which allows for sixth order interactions.", "We have \u2018boosted\u2019 to an accuracy of over 95% on our test data!", "Tree based classification remains one of the most popular algorithms used today, mainly due to there easy interpretation and ability to deal with a large number of predictors. Decision trees are the most basic of the tree based methods, and can be easily interpreted, but are extremely prone to overfitting on the training data. Fortunately, several improvements have been made over the years to address the weaknesses faced by simple decision trees. Random Forests utilise Bagging (Bootstrap Aggregation) to obtain many trees, and use the wisdom of crowds to obtain a lower variance prediction. Boosted Trees further improve on the previous algorithms by considering how intermediate trees preform, and tweaking trees to perform better on places they were previously doing poorly (and weighting powerful trees more). All of these techniques and methods are easy to utilise in R, Python and about every other coding language. Gradient Boosting in particular remains one of the most popular algorithms used in Machine Learning competitions, such as those hosted on Kaggle.", "I hope this article has provided a broad overview, and a simple example of these fundamental ML techniques. Thanks for reading!", "[4] Hastie, Tibshirani, Friedman (2009). The Elements of Statistical Learning II.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6c7dabc516c3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rob-sneiderman.medium.com/?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": ""}, {"url": "https://rob-sneiderman.medium.com/?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "Robby Sneiderman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd494db13357&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&user=Robby+Sneiderman&userId=cd494db13357&source=post_page-cd494db13357----6c7dabc516c3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c7dabc516c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c7dabc516c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/LPZy4da9aRo", "anchor_text": "Brett Jordan on Unsplash."}, {"url": "https://archive.ics.uci.edu/ml/datasets/spambase", "anchor_text": "spam dataset"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.info.txt", "anchor_text": "The Elements of Statistical Learning, II edition"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "https://web.stanford.edu/~hastie/ElemStatLearn/"}, {"url": "https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/", "anchor_text": "Gini Impurity"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "https://web.stanford.edu/~hastie/ElemStatLearn/"}, {"url": "https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg", "anchor_text": "Creative Commons."}, {"url": "https://commons.wikimedia.org/wiki/File:Ensemble_Boosting.svg", "anchor_text": "Image Citation: Creative Commons."}, {"url": "https://github.com/Robby955/spam", "anchor_text": "https://github.com/Robby955/spam"}, {"url": "https://www.cc.gatech.edu/~hic/CS7616/pdf/lecture5.pdf", "anchor_text": "https://www.cc.gatech.edu/~hic/CS7616/pdf/lecture5.pdf"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "Hastie, Tibshirani, Friedman (2009). The Elements of Statistical Learning II."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6c7dabc516c3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6c7dabc516c3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/classification?source=post_page-----6c7dabc516c3---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----6c7dabc516c3---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----6c7dabc516c3---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c7dabc516c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&user=Robby+Sneiderman&userId=cd494db13357&source=-----6c7dabc516c3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c7dabc516c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&user=Robby+Sneiderman&userId=cd494db13357&source=-----6c7dabc516c3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c7dabc516c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6c7dabc516c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6c7dabc516c3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6c7dabc516c3--------------------------------", "anchor_text": ""}, {"url": "https://rob-sneiderman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rob-sneiderman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Robby Sneiderman"}, {"url": "https://rob-sneiderman.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcd494db13357&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&user=Robby+Sneiderman&userId=cd494db13357&source=post_page-cd494db13357--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2e6ea218c897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-and-random-forests-to-gradient-boosting-6c7dabc516c3&newsletterV3=cd494db13357&newsletterV3Id=2e6ea218c897&user=Robby+Sneiderman&userId=cd494db13357&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}