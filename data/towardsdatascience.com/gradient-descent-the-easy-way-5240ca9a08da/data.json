{"url": "https://towardsdatascience.com/gradient-descent-the-easy-way-5240ca9a08da", "time": 1682994342.818505, "path": "towardsdatascience.com/gradient-descent-the-easy-way-5240ca9a08da/", "webpage": {"metadata": {"title": "Gradient Descent, the Easy Way. A gentle and intuitive way to\u2026 | by Ziad SALLOUM | Towards Data Science", "h1": "Gradient Descent, the Easy Way", "description": "Gradient Descent is at the core of all Deep Learning. This article explains in simple details the technicalities of this method. Update: The best way of learning and practicing Reinforcement Learning\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com", "anchor_text": "http://rl-lab.com", "paragraph_index": 1}], "all_paragraphs": ["Gradient Descent is at the core of all Deep Learning. This article explains in simple details the technicalities of this method.", "Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "Consider a system in which it takes an input, then applies some computations based on given parameters and then output the results.", "The output is then compared to some expected results and what we would like to see is to have the output as close to those expected results as possible.", "Take for example the analogy of old TV set or a radio, where you have a signal coming, and using a button you fine tune the device to have the best audio/video with little or no noise.", "The difference between the output and the expected result is described by a function that we call the Loss function (\ud835\udcdb).", "To keep things simple, we will assume that the loss function of the system is a parabola in form of :", "\ud835\udcdb = a\ud835\udf03 \u00b2 + bwhere \ud835\udf03 is the fine tuning parameter, a and b are some values.", "This loss function looks like the following graph:", "The graph above gives the amount of error in the output based on the parameters \ud835\udf03.The aim is to minimize the error, judging from the graph it is easy to spot that the minimal error is at the bottom of the curve (point: \ud835\udf03 = 0, \ud835\udcdb = 1).", "As usual the analytical solution is a possibility but it is not always easy when the loss function gets complicated, for this reason we fallback to iterative solutions.", "The way we proceed is rather intuitive, we vary \ud835\udf03 and we look if \ud835\udcdb decreases.We keep doing that until we find the value of \ud835\udf03 for which is \ud835\udcdb minimum.", "As we can see when \ud835\udf03 moves from -2 to -1.4, \ud835\udcdb drops which means \ud835\udf03 is moving in the right direction. On the other hand when \ud835\udf03 moves from +1.5 to +2, \ud835\udcdb increases which means \ud835\udf03 is moving in the wrong direction, it must go in the opposite direction as shown by the arrow.", "Obviously we need to vary \ud835\udf03, but by how much and in which direction ?As we have seen above that as we vary \ud835\udf03 from the lowest values to the highest values (ex: -\u221e to \ufe62\u221e), we track the variation of \ud835\udcdb. If \ud835\udcdb keep decreasing we keep increasing \ud835\udf03, otherwise if \ud835\udcdb starts increasing we start decreasing \ud835\udf03 until we find the right value.This solves the direction, but what about the amount by which the \ud835\udf03 should vary ?A simple solution is to take a fraction of the variation of \ud835\udcdb, we call it \ud835\udec2.", "Mixing all of these features together we get the following formula:", "This formula clearly says that the next value of \ud835\udf03, is based on the previous value minus a fraction of the variation of \ud835\udcdb.If \ud835\udcdb is decreasing then \u2206\ud835\udcdb will be < 0 and -\ud835\udec2 . \u2206\ud835\udcdb > 0 so \ud835\udf03 will increase.If \ud835\udcdb is increasing then \u2206\ud835\udcdb will be > 0 and -\ud835\udec2 . \u2206\ud835\udcdb < 0 so \ud835\udf03 will decrease.", "Of course the most direct way to do this is \u2206\ud835\udcdb = \ud835\udcdb(end) \u2014 \ud835\udcdb(start) but this will require some management. A better way to do it, is to compute the derivative which will give the variation of \ud835\udcdb at each point.For example the derivative of \ud835\udcdb = a\ud835\udf03 \u00b2 + b becomes d\ud835\udcdb/d\ud835\udf03 = 2a\ud835\udf03, and so by plugging a value of \ud835\udf03 we get d\ud835\udcdb.The minimum of \ud835\udcdb is when d\ud835\udcdb becomes zero for a certain \ud835\udf03.The target now becomes finding \ud835\udf03 that gives d\ud835\udcdb = 0", "The \ud835\udec2 is usually decided upon experience, several values should be tried to find the one that makes the conversion faster, but there is still a catch here which is the constance of the parameter (more on that later).", "Consider that \ud835\udec2 = .9 , \ud835\udcdb = \ud835\udf03 \u00b2 , and a value \u025b = .00001 which tells how small \u2206\ud835\udcdb should become in order to consider it as zero.After iterating over the gradient descent formula (\ud835\udf03\ud835\udc5b\u208a\u2081 = \ud835\udf03\ud835\udc5b -\ud835\udec2 . \u2206\ud835\udcdb) we get the following table.", "As we can see it took 77 iterations in order for \u2206\ud835\udcdb to be smaller than .00001 and be able to find an acceptable value of \ud835\udf03.It is somehow long, and this is because we have a constant \ud835\udec2 !The reason is that we are making the same step even when we are close to the minimum, while in fact what we should do is use smaller steps in order to readjust the descent.", "Schematically the gradient descent is doing something like the following graph (PS. this is not accurate representation, but it gives an intuition)", "However, what is really needed, is that as the we approach the minimum, we have to adapt our steps in order to pin point that minimum and avoid zig-zagging around it. So we need something like the following:", "One way of achieving this is to decrease \ud835\udec2 as we go on, for example with each iteration i compute a factor f = 1 + (i/10) then divide \ud835\udec2 by f in the gradient descent formula:", "This modification will give a much improved result, in which we reach a solution in just 8 iterations, as shown in the following table:", "Finally here is a a python code that will compute a gradient descent for any loss function, provided that the right derivative is given in the function dF(x)", "Gradient descent is a handy tool to find the minimum of a loss function, it is very useful in deep learning field.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5240ca9a08da&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zsalloum.medium.com/?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----5240ca9a08da---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5240ca9a08da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5240ca9a08da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://rl-lab.com", "anchor_text": "http://rl-lab.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5240ca9a08da---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----5240ca9a08da---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5240ca9a08da---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5240ca9a08da---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5240ca9a08da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----5240ca9a08da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5240ca9a08da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----5240ca9a08da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5240ca9a08da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5240ca9a08da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5240ca9a08da---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5240ca9a08da--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5240ca9a08da--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5240ca9a08da--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "845 Followers"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-the-easy-way-5240ca9a08da&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}