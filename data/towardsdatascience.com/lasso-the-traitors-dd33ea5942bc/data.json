{"url": "https://towardsdatascience.com/lasso-the-traitors-dd33ea5942bc", "time": 1683008501.809193, "path": "towardsdatascience.com/lasso-the-traitors-dd33ea5942bc/", "webpage": {"metadata": {"title": "Your Labels and Data are Noisy? LASSO The Traitors! | by Dr. Oliver Borchers | Towards Data Science", "h1": "Your Labels and Data are Noisy? LASSO The Traitors!", "description": "This article develops the LASSO The Traitors (LTT) method. LTT filters out noisy observations from a dataset based on an exogenous performance metric. LTT significantly improves the performance of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://colab.research.google.com/drive/1E9FzjgezQDePtqvmMYdDBM7WL-1iL6JA?usp=sharing", "anchor_text": "Colab", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Garbage_in,_garbage_out", "anchor_text": "GIGO", "paragraph_index": 8}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "dataset", "paragraph_index": 13}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "dataset", "paragraph_index": 15}, {"url": "http://lib.stat.cmu.edu/datasets/boston", "anchor_text": "dataset", "paragraph_index": 17}, {"url": "https://colab.research.google.com/drive/1E9FzjgezQDePtqvmMYdDBM7WL-1iL6JA?usp=sharing", "anchor_text": "Colab", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Jackknife_resampling", "anchor_text": "Jackknife", "paragraph_index": 39}, {"url": "https://colab.research.google.com/drive/1E9FzjgezQDePtqvmMYdDBM7WL-1iL6JA?usp=sharing", "anchor_text": "Colab", "paragraph_index": 57}], "all_paragraphs": ["This article develops the LASSO The Traitors (LTT) method. LTT filters out noisy observations from a dataset based on an exogenous performance metric. LTT significantly improves the performance of estimators based on the cleaned dataset. LTT is fast, easily applicable, and task agnostic.", "Credits: LTT was developed during my time as a visiting scholar at the University of North Carolina at Chapel Hill in cooperation with my long term co-author and good friend, Daniel M. Ringel.", "You can find the replicable code for this article in this Colab notebook.", "If you use LTT in your research, please cite this very article:", "When working with supervised algorithms, you will encounter data X and label Y. Our goal is to find a mapping f : X \u2192 Y, which best generalizes to unseen data X*.", "The mapping function f is the learning algorithm. This algorithm learns to map values from input domain X to an output domain Y from data. The simplest cases are, for example, a linear regression or a logistic regression. So far, so standard.", "However, the world is inherently messy (or better messed up?). If you ever touched a dataset, which is not included in some fancy API, you most likely burnt your fingers in one way or the other (I certainly did so multiple times).", "This is how I learned, that the quality of the learned mapping is fundamentally impacted by the quality of the provided labels.", "The principle is also, more formally, known as \u201cGIGO\u201d: Garbage in \u2014 garbage out.", "While this insight sounds so extraordinarily mundane, I think it is often overlooked.", "Let\u2019s just grab some freely available data without considering it's nuances, or, for that matter, errors, and we are good to go. Because training and tuning an algorithm is much more rewarding and challenging than going through tens of thousands of observations manually.", "Any training on data imposes the implicit assumption, that the available data contains \u201c information\u201d. Or at least information, which is good enough to properly train an algorithm. This may be the case most of the time. However, there is likely also a special form of noise present in the data. Errors, which have the potential to degrade the quality of our mapping function.", "Let\u2019s consider some examples to make this point clear:", "A) Adapted from the IMDb sentiment classification dataset.", "x = This is a brilliant documentary that follows the life of Herge and his creating TinTin.y = 0", "B) Adapted from the IMDb sentiment classification dataset.", "x = This is a brilliant documentary that follows the life of Herge and his creating TinTin. I only disliked the score, due to its blandness and improper instrumentation.y = 1", "C) Adapted from the Boston housing dataset.", "Something is off here. Continue reading for an analysis of each case.", "This is what we are tackling in this article: Automating the detection of such errors in the training data using the LASSO regression. We are essentially abusing the LASSO as a data cleansing method. This method is called: LASSO THE TRAITORS (LTT).", "There exists a whole research stream on the problem of \u201clabel noise\u201d. See [1] for a survey. The authors in [2] define label noise \u201cas anything that obscures the relationship between the features of an instance and its class\u201d. The literature discussed in [1] basically considers two types of label noise:", "A whole bunch of methods have been developed, which are capable of dealing with label noise, as described in [1]. For our purpose, we regard the two types of noise as indistinguishable, because the resulting effect is probably either one of the following two:", "Before we proceed into the actual description of LTT, let\u2019s discern what went wrong in the earlier examples:", "A) Straightforward. It's just the wrong label. The label should be y = 1, because this is clearly a positive review.", "B) Less straightforward. Here we have a nuanced opinion, which cannot easily be classified as positive or negativ. It\u2019s just something in between. Classifying each sentence separately could do the trick, but we have just one label. So this example might degrade overall performance due to its ambiguity.", "C) This one is difficult. If we didn\u2019t knew the data specifications, we wouldn\u2019t have noticed that we missed a comma in x3 = 2.310. This one could obviously be identified as an outlier by a descriptive analysis. But it is easy to think of situations, where two distinct variables are valid in isolation, but represent an illegal, or at least highly unusual, combination in the input domain (age = 5, married = 1).", "D) Impossible. We cannot find anything manually here, because we don\u2019t know the corresponding sentence, which lead to the embedding. It doesn\u2019t matter, if our vector x consists of five or a thousand latent variables. We are flying blind, while imposing the assumption, that the source sentence is valid and \u201cgood\u201d training material.", "To tackle these examples, I am proposing a new* method, called LTT. Why do I post this here on Medium?", "(*: I couldn\u2019t find anything in a brief literature research. If you know LTT is not new, please notify me).", "Let\u2019s consider a tuple (xd \u2208 X, yd \u2208 Y), and an exclusion variable id for observation d. The exclusion variable id = 0 indicates, that the (xd, yd) tuple is present in the training set. The tuple is excluded, if id = 1.", "Next, we define an exogenous performance metric r, which is not based on the training set. This could be the performance on a validation dataset or something more downstream. A tuple is considered clean, if its inclusion contributes positively to the exogenous performance (id = 0 \u2192 r \u2264 r + \u2206) and noisy, if its inclusion deteriorates the exogenous performance (id = 0 \u2192 r > r + \u2206) by some contribution \u2206. The resulting exclusion vector i \u2208 [0,1]^D is a D-dimensional one-hot vector and encodes, which of the D observations are excluded (1) from the training set and which ones are included (0).", "Next, we draw up to S random observations without replacement of the original data: Each exclusion vector i is randomly sampled from a Bernoulli distribution with parameter p: is ~ B(p) \u2200 s \u2208 1,\u2026,S. Depending on the data size, the probability of exclusion p, takes on any value 0 < p < 1. For each exclusion vector is, there exists the corresponding exogenous performance metric rs. The exogenous performance metric is the result of training an estimator on the reduced dataset and its subsequent evaluation on an exogenous task. We assume, that the exclusion of a tuple affects the exogenous performance independently.", "We end up with an exclusion matrix I \u2208 [0,1]^(S x D) and with a vector of performance metrics r \u2208 \u211c^S. Next, we regress r on I using the LASSO [3] with some value for the regularization parameter \ud835\udf06. One can repeat this process multiple times with varying \ud835\udf06s. The estimated parameter vector \ud835\udefd of the LASSO regression indicates, if the observation d in the training set", "Afterwards, we can either manually or automatically intervene and deal with the respective observations. The strength of the LTT approach lies in its simplicity, and that it is model agnostic. Its weakness is \u2014 you might have already guessed it \u2014 that you have to train many models to obtain the exogenous performance vector r.", "We make the assumption, that removing observations from said training set potentially influences the exogenous task performance. This assumption might not necessarily hold for every application or performance metric. It\u2019s up to the user to check, if this assumption holds.", "While LTT is intended to filter out performance degrading observations, you can obviously use the approach to investigate observations, which enhance the performance.", "You can find the replicable code for this article in this Colab notebook.", "To demonstrate the application and performance of LTT, we continue with a simulation. We generate a random regression problem with 2500 observations, 25 features (5 of them are informative), and one outcome variable. We split the dataset to training (50%), validation (25%), and testing (25%) set. This is our raw data. The training dataset consists of 1250 observations.", "For the outcome variable y of the training set (!), we invert the sign of 5% of the observations, inducing a very simple label noise to the training data. We end up with two datasets: RAW and NOISE. The former contains the untouched original data, the latter contains the data plus label noise in y.", "We evaluate the performance of a linear regression on both datasets using Jackknife sampling (leave-n-out). We drop 5% of the observations without replacement, fit a regression, and evaluate the exogenous performance. Lather, rinse, repeat 1500 times. Our exogenous performance r is the mean squared error (MSE) on the validation data. Thus, we end up with 1500 MSEs. The descriptive results look as follows:", "By changing the sign of approximately 5% of the labels in y, we increased the mean validation MSE from 0.04 to 327.36! (TBH: This is quite more than I expected)", "Let\u2019s continue by creating the exclusion matrix I for LTT. The exclusion matrix I is simply a random sparse boolean matrix with a predefined density. The density indicates the percentage of observations to be dropped during each LTT iteration. This value represents the expected value for each row.", "Next, we iterate the rows of the exclusion matrix. Each row corresponds to an experiment, where we remove a subset of the observations. It is important, that we store the exclusion matrix I for the LASSO. For each iteration, we fit an arbitrary estimator to the reduced dataset and store its performance on the exogenous task.", "The resulting array of exogenous performances is our vector r, as explained earlier. The performance metric can be anything. You may also use entropy, log-loss, R2, or whatever you desire, as long as it is related to the task and exogenous to the training process.", "Next, we regress the resulting vector of exogenous performances r on our exclusion matrix I using the LASSO. We repeat the process for \ud835\udf06 = (10e-4, 10e-3, 10e-2, 10e-1, 10e1). The resulting percentage of observations, which contribute to the exogenous performance in one way or the other (\ud835\udefd \u2260 0), looks as follows:", "The figure reads as follows: For \ud835\udf06 = 0.1, 7.28% of the observations contribute positively to the exogenous performance (\ud835\udefd > 0). On the contrary, 3.36% deteriorate the exogenous performance (\ud835\udefd < 0). In total, 10.64% of the observations contribute to the exogenous performance.", "We also see, that the percentage of observations with a positive contribution declines rather rapidly, as we increase \ud835\udf06. This indicates, that there are probably a few observations in the training data with a relatively strong negative contribution. Recall: we inverted the sign of only 5% of the labels in the training data y. Based on the graph, a \ud835\udf06 between 0.1 and 1 seems like a fairly good choice for analyzing the label noise in the data.", "If we look at the \ud835\udefd vector for \ud835\udf06 = 0.1, we get a lot more insights into what is really going on in our training data. Remember, each \ud835\udefd corresponds to each observations d in the training dataset.", "The figure is to be read as follows: Excluding the observation (i = 1) with index d = 604 increases the expected exogenous performance r by -39.23. More concrete: Excluding the observation reduces the MSE on the validation data, because MSE =\u2211 id * \ud835\udefdd. We see, that multiple observations have a strong negative impact on the exogenous performance.", "To test, if we correctly identified the labels, let us perform a sanity check. We select the top 50 coefficients by the size of \ud835\udefd and compare the corresponding indices to the indices of the labels we purposefully introduced errors to.", "The LTT approach correctly identified 39 out of the top 50 noisy observations. There is obviously room for improvement and future research. One might check each observation individually or write some custom logic for more effective filtering. But this is up to the end user.", "Next, we remove the 50 observations with the largest \ud835\udefd from the training data and fit a new estimator. The exogenous performance (as computed earlier via Jackknife) of the estimator trained on the new data is as follows:", "By removing the noisy observations from the training data using LTT, we were able to increase the exogenous performance significantly.", "A perfect score would be 0.04. Yet, the increase in performance is already extraordinarily and was only achieved by removing a measly 50 observations from the training data.", "Replicating the results on the test data essentially reproduces the results shown above, thus I will not repeat it here.", "We developed and implemented the LASSO The Traitors (LTT) method. LTT is capable of filtering out noisy observations based on an exogenous performance metric. LTT is easily applicable, readily available, and agnostic to the underlying problem.", "For inquiries and questions, feel free to contact me.", "You can find the replicable code for this article in this Colab notebook.", "If you use LTT in your research, please cite this very article:", "Opinions expressed are solely my own and do not express the views or opinions of my employer. The author assumes no responsibility or liability for any errors or omissions in the content of this site. The information contained in this site is provided on an \u201cas is\u201d basis with no guarantees of completeness, accuracy, usefulness or timeliness.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "From data to business value: Lead Machine Learning Engineer @ Oxolo | Data Scientist| Programmer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd33ea5942bc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@oliverbor?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oliverbor?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "Dr. Oliver Borchers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2fcfdd820b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=post_page-c2fcfdd820b0----dd33ea5942bc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd33ea5942bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd33ea5942bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://colab.research.google.com/drive/1E9FzjgezQDePtqvmMYdDBM7WL-1iL6JA?usp=sharing", "anchor_text": "Colab"}, {"url": "https://en.wikipedia.org/wiki/Garbage_in,_garbage_out", "anchor_text": "GIGO"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "dataset"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "dataset"}, {"url": "http://lib.stat.cmu.edu/datasets/boston", "anchor_text": "dataset"}, {"url": "https://colab.research.google.com/drive/1E9FzjgezQDePtqvmMYdDBM7WL-1iL6JA?usp=sharing", "anchor_text": "Colab"}, {"url": "https://en.wikipedia.org/wiki/Jackknife_resampling", "anchor_text": "Jackknife"}, {"url": "https://colab.research.google.com/drive/1E9FzjgezQDePtqvmMYdDBM7WL-1iL6JA?usp=sharing", "anchor_text": "Colab"}, {"url": "https://ieeexplore.ieee.org/document/6685834", "anchor_text": "Classification in the Presence of Label Noise: A Survey"}, {"url": "https://dl.acm.org/doi/abs/10.1023/A%3A1022643204877", "anchor_text": "Induction of decision trees"}, {"url": "https://statweb.stanford.edu/~tibs/lasso/lasso.pdf", "anchor_text": "Regression Shrinkage and Selection via the Lasso"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dd33ea5942bc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/outlier-detection?source=post_page-----dd33ea5942bc---------------outlier_detection-----------------", "anchor_text": "Outlier Detection"}, {"url": "https://medium.com/tag/data-science?source=post_page-----dd33ea5942bc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----dd33ea5942bc---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----dd33ea5942bc---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd33ea5942bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=-----dd33ea5942bc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd33ea5942bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=-----dd33ea5942bc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd33ea5942bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdd33ea5942bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dd33ea5942bc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dd33ea5942bc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oliverbor?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oliverbor?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dr. Oliver Borchers"}, {"url": "https://medium.com/@oliverbor/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "284 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2fcfdd820b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=post_page-c2fcfdd820b0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbb76721149ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flasso-the-traitors-dd33ea5942bc&newsletterV3=c2fcfdd820b0&newsletterV3Id=bb76721149ab&user=Dr.+Oliver+Borchers&userId=c2fcfdd820b0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}