{"url": "https://towardsdatascience.com/gaming-on-reddit-revisited-74f12c8a95d2", "time": 1683002057.375987, "path": "towardsdatascience.com/gaming-on-reddit-revisited-74f12c8a95d2/", "webpage": {"metadata": {"title": "Gaming on Reddit, Revisited. An exercise in NLP and iterative data\u2026 | by Jeremy Ondov | Towards Data Science", "h1": "Gaming on Reddit, Revisited", "description": "One of the most popular projects in General Assembly\u2019s Data Science Immersive is centered around reddit. Your objective: pick two subreddits, collect a thousand posts from each, and build a model to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ondovj/subreddit_classification", "anchor_text": "full project repo here", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/jeremy-ondov/", "anchor_text": "on LinkedIn", "paragraph_index": 24}], "all_paragraphs": ["One of the most popular projects in General Assembly\u2019s Data Science Immersive is centered around reddit. Your objective: pick two subreddits, collect a thousand posts from each, and build a model to classify the posts back to their correct subreddits from the title. It\u2019s easy to see why this is such a popular project, it comes in at #3 of 5, so we\u2019re starting to get comfortable with our new skillset, and we get to pick our own subreddits, so we can really have some fun with it as well. It\u2019s a good taste of the powerful technology that\u2019s right at the fingertips of data scientists.", "For my implementation, I chose two subreddits in a subject that I have a fair amount of domain knowledge in \u2014 gaming! I\u2019ve been a gamer from at least the day I was born (rumored but unverified), so I was pretty excited to be able to find my own insights into the gaming world. I chose two subreddits with similar, but distinct subjects, r/gaming and r/pcgaming.", "If the difference between these two subreddits isn\u2019t immediately obvious, one of them deals with gaming in general, which can include any of the home consoles, mobile, even board games. The other subreddit deals specifically with gaming on computers, which is not just about the games themselves, but the hardware, software, peripherals, and much more that tends to be a unique experience when gaming on a computer. Of course, there is a lot of overlap between the two subreddits, which is exactly why this makes a great problem to work on!", "I recently decided to take some time and run through the project again, not just to collect more data, but also to try implementing some of the improvements I hadn\u2019t gotten around to originally. I have definitely grown as a programmer, so there were plenty of changes I knew I wanted to make even from an efficiency and readability standpoint. Cycling through projects isn\u2019t just a chance to improve the ML training, but a great time to improve your code!", "As always, the first step is to actually get the data that you\u2019re going to be working with. Originally I had just written the code to make the pulls and left it at that. This time, I wanted to turn it into a real function, so I could continue to make the pulls and not clog up the notebook with big copied code blocks.", "When I first pulled the posts from each subreddit, a large amount of what came back ended up being duplicates, and that remained true the second time around. At the end of the second scrape, there were about 2400 unique entries in total.", "The cleaning steps I had originally taken went largely unchanged with the addition of new data. There were a few more cross-posts (posts showing up in both subreddits) and outliers (titles with just emojis/emoticons), so where I had previously addressed these issues manually, I wrote some new scalable code that wouldn\u2019t have to be changed each time the data changes.", "I also created a new column from the last iteration, a count of the number of words in each title. I had already made a feature of the total character length, but I wanted to add another dimension to the information. It not only helps to gain insight into the data and the subreddits themselves, but it can be useful as a predictor in the modeling phase later on. Visualizing the data on a histogram, I could clearly see a large amount of overlap in the lengths of the titles, as well as the distinct shift in the length of titles for posts in r/pcgaming compared to r/gaming.", "The fact that more posts have titles in the 10\u201315 word range signals that there may be a higher complexity in what is being discussed in r/pcgaming. From a modeling standpoint, this could mean that a title that is longer may be more likely to be from r/pcgaming than r/gaming, but the effectiveness remains to be seen.", "I also took a deeper look into the words themselves, to get a better idea of what types of language make up the targets. I ran the data through a simpleCountVectorizer to find which words were the most common. Then through several iterations, I decided to use PorterStemmer to simplify the words, and identified typical stopwords to remove, including some additional ones that were specific to this data.", "With my new data, I had to go through this process a few more times, identifying even more stopwords to add to my list. In the end, about half of the top words from r/gaming were different this time around, with fewer changes for r/pcgaming, where the top words held a much higher lead.", "When going through the lifecycle of a data science project, it may be tempting to get your new data and just jump right back into the modeling phase, but EDA is an important step. Not every situation is the same, so maybe adding a few thousand entries to a dataset that is already in the millions may not make a huge difference, but in this case, I doubled my data so there was a potential for big changes.", "The first time around, modeling was already a fairly complex process. I had significant issues with highly overfit models, but ended up with an AdaBoostClassifier that had low variance and an accuracy in the high 70s. I had combined the vectorization step with the model training in a Pipeline with GridSearchCV, to be able to fully optimize each type of classifying algorithm.", "This time I made some changes. Since I wanted to add my engineered features \u2014 the word and character counts of the titles \u2014 I knew I couldn\u2019t just keep my old pipeline in place. I had also found that using TfidfVectorizer was giving me better results than CountVectorizer, so I decided to make it a permanent change, and shift it to the preprocessing step as opposed to a part of the grid search (This may not have been the best idea, but I\u2019ll get back to that). So with my words vectorized (and now a sparse matrix), I transformed them and my additional numeric columns into arrays and stacked them together to get my new training data.", "I ran through all of my old models again (Logistic Regression, Random Forest, and more), and the first thing I noticed was a large drop in the overfit that had previously been seen in these models. However, there did not seem to be much impact on the accuracy scores (in either direction). Since I had come across more types of algorithms since I had first done the project, I also wanted to try something new that might work even better with this NLP problem. I added a Support Vector Classifier to my lineup and hoped for an amazing result.", "I didn\u2019t get one. I pulled up the metrics for all my models \u2014 I was going with accuracy for this project because the classes were well balanced, and there was no greater detriment to false positives or false negatives \u2014 and compared them. While the SVC was at the top of my list, it actually had a somewhat similar performance to the Logistic Regression model.", "So in picking the \u201cbest\u201d model of the bunch, it was basically down to those two models, since they seemed to give the best bias/variance balance (unlike KNN or Random Forest). In the end, I chose the SVC as the \u201cbest\u201d because it minimized the variance more than the logistic regression improved the performance. However, since the SVC is something of a \u201cblack box\u201d model, I wanted to still utilize the logistic regression for its interpretability. Matching the coefficients up with the features, I was able to find what was most important in making the decision for the model.", "We can see that the words we had seen in the EDA did end up being very important in the modeling process \u2014 \u201cSteam\u201d, \u201ctrailer\u201d, and \u201crelease\u201d were the same top 3 used words in r/pcgaming. On the other side, having words like \u201chappy\u201d and \u201cfriend\u201d meant that a post was very unlikely to be from r/pcgaming. I don\u2019t think I want to dig into what that might say about PC gamers, but I do know that it was very helpful for the model. I also noted that the two extra features I made did not appear anywhere near the top of the coefficients list, which was somewhat disappointing.", "So, what did this new and improved iteration (with fresh data, features, and models) actually get me? Not much, as it turns out. I didn\u2019t make a large impact on the performance (I was already in the 78% accurate range), but I was able to further decrease my variance, which honestly was not huge to begin with.", "What happened? There are a few possibilities I\u2019ve thought of. One is that I haven\u2019t given enough new information \u2014 maybe 2400 posts just isn\u2019t enough to effectively train a model on this problem \u2014 but this feels like a cop-out since you can almost always say that you \u201cneed more data\u201d, and I\u2019ve seen fairly consistent performance between both iterations. Another is that I didn\u2019t do enough feature engineering \u2014 maybe I need to dig deeper and add data about how many comments there are, or another aspect of these posts hidden in the original JSON files that were pulled. I may also be approaching this too simplistically, and it requires more advanced NLP, like with Word2Vec and using embeddings instead of a \u201cbag of words\u201d.", "The other big issue I saw was in my modeling. In the original version, I was testing and optimizing different vectorizers as part of a grid searching/pipeline function. This time around, I added separately engineered features, and I decided to reconfigure my pre-processing to make that easier. In doing so, I removed the ability to optimize the vectorization in conjunction with the new features \u2014 maybe with this added information there was a different set of parameters for TF-IDF that would improve the accuracy of certain models, or maybe CVEC was actually the way to go. I recently learned about ScitKit-Learn\u2019s ColumnTransformer for exactly this kind of combination of different feature transformations into a pipeline, so I think next time around I will be adapting my code to use it", "This was absolutely worth coming back to after the GA course and offered a good lesson in what it\u2019s like to go through multiple cycles of a project (and tempering expectations). Of course, you always go back to something with the intent of improving it, but sometimes you don\u2019t make much of an impact. Either way, it\u2019s a chance to learn and grow, so don\u2019t let that pass you by.", "As always, thanks for taking this ride on the data science train, and let me know your thoughts on the project \u2014 is the SVC really the best model? Should I not engineer more features? Comment below!", "Please check out the full project repo here.", "You can also find me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F74f12c8a95d2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ondovj?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ondovj?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "Jeremy Ondov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7ed4a455863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&user=Jeremy+Ondov&userId=a7ed4a455863&source=post_page-a7ed4a455863----74f12c8a95d2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F74f12c8a95d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F74f12c8a95d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral", "anchor_text": "JESHOOTS.COM"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@abdulbarie_91?utm_source=medium&utm_medium=referral", "anchor_text": "Abdul Barie"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/ondovj/subreddit_classification", "anchor_text": "full project repo here"}, {"url": "https://www.linkedin.com/in/jeremy-ondov/", "anchor_text": "on LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----74f12c8a95d2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----74f12c8a95d2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reddit?source=post_page-----74f12c8a95d2---------------reddit-----------------", "anchor_text": "Reddit"}, {"url": "https://medium.com/tag/python?source=post_page-----74f12c8a95d2---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/programming?source=post_page-----74f12c8a95d2---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F74f12c8a95d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&user=Jeremy+Ondov&userId=a7ed4a455863&source=-----74f12c8a95d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F74f12c8a95d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&user=Jeremy+Ondov&userId=a7ed4a455863&source=-----74f12c8a95d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F74f12c8a95d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F74f12c8a95d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----74f12c8a95d2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----74f12c8a95d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ondovj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ondovj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Ondov"}, {"url": "https://medium.com/@ondovj/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "15 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7ed4a455863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&user=Jeremy+Ondov&userId=a7ed4a455863&source=post_page-a7ed4a455863--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fa7ed4a455863%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaming-on-reddit-revisited-74f12c8a95d2&user=Jeremy+Ondov&userId=a7ed4a455863&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}