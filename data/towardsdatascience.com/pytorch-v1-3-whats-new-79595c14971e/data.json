{"url": "https://towardsdatascience.com/pytorch-v1-3-whats-new-79595c14971e", "time": 1683000839.241801, "path": "towardsdatascience.com/pytorch-v1-3-whats-new-79595c14971e/", "webpage": {"metadata": {"title": "PyTorch 1.3 \u2014 What\u2019s new?. Support for Android and iOS, Named\u2026 | by Vaibhav Kumar | Towards Data Science", "h1": "PyTorch 1.3 \u2014 What\u2019s new?", "description": "Facebook just released PyTorch v1.3 and it is packed with some of the most awaited features. The three most attractive ones are: PyTorch v1.3 finally added the support for named tensors which allows\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_as", "anchor_text": "align_as()", "paragraph_index": 4}, {"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_to", "anchor_text": "align_to()", "paragraph_index": 4}, {"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_as", "anchor_text": "align_as()", "paragraph_index": 5}, {"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_to", "anchor_text": "align_to()", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95", "anchor_text": "Autograd", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Fixed-point_arithmetic", "anchor_text": "fixed-point", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Floating-point_arithmetic", "anchor_text": "floating-point arithmetic", "paragraph_index": 17}, {"url": "https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-dynamic-quantization", "anchor_text": "a model of size 113.9MB in 32 bit floating point representation can be quantized as int8 to be of size 76.8MB.", "paragraph_index": 18}, {"url": "https://pytorch.org/docs/master/quantization.html#quantized-torch-tensor-operations", "anchor_text": "Here", "paragraph_index": 24}, {"url": "https://pytorch.org/mobile/ios/", "anchor_text": "iOS", "paragraph_index": 26}, {"url": "https://pytorch.org/mobile/android/", "anchor_text": "Android", "paragraph_index": 26}, {"url": "https://github.com/pytorch/pytorch/releases", "anchor_text": "Github repository", "paragraph_index": 27}], "all_paragraphs": ["Facebook just released PyTorch v1.3 and it is packed with some of the most awaited features. The three most attractive ones are:", "I would be talking in brief about all of these and will link to some other important features.", "PyTorch v1.3 finally added the support for named tensors which allows users to access tensor dimensions using explicitly associated names rather than remembering the dimension number. For example, up until now in computer vision related tasks, we had to remember the general structure of a batch as follows \u2014 [N, C, H, W]. Where N is batch size, C is number of channels, H and W are height and width of images respectively. We had to keep track of such a structure while performing operations on this batch but now we can just use the dimension name rather keeping track of its index. Additionally, these named representations can provide augmented runtime error checks. I will discuss them further in this article.", "batch.namesprints the name of each dimension on tensor batch in order.", "Use align_as() or align_to() to align tensor dimensions by name to a specified ordering.", "In computer vision models often the representation of a batch needs to change in between [N, C, H, W] (for forward and backward pass of model) and [N, H, W, C] (for plotting and saving images). Until now this had to be done counter intuitively as batch.permute([0, 2, 3, 1]) but now it can be done in a much easier way by using the align_as() or align_to()operator.", "For a large number of dimensions, normal permute operator needs an explicit list of all dimensions even for exchanging the positions of just two dimensions. However, in the case of named tensors, the permutation or reordering of the dimensions can be done in an easier and simpler way as follows:", "Apart from making tensors more intuitive, named tensors also provide additional error checks. An operator, when applied on named tensors (in case of binary operator any one or both named tensors) will implicitly check that certain dimension names match at the run time. This provides extra security against errors. This is shown in the example below:", "In the above example, if not for named tensors, batch1 and batch2 can be added without any error because height = width = 100. However, batch2 has the height and width dimension interchanged and adding it to batch1 might not be the intended operation. Thanks to named tensors, this logical error will be caught by name checking as (\u2019N\u2019, \u2018C\u2019, \u2018H\u2019, \u2018W\u2019) and (\u2019N\u2019, \u2018C\u2019, \u2018W\u2019, \u2018H\u2019) are not the same.", "The rules are pretty much like the broadcasting rules for the dimensions in numpy or PyTorch. Quoting the official PyTorch docs:", "Two names match if they are equal (string equality) or if at least one is None. Nones are essentially a special \u201cwildcard\u201d name.\u201d", "This is showcased in the example below:", "After performing operations on tensors you don\u2019t need to enter the dimension names again, they will automatically be propagated. PyTorch makes use of two operators \u2014 match and unify for name propagation.", "unify(A, B) determines which of the names A and B to propagate to the outputs. It returns the more specific of the two names, if they match. If the names do not match, then it errors.", "Name propagation is showcased in an example below:", "At the time of writing this article, the named tensor functionality is in experimental mode and maybe subjected to many changes. However, one of the biggest current limitations of named tensors is that they don\u2019t fully support the Autograd engine. Although the gradient computations for named tensors is perfectly same but the autograd engine completely ignores the name and along with it, the additional security they provide.", "PyTorch 1.3 now supports Quantization of tensors. This is something Tensorflow already supports and was much awaited in PyTorch. Quantization is a fairly simple yet elaborate concept. I will try to write about them in brief with a high level of abstraction here by answering the three questions \u2014 What, Why and How.", "Quantization is the technique of performing operations in a low-precision format or converting high precision data formats to low precision data formats. This is done for example by treating a 32-bit floating point format as an 8-bit fixed point format. If interested, you might read about the fixed-point and floating-point arithmetic and their related complexities to better understand the need of Quantization.", "The whole purpose of pursuing research and creating neural network models is to deploy them and make them available for the public good. While the model training need only grows proportionally to the number of researchers and machine learning practitioners, the need for mode inference grows proportional to the number of consumers. To allow more and better access to the end users, the representation of the models deployed for inference needs to be much more compact than their representation while training. Another thing to keep in mind is that backpropagation needs high precision representations for the model weights and biases. However, during inference, the models are much more robust and do not require high precision representation. Thus, a model of size 113.9MB in 32 bit floating point representation can be quantized as int8 to be of size 76.8MB.", "How to use Quantization in PyTorch v1.3?", "PyTorch supports INT8 quantization compared to typical FP32 models allowing for a 4x reduction in the model size and a 4x reduction in memory bandwidth requirements. Hardware support for INT8 computations is typically 2 to 4 times faster compared to FP32 compute.", "For Quantization, PyTorch introduced three new data types as follows:", "PyTorch now offers three kinds of quantization methods for models:", "Another thing to notice is that PyTorch support Quantization from ground up. This means, we can also quantize tensors with the following equation (very intuitive) and code:", "Here is the list of all supported operations on quantized tensors.", "The natural purpose of allowing quantization is shipping PyTorch models on performance critical mobile phones (and other devices). They have implemented an end-to-end PyTorch API for both Android and iOS. This will have a great impact on reducing inference latency and user privacy. However, PyTorch mobile is currently in an early experimental stage where it has several limitations. For example, the current version only supports forward propagation (inference) and no backward operations are supported.", "You can access the hello world tutorials for both iOS and Android on the official PyTorch website.", "Apart from these three major updates, PyTorch v1.3 has implemented several other changes and bug fixes. You can view the list of all the changes on the official PyTorch Github repository. Some of these features are:", "Additionally, if you want to port your code from a previous version of PyTorch to PyTorch v1.3, you need to take of care features which might cause errors or unintended behavior. Some of these features are (I am directly quoting these changes from the previously mentioned release notes):", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An actual Time Traveller from the past"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F79595c14971e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----79595c14971e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----79595c14971e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@timetraveller1998?source=post_page-----79595c14971e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@timetraveller1998?source=post_page-----79595c14971e--------------------------------", "anchor_text": "Vaibhav Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30121c857f6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&user=Vaibhav+Kumar&userId=30121c857f6c&source=post_page-30121c857f6c----79595c14971e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79595c14971e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79595c14971e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_as", "anchor_text": "align_as()"}, {"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_to", "anchor_text": "align_to()"}, {"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_as", "anchor_text": "align_as()"}, {"url": "https://pytorch.org/docs/master/named_tensor.html#torch.Tensor.align_to", "anchor_text": "align_to()"}, {"url": "https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95", "anchor_text": "Autograd"}, {"url": "https://en.wikipedia.org/wiki/Fixed-point_arithmetic", "anchor_text": "fixed-point"}, {"url": "https://en.wikipedia.org/wiki/Floating-point_arithmetic", "anchor_text": "floating-point arithmetic"}, {"url": "https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-dynamic-quantization", "anchor_text": "a model of size 113.9MB in 32 bit floating point representation can be quantized as int8 to be of size 76.8MB."}, {"url": "https://pytorch.org/docs/master/quantization.html#torch.quantization.quantize_dynamic", "anchor_text": "documentation."}, {"url": "https://pytorch.org/docs/master/quantization.html#quantized-torch-tensor-operations", "anchor_text": "Here"}, {"url": "https://pytorch.org/mobile/ios/", "anchor_text": "iOS"}, {"url": "https://pytorch.org/mobile/android/", "anchor_text": "Android"}, {"url": "https://github.com/pytorch/pytorch/releases", "anchor_text": "Github repository"}, {"url": "https://github.com/pytorch/xla/tree/master/contrib/colab", "anchor_text": "Github repo"}, {"url": "https://github.com/pytorch/pytorch/releases", "anchor_text": "Additional changes in pyTorch for C++"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----79595c14971e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----79595c14971e---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/python?source=post_page-----79595c14971e---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----79595c14971e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----79595c14971e---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79595c14971e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&user=Vaibhav+Kumar&userId=30121c857f6c&source=-----79595c14971e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F79595c14971e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&user=Vaibhav+Kumar&userId=30121c857f6c&source=-----79595c14971e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F79595c14971e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----79595c14971e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F79595c14971e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----79595c14971e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----79595c14971e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----79595c14971e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----79595c14971e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----79595c14971e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----79595c14971e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----79595c14971e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----79595c14971e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----79595c14971e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@timetraveller1998?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@timetraveller1998?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vaibhav Kumar"}, {"url": "https://medium.com/@timetraveller1998/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "419 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30121c857f6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&user=Vaibhav+Kumar&userId=30121c857f6c&source=post_page-30121c857f6c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe2e0c2d3e1ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-v1-3-whats-new-79595c14971e&newsletterV3=30121c857f6c&newsletterV3Id=e2e0c2d3e1ba&user=Vaibhav+Kumar&userId=30121c857f6c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}