{"url": "https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171", "time": 1683013009.768932, "path": "towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171/", "webpage": {"metadata": {"title": "Train GPT-2 in your own language. A step-by-step guide to train your own\u2026 | by Arshabhi Kayal | Towards Data Science", "h1": "Train GPT-2 in your own language", "description": "We all know modern day Natural Language Processing (NLP) has progressed by leaps and bounds in the past couple of years following the development of attention networks and transformers. It paved the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "attention networks", "paragraph_index": 0}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 1}, {"url": "https://jalammar.github.io/illustrated-gpt2/", "anchor_text": "link", "paragraph_index": 1}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Satyajit_Ray", "anchor_text": "Satyajit Ray", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Feluda", "anchor_text": "Feluda", "paragraph_index": 7}], "all_paragraphs": ["We all know modern day Natural Language Processing (NLP) has progressed by leaps and bounds in the past couple of years following the development of attention networks and transformers. It paved the way for a plethora of new algorithms achieving State-Of-The-Art (SOTA) for the different tasks of NLP.", "OpenAI has been one of the leaders in providing their own language model (now released GPT-3) which is trained on a huge corpus of internet data. Since, GPT-3 is a recent phenomenon and in English at the moment, and is only accessible through API given by OpenAI, we shift our focus on the earlier version of it i.e. GPT-2. To know about the internal nuts and bolts of GPT-2, I\u2019d suggest you to go through this link. For more depths into Attention and Transformers, here are some excellent links:", "GPT-2 was also released for English, which makes it difficult for someone trying to generate text in a different language.", "So why not train your own GPT-2 model on your favourite language for text generation? That is exactly what we are going to do. So, without further ado, let us jump in.", "For the demo, I have considered a non-Latin alphabet script (Bengali here), because why not!! I have used Huggingface\u2019s implementation for the model.", "Gathering good quality data is one of the most important stages as all Data Scientists would agree. So, we are going to assume that you already have a folder containing .txt files having all the data cleaned and stored. For ease, you can use the Wikipedia article data, which is available and can be downloaded with the following code.", "This will create a folder containing all Wikipedia files looking like:", "Note: due to resource constraint, and since it is for demo purpose, I have trained the model in a small subset of books by Satyajit Ray, especially his detective Feluda series.", "Now, the second step will be to tokenize the data. For that, we use the following class.", "So what we do here is tokenize our data and save it in a folder. Two files will be created (merges.txt and vocab.json) in a specified directory. To run the file, use the following code:", "Before the real magic begins, we need to make sure the artilleries are ready. Let us start with some initializations.", "We also create a single string from all our documents and tokenize it.", "After we have encoded the whole string, we now move on to make a TensorFlow dataset, slicing the data into equal intervals, so that our model can learn. Here we use a block size of 100 (length of token in each example) and a batch size of 16. This is kept low else we can run it with ease on a RTX 2060 GPU.", "Now comes the part we\u2019ve been waiting for, making the model and training. So we define our optimizer, loss functions and the metrics, and start training.", "To predict, we just need to simply encode the input text and pass it to the model", "Now, if you are a Bengali, then you can point it out that the output although the sentence is syntactically correct, it doesn\u2019t look cohesive. True, but for this demo, I have kept this demo a minimal as possible.", "Well, after long training time, what good will it do if we close our session and all our trained model is just lost and we again need to train it from scratch. So, let\u2019s save the model and the tokenizer so that we can retrain from where we left off", "We have already done all the hard work, so to load the saved model and tokenizer, we only need to execute two lines of code and we\u2019re all set.", "Voila! Now you can train your own model in your own language. And create content which can race with some of the best literary works in any language.", "This blog gives a framework of how can one train GPT-2 model in any language. This is not at par with some of the pre-trained model available, but to reach that state, we need a lot of training data and computational power.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffc6ad4d60171&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@arshabhirk?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arshabhirk?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "Arshabhi Kayal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89455b6824b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&user=Arshabhi+Kayal&userId=89455b6824b5&source=post_page-89455b6824b5----fc6ad4d60171---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc6ad4d60171&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc6ad4d60171&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@korpa?utm_source=medium&utm_medium=referral", "anchor_text": "Jr Korpa"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "attention networks"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://jalammar.github.io/illustrated-gpt2/", "anchor_text": "link"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "The illustrated Transformer"}, {"url": "https://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "The Annotated Transformer"}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface"}, {"url": "https://en.wikipedia.org/wiki/Satyajit_Ray", "anchor_text": "Satyajit Ray"}, {"url": "https://en.wikipedia.org/wiki/Feluda", "anchor_text": "Feluda"}, {"url": "https://towardsdatascience.com/difference-between-nfd-nfc-nfkd-and-nfkc-explained-with-python-code-e2631f96ae6c", "anchor_text": "link"}, {"url": "https://huggingface.co/blog/how-to-train", "anchor_text": "How to train a new language model from scratch using Transformers and TokenizersOver the past few months, we made several improvements to our transformers and tokenizers libraries, with the goal of\u2026huggingface.co"}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "How to generate text: using different decoding methods for language generation with TransformersIn recent years, there has been an increasing interest in open-ended language generation thanks to the rise of large\u2026huggingface.co"}, {"url": "https://medium.com/tag/gpt-2?source=post_page-----fc6ad4d60171---------------gpt_2-----------------", "anchor_text": "Gpt 2"}, {"url": "https://medium.com/tag/multilanguage?source=post_page-----fc6ad4d60171---------------multilanguage-----------------", "anchor_text": "Multilanguage"}, {"url": "https://medium.com/tag/hugging-face?source=post_page-----fc6ad4d60171---------------hugging_face-----------------", "anchor_text": "Hugging Face"}, {"url": "https://medium.com/tag/transformers?source=post_page-----fc6ad4d60171---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/text-generation?source=post_page-----fc6ad4d60171---------------text_generation-----------------", "anchor_text": "Text Generation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc6ad4d60171&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&user=Arshabhi+Kayal&userId=89455b6824b5&source=-----fc6ad4d60171---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc6ad4d60171&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&user=Arshabhi+Kayal&userId=89455b6824b5&source=-----fc6ad4d60171---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc6ad4d60171&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffc6ad4d60171&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fc6ad4d60171---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fc6ad4d60171--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arshabhirk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arshabhirk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arshabhi Kayal"}, {"url": "https://medium.com/@arshabhirk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "55 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89455b6824b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&user=Arshabhi+Kayal&userId=89455b6824b5&source=post_page-89455b6824b5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1b6a4903b9ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-gpt-2-in-your-own-language-fc6ad4d60171&newsletterV3=89455b6824b5&newsletterV3Id=1b6a4903b9ec&user=Arshabhi+Kayal&userId=89455b6824b5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}