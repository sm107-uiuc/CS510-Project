{"url": "https://towardsdatascience.com/unsupervised-learning-clustering-algorithms-5b290967f746", "time": 1683002923.927691, "path": "towardsdatascience.com/unsupervised-learning-clustering-algorithms-5b290967f746/", "webpage": {"metadata": {"title": "Unsupervised Learning: Clustering Algorithms | by Max Miller | Towards Data Science", "h1": "Unsupervised Learning: Clustering Algorithms", "description": "Predictive models generally require what is called \u2018labeled\u2019 data to train on- that is that the data has some target variable that you have filled out already. Of course, the goal is to use the model\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "\u2018Manhattan Distance\u2019", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Minkowski_distance", "anchor_text": "Minkowski Distance", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "Cosine Similarity", "paragraph_index": 15}], "all_paragraphs": ["Predictive models generally require what is called \u2018labeled\u2019 data to train on- that is that the data has some target variable that you have filled out already. Of course, the goal is to use the model on unseen data where you don\u2019t know the value of the target variable, but without properly labeled training data, you have no way of validating your model. For this reason, data production is often the hardest part of a data science project. Say, for instance, you want to teach a computer to read handwriting. It\u2019s not enough to collect hundreds of pages of written words and scan them in. You also need to label this data, to have an accompanying \u2018correct reading\u2019 for each word. With something as complicated as handwriting, you might need hundreds of thousands of training examples and hand labeling that many entries will be laborious.", "Perhaps if you encounter a dataset without a labeled target variable there\u2019s a way to glean some insight out of it without going through the hassle of labeling it. Maybe you simply don\u2019t have the time or resources to label the data or maybe there isn\u2019t a clear target variable to predict, is there still a way to use the data? This is the world of \u2018unsupervised learning\u2019. One of the more common goals of unsupervised learning is to cluster the data, to find reasonable groupings where the points in each group seem more similar to each other than to those in the other groups. Two of the most common clustering methods are K-means clustering and agglomerative clustering.", "So, you have some data, you don\u2019t have any category labels for it, but you\u2019re pretty sure you can segment the data into sensible groups, you just don\u2019t know what those groups are. Let\u2019s generate some data to experiment with:", "This data was generated in three clumps, as you can see. For this example, that guarantees that there is a sensible grouping into three clusters, but remember that the point here is to see if you can find that grouping on your own, so let\u2019s remove the color coding:", "Visually you can probably still see the clusters, at least a little bit, but how do we get the computer to discover the clusters on its own? The first strategy we\u2019ll discuss is k-means. In the k-means strategy, you first specify how many clusters you\u2019re looking for, in this case we\u2019ll say 3, and the computer starts by placing that many new points randomly on the graph:", "These new points are called \u2018centroids\u2019 \u2014 they will be the centers of our clusters. Why place them randomly? Well, there isn\u2019t really a way for the computer to tell ahead of time where would be a sensible place to put the cluster centers, so the strategy is to place them randomly and then step by step move them to improve the clustering. To decide how to move the centroids, you first assign each of your points to the nearest centroid to it:", "I\u2019ve made the centroids star shaped and then color coded the points according to which centroid is closest. You can see that the red and green centroids are near each other and towards the edge of the data, this doesn\u2019t seem to make a lot of sense \u2014 for one thing the red cluster only has two points in it. So we\u2019ll try moving these centroids someplace more central. We move each centroid to the center of the cluster, that is the average location within the cluster. Once we\u2019ve moved the centroids, we re-assign the points to the nearest centroid. Here\u2019s what the new assignments look like one step into the process:", "The red centroid did not move far, it still only has two points in its cluster. The green centroid, however, took a big step towards the center of the cluster. When it did that, it captured a bunch of points from the blue cluster. As a result, when we repeat this process, the blue centroid will move further into the top corner, leaving the bottom to the green and the red:", "We continue this process, moving the centroids step by step and reassigning the points to the associated clusters, until we find a stable set of centers which have ceased to move. In this case, that equilibrium looks like this:", "Which isn\u2019t a bad way to segment this data set at all! It may also help to visualize the movement of the centroids themselves:", "In this process, the first fews steps were pretty big as the centroids get pulled towards the center of their clusters, but afterwards the steps they take become smaller and smaller as only a handful of points get reassigned in each step.", "In this instance, you\u2019ll notice that the cluster algorithm ends up essentially discovering the groups that were made when I generated the data. Crucially, though, remember that this being an unsupervised learning problem, there\u2019s no way for the algorithm to validate these groups in any way. The groupings were made without any reference to what \u2018class\u2019 the points belonged, merely their position, and anyways, the computer wouldn\u2019t be able to tell what these groupings represented. If you wanted an interpretation of the clusters, a person would need to look at the groups and think about them, and there\u2019s no guarantee that a person would be able to immediately see what features typified the clusters that had been created \u2014 especially if there were more than two feature variables making the data hard to visualize.", "Another challenge is that the groupings you create depend on where your initial centroids land and also how many there are, and, again, you may not have a way of validating whether the grouping you find is the best possible one or if you even have an appropriate number of clusters! Consider this example, which also has three clusters, but the centroids start in different places:", "The red and green clusters end up both migrating towards the blob in the upper right, splitting it up between themselves, leaving the blue centroid all alone with two thirds of the data. The k-means algorithm has no way of knowing whether this set of clusters makes more or less sense than the first one we found. Similarly, we have to specify how many clusters we want to group the data into, we didn\u2019t need to choose 3, we could have chosen any number. Here\u2019s another grouping with 5 clusters instead of 3:", "In some cases you might have a specific reason to choose one particular number of groups \u2014 perhaps you are dividing customer support cases up among some number of support staff so you just want as many groupings as staff members \u2014 but generally one of the challenges of unsupervised clustering is deciding on the number of clusters.", "One other thing to bear in mind when setting up a k-means algorithm or an agglomerative one which we\u2019ll discuss shortly is that there are actually a few different ways to calculate the \u2018distance\u2019 between two points in abstract space. Maybe the most common is \u2018Euclidean distance\u2019 which you\u2019ll remember from the Pythagorean Theorem. There\u2019s also \u2018Manhattan Distance\u2019 so-called because it adds up the distance along each axis individually as if you were walking from one intersection in Manhattan to another along the grid of streets at right angles. More generally, Euclidean and Manhattan distances are actually both specific cases of the general metric called Minkowski Distance. If you\u2019re dealing with many-dimensional but sparse data, you might conceivably also use Cosine Similarity as a measure of similarity \u2014 while not really a measure of distance per se, Cosine Similarity gets at something similar in a way that can be computationally quicker if you have a great many variables and each row tends to be \u2018sparse\u2019 (perhaps you\u2019re looking at customers who each may purchased a handful of products from an online store that has many thousands of different products). Different measures of distance may also yield slightly different results in the final groupings.", "K-means is sort of a \u2018top-down\u2019 approach; it groups everything into clusters at once and then tweaks the clusters over a number of steps. Agglomerative clustering is sort of bottom-up; it starts with no clusters, just all of the individual points, and then slowly groups points together one at a time. The process is simple enough. First you calculate the distance between each point and every other point. Choose the points that are closest to each other and group them together. You continue to repeat the process, calculating the distance between any individual point or cluster to all the other points and clusters, slowly joining points together into clusters and clusters into other clusters until you have only as many clusters left as you set out to find. As an illustration, here is an example of the algorithm at work on a smaller dataset with only 20 points:", "Here is how this agglomerative algorithm ends up dividing the first data set (you\u2019ll see a few points floating out by themselves that appear to have been grouped by themselves, but the algorithm has actually settled on three clusters, and those points are actually being grouped with the bigger nearby groups. Their seeming to float by themselves is a quirk of the kernel density estimator I used to automatically draw the enclosures around the clusters):", "With an algorithm like this, in addition to deciding on a measure of distance between two points, you actually also need to decide how you measure the distance between two clusters. You can measure the shortest distance between a point in one cluster to a point in the other, sort of like the distance between the inside edges of the two clusters \u2014 this is called \u2018single linkage\u2019. You could also measure the furthest distance between a point in one to a point in the other \u2014 this is called \u2018complete linkage\u2019. Or you could take the average distance between any point in one cluster to any point in the other \u2014 \u2018average linkage\u2019. Choosing a different measure of distance or type of linkage may result in different grouping decisions.", "One benefit of this style of clustering is that it is stable. It doesn\u2019t depend on a random seed like the k-means method. Once you decide on a measure of distance and a number of clusters, it will always end up with the same clusters. One downside is that it can be computationally expensive. You need to measure the distance between every two set of points (for n points the formula for the number of such links is n(n-1)/2) and that\u2019s just in order to decide on the first linkage you make!", "Deciding on a number of clusters", "I mentioned earlier that deciding on a number of clusters was one of the challenges of these clustering algorithms. So, how would you go about actually choosing the number of clusters? There is no hard and fast rule, but there are a couple of measures you can use to guide your choices. Metrics to judge clusters tend to focus on how \u2018tight\u2019 the clusters are, that is, how similar the points within them are. One simple metric might be the Within cluster Sum of Square (WSS) which will be larger if the points within the group are more spread out and smaller if the points are closer together. The challenge with using this method on its own is that as you add more clusters the WSS will always go down. You could minimize WSS by having as many clusters as points, then the WSS would be 0!", "How do you know when to stop adding clusters? The goal is to find a number of clusters where adding one more cluster no longer has a really significant benefit to the WSS measure. If the clusters are well defined, it will generally be pretty obvious when this is the case. Consider this dataset, generated to have 4 reasonably distinct blobs:", "Now I\u2019ll use k-means to group these in to clusters numerous times, each time increasing the number of clusters. I\u2019ll record the WSS value for each of these and then I plot it against the number of clusters to get this graph:", "You\u2019ll notice that my WSS measure drops a lot when I increase the number of clusters from 2 to 3, and again from 3 to 4, but not very much for each subsequent increase. Once I hit 4 clusters, the algorithm generally finds the reasonable groupings. When I add more clusters above 4, the algorithm starts to split up the natural clusters and the marginal benefit for my compactness measure isn\u2019t as large.", "This sort of method to determine the appropriate number of clusters is sometimes called an \u2018elbow method\u2019; you\u2019re looking for the crook or elbow in the graph where increasing k no longer yields big steps in terms of compactness. This is a sensible place to start, but no way of choosing a number of clusters is infallible. This method may be tricked by clusters that overlap, are a little fuzzy or are very different sizes.", "While these different algorithms are generally getting at the same sort of thing \u2014 namely, tightly grouped clusters \u2014 they go about the task in different ways and so respond to different shapes and distributions in different ways. As a maybe extreme example, consider these points:", "Perhaps I\u2019ve given the game away a little bit by color coding them, but I think many people would agree that there do seem to be two distinct classes here: an inner ring and an outer one. How would the k-means algorithm handle these points? Well, the results are not great:", "This example may strike you as a little contrived, perhaps even unfair. Can a simple algorithm possibly pick out the nested shapes? The answer, though, is actually yes; in this case the agglomerative method using single linkage to find the distance between clusters actually does identify the nested rings:", "There is, unfortunately, no master clustering algorithm that works equally well for all shapes, but after working and playing with clustering methods for a while, hopefully you\u2019ll develop an intuition for which situations are better served by one algorithm or one measure of distance.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b290967f746&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----5b290967f746--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b290967f746--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----5b290967f746---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b290967f746&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&user=Max+Miller&userId=dfd5ba1a8332&source=-----5b290967f746---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b290967f746&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&source=-----5b290967f746---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "\u2018Manhattan Distance\u2019"}, {"url": "https://en.wikipedia.org/wiki/Minkowski_distance", "anchor_text": "Minkowski Distance"}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "Cosine Similarity"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5b290967f746---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/unsupervised-learning?source=post_page-----5b290967f746---------------unsupervised_learning-----------------", "anchor_text": "Unsupervised Learning"}, {"url": "https://medium.com/tag/clustering?source=post_page-----5b290967f746---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://medium.com/tag/k-means?source=post_page-----5b290967f746---------------k_means-----------------", "anchor_text": "K Means"}, {"url": "https://medium.com/tag/data-science-ground-up?source=post_page-----5b290967f746---------------data_science_ground_up-----------------", "anchor_text": "Data Science Ground Up"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b290967f746&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&user=Max+Miller&userId=dfd5ba1a8332&source=-----5b290967f746---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b290967f746&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&user=Max+Miller&userId=dfd5ba1a8332&source=-----5b290967f746---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b290967f746&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----5b290967f746--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b290967f746--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----5b290967f746---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=-----5b290967f746---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Written by Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=post_page-----5b290967f746--------------------------------", "anchor_text": "409 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----5b290967f746---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-clustering-algorithms-5b290967f746&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=-----5b290967f746---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----5b290967f746----0---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----5b290967f746----0---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----5b290967f746----0---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Max Miller"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5b290967f746----0---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----5b290967f746----0---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "The Basics: KNN for classification and regressionBuilding an intuition for how KNN models work"}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----5b290967f746----0---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "\u00b711 min read\u00b7Oct 18, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&user=Max+Miller&userId=dfd5ba1a8332&source=-----c1e8a6c955----0-----------------clap_footer----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----5b290967f746----0---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=-----5b290967f746----0-----------------bookmark_preview----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b290967f746----1---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5b290967f746----1---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5b290967f746----1---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5b290967f746----1---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b290967f746----1---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b290967f746----1---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b290967f746----1---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----5b290967f746----1-----------------bookmark_preview----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b290967f746----2---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----5b290967f746----2---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----5b290967f746----2---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5b290967f746----2---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b290967f746----2---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b290967f746----2---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b290967f746----2---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----5b290967f746----2-----------------bookmark_preview----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----5b290967f746----3---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----5b290967f746----3---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----5b290967f746----3---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Max Miller"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5b290967f746----3---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----5b290967f746----3---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "The Basics: Logistic Regression and RegularizationExtensions to the linear model"}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----5b290967f746----3---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": "\u00b79 min read\u00b7Nov 4, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&user=Max+Miller&userId=dfd5ba1a8332&source=-----828b0d2d206c----3-----------------clap_footer----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----5b290967f746----3---------------------06c0e1a2_10b0_4682_b115_0713a6ca006d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=-----5b290967f746----3-----------------bookmark_preview----06c0e1a2_10b0_4682_b115_0713a6ca006d-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----5b290967f746--------------------------------", "anchor_text": "See all from Max Miller"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b290967f746--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-perform-kmeans-clustering-using-python-7cc296cec092?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://zoumanakeita.medium.com/?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://zoumanakeita.medium.com/?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Zoumana Keita"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-perform-kmeans-clustering-using-python-7cc296cec092?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "How to Perform KMeans Clustering Using PythonA complete overview of the KMeans clustering and implementation with Python"}, {"url": "https://towardsdatascience.com/how-to-perform-kmeans-clustering-using-python-7cc296cec092?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "\u00b77 min read\u00b7Jan 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7cc296cec092&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-perform-kmeans-clustering-using-python-7cc296cec092&user=Zoumana+Keita&userId=e6ae785a30d&source=-----7cc296cec092----0-----------------clap_footer----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-perform-kmeans-clustering-using-python-7cc296cec092?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7cc296cec092&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-perform-kmeans-clustering-using-python-7cc296cec092&source=-----5b290967f746----0-----------------bookmark_preview----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Carla Martins"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "How to Compare and Evaluate Unsupervised Clustering Methods?Using Python, Scikit-Learn, and Google Colab"}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "\u00b720 min read\u00b7Feb 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&user=Carla+Martins&userId=a1022761a1b&source=-----84f3617e3769----1-----------------clap_footer----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/how-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84f3617e3769&operation=register&redirect=https%3A%2F%2Fcdanielaam.medium.com%2Fhow-to-compare-and-evaluate-unsupervised-clustering-methods-84f3617e3769&source=-----5b290967f746----1-----------------bookmark_preview----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Thomas A Dorfer"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Density-Based Clustering: DBSCAN vs. HDBSCANWhich algorithm to choose for your data"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "\u00b75 min read\u00b7Dec 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&user=Thomas+A+Dorfer&userId=7c54f9b62b90&source=-----39e02af990c7----0-----------------clap_footer----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----5b290967f746----0---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&source=-----5b290967f746----0-----------------bookmark_preview----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://patriziacastagnod.medium.com/k-means-clustering-python-4eec18b4f0ec?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://patriziacastagnod.medium.com/?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://patriziacastagnod.medium.com/?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Patrizia Castagno"}, {"url": "https://patriziacastagnod.medium.com/k-means-clustering-python-4eec18b4f0ec?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "k-Means Clustering (Python)This section is a simple example of the section: Unsupervised Learning, I recommend reading the theory first before moving on to this\u2026"}, {"url": "https://patriziacastagnod.medium.com/k-means-clustering-python-4eec18b4f0ec?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "\u00b74 min read\u00b7Dec 27, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4eec18b4f0ec&operation=register&redirect=https%3A%2F%2Fpatriziacastagnod.medium.com%2Fk-means-clustering-python-4eec18b4f0ec&user=Patrizia+Castagno&userId=d58cc64055d2&source=-----4eec18b4f0ec----1-----------------clap_footer----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://patriziacastagnod.medium.com/k-means-clustering-python-4eec18b4f0ec?source=read_next_recirc-----5b290967f746----1---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eec18b4f0ec&operation=register&redirect=https%3A%2F%2Fpatriziacastagnod.medium.com%2Fk-means-clustering-python-4eec18b4f0ec&source=-----5b290967f746----1-----------------bookmark_preview----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://medium.com/codex/understanding-dbscan-clustering-hands-on-with-scikit-learn-a95cb27f0408?source=read_next_recirc-----5b290967f746----2---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----5b290967f746----2---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://cdanielaam.medium.com/?source=read_next_recirc-----5b290967f746----2---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Carla Martins"}, {"url": "https://medium.com/codex?source=read_next_recirc-----5b290967f746----2---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/understanding-dbscan-clustering-hands-on-with-scikit-learn-a95cb27f0408?source=read_next_recirc-----5b290967f746----2---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Understanding DBSCAN Clustering: Hands-On With Scikit-LearnUnsupervised Learning \u2014 Clustering"}, {"url": "https://medium.com/codex/understanding-dbscan-clustering-hands-on-with-scikit-learn-a95cb27f0408?source=read_next_recirc-----5b290967f746----2---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "\u00b75 min read\u00b7Dec 21, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2Fa95cb27f0408&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Funderstanding-dbscan-clustering-hands-on-with-scikit-learn-a95cb27f0408&user=Carla+Martins&userId=a1022761a1b&source=-----a95cb27f0408----2-----------------clap_footer----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://medium.com/codex/understanding-dbscan-clustering-hands-on-with-scikit-learn-a95cb27f0408?source=read_next_recirc-----5b290967f746----2---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa95cb27f0408&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Funderstanding-dbscan-clustering-hands-on-with-scikit-learn-a95cb27f0408&source=-----5b290967f746----2-----------------bookmark_preview----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d?source=read_next_recirc-----5b290967f746----3---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://anmol3015.medium.com/?source=read_next_recirc-----5b290967f746----3---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://anmol3015.medium.com/?source=read_next_recirc-----5b290967f746----3---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Anmol Tomar"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5b290967f746----3---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d?source=read_next_recirc-----5b290967f746----3---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "Stop Using Elbow Method in K-means Clustering, Instead, Use this!Learn how to find the number of clusters in K-means clustering"}, {"url": "https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d?source=read_next_recirc-----5b290967f746----3---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": "\u00b76 min read\u00b7Nov 17, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc820da0631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d&user=Anmol+Tomar&userId=d80580992695&source=-----fc820da0631d----3-----------------clap_footer----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d?source=read_next_recirc-----5b290967f746----3---------------------1a44b967_0bfc_4b15_bb76_e991b4e18406-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc820da0631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d&source=-----5b290967f746----3-----------------bookmark_preview----1a44b967_0bfc_4b15_bb76_e991b4e18406-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----5b290967f746--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b290967f746--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----5b290967f746--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}