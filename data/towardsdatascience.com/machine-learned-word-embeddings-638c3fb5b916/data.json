{"url": "https://towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916", "time": 1683006467.716084, "path": "towardsdatascience.com/machine-learned-word-embeddings-638c3fb5b916/", "webpage": {"metadata": {"title": "Machine-learned Word Embeddings. An introduction minus the math | by Arun Jagota | Towards Data Science", "h1": "Machine-learned Word Embeddings", "description": "Words are key constructs in many natural languages. They are \u201cloaded\u201d with meaning. Often varying widely by context. Take python. It means something entirely different in computer programming versus\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/word-and-text-representations-in-natural-language-processing-33349649be2e", "anchor_text": "Word and Text Representations in Natural Language Processing", "paragraph_index": 3}], "all_paragraphs": ["Words are key constructs in many natural languages. They are \u201cloaded\u201d with meaning. Often varying widely by context. Take python. It means something entirely different in computer programming versus biology.", "To master a language, people and computers start by mastering the key words. This post seeks to explain how computers master words.", "There is a long history of representing words in forms suited for natural language processing or for information retrieval (search). Two broad categories of representations have emerged: local and distributed.", "This post focuses on representations that are (i) distributed and (ii) machine-learned. Readers unfamiliar with local or other distributed representations might want to read Word and Text Representations in Natural Language Processing first.", "From this point on, we switch to using the term word embeddings. This is the term commonly used to denote \u201cmachine-learned distributed word representations\u201d.", "Reason 1. Accurate and rich representations of words can be learned solely from a rich corpus of documents. Take an example. From the Wikipedia corpus, we can machine-learn good word embeddings of millions of distinct words. Just run the algorithm on them. Out pop the representations.", "Reason 2. Domain-specific word embeddings can be learned merely by training on a domain-specific corpus. For example, from a corpus of documents on computer programming, a words-embedding learner can learn the meaning of python in computer programming contexts. This meaning, of course, is completely different than in non-programming contexts.", "Imagine how long it would take a person who knows nothing about a certain domain, say computer programming, to understand its vocabulary? Even at a basic level.", "At a high-level, it\u2019s clear. We want to learn \u201caccurate and rich\u201d representations of words. A bit too general though. How would we know whether (and when) the algorithm has learned well enough? People constantly interact with other people so their lack of understanding gets exposed quickly. Computers are not there yet. So we\u2019ll want to specify the learning objective with more specificity.", "How about this? We\u2019d like to learn representations in which similar words have similar representations. Well, this also seems a bit broad. Actually, we have gained useful specificity.", "It is easy to construct a test set of pairs of words, each pair labeled \u201csimilar\u201d or \u201cnot similar\u201d. Below are some examples", "The labeling can be refined, for example, we could introduce multiple levels of similarity or dissimilarity.", "Learning Problem Formulation: We Are Not There Yet", "Readers familiar with machine learning might be inclined to think that the aforementioned test set also specifies the machine learning problem. Not so. Let\u2019s explain.", "Let\u2019s see what we get if we take the form of the test set to be a literal specification of the learning problem. We get: train a binary classifier that inputs two words and outputs whether they are similar or not.", "While this is a worthwhile problem to solve, this is not our goal here. We want the input to be a single word. We want the algorithm to output an accurate and rich representation of this word. The test set just serves as an indirect means of assessing how good the learned model is towards this task.", "What is \u201ccontext\u201d? The other words in whose proximity the word frequently occurs. The context of cat would include whiskers. But not computer.", "It is reasonable to suppose that words whose contextual representations are similar are themselves similar. So a solution to P2 is also a good solution to P1.", "We\u2019ve made progress. Our earlier formulation involved inputting two words. This just one!", "Hmm. How do we get a training set of (word,context) pairs? At this point, we don\u2019t even know how to stuff the concept of context into a variable that can serve as our prediction target!", "Okay, let\u2019s tackle \u201cstuffing context into a variable\u201d first. We\u2019ll model a word\u2019s context as a probability distribution over the words in the lexicon. Take cat as an example. The probability that whiskers is in cat\u2019s context should be high. The probability that computer is in cat\u2019s context should be low.", "The learning approach we describe in this post is best described in a neural network setting. Neural networks like to operate on vector spaces. That is, they map input vectors to output vectors (or scalars), usually with some magic happening in between.", "We have the output covered. A probability distribution over words in a lexicon is a vector in a space whose dimensions are the words. (So the vector space of a one-million-word lexicon would have one-million dimensions.) The value of a component of the vector is simply the probability of the corresponding word.", "What about the input? It\u2019s a particular word. This may also be expressed as a probability vector in this same space. The dimension associated with the word has value 1, the rest 0.", "So our current architecture may be depicted as", "I and O are probability vectors on the same space and \u201cmagic\u201d is whatever happens in between.", "To learn, we need a training set of (word, context) pairs. Not obvious how to build. Words don\u2019t come labeled with their context.", "We can estimate a word\u2019s context vector from a representative corpus of documents. Words that occur (frequently enough) in this word\u2019s proximity are in its context, those that don\u2019t aren\u2019t.", "What do we mean by \u201crepresentative\u201d? We mean a corpus that matches what we are trying to learn. To learn computer programming terms, our corpus should be composed of computer programming documents.", "Building the aforementioned training set involves learning, from the corpus, the context vectors of the various words. We can simplify this. Instead, we can work with a training set of pairs of words that occur sufficiently nearby, for example in the same sentences. Denote such a pair (word1,word2). We interpret this as an instance in which word2 occurs in word1\u2019s context.", "The training instances are restricted to the bolded ones. (Think of the rest as stop words.)", "Streaming such pairs from all sentences in all documents in our corpus to the learning algorithm will, effectively, reveal each word\u2019s context vector to the algorithm.", "This simplification benefits us in two ways.", "First, it simplifies the process of building the training set. We can process each sentence independently. Simply emit the training pairs that result from it. We can even do this in a streaming, i.e. continuous-learning, setting. Imagine new documents arriving continuously, as to the indexer of a web-scale search engine. We can stream out new training instances from them on-the-fly.", "Second, it benefits the learning algorithm. The target associated with an input is the probability vector whose mass is concentrated on one word. This target is crisper than an arbitrary probability vector.", "Okay, now to the learning algorithm. All we will see is one. It will suffice to convey the main ideas. That said, those seeking to build industrial-strength word embeddings on large lexicons from large corpora will need to read more.", "The Continuous Bag Of Words Model", "This description is somewhat simplified from the original one, though just fine for the problem we are addressing in this post.", "This a neural network with one hidden layer.", "Okay, we\u2019ve substituted H for magic. Magic indeed!", "What happens beneath the two \u21d2 symbols? This is described below.", "Let\u2019s explain how the data flows from input to output. We start with the input vector I. We multiply I by the matrix W1. This gives us a hidden vector H. Next, we multiply H by the matrix W2. The resulting vector isn\u2019t necessarily a probability vector yet. We apply the probability_normalizer to it to transform it into one. This becomes our output vector O.", "Some questions remain. First, where is the learning happening? Second, this probability_normalizer concept is a bit abstract.", "Response to the first question. Actually what we have described so far is the input-to-output data flow. We\u2019ll get to the actual learning in a minute or two. The parameters we will learn will be the values in the matrices W1 and W2.", "Response to the second question. Below are two concrete examples of probability normalizers. Both assume the input is a vector with nonnegative values.", "The first one, let\u2019s call it simple, just takes each value and divides it by the sum of all the values. The second one, known as softmax, first exponentiates the values, then applies the simple normalizer. The softmax one is the one to use in practice. Why? We\u2019ll give some intuition after we describe the actual learning algorithm.", "Here are examples of the two normalizer functions.", "In neural network terminology, the probability normalizer would be an instance of a vector activation function.", "Whereas the process of learning will involve learning to predict O from I, once we have learned sufficiently well, we will only extract out the weight matrix W1. We will throw away W2.", "We only care about the mapping from I to H. H will serve as the representation of the word represented by I. (That said, to learn a good mapping I \u2192 H, we have no choice but to solve the full learning problem as we don\u2019t have targets on H, only on O.)", "Another key point is that we will intentionally limit H\u2019s dimensionality to be much less than that of I and O. Whereas I and O may have millions of dimensions, we will force H to have a few hundred, possibly a thousand, maybe less. The idea is to force the learning algorithm to learn dense word representations.", "The reasoning goes like this. First, is this even feasible? Yes. Words have structure, else there wouldn\u2019t be any notion of word similarity, to begin with. So we should be able to compress the I-space down to one whose dimensionality is far lower. Indeed it\u2019s hard to imagine more than twenty orthogonal latent properties that describe words! Maybe we are just not imaginative enough. So let\u2019s multiply this five-fold. Even better, a hundred-fold. This is still in the low thousands!", "Okay, so we think we have settled the feasibility issue. Next, why would we want to do it? (Just because it is feasible doesn\u2019t mean it\u2019s a good idea.) The reason is that dense representations generalize better. The intuition is simple: if we think words have way fewer latent characteristics than their number, then forcing the learner to map to a low-dimensionality space will generalize better than try to learn a mapping to the original vector space.", "We\u2019ll focus on conveying the intuition, with a simple example. No math.", "Our example is on a lexicon having just four words A, B, C, D. We\u2019ll set H to have two dimensions. We\u2019ll use the training set", "We\u2019ll start our learning from a blank state, initializing all the values in W1 and W2 randomly to small positive or negative values. Next, we\u2019ll present the input A of the first training instance, under its encoding 1000. All the learnable weights are near 0; so H will be (~0,~0) and W2*H will be (~0,~0,~0,~0). We\u2019ll suppose the probability_normalizer activation function maps this H to (~\u00bc,~\u00bc,~\u00bc,~\u00bc). The target is B, specifically, it's encoding (0,1,0,0). This target is quite different from the prediction. So we need to learn.", "What happens next is best conveyed with a picture.", "In the figure, T denotes the target output. The +s and the -s on the arcs from H to O denote the direction in which the weights should be changed based on the difference between O and T. We see that the weights to B\u2019s output should be increased; the rest decreased. (This is because B\u2019s output is too low; the output of the rest too high.)", "We are not done yet. We also need to calculate the adjustments to the weights from I to H. Why? Because the values of these weights could also be contributing to the error we see at the outputs.", "These adjustments are calculated via the famous backpropagation algorithm. It is this algorithm, leveraged within this hidden layer-based architecture with suitably chosen activation functions (in our case the probability normalizer), that delivers the magic.", "What happens next is the easiest to describe numerically. Let H1 denote the first component of H. H1\u2019s current value is 0.1. Consider B\u2019s output. The error it sees is ~ 1\u2013\u00bc = \u00be . If we could somehow put a sensible target on H1 in this situation, we would know the direction in which to move the weights from I to H1. If H1\u2019s target was greater than 0.1, we would want to lower the weights. If H1\u2019s target was less than 0.1, we would want to increase the weights.", "Well, estimating H1\u2019s target is the same as estimating the error H1 sees. This we can estimate, as follows. Consider a particular output node. Its error is the target output minus its actual output. We backpropagate this error through the probability_normalizer first, then through the weight from H1 to this output node. This gives us the error that H1 sees from this output node. The sum of these errors, over the output nodes. is the total error that H1 sees.", "Let\u2019s work through this step by step. Three of the outputs have an error -\u00bc each, one of them (B) has error \u00be. Imagine that these errors backpropagate proportionally through the probability_normalizer. (We are only asking you to imagine, not to assume.) Plus recall that the weights from H1 to O are random with small magnitudes. Given these two, summing up these errors at H1 will approximately cancel them out, i.e. yield close to 0.", "Similarly, the error that H2 sees will also be close to 0. Is this disconcerting? No. At this point, neither H1 or H2 knows what it wants to be. The H1 and H2 errors, while likely close to 0, will likely not be the same. So the weights from I to H1 and H2 will likely undergo at least slightly different adjustments. This will start the process of H1 and H2 diverging in what they learn. Over many training instances, in our case that means multiple iterations over the same 4 training instances, H1 and H2 will learn to pick out different things from the output. The phenomenon that kicks this off is called symmetry breaking.", "Oh, we forgot. We promised to explain this. The intuition is that we want to encourage the learning to draw distinctions. Think this way. We have a pool of neurons in the hidden layer. Initially, they have no idea what they should be doing. If we don\u2019t encourage them to start discriminating, they may stay non-committed. We all know how that can go. Ever been on a committee in which no one wants to stand out?", "Well, the softmax function is like an executive who pushes people to explore outside their comfort zone. The exponentiation in the softmax has the effect of amplifying differences. So slight winners (losers) become strong winners (losers).", "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013b). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111\u20133119.", "PhD, Computer Science, neural nets. 14+ years in industry: data science algos developer. 24+ patents issued. 50 academic pubs. Blogs on ML/data science topics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F638c3fb5b916&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://jagota-arun.medium.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----638c3fb5b916---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F638c3fb5b916&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&user=Arun+Jagota&userId=ef9ed921edad&source=-----638c3fb5b916---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F638c3fb5b916&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&source=-----638c3fb5b916---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/word-and-text-representations-in-natural-language-processing-33349649be2e", "anchor_text": "Word and Text Representations in Natural Language Processing"}, {"url": "https://arxiv.org/pdf/1411.2738.pdf", "anchor_text": "https://arxiv.org/pdf/1411.2738.pdf"}, {"url": "https://medium.com/tag/nlp?source=post_page-----638c3fb5b916---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----638c3fb5b916---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/bag-of-words?source=post_page-----638c3fb5b916---------------bag_of_words-----------------", "anchor_text": "Bag Of Words"}, {"url": "https://medium.com/tag/word-representations?source=post_page-----638c3fb5b916---------------word_representations-----------------", "anchor_text": "Word Representations"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F638c3fb5b916&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&user=Arun+Jagota&userId=ef9ed921edad&source=-----638c3fb5b916---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F638c3fb5b916&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&user=Arun+Jagota&userId=ef9ed921edad&source=-----638c3fb5b916---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F638c3fb5b916&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----638c3fb5b916---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=-----638c3fb5b916---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Written by Arun Jagota"}, {"url": "https://jagota-arun.medium.com/followers?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "685 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----638c3fb5b916---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learned-word-embeddings-638c3fb5b916&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=-----638c3fb5b916---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----638c3fb5b916----0---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----638c3fb5b916----0---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----638c3fb5b916----0---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Arun Jagota"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----638c3fb5b916----0---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----638c3fb5b916----0---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Text Sentiment Analysis in NLPProblems, use-cases, and methods: from simple to advanced"}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----638c3fb5b916----0---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "\u00b717 min read\u00b7Jul 19, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce6baba6d466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-sentiment-analysis-in-nlp-ce6baba6d466&user=Arun+Jagota&userId=ef9ed921edad&source=-----ce6baba6d466----0-----------------clap_footer----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----638c3fb5b916----0---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce6baba6d466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-sentiment-analysis-in-nlp-ce6baba6d466&source=-----638c3fb5b916----0-----------------bookmark_preview----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----638c3fb5b916----1---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----638c3fb5b916----1---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----638c3fb5b916----1---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----638c3fb5b916----1---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----638c3fb5b916----1---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----638c3fb5b916----1---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----638c3fb5b916----1---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----638c3fb5b916----1-----------------bookmark_preview----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----638c3fb5b916----2---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----638c3fb5b916----2---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----638c3fb5b916----2---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----638c3fb5b916----2---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----638c3fb5b916----2---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----638c3fb5b916----2---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----638c3fb5b916----2---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----638c3fb5b916----2-----------------bookmark_preview----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----638c3fb5b916----3---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----638c3fb5b916----3---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----638c3fb5b916----3---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Arun Jagota"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----638c3fb5b916----3---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----638c3fb5b916----3---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "Named Entity Recognition in NLPReal-world use cases, models, methods: from simple to advanced"}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----638c3fb5b916----3---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": "\u00b730 min read\u00b7Jul 9, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&user=Arun+Jagota&userId=ef9ed921edad&source=-----be09139fa7b8----3-----------------clap_footer----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----638c3fb5b916----3---------------------472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=-----638c3fb5b916----3-----------------bookmark_preview----472b86f0_9ca8_4438_a9e4_7bb31b89c6fa-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "See all from Arun Jagota"}, {"url": "https://towardsdatascience.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Angel Das"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in PythonIntroduction to embeddings in natural language processing using Artificial Neural Network and Gensim"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "\u00b713 min read\u00b7Nov 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&user=Angel+Das&userId=8418ab50405a&source=-----a8873b225ab6----0-----------------clap_footer----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&source=-----638c3fb5b916----0-----------------bookmark_preview----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----638c3fb5b916----1-----------------bookmark_preview----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----0-----------------clap_footer----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----638c3fb5b916----0---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----638c3fb5b916----0-----------------bookmark_preview----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----638c3fb5b916----1---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----638c3fb5b916----1-----------------bookmark_preview----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/openai-releases-embeddings-ai-3380dacfa3c5?source=read_next_recirc-----638c3fb5b916----2---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://ithinkbot.com/?source=read_next_recirc-----638c3fb5b916----2---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://ithinkbot.com/?source=read_next_recirc-----638c3fb5b916----2---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Dr. Mandar Karhade, MD. PhD."}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----638c3fb5b916----2---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/openai-releases-embeddings-ai-3380dacfa3c5?source=read_next_recirc-----638c3fb5b916----2---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "OpenAI Releases Embeddings model: text-embedding-ada-002It is Powerful, cheaper, and more flexible!"}, {"url": "https://pub.towardsai.net/openai-releases-embeddings-ai-3380dacfa3c5?source=read_next_recirc-----638c3fb5b916----2---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "\u00b75 min read\u00b7Dec 16, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F3380dacfa3c5&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fopenai-releases-embeddings-ai-3380dacfa3c5&user=Dr.+Mandar+Karhade%2C+MD.+PhD.&userId=aa38fec37f51&source=-----3380dacfa3c5----2-----------------clap_footer----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/openai-releases-embeddings-ai-3380dacfa3c5?source=read_next_recirc-----638c3fb5b916----2---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3380dacfa3c5&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fopenai-releases-embeddings-ai-3380dacfa3c5&source=-----638c3fb5b916----2-----------------bookmark_preview----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----638c3fb5b916----3---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----638c3fb5b916----3---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----638c3fb5b916----3---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----638c3fb5b916----3---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----638c3fb5b916----3---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----638c3fb5b916----3---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----3-----------------clap_footer----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----638c3fb5b916----3---------------------34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----638c3fb5b916----3-----------------bookmark_preview----34ec3014_dedd_4f24_b76a_30dc4f0a13d6-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----638c3fb5b916--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}