{"url": "https://towardsdatascience.com/activation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456", "time": 1683013089.743834, "path": "towardsdatascience.com/activation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456/", "webpage": {"metadata": {"title": "Activation Functions in Deep Learning: From Softmax to Sparsemax \u2014 Math Proof | by Ester Hlav | Towards Data Science", "h1": "Activation Functions in Deep Learning: From Softmax to Sparsemax \u2014 Math Proof", "description": "The objective of this post is three-fold. The first part discusses the motivation behind sparsemax and its relation to softmax, summary of the original research paper in which this activation\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "\u201cFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\u201d", "paragraph_index": 1}, {"url": "https://projecteuclid.org/download/pdf_1/euclid.aos/1079120130", "anchor_text": "Zhang, Tong. Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization. Annals of Statistics, pp. 56\u201385, 2004", "paragraph_index": 6}, {"url": "http://ww.web.stanford.edu/~hastie/Papers/margin.pdf", "anchor_text": "Zou, Hui, Zhu, Ji, and Hastie, Trevor. The Margin Vector, Admissible Loss and Multi-class Margin-Based Classifiers. Technical report, Stanford University, 2006", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need, Vaswani et al., 2017", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, by Devlinet al, 2018", "paragraph_index": 10}, {"url": "http://github.com/EsterHlav", "anchor_text": "github.com/EsterHlav", "paragraph_index": 61}], "all_paragraphs": ["The objective of this post is three-fold. The first part discusses the motivation behind sparsemax and its relation to softmax, summary of the original research paper in which this activation function was first introduced, and an overview of advantages from using sparsemax. Part two and three are dedicated to the mathematical derivations, concretely finding a closed-form solution as well as an appropriate loss function.", "In the paper \u201cFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\u201d, Martins et al. propose a new alternative to the widely known softmax activation function by introducing Sparsemax.", "While softmax is an appropriate choice for multi-class classification that outputs a normalized probability distribution over K probabilities, in many tasks, we want to obtain an output that is more sparse. Martins et al. introduce a new activation function, called sparsemax, that outputs sparse probabilities of a multinomial distribution and, therefore, filters out noise from the mass of the distribution. This means that sparsemax would assign a probability of exactly 0 for some classes, while softmax would instead keep those classes and assign them very small values like 10\u207b\u00b3. Sparsemax can be especially favorable in large classification problems; for instance in Natural Language Processing (NLP) tasks, where the softmax layer is modeling a multinomial distribution over a very large vocabulary set.", "In practice, however, changing the softmax function into a sparse estimator is not a straightforward task. Obtaining such a transformation while preserving some of the fundamental properties of softmax \u2014 e.g. simple to evaluate, inexpensive to differentiate and easily transformed to a convex loss function \u2014 turns out to be quite challenging. A traditional way around it in machine learning is to use the L1 penalty that allows for some level of sparsity with regards to the input variables and/or deep layers in neural networks. While this approach is relatively straightforward, L1 penalty influences the weights of a neural network rather than the targeted outputs as sparse probabilities. Therefore, Martins et al. recognize the need for a supplementary activation function, i.e. sparsemax, which they formulate as a solvable quadratic problem and find a solution under a set of constraints to get similar properties to softmax.", "Before diving into the proofs behind sparsemax implementation, let us first discuss few important high-level findings from the paper. The following bullet points summarize some of the main takeaways:", "While softmax shape is equivalent to the traditional sigmoid, sparsemax is a \u201chard\u201d sigmoid in one dimension. Additionally, in two dimensions, sparsemax is a piecewise linear function with entire saturated zones (0 or 1). Here is a figure from the paper to help you visualize softmax and sparsemax.", "The derived sparsemax loss function in a binary case is directly related to the modified Huber loss used for classification (defined in Zhang, Tong. Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization. Annals of Statistics, pp. 56\u201385, 2004 and Zou, Hui, Zhu, Ji, and Hastie, Trevor. The Margin Vector, Admissible Loss and Multi-class Margin-Based Classifiers. Technical report, Stanford University, 2006). That is, if x and y are the two scores before sparsemax, using a sparsemax layer and a sparsemax loss, with t = x - y, and assuming without loss of generality that the correct label is 1, we can show that:", "This is a nice property that proves the theoretical foundation of sparsemax; Huber loss is a tradeoff between L1 and L2 penalties, which is exactly what we are trying to obtain from the softmax activation while including sparsity. Additionally, this similarity to Huber loss can be demonstrated by comparing the loss to other standard classification losses:", "In the above graph, you can see that for negative values of t, i.e. for cases of big error, the loss is linearly scaling with the error, similarly to the hinge loss. However, as t converges to 1, i.e. the error diminishes, we observe a squared relationship, similar to the least squares loss.", "The sparsemax framework has been shown to perform particularly well on datasets with large number of labels. In the example below, you can see several datasets and their details in Table 1 and the micro-averaged/macro-averaged F1 scores for different activation functions, i.e sigmoid, softmax, and sparsemax, in Table 2. We observe that with an increasing number of labels (i.e. lower rows), the sparsemax performance boost becomes more and more apparent compared to softmax.", "The idea of sparse outputs can also be leveraged in deep learning models with attention mechanisms \u2014 a class of neural networks computing attention weights over a potentially large number of entities. Such attention mechanisms are proved to be particularly efficient in NLP tasks like translation or language modeling, which led to the creation of so-called Transformers, i.e. non-recurrent model architecture leveraging self-attention (more details in Attention Is All You Need, Vaswani et al., 2017), widely used in state-of-the-art language models like BERT (see BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, by Devlinet al, 2018). Obtaining strict null probabilities from sparsemax has the advantage of completely removing the influence of some hidden states (words) if they are judged as irrelevant \u2014 compared to softmax, where the sum of infinitesimal contributions of all irrelevant states eventually accumulates and can affect the model performance. Furthermore, with zero probabilities, we amplify one major advantage of attention: explainability. Using sparse scores helps to clean up the attention maps and clarifies how the attention system is working.", "Empirically, however, the paper reports only small performance increase due to the sparse attention in natural language inference task.", "Now that we have highlighted some of the sparsemax advantages and key findings of the paper, let us proceed to two important derivations behind sparsemax: namely, finding its closed-form solution as well as the equation for its loss function.", "Softmax is a generalization of sigmoid to multi-class classification. It uses a logit transformation to map all scores z\u200b to probabilities \u200bp\u2208[0,1]:", "Conceptually, for a set of K\u200b classes, softmax is a function mapping vectors in \u200b \u200b\u200b\u200b\u211d\u1d37 to a probability distribution in \u0394\u1d37\u00af\u00ad\u00b9\u200b, i.e. to the K\u200b-1 dimensional probability simplex. More precisely : \u200b", "It is important to note that only K-1\u200b degree of liberty are necessary as probabilities always sum up to 1.", "Softmax is defined to have full support, i.e. non-zero value outputs, mathematically defined as", "Modifying this property to allow for zero outputs is exactly what will enable us to obtain sparse probabilities.", "The authors formulate sparsemax activation function as a quadratically constrained optimization problem:", "This is equivalent to defining it as the Euclidian projection of z\u200b onto the probability simplex \u0394\u1d37\u00af\u00ad\u00b9\u200b. The sparsity is invoked by the probability of hitting the boundary of the simplex during the projection being high, and thus making certain dimensions zero.", "The above definition of sparsemax can be written in its closed-form solution as", "with \ud835\udf0f representing the threshold function. We will formally derive this equation step-by-step in section 3.", "Similarly, \ud835\udf0f can also be expressed in its closed-form solution as", "The pseudocode of Algorithm 1 below summarizes this set of equations and can help to better understand the steps behind the sparsemax computation of a vector z:", "The challenging part is to determine the threshold value \ud835\udf0f(z)\u200b; we will come back to this during our proof in section 3. Finally, the outputted probability for each class i\u200b is z minus the threshold \u200b\ud835\udf0f(z),\u200b if the value is positive, and 0, if it is negative.", "Finally, we also want to derive the loss function corresponding to sparsemax; this will be part of section 4. While the first proof of closed-form solution was determined directly from the original definition of \u200bsparsemax, the loss function is a question of preference and can take different forms. Let\u2019s explain why.", "It can be shown that using a cross-entropy loss (i.e. negative log-likelihood on a multinomial distribution) in conjunction with softmax, the loss function simplifies to", "with k equal to the index of the true label.", "The advantage that arises from using the combination of cross-entropy loss and softmax simplifies the gradient to", "This means that during backpropagation, evaluating softmax(z)\u200b is enough for both the forward and backward pass, and no extra computations are required. Such behavior is a property that we also want to maintain in the sparsemax framework.", "However, empirically, this setup becomes infeasible with sparsemax. One problem that arises from an attempt to combine sparsemax with cross-entropy is that this loss function would now require full-support, i.e only non-zero valued outputs. But since the loss function takes the logarithm of probabilities, if a probability is strictly null, then its log is not defined. That is why cross-entropy loss cannot be used for the sparsemax activation function. The authors propose to find a differentiable loss function that would satisfy a similar gradient expression, namely", "By adding a further constraint that the minimum of sparsemax loss is 0\u200b, obtained when S(z)={k},\u200b i.e. only the correct class is non-zero, we can show that the sparsemax loss function has the form", "The goal of this proof is to show the following equivalence:", "In words, we want to solve the arg min optimization problem of squared Euclidean norm of the difference between probability p and score z. This can be understood as choosing the closest point in \u0394\u1d37\u00af\u00ad\u00b9 from the score vector z.", "Karush\u2013Kuhn\u2013Tucker (KKT) conditions are a concept in mathematical optimization. They represent the first-order necessary conditions in order to satisfy a non-linear programming solution, given a set of specific constraints. In our sparsemax setup, we want to find the minimum point of some function \u200b\u200b\u200b\u200b\u200bf : \u211d\u207f \u2192 \u211d under certain conditions.", "The optimization problem can then be written as follows: find the x that minimizes function f such that the conditions on g(x) and h(x) are fulfilled, i.e.:", "In order to solve this problem, we first need to define the Lagrangian function L(x, \u03bc, \u03bb):", "The KKT approach states (at a high level) that given the Lagrangian function L, if (x*, \u03bc\u200b*) is a saddle point of L\u200b with\u200b \u03bc\u200b\u22650\u200b and complementary slackness \u03bc\u1d62g\u1d62(x*)\u22650\u200b \u2200 i\u2208 [0,n], then x*\u200b is the optimal vector of the optimization problem stated above.", "In a concrete manner, we simply look for the value at which the gradient of the Lagrangian is equal to zero, that is:", "Given that sparsemax\u200b is a constrained optimization problem, we rewrite it with the earlier notations of KKT, i.e. with f\u200b, g and h as follows:", "Then the Lagrangian takes the form", "We can now differentiate the Lagrangian function with regards to x:", "The solution becomes a system of three equations:", "The first equation (1) comes from the gradient of the Lagrangian being null. The second equation (2) comes from the original slackness condition that \u200b \u03bc\u22650\u200b and from p\u200b being a positive vector of probabilities. Finally, equation (3) is the complementary slackness condition.", "Subsequently, we differentiate between two cases based on equations (2) and (3). For each dimension i \u2208 [0, n], either p\u1d62*>0 and thus \u03bc\u1d62*=0 , or \u03bc\u1d62*>0 and thus p\u1d62*=0. More precisely, this means that we consider two cases: elements of the support S(z), where p>0, and elements outside of the support S(z), where p=0.", "As we continue this proof of sparsemax, we need to remember that we aim at two things: determining the value of non-zeros probabilities, and determining the conditions on which probabilities are 0. Thus:", "This concludes the first proof of sparsemax closed-form solution derivation.", "The goal of this second proof is to show the following equivalence:", "In other words, we want to derive the equivalence between the gradient of the loss function of sparsemax and the sparsemax loss function itself.", "Before starting the proof, we need to define some important notation and establish two important results:", "For lemma 1, we can directly compute the partial derivative of \ud835\udf0f\u00b2 with regards to z\u200b.", "Indeed, if z\u200b\u1d62 is in \u200bS(z), then it will be present in the numerator sum and its derivative will be of scale \u200binversely proportional to |S(z)|; otherwise, the derivative will be null.", "Next, using chain rule, we can infer the derivative of \ud835\udf0f\u00b2\u200b with regards to \u200bz:", "In Lemma 2, we are interested in the so-called overconfident sparsemax, i.e. when the prediction assigns 100% of the weight to only the true class \u200bk. In that case, we have that spar\u200bsemax(z,k)=\u03b4_k. This has two consequences, namely:", "We want to obtain the loss function of sparsemax such that", "First, let\u2019s look at the partial derivative of sparsemax with regard to z\u1d62\u200b in a non-vectorial form:", "We can then infer that for K\u2208 \u211d:", "The last remaining step is to determine the integration constant. We could simply pick \u200bK=0 and the gradient would still be correct, but we might be able to have a more appropriate solution. This is where we use the second lemma we defined above. In the case of perfect prediction, we would like the loss to be equal zero, similarly to softmax or other loss functions like MAE/MSE.", "More precisely, we need to meet the following requirement:", "This concludes the second proof on the derivation of sparsemax loss function.", "In this post, we introduced the idea and mathematical formulations behind the sparsemax activation function, which allows for sparser output domain than in the traditional softmax. We started by summarizing some of the key findings from the Martins et al. paper, concluding that, empirically, sparsemax can improve performance of classification models as the number of classes increases. Furthermore, performance boost as well as better explainability capabilities are prevalent in NLP attention models trained with sparsemax. Finally, the main portion of the post was dedicated to two important proofs behind sparsemax; namely the derivation of closed-form solution and the underlying loss function.", "Machine Learning Software Engineer, Mathematics Graduate | github.com/EsterHlav"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F50c1eb293456&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@EsterHlav?source=post_page-----50c1eb293456--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Ester Hlav"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7476ea235ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&user=Ester+Hlav&userId=7476ea235ae9&source=post_page-7476ea235ae9----50c1eb293456---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50c1eb293456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&user=Ester+Hlav&userId=7476ea235ae9&source=-----50c1eb293456---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50c1eb293456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&source=-----50c1eb293456---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "\u201cFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\u201d"}, {"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "\u201cFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\u201d"}, {"url": "https://projecteuclid.org/download/pdf_1/euclid.aos/1079120130", "anchor_text": "Zhang, Tong. Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization. Annals of Statistics, pp. 56\u201385, 2004"}, {"url": "http://ww.web.stanford.edu/~hastie/Papers/margin.pdf", "anchor_text": "Zou, Hui, Zhu, Ji, and Hastie, Trevor. The Margin Vector, Admissible Loss and Multi-class Margin-Based Classifiers. Technical report, Stanford University, 2006"}, {"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "\u201cFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\u201d"}, {"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "ource"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need, Vaswani et al., 2017"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, by Devlinet al, 2018"}, {"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1602.02068", "anchor_text": "\u201cFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\u201d"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----50c1eb293456---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----50c1eb293456---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----50c1eb293456---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----50c1eb293456---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----50c1eb293456---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50c1eb293456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&user=Ester+Hlav&userId=7476ea235ae9&source=-----50c1eb293456---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50c1eb293456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&user=Ester+Hlav&userId=7476ea235ae9&source=-----50c1eb293456---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50c1eb293456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=post_page-----50c1eb293456--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7476ea235ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&user=Ester+Hlav&userId=7476ea235ae9&source=post_page-7476ea235ae9----50c1eb293456---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2373198e349f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&newsletterV3=7476ea235ae9&newsletterV3Id=2373198e349f&user=Ester+Hlav&userId=7476ea235ae9&source=-----50c1eb293456---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Written by Ester Hlav"}, {"url": "https://medium.com/@EsterHlav/followers?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "300 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://github.com/EsterHlav", "anchor_text": "github.com/EsterHlav"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7476ea235ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&user=Ester+Hlav&userId=7476ea235ae9&source=post_page-7476ea235ae9----50c1eb293456---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2373198e349f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-in-deep-learning-from-softmax-to-sparsemax-math-proof-50c1eb293456&newsletterV3=7476ea235ae9&newsletterV3Id=2373198e349f&user=Ester+Hlav&userId=7476ea235ae9&source=-----50c1eb293456---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=author_recirc-----50c1eb293456----0---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=author_recirc-----50c1eb293456----0---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=author_recirc-----50c1eb293456----0---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Ester Hlav"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c1eb293456----0---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=author_recirc-----50c1eb293456----0---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Xavier Glorot Initialization in Neural Networks \u2014 Math ProofDetailed derivation for finding optimal initial distributions of weight matrices in deep learning layers with tanh activation function"}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=author_recirc-----50c1eb293456----0---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "\u00b79 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4682bf5c6ec3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3&user=Ester+Hlav&userId=7476ea235ae9&source=-----4682bf5c6ec3----0-----------------clap_footer----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=author_recirc-----50c1eb293456----0---------------------dda634df_1bed_40eb_be5b_8709b795393d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4682bf5c6ec3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3&source=-----50c1eb293456----0-----------------bookmark_preview----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c1eb293456----1---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----50c1eb293456----1---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----50c1eb293456----1---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c1eb293456----1---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c1eb293456----1---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c1eb293456----1---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----50c1eb293456----1---------------------dda634df_1bed_40eb_be5b_8709b795393d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----50c1eb293456----1-----------------bookmark_preview----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c1eb293456----2---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----50c1eb293456----2---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----50c1eb293456----2---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c1eb293456----2---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c1eb293456----2---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c1eb293456----2---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----50c1eb293456----2---------------------dda634df_1bed_40eb_be5b_8709b795393d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----50c1eb293456----2-----------------bookmark_preview----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4?source=author_recirc-----50c1eb293456----3---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=author_recirc-----50c1eb293456----3---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=author_recirc-----50c1eb293456----3---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Ester Hlav"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----50c1eb293456----3---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4?source=author_recirc-----50c1eb293456----3---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "Kaiming He Initialization in Neural Networks \u2014 Math ProofDeriving optimal initial variance of weight matrices in neural network layers with ReLU activation function"}, {"url": "https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4?source=author_recirc-----50c1eb293456----3---------------------dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": "\u00b710 min read\u00b7Feb 15"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F73b9a0d845c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4&user=Ester+Hlav&userId=7476ea235ae9&source=-----73b9a0d845c4----3-----------------clap_footer----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4?source=author_recirc-----50c1eb293456----3---------------------dda634df_1bed_40eb_be5b_8709b795393d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F73b9a0d845c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4&source=-----50c1eb293456----3-----------------bookmark_preview----dda634df_1bed_40eb_be5b_8709b795393d-------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "See all from Ester Hlav"}, {"url": "https://towardsdatascience.com/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----50c1eb293456----0-----------------bookmark_preview----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----50c1eb293456----1-----------------bookmark_preview----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----0-----------------clap_footer----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----50c1eb293456----0---------------------86b10ead_7b23_4866_aa38_7658b0865208-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----50c1eb293456----0-----------------bookmark_preview----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----50c1eb293456----1---------------------86b10ead_7b23_4866_aa38_7658b0865208-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----50c1eb293456----1-----------------bookmark_preview----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01?source=read_next_recirc-----50c1eb293456----2---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/@mazzanti.sam?source=read_next_recirc-----50c1eb293456----2---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/@mazzanti.sam?source=read_next_recirc-----50c1eb293456----2---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Samuele Mazzanti"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50c1eb293456----2---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01?source=read_next_recirc-----50c1eb293456----2---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Is F1-Score Really Better than Accuracy?What\u2019s the cost of being wrong (and the gain of being right) according to different metrics"}, {"url": "https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01?source=read_next_recirc-----50c1eb293456----2---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "\u00b710 min read\u00b7Apr 18"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f87be75ae01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-f1-score-really-better-than-accuracy-5f87be75ae01&user=Samuele+Mazzanti&userId=e16f3bb86e03&source=-----5f87be75ae01----2-----------------clap_footer----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/is-f1-score-really-better-than-accuracy-5f87be75ae01?source=read_next_recirc-----50c1eb293456----2---------------------86b10ead_7b23_4866_aa38_7658b0865208-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f87be75ae01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-f1-score-really-better-than-accuracy-5f87be75ae01&source=-----50c1eb293456----2-----------------bookmark_preview----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=read_next_recirc-----50c1eb293456----3---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=read_next_recirc-----50c1eb293456----3---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/@EsterHlav?source=read_next_recirc-----50c1eb293456----3---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Ester Hlav"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----50c1eb293456----3---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=read_next_recirc-----50c1eb293456----3---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "Xavier Glorot Initialization in Neural Networks \u2014 Math ProofDetailed derivation for finding optimal initial distributions of weight matrices in deep learning layers with tanh activation function"}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=read_next_recirc-----50c1eb293456----3---------------------86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": "\u00b79 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4682bf5c6ec3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3&user=Ester+Hlav&userId=7476ea235ae9&source=-----4682bf5c6ec3----3-----------------clap_footer----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=read_next_recirc-----50c1eb293456----3---------------------86b10ead_7b23_4866_aa38_7658b0865208-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4682bf5c6ec3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3&source=-----50c1eb293456----3-----------------bookmark_preview----86b10ead_7b23_4866_aa38_7658b0865208-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----50c1eb293456--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----50c1eb293456--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}