{"url": "https://towardsdatascience.com/lets-make-some-anime-using-deep-learning-d258806954f5", "time": 1683010676.227438, "path": "towardsdatascience.com/lets-make-some-anime-using-deep-learning-d258806954f5/", "webpage": {"metadata": {"title": "Let\u2019s make some Anime using Deep Learning | by Arpan Mishra | Towards Data Science", "h1": "Let\u2019s make some Anime using Deep Learning", "description": "The motivation for this project was to see how far technology has come in just a few years in the NLP domain especially when it comes to generating creative content. I have explored two text\u2026"}, "outgoing_paragraph_urls": [{"url": "http://imelist.net/anime.php", "anchor_text": "myanimelist", "paragraph_index": 6}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "blog", "paragraph_index": 9}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "blog", "paragraph_index": 25}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "blog", "paragraph_index": 35}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face", "paragraph_index": 36}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "blog", "paragraph_index": 49}, {"url": "https://github.com/Arpan-Mishra/Anime-Generation-using-Deep-Learning", "anchor_text": "repository", "paragraph_index": 55}, {"url": "http://lk.org/2019/02/08/text-generation-with-pytorch/", "anchor_text": "Text generation with LSTM", "paragraph_index": 57}, {"url": "https://machinetalk.org/author/chunml/", "anchor_text": "Trung Tran", "paragraph_index": 57}, {"url": "https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912", "anchor_text": "Fine tuning GPT2 Example", "paragraph_index": 58}], "all_paragraphs": ["The motivation for this project was to see how far technology has come in just a few years in the NLP domain especially when it comes to generating creative content. I have explored two text generation techniques by generating Anime synopsis, first with LSTM units which is a relatively old technique and then with a fine tuned GPT2 transformer.", "In this post you will see how AI went from creating this piece of nonsense\u2026", "A young woman capable : a neuroi laborer of the human , where one are sent back home ? after defeating everything being their resolve the school who knows if all them make about their abilities . however of those called her past student tar barges together when their mysterious high artist are taken up as planned while to eat to fight !", "A young woman named Haruka is a high school student who has a crush on a mysterious girl named Miki. She is the only one who can remember the name of the girl, and she is determined to find out who she really is.", "To get the most out of this post you must have knowledge of :", "Alright then, lets see some code!", "The data used here has been scraped from myanimelist, it initially contained over 16000 data points and it was a really messy dataset. I have taken the following steps to clean it:", "The following functions take care of all this", "The traditional approach for text generation uses recurrent LSTM units. LSTM (or long short term memory) are specifically designed to capture long term dependencies in sequential data which the normal RNNs can\u2019t and it does so by using multiple gates which govern the information that passes from one time step to another.", "Intuitively, in a time step the information that reaches an LSTM unit goes through these gates and they decide if the information needs to be updated, if they are updated then the old information is forgotten and then this new updated values are sent to the next time step. For a more detailed understanding of LSTMs I would suggest you to go through this blog.", "So before we build out model architecture, we must tokenize the synopses and process them in such a way that the model accepts it.", "The input and the output is same in text generation except that the output tokens are shifted one step to the right. This basically means that the model takes in input the past words and predicts the next word. The input and output tokens are passed into the model in batches and each batch has a fixed sequence length. I have followed these steps to create the dataset:", "Our model consists of an embedding layer, a stack of LSTM layers (I have used 3 layers here), dropout layer and then finally a linear layer which outputs the scores of each vocabulary token. We haven\u2019t used the softmax layer just yet you will understand why shortly.", "Since the LSTM units also outputs the hidden states, the model also returns these hidden states so that they can be passed onto the model in the next time step (next batch of word sequence). Also, after every epoch we need to reset the hidden states to 0 as we won\u2019t need the information from the last time step of the previous epoch in the first time step of the current epoch, so we have a \u201czero_state\u201d function as well.", "We then just define out training function, store the losses from every epoch and save the model with the best loss. We are also calling the zero state function before every epoch to reset the hidden states.", "The loss function that we are using is the cross entropy loss, this is the reason we are not passing the output through an explicit softmax layer as this loss function calculates that internally.", "All the training is being done on GPU, following are the parameters that are being used (as provided in config class):", "During the text generation step, we feed into the model some input text for example, \u2018A young woman\u2019, our function will first tokenize this, then pass it into the model. The function also takes the length of the synopsis that we want to output.", "The model will output the scores of each vocabulary token. We will then apply softmax to these scores to convert them into a probability distribution.", "Then we use top-k sampling i.e we select top k tokens with highest probability out of the n vocabulary tokens and then randomly sample one token which we return as the output.", "This output is then concatenated into out initial input string. This output token becomes the input for the next time step. Say the output was \u201ccapable\u201d then our concatenated text is \u201cA young woman capable\u201d. We keep doing this until we output the final token and then we print the output.", "Here is a nice diagram to understand what the model is doing", "In the above example, I have given the max length as 100 and the input text as \u201cIn the\u201d, and this the output that we get", "In the days attempt it 's . although it has , however ! what they believe that humans of these problems . it seems and if will really make anything . as she must never overcome allowances with jousuke s , in order her home at him without it all in the world : in the hospital she makes him from himself by demons and carnage . a member and an idol team the power for to any means but the two come into its world for what if this remains was to wait in and is n't going ! on an", "This seems grammatically correct but it makes no sense at all. LSTM though are better at capturing long term dependencies than basic RNN but they can only see a few steps (words) back or a few steps forward if we use bidirectional RNNs to capture the context of the text hence when generating very long sentences we see that they make no sense.", "Transformers do a much better job in capturing the context of the text piece provided. They use only attention layers (no RNNs) which allows them to understand the context of the text much better as they can see as many time steps back (and forth depending on the attention) as they wish to. There are different types of attention but the attention that is used by GPT2, one of the best models out there for language modeling, is called masked self attention. If you\u2019re not familiar with transformers please go through this blog before proceeding.", "GPT2 instead of using both the transformer encoder and decoder stacks uses a high stack of just transformer decoders. There are 4 variants of the GPT2 transformer depending on the number of decoders stacked.", "Each decoder unit consists of mainly 2 layers:", "There is a layer normalization step and a residual connection after each step as well. This is what I mean\u2026", "If you went through the blog post earlier you must know how self attention is calculated. Intuitively, the self attention scores give us the importance or the attention the word in the current time step should give to the other words (past time step or future depending on the attention).", "In masked self attention however, we are not concerned with the next or the future words. So the transformer decoder is allowed to only attend to the present and the past words and the future words are masked.", "Here is a beautiful representation of this idea\u2026", "In the above example the current word is \u201cit\u201d and as you can see the words \u201ca\u201d and \u201crobot\u201d have high attention scores. This is because \u201cit\u201d is being used to refer to \u201crobot\u201d and so is \u201ca\u201d.", "You must have noticed that <s> token in the beginning of the above input text. <s> is just being used to mark the start of an input string. Traditionally instead of the <s> token <|endoftext|> is used.", "Another thing you must have noticed that, this is similar to the traditional language modeling where seeing the present and the past tokens the next token is predicted. Then this predicted token is added to the inputs and then again the next token is predicted.", "I have given a very intuitive and high-level understanding of GPT2. Even though this is enough to dive into the code, it would be a good idea to read up more on this concept to get a deeper understanding. I suggest Jay Allammar`s blog.", "I have used GPT2 with a linear model head from the Hugging Face library for text generation. Out of the 4 variants, I have used GPT2 small which has 117M parameters.", "I have trained the model on Google Colab, the main issue in training was figuring out the batch size and the maximum sequence length so that I don\u2019t run out of memory while training on GPU, a batch size of 10 and maximum sequence length of 300 finally worked for me.", "For this reason I have also removed the synopses with words more than 300 so that when we generate the synopsis till a length of 300, it is actually complete.", "For fine tuning the first task is to get the data in the required format, dataloader in Pytorch allows us to do that very easily.", "Here, we don\u2019t explicitly need to create a model architecture as Hugging Face library takes care of that for us. We just simply import the pre-trained GPT2 model with language model head.", "This LM head is nothing but actually just a linear layer which outputs the scores of each vocabulary token (before softmax).", "The cool thing about the GPT2Model with LM head provided by Hugging Face is that we can directly pass the labels (our input tokens) here and they are internally shifted to the right by one step, and the model along with the prediction scores returns the loss as well. It actually also return the hidden states of every layer in the model and the attention scores as well but we are not interested in that.", "We can import the model and the tokenizer and define all the hyperparameters in a config class like so\u2026", "The model outputs a tuple:- (loss,predicted scores, list of key&value pairs of every masked attention layer, list of hidden states of every layer,attention scores) We are only interested in the first 2 items in this tuple so we access those.", "In the generation step I have used top-k sampling (as in LSTM way) as well as top-p sampling. In top-p sampling we provide a cumulative probability say p, then the top vocabulary tokens that are selected must have a sum probability of p.", "We can combine both the top-k and top-p methods, first the top-k tokens are selected with highest probability scores then for these k tokens a normalized score is calculated. This is such that the sum of these scores for the k tokens is 1, we can also say that the probability mass is just re-distributed among the k tokens.", "Next top-p sampling is done on these k scores, then out of the selected tokens finally we just sample using the probabilities to get the final output token.", "And again we do not have to code all of this, hugging face takes care of it all with its generate method.", "You must have noticed that the generate method has a lot of parameters. They can be tuned to get the most optimal output. Check out this blog by hugging face that explains these parameters in detail.", "For the input text, \u201cIn the year\u201d this is the output that we get\u2026.", "In the year 2060, mankind has colonized the solar system, and is now on the verge of colonizing other planets. In order to defend themselves against this new threat, the Earth Federation has established a special unit known as the Planetary Defense Force, or PDF. The unit is composed of the elite Earth Defense Forces, who are tasked with protecting the planet from any alien lifeforms that might threaten the safety of Earth. However, when a mysterious alien ship crashes in the middle of their patrol, they are forced to use their special mobile suits to fend off the alien threat.", "You know what? I would actually watch this.", "The difference in between the synopsis generated by LSTMs and GPT2 is just huge! Not only is the model able to capture long term dependecies well, but the context is also maintained throughout.", "A shinigami (death god) who is a descendant of the legendary warrior Shigamis father, is sent to Earth to fight against the evil organization known as the Dark Clan. However, his mission is to steal the sacred sword, the Sword of Light, which is said to grant immortality to those who wield it.", "Checkout my github repository, where you can go through the code and see some more cool AI generated Anime.", "I hope you enjoyed this post and were easily able to follow it. Please provide valuable feedback in the comments. I would love to know how you would approach this task and if you have a better way of doing this and improving the model.", "Text generation with LSTM by Trung Tran", "Fine tuning GPT2 Example by Martin Frolovs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd258806954f5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@mishraarpan6?source=post_page-----d258806954f5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d258806954f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Arpan Mishra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea811b84fa53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=post_page-ea811b84fa53----d258806954f5---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd258806954f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=-----d258806954f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd258806954f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&source=-----d258806954f5---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@brucetml?utm_source=medium&utm_medium=referral", "anchor_text": "Bruce Tang"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://imelist.net/anime.php", "anchor_text": "myanimelist"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "blog"}, {"url": "https://machinetalk.org/2019/02/08/text-generation-with-pytorch/", "anchor_text": "machinetalk"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "blog"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "Jalammar"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "jalammar"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "blog"}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face"}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "blog"}, {"url": "https://github.com/Arpan-Mishra/Anime-Generation-using-Deep-Learning", "anchor_text": "repository"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM"}, {"url": "http://lk.org/2019/02/08/text-generation-with-pytorch/", "anchor_text": "Text generation with LSTM"}, {"url": "https://machinetalk.org/author/chunml/", "anchor_text": "Trung Tran"}, {"url": "https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912", "anchor_text": "Fine tuning GPT2 Example"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformers Blog"}, {"url": "http://jalammar.github.io/illustrated-gpt2/", "anchor_text": "GPT2 Blog"}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "Generate Method Hugging Face"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d258806954f5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----d258806954f5---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d258806954f5---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/anime?source=post_page-----d258806954f5---------------anime-----------------", "anchor_text": "Anime"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d258806954f5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd258806954f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=-----d258806954f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd258806954f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=-----d258806954f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd258806954f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=post_page-----d258806954f5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d258806954f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea811b84fa53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=post_page-ea811b84fa53----d258806954f5---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fea811b84fa53%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=-----d258806954f5---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Written by Arpan Mishra"}, {"url": "https://medium.com/@mishraarpan6/followers?source=post_page-----d258806954f5--------------------------------", "anchor_text": "26 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea811b84fa53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=post_page-ea811b84fa53----d258806954f5---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fea811b84fa53%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flets-make-some-anime-using-deep-learning-d258806954f5&user=Arpan+Mishra&userId=ea811b84fa53&source=-----d258806954f5---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/decoding-support-vector-machines-5b81d2f7b76f?source=author_recirc-----d258806954f5----0---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=author_recirc-----d258806954f5----0---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=author_recirc-----d258806954f5----0---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Arpan Mishra"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d258806954f5----0---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/decoding-support-vector-machines-5b81d2f7b76f?source=author_recirc-----d258806954f5----0---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Decoding Support Vector MachinesIntuitively understand how Support Vector Machines work"}, {"url": "https://towardsdatascience.com/decoding-support-vector-machines-5b81d2f7b76f?source=author_recirc-----d258806954f5----0---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "\u00b712 min read\u00b7Jun 24, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&user=Arpan+Mishra&userId=ea811b84fa53&source=-----5b81d2f7b76f----0-----------------clap_footer----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/decoding-support-vector-machines-5b81d2f7b76f?source=author_recirc-----d258806954f5----0---------------------71e45384_4225_46e2_b0ed_87777e461da5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b81d2f7b76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecoding-support-vector-machines-5b81d2f7b76f&source=-----d258806954f5----0-----------------bookmark_preview----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d258806954f5----1---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d258806954f5----1---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d258806954f5----1---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d258806954f5----1---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d258806954f5----1---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d258806954f5----1---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d258806954f5----1---------------------71e45384_4225_46e2_b0ed_87777e461da5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----d258806954f5----1-----------------bookmark_preview----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d258806954f5----2---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d258806954f5----2---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d258806954f5----2---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d258806954f5----2---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d258806954f5----2---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d258806954f5----2---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d258806954f5----2---------------------71e45384_4225_46e2_b0ed_87777e461da5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----d258806954f5----2-----------------bookmark_preview----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/predicting-hr-attrition-using-support-vector-machines-d8b4e82d5351?source=author_recirc-----d258806954f5----3---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=author_recirc-----d258806954f5----3---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=author_recirc-----d258806954f5----3---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Arpan Mishra"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d258806954f5----3---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/predicting-hr-attrition-using-support-vector-machines-d8b4e82d5351?source=author_recirc-----d258806954f5----3---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "Predicting HR Attrition using Support Vector MachinesLearn to train an SVM model following best practices"}, {"url": "https://towardsdatascience.com/predicting-hr-attrition-using-support-vector-machines-d8b4e82d5351?source=author_recirc-----d258806954f5----3---------------------71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": "\u00b78 min read\u00b7Jul 1, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8b4e82d5351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-hr-attrition-using-support-vector-machines-d8b4e82d5351&user=Arpan+Mishra&userId=ea811b84fa53&source=-----d8b4e82d5351----3-----------------clap_footer----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/predicting-hr-attrition-using-support-vector-machines-d8b4e82d5351?source=author_recirc-----d258806954f5----3---------------------71e45384_4225_46e2_b0ed_87777e461da5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8b4e82d5351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-hr-attrition-using-support-vector-machines-d8b4e82d5351&source=-----d258806954f5----3-----------------bookmark_preview----71e45384_4225_46e2_b0ed_87777e461da5-------", "anchor_text": ""}, {"url": "https://medium.com/@mishraarpan6?source=post_page-----d258806954f5--------------------------------", "anchor_text": "See all from Arpan Mishra"}, {"url": "https://towardsdatascience.com/?source=post_page-----d258806954f5--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----d258806954f5----0-----------------bookmark_preview----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----d258806954f5----1-----------------bookmark_preview----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d258806954f5----0---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----d258806954f5----0-----------------bookmark_preview----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d258806954f5----1---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----d258806954f5----1-----------------bookmark_preview----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----d258806954f5----2---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----d258806954f5----2---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----d258806954f5----2---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Josep Ferrer"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----d258806954f5----2---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----d258806954f5----2---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Stop doing this on ChatGPT and get ahead of the 99% of its usersUnleash the Power of AI Writing with Effective Prompts"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----d258806954f5----2---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "\u00b78 min read\u00b7Mar 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&user=Josep+Ferrer&userId=8213af8f3ccf&source=-----f3441bf7a25a----2-----------------clap_footer----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----d258806954f5----2---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "71"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&source=-----d258806954f5----2-----------------bookmark_preview----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----d258806954f5----3---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----d258806954f5----3---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----d258806954f5----3---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "Mark Riedl"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----d258806954f5----3---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "A Very Gentle Introduction to Large Language Models without the Hype[This is a work in progress]"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----d258806954f5----3---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": "38 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&user=Mark+Riedl&userId=7247bdeb9655&source=-----5f67941fa59e----3-----------------clap_footer----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----d258806954f5----3---------------------db5e825f_344e_4e94_9dfb_3a124abb1c72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "53"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&source=-----d258806954f5----3-----------------bookmark_preview----db5e825f_344e_4e94_9dfb_3a124abb1c72-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----d258806954f5--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d258806954f5--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----d258806954f5--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}