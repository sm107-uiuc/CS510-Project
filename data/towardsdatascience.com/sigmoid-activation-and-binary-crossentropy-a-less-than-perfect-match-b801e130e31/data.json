{"url": "https://towardsdatascience.com/sigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31", "time": 1682995113.779875, "path": "towardsdatascience.com/sigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31/", "webpage": {"metadata": {"title": "Sigmoid Activation and Binary Crossentropy \u2014A Less Than Perfect Match? | by Harald Hentschke | Towards Data Science", "h1": "Sigmoid Activation and Binary Crossentropy \u2014A Less Than Perfect Match?", "description": "In neuronal networks tasked with binary classification, sigmoid activation in the last (output) layer and binary crossentropy (BCE) as the loss function are standard fare. Yet, occasionally one\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb", "anchor_text": "may result in numerical imprecision or even instability", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "anchor_text": "an excellent, in-depth explanation of BCE", "paragraph_index": 0}, {"url": "https://medium.com/u/c79695e37339?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Daniel Godoy", "paragraph_index": 0}, {"url": "https://stackoverflow.com/a/52111173/4334743", "anchor_text": "unfortunately", "paragraph_index": 3}], "all_paragraphs": ["In neuronal networks tasked with binary classification, sigmoid activation in the last (output) layer and binary crossentropy (BCE) as the loss function are standard fare. Yet, occasionally one stumbles across statements that this specific combination of last layer-activation and loss may result in numerical imprecision or even instability. I wanted to make sure I get the argument, down to the numbers, especially in the framework which I use so far, Keras. Sounds interesting? Realize that this may also be relevant for some image segmentation tasks, or multiclass, multilabel problems, not just cats_vs_dogs kind of problems? Then, please follow along. If you appreciate a refresher on BCE beforehand, I\u2019d like to point to an excellent, in-depth explanation of BCE by Daniel Godoy.", "Let\u2019s start by dissecting Keras\u2019 implementation of BCE:", "So, input argument output is clipped first, then converted to logits, and then fed into TensorFlow function tf.nn.sigmoid_cross_entropy_with_logits. OK\u2026what was logit(s) again? In mathematics, the logit function is the inverse of the sigmoid function, so in theory logit(sigmoid(x)) = x.", "In Deep Learning, logits usually and unfortunately means the \u2018raw\u2019 outputs of the last layer of a classification network, that is, the output of the layer before it is passed to an activation/normalization function, e.g. the sigmoid. Raw outputs may take on any value. This is what sigmoid_cross_entropy_with_logits, the core of Keras\u2019s binary_crossentropy, expects. In Keras, by contrast, the expectation is that the values in variable outputrepresent probabilities and are therefore bounded by [0 1] \u2014 that\u2019s why from_logitsis by default set to False. So, they need to be converted back to raw values before being fed into sigmoid_cross_entropy_with_logits. To repeat, we first run numbers through one function (sigmoid), only to convert them back using the inverse function (logit). This appears circuitous. The real potential problem, though, is the numerical instability that this to and fro may cause, resulting in an overflow in the extreme case. Look at the output of y = logit(sigmoid(x)) when x is of type float32, the default in Keras and, as far as I know, in most other frameworks, too:", "Starting at about x=14.6 errors hit the 1% range, and above about x=16.6 game\u2019s over due to division by zero. Division by zero occurs when the denominator in the sigmoid evaluates to exactly 1 and the denominator in the logit then evaluates to zero. We could have estimated the limit of x via np.log(np.finfo(np.float32).eps), but I found the numerical zigzagging entertaining and worth showing. Anyways, this is why the values of input variable outputin the Keras function need to be and in fact are clipped. So, to dispel one notion right at the beginning:", "Keras\u2019s binary_crossentropy, when fed with input resulting from sigmoid activation, will not produce over- or underflow of numbers.", "However, the result of the clipping is a flattening of the loss function at the borders. To visualize this, let\u2019s", "Thus, we are able to compare the values of BCE as computed from sigmoid activation in Keras with the values computed from raw outputs in TensorFlow. Here is the first model, using sigmoid activation and Keras\u2019s standard BCE:", "The model without sigmoid activation, using a custom-made loss function which plugs the values directly into sigmoid_cross_entropy_with_logits:", "So, if we evaluate the models on a sweeping range of scalar inputs x, setting the label (y) to 1, we can compare the model-generated BCEs with each other and also to the values produced by a naive implementation of BCE computed with a high-precision float. Note again that we\u2019re computing BCE from single samples here in order to distil the differences between the methods.", "Interesting! The curve computed from raw values using TensorFlow\u2019s sigmoid_cross_entropy_with_logitsis smooth across the range of x values tested, whereas the curve computed from sigmoid-transformed values with Keras\u2019s binary_crossentropyflattens in both directions (as predicted). At large positive x values, before hitting the clipping-induced limit, the sigmoid-derived curve shows a step-like appearance. Most notable may be what happens on the left border, which is best appreciated on a linear scale (Fig. 3, center): the more negative the raw value, the more severe the underestimation of its BCE value due to clipping. Finally, even with a float128 we won\u2019t get very far if we derive the BCE values from sigmoid-transformed input (gray curve in leftmost plot).", "As the loss function is central to learning, this means that a model employing last-layer sigmoid + BCE cannot discriminate among samples whose predicted class is either in extreme accordance or extreme discordance with their labels. So, in theory it is true that there is a drawback to using sigmoid + BCE. However, does it matter in practice? After all, we are talking about extreme edge cases. And before we answer that question, a more basic question is in order: do raw last layer outputs reach such extreme values in practice? Let\u2019s see.", "I trained three different networks to the task of categorizing dogs vs. cats, using a subset of the 2013 Kaggle competition data set (2000 training images, 1000 for validation; following the example of F. Chollet (Deep Learning with Python. Manning Publications Co, Shelter Island, New York. 2017.)). Yes, cats and dogs again, for the sake of ease and focusing on the issue at hand.", "The figures and numbers below stem from a simple, hand-crafted convnet: four pairs of 2D convolution/max pooling layers, followed by a single dropout and two dense layers, all with relu activation. The last, single-element, output layer was without activation, and as the loss function I used above-mentioned Keras wrapper for TensorFlow\u2019s sigmoid_cross_entropy_with_logits.", "First, let\u2019s find out whether individual images can in fact result in extreme raw values of the output layer. After training, I ran the same images as used for training through the network and obtained their raw output from the last layer. Additionally, I computed the sigmoid-transformed output, as well as the BCE values derived from both outputs. This is what I got after training for eight epochs, so with relatively little learning having taken place:", "Obviously, in the initial phase of training, we are outside the danger zone; raw last layer output values are bounded by ca [-3 8] in this example, and BCE values computed from raw and sigmoid outputs are identical. Also nice to see is the strong \u2018squashing\u2019 effect of the sigmoid (Fig. 4, center).", "What is the picture like when the network is fully trained (here defined after not having shown a reduced loss in 15 consecutive epochs)?", "Aha \u2014we see a much clearer separation between the classes, as expected. And a small number of images did in fact result in extreme logit values that fall into the clipping range. Let\u2019s focus on class 1 (dogs; orange color in the figures); the argument runs similar for the other class. None of the samples result in raw output values more negative than ca. -4, so again that is fine. However, a certain number of samples \u2014 the doggiest dogs \u2014 reach raw output values larger than about 16. Accordingly, the associated BCE, computed via sigmoid + Keras\u2019s binary_crossentropy, is clipped at ca. 10\u207b\u2077 (Fig. 5, right; see also Fig. 3). This is a really small value. Would we expect learning to happen in a systematically different fashion if BCE values of doggy dogs and catty cats were smaller (and individually different) when computed without clipping-induced limits? Particularly if we use reasonable batch sizes, the samples with intermediate or low raw output values will dominate the loss. Figure 6 illustrates this for the same data as above with a batch size of 4, which is still really on the low side.", "Results were qualitatively similar with VGG16- and Inception V3-based networks pretrained on the imagenet data set (trained without fine-tuning of the convolutional parts).", "First of all, let\u2019s reiterate that fears of number under- or overflow due to the combination of sigmoid activation in the last layer and BCE as the loss function are unjustified \u2014 in Keras using the TensorFlow backend. I am not familiar with other frameworks (yet), but I would be very surprised if they did not feature similar precautions.", "Based on the \u2018experiments\u2019 above with the venerable cats_vs_dogs data set it appears that sigmoid +BCE is fine also in terms of precision. Particularly if you use a reasonable batch size and properly scaled data, it should not matter how BCE is computed. However, this is just one data set and very few models tested on them.", "it can\u2019t harm to compute BCE from raw outputs. Otherwise, just stick with sigmoid+BCE.", "Comments, suggestions, experience with other frameworks? Happy to hear about it.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist with a neuroscience background."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb801e130e31&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b801e130e31--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hhntschk?source=post_page-----b801e130e31--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhntschk?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Harald Hentschke"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5725ebe4c3d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&user=Harald+Hentschke&userId=5725ebe4c3d8&source=post_page-5725ebe4c3d8----b801e130e31---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb801e130e31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb801e130e31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb", "anchor_text": "may result in numerical imprecision or even instability"}, {"url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "anchor_text": "an excellent, in-depth explanation of BCE"}, {"url": "https://medium.com/u/c79695e37339?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Daniel Godoy"}, {"url": "https://stackoverflow.com/a/52111173/4334743", "anchor_text": "unfortunately"}, {"url": "https://stackoverflow.com/questions/52125924/why-does-sigmoid-crossentropy-of-keras-tensorflow-have-low-precision", "anchor_text": "neat trick of setting up minimalistic networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b801e130e31---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----b801e130e31---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----b801e130e31---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/activation-functions?source=post_page-----b801e130e31---------------activation_functions-----------------", "anchor_text": "Activation Functions"}, {"url": "https://medium.com/tag/keras?source=post_page-----b801e130e31---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb801e130e31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&user=Harald+Hentschke&userId=5725ebe4c3d8&source=-----b801e130e31---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb801e130e31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&user=Harald+Hentschke&userId=5725ebe4c3d8&source=-----b801e130e31---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb801e130e31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b801e130e31--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb801e130e31&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b801e130e31---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b801e130e31--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b801e130e31--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b801e130e31--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b801e130e31--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b801e130e31--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhntschk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hhntschk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Harald Hentschke"}, {"url": "https://medium.com/@hhntschk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "60 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5725ebe4c3d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&user=Harald+Hentschke&userId=5725ebe4c3d8&source=post_page-5725ebe4c3d8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F337178643d63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31&newsletterV3=5725ebe4c3d8&newsletterV3Id=337178643d63&user=Harald+Hentschke&userId=5725ebe4c3d8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}