{"url": "https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34", "time": 1683018392.411149, "path": "towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34/", "webpage": {"metadata": {"title": "Transformers Explained Visually (Part 2): How it works, step-by-step | by Ketan Doshi | Towards Data Science", "h1": "Transformers Explained Visually (Part 2): How it works, step-by-step", "description": "Transformer detailed end-to-end operation of Embedding, Positional Encoding, Encoder, Decoder, Multi-head Attention, Masking, and Output"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "first article", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Part 1", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Part 1", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Part 1", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "previous article", "paragraph_index": 56}], "all_paragraphs": ["This is the second article in my series on Transformers. In the first article, we learned about the functionality of Transformers, how they are used, their high-level architecture, and their advantages.", "In this article, we can now look under the hood and study exactly how they work in detail. We\u2019ll see how data flows through the system with their actual matrix representations and shapes and understand the computations performed at each stage.", "Here\u2019s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.", "And if you\u2019re interested in NLP applications in general, I have some other articles you might like.", "As we saw in Part 1, the main components of the architecture are:", "Data inputs for both the Encoder and Decoder, which contains:", "The Encoder stack contains a number of Encoders. Each Encoder contains:", "The Decoder stack contains a number of Decoders. Each Decoder contains:", "Output (top right) \u2014 generates the final output, and contains:", "To understand what each component does, let\u2019s walk through the working of the Transformer while we are training it to solve a translation problem. We\u2019ll use one sample of our training data which consists of an input sequence (\u2018You are welcome\u2019 in English) and a target sequence (\u2018De nada\u2019 in Spanish).", "Like any NLP model, the Transformer needs two things about each word \u2014 the meaning of the word and its position in the sequence.", "The Transformer combines these two encodings by adding them.", "The Transformer has two Embedding layers. The input sequence is fed to the first Embedding layer, known as the Input Embedding.", "The target sequence is fed to the second Embedding layer after shifting the targets right by one position and inserting a Start token in the first position. Note that, during Inference, we have no target sequence and we feed the output sequence to this second layer in a loop, as we learned in Part 1. That is why it is called the Output Embedding.", "The text sequence is mapped to numeric word IDs using our vocabulary. The embedding layer then maps each input word into an embedding vector, which is a richer representation of the meaning of that word.", "Since an RNN implements a loop where each word is input sequentially, it implicitly knows the position of each word.", "However, Transformers don\u2019t use RNNs and all words in a sequence are input in parallel. This is its major advantage over the RNN architecture, but it means that the position information is lost, and has to be added back in separately.", "Just like the two Embedding layers, there are two Position Encoding layers. The Position Encoding is computed independently of the input sequence. These are fixed values that depend only on the max length of the sequence. For instance,", "These constants are computed using the formula below, where", "In other words, it interleaves a sine curve and a cos curve, with sine values for all even indexes and cos values for all odd indexes. As an example, if we encode a sequence of 40 words, we can see below the encoding values for a few (word position, encoding_index) combinations.", "The blue curve shows the encoding of the 0th index for all 40 word-positions and the orange curve shows the encoding of the 1st index for all 40 word-positions. There will be similar curves for the remaining index values.", "As we know, deep learning models process a batch of training samples at a time. The Embedding and Position Encoding layers operate on matrices representing a batch of sequence samples. The Embedding takes a (samples, sequence length) shaped matrix of word IDs. It encodes each word ID into a word vector whose length is the embedding size, resulting in a (samples, sequence length, embedding size) shaped output matrix. The Position Encoding uses an encoding size that is equal to the embedding size. So it produces a similarly shaped matrix that can be added to the embedding matrix.", "The (samples, sequence length, embedding size) shape produced by the Embedding and Position Encoding layers is preserved all through the Transformer, as the data flows through the Encoder and Decoder Stacks until it is reshaped by the final Output layers.", "This gives a sense of the 3D matrix dimensions in the Transformer. However, to simplify the visualization, from here on we will drop the first dimension (for the samples) and use the 2D representation for a single sample.", "The Input Embedding sends its outputs into the Encoder. Similarly, the Output Embedding feeds into the Decoder.", "The Encoder and Decoder Stacks consists of several (usually six) Encoders and Decoders respectively, connected sequentially.", "The first Encoder in the stack receives its input from the Embedding and Position Encoding. The other Encoders in the stack receive their input from the previous Encoder.", "The Encoder passes its input into a Multi-head Self-attention layer. The Self-attention output is passed into a Feed-forward layer, which then sends its output upwards to the next Encoder.", "Both the Self-attention and Feed-forward sub-layers, have a residual skip-connection around them, followed by a Layer-Normalization.", "The output of the last Encoder is fed into each Decoder in the Decoder Stack as explained below.", "The Decoder\u2019s structure is very similar to the Encoder\u2019s but with a couple of differences.", "Like the Encoder, the first Decoder in the stack receives its input from the Output Embedding and Position Encoding. The other Decoders in the stack receive their input from the previous Decoder.", "The Decoder passes its input into a Multi-head Self-attention layer. This operates in a slightly different way than the one in the Encoder. It is only allowed to attend to earlier positions in the sequence. This is done by masking future positions, which we\u2019ll talk about shortly.", "Unlike the Encoder, the Decoder has a second Multi-head attention layer, known as the Encoder-Decoder attention layer. The Encoder-Decoder attention layer works like Self-attention, except that it combines two sources of inputs \u2014 the Self-attention layer below it as well as the output of the Encoder stack.", "The Self-attention output is passed into a Feed-forward layer, which then sends its output upwards to the next Decoder.", "Each of these sub-layers, Self-attention, Encoder-Decoder attention, and Feed-forward, have a residual skip-connection around them, followed by a Layer-Normalization.", "In Part 1, we talked about why Attention is so important while processing sequences. In the Transformer, Attention is used in three places:", "The Attention layer takes its input in the form of three parameters, known as the Query, Key, and Value.", "The Transformer calls each Attention processor an Attention Head and repeats it several times in parallel. This is known as Multi-head attention. It gives its Attention greater power of discrimination, by combining several similar Attention calculations.", "The Query, Key, and Value are each passed through separate Linear layers, each with their own weights, producing three results called Q, K, and V respectively. These are then combined together using the Attention formula as shown below, to produce the Attention Score.", "The important thing to realize here is that the Q, K, and V values carry an encoded representation of each word in the sequence. The Attention calculations then combine each word with every other word in the sequence, so that the Attention Score encodes a score for each word in the sequence.", "When discussing the Decoder a little while back, we briefly mentioned masking. The Mask is also shown in the Attention diagrams above. Let\u2019s see how it works.", "While computing the Attention Score, the Attention module implements a masking step. Masking serves two purposes:", "In the Encoder Self-attention and in the Encoder-Decoder-attention: masking serves to zero attention outputs where there is padding in the input sentences, to ensure that padding doesn\u2019t contribute to the self-attention. (Note: since input sequences could be of different lengths they are extended with padding tokens like in most NLP applications so that fixed-length vectors can be input to the Transformer.)", "In the Decoder Self-attention: masking serves to prevent the decoder from \u2018peeking\u2019 ahead at the rest of the target sentence when predicting the next word.", "The Decoder processes words in the source sequence and uses them to predict the words in the destination sequence. During training, this is done via Teacher Forcing, where the complete target sequence is fed as Decoder inputs. Therefore, while predicting a word at a certain position, the Decoder has available to it the target words preceding that word as well as the target words following that word. This allows the Decoder to \u2018cheat\u2019 by using target words from future \u2018time steps\u2019.", "For instance, when predicting \u2018Word 3\u2019, the Decoder should refer only to the first 3 input words from the target but not the fourth word \u2018Ketan\u2019.", "Therefore, the Decoder masks out input words that appear later in the sequence.", "When calculating the Attention Score (refer to the picture earlier showing the calculations) masking is applied to the numerator just before the Softmax. The masked out elements (white squares) are set to negative infinity, so that Softmax turns those values to zero.", "The last Decoder in the stack passes its output to the Output component which converts it into the final output sentence.", "The Linear layer projects the Decoder vector into Word Scores, with a score value for each unique word in the target vocabulary, at each position in the sentence. For instance, if our final output sentence has 7 words and the target Spanish vocabulary has 10000 unique words, we generate 10000 score values for each of those 7 words. The score values indicate the likelihood of occurrence for each word in the vocabulary in that position of the sentence.", "The Softmax layer then turns those scores into probabilities (which add up to 1.0). In each position, we find the index for the word with the highest probability, and then map that index to the corresponding word in the vocabulary. Those words then form the output sequence of the Transformer.", "During training, we use a loss function such as cross-entropy loss to compare the generated output probability distribution to the target sequence. The probability distribution gives the probability of each word occurring in that position.", "Let\u2019s assume our target vocabulary contains just four words. Our goal is to produce a probability distribution that matches our expected target sequence \u201cDe nada END\u201d.", "This means that the probability distribution for the first word-position should have a probability of 1 for \u201cDe\u201d with probabilities for all other words in the vocabulary being 0. Similarly, \u201cnada\u201d and \u201cEND\u201d should have a probability of 1 for the second and third word-positions respectively.", "As usual, the loss is used to compute gradients to train the Transformer via backpropagation.", "Hopefully, this gives you a feel for what goes on inside the Transformer during Training. As we discussed in the previous article, it runs in a loop during Inference but most of the processing remains the same.", "The Multi-head Attention module is what gives the Transformer its power. In the next article, we will continue our journey and go one step deeper to really understand the details of how Attention is computed.", "And finally, if you liked this article, you might also enjoy my other series on Audio Deep Learning, Geolocation Machine Learning, and Image Caption architectures.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb49fa4a64f34&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47----b49fa4a64f34---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb49fa4a64f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb49fa4a64f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral", "anchor_text": "Joshua Sortino"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "first article"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Overview of functionality"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853", "anchor_text": "Multi-head Attention"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3", "anchor_text": "Why Attention Boosts Performance"}, {"url": "https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24", "anchor_text": "Beam Search"}, {"url": "https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b", "anchor_text": "Bleu Score"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504", "anchor_text": "Audio Deep Learning Made Simple (Part 1): State-of-the-Art TechniquesA Gentle Guide to the world of disruptive deep learning audio applications and architectures. And why we all need to\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/leveraging-geolocation-data-for-machine-learning-essential-techniques-192ce3a969bc", "anchor_text": "Leveraging Geolocation Data for Machine Learning: Essential TechniquesA Gentle Guide to Feature Engineering and Visualization with Geospatial data, in Plain Englishtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/image-captions-with-deep-learning-state-of-the-art-architectures-3290573712db", "anchor_text": "Image Captions with Deep Learning: State-of-the-Art ArchitecturesA Gentle Guide to Image Feature Encoders, Sequence Decoders, Attention, and Multi-modal Architectures, in Plain Englishtowardsdatascience.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b49fa4a64f34---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b49fa4a64f34---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b49fa4a64f34---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----b49fa4a64f34---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b49fa4a64f34---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb49fa4a64f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----b49fa4a64f34---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb49fa4a64f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----b49fa4a64f34---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb49fa4a64f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb49fa4a64f34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b49fa4a64f34---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b49fa4a64f34--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://ketanhdoshi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fae94feabe1c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34&newsletterV3=54f9ca55ed47&newsletterV3Id=ae94feabe1c9&user=Ketan+Doshi&userId=54f9ca55ed47&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}