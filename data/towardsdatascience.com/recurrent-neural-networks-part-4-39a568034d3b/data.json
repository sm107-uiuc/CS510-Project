{"url": "https://towardsdatascience.com/recurrent-neural-networks-part-4-39a568034d3b", "time": 1683011466.237891, "path": "towardsdatascience.com/recurrent-neural-networks-part-4-39a568034d3b/", "webpage": {"metadata": {"title": "Can GRUs outperform LSTMs? Deep Learning Lecture - RNNs Part 4 | Towards Data Science", "h1": "Recurrent Neural Networks \u2014 Part 4", "description": "In this blog post, we introduce the concept of gated recurrent units. Having fewer parameters than the LSTM, yet still empirically yield similar performance."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-3-1032d4a67757", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/Gt6GLTkuoTs", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-5-885fc3357792", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 19}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 19}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 19}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 19}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 19}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 19}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 19}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Character RNNs", "paragraph_index": 20}, {"url": "https://engineering.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/", "anchor_text": "CNNs for Machine Translation", "paragraph_index": 20}, {"url": "http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/", "anchor_text": "Composing Music with RNNs", "paragraph_index": 20}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome back to deep learning! Today we want to talk a bit about gated recurrent units (GRUs), a simplification of the LSTM cell.", "So again a neural network: Gated recurrent units. So the idea here is that the LSTM is, of course, great, but it has a lot of parameters and it\u2019s kind of difficult to train.", "So, Cho came up with the gated recurrent unit and it was introduced in 2014 for statistical machine translation. You could argue it\u2019s a kind of LSTM, but it\u2019s simpler and it has fewer parameters.", "So this is the general setup. You can see we don\u2019t have two different memories like in the LSTM. We only have one hidden state. One similarity to the LSTM is the hidden state flows only along a linear process. So, you only see multiplications and additions here. Again, as in the LSTM, we produce from the hidden state the output.", "So let\u2019s have a look into the ideas that Cho had in order to propose this cool cell. Well, it takes the concepts from the LSTM and it controls the memory by gates. The main difference is that there is no additional cell state. So, the memory only operates directly on the hidden state. The update of the state can be divided into four steps: There\u2019s a reset gate that is controlling the influence of the previous hidden state. Then there is an update gate that introduces newly computed updates. So, the next step proposes an updated hidden state which is then used to update the hidden state.", "So, how does this work? Well, first we determine the influence of the previous hidden state. This is done by a sigmoid activation function. We again have a matrix-type of update. We concatenate the input and the previous hidden state multiplied with a matrix and add some bias. Then, feed it to this sigmoid activation function which produces some reset value r subscript t.", "Next, we produce some z. This is essentially the update proposal on the new hidden state. So, this is again produced by a sigmoid function where we concatenate the last hidden state, and the input vector multiplied with a matrix W subscript z and add some bias.", "Next, we propose the update. So we combine the input and the reset state. This is done in the following manner: So, the update proposal h tilde is produced by a hyperbolic tangent where we take the reset gate times the last hidden state. So, we essentially remove entries that we don\u2019t want to see from the last hidden state and concatenate x subscript t multiplied with some matrix W subscript h and add some bias b subscript h. This is then fed to a tanh to produce the update proposal.", "Now, with the update proposal, we go to the update gate. The update gate controls the combination of the old state and the proposed state. So, we compute the new state by multiplying 1 \u2014 z subscript t. You remember this is the intermediate variable that we computed earlier with the old state. Further, we add z subscript t times h tilde. This is the proposed update. So, essentially the sigmoid function that produced a z subscript t is now used to select whether to keep the old information from the old state or to update it with information from the new state. This gives the new hidden state. With the new hidden state, we produce the new output and notice again that we are omitting the transformation matrices in this step. So, we write this as sigmoid of h subscript t, but there\u2019s actually the transformation matrices and biases. Noth we are not noting down here. So, this thing gives the final output y hat subscript t.", "Some remarks: The addition is essential for the preservation of the error in the backpropagation. The gates allow capturing diverse timescales and remote dependencies. The units are able to learn short-term dependencies by learning restrictive gates. So, if we have an r subscript t close to zero it will ignore the previous hidden state. We can also learn long-term dependencies by having restrictive update gates. So, here we have the z subscript t close to zero which means we ignore new input the gates. Then, a varying rhythm depending on the type of information emerges. Now, you\u2019d say \u201cOk. now we have our RNN units, we have LSTM units, and GRUs. So, which one should we take?\u201d", "So, let\u2019s have a short recap. In the simple RNNs, we had gradient-based training which was difficult to do. We had exploding gradients and vanishing gradient problems with long-term dependencies. Short-term dependencies were quite good but they could potentially hide the long-term dependencies due to exponentially small gradients and that the hidden state is overwritten in each time step.", "Then, we had LSTMs and GRUs. Both of them introduced gates that operate on memory. In LSTMs, we were splitting this into cell state and hidden state. In the GRU, we only have a hidden state. Both of them have memories that are completely linear which helps us with the long-term dependencies.", "So, the similarities here, of course, are that the information is controlled by gates and the ability to capture dependencies of different time scales. The additive calculation of the state preserves the error during backpropagation. So, we can do more efficient training.", "Of course, there are also differences the LSTMs have separate hidden and cell states. So, they control the exposure of the memory content through an output gate, an input, and a forget gate. They work independently. So, they could potentially do different things. New memory content is independent of the current memory. In the GRU, we have combined hidden and cell states. So, we have full exposure to the memory content without control. There is one common update gate that produces the new hidden state with our variable z subscript t. It essentially decides to either use the old state or to use the proposed update. So, the new memory content depends on the current memory.", "You can also compare the applications because you may ask so what should be used. In [3], you see an empirical evaluation of gated recurrent neural networks on sequence modeling. They compare the simple RNN, LSTMs, and GRU networks. The tasks were polyphonic music modeling and speech signal modeling. Results indicate that the gated recurrent units clearly outperformed the regular recurrent unit. The comparison between the GRU and the LSTM was not conclusive. They had a similar performance.", "So, you could argue both of them are very well-suited for sequence modeling. One has fewer parameters but for the task presented in this paper, it didn\u2019t make a big difference so both of them are viable options.", "Well, next time in deep learning, we want to talk a bit about generating sequences. We now have recurrent neural networks and the recurrency can, of course, not be used just to process long sequences, but, of course, we can also generate sequences. Thus, we will look a bit into sequence generation. So thank you very much for listening. I hope you like this video and hope to see you in the next one. Bye-bye!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced.", "Character RNNsCNNs for Machine TranslationComposing Music with RNNs", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F39a568034d3b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://akmaier.medium.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----39a568034d3b---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39a568034d3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&user=Andreas+Maier&userId=b1444918afee&source=-----39a568034d3b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39a568034d3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&source=-----39a568034d3b---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-3-1032d4a67757", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/Gt6GLTkuoTs", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-5-885fc3357792", "anchor_text": "Next Lecture"}, {"url": "https://imgflip.com/i/495hg2", "anchor_text": "imgflip"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://towardsdatascience.com/recurrent-neural-networks-part-1-498230290534", "anchor_text": "RNN cell"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://imgflip.com/i/495iim", "anchor_text": "imgflip"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "https://folkrnn.org/competition/", "anchor_text": "FolkRNN.org"}, {"url": "https://themachinefolksession.org/tunes/", "anchor_text": "MachineFolkSession.com"}, {"url": "https://github.com/IraKorshunova/folk-rnn/blob/master/soundexamples/successes/The%20Glas%20Herry%20Comment%2014128.mp3", "anchor_text": "The Glass Herry Comment 14128"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Character RNNs"}, {"url": "https://engineering.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/", "anchor_text": "CNNs for Machine Translation"}, {"url": "http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/", "anchor_text": "Composing Music with RNNs"}, {"url": "http://www.pnas.org/content/79/8/2554.full.pdf.", "anchor_text": "http://www.pnas.org/content/79/8/2554.full.pdf."}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----39a568034d3b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----39a568034d3b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----39a568034d3b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----39a568034d3b---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----39a568034d3b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39a568034d3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&user=Andreas+Maier&userId=b1444918afee&source=-----39a568034d3b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39a568034d3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&user=Andreas+Maier&userId=b1444918afee&source=-----39a568034d3b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39a568034d3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----39a568034d3b---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----39a568034d3b---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Written by Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----39a568034d3b---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-part-4-39a568034d3b&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----39a568034d3b---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----39a568034d3b----0---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----39a568034d3b----0---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----39a568034d3b----0---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----39a568034d3b----0---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----39a568034d3b----0---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "10 Ideas to Make Money from Large Language ModelsLarge Language Models work, but what can we do with them?"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----39a568034d3b----0---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "\u00b73 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&user=Andreas+Maier&userId=b1444918afee&source=-----86f2cb31bb25----0-----------------clap_footer----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----39a568034d3b----0---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&source=-----39a568034d3b----0-----------------bookmark_preview----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----39a568034d3b----1---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----39a568034d3b----1---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----39a568034d3b----1---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----39a568034d3b----1---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----39a568034d3b----1---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----39a568034d3b----1---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----39a568034d3b----1---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----39a568034d3b----1-----------------bookmark_preview----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----39a568034d3b----2---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----39a568034d3b----2---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----39a568034d3b----2---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----39a568034d3b----2---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----39a568034d3b----2---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----39a568034d3b----2---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----39a568034d3b----2---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----39a568034d3b----2-----------------bookmark_preview----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----39a568034d3b----3---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----39a568034d3b----3---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----39a568034d3b----3---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----39a568034d3b----3---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----39a568034d3b----3---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "Gradient Descent and Back-tracking Line SearchAn Introduction to Optimization using Gradient Descent"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----39a568034d3b----3---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": "\u00b713 min read\u00b7Apr 10, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&user=Andreas+Maier&userId=b1444918afee&source=-----d8bd120bd625----3-----------------clap_footer----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----39a568034d3b----3---------------------a98aed7c_318d_4283_9c4a_a04cd636c09c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&source=-----39a568034d3b----3-----------------bookmark_preview----a98aed7c_318d_4283_9c4a_a04cd636c09c-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "See all from Andreas Maier"}, {"url": "https://towardsdatascience.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----0-----------------clap_footer----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----39a568034d3b----0-----------------bookmark_preview----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----39a568034d3b----1-----------------bookmark_preview----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----39a568034d3b----0---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----39a568034d3b----0-----------------bookmark_preview----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----39a568034d3b----1---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----39a568034d3b----1-----------------bookmark_preview----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----39a568034d3b----2---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----39a568034d3b----2---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----39a568034d3b----2---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Prateek Gaurav"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----39a568034d3b----2---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "NLP: Zero To Hero [Part 2: Vanilla RNN, LSTM, GRU & Bi-Directional LSTM]Link to Part 1of this article: NLP: Zero To Hero [Part 1: Introduction, BOW, TF-IDF & Word2Vec] Link to Part 3 of this article: NLP: Zero\u2026"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----39a568034d3b----2---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "\u00b78 min read\u00b7Mar 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&user=Prateek+Gaurav&userId=966fe9bb6729&source=-----77fd60fc0b44----2-----------------clap_footer----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----39a568034d3b----2---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&source=-----39a568034d3b----2-----------------bookmark_preview----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----39a568034d3b----3---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----39a568034d3b----3---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----39a568034d3b----3---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----39a568034d3b----3---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----39a568034d3b----3---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----3-----------------clap_footer----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----39a568034d3b----3---------------------909ae8ec_72bb_41b9_a79b_fd89ad948063-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----39a568034d3b----3-----------------bookmark_preview----909ae8ec_72bb_41b9_a79b_fd89ad948063-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----39a568034d3b--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----39a568034d3b--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}