{"url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036", "time": 1683004040.7863119, "path": "towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036/", "webpage": {"metadata": {"title": "Regularization in Deep Learning \u2014 L1, L2, and Dropout | Towards Data Science", "h1": "Regularization in Deep Learning \u2014 L1, L2, and Dropout", "description": "Regularization prevents Overfitting in Neural Networks and improves the accuracy of the Deep Learning models when facing new data from the problem domain."}, "outgoing_paragraph_urls": [{"url": "https://www.deeplearning-academy.com/p/ai-wiki-overfitting-underfitting", "anchor_text": "overfitting in more detail in this article", "paragraph_index": 1}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe", "paragraph_index": 47}], "all_paragraphs": ["Regularization is a set of techniques that can prevent overfitting in neural networks and thus improve the accuracy of a Deep Learning model when facing completely new data from the problem domain. In this article, we will address the most popular regularization techniques which are called L1, L2, and dropout.", "One of the most important aspects when training neural networks is avoiding overfitting. We have addressed the issue of overfitting in more detail in this article.", "However let us do a quick recap: Overfitting refers to the phenomenon where a neural network models the training data very well but fails when it sees new data from the same problem domain. Overfitting is caused by noise in the training data that the neural network picks up during training and learns it as an underlying concept of the data.", "This learned noise, however, is unique to each training set. As soon as the model sees new data from the same problem domain, but that does not contain this noise, the performance of the neural network gets much worse.", "\u201cWhy does the neural network picks up that noise in the first place?\u201d", "The reason for this is that the complexity of this network is too high. A fit of a neural network with higher complexity is shown in the image on the right-hand side.", "The model with a higher complexity is able to pick up and learn patterns (noise) in the data that are just caused by some random fluctuation or error. The network would be able to model each data sample of the distribution one-by-one, while not recognizing the true function that describes the distribution.", "New arbitrary samples generated with the true function would have a high distance to the fit of the model. We also say that the model has a high variance.", "On the other hand, the lower complexity network on the left side models the distribution much better by not trying too hard to model each data pattern individually.", "In practice, overfitting causes the neural network model to perform very well during training, but the performance gets much worse during inference time when faced with brand new data.", "In short: Less complex neural networks are less susceptible to overfitting. To prevent overfitting or a high variance we must use something that is called regularization.", "Simple speaking: Regularization refers to a set of different techniques that lower the complexity of a neural network model during training, and thus prevent the overfitting.", "There are three very popular and efficient regularization techniques called L1, L2, and dropout which we are going to discuss in the following.", "The L2 regularization is the most common type of all regularization techniques and is also commonly known as weight decay or Ride Regression.", "The mathematical derivation of this regularization, as well as the mathematical explanation of why this method works at reducing overfitting, is quite long and complex. Since this is a very practical article I don\u2019t want to focus on mathematics more than it is required. Instead, I want to convey the intuition behind this technique and most importantly how to implement it so you can address the overfitting problem during your deep learning projects.", "During the L2 regularization the loss function of the neural network as extended by a so-called regularization term, which is called here \u03a9.", "The regularization term \u03a9 is defined as the Euclidean Norm (or L2 norm) of the weight matrices, which is the sum over all squared weight values of a weight matrix. The regularization term is weighted by the scalar alpha divided by two and added to the regular loss function that is chosen for the current task. This leads to a new expression for the loss function:", "Alpha is sometimes called as the regularization rate and is an additional hyperparameter we introduce into the neural network. Simply speaking alpha determines how much we regularize our model.", "In the next step we can compute the gradient of the new loss function and put the gradient into the update rule for the weights:", "Some reformulations of the update rule lead to the expression which very much looks like the update rule for the weights during regular gradient descent:", "The only difference is that by adding the regularization term we introduce an additional subtraction from the current weights (first term in the equation).", "In other words independent of the gradient of the loss function we are making our weights a little bit smaller each time an update is performed.", "In the case of L1 regularization (also knows as Lasso regression), we simply use another regularization term \u03a9. This term is the sum of the absolute values of the weight parameters in a weight matrix:", "As in the previous case, we multiply the regularization term by alpha and add the entire thing to the loss function.", "The derivative of the new loss function leads to the following expression, which the sum of the gradient of the old loss function and sign of a weight value times alpha.", "The question you might be asking yourself right now is:", "\u201cWhy does all of this help to reduce the overfitting issue?\u201d", "Please consider the plots of the and functions, where represents the operation performed during L1 and the operation performed during L2 regularization.", "In the case of L2 regularization, our weight parameters decrease, but not necessarily become zero, since the curve becomes flat near zero. On the other hand during the L1 regularization, the weight are always forced all the way towards zero.", "We can also take a different and more mathematical view on this.", "In the case of L2, you can think of solving an equation, where the sum of squared weight values is equal or less than a value s. s is the constant that exists for each possible value of the regularization term \u03b1. For just two weight values W1 and W2 this equation would look as follows: W1 \u00b2 + W\u00b2\u00b2 \u2264 s", "On the other hand, the L1 regularization can be thought of as an equation where the sum of modules of weight values is less than or equal to a value s. This would look like the following expression: |W1| + |W2| \u2264 s", "Basically the introduced equations for L1 and L2 regularizations are constraint functions, which we can visualize:", "The left image shows the constraint function (green area) for the L1 regularization and the right image shows the constraint function for the L2 regularization. The red ellipses are contours of the loss function that is used during the gradient descent. In the center of the contours there is a set of optimal weights for which the loss function has a global minimum.", "In the case of L1 and L2 regularization, the estimates of W1 and W2 are given by the first point where the ellipse intersects with the green constraint area.", "Since L2 regularization has a circular constraint area, the intersection won\u2019t generally occur on an axis, and this the estimates for W1 and W2 will be exclusively non-zero.", "In the case of L1, the constraints area has a diamond shape with corners. And thus the contours of the loss function will often intersect the constraint region at an axis. Then this occurs, one of the estimates (W1 or W2) will be zero.", "In a high dimensional space, many of the weight parameters will equal zero simultaneously.", "Intuitively speaking smaller weights reduce the impact of the hidden neurons. In that case, those hidden neurons become neglectable and the overall complexity of the neural network gets reduced.", "As mentioned earlier: less complex models typically avoid modeling noise in the data, and therefore, there is no overfitting.", "But you have to be careful. When choosing the regularization term \u03b1. The goal is to strike the right balance between low complexity of the model and accuracy", "In addition to the L2 and L1 regularization, another famous and powerful regularization technique is called the dropout regularization. The procedure behind dropout regularization is quite simple.", "In a nutshell, dropout means that during training with some probability P a neuron of the neural network gets turned off during training. Let\u2019s look at a visual example.", "Assume on the left side we have a feedforward neural network with no dropout. Using dropout with let\u2019s say a probability of P=0.5 that a random neuron gets turned off during training would result in a neural network on the right side.", "In this case, you can observe that approximately half of the neurons are not active and are not considered as a part of the neural network. And as you can observe the neural network becomes simpler.", "A simpler version of the neural network results in less complexity that can reduce overfitting. The deactivation of neurons with a certain probability P is applied at each forward propagation and weight update step.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning & AI Software Developer | MSc. Physics | https://artem-oppermann.medium.com/subscribe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F377e75acc036&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----377e75acc036--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----377e75acc036--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----377e75acc036--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----377e75acc036--------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----377e75acc036---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.spacetelescope.org/images/heic0611b/", "anchor_text": "https://www.spacetelescope.org/images/heic0611b/"}, {"url": "https://www.deeplearning-academy.com/p/ai-wiki-overfitting-underfitting", "anchor_text": "overfitting in more detail in this article"}, {"url": "https://www.researchgate.net/publication/332412613", "anchor_text": "https://www.researchgate.net/publication/332412613"}, {"url": "https://www.deeplearning-academy.com/p/ai-wiki-regularization", "anchor_text": "https://www.deeplearning-academy.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----377e75acc036---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----377e75acc036---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----377e75acc036---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----377e75acc036---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----377e75acc036---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&user=Artem+Oppermann&userId=619319ac8220&source=-----377e75acc036---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&user=Artem+Oppermann&userId=619319ac8220&source=-----377e75acc036---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----377e75acc036--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F377e75acc036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----377e75acc036---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----377e75acc036--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----377e75acc036--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----377e75acc036--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----377e75acc036--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----377e75acc036--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----377e75acc036--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----377e75acc036--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----377e75acc036--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://artem-oppermann.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc09555d6711e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&newsletterV3=619319ac8220&newsletterV3Id=c09555d6711e&user=Artem+Oppermann&userId=619319ac8220&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}