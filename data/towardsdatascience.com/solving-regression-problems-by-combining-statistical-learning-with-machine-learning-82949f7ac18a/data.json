{"url": "https://towardsdatascience.com/solving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a", "time": 1682999786.7068422, "path": "towardsdatascience.com/solving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a/", "webpage": {"metadata": {"title": "Solving regression problems by combining statistical learning with machine learning | by Andrea Xue | Towards Data Science", "h1": "Solving regression problems by combining statistical learning with machine learning", "description": "Statistical learning and machine learning are usually deemed as two separate camps in the world of data science. People in the first camp often pivot more towards understanding the statistical\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/airbnb/seattle", "anchor_text": "Airbnb Seattle dataset", "paragraph_index": 4}, {"url": "https://www3.nd.edu/~rwilliam/stats1/x92b.pdf", "anchor_text": "article1", "paragraph_index": 66}, {"url": "https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/", "anchor_text": "article2", "paragraph_index": 66}], "all_paragraphs": ["Statistical learning and machine learning are usually deemed as two separate camps in the world of data science. People in the first camp often pivot more towards understanding the statistical significance while the latter cares more about the model prediction performance. But in reality, these two different approaches are complementary when going hand in hand to solve a data science problem.", "Most of the data science projects fall into two main categories \u2014 a regression problem(when the target variable is continuous/numerical) or classification problem(when the target variable(s) are discrete/categorical). This post will focus solely on how to approach a regression problem by combining both statistics and machine learning step-by-step.", "Linear regression is usually the very first model we learned in any data science courses. It\u2019s simple, straightforward and best of all it can produce interpretable coefficients. Later on, we may have learned many other more sophisticated and higher performing tree-based algorithms like Random Forest, and Gradient Boosting Trees, etc that can solve both regression and classification problems but with more flexibility and tolerance in the underlying assumption of the features.", "The tree family algorithms are so great that they become our go-to models, but they somehow make us lazier since they need very little effort in feature preprocessing. However, working with basic algorithms like linear regression renders great opportunities to learn how to select and massage the data, which is a skill I believe to be more important than running the models itself.", "In this post, I will guide you through a regression project using the latest version of Airbnb Seattle dataset from Kaggle. It is intended for readers with basic knowledge of the regression algorithms. The purpose is more to illustrate the entire workflow when you are given a dataset without any explicit goal or direction \u2014 you are expected to define your own question of interest and figure out a roadmap to solve it.", "The main questions of interest for this project are:", "Like any typical data science projects, my approach to the problem I just proposed will follow this flowchart below. It is an iterating process through all the steps rather than a one-way tunnel.", "First, let\u2019s take a look at the data. There are 3 datasets available on Kaggle:", "Listing.csv: contains a comprehensive list of attributes about the place, the host and the average review score", "Calendar.csv: includes listing id and the price and availability for each day", "Review.csv: includes listing id, reviewer id, and detailed comments", "For this project, since my goal is to predict the listing prices, I will be only using the listing dataset. The review dataset is great for sentiment analysis to predict the listing ratings, but I will leave that part for my future post.", "There are 93 columns in total in the listing data. It\u2019s hard to examine all the columns at once with a data frame head call, so I will split them by data types and check them in chunks.", "Couple things caught my eye by glancing through the columns. First of all, there are a few features that are supposed to be numerical are currently in the \u201cobject\u201d data type, such as \u2018weekly_price\u2019, \u2018monthly_price\u2019, \u2018security_deposit\u2019, \u2018cleaning_fee\u2019, \u2018extra_people\u2019, etc. Here is what they look like right now. I will convert them to numerical in the data cleaning step later.", "I also realized two feature groups that might contain very similar information- one is about the listing neighborhood, the other is about review ratings. Let\u2019s examine them one by one.", "It seems that \u201cneighbourhood_cleansed\u201d is almost identical to \u201cneighborhood\u201d except that the latter contains NaN while the former doesn\u2019t. I will discard the \u201cneighborhood\u201d column in favor of \u201cneighbourhood_cleansed\u201d since the latter is a cleaner version and doing so won\u2019t cause any information loss.", "For the review feature group, \u201creview_scores_rating\u201d seems a weighted sum of the other 6 individual rating categories. I think there might be strong collinearity in this feature group that I can just use the overall rating to represent the rest in the group. I\u2019d like to test out this theory later.", "1.1 Do super hosts always have higher ratings than regular Airbnb hosts? Do super hosts also charge differently than the regular hosts?", "The first hypothesis I wanted to check is the difference in review rating and price between regular and super hosts. The violin charts below show the review score and listing price distributions between super hosts and regular hosts only and all hosts combined.", "The review score rating shows an evident distinction between the super hosts and regular hosts \u2014 the majority of super host ratings clutter around 90\u2013100, compared to only 61% ratings above 90 for the regular hosts, due to the long tail in the lower ratings. The price distribution, on the other hand, seems to be the same for both types of hosts except a few rare extremely high price outliers from the regular hosts.", "1.2 What are the average listing prices of different property types by neighborhood?", "The original property types in the dataset are fairly scattered. I cleaned it up by grouping them into 4 main categories. From the bar chart below we get can get an idea about how average listing prices vary across different neighborhoods and property types.", "It seems that prices of apartment listings have much smaller variations in different neighborhoods compared to the other three property groups. Among all the neighborhoods, Downtown and Magnolia are in general the most expensive in most popular property types \u2014 apartment, house, townhouse, and condom.", "This chart also confirms that neighborhood and property type definitely has an impact on the listing prices.", "1.3 Is there multicollinearity among the review feature group?", "I divided it by 10 to bring the \u201creview_scores_rating\u201d to the same 1\u201310 scale as the rest of the features in this group. Here is how their distributions overlap with each other. They all look heavily right-skewed.", "To further test my collinearity theory, I think the easiest way is to run a simple linear regression to see if \u201creview_scores_rating\u201d can be represented by a linear equation of rest 6 review features.", "And clearly, the answer is yes. So I can be confident that I can use the overall rating to represent the others without losing much information and reduce multicollinearity in the linear model.", "Since my goal is to predict the listing price and understand the differences between regular hosts and super hosts, I want to only include attributes that are related to the listings and the hosts. Below are the features I decided to keep.", "As we have found in the exploratory analysis, some numeric columns are currently all in string format. Those columns below should be transformed into float type by stripping the \u201c%\u201d and \u201c$\u201d signs from the values.", "After a simple cleaning step, now they look good as numerical.", "Now the numerical features have been taken care of, let\u2019s take a look at correlation heatmap.", "Clearly, there are two main correlated groups from the heatmap. One group is about the listing facilities like the number of bedrooms, bathrooms, the number of people it can accommodate, etc. I want to keep all the features in this group because they all tell the fundamental information about the listing itself. The other correlated group is about review scores as we have already discussed before. I decided to keep only one overall review score \u201creview_scores_rating\u201d and drop the rest of the review columns.", "Before I go ahead dummy code all the categorical variables and triple the number of features, I want to see how effective they are to explain the variability of the listing price to decide if I want to use all of them for the model. This step is optional but I found it especially helpful when the dataset contains many categorical features with dozens of levels.", "You may have read about the three feature selection techniques from other articles on Medium: filtering method using statistical tests, wrapper method and embedded method using machine learning algorithms like LASSO and Random Forest.", "Since I want to test the categorical features against a continuous target variable, the easiest statistical filtering method is the ANOVA test. For those who are confused about which statistical test can be used for what type of data as I used to be, this is a very useful reference.", "The code below loops through each categorical feature in the data and calculates the F statistics and P-value from the two-way ANOVA test against the target variable \u201cprice\u201d.", "The definition of F statistics (AVONA) = between-group variances of the population mean/ within-group variances of the sample mean.", "If the P-value is high than 0.05 (rule of thumb of the significance level) and F statistics is close to 1, that means the within-group(within each categorical level) variance of the price is the same as between-group(between different categorical levels) variance, so we can say the categorical feature is independent of the continuous variable (in this case, price) and it does not have any predicting power because there is no observed variance in price between categorical levels.", "Conversely, the higher the F test statistics, the more the differences are of average prices between different levels of the categorical variable, and the more variance of price the categorical feature can explain, the more power it has in predicting the price.", "Apparently, from the ANOVA test result above, we can first eliminate the bottom two features because their P-values are too high to be considered statistically significant. Also given their small F statistics, we can conclude there is ver small variability of price between the two levels of host_is_superhost and instant_bookable.", "Also, there are two features related to neighbourbood. From the exploratory analysis above, it is obvious that neighborhoods affect the listing price. But since both of them basically contain very similar information except that neighborhood_cleansed is in a more granular level, I want to keep only one of them. The F statistics gives a pretty clear idea about which one to choose. I will keep neighbourhood_group_cleansed because it has a higher F score.", "After finishing up the feature selection, let\u2019s take a look at the missing values in the dataset.", "square_feet has over 97% NaN values, so I will drop it. For the rest of the numerical features, I will impute the NaN values in the machine learning pipeline later to avoid data leakage in the test set. The final step is converting the categorical features into dummy variables. Now the dataset is ready, let\u2019s go straight to the modeling.", "Model 1: Linear regression on original price", "First I want to start with the simplest model \u2014 linear regression to see without any treatment of the feature, what the baseline would look like.", "I used the handy regressor package to get the model coefficients P values.", "Without any feature standardization, the adjusted R-squared for the training set is about 0.62. But if we take a look at the p values, almost all the p values are too large to be statistically significant. The only few features that have p<0.05 are \u201caccommodate\u201d, \u201cbedroom\u201d, \u201cbathroom\u201d and \u201creviews_per_month\u201d. That means only these couple variables contribute to the model performance even if most features have large coefficients.", "Right now there are 51 features excluding the target variable. Since I kind of hand-picked the features in the very beginning, it\u2019s only based on my intuition what I think is relevant to the listing price, but I cannot be sure if that\u2019s true. To make sure the linear model doesn\u2019t get \u201cdistracted\u201d by some irrelevant features which usually causes overfitting, I will apply regularizations to the linear regression model that penalize large coefficients in return for a simpler model to reduce the variance in the absence of sufficient training data.", "L1 LASSO regularization has both the advantage of preventing overfitting and feature selection due to its nature of the penalty term that tends to shrink the coefficient weights of less irrelevant features to zero in the course of minimizing the cost function SSE. Also given we have a small dataset (only 3818 listings in the whole dataset), I also want to leverage cross-validation during the training to make the model more robust in generalization performance.", "Last but not least, the dataset contains numerical features that vary in a wide range of scale. So I need to do two preprocessing steps in between the train test split and running the model. First, fill in the missing values in the numerical variables with the median. And second, standardize the values. With sklearn pipeline, these sequential steps can be made into one.", "Model 2: LASSO regression on original price", "Note that this time I only standardized numerical features and left dummy variables intact. The reason I treated them differently is that I care not only about the prediction accuracy but I want to later interpret the coefficients of dummy variables in an intuitive way. If your purpose is only to predict and you don\u2019t care about the interpretability of the coefficients, you may not need to treat them separately.", "M1 Model performance measured by R-squared:", "R-squared is to measure the goodness of fit. It is the proportion of the variance for a dependent variable that\u2019s explained by independent variables in a model.", "It is not much different from the last linear regression. But I\u2019m more curious about the coefficients.", "Evidently, LASSO has effectively pushed some feature coefficients to zero like the ones in the middle.", "Since all the numerical features in the training data have been standardized, the 1 unit change in any of the numerical features is no longer in their original scale but by the standard deviation. We did not standardize the categorical variables, so their unit of change is unchanged.", "So we can say if the number of people the listing accommodates increases by 1 standard deviation(~ 2 people), the price of the listing will increase by $23 on average, holding everything else constant. Or if the review scores rating increases by 1 std(6.6 in the scale of 100), the price will go up by $27.5 on average. For categorical variables, the average price of a listing in the downtown area is $33 more expensive than that of a similar listing elsewhere.", "Model 3: LASSO regression on log-transformed price", "Now let\u2019s take a look at the distribution of the target variable price. It\u2019s clearly in a long tail shape. A natural log transformation can bring it to normal distribution. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.", "Run the same pipeline again but this time on np.log(y_train).", "The test set performance improved by 12% so log-transformation of the target variable did help the prediction.", "We fitted the model in this form: Log(Y) ~ scaled(numerical variables) + dummy variables. The general interpretation for the log-transformed variable is like this: 1 unit increase in X will lead to (exp(coefficient)-1)*100%.", "Since the numerical data is standardized, the 1 unit still means 1 standard deviation. Using the \u201caccommodates\u201d as an example, 1 std increase of the number of bathrooms(about 0.6 bathrooms) will raise the listing price by 24% ((exp(0.22)-1)*100%) on average.", "The bottom feature on the chart the Lake City area is most negatively correlated to the price. If a listing is in the Lake City area, the price will be 36% ((exp(-0.45)-1)*100%) lower than non-Lake City area on average, holding everything else constant.", "For more details in coefficient interpretation in the context of standardized independent or dependent variable, I recommend reading these two articles article1, article2. Both provided excellent explanations in great detail.", "Model 4: Random Forest Regressor on log-transformed price", "Since the linear model did not produce very satisfying R-squared results, I want to switch to the tree-based models because maybe there are no linear relations between the predictors and the price.", "With the tree-based models, I don\u2019t need to standardized the features because the decision trees decide on the best split based only on \u201cinformation gain\u201d or \u201cdecrease of impurity(entropy)\u201d, scaling the features would not make any difference to tree models splits. I will fit Random Forest Regressor still on the log-transformed price to compare the performance with the previous model with GridSearchCV to find its optimal parameters.", "The test set R-squared score has improved by 3% from 69% with LASSO on the same log-transformed price.", "The residual vs. fitted value plot below shows how far away the predicted prices are from the actual prices. Unfortunately, the residuals do not look randomly scattered around the blue line. There are some outliers with very high error from the actual values.", "The Random Forest model finds a different set of most important features, particularly the room type, bedrooms, and bathrooms are the three dominating features. Let\u2019s take a look at how the room types affect the price.", "There is a clear pricing tier among the 3 room types of listing- the average price follows this order: shared room< private room <entire home/apt. There are outliers of extremely highly-priced listings that belong to the entire home category. No wonder room type = entire home/apt is identified as the top feature by random forest.", "After several attempts trying different models, the R-squared score in the test set is still not above 0.8. I think it can be caused by a couple of reasons, the biggest one being the data sample is too small(only about 3000). It is hard to draw a generalized pattern with this small sample size and that also leads to overfitting in the training set for random forest.", "One way to fix this is to get more Airbnb listing data for other cities to increase the data samples. I can also try running gradient boosting tree model for a better prediction accuracy when there is sufficient data collected.", "Statistical learning and machine learning are two indispensable parts to address regression problems. While machine learning provides us with more sophisticated models for predictions, statistical tests can be useful in feature selection, multicollinearity detection and to tell the statistical significance of regression coefficients. As data scientists, we are expected to know both sides really well.", "Thanks for reading! As the learning never stops, I will keep updating this post with more results as I progress through the iterations. Stay tuned.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A data scientist and nature lover who never stops learning and hiking new heights."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F82949f7ac18a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andrea.xyw?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrea.xyw?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "Andrea Xue"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3cb7e36d5b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&user=Andrea+Xue&userId=3cb7e36d5b8c&source=post_page-3cb7e36d5b8c----82949f7ac18a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F82949f7ac18a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F82949f7ac18a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@benjamiindutton?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Ben Dutton"}, {"url": "https://unsplash.com/search/photos/airbnb-seattle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/airbnb/seattle", "anchor_text": "Airbnb Seattle dataset"}, {"url": "https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/", "anchor_text": "source: Analytics Vindhya"}, {"url": "https://www3.nd.edu/~rwilliam/stats1/x92b.pdf", "anchor_text": "article1"}, {"url": "https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/", "anchor_text": "article2"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----82949f7ac18a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/regression?source=post_page-----82949f7ac18a---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/tag/statistical-learning?source=post_page-----82949f7ac18a---------------statistical_learning-----------------", "anchor_text": "Statistical Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F82949f7ac18a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&user=Andrea+Xue&userId=3cb7e36d5b8c&source=-----82949f7ac18a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F82949f7ac18a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&user=Andrea+Xue&userId=3cb7e36d5b8c&source=-----82949f7ac18a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F82949f7ac18a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F82949f7ac18a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----82949f7ac18a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----82949f7ac18a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----82949f7ac18a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----82949f7ac18a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrea.xyw?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andrea.xyw?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrea Xue"}, {"url": "https://medium.com/@andrea.xyw/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "79 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3cb7e36d5b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&user=Andrea+Xue&userId=3cb7e36d5b8c&source=post_page-3cb7e36d5b8c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69639d48d09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-regression-problems-by-combining-statistical-learning-with-machine-learning-82949f7ac18a&newsletterV3=3cb7e36d5b8c&newsletterV3Id=69639d48d09&user=Andrea+Xue&userId=3cb7e36d5b8c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}