{"url": "https://towardsdatascience.com/regularization-part-2-5b729698d026", "time": 1683010339.697071, "path": "towardsdatascience.com/regularization-part-2-5b729698d026/", "webpage": {"metadata": {"title": "Regularization in Deep Learning - Part 2 | Towards Data Science", "h1": "Regularization \u2014 Part 2", "description": "In this blog, we describe classical techniques such as early stopping and L1 and L2 weight regularization."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/regularization-part-1-db408819b20f", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/1RqnSkp9YS0", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/regularization-part-4-2ee8e7aa60ec", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/the-seven-sins-of-machine-learning-54dbf63fd71d", "anchor_text": "very likely to be a complete overestimate of the performance", "paragraph_index": 3}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 15}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 15}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 15}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 15}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 15}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 15}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 15}, {"url": "https://www.springer.com/us/book/9780387310732", "anchor_text": "Link", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1206.5533", "anchor_text": "Link", "paragraph_index": 16}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&rep=rep1&type=pdf", "anchor_text": "Link", "paragraph_index": 16}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome back to Deep Learning! We want to continue our analysis of regularization methods and today I want to talk about classical techniques.", "So here is a typical example of a loss curve over the iterations on the training set. What I show here on the right-hand side is the loss curve on the test set. You see that although the training loss goes down the test loss goes up. So at some point, the training data set is overfitted and it doesn\u2019t produce a model that is representative of the data anymore. By the way, always keep in mind that the test set must never be used for training. If you\u2019re trained on your test set, then you will get very good results but it\u2019s very likely to be a complete overestimate of the performance. So there\u2019s the typical situation that somebody who runs into my office and says: \u201cYes! I have 99% recognition rate!\u201d. The first thing that somebody in pattern recognition or machine learning does when he reads \u201c99% recognition rate\u201d is ask: \u201cDid you train on your test data?\u201d This is the very first thing you make sure that has not happened. When you did some stupid mistake, there\u2019s some data set pointer that was not pointing to the right data set and suddenly your recognition rate jumps up. So, be careful if you have very good results. Always scrutinize that they are really appropriate and they\u2019re really general. So instead if you want to produce curves like the ones that I\u2019m showing here you may want to use a validation set that you take off the training data set. You never use this set in training but you can use it to get an estimate for your model overfitting.", "So, if you do that then we can already use the first trick. You use the validation set. We observe at what point we have the minimum error in the validation set is. If we\u2019re at this point, we can use that as a stopping criterion and use that model for our test evaluation. So it\u2019s a common technique to use the parameters with the minimum validation results.", "Another very useful technique is data augmentation. So, the idea here is to artificially enlarge the data set. There are transformations on the label which should be invariant to the class. Let\u2019s say you have the image of a cat and you rotate it by 90 degrees, it still shows a cat. Obviously, those augmentation techniques have to be done carefully. So, in the right-hand example, you can see that a rotation by 180 degrees is probably not a good way of augmenting numbers, because it may switch the label.", "So, there are very common transformations here: Random spatial transforms like affine or elastic transforms. Then, there are pixel transforms like changing the resolution, changing noise, or changing pixel distributions like color brightness, and so on. So these are typical augmentation techniques in image processing.", "What else? We can regularize the loss function. Here, we can see that this essentially leads to the maximum a-posteriori (MAP) estimation. We can do this in a Bayesian approach where we want to consider the uncertain weights w. They follow a prior distribution p(w). If you have some data set X with some associated labels Y, we can see that the joint probability p(w, Y, X) is the probability p(w|Y, X) times the probability p(Y, X). We can reformulate that into the probability P(Y|X, w) times the probability p(X, w). From these equalities, we can derive the Bayes theorem that the conditional probability p(w|Y, X) can be expressed as the probability p(Y|X, w) times the probability P(X, w) divided by the probability p(Y, X). So this, we can rearrange a bit further and here you see that then the probability p(X) the probability p(Y|X) pop up. By removing the terms that are independent of w, this yields the MAP estimate. So, we can actually seek to maximize the joint probability as maximizing the conditional probability p(Y|X, w) times the probability p(w). So typically, we solve this as a maximum likelihood estimator following the left-hand side problem times the prior on w which is the right-hand side part. We can say this is a maximum likelihood estimator, but we augment it with some additional prior information. Here, the prior information is that we have some knowledge about the distribution of w for example w could be sparse. We can also use some other source of knowledge where we know something about w. In image processing what is used very often is for example that natural images are sparse with respect to the gradients so there are all kinds of sparsities that can be employed by such a prior.", "Now the interesting part is that this MAP estimator can be reformulated and if you attended pattern recognition, you know what I\u2019m talking about. We\u2019ve seen that the maximization of the maximum likelihood estimates results in a minimization of the negative log-likelihood. The typical loss functions that we\u2019re talking about have this form. Now, if you start with the map estimate, you essentially end up with a very similar estimate but the shape of the loss function has slightly changed. So, we get a new loss function L tilde. It\u2019s like the L2 loss or the cross-entropy loss plus some \u03bb and some constraint on the weights w. So here, we enforce a minimum l2 norm. Now with a positive \u03bb, we can identify this by the way as the lagrangian function of minimizing the loss function subject to constrained L2 norm of w being smaller than \u03b1 with some unknown data-dependent \u03b1. So this is exactly the same formulation. We can now bring this into the backpropagation of the augmented loss. How this is typically implemented? You follow the loss of the gradient. So, this is the right-hand part of the loss that we already computed with the learning rate \u03b7 and in addition, you apply this kind of shrinkage to w. So, the shrinkage step here can be used in order to implement the additional L2 regularization. So the nice thing is now that we can simply compute the backpropagation as we used to do it. Then, in addition, we use the shrinkage in the weight update. So we also get very simple weight updates and they allow us to involve those regularizers. If we choose different regularizers, the shrinkage functions change. If we would optimize the training loss now for \u03bb we would usually receive \u03bb = 0 because every time we introduce regularization, we are doing something that is not optimal with respect to training loss. Of course, we introduced it because we want to reduce overfitting. So this is something that we cannot observe directly in our training data but we want to get better properties on an unseen test set. This will even increase the loss value of our training data. So be careful of that. Again, we increase the bias for a reduced variance.", "Here, we have a visualization of the effect of the L2 regularizer. The unregularized loss would of course result in the center of the ellipses in red. But now you do the additional regularization which enforces your w to be small. This means that the farther you away from the origin the higher your L2 loss will be. So, the l2 loss pulls you away from the data optimal loss with respect to your training dataset. Hopefully, it describes some prior knowledge that we haven\u2019t seen this way in the training dataset. Hence, it will then result in a model that is simply better suited for the unseen test data set.", "We can also use other norms, for example, the L1 norm. So here, we then again end up in a Lagrangian formulation where we have the original loss function subject to the L1 norm being smaller than some value \u03b1 with an unknown data-dependent \u03b1. Here, we simply get a different shrinkage operation which now involves the use of the sign function. So this is again an implication of the sub-gradient. Here, a different way of shrinkage has to be employed in order to make this optimization feasible. Again, we used exactly the same gradient for loss function as we used previously. So only the shrinkage is replaced.", "Now, we can also visualize this in our small graph. The shape of the L1 norm is of course different. With L2, we had this circle and with the L1 norm, we get a diamond looking shape. Now, you can see that the minima that are selected are likely to be located on the coordinate axis. So, if you try to find the minimum position of this L1 norm and the unregularized loss, you will see that the point of minimum unregularized loss intersected with this L1 loss is essentially on the y-axis in this plot. This is a solution that is very sparse meaning that we only have entries for y in our weight vector and the entry for x is very close to 0 or equals to 0 in this case. So if you want weights to be sparse or if you want to create networks with few connections, you may want to introduce an additional L1 regularization on the weights. This will cause sparse weights.", "What else? There are also more known constraints for example we can set a limit on the norm of the weights. Here, we just enforce them to be below a certain maximum. We want to have the magnitude of w to be below \u03b1 where \u03b1 is a positive constant. If you do so, we essentially have to project onto the unit ball with every parameter update and this is again a kind of shrinkage that essentially prohibits exploding gradients. Be careful, it may also simply hide them such that you don\u2019t see them anymore.", "There are many other variants of changing the loss. You can have a constraint and an individual \u03bb for every layer. So you could constrain every layer differently but we haven\u2019t seen any gains reported in the literature. Instead of the weights, also the activations can be constrained. This leads to different variants for example in sparse auto-encoders. We will talk about this how they were not regularizing the weights, but the activations to form a specific distribution to produce sparse activations. This is also a very interesting problem and we will talk about this a bit more when we talk about autoencoders and unsupervised learning.", "So next time in deep learning, we want to continue to talk about regularization methods. We will look into the very typical ones that are particularly made for deep learning. So very interesting approaches that are slightly different from what you\u2019ve seen in this lecture. So thank you very much for watching and goodbye!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a clap or a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced.", "Link \u2014 for details on Maximum A Posteriori estimation and the bias-variance decompositionLink \u2014 for a comprehensive text about practical recommendations for regularizationLink \u2014 the paper about calibrating the variances", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b729698d026&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://akmaier.medium.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----5b729698d026---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b729698d026&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&user=Andreas+Maier&userId=b1444918afee&source=-----5b729698d026---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b729698d026&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&source=-----5b729698d026---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "https://towardsdatascience.com/regularization-part-1-db408819b20f", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/1RqnSkp9YS0", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/regularization-part-4-2ee8e7aa60ec", "anchor_text": "Next Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://towardsdatascience.com/the-seven-sins-of-machine-learning-54dbf63fd71d", "anchor_text": "very likely to be a complete overestimate of the performance"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "https://www.springer.com/us/book/9780387310732", "anchor_text": "Link"}, {"url": "https://arxiv.org/abs/1206.5533", "anchor_text": "Link"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&rep=rep1&type=pdf", "anchor_text": "Link"}, {"url": "http://www.deeplearningbook.org.", "anchor_text": "http://www.deeplearningbook.org."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5b729698d026---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5b729698d026---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5b729698d026---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----5b729698d026---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5b729698d026---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b729698d026&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&user=Andreas+Maier&userId=b1444918afee&source=-----5b729698d026---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b729698d026&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&user=Andreas+Maier&userId=b1444918afee&source=-----5b729698d026---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b729698d026&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----5b729698d026---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----5b729698d026---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Written by Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=post_page-----5b729698d026--------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----5b729698d026---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-2-5b729698d026&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----5b729698d026---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----5b729698d026----0---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----5b729698d026----0---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----5b729698d026----0---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----5b729698d026----0---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----5b729698d026----0---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "10 Ideas to Make Money from Large Language ModelsLarge Language Models work, but what can we do with them?"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----5b729698d026----0---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "\u00b73 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&user=Andreas+Maier&userId=b1444918afee&source=-----86f2cb31bb25----0-----------------clap_footer----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----5b729698d026----0---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&source=-----5b729698d026----0-----------------bookmark_preview----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b729698d026----1---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5b729698d026----1---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----5b729698d026----1---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5b729698d026----1---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b729698d026----1---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b729698d026----1---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----5b729698d026----1---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----5b729698d026----1-----------------bookmark_preview----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b729698d026----2---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----5b729698d026----2---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----5b729698d026----2---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----5b729698d026----2---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b729698d026----2---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b729698d026----2---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----5b729698d026----2---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----5b729698d026----2-----------------bookmark_preview----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----5b729698d026----3---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----5b729698d026----3---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----5b729698d026----3---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----5b729698d026----3---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----5b729698d026----3---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "Gradient Descent and Back-tracking Line SearchAn Introduction to Optimization using Gradient Descent"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----5b729698d026----3---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": "\u00b713 min read\u00b7Apr 10, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&user=Andreas+Maier&userId=b1444918afee&source=-----d8bd120bd625----3-----------------clap_footer----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----5b729698d026----3---------------------47f2733e_8df6_4a3f_b484_627ec58b93be-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&source=-----5b729698d026----3-----------------bookmark_preview----47f2733e_8df6_4a3f_b484_627ec58b93be-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "See all from Andreas Maier"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----0-----------------clap_footer----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----5b729698d026----0-----------------bookmark_preview----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----5b729698d026----1-----------------bookmark_preview----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----0-----------------clap_footer----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----5b729698d026----0---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----5b729698d026----0-----------------bookmark_preview----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----5b729698d026----1---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----5b729698d026----1-----------------bookmark_preview----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----5b729698d026----2---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://medium.com/@dimitris.effrosynidis?source=read_next_recirc-----5b729698d026----2---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://medium.com/@dimitris.effrosynidis?source=read_next_recirc-----5b729698d026----2---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Dimitris Effrosynidis"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5b729698d026----2---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----5b729698d026----2---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Ensemble Feature Selection for Machine LearningSelect the best features by combining individual feature selection methods"}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----5b729698d026----2---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "\u00b75 min read\u00b7Nov 2, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc0df77b970f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-feature-selection-for-machine-learning-c0df77b970f9&user=Dimitris+Effrosynidis&userId=ff294d269093&source=-----c0df77b970f9----2-----------------clap_footer----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ensemble-feature-selection-for-machine-learning-c0df77b970f9?source=read_next_recirc-----5b729698d026----2---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc0df77b970f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-feature-selection-for-machine-learning-c0df77b970f9&source=-----5b729698d026----2-----------------bookmark_preview----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----5b729698d026----3---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----5b729698d026----3---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----5b729698d026----3---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----5b729698d026----3---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----5b729698d026----3---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----5b729698d026----3---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----3-----------------clap_footer----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----5b729698d026----3---------------------f7803449_33f9_4fe0_8391_2a3692a95e88-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----5b729698d026----3-----------------bookmark_preview----f7803449_33f9_4fe0_8391_2a3692a95e88-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b729698d026--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----5b729698d026--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}