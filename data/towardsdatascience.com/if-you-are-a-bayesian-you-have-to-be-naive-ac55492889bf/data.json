{"url": "https://towardsdatascience.com/if-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf", "time": 1683002187.595006, "path": "towardsdatascience.com/if-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf/", "webpage": {"metadata": {"title": "If you are a bayesian you have to be naive! | by Hemanth Devarapati | Towards Data Science", "h1": "If you are a bayesian you have to be naive!", "description": "Let\u2019s consider there are 200 people who took the interview, of that 100 received a second-round and 100 didn\u2019t. Out of the first bucket, 95 felt they had a great first interview. Out of the second\u2026"}, "outgoing_paragraph_urls": [{"url": "http://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli Distribution", "paragraph_index": 9}, {"url": "http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "anchor_text": "independent and identically distributed", "paragraph_index": 14}, {"url": "https://people.richland.edu/james/lecture/m170/ch05-cnd.html", "anchor_text": "page", "paragraph_index": 22}, {"url": "https://github.com/hdev7/medium-article-naive-bayes/blob/master/naive%20bayes.ipynb", "anchor_text": "here", "paragraph_index": 29}], "all_paragraphs": ["If you had a good first interview, what is the probability you will receive a second interview?", "We can solve this by using common sense. It is not as scary as it sounds.", "Let\u2019s consider there are 200 people who took the interview, of that 100 received a second-round and 100 didn\u2019t. Out of the first bucket, 95 felt they had a great first interview. Out of the second bucket, 75 of them felt they had a good first interview. In total 95 + 75 (170) people felt they had a good 1st interview.", "Finally, of the 170 people, only 95 actually got a 2nd interview. This is it.", "We can complicate it by using Bayes theorem. Which is what I would be talking about in this article.", "Bayes theorem is what allows us to go from a sampling (or likelihood) distribution and a prior distribution to a posterior distribution.", "In simple terms, it can get you something that you don\u2019t know yet using the knowledge of a few relevant observations.", "A sampling distribution is a probability of seeing our data (X) given our parameters (\u03b8). This is written as p(X|\u03b8).", "For example, we might have data on 1,000 coin flips. Where 1 indicates a head. This can be represented in python as:", "A sampling distribution allows us to specify how we think these data were generated. For our coin flips, we can think of our data as being generated from a Bernoulli Distribution. This distribution takes one parameter p which is the probability of getting a 1 (or a head for a coin flip). It then returns a value of 1 with probability p and a value of 0 with probability (1-p).", "You can see how this is perfect for a coin flip. With a fair coin, we know our p = .5 because we are equally likely to get a 1 (head) or 0 (tail). We can create samples from this distribution like this:", "Now that we have defined how we believe our data were generated, we can calculate the probability of seeing our data given our parameters \ud835\udc5d(\ud835\udc4b|\ud835\udf03). Since we have selected a Bernoulli distribution, we only have one parameter: p.", "We can use the probability mass function (PMF) of the Bernoulli distribution to get our desired probability for a single coin flip. The PMF takes a single observed data point and then given the parameters (p in our case) returns the probability of seeing that data point given those parameters. For a Bernoulli distribution, it is simple: if the data point is a 1 the PMF returns p if the data point is a 0 it returns (1-p).", "This is a pretty simple PMF, but other distributions can get much more complicated. So it is good to know that Scipy has most of these built-in. We can draw from the PMF as follows:", "This is nice, but what we really want to know is the probability of seeing all 1,000 of our data points. How do we do that? The trick here is to assume that our data are independent and identically distributed. This assumption allows us to say the probability of seeing all of our data is just the product of each individual probability: \ud835\udc5d(\ud835\udc651,\u2026,\ud835\udc65\ud835\udc5b|\ud835\udefd)=\ud835\udc5d(\ud835\udc651|\ud835\udefd)\u2217\u2026\u2217\ud835\udc5d(\ud835\udc65\ud835\udc5b|\ud835\udefd)p(x1,\u2026,xn|\u03b2)=p(x1|\u03b2)\u2217\u2026\u2217p(xn|\u03b2). This is easy to do:", "How does that number help us? Well by itself, it doesn\u2019t really help too much. What we need to do now is get more of a distribution for our sampling model. Currently, we have only tested our model with p = .5, but what if p = .8? or .2? What would the probability of our data look like then? This can be done by defining a grid of values for our p. Below I will make a grid of 100 values between 0 and 1 (because p has to be between 0 and 1) and then I will calculate the probability of seeing our data given each of these values:", "Now we are getting somewhere. We can see that the probability of seeing our data peaks at p=.5 and almost certainly is between p=.4 and p=.6. Nice. So now we have a good idea of what p value generated our data assuming it was drawn from a Bernoulli distribution. We\u2019re done, right? Not quite\u2026", "Bayes theorem says that we need to think about both our sampling distribution and our prior distribution. What do I mean by prior distribution? It is the \ud835\udc5d(\ud835\udf03) or the probability of seeing a specific value for our parameter. In our sampling distribution, we defined 100 values from 0 to 1 for our parameter p. Now we must define the prior probability of seeing each of those values. That is the probability we would have assumed before seeing any data. Most likely, we would have assumed a fair coin, which looks like the distribution above. Let's see how we can do this:", "Basically we created 1,000 fair coin flips and then generated the sampling distribution as we did before (except we divided by the sum of the sampling distribution to make the values sum to 1). Now we have a \u201cfair coin\u201d prior to our parameters. This basically means that before we saw any data we thought coin flips were fair. And we can see that assumption in our prior distribution by the fact that our prior distribution peaks at .5 and are almost all between .4 and .6.", "I know what you are thinking \u2014 this is pretty boring. The sampling and prior distributions look exactly the same. So let's mix things up. Let's keep our fair prior but change our data to be an unfair coin:", "Ah \u2014 now this is interesting. We have strong data evidence of an unfair coin (since we generated the data we know it is unfair with p=.8), but our prior beliefs are telling us that coins are fair. How do we deal with this?", "Bayes theorem is what allows us to go from our sampling and prior distributions to our posterior distribution. The posterior distribution is the \ud835\udc43(\ud835\udf03|\ud835\udc4b). Or in English, the probability of our parameters given our data. And if you think about it that is what we really want. We are typically given our data \u2014 from maybe a survey or web traffic \u2014 and we want to figure out what parameters are most likely given our data. So how do we get to this posterior distribution? Here comes some math (don\u2019t worry it is not too bad):", "By definition, we know that (If you don\u2019t believe me, check out this page for a refresher):", "You will notice that both of these values share the same numerator, so:", "Nice! Now we can plug in some terminology we know:", "But what is the \ud835\udc43(\ud835\udc4b)? Or in English, the probability of our data? That sounds weird\u2026 Let\u2019s go back to some math and use \ud835\udc35 and \ud835\udc34 again:", "And from our definitions above, we know that:", "Plug in our \ud835\udf03 and \ud835\udc4b:", "Wow! Isn\u2019t that awesome! But what do we mean by \u2211\ud835\udf03? This means to sum over all the values of our parameters. In our coin flip example, we defined 100 values for our parameter p, so we would have to calculate the likelihood * prior for each of these values and sum all those answers. That is our denominator for Bayes Theorem. Thus our final answer for Bayes is:", "That was a lot of text. Let\u2019s do some more coding and put everything together. code can be found here.", "You will notice that I set 100 as the number of observations for the prior and likelihood. This increases the variance of our distributions. More data typically decreases the spread of a distribution. Also, as you get more data to estimate your likelihood, the prior distribution matters less.", "I have changed it to 10. You can see that effect in the graphs above. Because we have more data to help us estimate our likelihood our posterior distribution is closer to our likelihood. Pretty cool.", "There you have it. An introduction to Bayes Theorem. Now if you ever are doubting the fairness of a coin you know how to investigate that problem! Or maybe the probability of a population voting yes for a law? Or any other yes/no outcome.", "In machine learning, there is a very common model that utilizes Bayes theorem called Naive Bayes. It is a classification model and looks like this:", "So we are using the concept of Bayes Theorem to calculate the probability of one of our classes given our data \u2014 makes sense.", "In this model, our prior is very easy to calculate \u2014 it is just the probability of being in that class as seen in the training data. For example, if 40% of your training data heads, then your prior to P(Heads|Data) is .40.", "What is P(Data|Class)? Well, obviously this is our likelihood. In the examples, above we used a Bernoulli distribution for our likelihood. For Naive Bayes, you typically see three distributions used:", "What you do, is use the training data to estimate the necessary parameters for each feature. For example, the mean and standard deviation for Gaussian (given the class of interest). So \u2014 if you had a feature which was the weight of the coin being flipped, you would take the mean and standard deviation of weight for each class (heads and tails). You would then do this for all your features and you assume that all the features follow the same distribution \u2014 in this example gaussian. This is one reason why it is called naive.", "You also assume that all the features are independent (naive), so that means you can take the product of all the feature\u2019s likelihoods to get P(Data|Class).", "Thus, for a row of data, you calculate P(Class|Data) by taking each feature for that row, plugging it into the likelihood for the chosen distribution given the calculated parameters from the training data, and multiplying all these values together and lastly multiplying by the probability of being in that class from the training data.", "You repeat this exercise for each class and then your predicted class is just the class with the largest value. That\u2019s it!", "This model is very fast, so it scales well. It is also famous in the world of spam classification as it was one of the original models to work well on that problem. Still, it is most commonly used in text classification with a multinomial likelihood.", "Sklearn has a nice write up with an example of how to use the model here:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I love data and writing is my first love. This blend of both worlds brings out a purpose."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac55492889bf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac55492889bf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac55492889bf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----ac55492889bf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----ac55492889bf--------------------------------", "anchor_text": "Hemanth Devarapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd994668b1ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&user=Hemanth+Devarapati&userId=fd994668b1ec&source=post_page-fd994668b1ec----ac55492889bf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac55492889bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac55492889bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli Distribution"}, {"url": "http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables", "anchor_text": "independent and identically distributed"}, {"url": "https://people.richland.edu/james/lecture/m170/ch05-cnd.html", "anchor_text": "page"}, {"url": "http://en.wikipedia.org/wiki/Marginal_distribution", "anchor_text": "page"}, {"url": "https://github.com/hdev7/medium-article-naive-bayes/blob/master/naive%20bayes.ipynb", "anchor_text": "here"}, {"url": "http://scikit-learn.org/stable/modules/naive_bayes.html", "anchor_text": "http://scikit-learn.org/stable/modules/naive_bayes.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ac55492889bf---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ac55492889bf---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----ac55492889bf---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----ac55492889bf---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----ac55492889bf---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac55492889bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----ac55492889bf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac55492889bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----ac55492889bf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac55492889bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac55492889bf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fac55492889bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ac55492889bf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac55492889bf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ac55492889bf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac55492889bf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac55492889bf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac55492889bf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac55492889bf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ac55492889bf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ac55492889bf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hemanth Devarapati"}, {"url": "https://medium.com/@hemanthsaid7/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "33 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd994668b1ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&user=Hemanth+Devarapati&userId=fd994668b1ec&source=post_page-fd994668b1ec--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffd994668b1ec%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fif-you-are-a-bayesian-you-have-to-be-naive-ac55492889bf&user=Hemanth+Devarapati&userId=fd994668b1ec&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}