{"url": "https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318", "time": 1683015586.520211, "path": "towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318/", "webpage": {"metadata": {"title": "Machine Learning Algorithms from Start to Finish in Python: Logistic Regression | by Vagif Aliyev | Towards Data Science", "h1": "Machine Learning Algorithms from Start to Finish in Python: Logistic Regression", "description": "Logistic Regression is essentially a must-know for any upcoming Data Scientist or Machine Learning Practitioner. It is most likely the first classification model one has encountered. But, the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-linear-regression-aa8c1d6b1169", "anchor_text": "previous article", "paragraph_index": 4}, {"url": "https://www.youtube.com/watch?v=ARfXDSkQf1Y&t=315s", "anchor_text": "video", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Normal_distribution", "anchor_text": "Normal Distribution", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Binomial_distribution", "anchor_text": "Binomial Distribution", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Generalized_linear_model", "anchor_text": "Source", "paragraph_index": 28}, {"url": "https://www.youtube.com/watch?v=BfKanl1aSG0&frags=pl%2Cwn", "anchor_text": "video", "paragraph_index": 50}, {"url": "http://photo%20by%20analytics%20vidhya/", "anchor_text": "blogpost", "paragraph_index": 67}, {"url": "https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/Logistic%20Regression%20from%20Scratch.py", "anchor_text": "this", "paragraph_index": 70}, {"url": "http://Upword.ai", "anchor_text": "Upword.ai", "paragraph_index": 93}], "all_paragraphs": ["Logistic Regression is essentially a must-know for any upcoming Data Scientist or Machine Learning Practitioner. It is most likely the first classification model one has encountered. But, the question is, how does it really work? What does it do? Why is it used for classification? In this article, I hope to answer all these questions, and by the time you finish reading this article, you will have:", "So, get ready for the wild adventure ahead, partner!", "Logistic Regression is a statistical model that uses a logistic function to predict the probability of an instance belonging to a particular class. if the estimated probability is greater that 50%, then the model predicts that the instance belongs to the positive class(1). If it does not exceed 50%, then the model predicts that it belongs to the negative class. Logistic Regression is used in scenarios such as", "All of these are examples of where Logistic Regression can be used. Well, great! Now you know how to define what Logistic Regression is, and explain what it does. But the question is, how does it work, step-by-step? To explain this, we will compare Logistic Regression to Linear Regression!", "In my previous article, I explained Linear Regression; how it works, what it does and how to implement it. In many ways, Logistic Regression is quite similar to Linear Regression. So, let\u2019s briefly summarise what Linear Regression does:", "3. We then use an optimisation algorithm(More on that later) to \u201cshift\u201d the algorithm so that it can fit the data better, based on a cost function", "4. Steps 2 + 3 are repeated until we reach a desirable output, or our error is close to 0.", "In Logistic Regression, it is quite similar, but there are a few differences:", "All right, so now you know what Logistic Regression is(in layman\u2019s terms), a high level overview of what it does and the steps it takes to operate. But I\u2019m certain you have some questions such as:", "I assure you, now, all that you seek will be answered!", "Note: right now, these concepts may seem irrelevant, but bear with me, as these concepts form the basis of Logistic Regression and will help give you a much better understanding of the algorithm.", "We have seen one way of calculating the odds, but we can also calculate odds from the probability using this formula:", "where p = probability of something happening. To verify this, let\u2019s try see if we can get the odds of the Lakers winning by using this formula:", "And sure enough, we get the same result!", "Going back to our example, let\u2019s assume that the Lakers were having a terrible season(clearly not the case), and out of 20 games, they only won 1. so the odds to the Lakers winning would be:", "If they played even worse throughout the season, and won 2 games out of 100, then the odds of them winning would be:", "We can make a simple observation: the worse they play, the more close their odds of winning will be to 0. Concretely, when the odds are against them winning, then the odds will range between 0 and 1.", "Now let\u2019s look at the opposite. If the lakers were to play 20 games, and win 19, then their odds of winning would now be:", "If they were to play 200 games, and win 194, then their odds of winning would be:", "In other words, when the odds are for the Lakers winning, they begin at 1 and they can go all the way up to infinity.", "Clearly, there is a problem here. The odds against the Lakers winning ranges from 0\u20131, but the odds for them winning ranges from 1-infinity. This asymmetry makes it hard to compare the odds for or against Lakers winning. If only we had a function that makes everything symmetrical\u2026", "Fortunately for us, this exact function exists! It is called the logit function, or the log of the odds. Essentially, it outputs the\u2026log of the odds!", "Let\u2019s demonstrate this with an example using yet again the Lakers(sorry!). If the Lakers were to play 7 games, and win only one, the log of the odds for them winning would be:", "And if instead they won 6 games and and only lost 1:", "Using the log(odds), the distance from the origin is equal for 1 to 6, or 6 to 1. This logit function is vital to understand as it forms the basic of Logistic Regression. If you are still unsure about what odds and log(odds) are, check out this great video by Statquest.", "Ok guys, I\u2019m am officially going to drop the bomb on you:", "Logistic Regression(on its own) is not a classification algorithm. It is actually a regression model(hence the name Logistic Regression)! How? Well, without further ado, let\u2019s take a deep dive of the Logistic Regression model.", "Logistic Regression is actually a part of the Generalised Linear Model (GLM) which was originally created by John Nelder and Robert Wedderburn. While Linear Regression has response values coming from the Normal Distribution, Logistic Regression\u2019s response values come from the Binomial Distribution(having values of 0 and 1).", "Logistic Regression is a special type of GLM that can generalise linear regression by enabling the to have a relation to the response variable using a link function and allowing the magnitude of the variance of each measurement to be a function of its predicted value. (Source).", "Basically, Logistic Regression is type of GLM that output values of 0 and 1 by using a logit link function, and can use a special cost function to calculate the variance of the model.", "Right now, you must me mind-boggled as to how this all works, but stick with me for now and it will make sense soon.", "As we previously discussed, the y-axis values(or target values) of Linear Regression could, in theory, be any number ranging from -infinity to +infinity.", "However, in Logistic Regression, the y-axis values only range between 0 and 1.", "So, to solve this problem, the y-axis values are transformed from the probability of X happening to\u2026 the log(odds) of something happening! Now, the values can range from infinity to +infinity, just like in Linear Regression. We do this transformation using the logit function that we previously discussed.", "So, in other words, we transform our S shaped curve into a straight line. This means that while we still use the S shaped curve in Logistic Regression, the coefficients are calculated in terms of the log(odds).", "After this, the algorithm is essentially a linear model; We have our straight trendline, and by we then fit it onto the data using an optimisation algorithm along with a modified cost function for Logistic Regression(more on this later). Once we have made predictions, we \u201ctranslate\u201d or invert our predictions using a link function, with the most common one known as the sigmoid function, back into a probability.", "Ok, so now you have a slightly better idea of how Logistic Regression works. Let\u2019s try solidify your knowledge with the an example using math.", "Let\u2019s say we have a model with two features, X1 + X2, and single binomial response variable Y, which we denote p = P(Y=1):", "The log of the odds of the event that Y=1 can be expressed as:", "We can recover the odds by exponentiating the log-odds:", "By simple algebraic manipulation, the probability that Y=1 is:", "This formula is essentially the link function that translates our log-odds predictions into probabilities. The function, also known as the sigmoid function, can also be represented as so:", "This can be visually seen as an S shaped curve:", "Here, we can clearly see how we can calculate the log of the odds for a given instance or the probability that Y=0 given the weights. The part that makes Logistic Regression a classification model is the threshold used to split the predicted probabilities. Without this, Logistic Regression is truly a Regression model.", "So now you should have a good understanding of how Logistic Regression works. But I\u2019m sure you may still have questions such as:", "Ok, you got me. So, just before we start coding, let me explain these concepts in layman\u2019s terms.", "A cost function is essentially a formula that measures the loss, or the \u201ccost\u201d of your model. If you have ever done any Kaggle competitions, you may have come across some of them. A few common ones include:", "Now, It is worth highlighting an important difference between Linear Regression and Logistic Regression:", "Note the green lines coming from the each data point. This is the residual of the model and is used in calculating the cost of the model for Linear Regression.", "Now, we can\u2019t do our calculation of the residual, because the distance between the point and the hypothesis could be infinity. That\u2019s why, we use a special cost function called the log-loss:", "this function is derived from something known as maximum likelihood estimation. I\u2019m aware of the length of this article, so If you want to get more info on it, I recommend checking out this video for more intuition on the function.", "These functions are essential to model training and development as they answer the fundamental question of \u201chow well is my model predicting new instances?\u201d. Keep this in mind, as this ties in with our next topic.", "Optimisation is usually defined as the process of improving something so that it operates at its full potential. This is also applicable in Machine Learning. In the world of ML, optimisation is essentially trying to find the best combination of parameters for a certain dataset. This is essentially the \u201clearning\u201d bit of Machine Learning.", "While may optimisation algorithms exist, I will discuss two of the most common ones: Gradient Descent and The Normal Equation.", "Gradient Descent is an optimisation algorithm that aims to find the minimum of a function. It achieves this goal by iteratively taking steps in the negative direction of the slope. In our example, gradient descent would continuously update the weights by moving in the slope of the tangential line to the function. Well fantastic, sound great. English please? :)", "To better illustrate Gradient Descent, Let\u2019s go through a simple example. Imagine a human is at the top of a mountain, and he/she want to get to the bottom. What they might do is look around and see in what direction they should take a step in in order to get down quicker. Then, they might take a step in that direction and now they are closer to their goal. However, they have to be careful when coming down as they might get stuck at a certain point, so we have to make sure to choose our step sizes accordingly.", "Similarly, The objective of gradient descent is to minimise a function. In our case, it is to minimise the cost of our model. It does this by finding the tangential line to the function and moving in that direction. The size of the \u201cstep\u201d of the algorithm is defined by what is known as a learning rate. This essentially controls how far we move down. With this parameter, we have to be careful of two cases:", "We also have a parameter that controls the number of times the algorithm iterates over the dataset.", "Visually, the algorithm would do something like this:", "Because this algorithm is so essential to Machine Learning, let\u2019s recap what it does:", "Now that you have been introduced to Gradient Descent, let\u2019s introduce the Normal Equation.", "If we were to go back to our example, instead of taking steps iteratively down the mountain, we would be able to immediately get to the bottom. This is the case with the Normal Equation. It leverages linear algebra to produce weights that can produce results just as well as Gradient Descent would, in a fraction of the time.", "This is an important preprocessing step of many Machine Learning Algorithms, especially those that use distance metrics and calculations(like Linear Regression,Gradient Descent and of course, Logistic Regression as it really is a regression model!). It essentially scales our features so that they are in a similar range. Think of it like a house, and a scaled model of a house. The shape of both are the same(they are both houses), but the size is different(5m != 500m). We do this for the following reasons:", "To demonstrate this, let\u2019s suppose we have three features, named A, B and C:", "Distance of BC before scaling =>", "Distance of AB after scaling =>", "Distance of BC after scaling =>", "We can clearly see that the features are much more comparable and unbiased than they were before scaling. If you want a great tutorial on feature scaling, please check out this blogpost by Analytics Vidhya.", "Wow! Your brain must be filled with tons of information! So, I suggest taking a break, strolling around, enjoying life and doing some stretches before we get in to actually coding the Algorithm from Scratch!", "Ok, now the moment you have been waiting for; the implementation! Without further ado, let\u2019s begin!", "Note: all the code can be downloaded from this Github repo. However, I recommend you to follow along the tutorial before you do so, because then you will gain a better understanding of what you are actually coding!", "First, let\u2019s do some basic imports:", "All we are using is numpy for the mathematical computations, matplotlib to plot graphs, and the breast cancer dataset from scikit-learn.", "Next, let\u2019s load our data and define our features and our labels:", "Next, let\u2019s create a custom train_test_split function to split our data into a training and test set:", "In order to make clean, repeatable and efficient code, as well as adhere to software development practices, we will make create a Logistic Regression class:", "Let\u2019s add our link function as a method of the class:", "Now, let\u2019s implement our cost function using the formula described above:", "We will also add a method to insert an intercept term:", "Next, we will scale our features:", "We will add a method to randomly initialise the parameters of the model:", "Now, we will implement the Normal Equation using the formula below:", "Essentially, we split the algorithm into 3 parts:", "So, that\u2019s the Normal Equation! Not too bad! Now, we will implement Batch Gradient Descent using the following formula:", "Let\u2019s create a fit function to fit our data:", "We will also create a plot_function method to show the magnitude of the cost function over the number of epochs:", "Finally, we will create a predict function for inference, setting the threshold to 0.5:", "The full code for the class looks like the following:", "Now, let\u2019s call our newly made class using the Normal Equation:", "And now, with Gradient Descent, we can also plot the cost function:", "So, Now you have successfully implemented Logistic Regression from Scratch!", "I have really enjoyed blogging these past few months, and I am grateful to all my followers who always show their appreciation for my work. Therefore, I would like to thank you for taking time in your day to read this article, and I hope to continue to produce more interesting articles to share with the world. Stay tuned, and have a good time!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "19 y/o student ex-founder of Snapstudy (acquired), founding engineer at Upword.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5a62e3495318&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a62e3495318--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a62e3495318--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://vagifaliyev.medium.com/?source=post_page-----5a62e3495318--------------------------------", "anchor_text": ""}, {"url": "https://vagifaliyev.medium.com/?source=post_page-----5a62e3495318--------------------------------", "anchor_text": "Vagif Aliyev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b4e6b48584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&user=Vagif+Aliyev&userId=6b4e6b48584&source=post_page-6b4e6b48584----5a62e3495318---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a62e3495318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a62e3495318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@drew_beamer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Drew Beamer"}, {"url": "https://unsplash.com/s/photos/split?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@officestock?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Sebastian Herrmann"}, {"url": "https://unsplash.com/s/photos/handshake?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-linear-regression-aa8c1d6b1169", "anchor_text": "previous article"}, {"url": "https://en.wikipedia.org/wiki/Linear_regression", "anchor_text": "Wikipedia"}, {"url": "https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com/s/photos/basketball?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@eduardoflorespe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Eduardo Flores"}, {"url": "https://unsplash.com/s/photos/symmetrical?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/watch?v=ARfXDSkQf1Y&t=315s", "anchor_text": "video"}, {"url": "https://unsplash.com/@jens_johnsson?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Jens Johnsson"}, {"url": "https://unsplash.com/s/photos/explosion?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Normal_distribution", "anchor_text": "Normal Distribution"}, {"url": "https://en.wikipedia.org/wiki/Binomial_distribution", "anchor_text": "Binomial Distribution"}, {"url": "https://en.wikipedia.org/wiki/Generalized_linear_model", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Activation_function", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Linear_regression", "anchor_text": "Wikipedia"}, {"url": "https://www.youtube.com/watch?v=BfKanl1aSG0&frags=pl%2Cwn", "anchor_text": "video"}, {"url": "https://unsplash.com/@lux17?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Lucas Clara"}, {"url": "https://unsplash.com/s/photos/mountain?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "Wikipedia"}, {"url": "https://unsplash.com/@saffu?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Saffu"}, {"url": "https://unsplash.com/s/photos/fast?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@calum_mac?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Calum MacAulay"}, {"url": "https://unsplash.com/s/photos/scaling?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/", "anchor_text": "Analytics Vidhya"}, {"url": "https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/", "anchor_text": "Analytics Vidhya"}, {"url": "https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/", "anchor_text": "Analytics Vidhya"}, {"url": "https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/", "anchor_text": "Analytics Vidhya"}, {"url": "http://photo%20by%20analytics%20vidhya/", "anchor_text": "blogpost"}, {"url": "https://unsplash.com/@cdr6934?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Chris Ried"}, {"url": "https://unsplash.com/s/photos/code?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/Logistic%20Regression%20from%20Scratch.py", "anchor_text": "this"}, {"url": "https://unsplash.com/@reneefisherandco?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Renee Fisher"}, {"url": "https://unsplash.com/s/photos/goodbye?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5a62e3495318---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5a62e3495318---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----5a62e3495318---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5a62e3495318---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data?source=post_page-----5a62e3495318---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a62e3495318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&user=Vagif+Aliyev&userId=6b4e6b48584&source=-----5a62e3495318---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a62e3495318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&user=Vagif+Aliyev&userId=6b4e6b48584&source=-----5a62e3495318---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a62e3495318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a62e3495318--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5a62e3495318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5a62e3495318---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a62e3495318--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5a62e3495318--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5a62e3495318--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5a62e3495318--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5a62e3495318--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5a62e3495318--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5a62e3495318--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5a62e3495318--------------------------------", "anchor_text": ""}, {"url": "https://vagifaliyev.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://vagifaliyev.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vagif Aliyev"}, {"url": "https://vagifaliyev.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "209 Followers"}, {"url": "http://Upword.ai", "anchor_text": "Upword.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b4e6b48584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&user=Vagif+Aliyev&userId=6b4e6b48584&source=post_page-6b4e6b48584--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6f056d3d77d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-logistic-regression-5a62e3495318&newsletterV3=6b4e6b48584&newsletterV3Id=6f056d3d77d3&user=Vagif+Aliyev&userId=6b4e6b48584&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}