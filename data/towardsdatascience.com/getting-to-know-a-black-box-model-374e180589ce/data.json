{"url": "https://towardsdatascience.com/getting-to-know-a-black-box-model-374e180589ce", "time": 1682993531.217154, "path": "towardsdatascience.com/getting-to-know-a-black-box-model-374e180589ce/", "webpage": {"metadata": {"title": "Getting to know a black-box model: | by Adrian Botta | Towards Data Science", "h1": "Getting to know a black-box model:", "description": "As the hype about AI has grown, so have discussions around Adversarial examples. An adversarial example, also referred to as an attack, is an input that has been crafted to be misclassified by a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/", "anchor_text": "discussions", "paragraph_index": 0}, {"url": "https://blog.openai.com/adversarial-example-research/", "anchor_text": "photo", "paragraph_index": 0}, {"url": "https://nicholas.carlini.com/code/audio_adversarial_examples/", "anchor_text": "audio sample", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1712.06751", "anchor_text": "string of text", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1702.05983", "anchor_text": "software code", "paragraph_index": 0}, {"url": "https://blog.ycombinator.com/how-adversarial-attacks-work/", "anchor_text": "attacks", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/the-modeler-strikes-back-defense-strategies-against-adversarial-attacks-9aae07b93d00", "anchor_text": "defenses", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/know-your-adversary-understanding-adversarial-examples-part-1-2-63af4c2f5830", "anchor_text": "theoretically", "paragraph_index": 1}, {"url": "https://ml.berkeley.edu/blog/2018/01/10/adversarial-examples/", "anchor_text": "introductory level", "paragraph_index": 1}, {"url": "https://github.com/adrian-botta/understanding_adversarial_examples/blob/master/adversarial_examples_logistic_regression.ipynb", "anchor_text": "All of the code used to produce this post can be seen here", "paragraph_index": 3}, {"url": "https://medium.com/u/2d1b498ccdb5?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Mart\u00edn Pellarolo", "paragraph_index": 4}, {"url": "https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac", "anchor_text": "building a logistic regression from a subset of the iris data set", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Explaining and Harnessing Adversarial Examples", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1801.01944.pdf", "anchor_text": "paper", "paragraph_index": 15}, {"url": "https://www.nytimes.com/2018/05/10/technology/alexa-siri-hidden-command-audio-attacks.html", "anchor_text": "article", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1602.02697", "anchor_text": "Practical Black-Box Attacks against Machine Learning", "paragraph_index": 17}, {"url": "http://karpathy.github.io/2015/03/30/breaking-convnets/", "anchor_text": "more detail here", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1712.02976", "anchor_text": "more", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1805.10652", "anchor_text": "more", "paragraph_index": 26}], "all_paragraphs": ["As the hype about AI has grown, so have discussions around Adversarial examples. An adversarial example, also referred to as an attack, is an input that has been crafted to be misclassified by a machine learning model. These inputs are usually a high dimensional input such as a photo, audio sample, string of text, or even software code.", "There are several wonderful blogs that cover adversarial attacks and defenses theoretically and at an introductory level. One of the keys to understanding adversarial examples is to first understand:", "Since these concepts are challenging to visualize in a high dimensional space, we\u2019ll walk through an example of some of the core techniques used in adversarial attacks with a simple two-dimensional example. This will help us gain a better understanding of these concepts in higher dimensions.", "We\u2019ll build a logistic regression classifier which will act as the model we intend to attack or trick, or our \u2018victim\u2019 model. Then, we will walk through how use a gradient-based method to attack our victim and a black-box version of our victim. (All of the code used to produce this post can be seen here)", "Let\u2019s borrow Mart\u00edn Pellarolo\u2019s example of building a logistic regression from a subset of the iris data set. We\u2019ll call the two input variables X1 and X2 and the classes class 0 and class 1 to keep it simple.", "Our goal when training a machine learning model is to determine the line in this two-dimensional space that best separates the two classes. Luckily, it\u2019s an easy task given that the two classes are visibly separated and there\u2019s not much overlap. To do this, we\u2019ll fit a logistic regression which will create a probability distribution of a data point belonging to class 1. Using the sigmoid function (represented as g) and some parameters \u03b8, we\u2019ll fit this probability distribution to our data.", "By changing the parameters in the matrix \u03b8, we can adjust the function g to best fit our data X.", "We\u2019ll use binary cross entropy loss as the loss function to determine how close the model\u2019s predictions are to the ground truth.", "The partial derivative of the loss function with respect to (w.r.t.) \u03b8 tells us the direction we need to change the values of \u03b8 to change the loss. In this case, we would like to minimize the loss.", "Once we\u2019ve minimized the loss function, by making directed updates to \u03b8, our victim model is trained!", "The graph above shows the model\u2019s probability distribution for any point in this space belonging to class 1, and inversely to class 0 (1- P(y=1)). It also features our model\u2019s decision boundary at a probability threshold of 0.5 \u2014 If a point is above the line, the probability of it belonging to class 1 will be below 50%. Since the model \u2018decides\u2019 at this threshold, it will assign 0 as its label prediction.", "The objective of a jacobian or gradient based attack, described in Explaining and Harnessing Adversarial Examples by Goodfellow et. al., is to move a point over a victim model\u2019s decision boundary. In our example, we\u2019ll take a point that is normally classified as class 0 and \u2018push\u2019 it over the victim model\u2019s decision boundary to be classified as class 1. This change to the original point is also called a perturbation when using higher dimensional data because we\u2019re making a very small change to the input.", "As you may recall, when training the logistic regression, we used the loss function and derivative of the loss function w.r.t. \u03b8 to determine how \u03b8 needs to change to minimize the loss. As an attacker, with full knowledge of how the victim model works, we can determine how to change the loss by changing the other input to our function.", "The derivative of the loss function w.r.t. X tells us exactly in which direction we need to change the values of X to change the victim model\u2019s loss.", "Since we want to attack the model, we need to maximize its loss. Changing the values of X essentially moves X in the 2-dimensional space. Direction is only one of the components in an adversarial perturbation. We also need to take into account how large of a step (represented as epsilon) is needed to move in that direction to cross the decision boundary.", "An adversary must consider which data points or inputs to use as well as the smallest epsilon necessary to successfully push a point over the decision boundary. If the adversary starts with points that are very far into the class 0 space they\u2019ll need a larger and more noticeable perturbation to convert it into an adversarial example. Let\u2019s convert some of our points closest to the decision boundary and from class 0. (Note: Other techniques allow for creating adversarial examples from random noise - paper & article)", "We\u2019ve successfully created some adversarial examples!", "and you would be right. We\u2019ll come back to point 1, but the techniques from Practical Black-Box Attacks against Machine Learning by Nicolas Papernot et. al. will help us with point 2.", "When we know everything about a model, we refer to it as a \u2018white-box\u2019 model. In comparison, when we know nothing about how a model works, we refer to it as a \u2018black-box\u2019. We can imagine black-box models as an API that we ping by sending inputs and receiving some outputs (labels, class numbers, etc). Understanding black-box attacks are vital because they prove that models hidden behind an API may seem safe, but are in fact still vulnerable to attacks.", "Papernot\u2019s paper discusses the jacobian-based dataset augmentation technique which aims to train another model, called the substitute model, to share very similar decision boundaries as the victim model. Once a substitute model is trained to have almost the same decision boundaries as the victim model, an adversarial perturbation that is created to move a point over the substitute model\u2019s decision boundary will likely also cross the victim model\u2019s decision boundary. They achieve this by exploring the space around the victim model\u2019s decision space and determining how the victim responds.", "This technique can be described as a child learning to annoy their parents. The child starts with no preconception of what makes their parent angry or not, but they can test their parent by picking a random-set of actions over the course of a week and noting how their parent respond to those actions. While a parent may exhibit a non-binary response for each of these, let\u2019s pretend that the child\u2019s actions are either bad or good (two classes). After the first week, the child has learned a bit about what bothers their parents and makes an educated guess as to what else would bother their parents. The next week, the child dials down the actions which were successful and takes actions that weren\u2019t successful a step further. The child repeats this, each week, noting their parents\u2019 responses and adjusting their understanding of what will bother their parents until they know exactly what annoys them and what doesn\u2019t.", "Jacobian-based dataset augmentation works in the same way where a random sample of the initial data is taken and used to train a very poor substitute model. The adversarial examples are created from the dataset (using the gradient based attacks from earlier). Here, the adversarial examples are a step in the direction of the model\u2019s gradient to determine if the black-box model will classify the new data points the same way as the substitute model.", "The augmented data is labeled by the black-box model and used to train a better substitute model. Just like the child, the substitute model gets a more precise understanding of where the black-box model\u2019s decision boundary is. After a few iterations of this, the substitute model shares almost the exact same decision boundaries as the black-box model.", "The substitute model doesn\u2019t even need to be the same type of ML model as the black-box. In fact, a simple Multi-Layer Perceptron is enough to learn close enough decision boundaries of a complex Convolutional Neural Network. Ultimately, with a small sample of data, a few iterations of the data augmentation and labeling, a black-box model can be successfully attacked.", "Now, back to point 1: You\u2019re right, I was moving points in a 2-dimensional space. While that was for the sake of keeping the example simple, adversarial attacks take advantage of a property of neural networks that amplifies signals. Andrej Karpathy explains the effect of the dot-product in more detail here.", "In our two dimensional example, to move the point across the victim model\u2019s decision boundary, we need to move it with step size epsilon. When weights matrices of a neural network are multiplied with a normal input, the product of each weight and each input value are summed. However, with an adversarial input, the additional summation of the adversarial signal amplifies the signal as a function of the total input dimensionality. Meaning, to achieve the step size needed to cross the decision boundary, we need to make smaller changes to each X value as the number of input dimensions increase. The larger the input dimensionality, the harder it is for us to notice the adversarial perturbations \u2014this effect is one of the reasons why adversarial examples with MNIST are more noticeable than adversarial examples with ImageNet.", "Gradient-based attacks have proven to be effective techniques that exploit the way deep learning models process high dimensional inputs into probability distributions. Black-box attacks demonstrate that as long as we have access to a victim model\u2019s inputs and outputs, we can create a good enough copy of the model to use for an attack. However, these techniques have weaknesses. To use a gradient based attack, we need to know exactly how inputs are embedded (turned into a machine readable format like a vector). For instance, an image is usually represented as a 2-d matrix of pixels or a 3-d matrix, a consistent representation of information. On the other hand, other types of unstructured data like text may be embedded using some secret pre-trained word embeddings or learned embeddings. Since we can\u2019t take the derivative of something w.r.t. a word, we need to know how that word is being represented. Training a substitute model requires a set of possibly detectable pings to the black-box model. And researchers are finding more and more ways to defend their models against adversarial attacks. Regardless, we must be proactive in understanding the vulnerabilities of our models.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Food Science Nerd, and Avid Backpacker"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F374e180589ce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----374e180589ce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianpbotta?source=post_page-----374e180589ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianpbotta?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Adrian Botta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc9c796f8bb96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&user=Adrian+Botta&userId=c9c796f8bb96&source=post_page-c9c796f8bb96----374e180589ce---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F374e180589ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F374e180589ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/", "anchor_text": "discussions"}, {"url": "https://blog.openai.com/adversarial-example-research/", "anchor_text": "photo"}, {"url": "https://nicholas.carlini.com/code/audio_adversarial_examples/", "anchor_text": "audio sample"}, {"url": "https://arxiv.org/abs/1712.06751", "anchor_text": "string of text"}, {"url": "https://arxiv.org/abs/1702.05983", "anchor_text": "software code"}, {"url": "https://blog.ycombinator.com/how-adversarial-attacks-work/", "anchor_text": "attacks"}, {"url": "https://towardsdatascience.com/the-modeler-strikes-back-defense-strategies-against-adversarial-attacks-9aae07b93d00", "anchor_text": "defenses"}, {"url": "https://towardsdatascience.com/know-your-adversary-understanding-adversarial-examples-part-1-2-63af4c2f5830", "anchor_text": "theoretically"}, {"url": "https://ml.berkeley.edu/blog/2018/01/10/adversarial-examples/", "anchor_text": "introductory level"}, {"url": "https://github.com/adrian-botta/understanding_adversarial_examples/blob/master/adversarial_examples_logistic_regression.ipynb", "anchor_text": "All of the code used to produce this post can be seen here"}, {"url": "https://medium.com/u/2d1b498ccdb5?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Mart\u00edn Pellarolo"}, {"url": "https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac", "anchor_text": "building a logistic regression from a subset of the iris data set"}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Explaining and Harnessing Adversarial Examples"}, {"url": "https://arxiv.org/pdf/1801.01944.pdf", "anchor_text": "paper"}, {"url": "https://www.nytimes.com/2018/05/10/technology/alexa-siri-hidden-command-audio-attacks.html", "anchor_text": "article"}, {"url": "https://arxiv.org/abs/1602.02697", "anchor_text": "Practical Black-Box Attacks against Machine Learning"}, {"url": "http://karpathy.github.io/2015/03/30/breaking-convnets/", "anchor_text": "more detail here"}, {"url": "https://arxiv.org/abs/1712.02976", "anchor_text": "more"}, {"url": "https://arxiv.org/abs/1805.10652", "anchor_text": "more"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----374e180589ce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/adversarial-examples?source=post_page-----374e180589ce---------------adversarial_examples-----------------", "anchor_text": "Adversarial Examples"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----374e180589ce---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----374e180589ce---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/ai?source=post_page-----374e180589ce---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F374e180589ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&user=Adrian+Botta&userId=c9c796f8bb96&source=-----374e180589ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F374e180589ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&user=Adrian+Botta&userId=c9c796f8bb96&source=-----374e180589ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F374e180589ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----374e180589ce--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F374e180589ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----374e180589ce---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----374e180589ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----374e180589ce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----374e180589ce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----374e180589ce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----374e180589ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianpbotta?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianpbotta?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Botta"}, {"url": "https://medium.com/@adrianpbotta/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc9c796f8bb96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&user=Adrian+Botta&userId=c9c796f8bb96&source=post_page-c9c796f8bb96--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fc9c796f8bb96%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-to-know-a-black-box-model-374e180589ce&user=Adrian+Botta&userId=c9c796f8bb96&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}