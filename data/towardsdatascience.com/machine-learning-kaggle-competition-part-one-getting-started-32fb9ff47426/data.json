{"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426", "time": 1682993398.3988302, "path": "towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426/", "webpage": {"metadata": {"title": "Machine Learning Kaggle Competition Part One: Getting Started | by Will Koehrsen | Towards Data Science", "h1": "Machine Learning Kaggle Competition Part One: Getting Started", "description": "In this article we will take a brief walk through the Kaggle machine learning competition environment and look at an introductory notebook you can use to get started."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Kaggle#How_Kaggle_competitions_work", "anchor_text": "machine learning competitions", "paragraph_index": 1}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle", "paragraph_index": 1}, {"url": "http://blog.kaggle.com/2018/04/03/q1-2018-product-update/", "anchor_text": "data science resources", "paragraph_index": 1}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit Default Risk problem", "paragraph_index": 3}, {"url": "https://www.kaggle.com/c/trackml-particle-identification", "anchor_text": "said for all of the competitions", "paragraph_index": 3}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "available here", "paragraph_index": 4}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit Default Risk competition", "paragraph_index": 5}, {"url": "http://cs231n.github.io/convolutional-networks/", "anchor_text": "convolutional neural networks", "paragraph_index": 6}, {"url": "http://www.homecredit.net/about-us.aspx", "anchor_text": "Home Credit", "paragraph_index": 7}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "Python Jupyter Notebook I put together in a kernel", "paragraph_index": 14}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "first notebook", "paragraph_index": 21}, {"url": "https://www.kaggle.com/c/home-credit-default-risk/data", "anchor_text": "column descriptions of each data file", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166", "anchor_text": "Pairs Plot", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c", "anchor_text": "that is a poor choice", "paragraph_index": 27}, {"url": "https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it", "anchor_text": "Receiver Operating Characteristic curve Area Under the Curve", "paragraph_index": 27}, {"url": "http://gim.unmc.edu/dxtests/roc3.htm", "anchor_text": "research on this one", "paragraph_index": 27}, {"url": "http://gim.unmc.edu/dxtests/roc2.htm", "anchor_text": "then shows the True Positive Rate versus the False Positive Rate", "paragraph_index": 27}, {"url": "https://machinelearningmastery.com/logistic-regression-for-machine-learning/", "anchor_text": "Logistic Regression", "paragraph_index": 28}, {"url": "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/", "anchor_text": "Gradient Boosting Machine", "paragraph_index": 32}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "anchor_text": "LightGBM library", "paragraph_index": 32}, {"url": "https://brage.bibsys.no/xmlui/handle/11250/2433761", "anchor_text": "model wins nearly every structured Kaggle competition", "paragraph_index": 32}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "notebook kernel", "paragraph_index": 35}, {"url": "http://twitter.com/koehrsen_will", "anchor_text": "@koehrsen_will.", "paragraph_index": 36}], "all_paragraphs": ["In the field of data science, there are almost too many resources available: from Datacamp to Udacity to KDnuggets, there are thousands of places online to learn about data science. However, if you are someone who likes to jump in and learn by doing, Kaggle might be the single best location for expanding your skills through hands-on data science projects.", "While it originally was known as a place for machine learning competitions, Kaggle \u2014 which bills itself as \u201cYour Home for Data Science\u201d \u2014 now offers an array of data science resources. Although this series of articles will focus on a competition, it\u2019s worth pointing out the main aspects of Kaggle:", "Overall, Kaggle is a great place to learn, whether that\u2019s through the more traditional learning tracks or by competing in competitions. When I want to find out about the latest machine learning method, I could go read a book, or, I could go on Kaggle, find a competition, and see how people use it in practice. Personally, I find this much more enjoyable and a more effective teaching method. Moreover, the community is extremely supportive and always willing to answer questions or provide feedback on a project.", "In this article, we\u2019ll focus on getting started with a Kaggle machine learning competition: the Home Credit Default Risk problem. This is a fairly straightforward competition with a reasonable sized dataset (which can\u2019t be said for all of the competitions) which means we can compete entirely using Kaggle\u2019s kernels. This significantly lowers the barrier to entry because you don\u2019t have to worry about any software on your computer and you don\u2019t even have to download the data! As long as you have a Kaggle account and an Internet connection, you can connect to a kernel and run the code.", "I plan to do the entire competition on Kaggle and the kernel (a Python Jupyter Notebook) for this post is available here. To get the most from this article, copy the kernel by creating a Kaggle account, then hitting the blue Fork Notebook button. This will open up the notebook for editing and running in the kernel environment.", "The Home Credit Default Risk competition is a standard supervised machine learning task where the goal is to use historical loan application data to predict whether or not an applicant will repay a loan. During training, we provide our model with the features \u2014 the variables describing a loan application \u2014 and the label \u2014 a binary 0 if the loan was repaid and a 1 if the loan was not repaid \u2014 and the model learns a mapping from the features to the label. Then, during testing, we feed the model the features for a new batch of applications and ask it to predict the label.", "All the data for this competition is structured meaning it exists in neat rows and columns \u2014 think a spreadsheet. This means we won\u2019t need to use any convolutional neural networks (which excel at processing image data ) and it will give us great practice on a real-world dataset.", "Home Credit, the host of the competition, is a finance provider that focuses on serving the unbanked population. Predicting whether or not an application will repay a loan is a vital business need, and Home Credit has developed this competition in the hopes that the Kaggle community can develop an effective algorithm for this task. This competition follows the general idea of most Kaggle competitions: a company has data and a problem to solve, and rather than (or in addition to) hiring internal data scientists to build a model, they put up a modest prize to entice the entire world to contribute solutions. A community of thousands of skilled data scientists (Kagglers) then work on the problem for basically no charge to come up with the best solution. As far as cost effective business plans go, this seems like a brilliant idea!", "When you go to the competition homepage, you\u2019ll see this:", "Here\u2019s a quick run through of the tabs", "Although they are called competitions, Kaggle machine learning events should really be termed \u201ccollaborative projects\u201d because the main goal is not necessarily to win but to practice and learn from fellow data scientists. Once you realize that it\u2019s not so much about beating others but about expanding your own skills, you will get a lot more out of the competitions. When you sign up for Kaggle, you not only get all the resources , you also get to be part of a community of data scientists with thousands of years of collective experience.", "Take advantage of all that experience by trying to be an active part of the community! That means anything from sharing a kernel to asking a question in a discussion forum. While it can be intimidating to make your work public, we learn best by making mistakes, receiving feedback, and improving so we don\u2019t make the same mistake again. Everyone starts out a beginner, and the community is very supportive of data scientists of all skill levels.", "In that mindset, I want to emphasize that discussion with others and building on others\u2019 code is not only acceptable, but encouraged! In school, working with others is called cheating and gets you a zero, but in the real world, it\u2019s called collaboration and an extremely important skill.", "A great method for throwing yourself into a competition is to find a kernel someone has shared with a good leaderboard score, fork the kernel, edit it to try and improve the score, and then run it to see the results. Then, make the kernel public so others can use your work. Data scientists stand not on the shoulders of giants, but on the backs of thousands of individuals who have made their work public for the benefit of all. (Sorry for getting philosophical, but this is why I love data science!)", "Once you have a basic understanding of how Kaggle works and the philosophy of how to get the most out of a competition, it\u2019s time to get started. Here, I\u2019ll briefly outline a Python Jupyter Notebook I put together in a kernel for the Home Credit Default Risk problem, but to get the full benefit, you\u2019ll want to fork the notebook on Kaggle and run it yourself (you don\u2019t have to download or set-up anything so I\u2019d highly encourage checking it out).", "When you open the notebook in a kernel, you\u2019ll see this environment:", "Think of this as a standard Jupyter Notebook with slightly different aesthetics. You can write Python code and text (using Markdown syntax) just like in Jupyter and run the code completely in the cloud on Kaggle\u2019s servers. However, Kaggle kernels have some unique features not available in Jupyter Notebook. Hit the leftward facing arrow in the upper right to expand the kernel control panel which brings up three tabs (if the notebook is not in fullscreen, then these three tabs may already be visible next to the code).", "In the data tab, we can view the datasets to which our Kernel is connected. In this case, we have the entire competition data, but we can also connect to any other dataset on Kaggle or upload our own data and access it in the kernel. Data files are available in the ../input/ directory from within the code:", "The Settings tab lets us control different technical aspects of the kernel. Here we can add a GPU to our session, change the visibility, and install any Python package which is not already in the environment.", "Finally, the Versions tab lets us see any previous committed runs of the code. We can view changes to the code, look at log files of a run, see the notebook generated by a run, and download the files that are output from a run.", "To run the entire notebook and record a new Version, hit the blue Commit & Run button in the upper right of the kernel. This executes all the code, shows us the completed notebook (or any errors if there are mistakes), and saves any files that are created during the run. When we commit the notebook, we can then access any predictions our models made and submit them for scoring.", "The first notebook is meant to get you familiar with the problem. We start off much the same way as any data science problem: understanding the data and the task. For this problem, there is 1 main training data file (with the labels included), 1 main testing data file, and 6 additional data files. In this first notebook, we use only the main data, which will get us a decent score, but later work will have to incorporate all the data in order to be competitive.", "To understand the data, it\u2019s best to take a couple minutes away from the keyboard and read through the problem documentation, such as the column descriptions of each data file. Because there are multiple files, we need to know how they are all linked together, although for this first notebook we only use the main file to keep things simple. Reading through other kernels can also help us get familiar with the data and which variables are important.", "Once we understand the data and the problem, we can start structuring it for a machine learning task This means dealing with categorical variables (through one-hot encoding), filling in the missing values (imputation), and scaling the variables to a range. We can do exploratory data analysis, such as finding correlations with the label, and graphing these relationships.", "We can use these relationships later on for modeling decisions, such as including which variables to use. (See the notebook for implementation).", "Of course, no exploratory data analysis is complete without my favorite plot, the Pairs Plot.", "After thoroughly exploring the data and making sure it\u2019s acceptable for machine learning, we move on to creating baseline models. However, before we quite get to the modeling stage, it\u2019s critical we understand the performance metric for the competition. In a Kaggle competition, it all comes down to a single number, the metric on the test data.", "While it might make intuitive sense to use accuracy for a binary classification task, that is a poor choice because we are dealing with an imbalanced class problem. Instead of accuracy, submissions are judged in terms of ROC AUC or Receiver Operating Characteristic curve Area Under the Curve. I\u2019ll let you do the research on this one, or read the explanation in the notebook. Just know that higher is better, with a random model scoring 0.5 and a perfect model scoring 1.0. To calculate a ROC AUC, we need to make predictions in terms of probabilities rather than a binary 0 or 1. The ROC then shows the True Positive Rate versus the False Positive Rate as a function of the threshold according to which we classify an instance as positive.", "Usually we like to make a naive baseline prediction, but in this case, we already know that random guessing on the task would get an ROC AUC of 0.5. Therefore, for our baseline model, we will use a slightly more sophisticated method, Logistic Regression. This is a popular simple algorithm for binary classification problems and it will set a low bar for future models to surpass.", "After implementing the logistic regression, we can save the results to a csv file for submission. When the notebook is committed, any results we write will show up in the Output sub-tab on the Versions tab:", "From this tab, we can download the submissions to our computer and then upload them to the competition. In this notebook, we make four different models with scores as follows:", "These scores don\u2019t get us anywhere close to the top of the leaderboard, but they leave room for plenty of future improvement! We also get an idea of the performance we can expect using only a single source of data.", "(Not surprisingly, the extraordinary Gradient Boosting Machine (using the LightGBM library) performs the best. This model wins nearly every structured Kaggle competition (where the data is in table format) and we will likely need to use some form of this model if we want to seriously compete!)", "This article and introductory kernel demonstrated a basic start to a Kaggle competition. It\u2019s not meant to win, but rather to show you the basics of how to approach a machine learning competition and also a few models to get you off the ground (although the LightGBM model is like jumping off the deep end).", "Furthermore, I laid out my philosophy for machine learning competitions, which is to learn as much as possible by taking part in discussions, building on other\u2019s code, and sharing your own work. It\u2019s enjoyable to best your past scores, but I view doing well not as the main focus but as a positive side effect of learning new data science techniques. While these are known as competitions, they are really collaborative projects where everyone is welcome to participate and hone their abilities.", "There remains a ton of work to be done, but thankfully we don\u2019t have to do it alone. In later articles and notebooks we\u2019ll see how to build on the work of others to make even better models. I hope this article (and the notebook kernel) has given you the confidence to start competing on Kaggle or taking on any data science project.", "As always, I welcome constructive criticism and discussion and can be reached on Twitter @koehrsen_will.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F32fb9ff47426&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----32fb9ff47426---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32fb9ff47426&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32fb9ff47426&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Kaggle#How_Kaggle_competitions_work", "anchor_text": "machine learning competitions"}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle"}, {"url": "http://blog.kaggle.com/2018/04/03/q1-2018-product-update/", "anchor_text": "data science resources"}, {"url": "https://www.kaggle.com/datasets", "anchor_text": "Datasets:"}, {"url": "https://www.kaggle.com/competitions", "anchor_text": "Machine Learning Competitions:"}, {"url": "https://www.kaggle.com/learn/overview", "anchor_text": "Learn"}, {"url": "https://www.kaggle.com/discussion", "anchor_text": "Discussion:"}, {"url": "https://www.kaggle.com/kernels", "anchor_text": "Kernels:"}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit Default Risk problem"}, {"url": "https://www.kaggle.com/c/trackml-particle-identification", "anchor_text": "said for all of the competitions"}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "available here"}, {"url": "https://www.kaggle.com/c/home-credit-default-risk", "anchor_text": "Home Credit Default Risk competition"}, {"url": "http://cs231n.github.io/convolutional-networks/", "anchor_text": "convolutional neural networks"}, {"url": "http://www.homecredit.net/about-us.aspx", "anchor_text": "Home Credit"}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "Python Jupyter Notebook I put together in a kernel"}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "first notebook"}, {"url": "https://www.kaggle.com/c/home-credit-default-risk/data", "anchor_text": "column descriptions of each data file"}, {"url": "https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166", "anchor_text": "Pairs Plot"}, {"url": "https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c", "anchor_text": "that is a poor choice"}, {"url": "https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it", "anchor_text": "Receiver Operating Characteristic curve Area Under the Curve"}, {"url": "http://gim.unmc.edu/dxtests/roc3.htm", "anchor_text": "research on this one"}, {"url": "http://gim.unmc.edu/dxtests/roc2.htm", "anchor_text": "then shows the True Positive Rate versus the False Positive Rate"}, {"url": "https://machinelearningmastery.com/logistic-regression-for-machine-learning/", "anchor_text": "Logistic Regression"}, {"url": "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/", "anchor_text": "Gradient Boosting Machine"}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-Intro.html", "anchor_text": "LightGBM library"}, {"url": "https://brage.bibsys.no/xmlui/handle/11250/2433761", "anchor_text": "model wins nearly every structured Kaggle competition"}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "notebook kernel"}, {"url": "http://twitter.com/koehrsen_will", "anchor_text": "@koehrsen_will."}, {"url": "https://medium.com/tag/data-science?source=post_page-----32fb9ff47426---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----32fb9ff47426---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----32fb9ff47426---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----32fb9ff47426---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/education?source=post_page-----32fb9ff47426---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32fb9ff47426&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----32fb9ff47426---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32fb9ff47426&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----32fb9ff47426---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32fb9ff47426&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F32fb9ff47426&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----32fb9ff47426---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----32fb9ff47426--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----32fb9ff47426--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----32fb9ff47426--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}