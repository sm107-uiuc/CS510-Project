{"url": "https://towardsdatascience.com/maximum-likelihood-ml-vs-reml-78cf79bef2cf", "time": 1683013612.899939, "path": "towardsdatascience.com/maximum-likelihood-ml-vs-reml-78cf79bef2cf/", "webpage": {"metadata": {"title": "Maximum Likelihood (ML) vs. REML. Linear Mixed Model via Restricted\u2026 | by Nikolay Oskolkov | Towards Data Science", "h1": "Maximum Likelihood (ML) vs. REML", "description": "This is the nineteenth article from the column Mathematical Statistics and Machine Learning for Life Sciences where I try to explain some mysterious analytical techniques used in Bioinformatics and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/tagged/stats-ml-life-sciences?source=post_page---------------------------", "anchor_text": "Mathematical Statistics and Machine Learning for Life Sciences", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/how-linear-mixed-model-works-350950a82911", "anchor_text": "How Linear Mixed Model Works", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "Linear Mixed Model from Scratch", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Linear_regression", "anchor_text": "Linear Regression", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Nuisance_parameter", "anchor_text": "nuisance parameter", "paragraph_index": 2}, {"url": "https://github.com/NikolayOskolkov/REML", "anchor_text": "github", "paragraph_index": 4}, {"url": "https://people.csail.mit.edu/xiuming/docs/tutorials/reml.pdf", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM from Scratch", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Laplace%27s_method", "anchor_text": "Laplace approximation", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Maxima_and_minima", "anchor_text": "extremum condition", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM from Scratch", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Lasso_(statistics)", "anchor_text": "Ridge / Lasso / Elastic Net", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM from Scratch", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM_from_Scratch", "paragraph_index": 15}, {"url": "https://www.maplesoft.com/", "anchor_text": "Maple software", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "previously found", "paragraph_index": 18}, {"url": "https://github.com/NikolayOskolkov/REML", "anchor_text": "Github", "paragraph_index": 21}, {"url": "https://medium.com/u/8570b484f56c?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Nikolay Oskolkov", "paragraph_index": 21}, {"url": "http://linkedin.com/in/nikolay-oskolkov-abb321186?source=post_page---------------------------", "anchor_text": "Linkedin", "paragraph_index": 21}], "all_paragraphs": ["This is the nineteenth article from the column Mathematical Statistics and Machine Learning for Life Sciences where I try to explain some mysterious analytical techniques used in Bioinformatics and Computational Biology in a simple way. This is the final article in the series dedicated to the Linear Mixed Model (LMM). Previously we talked about How Linear Mixed Model Works, how to derive and program Linear Mixed Model from Scratch in R from the Maximum Likelihood (ML) principle. Today we will discuss the concept of Restricted Maximum Likelihood (REML), why it is useful and how to apply it to the Linear Mixed Models.", "The idea of Restricted Maximum Likelihood (REML) comes from realization that the variance estimator given by the Maximum Likelihood (ML) is biased. What is an estimator and in which way it is biased? An estimator is simply an approximation / estimate of model parameters. Assuming that statistical observations follow Normal distribution, there are two parameters: \u03bc (mean) and \u03c3\u00b2 (variance) to estimate if one wants to summarize the observations. It turns out that the variance estimator given by Maximum Likelihood (ML) is biased, i.e. the value we obtain from the ML model over- or under-estimates the true variance, see the figure below.", "In practice, when we e.g. solve a Linear Regression model using ML, we rarely think about the bias in the variance estimator, since we are usually interested in the coefficients of the linear model, which is the mean, and often do not even realize that in parallel we estimate one more fitting parameter, which is the variance. In this case the variance is considered to be a so-called nuisance parameter that is not of our primary interest.", "To demonstrate that ML indeed gives a biased variance estimator, consider a simple one-dimensional case with a variable y = (y1,y2,\u2026,yN) following e.g. the Normal distribution.", "Maximization of the likelihood, Eq.(1), leads to the estimators for mean and variance, Eq. (2), for derivation please check the notebook at my github. To manifest that the variance estimator in Eq. (2) is biased, we will derive the expected value of the variance estimator and prove that it is not equal to the true value, \u03c3\u00b2, of variance. For this purpose, we first rearrange the variance estimator, Eq. (2), by explicitly including the unknown true mean \u03bc into the equation:", "Finally, let us compute the the expected value of the variance estimator:", "Here we can see that the expected value of the ML variance estimator is not equal to the true variance \u03c3\u00b2, although it approaches the true variance at large sample sizes. Thus, the variance estimator given by ML is biased downwards, i.e. it underestimates the true variance. When N >> 1, the bias seems to be negligible until we realize that Eq. (8) was obtained for one-dimensional data. Working with high-dimensional data, that is typical for real world problems, we can get severely biased estimates of the variance because it can be derived, check for example the wonderful tutorial here, that for k-dimensional data the expected value of the variance estimator takes form:", "Therefore, the problem of underestimating the true variance by ML becomes especially acute when the number of dimensions k approaches the number of samples / statistical observations, N. We conclude, that in high-dimensional space the Maximum Likelihood (ML) principle works only in the limit k<<N, while biased results can be obtained when k \u2248 N. This bias needs to be taken somehow into account, this is exactly where REML comes into play.", "The problem with the biased variance estimator by ML appears to be due to the fact that we used an unknown estimator for the mean for computing the variance estimator. Instead, if we make sure that the log-likelihood function does not contain any information about the mean, we can optimize it with respect to the variance components and get an unbiased variance estimator. This is essentially what Restricted Maximum Likelihood (REML) does. In this case, the mean (not the variance like for ML) is considered to be a nuisance parameter that should be somehow removed from the equation. A way to get rid of the information about the mean from the log-likelihood function is to compute a marginal probability, i.e. integrate the log-likelihood over the mean. In the previous post, LMM from Scratch, we saw that for multivariate analysis working with high-dimensional data, the extension of Eq. (1) is given by the multivariate Gaussian distribution:", "where \u03a3y is the variance-covariance matrix, which is a generalization of the simple residual variance from Eq. (1). Here, we are going to take logarithm of the multivariate Gaussian distribution (log-likelihood), and integrate the log-likelihood with respect to the mean, \u03b2, and get an unbiased estimate for the variance components. Therefore, we need to compute the following integral:", "To do it we will use the saddle point approach (Laplace approximation). In this approach, since the exponential function under the integral in the third term of Eq. (9) decreases very quickly, it is enough to compute the integral in the maximum of the function f(\u03b2) in the exponent, exp( f(\u03b2) ), that will give the greatest contribution to the exponent, and therefore to the integral in Eq. (9), and hence the likelihood. Denoting the function in the exponent via f(\u03b2), we can approximate it via the Taylor series expansion in the proximity of the mean estimator point:", "Here, the linear term is zero because of the extremum condition. Here, we assume that in reality the likelihood is maximum in the true mean, however the estimator is not far from the true mean so the Taylor series expansion can be performed. Coming back to the third term in the Eq. (9), and denoting the function in the exponent as f(\u03b2), the Taylor series expansion around the mean estimator gives:", "where |\u2026| is the notation for determinant. The first two terms in Eq. (12) are the ML solution that we also obtained in LMM from Scratch (Eq. (10) in that article). In contrast, the third term comes from the REML approach. One can think about this additional term as a penalty (or a bias) in a penalized model (Ridge / Lasso / Elastic Net), where we put a constraint on the coefficients in the linear regression (or LMM) model. Let us check how this additional term coming from REML affects the solution of the Linear Mixed Model (LMM) for the toy data set that was introduced in the LMM from Scratch post.", "To recap, we were considering only 4 data points for simplicity: 2 originating from Individual #1 and the other 2 coming from Individual #2. Further, the 4 points are spread between two conditions: untreated and treated, please see the figure below. In the Treat column 0 means untreated and 1 means treated.", "We fit the data using LMM with fixed effects for slopes and intercepts and random effects for intercepts using the lmer function from lme4 R package. Including random effects intercepts that account for grouping factor Ind (the individual ID), we will need too use a special syntax (1 | Ind) for the lmer function. Now, we can fit the LMM model using the Restricted Maximum Likelihood (REML) approach, for this purpose, we specify REML = TRUE:", "Please notice the shared and residual standard deviations, 8.155 and 6.0, respectively, that we denoted as \u03c3s and \u03c3 in the previous post. We will later reproduce these values when implementing REML solution for LMM. As we saw in the previous LMM_from_Scratch tutorial, knowing the coordinates of the data points y11, y12, y21 and y22, the first two terms in Eq. (12) can be computed as:", "The third term in Eq. (12) can also be analytically derived, since we know both the X, design matrix, and the inverse variance covariance matrix, \u03a3y. Below we present a screenshot from Maple software, where it is shown that the third term in Eq. (12) takes form:", "Thus, third term in Eq. (12) has the following simple expression:", "Therefore, we can now minimize the log-likelihood function in the Restricted Maximum Likelihood (REML) approximation, i.e. when the log-likelihood Eq. (12) function does not contain any information about the mean \u03b2, i.e. this is not a parameter of optimization any more but has a fixed / estimated values \u03b21=6, and \u03b22=15.5, that were previously found. Now, everything is ready for performing numerical minimization of the log-likelihood function, Eq. (12), with respect to \u03c3s and \u03c3 in the REML approximation:", "From the minimization of the log-likelihood function we obtain \u03c3 = 6.00 and \u03c3s = 8.155, exactly the standard deviations that we also obtained by the lmer function with REML = TRUE. We reproduced the Random Effects residual variance \u03c3 and shared across data points variance \u03c3s for both Maximum Likelihood (REML=FALSE), in the previous post, and Restricted Maximum Likelihood (REML=TRUE), in this post. And, we have derived and codded it from scratch using R, well done!", "In this article, we have learnt that the Maximum Likelihood (ML) variance estimator is biased, especially for high-dimensional data, due to using an unknown mean estimator. Restricted Maximum Likelihood (REML) fixes this issue by removing first all the information about the mean estimator prior to minimizing the log-likelihood function. We have successfully reproduced the variance components reported by lmer with REML = TRUE, and we derived and coded REML from scratch using R.", "In the comments below, let me know which analytical techniques from Life Sciences seem especially mysterious to you and I will try to cover them in the future posts. Check the codes from the post on my Github. Follow me at Medium Nikolay Oskolkov, in Twitter @NikolayOskolkov and do connect in Linkedin. In the next post, we will cover how to cluster in UMAP space, stay tuned.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F78cf79bef2cf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://nikolay-oskolkov.medium.com/?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": ""}, {"url": "https://nikolay-oskolkov.medium.com/?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Nikolay Oskolkov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8570b484f56c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&user=Nikolay+Oskolkov&userId=8570b484f56c&source=post_page-8570b484f56c----78cf79bef2cf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78cf79bef2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78cf79bef2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/stats-ml-life-sciences", "anchor_text": "Mathematical Statistics and Machine Learning for Life Sciences"}, {"url": "https://towardsdatascience.com/tagged/stats-ml-life-sciences?source=post_page---------------------------", "anchor_text": "Mathematical Statistics and Machine Learning for Life Sciences"}, {"url": "https://towardsdatascience.com/how-linear-mixed-model-works-350950a82911", "anchor_text": "How Linear Mixed Model Works"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "Linear Mixed Model from Scratch"}, {"url": "https://en.wikipedia.org/wiki/Linear_regression", "anchor_text": "Linear Regression"}, {"url": "https://en.wikipedia.org/wiki/Nuisance_parameter", "anchor_text": "nuisance parameter"}, {"url": "https://github.com/NikolayOskolkov/REML", "anchor_text": "github"}, {"url": "https://people.csail.mit.edu/xiuming/docs/tutorials/reml.pdf", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM from Scratch"}, {"url": "https://en.wikipedia.org/wiki/Laplace%27s_method", "anchor_text": "Laplace approximation"}, {"url": "https://en.wikipedia.org/wiki/Maxima_and_minima", "anchor_text": "extremum condition"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM from Scratch"}, {"url": "https://en.wikipedia.org/wiki/Lasso_(statistics)", "anchor_text": "Ridge / Lasso / Elastic Net"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM from Scratch"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "LMM_from_Scratch"}, {"url": "https://www.maplesoft.com/", "anchor_text": "Maple software"}, {"url": "https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4", "anchor_text": "previously found"}, {"url": "https://github.com/NikolayOskolkov/REML", "anchor_text": "Github"}, {"url": "https://medium.com/u/8570b484f56c?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Nikolay Oskolkov"}, {"url": "http://linkedin.com/in/nikolay-oskolkov-abb321186?source=post_page---------------------------", "anchor_text": "Linkedin"}, {"url": "https://medium.com/tag/data-science?source=post_page-----78cf79bef2cf---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----78cf79bef2cf---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/statistics?source=post_page-----78cf79bef2cf---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/stats-ml-life-sciences?source=post_page-----78cf79bef2cf---------------stats_ml_life_sciences-----------------", "anchor_text": "Stats Ml Life Sciences"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----78cf79bef2cf---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78cf79bef2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&user=Nikolay+Oskolkov&userId=8570b484f56c&source=-----78cf79bef2cf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78cf79bef2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&user=Nikolay+Oskolkov&userId=8570b484f56c&source=-----78cf79bef2cf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78cf79bef2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F78cf79bef2cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----78cf79bef2cf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----78cf79bef2cf--------------------------------", "anchor_text": ""}, {"url": "https://nikolay-oskolkov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://nikolay-oskolkov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nikolay Oskolkov"}, {"url": "https://nikolay-oskolkov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8570b484f56c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&user=Nikolay+Oskolkov&userId=8570b484f56c&source=post_page-8570b484f56c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff4a74ad409c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-ml-vs-reml-78cf79bef2cf&newsletterV3=8570b484f56c&newsletterV3Id=f4a74ad409c6&user=Nikolay+Oskolkov&userId=8570b484f56c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}