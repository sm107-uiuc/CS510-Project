{"url": "https://towardsdatascience.com/how-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72", "time": 1683015920.620687, "path": "towardsdatascience.com/how-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72/", "webpage": {"metadata": {"title": "How Convolutional Layers Work in Deep Learning Neural Networks? | by Jingles (Hong Jing) | Towards Data Science", "h1": "How Convolutional Layers Work in Deep Learning Neural Networks?", "description": "Convolution is a linear operation that involves a multiplicating of weights with input and producing an output. Like a sliding window function applied to a matrix."}, "outgoing_paragraph_urls": [{"url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d", "anchor_text": "PyTorch documentation", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1606.00915", "anchor_text": "DeepLab", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/improve-glaucoma-assessment-with-brain-computer-interface-and-machine-learning-6c3b774494f8?sk=5883c05cc3668079ef78a0b932520c3c", "anchor_text": "applied dilated convolutions in my work", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1606.00915", "anchor_text": "DeepLab", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1511.07122.pdf", "anchor_text": "Multi-Scale Context Aggregation by Dilated Convolutions", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/improve-glaucoma-assessment-with-brain-computer-interface-and-machine-learning-6c3b774494f8?sk=5883c05cc3668079ef78a0b932520c3c", "anchor_text": "In my work", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "Network in Network", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1409.4842", "anchor_text": "GoogLeNet", "paragraph_index": 32}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet", "paragraph_index": 33}, {"url": "https://arxiv.org/abs/1803.01271", "anchor_text": "TCN", "paragraph_index": 34}, {"url": "http://linkedin.com/in/jingles", "anchor_text": "linkedin.com/in/jingles", "paragraph_index": 36}], "all_paragraphs": ["In deep learning, convolutional layers have been major building blocks in many deep neural networks. The design was inspired by the visual cortex, where individual neurons respond to a restricted region of the visual field known as the receptive field. A collection of such fields overlap to cover the entire visible area.", "Though convolutional layers were initially applied in computer vision, its shift-invariant characteristics have allowed convolutional layers to be applied in natural language processing, time series, recommender systems, and signal processing.", "The easiest way to understand a convolution is by thinking of it as a sliding window function applied to a matrix. This article will see how 1D convolution works and explore the effects of each parameter:", "Convolution is a linear operation that involves a multiplicating of weights with input and producing an output. The multiplication is performed between an array of input data and an array of weights, called a kernel (or a filter). The operation applied between the input and the kernel, is a sum of an element-wise dot product. The result of each operation is a single value.", "Let us start with the simplest example, using 1D convolution when you have 1D data. Applying a convolution on a 1D array performs the multiplication of the value in the kernel with every value in the input vector.", "Assume that the value in our kernel (also known as \u201cweights\u201d) is \u201c2\u201d, we will multiply each element in the input vector by 2, one after another until the end of the input vector, and get our output vector. The size of the output vector is the same as the size of the input.", "First, we multiply 1 by the weight, 2, and get \u201c2\u201d for the first element. Then we shift the kernel by 1 step, multiply 2 by the weight, 2 to get \u201c4\u201d. We repeat this until the last element, 6, and multiply 6 by the weight, and we get \u201c12\u201d. This process produces the output vector.", "The different sized kernel will detect differently sized features in the input and, in turn, will result in different sized feature maps. Let\u2019s look at another example, where the kernel size is 1x2, with the weights \u201c2\u201d. Like before, we slide the kernel across the input vector over each element. We perform convolution by multiply each element to the kernel and add up the products to get the final output value. We repeat this multiplication and addition, one after another until the end of the input vector, and produce the output vector.", "First, we multiply 1 by 2 and get \u201c2\u201d, and multiply 2 by 2 and get \u201c2\u201d. Then we add the two numbers, 2 and 4, and we get \u201c6\u201d\u2013that is the first element in the output vector. We repeat the same process until the end of the input vector and produce the output vector.", "As you might have noticed, the output vector is slightly smaller than before. That is because we increased the kernel\u2019s size, from 1x1 to 1x2. Looking at the PyTorch documentation, we can calculate the output vector\u2019s length with the following:", "If we apply a kernel with size 1x2 on an input vector of size 1x6, we can substitute the values accordingly and get the output length of 1x5:", "Calculate the output feature\u2019s size is essential if you are building neural network architectures.", "In the previous example, a kernel size of 2 is a little uncommon, so let\u2019s take another example where our kernel size is 3, where its weights are \u201c2\u201d. Like before, we perform convolution by multiply each element to the kernel and add up the products. We repeat this process until the end of the input vector, which produces the output vector.", "Likewise, the output vector is smaller than the input. Applying a 1x3 kernel on a 1x6 input vector will result in a feature vector with a size of 1x4.", "In image processing, it is common to use 3\u00d73, 5\u00d75 sized kernels. Sometimes we might use kernels of size 7\u00d77 for larger input images.", "Applying convolution with a 1x3 kernel on a 1x6 input, we got a shorter output vector, 1x4. By default, a kernel starts on the left of the vector. The kernel is then stepped across the input vector one element at a time until the rightmost kernel element is on the last element of the input vector. Thus, the larger the kernel size is, the small the output vector is going to be.", "When to use paddings? Sometimes, it is desirable to produce a feature vector of the same length as the input vector. We can achieve that by adding padding. Padding is adding zeros at the beginning and the end of the input vector.", "By adding 1 padding to the 1x6 input vector, we are artificially creating an input vector with size 1x8. This adds an element at the beginning and the end of the input vector. Performing convolutions with a kernel size of 3, the output vector is essentially the same size as the input vector. The padding added has zero value; thus it has no effect on the dot product operation when the kernel is applied.", "For a convolution with a kernel size of 5, we can also produce an output vector of the same length by adding 2 paddings at the front and the end of the input vector. Likewise, for images, applying a 3x3 kernel to the 128x128 images, we can add a border of one pixel around the outside of the image to produce the size 128x128 output feature map.", "So far, we have been sliding the kernel by 1 step at a time. The amount of movement on the kernel to the input image is referred to as \u201cstride\u201d, the default stride value is 1. But we can always shift the kernel by any number of elements, by increasing the stride size.", "For example, we can shift our kernel with a stride of 3. First, we will multiply and sum the first three elements. Then we will slide the kernel by three steps and perform the same operation for the next three elements. As a result, our output vector is of size 2.", "When to increase stride size? In most cases, we increase the stride size to down-sample the input vector. Applying a stride size of 2 will reduce the length of the vector by half. Sometimes, we can use a larger stride to replace pooling layers to reduce the spatial size, reducing the model\u2019s size and increasing speed.", "While you were reading deep learning literature, you may have noticed the term \u201cdilated convolutions\u201d. Dilated convolutions \u201cinflate\u201d the kernel by inserting spaces between the kernel elements, and a parameter controls the dilation rate. A dilation rate of 2 means there is a space between the kernel elements. Essentially, a convolution kernel with dilation = 1 corresponds to a regular convolution.", "Dilated convolutions are used in the DeepLab architecture, and that is how the atrous spatial pyramid pooling (ASPP) works. With ASPP, high-resolution input feature maps were extracted, and it manages to encode image context at multiple scales. I have also applied dilated convolutions in my work for signal processing, as it can effectively increase the output vector\u2019s receptive field without increasing the kernel size (without increasing the model\u2019s size too).", "When to use dilated convolutions? Generally, dilated convolutions have shown better segmentation performance in DeepLab and in Multi-Scale Context Aggregation by Dilated Convolutions. You might want to use dilated convolutions if you want an exponential expansion of the receptive field without loss of resolution or coverage. This allows us to have a larger receptive field with the same computation and memory costs while preserving resolution.", "By default, the \u201cgroups\u201d parameter is set to 1, where all the inputs channels are convolved to all outputs. To use groupwise convolution, we can increase the \u201cgroups\u201d value; this will force the training to split the input vector\u2019s channels into different groupings of features.", "When groups=2, this is essentially equivalent to having two convolution layers side by side, where each only process half the input channels. Each group then produce half the output channels and then subsequently concatenated to form the final output vector.", "Depthwise convolution. Groups are utilized when we want to perform depthwise convolution, for example, if we want to extract image features on R, G, and B channels separately. When groups == in_channels and out_channels == K * in_channels; this operation is also termed in literature as depthwise convolution.", "In 2012, grouped convolutions were introduced in the AlexNet paper, where their primary motivation was to allow the network\u2019s training over two GPUs. However, there was an interesting side-effect to this engineering hack, that they learn better representations. Training an AlexNet with and without grouped convolutions have different accuracy and computational efficiency. AlexNet without grouped convolutions is less efficient and is also slightly less accurate.", "In my work, I have also applied grouped convolutions to effectively trained a scalable multi-task learning model. I can tweak and scale to any number of tasks by tweaking the \u201cgroup\u201d parameter.", "Several papers use 1x1 convolutions, as first investigated by Network in Network. It can be confusing to see 1x1 convolutions, and seems like it does not make sense as it is just pointwise scaling.", "However, this is not the case because, for example, in computer vision, we are operating over 3-dimensional volumes; the kernels always extend through the full depth of the input. If the input is 128x128x3, then doing 1x1 convolutions would effectively be doing 3-dimensional dot products since the input depth is 3 channels.", "In GoogLeNet, the 1\u00d71 kernel was used for dimensionality reduction and for increasing the dimensionality of feature maps. The 1\u00d71 kernel is also used to increase the number of feature maps after pooling; this artificially creates more feature maps of the downsampled features.", "In ResNet, the 1\u00d71 kernel was used as a projection technique to match the number of filters of input to the residual output modules in the design of the residual network.", "In TCN, the 1\u00d71 kernel was added to account for discrepant input-output widths, as the input and output could have different widths. 1\u00d71 kernel convolution ensures that the elementwise addition receives tensors of the same shape.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Alibaba PhD in machine learning | write about machine learning, neuroscience, healthcare & blockchain | reach me at linkedin.com/in/jingles"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2913af333b72&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2913af333b72--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2913af333b72--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jinglesnote.medium.com/?source=post_page-----2913af333b72--------------------------------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=post_page-----2913af333b72--------------------------------", "anchor_text": "Jingles (Hong Jing)"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F641197e9ee36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=post_page-641197e9ee36----2913af333b72---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2913af333b72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2913af333b72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://pixabay.com/users/stokpic-692575/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=629726", "anchor_text": "stokpic"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=629726", "anchor_text": "Pixabay"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d", "anchor_text": "PyTorch documentation"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d", "anchor_text": "source"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://arxiv.org/pdf/1606.00915", "anchor_text": "DeepLab"}, {"url": "https://towardsdatascience.com/improve-glaucoma-assessment-with-brain-computer-interface-and-machine-learning-6c3b774494f8?sk=5883c05cc3668079ef78a0b932520c3c", "anchor_text": "applied dilated convolutions in my work"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://arxiv.org/pdf/1606.00915", "anchor_text": "DeepLab"}, {"url": "https://arxiv.org/pdf/1511.07122.pdf", "anchor_text": "Multi-Scale Context Aggregation by Dilated Convolutions"}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": ""}, {"url": "https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/", "anchor_text": "image by author"}, {"url": "https://towardsdatascience.com/improve-glaucoma-assessment-with-brain-computer-interface-and-machine-learning-6c3b774494f8?sk=5883c05cc3668079ef78a0b932520c3c", "anchor_text": "In my work"}, {"url": "https://arxiv.org/abs/1312.4400", "anchor_text": "Network in Network"}, {"url": "https://arxiv.org/abs/1409.4842", "anchor_text": "GoogLeNet"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet"}, {"url": "https://arxiv.org/abs/1803.01271", "anchor_text": "TCN"}, {"url": "https://towardsdatascience.com/improve-glaucoma-assessment-with-brain-computer-interface-and-machine-learning-6c3b774494f8", "anchor_text": "Improve Glaucoma Assessment with Brain-Computer Interface and Machine LearningMy research used multitask learning to provide rapid point-of-care diagnostics to detect peripheral vision losstowardsdatascience.com"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-transformer-cf6969ffa067", "anchor_text": "Illustrated Guide to TransformerA component by component breakdown analysistowardsdatascience.com"}, {"url": "https://jinglescode.github.io/", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/", "anchor_text": ""}, {"url": "https://jingles.substack.com/subscribe", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2913af333b72---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/technology?source=post_page-----2913af333b72---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2913af333b72---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----2913af333b72---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----2913af333b72---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2913af333b72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----2913af333b72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2913af333b72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----2913af333b72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2913af333b72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2913af333b72--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2913af333b72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2913af333b72---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2913af333b72--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2913af333b72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2913af333b72--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2913af333b72--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2913af333b72--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2913af333b72--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2913af333b72--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2913af333b72--------------------------------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jingles (Hong Jing)"}, {"url": "https://jinglesnote.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://linkedin.com/in/jingles", "anchor_text": "linkedin.com/in/jingles"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F641197e9ee36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=post_page-641197e9ee36--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb9c0df6b9368&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72&newsletterV3=641197e9ee36&newsletterV3Id=b9c0df6b9368&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}