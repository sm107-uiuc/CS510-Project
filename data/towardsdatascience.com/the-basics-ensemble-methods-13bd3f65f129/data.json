{"url": "https://towardsdatascience.com/the-basics-ensemble-methods-13bd3f65f129", "time": 1683002529.3635032, "path": "towardsdatascience.com/the-basics-ensemble-methods-13bd3f65f129/", "webpage": {"metadata": {"title": "The Basics: Ensemble Methods. Bagging, Random Forest and Boosted\u2026 | by Max Miller | Towards Data Science", "h1": "The Basics: Ensemble Methods", "description": "Ensemble methods are the data science version of the old adage about two heads being better than one: if one model works well, multiple models working in concert can do even better. Of course, one of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-basics-decision-tree-classifiers-b0d20394eaeb", "anchor_text": "introduction to tree models", "paragraph_index": 1}], "all_paragraphs": ["Ensemble methods are the data science version of the old adage about two heads being better than one: if one model works well, multiple models working in concert can do even better. Of course, one of the challenges is that it can be hard to gather enough data to make multiple independent models. As it turns out, with only a little bit of sleight of hand, we can build different models out of a single dataset and combine their outputs for improved results. These ensemble methods are particularly helpful for extending tree based models, both classifiers and regressors, and often outperform the tree models on their own.", "One challenge with decision tree models is that their structure is very brittle: as I discussed in my introduction to tree models, slight changes to the training data can result in large changes to the final tree. If one of the early splits in the tree is made in a different place or along a different variable as a result of some change in the training data, all the following decisions of the tree will necessarily be different. As a result, decision trees built on similar but not identical data can be very different from each other in both structure and results \u2014 more or less accurate, more or less overfit or even overfit in different ways.", "To visualize what this means in practical terms, I generated an example dataset with a thousand points. I then randomly drew three quarters of those points as a sample, fit a decision tree model on this sample and tested its accuracy. I did this 500 times, ranked the models by accuracy and plotted the results:", "On average the models generated were about 77% accurate, but there was a wide amount of variance in outcomes. Some models were less than 70% accurate, while a select few were nearly 85% accurate. What is significant is that each of these models, despite very different outcomes, was trained on very similar data. Indeed, every single model was trained on more than half of the total dataset, so there must have been at least some overlap between the training data of any two of the models created.", "The fact that decision trees are so brittle, that two trees trained on most of the same data can produce such different results, actually allows us to create many different and sufficiently distinct models from the same dataset. The processing is called \u2018bagging\u2019, a shortening of the two steps involved: bootstrapping and aggregating. In the first step, bootstrapping, we take our training data and create a number of smaller datasets by pulling random samples. Each of these smaller datasets is used to train a separate model. All of these model are then used to generate predictions by aggregating the results, averaging the output of each of the individual models (or in a classification problem, you can think about it as having the models \u2018vote\u2019 on which class to predict).", "Somewhat miraculously, this aggregated prediction frequently performs substantially better than individual models tend to do on average. To demonstrate this, let\u2019s revisit the training data I created before. Again, I took a bunch of random samples, created models and measured how accurate they were, but this time with one added wrinkle. Instead of simply creating one decision tree model on each of my samples, I created 100 different models and combined their predictions using this bagging method. Lining up the resulting accuracies like we did before, again we see a similar shape, with some of bagged models being more accurate than others, but our accuracies have been lifted up across the entire curve!", "Previously, a single decision tree derived on a subsample of the dataset was around 77% accurate. A single bagged model, however, is 85% accurate on average.", "You may have a sense that this seems like it shouldn\u2019t really work. Sure, we\u2019ve gone from one model to multiple models working in tandem, but each of our new models is only trained on a portion of the available data. Before we only had one model, but it utilized all of the data. We haven\u2019t gained any more data, in aggregate, and don\u2019t even use all our data at once, yet somehow parcelling the data up into a bunch of little samples allows us to extract 8% more accuracy out of our models. The key lies in the fact that the bootstrapped models are likely to be very different from each other. As we\u2019ve noted, two decision trees built on largely the same data can end up having very different structures. Combining the results of many different models, all of which are built on slightly different data actually gives us many different decision tree structures.", "As you can imagine, the number of bootstrapped models being used is a key factor. You start to accrue benefits when you have numerous, distinct models that respond to different features of the data and yield different results. If you only have two or three models, you\u2019re not likely to outperform a single decision tree, since even though you have more than one model, your models are only built from smaller samples of the data. On the other end of the spectrum, once you have a sufficient number of models, you\u2019ve extracted as much inferential power from the data as is possible and adding more bootstrapped models doesn\u2019t yield any extra benefit (luckily, having too many models in a bagged method doesn\u2019t generally yield overfit, although it becomes more computationally expensive to fit and predict with such a model). Graphing the number of bootstrapped models versus the accuracy of the aggregated predictions is informative:", "At first, the bagged model underperforms the average single decision tree, but after only a handful of bootstrapped models have been added to our ensemble (in this case, four or five) the bagged model begins to significantly outperform the average single model accuracy. After 40 or 50 bootstrapped models, the performance of the bagged model seems to level out and it doesn\u2019t get a lot of extra benefit from subsequent models being added to the ensemble.", "Seeing this big improvement in predictive power, why would you ever use a single tree? Well, for starters, as you can probably guess training and running one tree is less computationally intensive than training and running 100 trees (even if each individual tree is a little smaller). For another thing, bagged models lose a lot of interpretability. Previously, it was easy to map out the structure of the decision tree into something like a flow chart and follow along, seeing what features seemed important and where the critical values might be. Now you\u2019ve got a whole host of trees to map out, each with a different structure, a different set of critical values, each giving a different sense of what was \u2018important\u2019 to the model. They necessarily won\u2019t agree with each other, making it much harder to interpret.", "As we\u2019ve emphasized, the reason bagged models work is because the underlying decision trees being aggregated are sufficiently different from each other. You can think of each of the underly trees as approaching the problem in a slightly different way, making a different set of decisions and possibly gleaning slightly different information from the process. A natural progression might be to make these underlying models even more distinct from each other, which is at the heart of the Random Forest strategy.", "A Random Forest model is built in much the same way as a bagged model is, with numerous different decision trees being trained and then used in tandem. The key difference is that in a random forest, in addition to the trees being trained on slightly different sub-samples of the overall training data, they each use only a randomly selected sample of feature variables. Say, for instance, I am using the common Titanic dataset, which has around 8 meaningful feature variables including the passenger\u2019s sex, class, fare and the location from which they embarked. An ordinary bagged model would aggregate numerous decision trees that used all 8 of these features. A random forest model would aggregate decisions trees that each used a different subset of these features. One tree might use sex, class, age and cabin. Another might also happen to use sex and age, but not class or cabin and use embarked location and number of siblings instead.", "By randomly selecting both the sample of training data points and the features being utilized, random forests create even more diversity among the aggregated decision trees and can therefore approach the prediction problem from even more angles. Like an ordinary bagged model, the accuracy of a random forest increases as the number of trees in the ensemble is increased, up to a point. Consider the performance of bagged models and random forests on my example dataset as the number of aggregated trees increases:", "Like the bagged model, our random forest starts out underperforming the average of a single tree. Actually, the random forest starts out underperforming the bagged model as well \u2014 remember that each tree in the random forest is built not just with only a fraction of the available training data, but also with only a fraction of the available features, so each individual tree is going to be less powerful than a single tree using all the features and you might very well expect a random forest with only a handful of trees to underperform a bagged model with the same number of trees. Once a few trees have been added to the forest, performance improves quickly. Once the forest has ten trees, the model is probably incorporating information from every feature and row in the dataset somewhere and it starts to benefit from the diversity we\u2019ve induced in the trees. The random forest in this case ends up adding around another 2.5% accuracy to our model.", "The final ensemble method to consider is Boosting, which operates in a different manner than our bagging or random forest methods. Ordinary bagging and random forests both generate multiple models in parallel and run them at the same time, somehow averaging the results. Boosting also creates multiple trees, but it creates and runs them sequentially. A first decision tree is created and fit to the data. Then a subsequent tree is created, but instead of being fit to the target variable of the data directly, this tree is fit to the residuals of the first tree: that is, it is fit to the amount that the predictions provided by the first tree are wrong. With a regression tree, this will be some positive or negative number depending on how much the model over or undershot the mark. In a classification problem, this error term will simply be 0 (if the previous tree got the prediction correct) or +/-1.", "To make a prediction, you need to use the two trees in sequence. The first tree makes a prediction, the second tree provides an adjustment based on how incorrect it thinks the results of the first tree are. Of course, you don\u2019t need to stop there, you can fit a third tree to residuals of the result of the first two trees used together, and so on until you have many trees. Hopefully, with each subsequent tree, you can further hone the predictions, although, unlike bagged models, boosted models are prone to overfit if the number of trees being used becomes excessive.", "Actually, there\u2019s one further wrinkle in order to get this chain of models to yield sensible results. If you simply connect multiple models as I\u2019ve described, your results may not actually improve at all. One model will overshoot, then the next model will overcorrect and swing too far in the other direction. To ensure that the models actually converge on an answer, the results of subsequent models are discounted by a shrinkage factor called the \u2018learning rate\u2019. Lowering the learning rate can help avoid overfit, but will require more models to achieve good results. Consider, our dataset attacked by boosted trees with various learning rates and numbers of trees:", "You\u2019ll notice in this case that the boosted models all seem to top out at around the same level of accuracy, though the ones with lower learning rates take a little long to get to that level accuracy.", "On the example data set I created, the boosted algorithm works about as well as the random forest, though perhaps it goes without saying that your mileage may vary depending on what sort of data you are dealing with:", "In any case, you\u2019ll notice that all three of these ensemble methods handily outperform the single decision tree upon which they are conceptually dependent.", "Addendum: what about ensembles using other types of models?", "At this point you may be convinced of the power of these ensemble methods and start wondering why we\u2019ve limited ourselves to decision trees. Can we use bootstrap aggregating to improve the performance of other types of models as well? The short answer is that a method like bagging typically doesn\u2019t work for other model types. For instance, what if instead of decision trees, we tried bagging logistic models for out classification problem? Here are the results as we aggregate bootstrapped logistic models:", "The line representing the bagged logistic models no longer shows a clear trend and doesn\u2019t represent any significant improvement over the single logistic model. The measured accuracy bounces around a little with the randomness of the samples used to test the model, but at any level of aggregation, the bagged accuracy rate is just as likely to fall below the average of a single logistic model.", "Why is this? Well, because the logistic models we\u2019re trying to aggregate here are too similar. This method works because decision trees are brittle and even small changes to the training data can create trees of very different structure. By contrast, two logistic models produced using slightly different datasets are likely to be very similar to each other; they will each use all of the features and the coefficients the models find for each feature are likely to be close to each other. The coefficients in a logistic regression can certainly be influenced by a handful of outlier values, but once your sample is in the hundreds, a single new value is unlikely to nudge the final coefficient value all that much.", "Unfortunately, bagging is not a one size fits all solution, but for tree based methods it can be quite powerful.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F13bd3f65f129&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----13bd3f65f129---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13bd3f65f129&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13bd3f65f129&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/the-basics-decision-tree-classifiers-b0d20394eaeb", "anchor_text": "introduction to tree models"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----13bd3f65f129---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ensemble-learning?source=post_page-----13bd3f65f129---------------ensemble_learning-----------------", "anchor_text": "Ensemble Learning"}, {"url": "https://medium.com/tag/random-forest?source=post_page-----13bd3f65f129---------------random_forest-----------------", "anchor_text": "Random Forest"}, {"url": "https://medium.com/tag/data-science-ground-up?source=post_page-----13bd3f65f129---------------data_science_ground_up-----------------", "anchor_text": "Data Science Ground Up"}, {"url": "https://medium.com/tag/data-science?source=post_page-----13bd3f65f129---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13bd3f65f129&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&user=Max+Miller&userId=dfd5ba1a8332&source=-----13bd3f65f129---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13bd3f65f129&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&user=Max+Miller&userId=dfd5ba1a8332&source=-----13bd3f65f129---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13bd3f65f129&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F13bd3f65f129&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----13bd3f65f129---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----13bd3f65f129--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----13bd3f65f129--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----13bd3f65f129--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "409 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-ensemble-methods-13bd3f65f129&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}