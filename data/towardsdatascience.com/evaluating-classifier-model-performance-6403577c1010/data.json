{"url": "https://towardsdatascience.com/evaluating-classifier-model-performance-6403577c1010", "time": 1683010364.696956, "path": "towardsdatascience.com/evaluating-classifier-model-performance-6403577c1010/", "webpage": {"metadata": {"title": "Evaluating Classifier Model Performance | by Andrew Hetherington | Towards Data Science", "h1": "Evaluating Classifier Model Performance", "description": "It\u2019s 4am and you\u2019re on your seventh coffee. You\u2019ve trawled the forums to find the most sophisticated model you can. You\u2019ve set up your preprocessing pipeline and you\u2019ve picked your hyperparameters\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/andrewhetherington/python-projects/blob/master/Blog%E2%80%94Evaluating%20Classifier%20Model%20Performance/Evaluating%20Classifier%20Model%20Performance.ipynb", "anchor_text": "the notebook used to produce this work", "paragraph_index": 12}, {"url": "https://machinelearningmastery.com/k-fold-cross-validation/", "anchor_text": "cross-validation", "paragraph_index": 19}, {"url": "https://unsplash.com/@freegraphictoday?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "AbsolutVision,", "paragraph_index": 54}, {"url": "https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Markus Spiske", "paragraph_index": 54}, {"url": "https://unsplash.com/@m3design?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Marat Gilyadzinov", "paragraph_index": 54}], "all_paragraphs": ["It\u2019s 4am and you\u2019re on your seventh coffee. You\u2019ve trawled the forums to find the most sophisticated model you can. You\u2019ve set up your preprocessing pipeline and you\u2019ve picked your hyperparameters. Now, time to evaluate your model\u2019s performance.", "You\u2019re shaking with excitement (or it could be the caffeine overdose). This is it \u2014 your big debut onto the Kaggle world stage. As your predictions are being submitted, your thoughts turn to what you\u2019re going to do with the prize money. A Lamborghini or a Ferrari? And in what colour? The red goes best with cream upholstery, but at the same time\u2026", "The leader board pops up on your screen as if to announce something.", "Your model performance has gotten worse. You sit in silence for what seems like an age.", "Eventually, you sigh, close your laptop lid, and go to bed.", "If you\u2019ve tried building a model before, you\u2019ll know that it\u2019s an iterative process. Progress isn\u2019t linear \u2014 there can be long periods where it seems like you\u2019re getting no closer to your objective, or even going backwards \u2014 until that breakthrough occurs and you surge forward\u2026 and right into the next problem.", "Monitoring model performance on a validation set is an excellent way to get feedback on whether what you\u2019re doing is working. It\u2019s also a great tool for comparing two different models \u2014 ultimately, our aim is to build better, more accurate models that will help us make better decisions in real world applications. When we\u2019re trying to communicate the value of a model to a stakeholder in a particular situation, they\u2019re going to want to know why they should care: what\u2019s in it for them? How is it going to make their life easier? We need to be able to compare what we\u2019ve built to the systems that are already in place.", "Evaluating model performance can tell us if our approach is working \u2014 this turns out to be helpful. We can continue to explore and see how far we can push our existing conceptualisation of the problem we\u2019re working on. It can also tell us if our approach isn\u2019t working \u2014 this turns out to be even more helpful because if our adjustments are making the model worse at what it\u2019s supposed to be doing, then it indicates that we may have misunderstood our data or the situation being modelled.", "So evaluation of model performance is useful \u2014 but how exactly do you do it?", "For the moment, we are going to concentrate on a particular class of model \u2014 classifiers. These models are used to put unseen instances of data into a particular class \u2014 for example, we could set up a binary classifier (two classes) to distinguish whether a given image is of a dog or a cat. More practically, a binary classifier could be used to decide whether an incoming email should classified as spam, whether a particular financial transaction is fraudulent, or whether a promotional email should be sent to a particular customer of an online store based on their shopping history.", "The techniques and metrics used to assess the performance of a classifier will be different from those used for a regressor, which is a type of model that attempts to predict a value from a continuous range. Both types of model are common, but for now, let\u2019s limit our analysis to classifiers.", "To illustrate some of the important concepts, we\u2019ll set up a simple classifier that predicts whether an image of a particular image is a seven or not. Let\u2019s use the famous NMIST dataset to train and test our model.", "(If you want to see the full Python code or follow along at home, check out the notebook used to produce this work on GitHub!)", "Below, we use Scikit-Learn to download our data and to build our classifier. First, import NumPy for maths and Matplotlib for plotting:", "Then, use scikit-learn\u2019s built-in helper function to download the data. The data comes as a dictionary \u2014 we can use the \u201cdata\u201d key to access instances of training and test data (the images of the digits) and the \u201ctarget\u201d key to access the labels (what the digits have been hand-labelled as).", "The full dataset contains a whopping 70,000 images \u2014 let\u2019s take a subset of 10% of the data to make it easier to quickly train and test our model.", "Now, let\u2019s briefly inspect the first few digits in our training data. Each digit is actually represented by 784 values from 0 to 255, which represent how dark each pixel in a 28 by 28 grid should be. We can easily reshape the data into a grid and plot the data using matplotlib\u2019s imshow() function:", "Now, let\u2019s create a new set of labels \u2014 we are only interested at the moment in whether an image of a seven or not. Once that\u2019s done, we can train a model to hopefully pick up the characteristics that make a digit \u201cseven-y\u201d:", "For our model, we\u2019ve used an SGD (or Stochastic Gradient Descent) classifier. And for our array of nine example digits, it looks like our model is doing the right kind of thing \u2014 it has correctly identified the one seven from the eight other not-sevens. Let\u2019s make predictions on our data using 3-fold cross-validation:", "Ok \u2014 now we have a set of predictions for each instance in our training set. It may sound strange that we are making predictions on our training data, but we avoid making predictions on data the model has already been trained on by using cross-validation \u2014 follow the link to learn more.", "At this point, let\u2019s take a step back and think about the different situations we might find ourselves in for a given model prediction.", "We use the terms true/false positive (TP/FP) and true/false negative (TN/FN) to describe each of the four possible outcomes listed above. The true/false part refers to whether the model was correct or not. The positive/negative part refers to whether the instance being classified actually was or was not the instance we wanted to identify.", "A good model will have a high level of true positive and true negatives, because these results indicate where the model has got the right answer. A good model will also have a low level of false positives and false negatives, which indicate where the model has made mistakes.", "These four numbers can tell us a lot about how the model is doing and what we can do to help. Often, it\u2019s helpful to represent them as a confusion matrix.", "We can use sklearn to easily extract the confusion matrix:", "The columns of this matrix represent what our model has predicted \u2014 not-7 on the left and 7 on the right. The rows represent what each instance that the model predicted actually was \u2014 not-7 on the top and 7 on the bottom. The number in each position tells us the number of each situation that was observed when comparing our predictions to the actual results.", "So in summary, out of 6,000 test cases, we observed (considering a \u201cpositive\u201d result as being a 7 and a \u201cnegative\u201d one being some other digit):", "You may have noticed that ideally our confusion matrix would be diagonal \u2014 that is, only consisting of true positives and true negatives. The fact that our classifier seems to be struggling more with false positives than false negatives gives us useful information when deciding how we should proceed to improve our model further.", "We can use the information encoded in the confusion matrix to calculate some further useful quantities. Precision is defined as the number of true positives as a proportion of all (true and false) positives. Effectively, this number represents how many of the model\u2019s positive predictions actually turned out to be right.", "However, precision must also be considered in tandem with recall. Recall is defined as the number of true positives as a proportion of both true positives and false negatives. Remember that both true positives and false negatives relate to cases where the digit being considered actually was a 7. So in summary, recall represents how good the model is at correctly identifying positive instances.", "We can also save ourselves from having to calculate these quantities manually by getting them directly from our model:", "We can set a desired level of precision or recall by playing about with the threshold of the model. In the background, our SGD classifier has come up with a decision score for each digit in the data which corresponds to how \u201cseven-y\u201d a digit is. Digits that appear to be very seven-y will have a higher score. Digits that the model doesn\u2019t think look like sevens at all will have a low score. Ambiguous cases (perhaps poorly drawn sevens that look a bit like ones) will be somewhere in the middle. All digits with a decision score above the model\u2019s threshold will be predicted to be 7s, and all those with scores below the threshold will be predicted as not-7s. Hence, if we want to increase our recall (and increase the number of 7s that we successfully identify) we can lower the threshold. By doing this, we are effectively saying to the model, \u201cLower your standards for identifying sevens a bit\u201d. We will then catch more of the ambiguous sevens, the proportion of the actual 7s in the data that we correctly identify will increase, and our recall will go up. In addition to increasing the number of TPs, we will also lower the number of FNs \u2014 7s that were previously mistakenly identified as not-7s will start to be correctly identified.", "You may have noticed, however, that in reducing the threshold, we are likely to now misclassify more not-7s. That one which was drawn to look a bit like a seven may now be predicted incorrectly to be a 7. Hence, we will start to get an increasing number of false positive results and our precision will trend downwards as our recall increases. This is the precision\u2013recall trade-off. You can\u2019t have your cake and eat it too. The best models will be able to strike a good balance between the two so that both precision and recall are at an acceptable level.", "What level of precision and recall constitutes \u201cacceptable\u201d? That depends on how the model is going to be applied. If the consequences of failing to identify a positive instance are severe, for example, if you were a doctor aiming to detect the presence a life-threatening disease, you may be willing to suffer a few more false positives (and send some people who don\u2019t have the disease for some unnecessary further tests) to reduce false negatives (a circumstance under which someone would not receive the life-saving treatment they need). Similarly, if you had a situation in which it was necessary to be very confident in your positive predictions, for example, if you were choosing a property to purchase or picking a location for a new oil well, you may be happy to pass up a few opportunities (ie have your model predict a few more false negatives) if it means that you are less likely to invest time and money as a result of a false positive.", "We can observe how precision and recall vary with the decision threshold (for the calculation of our metrics above, scikit-learn has used a threshold of zero):", "As you can see, precision and recall are two sides of the same coin. Generally (that is, in the absence of a specific reason to seek out more of one at the possible expense of the other) we will want to tune our model so that our decision threshold is set in the region where both precision and recall are high.", "We can also plot precision and recall directly against each other as the decision threshold is varied:", "We\u2019ve also included the point along the curve where our current model (with a decision threshold of zero) lies. In an ideal world, we would have a model that could achieve both 100% precision and 100% recall \u2014 that is, we would want a precision\u2013recall curve that passes through the top right-hand corner of our plot above. Hence, any adjustments that we make to our model that push our curve outwards and towards the upper right can be regarded as improvements.", "For applications where precision and recall are of similar importance, it is often convenient to combine them into a single quantity called the F1 score. The F1 score is defined as being the harmonic mean of the precision and recall scores. It\u2019s a bit harder to intuitively interpret the F1 score than it is for precision and recall individually, but it may be desirable to summarise the two quantities into one, easy-to-compare metric.", "To obtain a high F1 score, a model needs to have both high precision and recall. This is because the F1 score is dragged down quite significantly when taking the harmonic mean if one of precision or recall is low.", "Another common way of assessing (and visualising) model performance is by using the ROC curve. Historically, it has its origins in signal detection theory and was first used in the context of detecting enemy aircraft in World War II, where the ability of a radar receiver operator to detect and make decisions based on incoming signals was referred to as Receiver Operating Characteristic. Although the precise context in which it is generally used today has changed, the name has stuck.", "The ROC curve plots how the True Positive rate and the False Positive rate change as the model threshold is varied. The true positive rate is simply the percentage of positive instances that were correctly identified (ie the number of 7s we correctly predicted). The false positive rate is, correspondingly, the number of negative instances that were incorrectly identified as being positive (ie the number of not-7s that were incorrectly predicted to be 7s). You may have noticed that the definition for the true positive rate is equivalent to that of recall. And you\u2019d be correct \u2014 these are simply different names for the same metric.", "To get a bit of an intuition about what a good and a bad ROC curve would look like (as it\u2019s often a bit tricky to think about what these more abstract quantities actually mean), let\u2019s consider the extreme cases \u2014 the best and worst possible classifiers we could possibly have.", "The worst possible classifier will essentially make a random, 50/50 guess as to whether a given digit is a 7 or not a 7 \u2014 it has made no attempt to learn what distinguishes the two classes. The decision scores for each instance will essentially be randomly distributed. Say we initially set the decision threshold at a very high value so that all instances are classified as negative. We will have identified no positive instances \u2014 true or false \u2014 so both the true positive and false positive rates are zero. As we decrease the decision threshold, we will gradually start classifying equal numbers of positive and negative instances as being positive, so that the true and false positive rates increase at the same pace. This continues until our threshold is at some very low value where we have the reverse of the situation we started with \u2014 correctly classifying all positive instances (so true positive rate is equal to one) but also incorrectly classifying all negative instances (false positive rate also one). Hence, the ROC curve for a random, 50/50 classifier is a diagonal line from (0,0) to (1,1).", "(You may think it\u2019s possible to have an even worse classifier than this \u2014 perhaps a classifier that gets every digit wrong by predicting the opposite every time. But in that case, then the classifier would essentially be a perfect classifier \u2014 to be so wrong, the model would have needed to learn the classification perfectly and then just switch around the outputs before making a prediction!)", "The best possible classifier will be able to correctly predict every given instance as positive or negative with 100% accuracy. The decision scores for all positive instances will be some high value, to represent the fact that the model is supremely confident in its predictions, and equally so for every digit it has been given. All negative instances will have some low decision score, since the model is, again, supremely and equally confident that all these instances are negative. If we start the decision threshold at a very high value so that all instances are classified as negative, we have the same situation as described for the 50/50 classifier \u2014 both true and false positive rates are zero because we have not predicted any positive instances. In the same way, when the decision threshold is very low, we will predict all instances to be positive and so will have a value of one for both true and false positive rates. Hence, the ROC curve will start at (0,0) and end at (1,1), as for the 50/50 classifier. However, when the decision threshold is set at any level between the high and low decision scores that identify positive and negative instances, the classifier will operate perfectly with a 100% true positive rate and a 0% false positive rate. Amazing! The consequences of the above is that the ROC curve for our perfect classifier is the curve that joins up (0,0), (0,1) and (1,1) \u2014 a curve that hugs the upper left-hand corner of the plot.", "Most real-world classifiers will be somewhere between these two extremes. Ideally, we will want our classifier\u2019s ROC curve to be closer to looking like a perfect classifier than one that guesses randomly \u2014 so a ROC curve that lies closer to the upper left-hand corner of the plot (closer to the behaviour of a perfect classifier) represents a superior model.", "Armed with this new knowledge, let\u2019s plot some ROC curves.", "It\u2019s clear that our SGD classifier is doing better than one that guesses randomly! All that work was worth it after all. Instead of relying on the vague statement that \u201ccloser to the upper left corner is better\u201d, we can quantify model performance by calculating the Area Under a model\u2019s ROC Curve \u2014 referred to as the AUC.", "If you remember your formulae for calculating the area of a triangle, you should be able to see that the AUC for the random classifier is 0.5. You should also be able to see that the AUC for the perfect classifier is 1. Hence, a higher AUC is better and we will want to aim for an AUC as close to 1 as possible. Our SGD classifier\u2019s AUC can be obtained as follows:", "In isolation, it\u2019s hard to tell exactly what our AUC of 0.934 means. As with the metrics was saw earlier based on precision and recall, these are standard comparative tools. We should use them purposefully to develop and tune our current model, and to compare our model to other algorithms to seek out the best possible performance at our particular task.", "We covered a lot of ground today, so congratulations if you made it all the way through! When first taking on a problem, it\u2019s easy to become overwhelmed by the sheer range of algorithms, methods and parameters that could be used as part of an approach to solving it. Model performance metrics can act as the compass that guides us through this wilderness \u2014 as long as we break a problem down, decide on an initial approach (we can change later if needed), and use sensible tools like those explained above to evaluate whether we are making progress, we will be moving towards where we need to go. And as heart-breaking as it is to not see hours of research and tinkering with a model reflected in an improvement in a particular performance metric, it\u2019s always better to know whether we are moving forwards or backwards.", "So get stuck in \u2014 and don\u2019t get too invested in a particular method if the numbers show that it\u2019s not working out. Review and compare models and approaches dispassionately. If you can manage that, you\u2019ll be sitting in that Ferrari with the cream upholstery in no time at all.", "Andrew Hetherington is an actuary-in-training and data enthusiast based in London, UK.", "Photos by AbsolutVision, Markus Spiske and Marat Gilyadzinov.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Actuary-in-training and data enthusiast based in London, UK."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6403577c1010&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6403577c1010--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6403577c1010--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andrew-m-hetherington.medium.com/?source=post_page-----6403577c1010--------------------------------", "anchor_text": ""}, {"url": "https://andrew-m-hetherington.medium.com/?source=post_page-----6403577c1010--------------------------------", "anchor_text": "Andrew Hetherington"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1c776b81964&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&user=Andrew+Hetherington&userId=f1c776b81964&source=post_page-f1c776b81964----6403577c1010---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6403577c1010&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6403577c1010&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@freegraphictoday?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "AbsolutVision"}, {"url": "https://unsplash.com/s/photos/classified?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/andrewhetherington/python-projects/blob/master/Blog%E2%80%94Evaluating%20Classifier%20Model%20Performance/Evaluating%20Classifier%20Model%20Performance.ipynb", "anchor_text": "the notebook used to produce this work"}, {"url": "https://machinelearningmastery.com/k-fold-cross-validation/", "anchor_text": "cross-validation"}, {"url": "https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com/s/photos/scope?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@m3design?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Marat Gilyadzinov"}, {"url": "https://unsplash.com/s/photos/radar?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.andrewhetherington.com/", "anchor_text": "website"}, {"url": "https://www.linkedin.com/in/andrewmhetherington/", "anchor_text": "LinkedIn"}, {"url": "https://github.com/andrewhetherington/python-projects", "anchor_text": "GitHub"}, {"url": "https://github.com/andrewhetherington/python-projects/blob/master/Blog%E2%80%94Evaluating%20Classifier%20Model%20Performance/Evaluating%20Classifier%20Model%20Performance.ipynb", "anchor_text": "here"}, {"url": "https://www.amazon.co.uk/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=pd_lpo_14_t_0/257-3076774-3184456?_encoding=UTF8&pd_rd_i=1492032646&pd_rd_r=dfb4d6ef-830b-45b4-a51e-655f75eb07c5&pd_rd_w=2JphL&pd_rd_wg=NXcYK&pf_rd_p=7b8e3b03-1439-4489-abd4-4a138cf4eca6&pf_rd_r=WR7PA4BFNEHM2DCHCMYC&psc=1&refRID=WR7PA4BFNEHM2DCHCMYC", "anchor_text": "A. G\u00e9ron\u2019s 2019 book"}, {"url": "https://unsplash.com/@freegraphictoday?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "AbsolutVision,"}, {"url": "https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com/@m3design?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Marat Gilyadzinov"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6403577c1010---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6403577c1010---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6403577c1010---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----6403577c1010---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----6403577c1010---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6403577c1010&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&user=Andrew+Hetherington&userId=f1c776b81964&source=-----6403577c1010---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6403577c1010&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&user=Andrew+Hetherington&userId=f1c776b81964&source=-----6403577c1010---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6403577c1010&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6403577c1010--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6403577c1010&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6403577c1010---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6403577c1010--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6403577c1010--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6403577c1010--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6403577c1010--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6403577c1010--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6403577c1010--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6403577c1010--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6403577c1010--------------------------------", "anchor_text": ""}, {"url": "https://andrew-m-hetherington.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andrew-m-hetherington.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrew Hetherington"}, {"url": "https://andrew-m-hetherington.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "47 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1c776b81964&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&user=Andrew+Hetherington&userId=f1c776b81964&source=post_page-f1c776b81964--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F464e8b563032&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-classifier-model-performance-6403577c1010&newsletterV3=f1c776b81964&newsletterV3Id=464e8b563032&user=Andrew+Hetherington&userId=f1c776b81964&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}