{"url": "https://towardsdatascience.com/ensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090", "time": 1683012442.707526, "path": "towardsdatascience.com/ensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090/", "webpage": {"metadata": {"title": "Ensembles: the only (almost) free Lunch in Machine Learning | by Dmitry Borisenko | Towards Data Science", "h1": "Ensembles: the only (almost) free Lunch in Machine Learning", "description": "In this post, I cover the somewhat overlooked topic of ensemble optimization. I begin with a brief overview of some common ensemble techniques and outline their weaknesses. I then introduce a simple\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "A notebook accompanying this post can be found here.", "paragraph_index": 0}, {"url": "https://www.linkedin.com/in/tetyana-drobot/", "anchor_text": "Tetyana Drobot", "paragraph_index": 1}, {"url": "https://www.linkedin.com/in/igor-pozdeev/", "anchor_text": "Igor Pozdeev", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/a-guide-to-ensemble-learning-d3686c9bed9a", "anchor_text": "this post", "paragraph_index": 7}, {"url": "https://medium.com/u/b986eefd54ba?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Gilbert Tanner", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "the one", "paragraph_index": 7}, {"url": "https://medium.com/u/b17ebd108358?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Joseph Rocca", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2", "anchor_text": "this post", "paragraph_index": 7}, {"url": "https://medium.com/u/dc327e50cf35?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Juhi Ramzai", "paragraph_index": 7}, {"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "PyTorch implementation", "paragraph_index": 11}, {"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "the accompanying notebook", "paragraph_index": 23}, {"url": "https://medium.com/u/38025716cba8?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Max Pechyonkin", "paragraph_index": 46}, {"url": "https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a", "anchor_text": "an excellent overview", "paragraph_index": 46}, {"url": "http://www.linkedin.com/in/dmitry-borisenko", "anchor_text": "LinkedIn", "paragraph_index": 49}, {"url": "http://www.linkedin.com/in/dmitry-borisenko", "anchor_text": "www.linkedin.com/in/dmitry-borisenko", "paragraph_index": 51}], "all_paragraphs": ["A notebook accompanying this post can be found here.", "I am grateful to Tetyana Drobot and Igor Pozdeev for their comments and suggestions.", "In this post, I cover the somewhat overlooked topic of ensemble optimization. I begin with a brief overview of some common ensemble techniques and outline their weaknesses. I then introduce a simple ensemble optimization algorithm and demonstrate how to apply it to build ensembles of neural networks with Python and PyTorch. Towards the end of the post, I discuss the effectiveness of ensemble methods in deep learning in the context of the current literature on the loss surface geometry of neural networks.", "The post is organized as follows:I. IntroductionII. Ensemble OptimizationIII. Building Ensembles of Neural NetworksIV. Ensembles of Neural Networks: the Role of Loss Surface GeometryV. Conclusion", "An ensemble is a collection of models designed to outperform every single one of them by combining their predictions. Strong ensembles comprise models that are accurate, performing well on their own, yet diverse in the sense of making different mistakes. This resonates with me deeply, as I am a finance professional\u2014ensembling is akin to building a robust portfolio consisting of many individual assets and sacrificing higher expected returns on some of them in favor of an overall reduction in risk by diversifying investments. \u201cDiversification is the only free lunch in finance\u201d is the quote attributed to Harry Markowitz, the father of the Modern Portfolio Theory. Given that ensembling and diversification are conceptually related, and in some problems, the two are mathematically equivalent, I decided to give the post its title.", "Why almost, though? Because there is always a lingering problem of computational cost given how resource hungry the most powerful models (yes, neural networks) are. In addition to that, ensembling can hurt interpretability of more transparent machine learning algorithms like decision trees by blurring the decision boundaries of individual models \u2014 this point does not really apply to neural networks for which the issue of interpretability arises already on the individual model level.", "There are several approaches to building ensembles:", "See, for example, this post by Gilbert Tanner or the one by Joseph Rocca, or this post by Juhi Ramzai for an extensive overview of these methods.", "Of course, the methods above come with some common problems. First, given a set of trained models how to select the ones that are most likely to generalize well? In the case of stacking, this question would read \u2018how to reduce the number of ensemble candidates to a manageable amount so the stacking model can handle them without a large validation set or high risk of overfitting?\u2019 Well, just pick the best performing models and maybe apply weights inversely proportional to their loss, right? Wrong. Though often it is a good starting point. Recall that a good ensemble consists of both accurate and diverse models: pooling several highly accurate models with strongly correlated predictions would typically result in all models stepping on the same rake.", "The second problem is more subtle. Often the machine learning algorithms we train are all but glorified feature extractors, i.e. the objective in a real-life application might differ significantly from the loss function used to train a model. For instance, the cross-entropy loss is a staple in classification tasks in deep learning because of its differentiability and stable numerical behavior during optimization, however, depending on the domain we might be interested in accuracy, F1 score or false negative rate. As a concrete example consider classifying extreme weather events like floods or hurricanes, where the cost of making a Type II error (false negative) could be astronomically high rendering even the accuracy let alone the cross-entropy useless as an evaluation metric. Similarly, in a regression setting, the common loss function is the mean squared error. In finance, for example, it is common to train the same model for every asset in the sample predicting the return over the next period, while in reality there are hundreds of assets in multiple portfolios with optimization objectives similar to the ones encountered in reinforcement learning and optimal control: multiple time horizons along with state and path dependencies. In any case, you are neither judged by nor compensated for low MSE (unless you are in academia).", "In this post I thoroughly discuss the ensemble optimization algorithm of Caruana et al. (2004) which addresses the problems outlined above. The algorithm can be broadly described as model-free greedy stacking, i.e. at every optimization step the algorithm either adds a new model to the ensemble or changes the weights of the current constituents minimizing the total loss without any overarching trainable model guiding the selection process. Equipped with several features allowing it to alleviate the overfitting problem, the Caruana et al. (2004) approach also allows building ensembles optimizing custom metrics that may differ from those used to train individual models, thus addressing the second problem. I further demonstrate how to apply the algorithm: first, to a simple example with a closed form solution and next, to a realistic problem by building an optimal ensemble of neural networks for the MNIST dataset (a complete PyTorch implementation can be found here). Towards the end of the post, I explore the mechanisms underpinning the effectiveness of ensembles in deep learning and discuss the current literature on the role of the loss surface geometry in the generalization properties of neural networks.", "The remainder of the post is structured as follows: Section II presents the ensemble optimization approach of Caruana et al. (2004) and illustrates it with a simple numerical example. In Section III I optimize an ensemble of neural networks for the MNIST dataset (PyTorch implementation). Section IV briefly discusses the literature on the optimization landscape in deep learning and its impact on ensembling. Section V concludes.", "II. Ensemble Optimization: the Caruana et al. (2004) Algorithm", "The approach of Caruana et al. (2004) is rather straightforward. Given a set of trained models and their predictions on a validation set, a variant of their ensemble construction algorithm is as follows:", "This version of the algorithm includes a couple of features designed to prevent overfitting the validation set. First, initializing the ensemble with several well-performing models forms a strong initial ensemble; second, drawing models with replacement practically guarantees that the ensemble loss on the validation set does not increase as the algorithm iterations progress \u2014 if adding another model can not further improve the ensemble loss the algorithm adds copies of the incumbent models essentially adjusting their weights in the final prediction. This weight adjustment property allows thinking of the algorithm as model-free stacking. Another interesting feature of this approach is that loss functions used for ensemble construction and to train individual models are not required to be the same: as mentioned earlier, often we train models with a particular loss function because of its mathematical or computational convenience in the (reasonable) hope that the models will generalize well with a related performance metric which is hard to optimize directly. Indeed, the value of cross-entropy on the test set in a malignant tumor classification task should not be our primary concern, in contrast to, for instance, the false negative rate.", "The following Python function implements the algorithm:", "Consider the following toy example: assume we have 10 models with zero-mean normally distributed uncorrelated predictions. Furthermore, assume that the variance of the predictions decreases linearly from 10 to 1, i.e. the first model has the highest variance and the last model has the lowest. Given a sample of data, the goal is to build an ensemble minimizing the mean squared error against the ground truth of 0. Note, that in the context of the Caruana et al. (2004) algorithm \u2018build an ensemble\u2019 means assigning a weight between 0 and 1 to each model\u2019s predictions such that the weighted prediction minimizes the MSE, subject to the constraint that all weights sum up to 1.", "The finance aficionados would recognize a special case of the minimum variance optimization problem by thinking of the models\u2019 predictions as returns on some assets, and the optimization objective as minimizing portfolio variance. The problem has a closed form solution:", "where w is the vector of model weights, and \u03a3 is the variance-covariance matrix of predictions. In our case the predictions are uncorrelated and the off-diagonal elements of \u03a3 are zero. The following code snippet solves this toy problem by both using the ensemble_selector function and the analytical approach, it also constructs a simple ensemble by averaging the predictions:", "Figure 1 below compares weights implied by the ensemble optimization (in blue) and the closed form solution (in orange). The results match pretty closely, especially given that to compute the analytical solution we use the true variances and not the sample estimates. Note, that although the models with low prediction uncertainty receive higher weights, the weights of the high uncertainty models do not go to zero: the predictions are uncorrelated, and we can always reduce the variance of a weighted sum of random variables by adding an uncorrelated variable (with finite variance, of course).", "The solid blue line on the next figure plots the ensemble loss for the first 25 iterations of the algorithm. The dashed black and red lines represent, respectively, the loss achieved by the best single model and by a simple ensemble that averages the predictions of all models. After approximately five iterations the optimized ensemble beats the naive one achieving significantly lower MSE values thereafter.", "What if the number of models in the pool is very large?", "If the model pool is very large some of the models could overfit the validation set purely by chance. Caruana et al. (2004) suggest using bagging to address this issue. In this case, the algorithm is applied to bags of M models randomly drawn from the pool with replacement with the final predictions being averaged over individual bags. For example, with a probability of 25% for a model to be drawn and 20 bags, the chance that any particular model will not be in any of the bags is only around 0.3%.", "Equipped with the techniques from the previous section, in this one, we will apply them to a realistic task, building and optimizing an ensemble of neural networks on the MNIST dataset. The results of this section can be completely replicated using the accompanying notebook, therefore I am restricting the code snippets in this section to a minimum primarily focusing on ensembling and not on model definitions and training.", "We start with a simple MLP having 3 hidden layers of 100 units each with ReLU activations. Naturally, the input for the MNIST datset is a 28x28 pixels image flattened into a 784-dimensional vector, and the output layer has 10 units corresponding to the number of digits. Therefore, the architecture specified by the MNISTMLP class implemented in PyTorch looks as follows:", "We then train 10 instances of the model with independent weight initializations (i.e. everything is identical except for the starting weights) for 3 epochs each with a batch size of 32 and a learning rate of 0.001, reserving 25% of the training set of 60,000 images for validation, with the final 10,000 images comprising the test set. The objective is to minimize the cross-entropy (equivalently, negative log-likelihood). Note, that only 3 epochs of training together with a rather small capacity of each model would likely result in underfitting the data, thus allowing to demonstrate the benefits of ensembling in a more dramatic fashion.", "After the training is complete we restore the best checkpoints (by validation loss) of each of the 10 models. The left panel on the figure below shows the validation (in blue) and test (in orange) loss for each model named M0 through M9. Similarly, the right panel plots the validation and test accuracy.", "As expected, all models perform rather poorly with the best one, M7, achieving only 96.8% accuracy on the test set.", "To build an optimal ensemble let us first call the ensemble_selector function defined in the previous section and then go over individual arguments in the context of the current problem:", "y_hats_val is a dictionary with the model names as keys and predicted class probabilities for the validation set as items:", "y_true_one_hot_val is a numpy array of the corresponding true one-hot encoded labels:", "The loss_function is a callable mapping arrays of predictions and labels to a scalar:", "Finally, init_size=1 means that we start with an ensemble of a single model; replacement=True means that the models are not removed from the model pool after being added to the ensemble, allowing the algorithm to add the same model several times, thus adjusting the weights of the ensemble constituents; max_iter=10 sets the number of steps the algorithm takes.", "Let us now examine the outputs. model_weights is a pandas dataframe containing the ensemble weight of each model for each optimization step. Dropping all models that have zero weights at each optimization step yields:", "The following figure plots weights of the ensemble constituents as a function of optimization steps with a darker hue corresponding to a higher average weight a model receives during all optimization steps. The ensemble initializes with a single strongest model M7 at step 0, and then progressively adds more models assigning an equal weight to each: at step 1 there are two models M7 and M5 with a 50% weight each, at step 2 the ensemble includes models M7, M5 and M9 each having a weight of one third. After step 4 no new model can further improve ensemble predictions, and the algorithm starts to adjust the weights of its constituents.", "The other output \u2014 ensemble_loss \u2014 contains the loss of the ensemble at each optimization step. Similar to Figure 2 from the previous section, the left panel on the figure below plots the ensemble loss on the validation set (solid blue line) as the optimization progresses. The dashed black and red lines represent, respectively, the validation loss achieved by the best single model and by a simple ensemble which assigns equal weights to all models. The ensemble loss decreases quite rapidly, surpassing the performance of its simple counterpart after a couple of iterations and stabilizing after the algorithm enters the weight adjustment mode, which is hardly surprising given that the model pool is rather small. The right panel reports the results for the test set: at each iteration I use the current ensemble weights to produce predictions and measure loss on the test set. The ensemble generalizes well on the test sample effectively repeating the pattern observed on the validation set.", "The Caruana et al. (2004) algorithm is very flexible and we can easily adapt ensemble_selector to, for instance, directly optimize the accuracy by changing the loss_function argument:", "where accuracy is defined as follows:", "The following figure repeats the analysis in the previous one but this time for the validation and test accuracy. The conclusions are similar, although the accuracy path of the ensemble is more volatile in both samples.", "The short answer \u2014 it is all about the loss surface. The current deep learning research emphasizes the importance of the optimization landscape. For instance, batch normalization (Ioffe and Szegedy (2015)) is traditionally thought to accelerate and regularize training by reducing internal covariate shift \u2014 the change in the distribution of network activations during training. However, Santurkar et al. (2018) provide a compelling argument that the success of the technique stems from another property: batch normalization makes the optimization landscape significantly smoother and thus stabilizes the gradients and speeds up training. In a similar vein, Keskar et al. (2016) argue that sharp minima on the loss surface have poor generalization properties in comparison with minima in flatter regions of the landscape.", "During training a neural network can be viewed as a function mapping parameters to loss values given the training data. The figure below plots a (very) simplified illustration of the networks\u2019 loss landscape: the space of solutions and loss are along the horizontal and vertical axes respectively. Each point on the x-axis represents all weights and biases of the network yielding the corresponding loss (the blue solid line). The red dots show local minima where we are likely to end up using gradient-based optimization (the two leftmost dots are the global minima).", "In the context of ensembling, this means that we would like to explore many local minima. In the previous section we already saw that the combinations of different initializations of the same neural network architecture result in a superior generalization ability. In fact, in their recent paper Fort et al. (2019) demonstrate that random initializations end up in distant optima and therefore are capable of exploring completely different models with similar accuracy and relatively uncorrelated predictions thus forming strong ensemble components. This finding complements the standard intuition of neural networks being the ultimate low bias-high variance algorithms capable of fitting anything with almost surgical precision albeit plagued by their sensitivity to noise, and therefore benefiting from ensembling due to variance reduction.", "But what to do if training several copies of the same model is infeasible?", "Huang et al. (2018) propose building an ensemble during a single training run using cyclical learning rate with annealing and storing a model checkpoint, or snapshot, at the end of each cycle. Intuitively, increasing the learning rate could allow the model to escape any of the local minima on the figure above and land at the neighboring region with a different local minimum eventually converging to it with a subsequent decrease in the learning rate. The next figure illustrates the snapshot ensemble technique. The left plot shows the path over the loss landscape a model traverses during the standard training regime with a constant learning rate and the final stopping point marked with a blue flag. The right plot depicts the path with a cyclical learning rate schedule and periodic snapshots marked with red flags.", "Remember, however, that on the two previous figures the whole parameter space is compressed into a single point on the x-axis and xy-plane respectively, meaning that a pair of neighboring points on the graph might be very far apart in the real parameter space, and therefore, the ability of a gradient descent algorithm to traverse multiple minima without getting stuck depends on whether the corresponding valleys on the loss surface are separated by regions of very high loss such that no meaningful increase in the learning would result in a transition to a new valley.", "Fortunately, Garipov et al. (2018) demonstrate that there exist low-loss paths connecting the local minima on the optimization landscape and propose the fast geometric ensembling (FGE) procedure exploiting these connections. Izmailov et al. (2018) propose a further refinement of FGE \u2014 stochastic weight averaging (SWA).", "Max Pechyonkin provides an excellent overview of snapshot ensembles, FGE, and SWA.", "Let us recap the key takeaways of this post:", "To sum up, ensemble learning techniques should arguably be among the most important tools in the arsenal of every machine learning practitioner. It is indeed quite fascinating how far the old adage of not putting all eggs in one basket goes.", "Thank you for reading. Comments and feedback are eagerly anticipated. Also, connect with me on LinkedIn.", "Caruana, R., Niculescu-Mizil, A., Crew, G., & Ksikes, A. (2004, July). Ensemble selection from libraries of models. In Proceedings of the twenty-first international conference on Machine learning (p. 18).", "Buy-side Quant, PhD in Economics and Finance @HSG www.linkedin.com/in/dmitry-borisenko"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F91af7ebe5090&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@dmitrijborisenko?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dmitrijborisenko?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Dmitry Borisenko"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7c47d176200e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=post_page-7c47d176200e----91af7ebe5090---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F91af7ebe5090&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=-----91af7ebe5090---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91af7ebe5090&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&source=-----91af7ebe5090---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@rihok?utm_source=medium&utm_medium=referral", "anchor_text": "Riho Kroll"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "A notebook accompanying this post can be found here."}, {"url": "https://www.linkedin.com/in/tetyana-drobot/", "anchor_text": "Tetyana Drobot"}, {"url": "https://www.linkedin.com/in/igor-pozdeev/", "anchor_text": "Igor Pozdeev"}, {"url": "https://towardsdatascience.com/a-guide-to-ensemble-learning-d3686c9bed9a", "anchor_text": "this post"}, {"url": "https://medium.com/u/b986eefd54ba?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Gilbert Tanner"}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "the one"}, {"url": "https://medium.com/u/b17ebd108358?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Joseph Rocca"}, {"url": "https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2", "anchor_text": "this post"}, {"url": "https://medium.com/u/dc327e50cf35?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Juhi Ramzai"}, {"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "here"}, {"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "PyTorch implementation"}, {"url": "https://github.com/DmitryBorisenko/ensemble_tutorial", "anchor_text": "the accompanying notebook"}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "Huang et al. (2017)"}, {"url": "https://medium.com/u/38025716cba8?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Max Pechyonkin"}, {"url": "https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a", "anchor_text": "an excellent overview"}, {"url": "http://www.linkedin.com/in/dmitry-borisenko", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a", "anchor_text": "Stochastic Weight Averaging \u2014 a New Way to Get State of the Art Results in Deep LearningUpdate: you can now enjoy this post on my personal blog, where math typography is much better (Medium doesn\u2019t support\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/a-guide-to-ensemble-learning-d3686c9bed9a", "anchor_text": "A guide to Ensemble LearningNobody can know everything but with help, we can overcome every obstacle. That\u2019s exactly the idea behind ensemble\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/abs/1912.02757", "anchor_text": "arXiv:1912.02757"}, {"url": "https://arxiv.org/abs/1802.10026", "anchor_text": "arXiv:1802.10026"}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "arXiv:1704.00109"}, {"url": "https://arxiv.org/abs/1502.03167", "anchor_text": "arXiv:1502.03167"}, {"url": "https://arxiv.org/abs/1803.05407", "anchor_text": "arXiv:1803.05407"}, {"url": "https://arxiv.org/abs/1609.04836", "anchor_text": "arXiv:1609.04836"}, {"url": "https://arxiv.org/abs/1805.11604", "anchor_text": "arXiv:1805.11604"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----91af7ebe5090---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----91af7ebe5090---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/ensemble-learning?source=post_page-----91af7ebe5090---------------ensemble_learning-----------------", "anchor_text": "Ensemble Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----91af7ebe5090---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/optimization?source=post_page-----91af7ebe5090---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F91af7ebe5090&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=-----91af7ebe5090---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F91af7ebe5090&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=-----91af7ebe5090---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91af7ebe5090&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@dmitrijborisenko?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7c47d176200e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=post_page-7c47d176200e----91af7ebe5090---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F7c47d176200e%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=-----91af7ebe5090---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@dmitrijborisenko?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Written by Dmitry Borisenko"}, {"url": "https://medium.com/@dmitrijborisenko/followers?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "240 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://www.linkedin.com/in/dmitry-borisenko", "anchor_text": "www.linkedin.com/in/dmitry-borisenko"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7c47d176200e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=post_page-7c47d176200e----91af7ebe5090---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F7c47d176200e%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensembles-the-almost-free-lunch-in-machine-learning-91af7ebe5090&user=Dmitry+Borisenko&userId=7c47d176200e&source=-----91af7ebe5090---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-and-momentum-investing-2273e8db5b86?source=author_recirc-----91af7ebe5090----0---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://medium.com/@dmitrijborisenko?source=author_recirc-----91af7ebe5090----0---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://medium.com/@dmitrijborisenko?source=author_recirc-----91af7ebe5090----0---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Dmitry Borisenko"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----91af7ebe5090----0---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-learning-and-momentum-investing-2273e8db5b86?source=author_recirc-----91af7ebe5090----0---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Deep Learning and Momentum InvestingDiscover how to apply deep learning models to financial data in a disciplined and intepretable way"}, {"url": "https://towardsdatascience.com/deep-learning-and-momentum-investing-2273e8db5b86?source=author_recirc-----91af7ebe5090----0---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "23 min read\u00b7Aug 20, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2273e8db5b86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-and-momentum-investing-2273e8db5b86&user=Dmitry+Borisenko&userId=7c47d176200e&source=-----2273e8db5b86----0-----------------clap_footer----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-and-momentum-investing-2273e8db5b86?source=author_recirc-----91af7ebe5090----0---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2273e8db5b86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-and-momentum-investing-2273e8db5b86&source=-----91af7ebe5090----0-----------------bookmark_preview----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----91af7ebe5090----1---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----91af7ebe5090----1---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----91af7ebe5090----1---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----91af7ebe5090----1---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----91af7ebe5090----1---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----91af7ebe5090----1---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----91af7ebe5090----1---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----91af7ebe5090----1-----------------bookmark_preview----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----91af7ebe5090----2---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----91af7ebe5090----2---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----91af7ebe5090----2---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----91af7ebe5090----2---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----91af7ebe5090----2---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----91af7ebe5090----2---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----91af7ebe5090----2---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----91af7ebe5090----2-----------------bookmark_preview----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----91af7ebe5090----3---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----91af7ebe5090----3---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----91af7ebe5090----3---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----91af7ebe5090----3---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----91af7ebe5090----3---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----91af7ebe5090----3---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----91af7ebe5090----3---------------------c33016a5_cda2_41fd_9987_6b5d4d969156-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----91af7ebe5090----3-----------------bookmark_preview----c33016a5_cda2_41fd_9987_6b5d4d969156-------", "anchor_text": ""}, {"url": "https://medium.com/@dmitrijborisenko?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "See all from Dmitry Borisenko"}, {"url": "https://towardsdatascience.com/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://rukshanpramoditha.medium.com/?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Rukshan Pramoditha"}, {"url": "https://medium.com/data-science-365?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Data Science 365"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Determining the Right Batch Size for a Neural Network to Get Better and Faster ResultsGuidelines for choosing the right batch size to maintain optimal training speed and accuracy while saving computer resources"}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "\u00b74 min read\u00b7Sep 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-science-365%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a8662830f15----0-----------------clap_footer----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/data-science-365/determining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a8662830f15&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-365%2Fdetermining-the-right-batch-size-for-a-neural-network-to-get-better-and-faster-results-7a8662830f15&source=-----91af7ebe5090----0-----------------bookmark_preview----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/practical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Youssef Hosni"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/practical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Practical Guide to Boosting Algorithms In Machine LearningUse weak learners to create a stronger one"}, {"url": "https://pub.towardsai.net/practical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "\u00b714 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F61c023107e12&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fpractical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12&user=Youssef+Hosni&userId=859af34925b7&source=-----61c023107e12----1-----------------clap_footer----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/practical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F61c023107e12&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fpractical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12&source=-----91af7ebe5090----1-----------------bookmark_preview----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----91af7ebe5090----0---------------------955b15df_aad4_421d_b58e_25266256c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----91af7ebe5090----0-----------------bookmark_preview----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/bagging-vs-boosting-vs-stacking-in-machine-learning-65fe4d1684c0?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Amy @GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo/bagging-vs-boosting-vs-stacking-in-machine-learning-65fe4d1684c0?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Bagging vs Boosting vs Stacking in Machine LearningData Science Interview Questions and Answers"}, {"url": "https://medium.com/grabngoinfo/bagging-vs-boosting-vs-stacking-in-machine-learning-65fe4d1684c0?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "\u00b76 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgrabngoinfo%2F65fe4d1684c0&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fbagging-vs-boosting-vs-stacking-in-machine-learning-65fe4d1684c0&user=Amy+%40GrabNGoInfo&userId=ef6171ffb4ed&source=-----65fe4d1684c0----1-----------------clap_footer----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/bagging-vs-boosting-vs-stacking-in-machine-learning-65fe4d1684c0?source=read_next_recirc-----91af7ebe5090----1---------------------955b15df_aad4_421d_b58e_25266256c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65fe4d1684c0&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fbagging-vs-boosting-vs-stacking-in-machine-learning-65fe4d1684c0&source=-----91af7ebe5090----1-----------------bookmark_preview----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----91af7ebe5090----2---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----91af7ebe5090----2---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----91af7ebe5090----2---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----91af7ebe5090----2---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----91af7ebe5090----2---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----91af7ebe5090----2---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----91af7ebe5090----2---------------------955b15df_aad4_421d_b58e_25266256c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----91af7ebe5090----2-----------------bookmark_preview----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----91af7ebe5090----3---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin?source=read_next_recirc-----91af7ebe5090----3---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin?source=read_next_recirc-----91af7ebe5090----3---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "Nitin Kishore"}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----91af7ebe5090----3---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "How to solve CUDA Out of Memory error**Freeze frame, scratch that record and cue \u2014 \u2018The Who\u2019 intro**"}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----91af7ebe5090----3---------------------955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": "\u00b77 min read\u00b7Nov 2, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F850bb247cfb2&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40snk.nitin%2Fhow-to-solve-cuda-out-of-memory-error-850bb247cfb2&user=Nitin+Kishore&userId=ef6a1cf849e2&source=-----850bb247cfb2----3-----------------clap_footer----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----91af7ebe5090----3---------------------955b15df_aad4_421d_b58e_25266256c7fd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F850bb247cfb2&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40snk.nitin%2Fhow-to-solve-cuda-out-of-memory-error-850bb247cfb2&source=-----91af7ebe5090----3-----------------bookmark_preview----955b15df_aad4_421d_b58e_25266256c7fd-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----91af7ebe5090--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}