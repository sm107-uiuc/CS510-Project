{"url": "https://towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c", "time": 1683003883.952482, "path": "towardsdatascience.com/a-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c/", "webpage": {"metadata": {"title": "A no-frills guide to most Natural Language Processing Models \u2014 The Pre-LSTM Ice-Age \u2014 (R)NNLM, GloVe, Word2Vec & fastText | by Ilias Miraoui | Towards Data Science", "h1": "A no-frills guide to most Natural Language Processing Models \u2014 The Pre-LSTM Ice-Age \u2014 (R)NNLM, GloVe, Word2Vec & fastText", "description": "As I learned more about Natural Language Processing (NLP), I realized that the information about recent models was particularly scattered and hard to reach. I hope to centralize a broad summary of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://tfhub.dev/google/nnlm-en-dim128/2", "anchor_text": "here", "paragraph_index": 5}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "A Neural Probabilistic Language Model", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Efficient Estimation of Word Representations in Vector Space", "paragraph_index": 16}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation", "paragraph_index": 23}, {"url": "https://fasttext.cc/", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1607.01759", "anchor_text": "Bag of Tricks for Efficient Text Classification", "paragraph_index": 30}, {"url": "https://imiraoui.github.io/", "anchor_text": "https://imiraoui.github.io/", "paragraph_index": 34}, {"url": "https://www.linkedin.com/in/iliasmiraoui/", "anchor_text": "https://www.linkedin.com/in/iliasmiraoui/", "paragraph_index": 34}], "all_paragraphs": ["As I learned more about Natural Language Processing (NLP), I realized that the information about recent models was particularly scattered and hard to reach. I hope to centralize a broad summary of the origins, the use-cases and the advantages/disadvantages of each of the different main models. In this post, I summarize the four first main models.", "While this may still contain some jargon unexplained, information about the various concepts should be easily accessible via other posts.", "It is a very early idea and was one of the very first embedding model. The model learns at the same time a representation of each word and the probability function for neighboring word sequences. It is able to \u201cunderstand\u201d the semantics of a sentence. The training was based on Continuous Bags of Words.", "Given that the model takes for input a sentence and outputs an embedding, it could potentially take into account the context. However, the architecture remains simple.", "The original version is not based on Recurrent Neural Networks (RNN) but an alternative was later developed that relied on the latter (neither based on Gated Recurrent Units (GRUs) nor on Long Short Term Memory (LSTM) but really on the \u201cvanilla\u201d RNN). While RNNs are slower and often have trouble keeping information of long-term dependencies, it allowed the NNLM model to overcome some of its limitations such as the need to specify the length of its input or the ability to keep the model at the same size despite a longer input.", "Google has open-sourced a pre-trained embedding model for most languages (The English version is here). The model uses three hidden layers of feed-forward Neural Network and is trained on the English Google News 200B corpus and outputs a 128-dimensional embedding.", "Advantages:- Simplicity: it is quick to train and generate embeddings (it may be enough for most simple applications)- Pre-trained versions are available in plenty of languages", "Disadvantages:- Doesn\u2019t take into account long-term dependencies- Its simplicity may bring limits to its potential use-cases- Newer models embeddings are often a lot more powerful for any task", "Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin, A Neural Probabilistic Language Model (2003), Journal of Machine Learning Research", "Originating from Google, it is generally seen as a turning point in NLP language models.", "For training the model, the widely-adopted version of Word2Vec moves away from NNLM\u2019s Continuous Bags of Words and adopts Skip-gram and negative sampling. Essentially, instead of attempting to predict the next word, the model tries to predict a surrounding word. To complicate the training, a lot of negative examples are given (4:1 often) and the model solves a simple classification task (are both words in the same context?) using a neural network with only one hidden layer.", "Word2Vec surprised everyone by their \u201cinterpretability\u201d (ex: Woman and Man are often separated by a vector very similar to the one distinguishing between King and Queen and could thus be interpreted as the \u201cgender\u201d vector).", "While very influential, Word2Vec embeddings are not really used anymore per se as they have been replaced by their successors.", "A pre-trained model is readily available online and can be imported using the gensim python library.", "Advantages:- Very simple architecture: feed-forward, 1 input, 1 hidden layer, 1 output- Simplicity: it is quick to train and generate embeddings (even your own!)and that may be enough for simple applications- Embeddings \u201chave meaning\u201d: it could allow to decipher bias- The methodology can be extended to plenty of other domains/problems (i.e. lda2vec)", "Disadvantages:- Trained at the word-level: no information on the sentence or the context in which the word is being used- Co-occurrences are ignored meaning the model technically ignores how a word may have a very different meaning depending on the context it is used (the main reason GloVe is generally preferred to Word2Vec)- Does not handle unknown and rare words too well", "Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, Efficient Estimation of Word Representations in Vector Space (2013), International Conference on Learning Representations", "GloVe is often associated very closely with Word2Vec given that they emerged around the same time and rely on some of the same key concepts (i.e. interpretability of the embedding vectors). Nevertheless, they have some important differences.", "In Word2Vec, the frequency of co-occurrence of words doesn\u2019t bear a lot of importance, it just helps generate additional training samples. For GloVe, however, it is a piece of central information that guides the learning.", "GloVe is not trained using neural networks/Skip-Gram/etc. Instead, the model minimizes the difference between the product of word embeddings and the log of the probability of co-occurrence using Stochastic Gradient Descent.", "GloVe embeddings are readily available on its dedicated page on Stanford\u2019s University website (here)", "Advantages:- Very simple architecture: no neural network - Simplicity: it is quick (multiple pre-trained embeddings) and that may be enough for simple applications- GloVe improves on Word2Vec by adding the frequency of words\u2019 co-occurrence and has out-performed Word2Vec on most benchmarks- Embeddings \u201chave meaning\u201d: it could allow to decipher bias", "Disadvantages:- While the co-occurrence matrix provides global information, GloVe remains trained at the word-level and has relatively low information on the sentence or the context in which the word is being used (especially compared to some of the models we will see in a future post)- Does not handle unknown and rare words too well", "Jeffrey Pennington, Richard Socher, and Christopher D. Manning, GloVe: Global Vectors for Word Representation (2014), Empirical Methods in Natural Language Processing", "Initially created at Facebook, fastText extends Word2Vec by treating each word as a composed of \u201ccharacter n-grams\u201d. Essentially, a word vector is the sum of all its n-grams (ex: \u201cthey\u201d could potentially have \u201cth\u201d, \u201che\u201d, \u201cey\u201d, \u201cthe\u201d, \u201chey\u201d depending on the hyper-parameters).", "As a result, the word embeddings tend to be better for less frequent words (given they share some n-grams). The model is thus also able to generate embeddings for unknown words (contrarily to Word2Vec and GloVe) given that it decomposes them by their n-grams.", "fastText performed better than both Word2Vec and GloVe on multiple different benchmarks.", "A pre-trained model for 157 different languages is available here", "Advantages:- Relatively simple architecture: feed-forward, 1 input, 1 hidden layer, 1 output (although n-grams add complexity in generating embeddings)- Embeddings \u201chave meaning\u201d: it could allow to decipher bias- The embedding performs much better than GloVe and Word2Vec on rare and out-of-vocabulary words thanks to its n-grams method", "Disadvantages:- Trained at the word-level: no information on the sentence or the context in which the word is being used - Co-occurrences are ignored meaning the model technically ignores how a word may have a very different meaning depending on the context it is used (the main reason GloVe could be preferred)", "Armand Joulin, Edouard Grave, Piotr Bojanowski and Tomas Mikolov, Bag of Tricks for Efficient Text Classification (2016), European Chapter of the Association for Computational Linguistics", "As you can see, all four models share a lot of similarities but each of them should be used in different contexts. Unfortunately, this is often ignored. I hope that this guide will make your model decision more informed and lead you to better outcomes.", "PS: I am currently a Master of Engineering Student at Berkeley and I am still learning about all of this. If there is anything that stands to be corrected or that is not clear, please let me know. You can also email me here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Enthusiast | Master of Engineering Student @ UC Berkeley | https://imiraoui.github.io/ | https://www.linkedin.com/in/iliasmiraoui/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F86055dd5d67c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "Ilias Miraoui"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde0de51bac0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&user=Ilias+Miraoui&userId=de0de51bac0a&source=post_page-de0de51bac0a----86055dd5d67c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://tfhub.dev/google/nnlm-en-dim128/2", "anchor_text": "here"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "A Neural Probabilistic Language Model"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Efficient Estimation of Word Representations in Vector Space"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation"}, {"url": "https://fasttext.cc/", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1607.01759", "anchor_text": "Bag of Tricks for Efficient Text Classification"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----86055dd5d67c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----86055dd5d67c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----86055dd5d67c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----86055dd5d67c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----86055dd5d67c---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----86055dd5d67c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&user=Ilias+Miraoui&userId=de0de51bac0a&source=-----86055dd5d67c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F86055dd5d67c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----86055dd5d67c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----86055dd5d67c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----86055dd5d67c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----86055dd5d67c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ilias.miraoui?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ilias Miraoui"}, {"url": "https://medium.com/@ilias.miraoui/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "105 Followers"}, {"url": "https://imiraoui.github.io/", "anchor_text": "https://imiraoui.github.io/"}, {"url": "https://www.linkedin.com/in/iliasmiraoui/", "anchor_text": "https://www.linkedin.com/in/iliasmiraoui/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde0de51bac0a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&user=Ilias+Miraoui&userId=de0de51bac0a&source=post_page-de0de51bac0a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6c78c71c0a4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-no-frills-guide-to-most-natural-language-processing-models-part-1-the-pre-lstm-ice-age-86055dd5d67c&newsletterV3=de0de51bac0a&newsletterV3Id=6c78c71c0a4d&user=Ilias+Miraoui&userId=de0de51bac0a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}