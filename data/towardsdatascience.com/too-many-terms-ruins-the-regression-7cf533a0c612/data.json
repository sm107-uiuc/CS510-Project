{"url": "https://towardsdatascience.com/too-many-terms-ruins-the-regression-7cf533a0c612", "time": 1683014729.929874, "path": "towardsdatascience.com/too-many-terms-ruins-the-regression-7cf533a0c612/", "webpage": {"metadata": {"title": "Too Many Terms Ruins the Regression | by Conor O'Sullivan | Towards Data Science", "h1": "Too Many Terms Ruins the Regression", "description": "By adding powers of existing features, polynomial regression can help you get the most out of your dataset. It allows us to model non-linear relationships even with simple models, like Linear\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/conorosully/medium-articles/blob/master/README.md", "anchor_text": "GitHub", "paragraph_index": 1}, {"url": "https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set", "anchor_text": "real estate valuation data set", "paragraph_index": 2}, {"url": "http://www.flaticon.com/", "anchor_text": "www.flaticon.com", "paragraph_index": 27}, {"url": "https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-", "anchor_text": "Premium Plan", "paragraph_index": 27}, {"url": "https://elitedatascience.com/overfitting-in-machine-learning", "anchor_text": "https://elitedatascience.com/overfitting-in-machine-learning", "paragraph_index": 28}], "all_paragraphs": ["By adding powers of existing features, polynomial regression can help you get the most out of your dataset. It allows us to model non-linear relationships even with simple models, like Linear Regression. This can improve the accuracy of your models but, if used incorrectly, overfitting can occur. We want to avoid this as it would leave you with a model that does not perform well in the future.", "In this article, we\u2019ll explain the concept of polynomial regression and show how it can lead to overfitting. We\u2019ll also discuss some techniques you can use to avoid overfitting. These include using k-fold cross-validation or a hold-out set but, most importantly, we\u2019ll discuss how applying domain knowledge will help you avoid overfitting. We won\u2019t discuss any code by you can find the full project on GitHub.", "Let\u2019s dive straight into the concept by fitting some Linear Regression models to a dataset. We will be using a real estate valuation data set which contains information on 414 houses sold. To keep things simple, we\u2019ll only be looking at two variables \u2014 house price of unit area and the age of the house. We can see the relationship between these two variables in Figure 1 below. The idea is to use the age of the house to try to predict the price.", "Looking at Figure 1, there doesn\u2019t seem to be a linear relationship between the two variables. This means that the relationship cannot be represented by a straight line. In our case, house price initially tends to decrease as age increases. However, after about 25 years, it starts to increase with age. This suggests the relationship may be quadratic. Before we get into the modelling, let\u2019s discuss why this could be. We\u2019ll see later that, when it comes to polynomial regression, having good reasons/justifications for the relationships in your data is important.", "The initial behaviour makes sense. As houses age, they will become increasingly worn and lose value. An expert in property would have a better understanding but perhaps, after a longer period of time, houses become antique. Their age would add value as people begin to value their long history. There could also be some selection bias that explains the upwards turn. That is, expensive houses tend not to be demolished so that the only old houses sold are these expensive houses.", "Whatever the reason, as the relationship is not linear, we wouldn\u2019t expect a standard linear model to do a good job. Let\u2019s show this by trying to use linear regression to model house price using only age. We do this by:", "Following this process, we will end up with a model represented by the equation:", "Where \u03b2\u2081 and \u03b2\u2080 are the parameters estimated by the model. This equation can also be called the model\u2019s prediction line. It gives the predicted house price for a given age.", "In Figure 2, we can see the result of using this model on our dataset. Here, the red line gives the prediction line. Looking at this line, we see the model does a bad job of capturing the underlying quadratic trend. We can summarise the accuracy of this model using the test set MSE, which was 145.91.", "Now let\u2019s try to improve our model using polynomial regression. Ultimately, as the relationship appears to be quadratic, we would expect the following equation to do a better job:", "The problem is, if we use linear regression with our current dataset, it is not possible to get such an equation. To get around this we can simply add a new variable to our dataset, age\u00b2. To avoid confusion let\u2019s relabel it age_squared. Adding this feature, allows us to rewrite our non-linear equation as a linear equation:", "We now have a linear function of two variables (i.e. age_squared & age) which is actually a non-linear function of one variable (i.e age) in disguise. This allows us to use linear regression as before to estimate the \u03b2 parameters. We can see the resulting prediction line in Figure 3. In this case, the test MSE is 127.42 which is 13% lower than our previous model. So this new model is doing a better job of predicting house price.", "By using this new feature, age\u00b2, we are doing polynomial regression. To generalise, you do polynomial regression whenever you use an nth degree polynomial to model the relationship between a target and feature. Such as:", "By adding in these features, we can model more complicated relationships in our dataset. In our above model, we have n =2 but we could potentially get better results by using a higher degree polynomial. That being said, by adding more features we could also end up overfitting to our data.", "We say a model is overfitted when it fits too closely to the training dataset. The model captures noise in the data and not just the underlying trends. The consequence of this is that the model may perform well on a training dataset but not as well on a testing dataset. In fact, we wouldn\u2019t expect the model to perform well on any dataset that it was not trained on.", "Overfitting generally occurs because the model is too complicated or has too many features. As you add more features, the more likely you are to overfit. The same can happen as you increase the degree in polynomial regression. In Figure 3, you can see what we mean by this. We follow the same modelling process but, with each step, we increase the degree of our polynomial. We start with n=1 and end with n=25. Notice how the prediction line becomes more distorted as you increase the degree.", "What is happening is that as you increase the degree you allow the model to have more turning points. A polynomial of degree 2 has 1 turning point, degree 3 has 2 turning points and so on\u2026 With each extra turning point, we are giving the model more freedom to fit closer to the training dataset. For higher degrees, the model if probably just capturing noise. It is unlikely that the true underlying trend is that complicated.", "Another way to visualise this is by looking at the MSE on the training and test set. See in Figure 4, the train MSE tends to decreases as you increase the degree. This means the model is becoming more and more accurate on the training set. The test MSE tells a different story. The test MSE is a minimum when n=2 and then tends to increase after that. This means the model is performing worse and worse on the test set. In other words, as we increase the degree the model is becoming more overfitted.", "So we\u2019ve seen how polynomial regression can go wrong. Now, the question is how do we choose the correct degree and avoid overfitting. Like any machine learning model, we want to train a model with a combination of features that performs well on the test set as well as the training set. In this way, the process for choosing the best features for polynomial regression is no different from any other machine learning problem.", "A common approach is to use a hold-out set. For this approach, the dataset is split into a training and hold-out/ test set. For different sets of features, the model is trained on the training set and evaluated on the test set. Where we evaluate the performance using some metric such as MSE. We would generally choose the set of features that performed the best on the test set.", "Looking at the work done above, we could apply this method to our house price example. In this case, the model with n=2 had the minimum test MSE. This means we would only use age\u00b2 and age in our final model. This seems like a reasonable result as the true underlying trend does appear to be quadratic. We also came up with some logical reasons to support this result.", "A similar approach is to use k-fold cross-validation. Here, we partition the dataset into k subsets\\folds of equal size. We then train on k-1 folds and calculated the MSE on the remaining fold. We repeat this step k times so that every fold takes a turn as being the test set. The model's final score would be the average of the MSE across all the test folds. The model with the lowest average MSE would be selected as the final model. Figure 5 shows an example of how the dataset would be divided using 5-fold cross-validation. In this case, we would calculate the average MSE on the 5 test folds.", "Using a hold-out set and k-fold cross-validation can often give you a good model. But data is messy and statistical anomalies can occur. By just blindly using these methods without thinking about your problem you still end up with a bad model by chance. So far, we have performed the above analysis using one specific train-test split. In this case, we concluded that the model with age\u00b2 was best. But what if we used a different random train-test split?", "You can see what we mean by this in Figure 6. Here we follow the exact same process as before except, this time, we use a different random train-test split. In this case, the test MSE was lowest with n = 8. By just using the hold-out method, we would use this as our final model.", "At this point, you should ask yourself if it makes logical sense to use a polynomial of degree 8. Our prediction line would take on the form:", "Is the relationship between price and age really that complicated or are the above results just a statistical anomaly? The answer is probably the latter which emphasises why we should not just rely on methods like k-fold cross-validation.", "It is important to think about your problem and apply any domain knowledge when selecting features. In our house price example, the model with n = 2 seemed to capture the underlying trend. We also came up with some good potential reasons for this relationship. With this in mind, this model is probably better. In general, if the features you include have a logical reason for why they would be predictive you are less likely to capture noise and overfit.", "Images are my own or obtain from www.flaticon.com. In the case of the latter, I have a \u201cFull license\u201d as defined under their Premium Plan.", "[2] Overfitting in Machine Learning: What It Is and How to Prevent It (2020), https://elitedatascience.com/overfitting-in-machine-learning", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Writer | Houseplant Addict | I write about IML, XAI, Algorithm Fairness and Data Exploration | New article (nearly) every week!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7cf533a0c612&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://conorosullyds.medium.com/?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": ""}, {"url": "https://conorosullyds.medium.com/?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "Conor O'Sullivan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ae48256fb37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=post_page-4ae48256fb37----7cf533a0c612---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7cf533a0c612&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7cf533a0c612&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/conorosully/medium-articles/blob/master/README.md", "anchor_text": "GitHub"}, {"url": "https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set", "anchor_text": "real estate valuation data set"}, {"url": "https://www.flaticon.com/premium-icon/teacher_3300897", "anchor_text": "flaticon"}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error#:~:text=In%20statistics%2C%20the%20mean%20squared,values%20and%20the%20actual%20value.", "anchor_text": "MSE."}, {"url": "http://www.flaticon.com/", "anchor_text": "www.flaticon.com"}, {"url": "https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-", "anchor_text": "Premium Plan"}, {"url": "https://statisticsbyjim.com/regression/overfitting-regression-models/", "anchor_text": "https://statisticsbyjim.com/regression/overfitting-regression-models/"}, {"url": "https://elitedatascience.com/overfitting-in-machine-learning", "anchor_text": "https://elitedatascience.com/overfitting-in-machine-learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7cf533a0c612---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7cf533a0c612---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/polynomial-regression?source=post_page-----7cf533a0c612---------------polynomial_regression-----------------", "anchor_text": "Polynomial Regression"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----7cf533a0c612---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7cf533a0c612---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7cf533a0c612&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=-----7cf533a0c612---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7cf533a0c612&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=-----7cf533a0c612---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7cf533a0c612&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7cf533a0c612&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7cf533a0c612---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7cf533a0c612--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7cf533a0c612--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7cf533a0c612--------------------------------", "anchor_text": ""}, {"url": "https://conorosullyds.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://conorosullyds.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Conor O'Sullivan"}, {"url": "https://conorosullyds.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ae48256fb37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=post_page-4ae48256fb37--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F93bef42da4fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-terms-ruins-the-regression-7cf533a0c612&newsletterV3=4ae48256fb37&newsletterV3Id=93bef42da4fd&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}