{"url": "https://towardsdatascience.com/swish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb", "time": 1683004358.5339, "path": "towardsdatascience.com/swish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb/", "webpage": {"metadata": {"title": "Swish: Booting ReLU from the Activation Function Throne | by Andre Ye | Towards Data Science", "h1": "Swish: Booting ReLU from the Activation Function Throne", "description": "Activation functions have long been a focus of interest in neural networks \u2014 they generalize the inputs repeatedly and are integral to the success of a neural network. ReLU has been defaulted as the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1710.05941v1.pdf", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership", "paragraph_index": 27}], "all_paragraphs": ["Activation functions have long been a focus of interest in neural networks \u2014 they generalize the inputs repeatedly and are integral to the success of a neural network. ReLU has been defaulted as the best activation function in the deep learning community for a long time, but there\u2019s a new activation function \u2014 Swish \u2014 that\u2019s here to take the throne.", "Activation functions have a long history. First, the sigmoid function was chosen for its easy derivative, range between 0 and 1, and smooth probabilistic shape. The tanh function was also considered as being an alternative to the sigmoid function, fitted on a scale between -1 and 1, but these classical activation functions have been replaced with ReLU. The Rectified Linear Unit (ReLU) is currently the most popular activation function because the gradient can flow when the input to the ReLU function is positive. Its simplicity and effectiveness has pushed ReLU and branching methods like Leaky ReLU and Parametrized ReLU past the sigmoid and tanh units.", "Prajit Ramachandran, Barret Zoph, and Quoc V. Le propose a new activation function in their paper (link at bottom), which they call Swish. Swish is simple \u2014 it\u2019s x times the sigmoid function.", "Research by the authors of the papers shows that simply be substituting ReLU units with Swish units improves the best classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2.", "Formally stated, the Swish activation function is\u2026", "Like ReLU, Swish is bounded below (meaning as x approaches negative infinity, y approaches some constant value) but unbounded above (meaning as x approaches positive infinity, y approaches infinity). However, unlike ReLU, Swish is smooth (it does not have sudden changes of motion or a vertex):", "Additionally, Swish is non-monotonic, meaning that there is not always a singularly and continually positive (or negative) derivative throughout the entire function. (Restated, the Swish function has a negative derivative at certain points and a positive derivative at other points, instead of only a positive derivative at all points, like Softplus or Sigmoid.", "The derivative of the Swish function is\u2026", "The first and second derivatives of Swish, plotted:", "For inputs less than about 1.25, the derivative has a magnitude of less than 1.", "Unboundedness is desirable for activation functions because it avoids a slow training time during near-zero gradients \u2014 functions like sigmoid or tanh are bounded above and below, so the network needs to be carefully initialized to stay within the limitations of these functions.", "The ReLU function is an improvement over tanh because it is bounded above \u2014 this property is so important that every successful activation function after ReLU is unbounded above.", "Being bounded below may be advantageous because of strong regularization \u2014 functions that approach zero in a limit to negative infinity are great at regularization because large negative inputs are discarded. This is important at the beginning of training when large negative activation inputs are common.", "These bounds are satisfied by Softplus, ReLU, and Swish, but Swish\u2019s non-monotonicity increases \u2018expressivity\u2019 of an input and improves gradient flow.", "Additionally, smoothness helps optimize and generalize the neural network. In the output landscapes below, it is obvious that ReLU\u2019s output landscape is sharp and jarring because of its non-smooth nature, whereas the Swish network landscape is much smoother.", "The output landscape smoothness directly correlates with the error landscape. A smoother error space is easier to traverse and find a minima \u2014 consider it like walking in the jarring range of altitudes of the Himalayan range versus walking in the smooth, rolling hills of the English countryside.", "The authors of the Swish paper compare Swish to the following other activation functions:", "The below table demonstrates how many times Swish performed better, equal, or worse than the outlined baseline activation functions at 9 experiments.", "The authors find that by substituting the ReLU units for Swish units, there is significant improvement over ReLU as the number of layers increases from 42 (when optimization becomes more difficult).", "The authors also found that Swish outperforms ReLU with diverse sizes of batches.", "The authors tested Swish against the following baseline activation functions with the following results:", "Implementing Swish is very simple \u2014 most deep learning libraries should support Swish\u2026", "\u2026or can be represented as x multiplied by the sigmoid function", "Next time you are training a deep neural network, give Swish a try!", "Thanks for reading! If you enjoyed, feel free to check out some of my other work as well.", "The Swish paper can be found here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML enthusiast. Join Medium through my referral link: https://andre-ye.medium.com/membership."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F78f87e5ab6eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andre-ye.medium.com/?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----78f87e5ab6eb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78f87e5ab6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78f87e5ab6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.buzzle.com/images/drawings/ninja-costume/martial-arts-kick.jpg", "anchor_text": "Source"}, {"url": "http://csci431.artifice.cc/images/relu.png", "anchor_text": "Source"}, {"url": "https://www.awesomestories.com/images/user/4ae0e8b631.jpg", "anchor_text": "Left image source"}, {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/2015_Swaledale_from_Kisdon_Hill.jpg/1200px-2015_Swaledale_from_Kisdon_Hill.jpg", "anchor_text": "right image source"}, {"url": "https://arxiv.org/pdf/1710.05941v1.pdf", "anchor_text": "here"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----78f87e5ab6eb---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----78f87e5ab6eb---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/activation-functions?source=post_page-----78f87e5ab6eb---------------activation_functions-----------------", "anchor_text": "Activation Functions"}, {"url": "https://medium.com/tag/math?source=post_page-----78f87e5ab6eb---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/tag/ai?source=post_page-----78f87e5ab6eb---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78f87e5ab6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&user=Andre+Ye&userId=be743a65b006&source=-----78f87e5ab6eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78f87e5ab6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&user=Andre+Ye&userId=be743a65b006&source=-----78f87e5ab6eb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78f87e5ab6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F78f87e5ab6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----78f87e5ab6eb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----78f87e5ab6eb--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://andre-ye.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.8K Followers"}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff44a966e4ff1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fswish-booting-relu-from-the-activation-function-throne-78f87e5ab6eb&newsletterV3=be743a65b006&newsletterV3Id=f44a966e4ff1&user=Andre+Ye&userId=be743a65b006&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}