{"url": "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768", "time": 1682994483.955678, "path": "towardsdatascience.com/scraping-reddit-data-1c0af3040768/", "webpage": {"metadata": {"title": "Scraping Reddit data. How to scrape data from Reddit using\u2026 | by Gilbert Tanner | Towards Data Science", "h1": "Scraping Reddit data", "description": "As its name suggests PRAW is a Python wrapper for the Reddit API, which enables you to scrape data from subreddits, create a bot and much more. In this article, we will learn how to use PRAW to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.reddit.com/prefs/apps", "anchor_text": "this page", "paragraph_index": 5}, {"url": "https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application", "anchor_text": "PRAW documentation", "paragraph_index": 6}, {"url": "https://praw.readthedocs.io/en/latest/tutorials/comments.html", "anchor_text": "comment documentation", "paragraph_index": 15}, {"url": "https://praw.readthedocs.io/en/latest/index.html", "anchor_text": "excellent documentation", "paragraph_index": 22}, {"url": "https://www.youtube.com/channel/UCBOKpYBjPe2kD8FSvGRhJwA", "anchor_text": "Youtube Channel", "paragraph_index": 23}, {"url": "https://github.com/TannerGilbert/Tutorials/tree/master/Reddit%20Webscraping%20using%20PRAW", "anchor_text": "Github Repository", "paragraph_index": 24}, {"url": "https://twitter.com/Tanner__Gilbert", "anchor_text": "Twitter", "paragraph_index": 25}], "all_paragraphs": ["As its name suggests PRAW is a Python wrapper for the Reddit API, which enables you to scrape data from subreddits, create a bot and much more.", "In this article, we will learn how to use PRAW to scrape posts from different subreddits as well as how to get comments from a specific post.", "PRAW can be installed using pip or conda:", "Now PRAW can be imported by writting:", "Before it can be used to scrape data we need to authenticate ourselves. For this we need to create a Reddit instance and provide it with a client_id , client_secret and a user_agent .", "To get the authentication information we need to create a reddit app by navigating to this page and clicking create app or create another app.", "This will open a form where you need to fill in a name, description and redirect uri. For the redirect uri you should choose http://localhost:8080 as described in the excellent PRAW documentation.", "After pressing create app a new application will appear. Here you can find the authentication information needed to create the praw.Reddit instance.", "Now that we have a praw.Reddit instance we can access all available functions and use it, to for example get the 10 \u201chottest\u201d posts from the Machine Learning subreddit.", "We can also get the 10 \u201chottest\u201d posts of all subreddits combined by specifying \u201call\u201d as the subreddit name.", "This variable can be iterated over and features including the post title, id and url can be extracted and saved into an .csv file.", "General information about the subreddit can be obtained using the .description function on the subreddit object.", "You can get the comments for a post/submission by creating/obtaining a Submission object and looping through the comments attribute. To get a post/submission we can either iterate through the submissions of a subreddit or specify a specific submission using reddit.submission and passing it the submission url or id.", "To get the top-level comments we only need to iterate over submission.comments .", "This will work for some submission, but for others that have more comments this code will throw an AttributeError saying:", "These MoreComments object represent the \u201cload more comments\u201d and \u201ccontinue this thread\u201d links encountered on the websites, as described in more detail in the comment documentation.", "There get rid of the MoreComments objects, we can check the datatype of each comment before printing the body.", "But Praw already provides a method called replace_more , which replaces or removes the MoreComments . The method takes an argument called limit, which when set to 0 will remove all MoreComments.", "Both of the above code blocks successfully iterate over all the top-level comments and print their body. The output can be seen below.", "However, the comment section can be arbitrarily deep and most of the time we surely also want to get the comments of the comments. CommentForest provides the .list method, which can be used for getting all comments inside the comment section.", "The above code will first of output all the top-level comments, followed by the second-level comments and so on until there are no comments left.", "Praw is a Python wrapper for the Reddit API, which enables us to use the Reddit API with a clean Python interface. The API can be used for webscraping, creating a bot as well as many others.", "This article covered authentication, getting posts from a subreddit and getting comments. To learn more about the API I suggest to take a look at their excellent documentation.", "If you liked this article consider subscribing on my Youtube Channel and following me on social media.", "The code covered in this article is available as a Github Repository.", "If you have any questions, recommendations or critiques, I can be reached via Twitter or the comment section.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1c0af3040768&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1c0af3040768--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1c0af3040768--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gilberttanner?source=post_page-----1c0af3040768--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gilberttanner?source=post_page-----1c0af3040768--------------------------------", "anchor_text": "Gilbert Tanner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb986eefd54ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&user=Gilbert+Tanner&userId=b986eefd54ba&source=post_page-b986eefd54ba----1c0af3040768---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c0af3040768&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c0af3040768&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@grohsfabian?utm_source=medium&utm_medium=referral", "anchor_text": "Fabian Grohs"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.reddit.com/prefs/apps", "anchor_text": "this page"}, {"url": "https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application", "anchor_text": "PRAW documentation"}, {"url": "https://praw.readthedocs.io/en/latest/tutorials/comments.html", "anchor_text": "comment documentation"}, {"url": "https://towardsdatascience.com/web-scraping-using-selenium-and-beautifulsoup-99195cd70a58", "anchor_text": "Web Scraping using Selenium and BeautifulSoupHow to use Selenium to navigate between pages and use it to scrap HTML loaded with JavaScript.towardsdatascience.com"}, {"url": "https://praw.readthedocs.io/en/latest/index.html", "anchor_text": "excellent documentation"}, {"url": "https://www.youtube.com/channel/UCBOKpYBjPe2kD8FSvGRhJwA", "anchor_text": "Youtube Channel"}, {"url": "https://github.com/TannerGilbert/Tutorials/tree/master/Reddit%20Webscraping%20using%20PRAW", "anchor_text": "Github Repository"}, {"url": "https://twitter.com/Tanner__Gilbert", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/reddit?source=post_page-----1c0af3040768---------------reddit-----------------", "anchor_text": "Reddit"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----1c0af3040768---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/api?source=post_page-----1c0af3040768---------------api-----------------", "anchor_text": "API"}, {"url": "https://medium.com/tag/python?source=post_page-----1c0af3040768---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1c0af3040768---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c0af3040768&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&user=Gilbert+Tanner&userId=b986eefd54ba&source=-----1c0af3040768---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c0af3040768&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&user=Gilbert+Tanner&userId=b986eefd54ba&source=-----1c0af3040768---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c0af3040768&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1c0af3040768--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1c0af3040768&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1c0af3040768---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1c0af3040768--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1c0af3040768--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1c0af3040768--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1c0af3040768--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1c0af3040768--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1c0af3040768--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1c0af3040768--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1c0af3040768--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gilberttanner?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gilberttanner?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gilbert Tanner"}, {"url": "https://medium.com/@gilberttanner/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "https://gilberttanner.com/", "anchor_text": "https://gilberttanner.com/"}, {"url": "https://www.youtube.com/c/GilbertTanner", "anchor_text": "https://www.youtube.com/c/GilbertTanner"}, {"url": "http://buymeacoff.ee/gilberttanner", "anchor_text": "buymeacoff.ee/gilberttanner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb986eefd54ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&user=Gilbert+Tanner&userId=b986eefd54ba&source=post_page-b986eefd54ba--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa3ff29165918&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscraping-reddit-data-1c0af3040768&newsletterV3=b986eefd54ba&newsletterV3Id=a3ff29165918&user=Gilbert+Tanner&userId=b986eefd54ba&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}