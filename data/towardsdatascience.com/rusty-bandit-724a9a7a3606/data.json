{"url": "https://towardsdatascience.com/rusty-bandit-724a9a7a3606", "time": 1683011457.6819708, "path": "towardsdatascience.com/rusty-bandit-724a9a7a3606/", "webpage": {"metadata": {"title": "Rusty Bandit. Building a Multi-armed Bandit with Rust | by Daniel Morton | Towards Data Science", "h1": "Rusty Bandit", "description": "Why not use Python like every other data scientist? Python is fine for front end work, but all of the powerful data science algorithms use something faster on the backend. Why not C++? The same\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/DanielMorton/ratel", "anchor_text": "Ratel", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Honey_badger", "anchor_text": "honey badger", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Law_of_large_numbers", "anchor_text": "law of large numbers", "paragraph_index": 12}, {"url": "https://doc.rust-lang.org/book/ch05-01-defining-structs.html", "anchor_text": "structs", "paragraph_index": 17}, {"url": "https://doc.rust-lang.org/book/ch10-02-traits.html", "anchor_text": "traits", "paragraph_index": 17}, {"url": "http://www.cplusplus.com/doc/tutorial/structures/", "anchor_text": "C++ structs", "paragraph_index": 17}, {"url": "https://www.javatpoint.com/interface-in-java", "anchor_text": "Java interfaces", "paragraph_index": 17}], "all_paragraphs": ["Why not use Python like every other data scientist? Python is fine for front end work, but all of the powerful data science algorithms use something faster on the backend. Why not C++? The same reasons a lot of people avoid C++; it\u2019s too easy to do something seriously unsafe. Why not Java or Scala? Nothing on the JVM wins any speed contests.", "But why Rust? Because it\u2019s almost as fast as C++ and lacks many of the drawbacks. Rust is almost paranoid about memory and thread safety. Mutability has to be specified explicitly, which encourages programmers to use it sparingly. A variable can have at most one mutable reference at any time, which the compiler checks zealously. Threads are examined in similar detail. The end result is a language which makes it difficult to do something stupid (at least with an IDE) and where almost all mistakes are caught at compile time. As an added bonus, the compiler\u2019s error messages are, a feature almost unique in the programming world, helpful.", "But Rust isn\u2019t a data science language, and isn\u2019t intended to be. Neither is C++, which is happily propping up the back-ends of Tensorflow, XGBoost, PyTorch, and LightGBM. Rust could have a similar future, running all the messy matrix calculations hidden by Keras\u2019s elegant syntax.", "Aside from that, it\u2019s there. Learning Rust is a challenge, which is a good enough reason for anything.", "With the preliminaries concluded, the remainder of this article will consist of three parts. The first will be a quick introduction to the multi-armed bandit problem. The second will describe how I designed and coded Ratel, a Rust program for performing multi-armed bandit experiments. The third will discuss the output of one of these experiments.", "(A note on the program name. Multi-armed bandit learning involves a combination of exploration (making new choices) and exploitation (making the same, informed, choice). I couldn\u2019t think of an animal better at exploration or exploitation than the honey badger, or in Afrikaans the ratel.", "One-armed bandit is an old slang term for a slot machine. With a one-armed bandit, the user pulls the lever, or arm, in the hopes of a reward, all the while letting the machine steal a bit of their own money for the privilege. A multi-armed bandit has several ams that can be pulled, each one producing a reward according to some probability distribution. The good news is that, in most applications, multi-armed bandits give better odds than Las Vegas. The bad news is that the reward distribution for each arm is secret, and is likely to remain so.", "The goal of multi-arm bandit learning is not to find the distribution of each arm, merely the arm with the highest average reward. Broadly speaking, there\u2019s exactly one way to do this; pull all the arms a lot of times until you have a reasonably accurate measure of each of their averages. That\u2019s not exactly an algorithm; the algorithm determines how the arms are chosen. We have a few options.", "An initial guess is made for each arm. This is usually on the high end of the known range of possible values. An initial arm is selected at random, and the guess for that arm is updated based on the output. Subsequent guesses take the arm with the highest average return to date, with any ties broken randomly.", "This isn\u2019t a bad strategy, but it does run the risk of picking a sub-optimal arm if the first few guesses are unlucky. I\u2019ll be able to quantify this risk in part three; for now an example will suffice.", "Suppose there are two arms, one of which pays out 1 25% of the time and another that pays out 1 75% of the time. The rest of the time they pay out 0. Initial guesses for the average payout for each arm are both 1. By chance, the first four pulls of the second arm pay out 0, while the first four pulls of the first arm pay out 0 three time and 1 once. Our estimate for the average return of the second arm, 0.2, is now below the real average return for the first arm, 0.25. Following a greedy strategy, I\u2019ll keep pulling arm 1 with little chance of correctly my initial error.", "This may seem like an extreme case; the chances of four 0s in a row from arm two is less than 1%. In real life the spread between the returns of the bandit arms is usually much smaller, and the chances of the greedy algorithm finding a bad local maximum are corresponding greater.", "The only way the greedy algorithm can get stuck in a local maximum is if the estimated return for a suboptimal arm is higher than the estimated return for the optimal arm. In practice, this will only happen relatively early in the experiment; given enough time the law of large numbers guarantees that the optimal arm will have the best estimate. The phrase \u201cenough time\u201d is a funny one in mathematics; it can be a very, very long time indeed. Furthermore, multi-armed bandits are usually trained \u201cin production\u201d and need to hit optimal, or near optimal, performance in a timely manner. We don\u2019t have \u201cenough time.\u201d", "Enter the epsilon-greedy algorithm. It differs from the greedy algorithm in only one particular. With some probability epsilon (0.01 and 0.1 are popular but it varies by problem) the epsilon-greedy algorithm will pull a random arm. This allows the optimal arm to get back in the game if the greedy algorithm has made a bad choice while still making the best guess based on current knowledge most of the time.", "To be really sophisticated, drop the value of epsilon over time.", "Both the algorithms I\u2019ve discussed so far only use the average of the bandit arm payouts. What if we want to use information about the variance. The best nonparametric estimate comes from Hoffding\u2019s Inequality and gives us the estimate:", "In this formula, Q_t(a) is the current estimated average of value of arm a, t is the number of trials, N_t(a) is the number of times arm a has been pulled, and c is a user defined constant. The algorithm picks the arm with the highest upper confidence bound. This has an effect similar to the epsilon greedy-algorithm with a decreasing value of epsilon, only without the need to pick an initial epsilon or a decay rule by hand. A value of c still needs to be chosen.", "A multi-armed armed bandit simulator can be split into two parts; the bandit itself and the player, or agent. In Python, C++, or Java, both components could be encoded as classes. Rust\u2019s object oriented setup doesn\u2019t have classes per se, rather relying on structs, which carry organized collects of data, and traits, which contain methods that can be implemented for structs. Structs are very similar to C++ structs, while traits are similar to Java interfaces. As is usual with coding concepts, this will become clearer with an example.", "My basic bandit trait is below, complete with all the functions a good bandit should need.", "The methods that could be defined have been. Those that require data from a specific type of struct will be defined in the implementation. The data needed for a bandit is simply the probability distribution of each arm. A case where each arm has a binomial distribution is below.", "The struct needs to be public because it will be referenced outside it\u2019s current module. There are a number of privacy levels for objects in Rust; it is almost always a good idea to use the most restrictive available.", "The other bits of this code that may be confusing to newcomers are the use of the ampersand & and the use of 'a. The former indicates that the value is a reference, which is similar to a pointer. In this struct nums and prob are not the vectors themselves, but merely the memory addresses of the vectors. The strange little 'a symbol governs the lifetimes of these references, and ensures that they don\u2019t live longer than the vectors they point to. Rust does not allow dangling pointers.", "A binomial distribution is defined by the number of trials and the probability of success for each trial. It may seem redundant to store both the defining parameters and a vector of distributions, and it is. As currently designed, the distributions cannot return their parameters, so it\u2019s worth the extra space to hold on to them separately.", "It often helps to define a custom constructor. To do so, implement the struct.", "Most of this is self-explanatory. A few basic assert statements ensure I don\u2019t load rubbish. The two vectors are then zipped and iterated over to create the vector of distributions. The one bit of code that may require comment is u64::from(n). Modern programming languages, at least the compiled variety, seem to be going off the idea of automatic type conversion, and Rust is no exception. Dividing an int by a float doesn\u2019t work. This is probably good for type safety, but does have the downside of forcing the use of ugly explicit type casting statements.", "At this point, I just have a bandit sitting around not able to do much. To fix that, I need to implement the Bandit trait for the BinomialBandit struct. This involves defining the five methods in Bandit that were dependent on struct data.", "The original Bandit trait left the type of the output to the individual implementations. Here, I specify that BinomialBandit is in the business of giving out unsigned integer rewards. If you look at GaussianBandit you will see it dishes out floats.", "Just as there can be multiple types of bandits, there are multiple types of agents as well. I designed one agent struct for each of the three algorithms for finding the optimal strategy and a general Agent trait containing the function they all need. As with the bandit, I\u2019ll describe the agent trait first.", "As with the bandit trait, most of the function are already filled in. Only method that requires much discussion is update. All agents update their estimates by a rule not dissimilar to stochastic gradient descent. Take the difference between the current reward and the estimated average reward and adjust the estimate by some multiple, analogous to the learning rate, of that value. Exactly what rule determines the \u201clearning rate\u201d is up to the user; a good choice is the harmonically decreasing rate, which makes the estimated average the sample average. The available stepper classes are in the stepper module in utils.", "ToPrimitive at the beginning of the trait is necessary to explicitly state that any type T used by Agent must be convertible to a float.", "A look at the EpsilonGreedy struct will be instructive in showing us how the the Agent trait is used.", "The most important component of the struct is q_star, which contains the estimates of the average value of each arm. Most of the rest helps with the updates. I\u2019ve already discussed stepper, epsilon is simply the frequency with which random arms are chosen, and the two uniform distributions are there to decide when and how to pick a random arm. The last parameter, phantom, is a reference necessary to ensure that the struct is seen to use something of type T, which is otherwise only used by the implementing trait.", "How do we implement Agent for EpsilonGreedyAgent? See below.", "The agent and the bandit have to interact in order to perform experiments. Interactions are managed by a Game struct that contains an Agent, a Bandit, as well as counters and methods to make them interact.", "The basic structure is self-explanatory. Apart from the Agent and Bandit it contains counters to keep track of wins (picking the best arm) and rewards (the results of all arm pulls.)", "The implementation is a little more interesting.", "As usual, most class methods are simple setters and getters. The two most interesting methods arepull_arm and run. The first chooses an action, checks if it\u2019s the best possible, and then returns the reward and whether it was the best. The second performs a full experiment by pulling the arm as many times as the experiment requires.", "Those are all the building blocks needed for running a multi-armed bandit experiment. The rest of the Ratel code is either glue for putting experiments together or code for producing output. Anyone using this module for their own work would have to write their own versions, so I won\u2019t discuss my code in detail. I will, in the next section, discuss one of my experiments.", "When thinking about any complicated system it pays to start simple. The simplest multi-armed bandit has only two arms and the simplest probability distribution is, arguably, Bernoulli. One is returned with some probability p, and zero is returned with probability 1-p. Given two arms, a left and a right, with Bernoulli outputs with different probabilities (otherwise it would be trivial) what\u2019s the likelihood of finding the best arm with the a) greedy algorithm, b) epsilon-greedy algorithm? (The optimistic algorithm works better with continuous distributions; I won\u2019t consider it here.) How does this likelihood change as the probabilities vary?", "I take the full range of probabilities from 0.01 to 0.99 in increments of 0.01. There\u2019s no virtue in testing arm A with probability p and arm B with probability q only to then turn around and test arm A with probability q and arm B with probability B, so I can cut the number of cases in half by always giving the left arm the smaller probability. I still have 4851 combinations to try. While I\u2019m at it, I\u2019ll also try different starting values, 0.1 to 1.0 in increments of 0.1. That\u2019s now 48510 combinations.", "And here\u2019s where working in Rust really starts to pay off. If I\u2019d been working in Python, this is last you\u2019d hear from me for about a month. Rust is about 100 times faster, which means I can run even hefty experiments overnight.", "The Greedy Algorithm converges very fast; 200 iterations is sufficient to get accurate results for all combinations. This allows me to run each experiment 100000 times. With a six-core i7-processor (12 threads total,) the greedy algorithm experiments run in about 3.5 hours.", "The lowest possible initial guess in the experiment is 0.1 for both arms. If I start with that, then the probability of the greedy algorithm picking the correct arm is high when only one arm has high probability, in which case the problem is very easy, or both arms have low probability, in which case their relative differences will still be significant. By the time the left arm reaches a probability of 0.7, the chances of picking the best arm are little better than even.", "Matters improve as the initial guess increases. Each time the initial guess is incremented by 0.1, the likelihood of picking the best arm increases, or at least does not decrease, for all choices of arm probabilities. By the time the initial guess reach 0.7, the region of low probability success is a small sliver where the probabilities of both arms are high and the difference of their probabilities is low (the lower part of the triangle diagonal)", "The moral is that, for the greedy algorithm, the initial guess is important and should be made as high as is practical. In this case, it was easy since the distributions were known to be Bernoulli. With less information about the distribution, a good initial guess will be harder to come by.", "The other lesson is that even with a good guess, there are cases where the greedy algorithm simply isn\u2019t that good. In this toy example, it is easy to qualify, if not quite quantify, the bad regions, but with more arms or more complicated distributions that luxury will no longer be available.", "An obvious question is whether it\u2019s possible to get more accuracy with longer runs; 200 iterations is not that much. It turns out to be enough. Plotting a subset of the experiments where the probabilities of the two arms differed only by 0.01 (taking the left arm to be 0.1, 0.2, 0.3, etc) it\u2019s clear that convergence was reached before the 100th iteration. This is as good as the greedy algorithm gets.", "Is there a better strategy? Yes, although it does come at a price. Picking a random arm with some low probability gets near optimal results for all but the most extreme cases. However, the epsilon-greedy algorithm comes with two catches. The first is that there is a hard upper bound, less than 1.0, on the probability of success. If a random arm is chosen with probability epsilon, then the best arm is can be chosen with probability at most (1-epsilon) + epsilon/(number of arms). As the number of arms gets larger, this approaches 1-epsilon, which is an incentive to keep epsilon small.", "The second catch is that epsilon-greedy takes longer to converge, with smaller values of epsilon taking longer than larger ones.", "I start with epsilon=0.1. With two arms, the probability of picking the best arm should max out at about 0.95. As with the greedy algorithm, I try ten different initial values, ranging from 0.1 to 1.0. Each run consists of 4000 iterations, and I make 10000 runs (one tenth of the number for the greedy algorithm). The experiment took about 7.5 hours.", "The first thing to notice is that the dependence on the initial guess has been all but eliminated. That alone is a significant argument in epsilon-greedy\u2019s favor. The second is that, except for the narrowest of slivers along the diagonal, all arm combinations have accuracy close to their maximum. The third is that, as expected, the maximum accuracy is not quite as high as it is for the greedy algorithm. Even with that one caveat, this is still an improvement over the previous result.", "And there\u2019s more good news in the region where the agent still has trouble picking the correct arm. Learning isn\u2019t saturated yet. Further iterations would likely lead to better results. Plotting the same subset of experiments as above shows that the win rate has not leveled off yet. Another 4000 iterations, and I\u2019d have even better results.", "What happens if I try a lower value of epsilon? With a value of epsilon equal to 0.01, I get a result that has most of the plusses of both the greedy algorithm and epsilon-greedy with a larger epsilon. Apart from the change in epsilon, all parameters for this experiment are the same as above.", "Once again, there is maximum possible accuracy, but now it is 0.995. Once again, the results are largely independent of the initial guess. Once again, this accuracy is reached for all but a fairly narrow, albeit slightly wider, subset where the probabilities of the two arms are similar.", "What about that narrow range? Is there any chance that more iterations will lead to better results there. Yes, but it will require a lot more. Accuracy is still increasing but at a much slower rate than for epsilon = 0.1.", "There are a number of enhancements that could be made to Ratel. At the moment, only two probability distributions are supported for bandits, Gaussian and binomial, but adding others would be trivial. Similarly, there are more sophisticated agent algorithms that could be added. A slightly less trivial enhancement would be to allow bandit arms from multiple distribution families, although most applications I\u2019m aware of do not require that.", "Of the agents implemented, it seems that the most successful overall is epsilon-greedy with a low value of epsilon. The greedy algorithm works best in cases where the arms can be easily distinguished. Epsilon-greedy is successful in all but the toughest to distinguish cases.", "Perhaps the most important conclusion is that Rust performs well as a language for modeling and simulation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F724a9a7a3606&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dacmorton?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dacmorton?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "Daniel Morton"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd75e3cd4a0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&user=Daniel+Morton&userId=d75e3cd4a0e&source=post_page-d75e3cd4a0e----724a9a7a3606---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F724a9a7a3606&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F724a9a7a3606&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/DanielMorton/ratel", "anchor_text": "Ratel"}, {"url": "https://en.wikipedia.org/wiki/Honey_badger", "anchor_text": "honey badger"}, {"url": "https://en.wikipedia.org/wiki/Law_of_large_numbers", "anchor_text": "law of large numbers"}, {"url": "https://doc.rust-lang.org/book/ch05-01-defining-structs.html", "anchor_text": "structs"}, {"url": "https://doc.rust-lang.org/book/ch10-02-traits.html", "anchor_text": "traits"}, {"url": "http://www.cplusplus.com/doc/tutorial/structures/", "anchor_text": "C++ structs"}, {"url": "https://www.javatpoint.com/interface-in-java", "anchor_text": "Java interfaces"}, {"url": "https://medium.com/tag/rust?source=post_page-----724a9a7a3606---------------rust-----------------", "anchor_text": "Rust"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----724a9a7a3606---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/multi-armed-bandit?source=post_page-----724a9a7a3606---------------multi_armed_bandit-----------------", "anchor_text": "Multi Armed Bandit"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F724a9a7a3606&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&user=Daniel+Morton&userId=d75e3cd4a0e&source=-----724a9a7a3606---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F724a9a7a3606&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&user=Daniel+Morton&userId=d75e3cd4a0e&source=-----724a9a7a3606---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F724a9a7a3606&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F724a9a7a3606&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----724a9a7a3606---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----724a9a7a3606--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----724a9a7a3606--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----724a9a7a3606--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dacmorton?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dacmorton?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daniel Morton"}, {"url": "https://medium.com/@dacmorton/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd75e3cd4a0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&user=Daniel+Morton&userId=d75e3cd4a0e&source=post_page-d75e3cd4a0e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F578b78cbd231&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frusty-bandit-724a9a7a3606&newsletterV3=d75e3cd4a0e&newsletterV3Id=578b78cbd231&user=Daniel+Morton&userId=d75e3cd4a0e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}