{"url": "https://towardsdatascience.com/natural-language-processing-the-age-of-transformers-a36c0265937d", "time": 1683006333.1999848, "path": "towardsdatascience.com/natural-language-processing-the-age-of-transformers-a36c0265937d/", "webpage": {"metadata": {"title": "Natural Language Processing: the Age of Transformers | by Olga Petrova | Towards Data Science", "h1": "Natural Language Processing: the Age of Transformers", "description": "In the recent past, if you specialized in natural language processing (NLP), there may have been times when you felt a little jealous of your colleagues working in computer vision. It seemed as if\u2026"}, "outgoing_paragraph_urls": [{"url": "https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/", "anchor_text": "ImageNet classification challenge", "paragraph_index": 0}, {"url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html", "anchor_text": "Neural Style Transfer", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "Generative Adversarial Networks", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is all you need", "paragraph_index": 0}, {"url": "https://blog.scaleway.com/2019/understanding-text-with-bert/", "anchor_text": "BERT", "paragraph_index": 0}, {"url": "https://www.aclweb.org/anthology/D14-1179", "anchor_text": "Ref", "paragraph_index": 3}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Ref", "paragraph_index": 3}, {"url": "http://hunterheidenreich.com/blog/intro-to-word-embeddings/", "anchor_text": "word embedding", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d", "anchor_text": "softmax", "paragraph_index": 3}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTMs", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be", "anchor_text": "GRUs", "paragraph_index": 4}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is All You Need", "paragraph_index": 11}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "original Transformer paper", "paragraph_index": 12}, {"url": "http://hunterheidenreich.com/blog/intro-to-word-embeddings/", "anchor_text": "word embedding", "paragraph_index": 13}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer", "paragraph_index": 14}, {"url": "https://www.quora.com/Why-is-there-no-character-for-superscript-q-in-Unicode", "anchor_text": "true story!", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Attention is all you need", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 22}, {"url": "https://blog.scaleway.com/2019/understanding-text-with-bert/", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/", "anchor_text": "beam search", "paragraph_index": 27}, {"url": "https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/", "anchor_text": "Scaleway\u2019s blog", "paragraph_index": 28}, {"url": "https://scaleway.typeform.com/to/V0JKT5lX#organization_id=xxxxx", "anchor_text": "Sign up to get notified when Smart Labeling becomes available!", "paragraph_index": 29}, {"url": "http://www.olgapaints.net", "anchor_text": "www.olgapaints.net", "paragraph_index": 31}], "all_paragraphs": ["In the recent past, if you specialized in natural language processing (NLP), there may have been times when you felt a little jealous of your colleagues working in computer vision. It seemed as if they had all the fun: the annual ImageNet classification challenge, Neural Style Transfer, Generative Adversarial Networks, to name a few. At last, the dry spell is over, and the NLP revolution is well underway! It would be fair to say that the turning point was 2017, when the Transformer network was introduced in Google\u2019s Attention is all you need paper. Multiple further advances followed since then, one of the most important ones being BERT.", "To lay the groundwork for the Transformer discussion, let\u2019s start by looking at one of the common categories of NLP tasks: the sequence to sequence (seq2seq) problems. They are pretty much exactly what their name suggests: both the inputs and the outputs of a seq2seq task are sequences. In the context of NLP, there are typicaly additional restrictions put in place:", "Next we shall take a moment to remember the fallen heros, without whom we would not be where we are today. I am, of course, referring to the RNNs \u2014 Recurrent Neural Networks, a concept that became almost synonymous with NLP in the deep learning field.", "This story takes us all the way back to 2014 (Ref, another Ref), when the idea of approaching seq2seq problems via two Recurrent Neural Networks combined into an Encoder-Decoder model, was born. Let\u2019s demonstrate this architecture on a simple example from the Machine Translation task. Take a French-English sentence pair, where the input is \u201dje suis \u00e9tudiant\u201d and the output \u201dI am a student\u201d. First, \u201dje\u201d (or, most likely, a word embedding for the token representing \u201dje\u201d), often accompanied by a constant vector h_E0which could be either learned or fixed, gets fed into the Encoder RNN. This results in the output vector h_E1 (hidden state 1), which serves as the next input for the Encoder RNN, together with the second element in the input sequence \u201dsuis\u201d. The output of this operation, h_E2, and \u201d\u00e9tudiant\u201d are again fed into the Encoder, producing the last Encoded hidden state for this training sample, h_E3. The h_E3 vector is dependent on all of the tokens inside the input sequence, so the idea is that it should represent the meaning of the entire phrase. For this reason it is also referred to as the context vector. The context vector is the first input to the Decoder RNN, which should then generate the first element of the output sequence \u201dI\u201d (in reality, the last layer of the Decoder is typically a softmax, but for simplicity we can just keep the most likely element at the end of every Decoder step). Additionally, the Decoder RNN produces a hidden state h_D1. We feed h_D1 and the previous output \u201cI\u201d back into the Decoder to hopefully get \u201dam\u201d as our second output. This process of generating and feeding outputs back into the Decoder continues until we produce an <EOS> \u2014 the end of the sentence token, which signifies that our job here is done.", "This architecture may seem simple (especially until we sit down to actually write the code with LSTMs or GRUs thrown in for good measure), but it actually turns out to be remarkably effective for many NLP tasks. In fact, Google Translate has been using it under the hood since 2016. However, the RNN Encoder-Decoder models do suffer from certain drawbacks:", "The RNN approach as described above does not work particularly well for longer sentences. Think about it: the meaning of the entire input sequence is expected to be captured by a single context vector with fixed dimensionality. This could work well enough for \u201cJe suis \u00e9tudiant\u201d, but what if your input looks more like this:", "\u201cIt was a wrong number that started it, the telephone ringing three times in the dead of night, and the voice on the other end asking for someone he was not.\u201d", "Good luck encoding that into a context vector! However, there turns out to be a solution, known as the Attention mechanism.", "The basic idea behind Attention is simple: instead of passing only the last hidden state (the context vector) to the Decoder, we give it all the hidden states that come out of the Encoder. In our example that would mean h_E1, h_E2 and h_E3. The Decoder will determine which of them gets attended to (i.e., where to pay attention) via a softmax layer. Apart from adding this additional structure, the basic RNN Encoder-Decoder architecture remains the same, yet the resulting model performs much better when it comes to longer input sequences.", "The other problem plaguing RNNs has to do with the R inside the name: the computation in a Recurrent neural network is, by definition, sequential. What does this property entail? A sequential computation cannot be parallelized, since we have to wait for the previous step to finish before we move on to the next one. This lengthens both the training time, and the time it takes to run inference.", "One of the ways around the sequential dilemma is to use Convolutional neural networks (CNNs) instead of RNNs. This approach has seen its share of success, until it got outshone by <drumroll>\u2026", "The Transformer architecture was introduced in the paper whose title is worthy of that of a self-help book: Attention is All You Need. Again, another self-descriptive heading: the authors literally take the RNN Encoder-Decoder model with Attention, and throw away the RNN. Attention is all you need! Well, it ends up being quite a bit more complicated than that in practice, but that is the basic premise.", "How does this work? To start with, each pre-processed (more on that later) element of the input sequence w_i gets fed as input to the Encoder network \u2014 this is done in parallel, unlike the RNNs. The Encoder has multiple layers (e.g. in the original Transformer paper their number is six). Let us use h_i to label the final hidden state of the last Encoder layer for each w_i. The Decoder also contains multiple layers \u2014 typically, the number is equal to that of the Encoder. All of the hidden states h_i will now be fed as inputs to each of the six layers of the Decoder. If this looks familiar to you, it is for a good reason: this is the Transformer\u2019s Encoder-Decoder Attention, which is rather similar in spirit to the Attention mechanism that we discussed above. Before we move on to how the Transformer\u2019s Attention is implemented, let us discuss the preprocessing layers (present in both the Encoder and the Decoder as we shall see later).", "There are two parts to preprocessing: first, there is the familiar word embedding, a staple in most modern NLP models. These word embeddings could be learned during training, or one could use one of the existing pre-trained embeddings. There is, however, a second part that is specific to the Transformer architecture. So far, nowhere have we provided any information on the order of the elements inside the sequence. How can this be done in the absence of the sequential RNN architecture? Well, we have the positions, let\u2019s encode them inside vectors, just as we embedded the meaning of the word tokens with word embeddings. The resulting post-processed vectors, carrying information about both the word\u2019s meaning and its position in the sentence, are passed on to the Encoder and Decoder layers.", "I come from a quantum physics background, where vectors are a person\u2019s best friend (at times, quite literally), but if you prefer a non linear algebra explanation of the Attention mechanism, I highly recommend checking out The Illustrated Transformer by Jay Alammar.", "Let\u2019s use X to label the vector space of our inputs to the Attention layer. What we want to learn during training are three embedding matrices, W\u1d37, W\u2c7d and W\u1d2c, which will permit us to go from X to three new spaces: K(keys), V (values) and Q (queries). Annoyingly, \u201cQ\u201d is not available as a subscript (true story!), so I resorted to W\u1d2c where \u201cA\u201d stands for \u201casking\u201d. Here is what we have so far:", "The way that these embedded vectors are then used in the Encoder-Decoder Attention is the following. We take a Q vector (a query, i.e., we specify the kind of information that we want to attend to) from the Decoder. Additionally, we take vectors V (values) that we can think of as something similar to linear combinations of vectors X coming from the Encoder (do not take \u201clinear combination\u201d literally however, as the dimensionality of Xand V is, in general, different). Vectors K are also taken from the Encoder: each key K\u207f indexes the kind of information that is captured by the value V\u207f.", "To determine which values should get the most attention, we take the dot product of the Decoder\u2019s query Q with all of the Encoder\u2019s keys K. The softmax of the result will give the weights of the respective values V (the larger the weight, the greater the attention). Such mechanism is known as the Dot-product attention, given by the following formula:", "where one can optionally divide the dot product of Q and K by the dimensionality of key vectors d_k. To give you an idea for the kind of dimensions used in practice, the Transformer introduced in Attention is all you need has d_q=d_k=d_v=64 whereas what I refer to as X is 512-dimensional.", "In addition to the Encoder-Decoder Attention, the Transformer architecture includes the Encoder Self-Attention and the Decoder Self-Attention. These are calculated in the same dot-product manner as discussed above, with one crucial difference: for self-attention, all three types of vectors (K, V, and Q) come from the same network. This also means that all three are associated with the elements of the same sequence (input for the Encoder and output for the Decoder). The purpose of introducing self-attention is to learn the relationships between different words in the sentence (this function used to be fulfilled by the sequential RNN). One way of looking at it is a representation of each element of the sequence as a weighted sum of the other elements in the sequence. Why bother? Consider the following two phrases:", "1. The animal did not cross the road because it was too tired.", "2. The animal did not cross the road because it was too wide.", "Clearly, it is most closely related to the animal in the first phrase and the road in the second one: information that would be missing if we were to use a uni-directional forward RNN! In fact, the Encoder Self-Attention, that is bi-directional by design, is a crucial part of BERT, the pre-trained contextual word embeddings, that I discussed here.", "Where are the calculations for the Encoder Self-Attention carried out? Turns out, inside every Encoder layer. This permits the network to pay attention to relevant parts of the input sequence at different levels of abstraction: the values V of the lower Encoder layers will be closest to the original input tokens, whereas Self-Attention of the deeper layers will involve more abstract constructions.", "By now we have established that Transformers discard the sequential nature of RNNs and process the sequence elements in parallel instead. We saw how the Encoder Self-Attention allows the elements of the input sequence to be processed separately while retaining each other\u2019s context, whereas the Encoder-Decoder Attention passes all of them to the next step: generating the output sequence with the Decoder. What happens at this stage may not be so clear. As you recall, the RNN Encoder-Decoder generates the output sequence one element at a time. The previously generated output gets fed into the Decoder at the subsequent timestep. Do Transformers really find a way to free us from the sequential nature of this process and somehow generate the whole output sequence at once? Well \u2014 yes and no. More precisely, the answer is [roughly] yes when training, and no at inference time.", "The inputs to the Decoder come in two varieties: the hidden states that are outputs of the Encoder (these are used for the Encoder-Decoder Attentionwithin each Decoder layer) and the previously generated tokens of the output sequence (for the Decoder Self-Attention, also computed at each Decoder layer). Since during the training phase, the output sequences are already available, one can perform all the different timesteps of the Decoding process in parallel by masking (replacing with zeroes) the appropriate parts of the \u201cpreviously generated\u201d output sequences. This masking results in the Decoder Self-Attention being uni-directional, as opposed to the Encoder one. Finally, at inference time, the output elements are generated one by one in a sequential manner.", "A final remark before we call it a day:", "The part of the Decoder that I refer to as postprocessing in the Figure above is similar to what one would typically find in the RNN Decoder for an NLP task: a fully connected (FC) layer, which follows the RNN that extracted certain features from the network\u2019s inputs, and a softmax layer on top of the FC one that will assign probabilities to each of the tokens in the model\u2019s vocabularly being the next element in the output sequence. At that point, we could use a beam search algorithm to keep the top few predictions at each step and choose the most likely output sequence at the end, or simply keep the top choice each time.", "A version of this article was first published on August 22, 2019 on Scaleway\u2019s blog.", "Smart Labeling by Scaleway is a managed data annotation platform that will help you label the data that you need for your computer vision (2021) and NLP (2022) projects! With auto pre-labeling and active learning features on our roadmap, your dataset will be ready in no time. Sign up to get notified when Smart Labeling becomes available!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Former quantum physicist & current techie who also creates art with stories behind it. Based in Paris, France. \ud83c\udf10 www.olgapaints.net"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa36c0265937d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a36c0265937d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a36c0265937d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@OlgaPaints?source=post_page-----a36c0265937d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@OlgaPaints?source=post_page-----a36c0265937d--------------------------------", "anchor_text": "Olga Petrova"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3ec417445414&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&user=Olga+Petrova&userId=3ec417445414&source=post_page-3ec417445414----a36c0265937d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa36c0265937d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa36c0265937d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/", "anchor_text": "ImageNet classification challenge"}, {"url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html", "anchor_text": "Neural Style Transfer"}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "Generative Adversarial Networks"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is all you need"}, {"url": "https://blog.scaleway.com/2019/understanding-text-with-bert/", "anchor_text": "BERT"}, {"url": "https://www.aclweb.org/anthology/D14-1179", "anchor_text": "Ref"}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Ref"}, {"url": "http://hunterheidenreich.com/blog/intro-to-word-embeddings/", "anchor_text": "word embedding"}, {"url": "https://towardsdatascience.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d", "anchor_text": "softmax"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTMs"}, {"url": "https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be", "anchor_text": "GRUs"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com/s/photos/transformers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is All You Need"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "original Transformer paper"}, {"url": "http://hunterheidenreich.com/blog/intro-to-word-embeddings/", "anchor_text": "word embedding"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://www.quora.com/Why-is-there-no-character-for-superscript-q-in-Unicode", "anchor_text": "true story!"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Attention is all you need"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://blog.scaleway.com/2019/understanding-text-with-bert/", "anchor_text": "here"}, {"url": "https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/", "anchor_text": "beam search"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com/s/photos/transformers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/", "anchor_text": "Scaleway\u2019s blog"}, {"url": "https://scaleway.typeform.com/to/V0JKT5lX#organization_id=xxxxx", "anchor_text": "Sign up to get notified when Smart Labeling becomes available!"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a36c0265937d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a36c0265937d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----a36c0265937d---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a36c0265937d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a36c0265937d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa36c0265937d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&user=Olga+Petrova&userId=3ec417445414&source=-----a36c0265937d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa36c0265937d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&user=Olga+Petrova&userId=3ec417445414&source=-----a36c0265937d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa36c0265937d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a36c0265937d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa36c0265937d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a36c0265937d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a36c0265937d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a36c0265937d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a36c0265937d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a36c0265937d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a36c0265937d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a36c0265937d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a36c0265937d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a36c0265937d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@OlgaPaints?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@OlgaPaints?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Olga Petrova"}, {"url": "https://medium.com/@OlgaPaints/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "104 Followers"}, {"url": "http://www.olgapaints.net", "anchor_text": "www.olgapaints.net"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3ec417445414&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&user=Olga+Petrova&userId=3ec417445414&source=post_page-3ec417445414--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F21863272452a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-the-age-of-transformers-a36c0265937d&newsletterV3=3ec417445414&newsletterV3Id=21863272452a&user=Olga+Petrova&userId=3ec417445414&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}