{"url": "https://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e", "time": 1683017770.933747, "path": "towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e/", "webpage": {"metadata": {"title": "Context-specific Pre-processing for NLP with spaCy: Tweets | by Wei-Ting Yap | Towards Data Science", "h1": "Context-specific Pre-processing for NLP with spaCy: Tweets", "description": "Natural Language Processing is a field of machine learning concerned with understanding human language. As opposed to numerical data, NLP works primarily with text. Exploring and pre-processing text\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "Real or Not? NLP with Disaster Tweets", "paragraph_index": 2}, {"url": "https://github.com/weiting109/disaster-tweets-classifier/blob/main/nb.ipynb", "anchor_text": "notebook and repository", "paragraph_index": 3}, {"url": "https://www.kaggle.com/c/nlp-getting-started/data", "anchor_text": "data description on Kaggle", "paragraph_index": 6}, {"url": "https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/", "anchor_text": "tuning our hyper-parameters", "paragraph_index": 31}, {"url": "http://t.co", "anchor_text": "http://t.co", "paragraph_index": 40}, {"url": "https://spacy.io/", "anchor_text": "spaCy", "paragraph_index": 41}, {"url": "https://spacy.io/usage/linguistic-features#how-tokenizer-works", "anchor_text": "How spaCy\u2019s Tokenizer Works", "paragraph_index": 46}, {"url": "https://towardsdatascience.com/a-simple-intro-to-regex-with-python-14d23a34d170#:~:text=Regular%20expressions%20are%20used%20to,projects%20that%20involve%20text%20analytics.", "anchor_text": "A simple intro to Regex with Python", "paragraph_index": 47}, {"url": "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/", "anchor_text": "tune the hyper-parameters", "paragraph_index": 56}, {"url": "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089", "anchor_text": "Term Frequency \u2014 Inverse Document Frequency (TFIDF)", "paragraph_index": 57}, {"url": "https://www.kaggle.com/matleonard/word-vectors", "anchor_text": "word vectors", "paragraph_index": 57}, {"url": "https://emma-stiefel.medium.com/standardizing-spelling-and-locations-with-python-pyspellchecker-and-mordecai-8b0fa7d44feb", "anchor_text": "advanced pre-processing by performing spell checks and correction with Pyspellchecker and Mordecai", "paragraph_index": 60}, {"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "Disaster tweets classification challenge on Kaggle", "paragraph_index": 61}, {"url": "https://www.kaggle.com/matleonard/intro-to-nlp", "anchor_text": "Intro to NLP", "paragraph_index": 62}, {"url": "https://www.kaggle.com/matleonard/text-classification", "anchor_text": "Text Classification with SpaCy", "paragraph_index": 63}, {"url": "https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1", "anchor_text": "Your Guide to Natural Language Processing", "paragraph_index": 64}, {"url": "http://breakintotech.substack.com", "anchor_text": "breakintotech.substack.com", "paragraph_index": 66}, {"url": "http://weitingyp.xyz", "anchor_text": "weitingyp.xyz", "paragraph_index": 66}], "all_paragraphs": ["Natural Language Processing is a field of machine learning concerned with understanding human language. As opposed to numerical data, NLP works primarily with text. Exploring and pre-processing text data requires different techniques and libraries, and this tutorial demonstrates the basics.", "However, pre-processing is not an algorithmic procedure. With data science tasks, often the context of the data determines what aspects of the data are valuable, and what aspects are irrelevant or unreliable. In this tutorial, we explore text pre-processing in the context of tweets, or more generally, social media.", "Kaggle\u2019s 9-year-old Getting Started Real or Not? NLP with Disaster Tweets competition presents a reasonably-sized dataset (around 7500 tweets in the training set) for practice. The challenge is to classify tweets, given their text, keyword and location, into whether they are really about disasters or not.", "The code for this tutorial can be followed at this notebook and repository.", "Before we get started, download the nlp-getting-started data from Kaggle. In my project directory, I put train.csv, test.csv, and sample_submission.csv under a data subdirectory.", "Let\u2019s start by importing typical and useful data science libraries and creating a dataframe out of train.csv. I won\u2019t delve into the details of libraries that are not NLP-specific.", "Our data comprises 4 columns, keyword, location, text and target. Quoting the data description on Kaggle:", "To ensure integrity with the number of rows and columns in our dataset, as well as make judgements on the generalizability of our training set, let\u2019s understand the size of our training data.", "Examining closer, we find there are 52 duplicate rows (different id\u2019s, but same keyword, location, text and target.", "So let\u2019s drop the duplicate rows. The index (set as id) remains intact. After removing duplicate rows, we are left with 7561 tweets (integrity check, as mentioned earlier), a manageable amount for this tutorial.", "However, 7561 data points is still relatively little for NLP, especially if we are using deep learning models. Given that there are probably close to a million tweets each day, I doubt a model trained on just 7561 data points is generalizable enough.", "Aside from training size, the balance of classes (target) in training set is also important. A training set with all target value = 0 will leave the model classifying every tweet as not about a disaster. The vice versa situation is also true. Ideally, the training set should have all classes represented equally.", "We can use panda\u2019s dataframe value_counts() method to count the number of rows for each class. With 4322 tweets not about disasters (target=0) and 3239 tweets about disasters (target=1), we have a 4:3 class balance. That's not perfectly balanced, but it's not disastrously imbalanced.", "Let\u2019s also look into the completeness of data. Using we can sum up the series returned by panda\u2019s dataframe isna() method to count the number of na entries for each column.", "Ideally, we would further characterize and explore the data by analyzing the word lengths, sentence lengths, word frequencies and more. While that\u2019s out of scope for this tutorial, you can learn more about it here.", "Now that we have explored the data, let\u2019s pre-process the tweets and represent them in a form our models can take as input.", "The most common numerical representation for texts is the bag-of-words representation.", "Bag of words is a way to represent text data numerically. Text data is essentially split into words (or more accurately, tokens), which are features. The frequency of each word in each text data is are the corresponding feature values. For example, we might represent \"I love cake very very much\" as a bag of words dictionary:", "Tokenization breaks text data up into its tokens at the level of NLP (word, phrase, sentence). The lowest (and most common) is words, which fits perfectly with our bag of words representation. However, these tokens could also include punctuation, stop words, and other custom tokens. We\u2019ll consider these in the context of tweets and the challenge in the next session.", "Stemming refers to truncating words of their affixes (prefixes or suffixes) to approximate them to their root form. This is often done with a lookup dictionary of prefixes and suffixes, making it computationally fast.", "However, there is a performance trade-off. In the English language, some affixes change the meaning of the word completely, resulting in inaccurate feature representation.", "The alternative to stemming is lemmatization, where words are reduced to their lemmas, or root form. This is done using a lookup dictionary of words and their lemmas, hence resulting in it being more computationally expensive. However, performance is often better, since features are represented more accurately.", "Given the relatively smaller size of our dataset, we will use lemmatization.", "Getting from tweets to their bag of words representation is less straightforward. What about:", "When deciding what we want to do with these elements, we have to consider the context of the data and reconcile that with the challenge.", "In internet lingo, different cases could communicate different sentiments (e.g. danger vs DANGER!) or different parts of speech (e.g. start of the sentence, or a pronoun like The Fight Club). By changing all tokens to upper- or lower- case, we could be losing data helpful for classification.", "However, since we have a small dataset (7500 tweets), there is unlikely to be sufficient data of each upper-/lower-case variant, let\u2019s go with lower-case.", "Tweets are undoubtedly going to contain punctuations, which can convey different sentiments or emotions too. Consider, in internet lingo, the difference between:", "We\u2019ll consider punctuations each as their own tokens, with special cases like \u2018\u2026\u2019 being a separate token from \u2018.\u2019. So we don\u2019t lose data, we can disregard them (and even tune which punctuations to ignore) when tuning our hyper-parameters.", "Stop words are essentially words so common that they have little significant contribution to the meaning of the text. These words include articles (the, a, that) and other commonly used words (what, how, many).", "Stop word tokens are often ignored in NLP processing. Plus, the character limit of a tweet (280 characters) often results in grammatically incorrect tweets, where articles are missed.", "However, rather than ignore stop words from the start, let\u2019s disregard them (and even tune which stop words to ignore) when tuning our hyper-parameters so we don\u2019t lose data.", "Numbers in tweets can convey the quantity of literal objects, but can also convey the scale of something (e.g. 7.9 Richter scale earthquake) or the year (e.g. 2005 Hurricane Katrina).", "In the latter two cases, such numerical information may be valuable depending on the level of NLP we choose to do later down the road (word-level vs phrase- or sentence- level), or if we want to filter tweets about historical disasters vs current disasters.", "As such, we will retain numbers as tokens, with the option of ignoring them (or even only counting numbers that are years) when tuning our hyper-parameters.", "On Twitter, mentions allows users to address each other through a tweet. While mentions between personal accounts may be less significant, mentions to authorities to alert them of disasters (consider @policeauthorities, gun shooting down brick lane right now!).", "Let\u2019s tokenize mentions along with their usernames, but also count the number of mentions, which could convey a conversation.", "Hashtags on Twitter allow users to discover content related to a specific theme or topic. When it comes to natural disasters, hashtags like #prayforCountryX and #RIPxyzShootings can differentiate tweets about disasters from everyday tweets.", "As such, let\u2019s tokenize hashtags with their content, but also count the number of hashtags. The number of hashtags could flag sensationalized social media marketing tweets (e.g. This beat drop is the bomb! #edm #music #dubstep #newrelease) that use disaster keywords.", "Disaster tweets could include URLs to news articles, relief efforts, or images. However, the same can be said of everyday tweets. Since we\u2019re unsure if disaster tweets are more likely to have URLs or a certain type of URL, let\u2019s keep URLs as tokens and the number of URLs as a feature.", "This challenge\u2019s dataset features tweets with Twitter-shortened URLs (http://t.co), but more current tweet data could include the domains, which can then be extracted (I imagine the red cross domain would be highly correlated with disaster tweets). For more complicated algorithms, one can also consider visiting the shortened URL and scraping web page elements.", "spaCy is an open-source python library for natural language processing. It integrates well with the rest of the python machine learning libraries (scikit-learn, TensorFlow, PyTorch) and more, and uses a object-oriented approach to keep its interface readable and easy to use. Notably, it its model returns Document type data, which consists of tokens with various useful annotations (e.g. its lemma, whether it's a stopword) as attributes.", "Let\u2019s import spaCy, download the model for American English, and load the relevant spaCy model.", "Before we customize spaCy, we can take a look at how the out-of-the-box model tokenizes tweets with its default rules. I created a tweet that included a number, a contraction, a hashtag, a mention and a link.", "As shown below, out-of-the-box spaCy already breaks up contractions and gives us the relevant lemmas. It also recognizes numbers, mentions and URLs as their own tokens according to the default rules. That leaves us with hashtags, which are split into a \u2018#\u2019 punctuation and hashtag content, instead of it staying as a whole token.", "We can modify spaCy\u2019s model to recognize hashtags as entire tokens.", "spaCy\u2019s tokenizer can be modified (you can also build a custom tokenizer if you want!) by redefining its default rules. spaCy\u2019s tokenizer prioritizes rules in the following order: token match pattern, prefix, suffix, infixes, URL, special cases (see How spaCy\u2019s Tokenizer Works).", "In our case, we\u2019ll modify the tokenizer\u2019s pattern matching regex pattern (read more about regex here: A simple intro to Regex with Python) by appending '#\\\\w+', which is a pattern for the hash symbol and a word.", "We can then proceed to create a preprocessing algorithm, and put it in a function so it can be called on every tweet in the training set. In the following preprocess function, each tweet:", "We\u2019ll create a copy our de-duplicated data as a best practice, so that any pre-processing changes does not affect the original state of our training data. Then, we will initialize a python set features, which will contain all features of each tweet. In addition to all lemmas encountered via tokenization of each tweet, features will include number of hashtags (#), number of mentions (@), and number of URLs (URL).", "Using our preprocess function, we'll preprocess every tweet, updating features each time with new lemmas seen. With each tweet, the tweet's bag of words representation freq is also appended to an array of bag of words for each tweet (bow_array).", "With all lemmas encountered across all tweets collected in features, we can create a dataframe bow to represent the features of all the tweets.", "Now, let\u2019s update our dataframe with the feature values of each tweet.", "Finally, we will join our training data dataframe with our bag-of-words. pandas Dataframe\u2019s join method allows us to add columns from one dataframe to another for rows where the index matches. Note that we append '_data' as a suffix to all columns from the 'left' dataframe, which is preprocess_df. This is to avoid conflicts between the keyword given as part of the training data, and 'keyword' as a lemma-token-feature. Remember to save the preprocessed .csv file for easier next steps!", "Now that we\u2019ve pre-processed our data, there\u2019s just one last step before we can jump into using it to train our model of choice. We have split it, stratified according to the distribution of classes, into training and validation sets. Using train_test_split from sklearn.model_selection:", "Just to be sure, we can check our balance.", "If you have seen other NLP pre-processing tutorials, you\u2019ll find that a lot of their steps have been included as considerations, but not implemented here. These include removing punctuations, numbers, and stop words. However, our training dataset is small, and these steps could remove information valuable in the context of tweets and the challenge. Hence, rather than eliminate these data at the pre-processing stage, we have left them as possible ways to tune the hyper-parameters of our model.", "Through this tutorial, we have pre-processed tweets into their bag-of-words representation. However, you may choose to go a few steps further with Term Frequency \u2014 Inverse Document Frequency (TFIDF). TF-IDF stands for \u201cTerm,Information Retrieval and Text Mining, which penalizes terms that appear too often (as they become less discriminatory as features), or word vectors, which also numerically account for the word\u2019s context and semantics. Word vectors encoding will result in better performance than TFIDF encoding, which will result in better performance than bag-of-words encoding.", "We have ignored location and keyword in this tutorial, focusing entirely on tweets. You could consider encoding location by similarity, accounting for different spellings of the same place (e.g. USA vs U.S.), and missing values. You can also weight keywords heavier and see how that affects the performance of your model.", "Lastly, there may be valuable information in the URLs that we are missing out. Given that they are in shortened form, we are unable to extract the domain name or page content from the text data alone. You could consider building an algorithm to visit the site and extract the domain name, as well as scrape relevant elements on the page (e.g. page title).", "Now that we have performed basic pre-processing our dataset, we can move forward in two possible directions. You can either continue on to advanced pre-processing by performing spell checks and correction with Pyspellchecker and Mordecai, or try out and evaluate candidate machine learning models! Possible models for such classification problems include logistic regression, neural networks, and SVMs.", "[1] Kaggle, Disaster tweets classification challenge on Kaggle (2020), Kaggle", "[2] D. Becker and M. Leonard, Intro to NLP (n.d.), Natural Language Processing Course on Kaggle", "[3] D. Becker and M. Leonard, Text Classification with SpaCy (n.d.), Natural Language Processing Course on Kaggle", "[4] Yse, D. L. Your Guide to Natural Language Processing (2019), Towards Data Science", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hacker, brand strategist & product manager. APM \u201921 @ Uber. Helping students break into tech @ breakintotech.substack.com | weitingyp.xyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4d01f6669a7e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://weitingyp.medium.com/?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": ""}, {"url": "https://weitingyp.medium.com/?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "Wei-Ting Yap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F94b2766fcfe9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&user=Wei-Ting+Yap&userId=94b2766fcfe9&source=post_page-94b2766fcfe9----4d01f6669a7e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d01f6669a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d01f6669a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://unsplash.com/@visuals?utm_source=medium&utm_medium=referral", "anchor_text": "visuals"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "Real or Not? NLP with Disaster Tweets"}, {"url": "https://github.com/weiting109/disaster-tweets-classifier/blob/main/nb.ipynb", "anchor_text": "notebook and repository"}, {"url": "https://www.kaggle.com/c/nlp-getting-started/data", "anchor_text": "data description on Kaggle"}, {"url": "https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools", "anchor_text": "here"}, {"url": "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/", "anchor_text": "tuning our hyper-parameters"}, {"url": "http://t.co", "anchor_text": "http://t.co"}, {"url": "https://spacy.io/", "anchor_text": "spaCy"}, {"url": "https://spacy.io/usage/linguistic-features#how-tokenizer-works", "anchor_text": "How spaCy\u2019s Tokenizer Works"}, {"url": "https://towardsdatascience.com/a-simple-intro-to-regex-with-python-14d23a34d170#:~:text=Regular%20expressions%20are%20used%20to,projects%20that%20involve%20text%20analytics.", "anchor_text": "A simple intro to Regex with Python"}, {"url": "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/", "anchor_text": "tune the hyper-parameters"}, {"url": "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089", "anchor_text": "Term Frequency \u2014 Inverse Document Frequency (TFIDF)"}, {"url": "https://www.kaggle.com/matleonard/word-vectors", "anchor_text": "word vectors"}, {"url": "https://emma-stiefel.medium.com/standardizing-spelling-and-locations-with-python-pyspellchecker-and-mordecai-8b0fa7d44feb", "anchor_text": "advanced pre-processing by performing spell checks and correction with Pyspellchecker and Mordecai"}, {"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "Disaster tweets classification challenge on Kaggle"}, {"url": "https://www.kaggle.com/matleonard/intro-to-nlp", "anchor_text": "Intro to NLP"}, {"url": "https://www.kaggle.com/matleonard/text-classification", "anchor_text": "Text Classification with SpaCy"}, {"url": "https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1", "anchor_text": "Your Guide to Natural Language Processing"}, {"url": "https://spacy.io/usage/spacy-101", "anchor_text": "spaCy\u2019s 101: Everything you need to know"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4d01f6669a7e---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4d01f6669a7e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/spacy?source=post_page-----4d01f6669a7e---------------spacy-----------------", "anchor_text": "Spacy"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----4d01f6669a7e---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----4d01f6669a7e---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d01f6669a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&user=Wei-Ting+Yap&userId=94b2766fcfe9&source=-----4d01f6669a7e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d01f6669a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&user=Wei-Ting+Yap&userId=94b2766fcfe9&source=-----4d01f6669a7e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d01f6669a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4d01f6669a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4d01f6669a7e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4d01f6669a7e--------------------------------", "anchor_text": ""}, {"url": "https://weitingyp.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://weitingyp.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wei-Ting Yap"}, {"url": "https://weitingyp.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "39 Followers"}, {"url": "http://breakintotech.substack.com", "anchor_text": "breakintotech.substack.com"}, {"url": "http://weitingyp.xyz", "anchor_text": "weitingyp.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F94b2766fcfe9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&user=Wei-Ting+Yap&userId=94b2766fcfe9&source=post_page-94b2766fcfe9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb17ce6e4ec29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-should-extract-context-specific-features-4d01f6669a7e&newsletterV3=94b2766fcfe9&newsletterV3Id=b17ce6e4ec29&user=Wei-Ting+Yap&userId=94b2766fcfe9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}