{"url": "https://towardsdatascience.com/shortest-path-distance-with-deep-learning-311e19d97569", "time": 1683009890.588477, "path": "towardsdatascience.com/shortest-path-distance-with-deep-learning-311e19d97569/", "webpage": {"metadata": {"title": "Shortest Path Distance Approximation Using Deep Learning: Node2Vec | by Asutosh Nayak | Towards Data Science", "h1": "Shortest Path Distance Approximation Using Deep Learning: Node2Vec", "description": "This article is an implementation of a research paper titled \u201cShortest Path Distance Approximation using Deep Learning Techniques\u201d, where the authors explain a new method to approximate the shortest\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2002.05257", "anchor_text": "Shortest Path Distance Approximation using Deep Learning Techniques", "paragraph_index": 0}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1607.00653", "anchor_text": "node2vec: Scalable Feature Learning for Networks", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2Vec", "paragraph_index": 2}, {"url": "http://networkrepository.com", "anchor_text": "here", "paragraph_index": 9}, {"url": "http://networkrepository.com/mtx-matrix-market-format.html", "anchor_text": "mtx", "paragraph_index": 9}, {"url": "https://networkx.github.io/", "anchor_text": "Networkx", "paragraph_index": 10}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/blob/master/src/data_prep.ipynb", "anchor_text": "data_prep.ipynb", "paragraph_index": 31}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/blob/master/src/train.ipynb", "anchor_text": "train.ipynb", "paragraph_index": 32}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/tree/master/outputs/logs/runs", "anchor_text": "tensorboard logs", "paragraph_index": 32}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/tree/master/outputs/logs/runs/run47_smallerNN_noDO", "anchor_text": "run47 folder", "paragraph_index": 41}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/blob/master/src/fun.ipynb", "anchor_text": "fun.ipynb", "paragraph_index": 56}, {"url": "https://www.linkedin.com/in/nayakasu92/", "anchor_text": "https://www.linkedin.com/in/nayakasu92/", "paragraph_index": 58}], "all_paragraphs": ["This article is an implementation of a research paper titled \u201cShortest Path Distance Approximation using Deep Learning Techniques\u201d, where the authors explain a new method to approximate the shortest path distance between the nodes of a graph. I will explain the paper and my implementation of it. You can find the project on my GitHub account here. First I will give an overview of the method proposed in this paper, then we will go through some of the concepts used in this paper to solve the problem and finally the implementation.", "1.1 Motivation: Why do we need to use deep learning to approximate distance between nodes when we have traditional exact methods like Dijkstra\u2019s and A* algorithms? The problem with these traditional algorithms is that they are slow on very large graphs and would consume a lot of memory to store the precomputed distances. And since for most of the applications an approximation of actual distance is good enough, it encourages one to explore various ways to approximate the distances. Also, once a neural network is trained, the inference time (finding node distance) is constant (O(1)).", "1.2 Algorithm: Now that we know the \u2018why\u2019, let\u2019s look into the \u2018how\u2019. The paper uses the ideas presented in another excellent paper \u201cnode2vec: Scalable Feature Learning for Networks\u201d. In fact, I would say that some of the ideas used in the paper under discussion were already presented in Node2Vec paper (for instance, the use of binary operators to represent an edge using corresponding node embeddings was proposed in Node2Vec paper, which was extended to represent path in this paper. We will discuss embeddings later). This paper is more of an application of Node2Vec. The Node2Vec itself is an extension of Word2Vec. Word2Vec is an algorithm to represent words with embeddings (vector of numbers) in a vector space, such that the semantically similar words are located closer to each other. It\u2019s a fascinating topic in itself.", "Here is a summary of the method proposed:", "data \u2014 which holds all the data consumed by the program, both downloaded and processed.", "outputs \u2014this holds all the outputs, including text logs, tensorboard logs, model backups etc.", "src \u2014 source code is placed here.", "tests \u2014 any relevant test cases", "Note: Please note that for this project I have worked on notebooks mostly because it involved a lot of exploration and experimentation with various approaches. As such, work is not polished since I am still exploring better ways. I have tried to explain the cells as much as I could. Feel free to reach out to me if something is unclear.", "I have used Facebook datasets from here. The downloaded graph data is in \u201cmtx\u201d format. This is just another format for sharing matrix data. It looks like this:", "The first line is called \u201cheader\u201d and defines some properties, which are used to decide how to parse the file to form the matrix. The line below it (size line) defines the size of data. The header always begins with \u201c%%MatrixMarket\u201d and the four fields that follow it are object, format, field, symmetry. Object field can be \u2018matrix\u2019 or \u2018vector\u2019 (in our case it will be matrix), format can be \u2018coordinate\u2019 or \u2018array\u2019. \u201ccoordinate\u201d format stores only non-zero values in the mtx file, so the \u201csize line\u201d holds, number of rows, number of columns in the matrix and the number of non-zero entries. If format is \u201carray\u201d, the \u201csize line\u201d is of form [number of rows number of columns]. The next section is \u201cdata lines\u201d, which hold the actual data. If format is \u201ccoordinate\u201d there would be \u201cnumber of non-zero entries\u201d number of lines of data, else in case of \u201carray\u201d you should expect {number of rows * number of columns} number of data lines, where each line represents an edge between two nodes. The data lines could have a 3rd column \u201cedge weight\u201d for weighted graphs. The rest of the details can be found in above \u201cmtx\u201d link. I have shared this information to get you acquainted with it a little bit so that if you face parsing errors you can write your own implementation. Most of the times we may not need to, as Scipy supports reading/writing mtx format. But here we will need to first convert mtx to edgelist format as both Networkx (the package I use to process graph data) and Node2Vec script use this format.", "Edgelist is just the data lines after stripping away the header, comments and size lines, like this:", "To calculate the node embeddings for our graph we will use the script provided by Node2Vec authors. But let me first introduce the algorithm briefly, since it\u2019s quite interesting and also because it\u2019s such an integral part of what we are doing here. Even if we don\u2019t have to implement the algorithm, it shouldn\u2019t stop us from learning about it. But you can choose to skip to next section.", "The idea is to sample the neighborhood of each node in the graph, also called a \u201cwalk\u201d (which is a fancy way of saying: collect the nearby nodes), convert the visited nodes\u2019 list to string (so that now you have samples of form ([list of nearby nodes as strings including source node])) and then pass all such lists to Word2Vec model to train and learn embeddings just like we train on list of sentences. But the best part of Node2Vec is how it samples the neighborhood. The authors argue that the classical approaches like BFS and DFS lie on two opposite ends of the spectrum. BFS tends to sample nodes closer to source node which causes the learned embeddings to capture structural similarity of nodes better (for e.g. whether a node acts as a hub/center in its neighborhood). But it only samples a small portion of the graph. On the other hand, DFS, tends to sample nodes far off from the source and thus learning from such \u2018walks\u2019 leads to embeddings which are better at capturing macroscopic view of graph (connectivity and \u201chomophily\u201d as then mention in the paper) but fail to capture finer details since walks are of finite length and they have a lot of grounds to cover.", "Hence, they propose a new way of sampling a node\u2019s neighborhood called \u201c2nd order Random Walk\u201d. Just to make it clearer, we are doing all this just to sample node neighborhood in a controlled way so that the walks include qualities of both BFS and DFS. Let\u2019s consider a walk \u2018c\u2019 of fixed length \u2018l\u2019 and assume you have started your walk from node \u2018u\u2019. Assume that you have traveled from node t to v. Instead of choosing the next node randomly or based on the edge of weights, they use the following probability distribution:", "which means if previous node in the walk was v then the probability of next node being x is given by pi_{vx}/z (can\u2019t use inline latex because Medium screws it up every time I edit something), if nodes v and x are connected by an edge, else 0. pi_{vx} is the unnormalized transition probability and z is the normalizing constant (which could be the sum of all the probabilities of edges from node v). They define pi_{vx} as :", "where d_{tx} is the shortest path distance between t and x. Let\u2019s understand the role of p and q, because these are the two parameters which control the nature of random walk (BFS or DFS), hence the term \u201c2nd order Random Walk\u201d.", "If we recap a bit, we reached at node v from node t, and now we need to decide which node to visit next from v. If you set p to be a low value then the walk would prefer to revisit the previous nodes. How, you ask? Imagine that you have reached from t to v, now you have following options\u200a\u2014\u200ago to other neighbors of v (x1, x2\u2026xn) or go back to t (don\u2019t forget \u2018t\u2019 is also a neighbor), in which case d_{tx} is zero, since x here is t itself (corresponds to the first option 1/p in the above equation). So if p is a very low value then 1/p would be quite large and random walk would prefer to go back to previous nodes and thus simulate the behavior of BFS. Please refer to the (ugly) image below to visualize this:", "The paper has a similar image but without distance values. Similarly, if we choose a lower value for q then we are encouraging the algorithm to venture farther away from t by assigning higher probability to 3rd option in equation defining alpha. This kind of decision is made for every node in the walk. And several such walks are done.", "If you have understood above explanation, you have an idea of how node2vec random walk works. You can also check the implementation of it on author\u2019s GitHub repository (but it\u2019s in python 2 format). In my project, I have also included the author\u2019s script but converted to Python3 format.", "Running the script is quite simple. You can choose to run with all the default values for its parameters or change it to suit your needs. I ran with default values:", "Just modify the input and output paths.", "From previous step we have a file with node and their embeddings in following format:", "First two numbers represent the number of nodes and dimension of embeddings respectively. Our next step is to select a certain number of nodes in the graph as landmarks and compute their distances from all the rest of the nodes. This would give us (number of landmarks*(number of nodes-1)) number of samples. We choose landmarks because finding distances for all the nodes would require a lot more compute.", "\u201cdistance_map\u201d dict holds distance of every node from a given landmark (as key). Now we need to get the corresponding node2vec embedding for each node/landmark pair and combine them to form a single embedding.", "Here emd_map is a dict which holds each node as key and it\u2019s embedding as value.", "Next, we need to form numpy ndarrays from embedding-distance dict like shown above. Please note two things from the output of the cell above \u2014 the number of samples is not equal to num_of_landmarks*(num_of_nodes-1). This is because I have ignored node pairs which have inf distance i.e. no path connecting them. Second, the size of training array is massive at ~927MB, since it is \u2018float64\u2019 type array. I have converted them to float32 to save space.", "As you can see we could save ~50% space. If you are worried about the precision loss due to this conversion, you can verify the data loss:", "This seems insignificant. Next, let\u2019s look at the distribution of target variable.", "As you can see distance values 2 and 3 really dominate the data. On the graph 6, 7, 8 can\u2019t be seen but they are present in the data, with the only sample for distance 8 which I dropped. Also note that in the paper, they ignore samples with distance value 1, but I have included them in training.", "Since data is massively imbalanced, I have stratified the train/test split:", "I have saved the split data so that this preprocessing can be done once only. Whew!! Dataset formation was a big portion of this project and finally I have explained the major steps. I may have missed some, for more details you can check the data_prep.ipynb in the project repository.", "Now comes the exciting part. The code for training is in train.ipynb file. You will notice that some of the cells look unfinished/note-to-self. I have left these deliberately to let the interested readers gain experience from my failed attempts and the steps taken after each failure. I have even uploaded the tensorboard logs (which include changes made, result, notes etc) into the GitHub repo so that all the history is available for me and others to follow later. If you are not interested in the logs, you can delete the folders without any issues.", "Since the data is skewed, I had to oversample the minority target values (I am not using the term \u201cclass\u201d here because I have trained a regression model).", "I have first undersampled the majority values and then oversampled the minority values. The intuition was to oversample the minority distance values to make them comparable to number of samples of majority values, while keeping the overall data size as small as possible. The fraction values \u201c0.7\u201d is not really calculated/derived from anywhere, it just seemed reasonable by looking at the frequencies of each distance values. And yes\u2026 don\u2019t forget to shuffle the data after over/under sampling. It may seem like a trivial thing, something you can ignore but as it turned out many of the batches had same distance values (like all 1s or 2s etc) which made training impossible!", "Now, let\u2019s define a baseline, a simple model and it\u2019s result to which we can compare to see how good we did:", "Linear Regression does surprisingly good! 50% is a good baseline to compare with, considering that the chance prediction in this case is ~14% (1/7).", "For all of my previous projects, I had used Keras but recently I switched to PyTorch and I haven\u2019t regretted it ever since. I was working on another project which involved writing a custom loss function which required some calculations on output of intermediate layer of the model and other custom stuffs. If you have ever tried such things with Keras, you know it\u2019s not straightforward. Even with eager execution I couldn\u2019t get it to work. To make matters worse, one fine day, Keras started throwing CUDA compatibility errors, and it wouldn\u2019t go away no matter how many times I built a new environment with required drivers. This was my breaking point. With PyTorch you may have to write a little bit of extra code for training loop but it\u2019s worth it. Now, I am not saying these issues couldn\u2019t have been solved with more effort, but, in a world where you have PyTorch, why break your head? So that concludes my rant about why I switched to PyTorch.", "But for readers who are not acquainted with PyTorch, don\u2019t be disheartened. Almost all the components used are available in Keras (except for may be Cyclic LR scheduler, but there are implementations of it that you can use). Explaining PyTorch would be out of the scope of this article.", "The model which gave the best result (yet) has following configuration (with Cyclic LR Scheduler, RMSProp optimizer and Poisson Loss):", "It\u2019s a 5 layer (3 hidden layers) model with dropouts, trained with cyclic learning rate.", "You may not find this model in the GitHub code if I have tried other configurations after this and thus parameters could have changed. But you could find the corresponding model and parameters in run47 folder. Please note that this configuration is different from what they have mentioned in the paper. As with almost every other paper I have read, they have skipped the low level details of the NN.", "Here is how you initialize Data Loaders for train/val/test data:", "Now we are all set to train the model:", "People who have only worked on Keras/Fastai yet, don\u2019t get scared. Code to train PyTorch model is not always this big. I am doing a lot of other stuff like early stopping, checkpoint saves etc. It\u2019s definitely more complicated than Keras, but nothing you can\u2019t learn in a day or two. But like I have mentioned before all these components are already there in Keras (including Poisson Loss) so you can easily try it out on the framework of your choice.", "I trained the model for ~110 epochs.", "Although this was a regression problem, I have still recorded accuracy because I find it more intuitive (it\u2019s not a metric, just something I used to compare models). It\u2019s not a boastful result but it\u2019s not bad. Also, I think, there is lot of margin for more improvements, which I will mention later.", "If you are wondering why does longer path distances have worse accuracy, similar observations were made by authors as well. Here\u2019s what they say:", "Observe that the larger errors caused by longer paths utilizing node2vec embeddings. In one hand, we do not have enough samples for longer distances in the training set. On the other hand, node2vec fails to learn structural features of faraway nodes.", "Note: There is one thing that\u2019s bugging me about this model. Despite using exp_range as CLR mode, the learning rate didn\u2019t change throughout the training (at least as per the plot). This happens for gamma value 0.95 always. Need to look into it. Or if you have faced this before and have any solutions, you are welcome to share here.", "In retrospect, here are some of the things that improved the model performance:", "There are a lot of things that could be explored and thus requires a lot of patience. Here is a non-exhaustive list:", "\u201cbinary operators do not have a consistent behavior over different datasets and different dimension sizes. For instance, the average operator outperforms others in Facebook graph while concatenation works better for Youtube dataset\u201d", "5. Feature selection: We could try to reduce the number of features by selecting only important features. (not used in the paper)", "6. Better sampling techniques: like KMeansSMOTE, SMOTE, Cluster Centroids etc. I tried using these techniques, but they took too long to finish.", "7. There is a large gap in training loss and val loss, suggesting that we could reduce the over-fitting to get a better result. Maybe try a smaller model and see if it\u2019s capable of learning.", "Well, this has already become a long post, so I will just end it without much blabbering. This has been an interesting project with lots of cool concepts to learn. I couldn\u2019t accommodate many of them here, but I have recorded and will record more fun facts about this problem, as and when I find them, in fun.ipynb file in the project. Thanks for sticking till the end. Happy coding!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning | Android | Bibliophile | Aspiring Writer. Feel free to connect on LinkedIn https://www.linkedin.com/in/nayakasu92/ | Twitter @i_m_brute"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F311e19d97569&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----311e19d97569--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----311e19d97569--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://asutoshnayak.medium.com/?source=post_page-----311e19d97569--------------------------------", "anchor_text": ""}, {"url": "https://asutoshnayak.medium.com/?source=post_page-----311e19d97569--------------------------------", "anchor_text": "Asutosh Nayak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1eb9039db85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&user=Asutosh+Nayak&userId=e1eb9039db85&source=post_page-e1eb9039db85----311e19d97569---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F311e19d97569&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F311e19d97569&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/2002.05257", "anchor_text": "Shortest Path Distance Approximation using Deep Learning Techniques"}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1607.00653", "anchor_text": "node2vec: Scalable Feature Learning for Networks"}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2Vec"}, {"url": "https://github.com/aditya-grover/node2vec", "anchor_text": "implementation"}, {"url": "http://networkrepository.com", "anchor_text": "here"}, {"url": "http://networkrepository.com/mtx-matrix-market-format.html", "anchor_text": "mtx"}, {"url": "https://networkx.github.io/", "anchor_text": "Networkx"}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/blob/master/src/data_prep.ipynb", "anchor_text": "data_prep.ipynb"}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/blob/master/src/train.ipynb", "anchor_text": "train.ipynb"}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/tree/master/outputs/logs/runs", "anchor_text": "tensorboard logs"}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/tree/master/outputs/logs/runs/run47_smallerNN_noDO", "anchor_text": "run47 folder"}, {"url": "https://github.com/nayash/shortest-distance-approx-deep-learning/blob/master/src/fun.ipynb", "anchor_text": "fun.ipynb"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----311e19d97569---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/node2vec?source=post_page-----311e19d97569---------------node2vec-----------------", "anchor_text": "Node2vec"}, {"url": "https://medium.com/tag/graph?source=post_page-----311e19d97569---------------graph-----------------", "anchor_text": "Graph"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----311e19d97569---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F311e19d97569&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&user=Asutosh+Nayak&userId=e1eb9039db85&source=-----311e19d97569---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F311e19d97569&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&user=Asutosh+Nayak&userId=e1eb9039db85&source=-----311e19d97569---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F311e19d97569&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----311e19d97569--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F311e19d97569&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----311e19d97569---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----311e19d97569--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----311e19d97569--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----311e19d97569--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----311e19d97569--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----311e19d97569--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----311e19d97569--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----311e19d97569--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----311e19d97569--------------------------------", "anchor_text": ""}, {"url": "https://asutoshnayak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://asutoshnayak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Asutosh Nayak"}, {"url": "https://asutoshnayak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "563 Followers"}, {"url": "https://www.linkedin.com/in/nayakasu92/", "anchor_text": "https://www.linkedin.com/in/nayakasu92/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1eb9039db85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&user=Asutosh+Nayak&userId=e1eb9039db85&source=post_page-e1eb9039db85--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa95bcc698252&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshortest-path-distance-with-deep-learning-311e19d97569&newsletterV3=e1eb9039db85&newsletterV3Id=a95bcc698252&user=Asutosh+Nayak&userId=e1eb9039db85&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}