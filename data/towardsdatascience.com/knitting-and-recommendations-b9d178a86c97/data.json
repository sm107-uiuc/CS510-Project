{"url": "https://towardsdatascience.com/knitting-and-recommendations-b9d178a86c97", "time": 1682995351.664465, "path": "towardsdatascience.com/knitting-and-recommendations-b9d178a86c97/", "webpage": {"metadata": {"title": "Knitting and Recommendations. How Computers Think: Part Three | by Simon Carryer | Towards Data Science", "h1": "Knitting and Recommendations", "description": "I don\u2019t, as it turns out, know nearly as much as I need to about knitting. Looking back, it seems absurdly naive of me to assume that I could pick up in a couple of hours of browsing the internet all\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/linear-regression-and-lines-of-succession-451569c39016", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://github.com/SimonCarryer/recommend", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://www.ravelry.com/patterns/library/mr-dangly", "anchor_text": "Mr. Dangly", "paragraph_index": 34}, {"url": "https://www.ravelry.com/patterns/library/socktopus", "anchor_text": "Socktopus", "paragraph_index": 34}, {"url": "http://www.ravelry.com", "anchor_text": "www.ravelry.com", "paragraph_index": 34}], "all_paragraphs": ["I don\u2019t, as it turns out, know nearly as much as I need to about knitting. Looking back, it seems absurdly naive of me to assume that I could pick up in a couple of hours of browsing the internet all the knowledge I\u2019d need to make meaningful statements about such a complex field. It is a dangerous kind of hubris to believe that a good dataset and a smart algorithm can entirely substitute for expert knowledge, yet it is all too common. It is born of a combination of bullish confidence in the power of mathematics to unravel complex problems, and a kind of chauvinism about the relative complexities of \u201chard\u201d engineering and mathematical problems compared to \u201csoft\u201d social and interpersonal problems. It results in a lot of ill-conceived start-ups, and a lot of well-intentioned attempts to address intractable social issues with a phone app. Happily, my own foray into blithe ignorance had substantially lower stakes.", "My plan for this essay was to demonstrate two kinds of recommendation algorithm. Recommendations are maybe the most common, and definitely the most visible manifestation of artificial intelligence on today\u2019s internet. Large online retailers, trying to squeeze every possible dollar out of browsers, will try to tempt you to an extra purchase by showing you ostensibly relevant items. Online content providers, trying to keep you on their site, prompt you with another article or video, on what they guess is a similar topic. Music sites have made maybe the deepest investments in providing good recommendations. Understanding the complexities of musical taste, when the factors that make one song great and another terrible are so nebulous and intangible, is a problem they are all competing to solve.", "Broadly speaking, recommendation algorithms are about calculating similarity. They extrapolate from one or more items that a user likes, to find a set of similar items that the user will also like. In terms of how they calculate this similarity, most of these recommendation systems fall into two broad categories. The first is perhaps the most obvious. \u201cContent-based\u201d recommendations assume that two items are similar if they have similar qualities. To make a content-based recommendation, you need to know a lot about the things you\u2019re recommending. That\u2019s easy if you\u2019re recommending things with clear, comparable and quantifiable properties. Comparing one refrigerator to another, for example, is straightforward: Size, wattage, freezer compartment or no, ice-cube maker, etc. Comparing a refrigerator to a blender is more difficult. Content-based recommendations also often miss more subtle context. If you have recently purchased a refrigerator, the algorithm is liable to assume that you must really like refrigerators, and therefore that you want to start a collection of them.", "The second kind of algorithm, called \u201ccollaborative filtering\u201d is more subtle. It assumes that two things are similar if the same people like them. In other words, if you and I both like refrigerators, and we both like blenders, then that means that, to some extent, refrigerators and blenders must have something in common. Obviously, if you look at a sample of just two people, this approach is subject to considerable error. That you and I happen to share an enthusiasm for both country music and nature documentaries does not imply a strong connection between those two things. But with a big enough group of people, it becomes remarkably reliable. By aggregating the opinions of a large numbers of users, spurious connections tend to be overwhelmed by the larger numbers of more meaningful connections \u2014 in general, people shopping for refrigerators are more likely to also look at blenders than they are to look at, say, an arc-welder. These algorithms are often much better than content based approaches at identifying subtle cultural markers that are difficult to label. For this reason they are often used for recommending things like music, or television shows. The difference between, for example, The Beatles and The Monkees might be difficult for a content-based algorithm to parse. They\u2019re both four-piece pop groups from the 1960\u2019s. They have substantially the same instrumentation, a similar upbeat sound, and they both made an unfortunate late-career foray into psychedelica. An algorithm looking solely at these facts might assume that these two groups are very similar. But a fan of the Beatles is unlikely to be pleased to hear the Monkees pop up on their playlist. A collaborative filtering algorithm is able to detect these differences \u2014 and might instead suggest something that sounds quite different from the Beatles, but has a similar fanbase. Collaborative filtering algorithms have a couple of weaknesses. Firstly, the fact that they rely on user behaviour means that they\u2019re vulnerable to over-interpreting unusual outliers. A small number of users with unusual habits can sometimes skew the data in unpredictable ways. Secondly, and related to the first point, they need a lot of interactions with a new item before they can make a sensible suggestion. An item that has only been seen by a handful of users will have a suggestion based on a small dataset, and will therefore potentially reflect the idiosyncrasies of that small set of users. This is called the \u201ccold start problem\u201d, because the algorithm needs a lot of data to get \u201cwarmed up\u201d to provide good recommendations.", "With those two algorithms in mind, let\u2019s get back to my embarrassing failure. To demonstrate these two approaches to recommendations, I needed a large dataset of items, information about some properties of those items (to form the basis of the content based algorithm), and also information about users, and their interactions with these items (to build the collaborative filtering algorithm). After some time sifting about the internet and talking to some friends, I settled on Ravelry, a website for sharing knitting patterns, as the source of my data. Ravelry is probably the internet\u2019s largest collection of knitting and crochet patterns online, and its vast numbers of users upload, comment on, share, download, and save hundreds of thousands of patterns, all over the world, all year round. They have a huge collection of exactly the kind of data I needed. One very audacious email later, and the extremely helpful people at Ravelry were setting me up with the means to download as much of this data as I needed.", "Now. This is where I should have paused. Taken stock. Thought about the project I was embarking on, made an honest assessment of my knowledge of the subject matter, and recruited some expert assistance. Reader, I did not. The consequences of this would not become apparent until much later.", "To help users find patterns on their site, Ravelry records a great deal of \u201cmetadata\u201d. This includes things like the \u201cdifficulty rating\u201d of the pattern, keywords about the style, if the pattern is knitting or crochet, if it\u2019s a jumper or a pair of mittens, as well as a whole host of more obscure metrics. For our purposes, this would form the basis of the \u201ccontent based\u201d algorithm. We\u2019re going to build an algorithm that can, given one knitting pattern, find another that is similar, based on the metadata of those patterns. Ignoring the more esoteric information available on the site, I downloaded the most important metrics for several hundred patterns, and went to work preparing the data.", "Here\u2019s how the major features of a pattern \u2014 in this case for an adorable knitted soft toy \u2014 appears on Ravelry\u2019s site:", "To turn this blob of unstructured information about a knitting pattern into a dataset suitable for our algorithm requires a series of mathematical tricks and transformations. To start, with the exception of \u201cDifficulty\u201d, all of this data is in the form of English words, which are as meaningful to a computer as a string of binary ones and zeroes are to you and I. Our first task is to turn these words into numbers. To do that we use a trick called \u201cOne-hot\u201d encoding.", "One-hot encoding gets its name from a technique in electronics engineering. Imagine you have a machine that can be in one of two states: On, or off. You can easily indicate the state of the machine with just one light. If the light is on (or \u201chot\u201d), the machine is on. If the light is off, so is the machine. But what if the machine has three states? Say, on, off, and \u201cwarming up\u201d. Now it\u2019s more complicated to show the machine\u2019s state. You need a second light. No lights means the machine is off, the first light means the machine is warming up, and if the second light is on, the machine is ready. If both lights are on, something\u2019s wrong with your machine. For every state your machine can be in, you need to add an extra light. This is not the most efficient way to encode information \u2014 you need a lot of lights if the machine is very complex \u2014 but its advantage is its reliability. It\u2019s hard to accidentally signal the wrong state, and it\u2019s easy to tell if something\u2019s gone wrong \u2014 there\u2019s more than one light on at a time.", "We can use the same technique to encode our category data. Instead of a machine that can be in various states, we have a single column of data, each row of which indicates one of several categories. Instead of lights, we translate our single column of data into multiple columns, one for each of the categories, \u2018shawl-wrap\u2019, \u2018pullover\u2019, \u2018beanie\u2019, \u2018cardigan\u2019, \u2018scarf\u2019, etc. For each row, all of these columns have a value of zero, except for the one that corresponds to the category of our item, which is set to one. We can determine the category of each item in the dataset by looking at which column has a value of one.", "The keyword data \u2014 things like \u201cfringe\u201d, \u201cgranny-squares\u201d and so on \u2014 receives essentially the same treatment: One column for each possible keyword, with each row containing a zero if that item does not include that keyword, and a one if it does. This creates a very \u201cwide\u201d table, with a lot of columns. Our stuffed friend, Mr. Dangly from above, might be represented (in part) by the following row in our table:", "Now that we\u2019ve encoded this data, we can use it to make content based recommendations.", "Recommendations of this sort are essentially about calculating similarity. We want to find the most similar pattern to Mr Dangly. To do that, we\u2019re going to use a measure called \u201cEuclidean distance\u201d.", "Euclidean distance measures distance just as we\u2019d measure distance in real life \u2014 the length of a line between two points. Just as we can calculate the distance between two sets of latitude and longitude coordinates on a map, we can calculate the distance between two rows of numeric data in a table. This is easy to visualise if we imagine our dataset has only two columns, \u201cDifficulty\u201d (2 for Mr. Dangly), and \u201cAnimal\u201d (1 for Mr. Dangly \u2014 he\u2019s a monkey). We can plot this on a two-dimensional chart, and compare it to three other hypothetical patterns: \u201cMr Tricky\u201d \u2014 a stuffed animal, like Mr. Dangly, but more complex to make, with a difficulty rating of 3; \u201cEasy Jumper\u201d \u2014 easy to make, like Mr. Dangly, but not a stuffed animal; and \u201cTricky Gloves\u201d \u2014 not like Mr. Dangly at all, neither easy to make, nor a stuffed animal.", "Our data for these hypothetical patterns looks like this:", "The distance calculations become clear when we draw them on a two-dimensional chart:", "The euclidean distance between Mr. Dangly and these three other patterns is literally the length of the line between them, just as you\u2019d measure it with a ruler. The \u201cTricky Gloves\u201d, by virtue of having a longer, diagonal line to Mr. Dangly, has the largest euclidean distance, and is the least similar. \u201cEasy Jumper\u201d and \u201cMr. Tricky\u201d are equally distant from Mr. Dangly \u2014 they\u2019re equally similar.", "The fascinating \u2014 and confusing \u2014 thing about euclidean distance is that the calculation holds for any number of extra dimensions. You can imagine adding a third, height axis to this chart, representing maybe one of the keywords. The line in three-dimensional space would be longer for those patterns that did not share the keyword. It gets harder (for me, at least) to visualise this in more than three dimensions, but I am assured that the maths is entirely sound, and practice bears this out.", "This chart also exposes one of the problems with this approach. You\u2019ll notice that the euclidean distance from Mr. Dangly to both \u201cMr Tricky\u201d and \u201cEasy Jumper\u201d are exactly equal. Our algorithm assumes that both of these patterns are equally similar to Mr. Dangly, that they are equally good recommendations. Is this a good assumption? Is the difference in one point of difficulty equivalent to the difference between a stuffed animal and a jumper? Intuitively, it feels like no. But how can I know? If these two differences are not of the same magnitude, how different are they? How much should I weight difference in difficulty compared to a difference in category? I am completely unqualified to answer. This is the point at which I (far, far too late) elected to enlist some outside help, so I could understand more about how knitters choose patterns. The result was\u2026 humbling.", "It turns out that all of the obscure metrics I had previously discarded, like \u201cyarn weight\u201d, \u201cneedle gauge\u201d and \u201cyardage\u201d were in fact vitally important to many knitters. I\u2019ve been a fool! Knitters will often look for patterns that work with the yarn they already have, rather than purchasing new yarn for each pattern. That means that their choice of patterns is often constrained by the kind of yarn required to make the pattern. Leaving them out of my algorithm means that the recommendations it generates are often superficially acceptable, but don\u2019t reflect knitters\u2019 actual preferences at all.", "To fix these issues, first I had to go back to the source of the data and, cursing myself, download it all again, including the extra metrics I now realised were vital to the analysis. Then, in consultation with my hastily-recruited knitting experts, I created weightings \u2014 a value to scale each metric by to reflect its importance to knitters in choosing patterns. With these improvements, I could make recommendations that started to make sense to actual knitters.", "The final algorithm chiefly considered the craft of the pattern (whether it was knitting, crochet, or something else), the category, the yarn weight, and the difficulty. But it also took into account keywords, the needle gauge, and a host of other factors. For each of these, it would calculate a \u201cdistance\u201d between patterns. The more factors they had in common, the smaller the distance.", "For Mr. Dangly, the knitted soft-toy monkey from above, this algorithm recommended the \u201cSpring Collection\u201d, a set of soft toy patterns including a hedgehog, a frog, a bunny, and a lamb. They were similarly easy to knit, employed similar techniques \u2014 a seamed construction, and use of fringing (for Mr. Dangly a dapper tuft of hair, and for the Spring Collection a hedgehog\u2019s spines). In many respects, this is a very good recommendation. But there was something not quite right. Mr. Dangly has a certain eccentricity about him, a degree of whimsy or style that makes him stand out. By contrast, while superficially similar to Mr Dangly, the Spring Collection are missing his unique charm. They seem a little bland, and slightly twee. I feel like friends of Mr. Dangly are unlikely to get on with the members of the Spring Collection. The algorithm has made a suggestion which is correct on paper, but which misses some of the nuance of the patterns. It misses something that no human could fail to see. It misses their personality.", "That left the collaborative algorithm to try to do better. Collaborative filtering algorithms are very different to their content-based cousins. Where our content-based algorithm uses metadata about the patterns themselves, a collaborative filtering approach looks at information about the people who interacted with those patterns. Ravelry allows users to \u201cfavourite\u201d patterns, and store a list of their favourites. These lists are public, which meant I was able to download lists of the \u201cfavourite\u201d patterns of several hundred thousand Ravelry users. Individually, these lists are idiosyncratic and not very informative. Someone who likes a chunky knitted scarf might also have an interest in novelty tea cosies, and this shouldn\u2019t imply anything to us about either scarves or chintzy home decor. But in aggregate, in the kinds of volumes I was able to retrieve from Ravelry, this information can become very useful indeed. Because collaborative filtering uses the comparatively more complex and subtle information about human behaviours and preferences, my hope was that I\u2019d be able to more accurately capture something about the elusive Mr. Dangly.", "In many ways this algorithm was much more simple to construct, although the volume of data was much larger. Where the content-based algorithm used a dataset in which each column represented some aspect of the pattern, for the collaborative algorithm, I built a dataset in which each column represents a user, and the values for each row reflected whether that user had \u201cfavourited\u201d that pattern:", "With this dataset, we can calculate similarity in exactly the same way as we did with the content-based algorithm. Each user is a \u201cdimension\u201d in the euclidean distance calculation. The patterns that are liked by the same users are considered similar.", "The first results of this algorithm were, to my eye, not promising. A tiny knitted purse was, according to the algorithm, very similar to a lace-edged washcloth. A pair of cabled socks was similar to an intricate shawl. It made no sense, until I spoke to my knitting experts, who pointed out more subtle similarities that my untrained eye had missed. The purse and the washcloth were both small projects that used a variety of techniques, patterns a new knitter might use to develop their skills. The socks and the shawl both used two colours of fine yarn, which might be preferred by someone looking to use up their existing stock of wool. But other recommendations were baffling even to my expert knitters. Patterns that had very few interactions \u2014 new patterns or obscure ones \u2014 tended to get idiosyncratic or plain wrong recommendations, the product of the algorithm having very little information on which to base its similarity calculation. This is a weakness of the collaborative algorithm \u2014 the \u201ccold start problem\u201d \u2014 rearing its head. There are some mathematical tricks you can use to get around this problem, and I ended up using a lot of these (I won\u2019t go into them here in any detail), but the algorithm continued to give poor recommendations for any item without a good number of user interactions.", "But how did it fare with our friend Mr. Dangly? Fortunately, Mr Dangly\u2019s peculiar charms had garnered him sufficient attention to create a good set of recommendations. And the leading recommendation was an ideal illustration of the strengths of a collaborative recommendation. \u201cSocktopus\u201d is, as the name implies, an octopus who wears socks. He\u2019s easy to make, and uses some similar techniques to Mr. Dangly, but is also missing some of Mr Dangly\u2019s features \u2014 there\u2019s no seam, and no fringing. But crucially, the Socktopus has something of the same air of whimsy, the same sense of personality, as Mr. Dangly. The collaborative algorithm has found something that the content-based algorithm hasn\u2019t \u2014 not just the raw information, but how people relate to that information. It understands something about their meaning.", "For this reason, collaborative algorithms are hugely popular with anyone trying to recommend complex cultural products, whether that is books, movies, or music, or even knitting patterns. But when online platforms make extensive use of collaborative recommendations to guide what content they show to users, they risk falling victim to another pernicious weakness of this kind of algorithm. Collaborative algorithms are very good at finding subtle connections between items, at defining the boundaries of taste. They can distinguish between sub-genres of music, so that Death Metal fans are never offended by hearing Doom Metal (which, I am assured, is a totally different thing). They can finely parse topics of news articles, so that someone who follows their local sports team is not bothered with local politics. But this policing of boundaries comes at a cost. They can create a \u201cbubble\u201d effect.", "A bubble effect is when users are only shown a small slice of a much larger pool of content, and never become aware of anything outside their tiny portion. When an algorithm is driven by what users interact with, and users only see what the algorithm shows them, there is a feedback loop. Users interact with what they are shown, and they are shown what they (or users like them) interact with. Without care, users can find themselves wrapped in a cocoon of \u201csafe\u201d content, only ever seeing the same sorts of things. This is a risk for online platforms commercially. If users never discover new things they might like, and if they never have an opportunity to expand their tastes, they get bored and move on. But this also has a more subtle social risk. In a feedback loop, people are only shown content that suits their taste, that they agree with, content that fits their existing worldview. It is unpleasant to encounter things which challenge what you believe about the world; to hear music you don\u2019t like, watch films about things you don\u2019t understand, or read news articles about terrible events. But encountering and attempting to understand new and challenging things is how we learn to accommodate and cooperate with people different from ourselves. By walling ourselves away with like-minded people, seeing only things we like, we wall ourselves off from the wider world.", "It\u2019s easy to make assumptions about something when you don\u2019t know a lot about it. I underestimated the complexity and nuance of knitting and its enthusiasts. I\u2019d never really encountered it before, and it was very uncomfortable to have that ignorance exposed. Both collaborative and content-based recommendation algorithms can only extrapolate patterns from what they\u2019ve seen in the past. They, like me, are apt to make assumptions based on what they already know. This is not a problem with the algorithms themselves \u2014 the maths is unimpeachable. The problem is with the people building the algorithms, and with the people using them. My downfall with the content-based algorithm was assuming that a lot of data and some clever maths were enough to understand a complex field. The risks with the collaborative algorithm are more subtle, but they\u2019re also born of a kind of arrogance, a belief that we can completely anticipate a users\u2019 needs, that they are as simple as finding something like what they already enjoy. In both cases, embracing complexity, and adopting a more holistic approach, can help mitigate these risks.", "This is a theme we\u2019ve seen with the previous algorithms, and we\u2019ll see again in future essays. It\u2019s a common feature of machine learning algorithms. They are only as clever as the data that was used to create them. If they are to truly learn, truly create something new, rather than just reflect our existing knowledge and biases, then it is us that must teach them. It is us that must confront our ignorance. It is us that must learn.", "The previous article in this series, \u201cLinear Regression and Lines of Succession\u201d is available here. Code for this article can be found in my github, here. The next article will be published in April.", "To make your own Mr. Dangly or Socktopus, go to www.ravelry.com (account required).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb9d178a86c97&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@simoncarryer?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@simoncarryer?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "Simon Carryer"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F29ad259db280&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&user=Simon+Carryer&userId=29ad259db280&source=post_page-29ad259db280----b9d178a86c97---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb9d178a86c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb9d178a86c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/linear-regression-and-lines-of-succession-451569c39016", "anchor_text": "here"}, {"url": "https://github.com/SimonCarryer/recommend", "anchor_text": "here"}, {"url": "https://www.ravelry.com/patterns/library/mr-dangly", "anchor_text": "Mr. Dangly"}, {"url": "https://www.ravelry.com/patterns/library/socktopus", "anchor_text": "Socktopus"}, {"url": "http://www.ravelry.com", "anchor_text": "www.ravelry.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b9d178a86c97---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/knitting?source=post_page-----b9d178a86c97---------------knitting-----------------", "anchor_text": "Knitting"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b9d178a86c97---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/science-communication?source=post_page-----b9d178a86c97---------------science_communication-----------------", "anchor_text": "Science Communication"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b9d178a86c97---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb9d178a86c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&user=Simon+Carryer&userId=29ad259db280&source=-----b9d178a86c97---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb9d178a86c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&user=Simon+Carryer&userId=29ad259db280&source=-----b9d178a86c97---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb9d178a86c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb9d178a86c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b9d178a86c97---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b9d178a86c97--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b9d178a86c97--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b9d178a86c97--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@simoncarryer?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@simoncarryer?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Simon Carryer"}, {"url": "https://medium.com/@simoncarryer/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "124 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F29ad259db280&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&user=Simon+Carryer&userId=29ad259db280&source=post_page-29ad259db280--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F29ad259db280%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknitting-and-recommendations-b9d178a86c97&user=Simon+Carryer&userId=29ad259db280&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}