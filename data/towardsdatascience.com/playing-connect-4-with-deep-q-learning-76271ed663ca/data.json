{"url": "https://towardsdatascience.com/playing-connect-4-with-deep-q-learning-76271ed663ca", "time": 1683009885.958804, "path": "towardsdatascience.com/playing-connect-4-with-deep-q-learning-76271ed663ca/", "webpage": {"metadata": {"title": "Playing Connect 4 with Deep Q-Learning | by Lee Schmalz | Towards Data Science", "h1": "Playing Connect 4 with Deep Q-Learning", "description": "Deep Q-Learning may be one of the most important algorithms in all of Reinforcement Learning as it lacks limitation on the observations it can make and the actions it can take within complex\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/connectx", "anchor_text": "https://www.kaggle.com/c/connectx", "paragraph_index": 0}, {"url": "https://oeis.org/A212693", "anchor_text": "oeis.org/A212693", "paragraph_index": 5}, {"url": "https://pypi.org/project/kaggle-environments/", "anchor_text": "https://pypi.org/project/kaggle-environments/", "paragraph_index": 7}], "all_paragraphs": ["Deep Q-Learning may be one of the most important algorithms in all of Reinforcement Learning as it lacks limitation on the observations it can make and the actions it can take within complex environments. This method of Reinforcement Learning incorporates deep neural networks in a way that allows an agent to \u2018play\u2019 an environment repeatedly and learn the environment over time through a system of observations, actions, and rewards. This structure has obvious benefits over a standard deep neural network implementation as it allows the agent to interact with its surroundings, receive feedback from its surroundings, and then optimize for desirable (highly rewarded) future actions. In this article, we will be looking at a familiar environment that was recently instantiated by Kaggle in one of their Kaggle competitions (https://www.kaggle.com/c/connectx).", "Before we start exploring the structure of a Deep Q-Learning agent to play Connect 4, let\u2019s first briefly overview the structure of a simple, much less useful Q-Learning agent. The basic idea of Q-Learning is to create a map of the entire observation space, and within this map, record the agents actions. Subsequently, every time the agent encounters the same observation, it will incrementally update the action it previously took based on whether its previous action earned a positive or negative reward. This data structure in which previous actions for each observation in the observation space are stored is known as a Q-Table. This incremental updating of the Q-Table is most commonly done by the following Q-Learning equation:", "While this equation appears complicated upon first glance, it is simply using the expected reward to update the specific location of the Q-Table that corresponds to the current observation. Understanding of this process is much better visualized through a code example, though not necessary to work through our Deep Q-Learning project, since we will instead be using a Deep Neural Network to update our \u201cQ-Table\u201d (more on this later).", "To better understand the nature of Q-learning, let\u2019s frame an example observation from the perspective of our very well-known environment, Connect 4. The following is the above example state of a Connect 4 board during a game, in other words an element of the observation space (Player 1 chips: 1, Player 2 chips: 2, empty spaces: 0)", "Here we see a specific arrangement of previously played pieces that would be sent to the agent as an observation. The agent would take this observation and find this observation within its Q-Table. Essentially asking itself the question, \u201cWhat did I do last time I saw this arrangement of pieces and how can I do better this time?\u201d. It would then proceed to act in the proper manner and throughout the process updating the action in the proper location in the Q-Table for each move. From a broad overview, this seems like a great use case for Q-Learning\u2026 other than one subtle exception. Did you catch it?", "The Q-Learning structure is very useful for some environments, but the number of environments in which it is functional is very limited. This is due to the previously stated phrase: \u201cThe basic idea of Q-Learning, is to create a map of the entire observation space\u2026\u201d. Think about what this means even in the context of our simple Connect 4 environment. In Connect 4, we have 42 entries that can be filled by any of a player 1 chip, a player 2 chip, or no chip. We must also give gravity its due; in that a chip can only be played in a location of the grid if there is a previously played chip in the space underneath it. Further, if any 4 chips of the same color are lined up, the game is over and thus future situations cannot be included in the observation space. This amounts to be a somewhat complex calculation as we\u2019ve laid out; nonetheless, you can see how the number of possible observations starts to explode very quickly. According to the online Encyclopedia of Integer Sequences, the result is 4,531,985,219,092 (oeis.org/A212693). I don\u2019t know about you, but it seems a 4 quadrillion entry Q-Table is just a bit too large for my 1TB hard drive. So, standard Q-Learning is not a good place to start, let\u2019s turn to a different solution.", "With the foundational structure of Q-Learning in mind, Deep Q-Learning is very easily understood; the only difference being a substitution of the Q-Table. The limitation of the Q-Table for environments with large observation spaces is reached very quickly in Q-Learning. If only we knew of some sort of algorithm that could take an observation along with some sort loss function that we calculate from the reward, and make some sort of generalization about the observed environment in order to choose a single output (or action), so that we don\u2019t have to store so many environment observations. As anyone with an interest in machine learning knows, Deep Neural Networks, are a tool aimed at just this issue. In summary, to simplify the staggering size of our observation space in the game of Connect 4, we will use a standard Q-Learning structure with a simple substitution of a Deep Neural Network for what would have otherwise been a Q-Table reference.", "For convenience, we will be using the Connect X framework from an ongoing Kaggle competition (with a few modifications) to build our agent. This will allow us to very simply get observations from and send actions to the environment without having to build the game of Connect 4 ourselves. The following 4 lines of code deals with the construction of the environment along with a simulation of two agents playing Connect 4 against each other, in this case two random agents (https://pypi.org/project/kaggle-environments/). This environment supplies both a \u2018random\u2019 agent and a \u2018negamax\u2019 agent for training; in which, the random agent simply makes random legal moves, and the negamax agent makes beginner level, but better than random legal moves. With this in mind, we realize the concept of self-play will be essential for training in which our model plays against itself.", "The only slight change made to environment provided by Kaggle was manual calculation of when the game is over. This allowed for reward determination for different kinds of wins, losses, and invalid moves. For example, when our agent is training against a random agent, vertical victories are very easy to come by since the random agent will only block the vertical victory 3/7 of the time. In this specific training instance it might be useful to place less reward on vertical victories that cause the agent to only play in column 3, for example, and still win 4 out of every 7 games using an obviously bad strategy. The win/loss determination is demonstrated by the following function. This is simply brute force iteration through all possible vertical, horizontal, and diagonal configurations of the board, respectively.", "These steps are somewhat self-explanatory with the assumption in mind that the reader has had an exposure to neural networks before; but, a brief overview of each of the creation of the model, the loss function, and the optimization follows. First, the model is simply a dense neural network built from the very convenient use of the tensorflow.keras.layers module. I first tried to use a convolutional neural network as I assumed relational information would be beneficial, but found this not to perform as well (I assume due to the small size of the board). The output of the network has 7 neurons, each corresponding to the action of dropping a chip into one of 7 different columns of the board.", "The loss function simply takes the sum of the rewards of an episode (or a complete game of Connect 4), applies them to each input and output of the network (observation and action, respectively), performs cross entropy and gives each observation/action pair a loss value that is inversely proportional to the reward received. For example, if the agent wins a game, all observation/action pairs throughout the game will receive a reward of 20, the highest reward offered in our reward structure. This will in turn result in a small loss value, hence the inverse relationship. This inverse relationship is so that our optimizer optimizes the network towards positive rewards, as the goal of any standard optimizer is to minimize the loss of the network. In theory (and much less often in practice), minimizing loss will then increase reward and in turn increase the ability of the network to make decisions that result in wins in the game of Connect 4.", "The optimizing step here in the train_step() function may look a bit new to even someone experienced in deep learning. This is because it uses the GradientTape() module which is new to Tensorflow as of version 2.0. This module in summary \u2018watches\u2019 (or records) the gradients of the network as they are adjusted and allows a network to be trained using custom loss functions with ease, as we have implemented here, a big improvement from the model.fit() method of older versions.", "In all Reinforcement Learning problems, there exists an unavoidable trade-off between exploration and exploitation. Exploration is where an agent makes some out of character decision (not dictated by the network) to try to find new strategies and potentially be rewarded from them; thus, an opportunity to learn better strategies. Exploitation is to exploit the strategies the network has already learned. For example, if an agent has learned a bit how to play Connect 4 and it always exploits the techniques it has already learned, it will never be presented with new observation/action pairs to learn from. On the other hand, if the agent is always exploring new strategies, it is essentially always playing random moves and never reaching a high level of game play to learn from. The way this problem is dealt with typically is through a phenomenon known as epsilon decay. The epsilon in epsilon decay can be thought of as the probability the agent will choose a random action rather than an action as chosen by then network. The decay in epsilon decay is just that: since towards the beginning of the training cycle we expect the agent to be dumb, for lack of a better term, we will let it make largely random decisions, thus letting the agent explore more than exploit, since it hasn\u2019t yet learned anything to exploit. Over time, the epsilon will get smaller and smaller, that is: the agent will start exploiting its learned knowledge more and more; and will explore random actions less and less. This agent will use the epsilon decay function epsilon = .99985^x where x is the number of episodes trained. This would be adjusted accordingly to the number of episodes of the training period, in this case we use 40,000.", "This part is very simple. The agent is simply a function that takes in an observation, and spits out an action. Shown below is a very simple agent in which it sends the observation through the neural network and outputs an action. Though there is a small addition, a bug fix if you will: in the \u2018else\u2019 statement you\u2019ll see it simply chooses the next highest probability action predicted by the network in the case that the action is determined to be invalid. An action is invalid if the network decides to play in a column that is already full. You\u2019ll notice though, after substantial training and proper reward structures, the agent eventually learns to play only valid actions; making this sort of a \u2018just in case\u2019 feature.", "The work flow here is relatively straight forward now that all of the structural functions are built. We start by instantiating our memory object that will be used to pass information to the network from the environment. We then iterate through each episode, 40,000 in this case, properly passing observations and rewards to the network as dictated by the previously described functions. Notice our custom specified rewards, as previously mentioned. The training process consisted of three training sessions in total: the first training session against the random agent, the second session against the negamax agent, and the third session playing against itself (or the same model trained to play player 2\u2019s role, more specifically).", "The following is a game played by our agent (blue) against the negamax agent (grey). Overall, I am impressed with the agent\u2019s performance; it is obvious the agent has figured out how to block wins, how to win itself, and also how to play only in the columns that are not already full (these would\u2019ve been corrected anyway, but it attempted no invalid moves at this point in training). The move I am most impressed by in this example is its second to last move, move 32. This move is impressive because it not only sets up a win, but it actually guarantees a win by setting up both a horizontal win and a diagonal win at the same time.", "A few main takeaways from this project to address:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F76271ed663ca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----76271ed663ca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76271ed663ca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@leeschmalz?source=post_page-----76271ed663ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@leeschmalz?source=post_page-----76271ed663ca--------------------------------", "anchor_text": "Lee Schmalz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc586527ff3b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&user=Lee+Schmalz&userId=c586527ff3b6&source=post_page-c586527ff3b6----76271ed663ca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76271ed663ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76271ed663ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/c/connectx", "anchor_text": "https://www.kaggle.com/c/connectx"}, {"url": "https://oeis.org/A212693", "anchor_text": "oeis.org/A212693"}, {"url": "https://pypi.org/project/kaggle-environments/", "anchor_text": "https://pypi.org/project/kaggle-environments/"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----76271ed663ca---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----76271ed663ca---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----76271ed663ca---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/q-learning?source=post_page-----76271ed663ca---------------q_learning-----------------", "anchor_text": "Q Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----76271ed663ca---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76271ed663ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&user=Lee+Schmalz&userId=c586527ff3b6&source=-----76271ed663ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76271ed663ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&user=Lee+Schmalz&userId=c586527ff3b6&source=-----76271ed663ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76271ed663ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76271ed663ca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F76271ed663ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----76271ed663ca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----76271ed663ca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----76271ed663ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----76271ed663ca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----76271ed663ca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----76271ed663ca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----76271ed663ca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----76271ed663ca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----76271ed663ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@leeschmalz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@leeschmalz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lee Schmalz"}, {"url": "https://medium.com/@leeschmalz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "209 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc586527ff3b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&user=Lee+Schmalz&userId=c586527ff3b6&source=post_page-c586527ff3b6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8fef7abd8313&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-connect-4-with-deep-q-learning-76271ed663ca&newsletterV3=c586527ff3b6&newsletterV3Id=8fef7abd8313&user=Lee+Schmalz&userId=c586527ff3b6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}