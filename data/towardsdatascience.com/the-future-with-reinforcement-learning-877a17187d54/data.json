{"url": "https://towardsdatascience.com/the-future-with-reinforcement-learning-877a17187d54", "time": 1682993594.013084, "path": "towardsdatascience.com/the-future-with-reinforcement-learning-877a17187d54/", "webpage": {"metadata": {"title": "The Future with Reinforcement Learning | by Hunter Heidenreich | Towards Data Science", "h1": "The Future with Reinforcement Learning", "description": "If you haven\u2019t yet read the reinforcement learning primer go back and check it out first here. That article will provide you with the key concepts in reinforcement learning. Then you will be ready to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://recast.ai/blog/the-future-with-reinforcement-learning-part-1/", "anchor_text": "reinforcement learning primer", "paragraph_index": 0}, {"url": "https://recast.ai/blog/the-future-with-reinforcement-learning-part-1/", "anchor_text": "here", "paragraph_index": 0}], "all_paragraphs": ["If you haven\u2019t yet read the reinforcement learning primer go back and check it out first here. That article will provide you with the key concepts in reinforcement learning. Then you will be ready to fully compare the different types of machine learning.", "You may have heard about other types of machine learning ie: supervised learning, unsupervised learning, etc. Understanding how reinforcement learning (RL) differs from them is a good way to grasp the machine learning landscape.", "The easiest type of ML to grasp is supervised learning. Supervised learning is learning with human labels. Image classification is a type of supervised learning. You have an algorithm and based on labeled images the system can classify the image as a cat or a dog. The algorithm learns from observing the training set and then can correctly infer the subject of an unknown image.", "Another good example of a supervised learning problem is a regression problem. In a regression problem you take a bunch of parameters and estimate a real, continuous value based on those parameters. For example you could take in information about a house (the number of rooms, square footage, number of windows, etc.) and output a price. We know what a lot of houses are worth and can feed those labeled examples into the algorithm. Then when you present a new house to the system, it can come up with a good estimate for the price on its own. These are problems that are easy to frame as a supervised learning problems.", "On the flip-side, we have unsupervised learning: Learning without labels. A good example of this is taking user purchase data and grouping your customers into categories with similar buying patterns. Your algorithm does the grouping and you can suggest products to people within a certain category. We do not tell the algorithm what a label or a category name is, we simply hand it a bunch of data and it creates groups based on patterns in the data. Unsupervised learning is also used extensively in visualizing a large amount of complex data. It makes it easier for a human to see all the information in one image.", "Reinforcement learning is frequently described as falling somewhere in between supervised and unsupervised learning. There are time-delay labels (rewards), that are given to an algorithm as it learns to interact in an environment. An algorithm learns based on how the problem of learning is phrased. This is exactly what makes reinforcement learning excel at things like real-time decision making, video game AI, robot navigation, and other complex tasks. The key is giving the system the ability to understand which decisions are good and which ones are bad, based the current state of the environment.", "In the previous article, we covered the basic concepts of reinforcement learning . Here is a little summary of what we have covered so far in the form of a concrete example:", "Imagine a mouse in a basic maze. The mouse will be our agent. To start, we will check the things our agent needs:", "Furthermore, let\u2019s look at the sub-elements of our problem and see how they measure up:", "Let\u2019s dissect a basic, greedy policy an agent might employ:", "One of the policies is a Q-table strategy. Q-table stands for \u2018quality table\u2019. It is a table of actions and states, as well as the rewards associated with them. We could employ a basic strategy that says when we encounter a state, choose the action that\u2019s going to give our agent the most reward. When our agent doesn\u2019t know what will give the most reward, choose an action randomly.", "In the beginning, our mouse\u2019s table is empty. It knows nothing. It chooses its strategy randomly, and may move right and receives a small amount of cheese, for example. That is good, and our agent receives a reward signal! The table gets updated accordingly, and our agent will keep choosing actions until it has exhausted all possibilities or has died.", "Already, you may see an issue cropping up: when we restart our maze, our agent is inclined to always move towards the small cheese, never opting for an unknown alternative. This is called the explorations-versus-exploitation tradeoff, but we will come back to that in a bit.", "Now that we have visualized how these components work together, let\u2019s take a dive into some of the things that are needed for any reinforcement learning problem that we wish to solve.", "One of the major components to look at for an reinforcement learning application is how is the task structured. These are typically broken down into two categories: episodic or continuous.", "Episodic tasks have distinct start and end states. We can save these \u201cepisodes\u201d and train on them \u201coff-line.\u201d A prime example would be our Mario levels from our previous article.", "Continuous tasks have no end. This could be like a decision-making algorithm that predicts when someone should buy or sell stocks in the stock market. This is always evolving and changing, with a lot of environmental factors. There are no clear starting and stopping states that would allow us to easily section off an episode to train on for fear of fitting our algorithm to fit too closely to a small segment of time.", "The stock market is always changing. To cut it up into episodes is to ignore the linked continuity of how it evolves", "How we formulate our agent\u2019s goals and rewards is shaped by the type of task we are looking to complete, because it can change the nature of when we learn (something we will talk about next).", "Timing is critical in how an agent will perform on a task. Perhaps an agent should be learning at every frame of gameplay, or maybe the agent learns in episodes. We could employ a Monte Carlo strategy of cycling through the entire episode of learning and then get better and smarter with each iteration. These options have different tradeoffs and may or may not be feasible depending on the type of task our agent is trying to complete (a continuous task may never use the Monte Carlo strategy since it requires cycling through an episode for training, something that doesn\u2019t even exist for a continuous task!).", "The exploration-versus-exploitation tradeoff is something that is quickly encountered when an agent explores an environment. If an agent finds out early on that if it does something simple, it will receive a small amount of reward, it will likely continue to do that simple thing over and over again, accumulating small rewards overtime. If it explores the unknown and tries to find new situations it may gain an even larger reward.", "In human terms, this is like asking the question do you go the restaurant that you always go to and that you know will be good? Or do you venture into the unknown and check out the place that you\u2019ve never tried before that might be completely fantastic?", "How an agent\u2019s policy is structured will determine what kind of actions it will learn to exploit and when it will decide to explore. Exploring early on may yield much higher long-term rewards, however, focusing too much on exploration may result in sub-optimal actions in states that we know a lot about. This leads you to end up with less rewards than we could have gotten.", "The exploration-versus-exploitation tradeoff is still very much an open question and is a particularly interesting area of research, in my opinion.", "This brings us to another significant factor in making a reinforcement learning application. Is it value-based or policy-based?", "We\u2019ve mentioned before that an agent\u2019s policy is how it makes decisions on what actions to take based on the current state of the environment. An RL agent with a policy-based approach to learning will try and learn a complex policy with a decision structure that allows it to try and take the optimal action in any given situation.", "On the other end of the spectrum, we have out value-based RL applications. The value function is the current estimate of the long-term reward that our RL algorithm will accumulate. If we have a value-based agent, it will focus on optimizing based on that function. That includes focusing on learning better and better estimates for the long-term reward as well as taking greedy actions to maximize that function at any given time. In a lot of ways, we can think of this as an agent learning an implicit greedy policy for taking actions.", "The decision between a value-based and a policy-based algorithm is a significant one in deciding what an reinforcement learning algorithm will look like. The cross-section of these two lines of thinking is called the actor-critic approach. It features keeping track of estimated future reward earnings (our value function) as well as learning new, more complex policies to follow to get our agent larger rewards over longer time scales. It quickly becomes a much harder problem since the algorithm now optimizes two functions at once.", "There is a lot of focus in the actor-critic domain and there have been many cool algorithms that have come out of it. Google\u2019s asynchronous advantage actor-critic (A3C) is a prime example of a cool actor-critic algorithm that has shown a lot of good results.", "Over the last two articles, we have covered the basic terminology as well as some of the more complicated concepts around a reinforcement learning problem. Hopefully, with these two components, you feel that you have a good grasp on what reinforcement learning is and some of the considerations that go into writing an algorithm using it.", "Right now, you might be feeling super excited about RL. You may be wondering how you can get started on systems that work on RL. In the next article, we will dive into where RL is excelling, what the major open questions are, and some resources on learning to write RL algorithms yourself!", "CS Undergrad at Drexel University | AI | ML | NLP | DS"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F877a17187d54&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----877a17187d54--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----877a17187d54--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8----877a17187d54---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F877a17187d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----877a17187d54---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F877a17187d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&source=-----877a17187d54---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://recast.ai/blog/the-future-with-reinforcement-learning-part-1/", "anchor_text": "reinforcement learning primer"}, {"url": "https://recast.ai/blog/the-future-with-reinforcement-learning-part-1/", "anchor_text": "here"}, {"url": "https://recast.ai/blog/the-future-with-reinforcement-learning-part-2-comparisons-and-applications/", "anchor_text": "recast.ai"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----877a17187d54---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----877a17187d54---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----877a17187d54---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F877a17187d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----877a17187d54---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F877a17187d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----877a17187d54---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F877a17187d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----877a17187d54--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----877a17187d54--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8----877a17187d54---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a81cf6d293c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&newsletterV3=66c914ddeac8&newsletterV3Id=8a81cf6d293c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----877a17187d54---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Written by Hunter Heidenreich"}, {"url": "https://medium.com/@hunterheidenreich/followers?source=post_page-----877a17187d54--------------------------------", "anchor_text": "915 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8----877a17187d54---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a81cf6d293c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-future-with-reinforcement-learning-877a17187d54&newsletterV3=66c914ddeac8&newsletterV3Id=8a81cf6d293c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----877a17187d54---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----877a17187d54----0---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----877a17187d54----0---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----877a17187d54----0---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----877a17187d54----0---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----877a17187d54----0---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "NEAT: An Awesome Approach to NeuroEvolutionNeuroEvolution can optimize and evolve neural network structure, and the NEAT algorithm was one of the first to show it as a viable\u2026"}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----877a17187d54----0---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "8 min read\u00b7Jan 4, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3eca5cc7930f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneat-an-awesome-approach-to-neuroevolution-3eca5cc7930f&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----3eca5cc7930f----0-----------------clap_footer----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f?source=author_recirc-----877a17187d54----0---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3eca5cc7930f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneat-an-awesome-approach-to-neuroevolution-3eca5cc7930f&source=-----877a17187d54----0-----------------bookmark_preview----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----877a17187d54----1---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----877a17187d54----1---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----877a17187d54----1---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----877a17187d54----1---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----877a17187d54----1---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----877a17187d54----1---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----877a17187d54----1---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----877a17187d54----1-----------------bookmark_preview----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----877a17187d54----2---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----877a17187d54----2---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----877a17187d54----2---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----877a17187d54----2---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----877a17187d54----2---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----877a17187d54----2---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----877a17187d54----2---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----877a17187d54----2-----------------bookmark_preview----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----877a17187d54----3---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----877a17187d54----3---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=author_recirc-----877a17187d54----3---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----877a17187d54----3---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----877a17187d54----3---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "Introduction to Word EmbeddingsWhat is a word embedding?"}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----877a17187d54----3---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": "10 min read\u00b7Aug 16, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cf857b12edc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-word-embeddings-4cf857b12edc&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----4cf857b12edc----3-----------------clap_footer----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc?source=author_recirc-----877a17187d54----3---------------------cd266086_3e7f_40b2_8707_18d4c48aa6a9-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cf857b12edc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-word-embeddings-4cf857b12edc&source=-----877a17187d54----3-----------------bookmark_preview----cd266086_3e7f_40b2_8707_18d4c48aa6a9-------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----877a17187d54--------------------------------", "anchor_text": "See all from Hunter Heidenreich"}, {"url": "https://towardsdatascience.com/?source=post_page-----877a17187d54--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----877a17187d54----0-----------------bookmark_preview----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----877a17187d54----1-----------------bookmark_preview----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----0-----------------clap_footer----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----877a17187d54----0---------------------7af57850_6423_49c6_b55a_423621532c1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----877a17187d54----0-----------------bookmark_preview----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----877a17187d54----1---------------------7af57850_6423_49c6_b55a_423621532c1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----877a17187d54----1-----------------bookmark_preview----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----877a17187d54----2---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----877a17187d54----2---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----877a17187d54----2---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Aniruddha Mukherjee"}, {"url": "https://medium.com/dsckiit?source=read_next_recirc-----877a17187d54----2---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "GDSC KIIT"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----877a17187d54----2---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Reinforcement Learning: An Introduction and Guide to its FundamentalsPolicies, Rewards, the Bellman Equation, and the Markov Decision Process (MDP)"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----877a17187d54----2---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "5 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdsckiit%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&user=Aniruddha+Mukherjee&userId=68f97387c191&source=-----467c6a2ed25e----2-----------------clap_footer----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----877a17187d54----2---------------------7af57850_6423_49c6_b55a_423621532c1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&source=-----877a17187d54----2-----------------bookmark_preview----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----877a17187d54----3---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----877a17187d54----3---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----877a17187d54----3---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----877a17187d54----3---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----877a17187d54----3---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----877a17187d54----3---------------------7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----3-----------------clap_footer----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----877a17187d54----3---------------------7af57850_6423_49c6_b55a_423621532c1d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----877a17187d54----3-----------------bookmark_preview----7af57850_6423_49c6_b55a_423621532c1d-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----877a17187d54--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----877a17187d54--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----877a17187d54--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}