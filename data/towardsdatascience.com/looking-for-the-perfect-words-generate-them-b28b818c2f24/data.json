{"url": "https://towardsdatascience.com/looking-for-the-perfect-words-generate-them-b28b818c2f24", "time": 1683001721.238879, "path": "towardsdatascience.com/looking-for-the-perfect-words-generate-them-b28b818c2f24/", "webpage": {"metadata": {"title": "Looking for the Perfect Words? Generate Them | by Emma Sheridan | Towards Data Science", "h1": "Looking for the Perfect Words? Generate Them", "description": "Ever try to find the right words to describe how you are feeling? Want to profess your love to your significant other in a poem? Why don\u2019t you just generate one based on thousands of well-known\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/tgdivy/poetry-foundation-poems", "anchor_text": "here on Kaggle", "paragraph_index": 6}, {"url": "https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10", "anchor_text": "several articles", "paragraph_index": 24}, {"url": "https://github.com/emmasheridan/ANNfinalproject\\", "anchor_text": "GitHub", "paragraph_index": 28}], "all_paragraphs": ["Ever try to find the right words to describe how you are feeling? Want to profess your love to your significant other in a poem? Why don\u2019t you just generate one based on thousands of well-known poems? This is what we attempted to do using a Long Short-Term Memory (LSTM) model.", "An LSTM model has an artificial recurrent neural network (RNN) architecture. Since it is a recurrent network, it means that the model has feedback connections. Additionally, it can process entire sequences of data such as sentences or video. LSTM is often used for handwriting recognition and speech recognition. It is good at working on these types of problems because of its selective remembering patterns. A typical LSTM unit consists of a cell, an input gate, an output gate, and a forget gate. When information enters the unit, it can either be forgotten, inputted to the rest of the cell, or summarized and outputted.", "The forget gate removes information from the cell via the multiplication of a filter (see the sigmoid function above on the far left with the arrow to the multiplication symbol).", "The input gate is how new information can be added to the cell. This happens in three steps. First, the sigmoid function acts as a filter to decide what values need to be added. Next, the tanh function creates a vector of all possible values that could be added. The output vector will be in the range of -1 to 1. Finally, the values from the sigmoid filter and the tanh function are multiplied together and then added to the cell.", "The output gate selects useful information from the cell state and shows it as output. This happens in three steps as well. First, the tanh function is applied to have values between -1 and 1. Again, we will use the sigmoid function as a filter. To complete the step, the vector created by the tanh function is multiplied with the filter and sends it as output to the next hidden cell.", "In conclusion, all these gates make LSTM models much better than your typical convolutional feedforward network or RNN at sequence prediction. This is why we chose to use an LSTM model to generate poems.", "The data for training our LSTM model for poem generation came from the complete collection of poems from PoetryFoundation.com. The data can be found here on Kaggle. Each poem entry contains several tags that describe the poem. We decided to focus on three categories of poems: Love, Nature, and Sadness. To get the right data, we needed to extract poems from the dataset that have certain tags. We decided to cap the characters at 1000 for each poem (we played around with more characters) because that way we included enough of the poem, and didn\u2019t put too much strain on our computers. In the code below, you can see how we got the data.", "Here is an example of what the raw data looks like.", "As you can see in the raw data above, there are several tags. For the purpose of this blog post, we are going to show you only how we extracted poems with the tag \u201clove\u201d (see code below). We also generated poems using other tags which you can see towards the end of this post.", "Once we gathered the poems we needed, we were able to create training and testing data. In the code below, we created our training data. First, we had to map the characters to indices and vice versa.", "Next, we had to create the X and Y data sets for training. This process chops up the giant string of poems into sequences with a length of 50 characters.", "After generating our data, we built our model. There is a LSTM layer and a Dense layer in its architecture. We will discuss below, but we played around with the number of hidden units for the LSTM layer. In addition, we tried a model with 2 LSTM layers and 3 LSTM layers. Keeping all the variables the same, we did not see a difference in performance of our model when changing the number of layers.", "Then, we were ready to fit our model. After training with a batch size of 128 and 10 epochs, we achieved an accuracy of 0.68.", "We also evaluated the model on a validation dataset and visualized the results. The plot below shows the training and validation loss when we had 128 hidden units for the LSTM layer.", "However, when we fit our model with 80 hidden units for LSTM layer, we saw the plot below.", "When comparing the two plots, we can see that the model with 80 hidden units in LSTM layer has a better fit than the model with 128 hidden units. The model with 128 hidden units shows a pattern of overfitting that we read about in our research. Over time, the training and validation data diverge which is not what we are looking for. However, with the 80 hidden units, the training and validation loss both decrease together indicating a good model fit.", "We also realize that our accuracy could have been higher. We tried training our model with 3000 characters from each poem in the dataset. This increases the amount of data we had, and we were able to increase the accuracy of the model to 78%\u2013a 10% increase. If we had more time and more computing power, we could use more of the data. We used a smaller amount of data than what we had access to because it took too long to train.", "After settling on the architecture of the model, we generated some poems using the code below.", "Below, we generated a poem with a length of 500 characters, a start index (which is a string of the sequence length from the original text), and diversity of 0.5. The diversity input changes the sampling probabilities. After a few trials, we found that a diversity of 0.5 gave us the best poems. A diversity of 0.1 made the poems too repetitive and a diversity of 1 made poems with little sense.", "Here is the output from a run of the code above:", "As you can see the output includes new line indicators (\\r\\r\\n). We played around with removing these from our training string, but we decided that keeping these characters allows our model to structure our generated poems like real poems. Below, we show a few more poems that we generated, formatted nicely.", "Overall, our poem output seems to be generating poems with a format similar to our dataset. They have similar amounts of \u201c\\r\\r\\n\u201d like sections to represent the spacing, and they are pretty good at generating sentences. The general mood of each category seems to reflect the data that it came from as well. However, they still struggle to make logical sense and are highly based on the starting index. The starting index plays a large role in determining how the poem turns out. The words in the starting seed are often repeated throughout the poem.", "In the future to improve the output of these poems, we could have the poems indexed so the starting seed starts at the beginning of a poem each time rather than the middle or end of the poem. We could do work to have it train on one full poem so it doesn\u2019t just start and end randomly\u2013creating a more concise poem.", "The logical sense of each sentence is another thing to improve. However, this is a great challenge in poetry because poems are artistic and don\u2019t necessarily follow traditional grammar rules. We could do more research into how to make poems more coherent as well.", "When we started the project we originally intended to use a generative adversarial network (GAN) where the generator was a recurrent neural network (RNN). After doing some research, we found several articles that mentioned difficulty and less accuracy when using the GAN on text formats. We also saw that an LSTM model was recommended so we switched to that instead. It would have been nice to use a GAN because we would have been able to combine the latent space of data instead of making a new model for each type of poem.", "Another thing we could have tried was separating the poem by time period. The poems written by Shakespeare have a very different language than modern poems and that could make for a confusing data set for the model to generate from. If we are to add onto this project we could separate by time period so the user could choose the style they want their poem to come out as.", "Another feature that could be added to future work would be poem titles. If we had more time, we could have formatted the data to also take in the poem title along with the poem content. We would then generate a title based on the tags that would match each generated poem. However, this may lead to the titles being unrelated to the actual poem content.", "Overall, we realize there are many things we could try improve the model\u2019s poem generation. However, we made a good start at poem generation and have learned a lot about how LSTM models work and how to tweak them.", "Project completed by Emma Sheridan and Jessica Petersen for Artificial Neural Networks course. The source code for this project can be found on GitHub.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Exploring HMW leverage design thinking in PM, understanding our designed world in order to build a better one, and centering in empathy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb28b818c2f24&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sheridanemma.medium.com/?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": ""}, {"url": "https://sheridanemma.medium.com/?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "Emma Sheridan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a1c659a8b94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&user=Emma+Sheridan&userId=5a1c659a8b94&source=post_page-5a1c659a8b94----b28b818c2f24---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb28b818c2f24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb28b818c2f24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@thoughtcatalog?utm_source=medium&utm_medium=referral", "anchor_text": "Thought Catalog"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/tgdivy/poetry-foundation-poems", "anchor_text": "here on Kaggle"}, {"url": "https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10", "anchor_text": "several articles"}, {"url": "https://unsplash.com/@thoughtcatalog?utm_source=medium&utm_medium=referral", "anchor_text": "Thought Catalog"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/", "anchor_text": "Essentials of Deep Learning : Introduction to Long Short Term MemoryIntroduction Sequence prediction problems have been around for a long time. They are considered as one of the hardest\u2026www.analyticsvidhya.com"}, {"url": "https://towardsdatascience.com/ai-generates-taylor-swifts-song-lyrics-6fd92a03ef7e", "anchor_text": "AI Generates Taylor Swift\u2019s Song LyricsTaylor Swift Lyrics Generatortowardsdatascience.com"}, {"url": "https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/", "anchor_text": "How to Diagnose Overfitting and Underfitting of LSTM ModelsIt can be difficult to determine whether your Long Short-Term Memory model is performing well on your sequence\u2026machinelearningmastery.com"}, {"url": "https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10", "anchor_text": "Generative Adversarial Networks for Text Generation \u2014 Part 1The issues with GANs for text generation and the methods being used to combat thembecominghuman.ai"}, {"url": "https://medium.com/cindicator/music-generation-with-neural-networks-gan-of-the-week-b66d01e28200", "anchor_text": "Music generation with Neural Networks \u2014 GAN of the weekGAN of the Week is a series of notes about Generative Models, including GANs and Autoencoders. Every week I\u2019ll review a\u2026medium.com"}, {"url": "https://github.com/emmasheridan/ANNfinalproject\\", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b28b818c2f24---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----b28b818c2f24---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/lstm?source=post_page-----b28b818c2f24---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b28b818c2f24---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/poetry?source=post_page-----b28b818c2f24---------------poetry-----------------", "anchor_text": "Poetry"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb28b818c2f24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&user=Emma+Sheridan&userId=5a1c659a8b94&source=-----b28b818c2f24---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb28b818c2f24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&user=Emma+Sheridan&userId=5a1c659a8b94&source=-----b28b818c2f24---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb28b818c2f24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb28b818c2f24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b28b818c2f24---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b28b818c2f24--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b28b818c2f24--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b28b818c2f24--------------------------------", "anchor_text": ""}, {"url": "https://sheridanemma.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sheridanemma.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Emma Sheridan"}, {"url": "https://sheridanemma.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "54 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a1c659a8b94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&user=Emma+Sheridan&userId=5a1c659a8b94&source=post_page-5a1c659a8b94--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6ee237f71c14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flooking-for-the-perfect-words-generate-them-b28b818c2f24&newsletterV3=5a1c659a8b94&newsletterV3Id=6ee237f71c14&user=Emma+Sheridan&userId=5a1c659a8b94&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}