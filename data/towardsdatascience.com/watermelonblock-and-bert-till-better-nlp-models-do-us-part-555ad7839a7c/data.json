{"url": "https://towardsdatascience.com/watermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c", "time": 1682994543.880456, "path": "towardsdatascience.com/watermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c/", "webpage": {"metadata": {"title": "WatermelonBlock and Google BERT | Till Better NLP Models Do Us Part | by Karthik Ramesh Kamath | Towards Data Science", "h1": "WatermelonBlock and Google BERT | Till Better NLP Models Do Us Part", "description": "The data science team at WatermelonBlock has embraced experimentation in the sense that we have started working on implementing deep text mining algorithms just so the accuracy rates are increased\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.watermelonblock.io/", "anchor_text": "WatermelonBlock", "paragraph_index": 0}, {"url": "https://finance.yahoo.com/news/watermelonblock-partners-ibm-watson-using-123000544.html", "anchor_text": "Our partners at IBM", "paragraph_index": 1}, {"url": "https://developer.ibm.com/linuxonpower/deep-learning-powerai/", "anchor_text": "leveraging the IBM PowerAI servers", "paragraph_index": 1}, {"url": "http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/", "anchor_text": "this awesome article by WildML", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax function for non-linearity", "paragraph_index": 19}, {"url": "https://www.watermelonblock.io/#team", "anchor_text": "engineers and data scientists at WatermelonBlock", "paragraph_index": 23}, {"url": "https://medium.com/u/2e4ae63ee111?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "WatermelonBlock", "paragraph_index": 23}, {"url": "https://www.watermelonblock.io/blog/", "anchor_text": "announcements page on our website", "paragraph_index": 23}], "all_paragraphs": ["The data science team at WatermelonBlock has embraced experimentation in the sense that we have started working on implementing deep text mining algorithms just so the accuracy rates are increased from time-to-time thereby resulting in an efficient data pipeline. A pipeline that could crunch the massive natural language corpora that we acquire from the web and attain the bias-variance trade-off with ease.", "Our partners at IBM work with us on leveraging the IBM PowerAI servers for conducting these experiments and get the desired results. The team has been getting their hands dirty with some of the incredible natural language processing models ever developed in order to serve our customers with a smart-yet-generalized real-time insights application. One such model is the trending open-source NLP model \u2014 \u2018BERT\u2019 developed by Google recently. The initial results that we obtained while testing out the BERT framework for our application showed that there was a 20% reduction in the classification error and just took 4 hours to train the ensemble model with an unstructured dataset comprising over 1.4 million examples.", "It was just too good to be true but yeah, it was one of the best results that we have recorded so far. A perfect New Year gift, any data science team would crave for. In fact, we have already fallen in love with the framework and couldn\u2019t wait to deploy it to production.", "So what\u2019s BERT and how does it work?", "BERT a.k.a Bidirectional Encoder Representations from Transformer is a new method of pre-training language representations which gives out state-of-the-art results on a wide array of natural language processing tasks like inter-lingual machine translation and fact-checking. Let\u2019s start by looking at the concepts behind the model\u2019s foundation.", "BERT is basically a transformer encoder stack with an attentive mechanism.", "If you don\u2019t know what an attention mechanism is, this awesome article by WildML will get you up to speed! On the other hand, a transformer is a deep learning model that speeds up the training process of the entire stack by representing the input data into a suitable form. Though, the transformer consists of two significant components namely \u2014 encoding component and decoding component interconnected with one another, BERT uses only the encoding component for its operations.", "The following diagram is a simple black-box case of translating a French sentence into English using a transformer system.", "The encoding component is a stack of encoders (for the sake of giving an example, we take stacks of 6 on top of each other, but one can definitely experiment with other arrangements). Each encoder has a feed-forward network layer and a self-attentive mechanism layer, while each decoder has the same along with an additional layer \u2014 encoder-decoder attention.", "The encoder\u2019s inputs initially flow through the self-attention layer \u2014 a neural layer that helps the encoder look at other words in the input sentence as it encodes a specific word.", "The outputs of the self-attention layer are then fed to a feed-forward layer \u2014 a layer that allocates weights and propagates the encoded word vector forward to the next encoding component.", "The decoder has both those layers, but between them is an encoder-decoder attention layer that is a safety measure that helps the decoder focus on relevant parts of the given input sentence.", "There are two types of BERT models that have been developed:", "1. BERT Base: Smaller in size, computationally affordable and not applicable to complex text mining operations.", "2. BERT Large: Larger in size, computationally expensive and crunches text data only to deliver the best results.", "Both the BERT models have \u2019N\u2019 encoder layers (a.k.a transformer blocks) that form a massive data encoding stack. Usually, N = 12 for the base version, and N = 24 for the large version. They have larger feed-forward layers (768 and 1024 hidden units respectively), and self-attention layers (12 and 16 respectively).", "For explanatory purposes, let\u2019s use a mini-version of BERT with 6 encoder layers, 512 feed-forward layers, and 8 self-attention layers for the purpose of classifying spam text or from genuine ones.", "Like a typical encoder in a transformer, BERT takes in a sequence of words as input which keeps flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder. The first input token is a classifier token [CLS] that transfers the encoded version of the input in that layer to the next encoder layer through a feed-forward network and a softmax function.", "As far as the outputs are concerned, each encoder position in the transformer stack gives out a resultant vector of a hidden size of 768 (in the case of BERT Base)", "For the task of spam classification, just a single encoder position is enough. The resultant vector is then sent to a classifier of our choice, which usually turns out to be a single-layer neural network that has a feed-forward network that allocates the final weights and a softmax function for non-linearity. The softmax is a wonderful activation function in the sense that it converts the numerically improper logits into a balanced probabilistic distribution with all the probabilities summing to one.", "If there are more target labels that can automatically sort emails with as a \u201csocial post\u201d or \u201cpromotional email\u201d, it\u2019s enough that the classifier network is adjusted to have more output neurons (in this case, 4 output neurons) that finally pass through softmax function.", "The probability values obtained from the classifier gives the likelihood of the email being spam or otherwise. While spam classification is only a gist of how BERT can easily do it better, complex tasks like fake news detection, social sentiment classification, token review monitoring, etc are all room for play.", "As far as our experimentation at WatermelonBlock is concerned, BERT has been one of the many models like ELMo, ULMFiT, etc and we strive to create the best insights app for our customers through rapid brainstorming sessions.", "If you liked this technical blog and would like to know more on how engineers and data scientists at WatermelonBlock are building the next-gen products, please give this article a hearty clap and follow our Medium blogging channel at WatermelonBlock and also visit the announcements page on our website for upcoming news!", "3. \u201cOpen Sourcing BERT: State-Of-The-Art Pre-Training for Natural Language Processing\u201d, Google AI, 2018", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "MSc. Student \u2014 Kommunikationstechnik at Universit\u00e4t Ulm | Co-Ideator at Golith| Python | C++| Product Development"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F555ad7839a7c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bhat.karthik2?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bhat.karthik2?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "Karthik Ramesh Kamath"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa4d68681d168&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&user=Karthik+Ramesh+Kamath&userId=a4d68681d168&source=post_page-a4d68681d168----555ad7839a7c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F555ad7839a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F555ad7839a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.watermelonblock.io/", "anchor_text": "WatermelonBlock"}, {"url": "https://finance.yahoo.com/news/watermelonblock-partners-ibm-watson-using-123000544.html", "anchor_text": "Our partners at IBM"}, {"url": "https://developer.ibm.com/linuxonpower/deep-learning-powerai/", "anchor_text": "leveraging the IBM PowerAI servers"}, {"url": "https://twitter.com/watermelonblock/status/1078929013094010885", "anchor_text": "https://twitter.com/watermelonblock/status/1078929013094010885"}, {"url": "http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/", "anchor_text": "this awesome article by WildML"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax function for non-linearity"}, {"url": "https://www.watermelonblock.io/#team", "anchor_text": "engineers and data scientists at WatermelonBlock"}, {"url": "https://medium.com/u/2e4ae63ee111?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "WatermelonBlock"}, {"url": "https://www.watermelonblock.io/blog/", "anchor_text": "announcements page on our website"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "https://jalammar.github.io/illustrated-transformer/"}, {"url": "https://dzone.com/articles/self-attention-mechanisms-in-natural-language-proc", "anchor_text": "https://dzone.com/articles/self-attention-mechanisms-in-natural-language-proc"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----555ad7839a7c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----555ad7839a7c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/blockchain?source=post_page-----555ad7839a7c---------------blockchain-----------------", "anchor_text": "Blockchain"}, {"url": "https://medium.com/tag/cryptocurrency?source=post_page-----555ad7839a7c---------------cryptocurrency-----------------", "anchor_text": "Cryptocurrency"}, {"url": "https://medium.com/tag/data-science?source=post_page-----555ad7839a7c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F555ad7839a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&user=Karthik+Ramesh+Kamath&userId=a4d68681d168&source=-----555ad7839a7c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F555ad7839a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&user=Karthik+Ramesh+Kamath&userId=a4d68681d168&source=-----555ad7839a7c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F555ad7839a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F555ad7839a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----555ad7839a7c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----555ad7839a7c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----555ad7839a7c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----555ad7839a7c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bhat.karthik2?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bhat.karthik2?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Karthik Ramesh Kamath"}, {"url": "https://medium.com/@bhat.karthik2/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa4d68681d168&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&user=Karthik+Ramesh+Kamath&userId=a4d68681d168&source=post_page-a4d68681d168--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fa4d68681d168%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwatermelonblock-and-bert-till-better-nlp-models-do-us-part-555ad7839a7c&user=Karthik+Ramesh+Kamath&userId=a4d68681d168&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}