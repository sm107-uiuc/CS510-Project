{"url": "https://towardsdatascience.com/consistent-value-function-definitions-e685a191c5de", "time": 1683015079.985051, "path": "towardsdatascience.com/consistent-value-function-definitions-e685a191c5de/", "webpage": {"metadata": {"title": "Consistent Value Function Definitions | by Andy Patterson | Towards Data Science", "h1": "Consistent Value Function Definitions", "description": "In this post, I will define the standard \u03b3-discounted value functions used in reinforcement learning. From these definitions, I will discuss two important emergent properties of value functions that\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In this post, I will define the standard \u03b3-discounted value functions used in reinforcement learning. From these definitions, I will discuss two important emergent properties of value functions that prove the self-consistency of the definitions. I will build up these concepts mathematically, focusing on writing out every step in the derivations and discussing the implications of each step. These equations are the foundation of many important mathematical proofs in RL and understanding them completely is important to building a theoretical understanding of RL.", "Value functions are at the core of reinforcement learning. For any given state, an agent can query a value function to determine the \u201cvalue\u201d associated with being in that state. We traditionally define \u201cvalue\u201d as being the sum of rewards obtained into the future. Because of its dependence on what rewards the agent will see in the future, a value function must be defined for a given strategy of behavior; a policy. That is, the value of a state depends on how the agent behaves after visiting that state; a discussion of \u201cvalue\u201d that is independent of behavior is meaningless.", "We denote a policy as a function which maps a state to a probability distribution over actions. Formally,", "where \ud835\udcae denotes the set of all possible states that the agent can visit (often called the \u201cstate space\u201d), \ud835\udc9c denotes the set of all possible actions (often called the \u201caction space\u201d), and \u0394(\ud835\udc9c) denotes the standard simplex over the set of actions. The standard simplex is simply a formal way of writing a probability distribution over the action space. Simply put, a policy takes a state and returns a weighting over which actions the agent should take in that state. A large weighting leads to a frequency of selecting that action, a small weighting leads towards a low frequency.", "Equipped with an intuitive definition of value functions and a formal definition of policies, we can write a formal definition of value function:", "Note that in episodic problem settings we can assume that \u03b3=0 for all transitions after termination, thus these transitions contribute only a sequence of zeros to the sum which can be ignored.", "It is important to note that the definition given above is only one possible definition of a value function. A majority of the theory in reinforcement learning is developed around the \u03b3-discounted reward value function as defined above, and thus is the definition that we will use. However other alternatives have been considered, especially recently, including the average reward value function.", "The value function defined above asks the question: \u201cgiven that we are in a certain state, how much reward can we expect to see in the future?\u201d To use this value function to make decisions, the agent could query the value of possible next states. For instance, suppose an agent is in state A and there are two actions that lead to states B and C respectively, the agent can ask v(B): \u201chow much reward would I see in state B?\u201d or v(C): \u201chow much reward would I see in state C?\u201d", "The difficulty with using a state-value function to select actions is that it (a) requires that the agent know the result of an action ahead of time (e.g. that taking the \u201cleft\u201d action results in going to state B) and (b) requires that the actions deterministically lead to the same state every time. The reinforcement learning framework is designed to handle very generic problem settings where both (a) and (b) are not necessarily true.", "To overcome these challenges, we introduce the state-action value function (generally just called the \u201caction-value function\u201d; however I dislike this naming convention). The state-action value function is defined as", "Notice first that (as the name might imply) this is now a function of a state and an action, and returns an average over future rewards. The difference between this and the state value function lies only in the first action of the (S, A, R, S, A, R, \u2026) sequence. In the case of the state value function, the first action is a random variable distributed according to \u03c0(S\u2080); however, in the case of the state-action value function, the first action is non-random and is given.", "With access to a state-action value function, an agent need not know the state dynamics of its environment. It does not need to know that taking the \u201cleft\u201d action in state A will lead to state B, which has value v(B). Instead, the agent can query its state-action value function for the value of taking the \u201cleft\u201d action in state A, q(A, left). As we will see in the next section, the fact that the agent no longer needs access to a model of the environment to make decisions is a defining characteristic of the state-action value function. Intuitively, the state-action value function implicitly learns a model of the environment dynamics.", "Both v and q satisfy a set of so-called Bellman consistency equations. That is, for any state and action we have", "where r(s, a) is a function which returns the average one-step reward given state s and action a. That\u2019s a bit dense, so let\u2019s walk through it piece-by-piece.", "Equation (1) is simply saying: the state value function at state s is equal to the state-action value function at state s if you average over the actions. Writing the expectation as a sum, we get", "Which says: the state value function at s is the sum over all possible actions of the state-action value function, weighted by the probability of taking each action. This makes sense. Both value functions have the same definition for future rewards, follow policy \u03c0 and sum up the \u03b3-discounted rewards as you go. The value functions only differ on the first step, whether the action is given to us (state-action value functions) or whether we take the average over actions according to \u03c0 (state value functions).", "With access to a state-action value function, we can construct arbitrarily many possible state value functions by considering alternative policies for the first action in the sequence. For instance, imagine the agent would like to consider the scenario where (S, A~\u03bc, R, S, A~\u03c0, R, S, A~\u03c0, R, \u2026) where the first action is chosen from a policy \u03bc and all future actions are chosen according to a policy \u03c0. The value function for this conjunction of policies can be written", "This exact scenario occurs in the policy-improvement setting, where the agent improves its policy at only one state at a time. The agent seeks a better policy for the first state, assuming that it follows \u03c0 for all future states then according to the policy improvement theorem we can show that this new joint policy is at least as good as, if not better than, \u03c0.", "Equation (2) is trickier. Let\u2019s start with the easy part, the definition of this new function r(s, a),", "which is simply the average reward that I would see on this next transition starting in state s and taking action a.", "To understand the second part of Equation (2), let\u2019s first recall the definition of the value function.", "where the expectation is conditioned on s and a, but I left that detail out for space and clarity. I also went ahead and substituted the expectation over the first reward for the average reward function we computed above.", "Now using what we learned from the first consistency equation (Equation 1), we can write", "and we recover the second consistency equation, Equation (2).", "This second consistency equation gives us some intuition about the difference between a state-action value function and a state value function. Particularly, the state-action value function differs only in the very first reward of the sequence. In the state-action case, we consider a reward chosen by the deterministic action a, regardless of where that action may have come from (it could be sampled by the policy \u03c0, it could be sampled from some other policy, etc.). In the state value case, we consider the average reward where the average is taken over all actions according to policy \u03c0. We can consider no other distribution over actions, nor can we consider specific individual actions (which of course is a particular case of action distributions where all of the probability mass lies on one action).", "Having an understanding of the definitions of value functions and policies, as well as some of their core emergent properties, is crucial for the more rigorous constructions in RL proofs. Having the intuition helps to ease reading proofs and getting \u201cfundamental ideas,\u201d but intuition alone is not enough to grasp some of the subtleties in RL; thus often leading to incongruent misconceptions, or bugs in code. For example, a common misconception is that access to a state-action value function means that one can trivially compute values for any other policy by taking an expectation over actions of that state-action value function. However, it is clear (hopefully) that this expectation alters only the first step in the trajectory, but all future steps are with respect to the original policy \u03c0.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a PhD student at the University of Alberta studying Statistical Machine Learning and focusing on Reinforcement Learning and Representation Learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe685a191c5de&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e685a191c5de--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e685a191c5de--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andnp?source=post_page-----e685a191c5de--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andnp?source=post_page-----e685a191c5de--------------------------------", "anchor_text": "Andy Patterson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F421da56b0337&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&user=Andy+Patterson&userId=421da56b0337&source=post_page-421da56b0337----e685a191c5de---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe685a191c5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe685a191c5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@antoine1003?utm_source=medium&utm_medium=referral", "anchor_text": "Antoine Dautry"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----e685a191c5de---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----e685a191c5de---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/value-function?source=post_page-----e685a191c5de---------------value_function-----------------", "anchor_text": "Value Function"}, {"url": "https://medium.com/tag/bellman-equation?source=post_page-----e685a191c5de---------------bellman_equation-----------------", "anchor_text": "Bellman Equation"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----e685a191c5de---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe685a191c5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&user=Andy+Patterson&userId=421da56b0337&source=-----e685a191c5de---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe685a191c5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&user=Andy+Patterson&userId=421da56b0337&source=-----e685a191c5de---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe685a191c5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e685a191c5de--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe685a191c5de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e685a191c5de---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e685a191c5de--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e685a191c5de--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e685a191c5de--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e685a191c5de--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e685a191c5de--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e685a191c5de--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e685a191c5de--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e685a191c5de--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andnp?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andnp?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andy Patterson"}, {"url": "https://medium.com/@andnp/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "104 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F421da56b0337&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&user=Andy+Patterson&userId=421da56b0337&source=post_page-421da56b0337--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe8e6ccbd87fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconsistent-value-function-definitions-e685a191c5de&newsletterV3=421da56b0337&newsletterV3Id=e8e6ccbd87fb&user=Andy+Patterson&userId=421da56b0337&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}