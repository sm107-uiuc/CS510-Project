{"url": "https://towardsdatascience.com/step-by-step-build-a-data-pipeline-with-airflow-4f96854f7466", "time": 1683012474.27273, "path": "towardsdatascience.com/step-by-step-build-a-data-pipeline-with-airflow-4f96854f7466/", "webpage": {"metadata": {"title": "Step by step: build a data pipeline with Airflow | by Tony Xu | Towards Data Science", "h1": "Step by step: build a data pipeline with Airflow", "description": "Each time we deploy our new software, we will check the log file twice a day to see whether there is an issue or exception in the following one or two weeks. One colleague asked me is there a way to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://hub.docker.com/r/apache/airflow", "anchor_text": "Airflow images", "paragraph_index": 8}, {"url": "https://hub.docker.com/", "anchor_text": "Docker Hub", "paragraph_index": 8}, {"url": "https://hub.docker.com/r/puckel/docker-airflow", "anchor_text": "Puckel", "paragraph_index": 8}, {"url": "https://github.com/puckel/docker-airflow", "anchor_text": "Puckel\u2019s Github repository", "paragraph_index": 8}, {"url": "https://hub.docker.com/", "anchor_text": "Docker Hub", "paragraph_index": 10}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080", "paragraph_index": 11}, {"url": "https://support.google.com/mail/answer/185833?hl=en", "anchor_text": "App Password", "paragraph_index": 36}], "all_paragraphs": ["Each time we deploy our new software, we will check the log file twice a day to see whether there is an issue or exception in the following one or two weeks. One colleague asked me is there a way to monitor the errors and send alert automatically if a certain error occurs more than 3 times. I am following the Airflow course now, it\u2019s a perfect use case to build a data pipeline with Airflow to monitor the exceptions.", "Airflow is an open-source workflow management platform, It started at Airbnb in October 2014 and later was made open-source, becoming an Apache Incubator project in March 2016. Airflow is designed under the principle of \u201cconfiguration as code\u201d. [1]", "In Airflow, a DAG \u2014 or a Directed Acyclic Graph \u2014 is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.[2]", "Airflow uses Python language to create its workflow/DAG file, it\u2019s quite convenient and powerful for the developer.", "Our log files are saved in the server, there are several log files. We can fetch them by the sftp command. After downloading all the log files into one local folder, we can use the grep command to extract all lines containing exceptions or errors. The following is an example of an error log:", "Next, we need to parse the error message line by line and extract the fields. Like the above example, we want to know the file name, line number, date, time, session id, app name, module name, and error message. We will extract all this information into a database table, later on, we can use the SQL query to aggregate the information. If any type of error happens more than 3 times, it will trigger sending an email to the specified mailbox.", "The whole process is quite straightforward as following:", "Airflow provides a lot of useful operators. An operator is a single task, which provides a simple way to implement certain functionality. For example, BashOperator can execute a Bash script, command, or set of commands. SFTPOperator can access the server via an SSH session. Furthermore, Airflow allows parallelism amongst tasks, since an operator corresponds to a single task, which means all the operators can run in parallel. Airflow also provides a very simple way to define dependency and concurrency between tasks, we will talk about it later.", "Normally, Airflow is running in a docker container. Apache publishes Airflow images in Docker Hub. A more popular Airflow image is released by Puckel which is configurated well and ready to use. We can retrieve the docker file and all configuration files from Puckel\u2019s Github repository.", "After installing Docker client and pulling the Puckel\u2019s repository, run the following command line to start the Airflow server:", "When it\u2019s the first time to run the script, it will download Puckel\u2019s Airflow image and Postgres image from Docker Hub, then start two docker containers.", "Airflow has a nice UI, it can be accessed from http://localhost:8080.", "From the Airflow UI portal, it can trigger a DAG and show the status of the tasks currently running.", "Let\u2019s start to create a DAG file. It\u2019s pretty easy to create a new DAG. Firstly, we define some default arguments, then instantiate a DAG class with a DAG name monitor_errors, the DAG name will be shown in Airflow UI.", "The first step in the workflow is to download all the log files from the server. Airflow supports concurrency of running tasks. We create one downloading task for one log file, all the tasks can be running in parallel, and we add all the tasks into one list. SFTPOperator needs an SSH connection id, we will config it in the Airflow portal before running the workflow.", "After that, we can refresh the Airflow UI to load our DAG file. Now we can see our new DAG - monitor_errors - appearing on the list:", "Click the DAG name, it will show the graph view, we can see all the download tasks here:", "Before we trigger a DAG batch, we need to config the SSH connection, so that SFTPOperator can use this connection. Click the Admin menu then select Connections to create a new SSH connection.", "To access an SSH server without inputting a password, it needs to use the public key to log in. Assume the public key has already been put into server and the private key is located in /usr/local/airflow/.ssh/id_rsa. Leave Password field empty, and put the following JSON data into the Extra field.", "Ok, let\u2019s enable the DAG and trigger it, some tasks turn green which means they are in running state, the other tasks are remaining grey since they are in the queue.", "When all tasks finished, they are shown in dark green. Let\u2019s check the files downloaded into the data/ folder. It will create the folder with the current date.", "Next, we will extract all lines containing \u201cexception\u201d in the log files then write these lines into a file(errors.txt) in the same folder. grep command can search certain text in all the files in one folder and it also can include the file name and line number in the search result.", "Airflow checks the bash command return value as the task\u2019s running result. grep command will return -1 if no exception is found. Airflow treats non-zero return value as a failure task, however, it\u2019s not. No error means we\u2019re all good. We check the errors.txt file generated by grep. If the file exists, no matter it\u2019s empty or not, we will treat this task as a successful one.", "Refresh the DAG and trigger it again, the graph view will be updated as above. Let\u2019s check the output file errors.txt in the folder.", "Next, we will parse the log line by line and extract the fields we are interested in. We use a PythonOperator to do this job using a regular expression.", "The extracted fields will be saved into a database for later on the queries. Airflow supports any type of database backend, it stores metadata information in the database, in this example, we will use Postgres DB as backend.", "We define a PostgresOperator to create a new table in the database, it will delete the table if it\u2019s already existed. In a real scenario, we may append data into the database, but we shall be cautious if some tasks need to be rerun due to any reason, it may add duplicated data into the database.", "To use the Postgres database, we need to config the connection in the Airflow portal. We can modify the existing postgres_default connection, so we don\u2019t need to specify connection id when using PostgresOperator or PostgresHook.", "Great, let\u2019s trigger the DAG again.", "The tasks ran successfully, all the log data are parsed and stored in the database. Airflow provides a handy way to query the database. Choose \u201cAd Hoc Query\u201d under the \u201cData Profiling\u201d menu then type SQL query statement.", "Next, we can query the table and count the error of every type, we use another PythonOperator to query the database and generate two report files. One contains all the error records in the database, another is a statistics table to show all types of errors with occurrences in descending order.", "Two report files are generated in the folder.", "In error_logs.csv, it contains all the exception records in the database.", "In error_stats.csv, it lists different types of errors with occurrences.", "At last step, we use a branch operator to check the top occurrences in the error list, if it exceeds the threshold, says 3 times, it will trigger to send an email, otherwise, end silently. We can define the threshold value in the Airflow Variables, then read the value from the code. So that we can change the threshold later without modifying the code.", "BranchPythonOperator returns the next task\u2019s name, either to send an email or do nothing. We use the EmailOperator to send an email, it provides a convenient API to specify to, subject, body fields, and easy to add attachments. And we define an empty task by DummyOperator.", "To use the email operator, we need to add some configuration parameters in the YAML file. Here we define configurations for a Gmail account. You may put your password here or use App Password for your email client which provides better security.", "So far, we create all the tasks in the workflow, we need to define the dependency among these tasks. Airflow provides a very intuitive way to describe dependencies.", "Now, we finish all our coding part, let\u2019s trigger the workflow again to see the whole process.", "In our case, there are two types of error, both of them exceeds the threshold, it will trigger sending the email at the end. Two reports are attached to the email.", "We change the threshold variable to 60 and run the workflow again.", "As you can see, it doesn\u2019t trigger sending the email since the number of errors is less than 60. The workflow ends silently.", "Let\u2019s go back to the DAG view.", "It lists all the active or inactive DAGs and the status of each DAG, in our example, you can see, our monitor_errors DAG has 4 successful runs, and in the last run, 15 tasks are successful and 1 task is skipped which is the last dummy_op task, it\u2019s an expected result.", "Now our DAG is scheduled to run every day, we can change the scheduling time as we want, e.g. every 6 hours or at a specific time every day.", "Airflow is a powerful ETL tool, it\u2019s been widely used in many tier-1 companies, like Airbnb, Google, Ubisoft, Walmart, etc. And it\u2019s also supported in major cloud platforms, e.g. AWS, GCP, Azure. It plays a more and more important role in data engineering and data processing.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Certified IBM Data Scientist, Senior Android Developer, Mobile Designer, Embracing AI, Machine Learning\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4f96854f7466&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f96854f7466--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f96854f7466--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@fihtony?source=post_page-----4f96854f7466--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fihtony?source=post_page-----4f96854f7466--------------------------------", "anchor_text": "Tony Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3e1c5ea6f89a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&user=Tony+Xu&userId=3e1c5ea6f89a&source=post_page-3e1c5ea6f89a----4f96854f7466---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f96854f7466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f96854f7466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://hub.docker.com/r/apache/airflow", "anchor_text": "Airflow images"}, {"url": "https://hub.docker.com/", "anchor_text": "Docker Hub"}, {"url": "https://hub.docker.com/r/puckel/docker-airflow", "anchor_text": "Puckel"}, {"url": "https://github.com/puckel/docker-airflow", "anchor_text": "Puckel\u2019s Github repository"}, {"url": "https://hub.docker.com/", "anchor_text": "Docker Hub"}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080"}, {"url": "https://support.google.com/mail/answer/185833?hl=en", "anchor_text": "App Password"}, {"url": "https://github.com/kyokin78/airflow", "anchor_text": "https://github.com/kyokin78/airflow"}, {"url": "https://en.wikipedia.org/wiki/Apache_Airflow", "anchor_text": "https://en.wikipedia.org/wiki/Apache_Airflow"}, {"url": "https://airflow.apache.org/docs/stable/concepts.html", "anchor_text": "https://airflow.apache.org/docs/stable/concepts.html"}, {"url": "https://github.com/puckel/docker-airflow", "anchor_text": "https://github.com/puckel/docker-airflow"}, {"url": "https://medium.com/tag/airflow?source=post_page-----4f96854f7466---------------airflow-----------------", "anchor_text": "Airflow"}, {"url": "https://medium.com/tag/workflow?source=post_page-----4f96854f7466---------------workflow-----------------", "anchor_text": "Workflow"}, {"url": "https://medium.com/tag/pipeline?source=post_page-----4f96854f7466---------------pipeline-----------------", "anchor_text": "Pipeline"}, {"url": "https://medium.com/tag/error-handling?source=post_page-----4f96854f7466---------------error_handling-----------------", "anchor_text": "Error Handling"}, {"url": "https://medium.com/tag/data?source=post_page-----4f96854f7466---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f96854f7466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&user=Tony+Xu&userId=3e1c5ea6f89a&source=-----4f96854f7466---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f96854f7466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&user=Tony+Xu&userId=3e1c5ea6f89a&source=-----4f96854f7466---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f96854f7466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f96854f7466--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4f96854f7466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4f96854f7466---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f96854f7466--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4f96854f7466--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4f96854f7466--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4f96854f7466--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4f96854f7466--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4f96854f7466--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4f96854f7466--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4f96854f7466--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fihtony?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fihtony?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tony Xu"}, {"url": "https://medium.com/@fihtony/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "68 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3e1c5ea6f89a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&user=Tony+Xu&userId=3e1c5ea6f89a&source=post_page-3e1c5ea6f89a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1bfb9d350f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-build-a-data-pipeline-with-airflow-4f96854f7466&newsletterV3=3e1c5ea6f89a&newsletterV3Id=1bfb9d350f4b&user=Tony+Xu&userId=3e1c5ea6f89a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}