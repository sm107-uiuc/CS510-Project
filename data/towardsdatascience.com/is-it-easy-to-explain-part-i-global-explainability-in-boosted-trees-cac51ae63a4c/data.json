{"url": "https://towardsdatascience.com/is-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c", "time": 1683003235.861768, "path": "towardsdatascience.com/is-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c/", "webpage": {"metadata": {"title": "Is it easy to explain? Global explainability in Boosted Trees | by Dina Berenbaum | Towards Data Science", "h1": "Is it easy to explain? Global explainability in Boosted Trees", "description": "In this blog post as well as in an additional future post we aim to survey the existing methods for non-linear model explainability, focusing on the practicalities, limitations and mostly on the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/dinaber/Global-explainability", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://github.com/dinaber/Global-explainability/blob/master/myxgboost_model.ipynb", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "documentation", "paragraph_index": 11}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "here", "paragraph_index": 26}, {"url": "https://www.kaggle.com/c/titanic", "anchor_text": "here", "paragraph_index": 32}], "all_paragraphs": ["In this blog post as well as in an additional future post we aim to survey the existing methods for non-linear model explainability, focusing on the practicalities, limitations and mostly on the out-of-the-box behavior of existing implemented methods.We will start from global explanations, specifically those implemented on trees and boosted trees. Link to a future post that will deal with Local explainability will be added here once published (very soon\u2026)", "As models get more complex, they usually also become more of a \u201cblack box\u201d. While accuracy seems to improve, in many fields this is not enough. In order for people and regulators to trust the prediction a model makes they usually need reasoning \u2014 understanding what caused the model to make a certain prediction. In addition, researching, developing and especially debugging a black box is nearly impossible. Insights into the forces that are active within the model can help data scientists and professionals understand their model better and when it is more likely to fail.", "While many methods for explainability exist, we can roughly divide them into two main groups: (i) Global explainability and (ii) Local explainability.Global explainability strives to demonstrate something regarding the model as a whole, approaching it holistically, while the Local explanations are relevant for a small environment or a specific instance.", "Global explainability is especially useful for \u201cbehind the scenes\u201d purposes. When we want to learn on which feature our model relies and be able to debug it, recognize data leakage or consider performing feature selection.", "In this blog post I will focus on three different global explainability approaches:1. Tree construction measures (feature importance)2. Model score measures (permutations)3. Globalized local methods (SHAP)", "A notebook with all the code used in this post can be viewed here.", "The first method for global explainability I will discuss is the tree based models feature importance. To demonstrate some of the points made I will first create a synthetic dataset and train an XGBoost model on it. This is a very simple toy data that doesn\u2019t actually require the \u201cbig gun\u201d in the form of XGBoost that is used, but bear with me for a while.", "Some background: We are working with a non profit organization that needs help in identifying potential scholarship candidates out of the entire population aged 18\u201325. The perhaps obvious condition to receive a scholarship is being a student in an official university. Additionally, the income average also has an effect on the chances of an individual. To help them, we build an XGBoost model that outputs classification predictions based on historical data of scholarship recipients and these two features: 1. Whether the person is currently a student2. The income average binned based on the Quartiles: Q1 (lowest), Q2, Q3, Q4 (highest).Since XGBoost can\u2019t handle categorical data we have to do either label encoding or one hot encoding. We chose label encoding given that quartiles clearly have an inner-order. Quartile 1 will be encoded as 0, Q2 as 1 etc..", "For the model, as mentioned above, we use XGBoost. It is a decision tree based ensemble model. Briefly, tree ensemble models belong to either of the approaches: Bagging or Boosting. While Bagging utilizes combining multiple decisions from multiple trees to form the final prediction, Boosting is more dynamic. In Boosting the learning is consecutive, where misclassified samples are given higher weights in the subsequent learner. XGBoost stands for \u201cExtreme Gradient Boosting\u201d. Gradient boosting is a special case of boosting, while extreme gradient boosting is, well, extreme gradient boosting. In gradient boosting we use gradient descent as an optimizer and a loss function. Each tree tries to recover the loss of the previous tree.", "The full XGBoost class, which also includes hyper parameter search using Bayesian optimization can be found here. The current usage of the class:", "After training the XGBoost model we have our ensemble of decision trees constructed. At this stage it is very tempting to start asking questions regarding the role each of the features played in building the trees. But, since this is an ensemble of trees, and more over a gradient boosted one, simply looking at the tree nodes and branches and inferring this is not possible. Luckily for us, XGBoost python module has a Plotting API that offers a \u201cplot_importance\u201d function that will plot the importance of the various features for us. This function can be used as a black box, but what do we actually get?", "Interestingly, the default metric to compare the importance of the different features in XGBoost is weight. From the documentation:", "\u2018weight\u2019: the number of times a feature is used to split the data across all trees", "Using the plot_importance on the trained model with the synthetic data above gives the following result:", "The result seems to suggest that the income group is more important than being a student or not. But if we change a minor thing: encode the income groups slightly differently, so now we are after group number 0 instead of 1:", "Next, we train the model n times (100) on the both data sets, \u201cencode1\u201d and \u201cencode 2\u201d and plot the mean feature importance and standard deviation of both:", "All of a sudden, the student feature and income feature have almost the same importance! Simply changing the encoding caused the function to output a completely different result.", "This very simple example demonstrates a major issue that can arise while using the \u201cweight\u201d metric. Since weight is defined as the number of times a feature is used to split the data, it is very biased against binary features as opposed to multi categorical/continuous features. Simply changing the encoding allowed the model to make less splits on income group, thus reducing its importance. But does the number of splits a feature is responsible for really demonstrates its importance?", "The feature importance function provides us with a few more metric options. Above I plotted the importance results using all of the options, raw (left plot) and normalized (right plot). Comparing the results is really confusing. Each metric produces a different result for the feature importance question, not just in absolute values but also the relative distribution of importance between features. So which criterion is the correct one? which feature is more important?", "At least a partial answer to these question can be provided by reviewing each of the metrics.", "Gain \u2014 the average gain across all splits the feature is used in", "Total Gain \u2014 the total gain across all splits the feature is used in", "NOTE: In XGBoost \u201cgain\u201d is averaged across splits while usually it is not (same as \u201ctotal gain\u201d here).", "The gain represents the amount of purity that is achieved thanks to the split. This is weighted by the amount of data reaching this node. In regular decision trees gain is calculated using the \u201cGini index\u201d:", "For each class in the labels, the probability of randomly picking this class is multiplied by the complementary probability.", "This index is calculated for each node, and the gain is defined by the increase in purity of the data passing through this node compared to the purity of the children nodes. The full equation also includes weighted normalization based on the percentage of samples reaching each node:", "In Xgboost the gain used is a bit more complicated. For those who are interested, you can read the full description in the documentation here.", "Cover \u2014the average coverage across all splits the feature is used in.", "Total Cover \u2014 the total coverage across all splits the feature is used in", "From within code documentation (not easy to find): \u201ccoverage is the sum of second order gradient of training data classified to the leaf, if it is square loss, this simply corresponds to the number of instances in that branch. Deeper in the tree a node is, lower this metric will be\u201dMeaning, for each split we need to compute the second order gradient per sample and multiply by the number of samples. For p (prediction )and y (true label), in the case of \u2018reg:logistic \u2019objective the cost function is (y-p)\u00b2 , thus the second derivative is a constant. While for the case of \u2018binary:logistic\u2019 objective the cost function is -(y*log(p) + (y-1)*(log(1-p)) and the second derivative (with respect to x, with p=1/(1+e^-x) ) is p(1-p).", "Looking at figure 3 and the definitions of the various metrics, we can make some general observations:", "Another way to infer importance of features is to look how the score is affected when the feature is absent. To be exact, the feature is not completely dropped since we need it for the model, but rather replaced with \u201cnoise\u201d. The noise is a random value drawn from the same distribution as the original data. To avoid the need to produce new values for the feature, the noise is introduced by shuffling the existing data.", "For this section and further on (including future post) I will be using a more sophisticated and higher dimensional dataset from Kaggle: \u201cTitanic: Machine Learning from Disaster\u201d. More details about the dataset can be found here.", "The eli5 package provides a permutation importance wrapper that is compatible with many models. We will be using it. Note: the test set we are using for the permutation importance calculation is not the test set that is provided by Kaggle, but a randomly generated test set separated from the training set before training, and thus has labels.", "The most basic usage of eli5 permutations feature importance function is shown below.", "Using this function, I permuted both the test set and the training set five consecutive times, collecting the outputs and plotting them:", "The y axis shows the weights of the different features, which are the decrease in score that resulted in permuting this specific feature. The higher the value, the higher the influence, thus the importance of the feature. The different colored bars represent a separate run of the entire function, while the error bars represent a within run standard deviation resulting from the permutations.", "Should we use the train set or the test set? Intuitively, the evaluation of the score should be done on the test set, thus also the feature importance. Specifically, we want to understand the importance of the features on samples that the model wasn\u2019t trained on, thus to use the test set. Alternatively, we might want to evaluate our model for debugging purposes. In this case, we would want to understand how much it needs a specific feature to make its prediction and then perhaps using the training set makes more sense.Finally, perhaps comparing the permutation importance between the train set and test set can give us the most insights. In particular, a mismatch between the ordering and importance of features in the training set vs. test set can uncover a lack of generalization in our model.", "In any case, the decision whether to use the test set, training set or to compare them should be made based on our objective: do we want to evaluate our training procedure or do we want to evaluate the usage of features on new data.", "How should we deal with variance? Both inner variance (standard deviation within a run) and cross variance (figure 4, different colored bars) exist. This is of course due to the randomness in the algorithm as well as the fact that while we equate permutations to \u201cdropping the feature\u201d, we are actually using the existing values which could lead to all different combinations. In small dataset especially, this can even lead to combinations that even increase the score. The variation of course is problematic but can be stabilized by averaging the repeats.", "How to deal with feature dependencies and correlation? This is a major issue for this method (and in fact for most methods reviewed). The importance of features that depend on each other makes sense only when measured together. Given their combination they will make a strong predictor, while each of them separately doesn\u2019t explain the output as well. Currently there isn\u2019t an option in the library to permute combinations of features.", "Additionally, correlated features that are randomly permuted might cause improbable combinations which may be considered out-of-distribution. This can create a bias, resulting in the wrong value that represents the \u201cfeature dropped\u201d case.", "In order to compare the methods discussed we will look at the same Titanic data that was presented in the permutations section. We will compare the importance ordering of all features (ignoring the actual values) produced by: XGBoost feature importance function with the \u201ctotal gain\u201d metric and permutation method on the test set and permutation method on the training set.", "The table above shows the ranking of the features importance for each method. Feature ranked #1 is the most important feature according to this method, while feature ranked #9 is less important. As can be easily seen in the table, while some similarity in rankings exists between permutations calculated on the test data and permutations calculated on training data, very little correlation exist with the feature importance using the total gain method. In fact, out of the top three features in all three models, only one feature is shared between all three of them:", "Interestingly, but not surprisingly (if you read the first part of this blog post), feature importance has a clear preference to the continuous/categorical features such as Age or Fare. Note that it is true even if we use one-hot encoding and sum the scores between the synthetic features (can\u2019t be done directly if you use an averaged importance metric such as \u201cgain\u201d).", "Finally, to contrast with a known local method, that has some more theoretical basis behind it, we will also compare with the SHAP method. For more explanation about the SHAP method for local prediction you can read in a future post of the series: Local explainability which will be released very soon. Here though,I will be using the SHAP method on all instances from the test set and average them to produce a global explanation.", "In the figure above we see the average absolute value SHAP of the various features as they were calculated on the entire test set. Some of the more \u201cimportant\u201d features here are also ranked high in the previously tested global methods (Sex, Age, Fare), while others (Cabin, Title) are ranked relatively low in the global methods but seem to have a bigger influence when looking globally using a local method. In total, the SHAP method seems to produce somewhat of a hybrid result of the other two methods, giving high ranking to both categorical(Title), continuous(Age) and binary(Sex) features. We will discuss its advantages and limitations in a future blog post.", "To summarize, while no method has been proven to give a full answer to the \u201cwhich are the most important feature sin our model\u201d question, it is also not very clear what \u201cimportant\u201d even means. Should importance reflect the features that our model depends most in the process of its construction? or perhaps those that help if generalize the most? or maybe we should look at the features that allow for the best average accuracy? Meanwhile, it seems that with careful examination of our data (dependencies, binary vs. categorical vs. continuous features) and with specific intent in mind we can start navigating between the methods.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data science researcher at Demystify. Working on various challenges on the interface of data, AI, explainability and humans."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcac51ae63a4c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dinber19?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dinber19?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "Dina Berenbaum"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd32c7bf3f32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&user=Dina+Berenbaum&userId=dd32c7bf3f32&source=post_page-dd32c7bf3f32----cac51ae63a4c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcac51ae63a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcac51ae63a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@plushdesignstudio?utm_source=medium&utm_medium=referral", "anchor_text": "Plush Design Studio"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/dinaber/Global-explainability", "anchor_text": "here"}, {"url": "https://github.com/dinaber/Global-explainability/blob/master/myxgboost_model.ipynb", "anchor_text": "here"}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "documentation"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "here"}, {"url": "https://www.kaggle.com/c/titanic", "anchor_text": "here"}, {"url": "https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf", "anchor_text": "https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "https://christophm.github.io/interpretable-ml-book/"}, {"url": "https://medium.com/tag/explainable-ai?source=post_page-----cac51ae63a4c---------------explainable_ai-----------------", "anchor_text": "Explainable Ai"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----cac51ae63a4c---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/permutation-importance?source=post_page-----cac51ae63a4c---------------permutation_importance-----------------", "anchor_text": "Permutation Importance"}, {"url": "https://medium.com/tag/feature-importance?source=post_page-----cac51ae63a4c---------------feature_importance-----------------", "anchor_text": "Feature Importance"}, {"url": "https://medium.com/tag/shap?source=post_page-----cac51ae63a4c---------------shap-----------------", "anchor_text": "Shap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcac51ae63a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&user=Dina+Berenbaum&userId=dd32c7bf3f32&source=-----cac51ae63a4c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcac51ae63a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&user=Dina+Berenbaum&userId=dd32c7bf3f32&source=-----cac51ae63a4c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcac51ae63a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcac51ae63a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cac51ae63a4c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cac51ae63a4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dinber19?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dinber19?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dina Berenbaum"}, {"url": "https://medium.com/@dinber19/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "24 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd32c7bf3f32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&user=Dina+Berenbaum&userId=dd32c7bf3f32&source=post_page-dd32c7bf3f32--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fdd32c7bf3f32%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-easy-to-explain-part-i-global-explainability-in-boosted-trees-cac51ae63a4c&user=Dina+Berenbaum&userId=dd32c7bf3f32&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}