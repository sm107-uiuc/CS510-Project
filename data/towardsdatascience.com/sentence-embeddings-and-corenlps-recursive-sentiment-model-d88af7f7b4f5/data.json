{"url": "https://towardsdatascience.com/sentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5", "time": 1683014537.895465, "path": "towardsdatascience.com/sentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5/", "webpage": {"metadata": {"title": "Sentence Embeddings and CoreNLP\u2019s Recursive Sentiment Model | by Laura Bravo Priegue | Towards Data Science", "h1": "Sentence Embeddings and CoreNLP\u2019s Recursive Sentiment Model", "description": "Hello there! A couple of weeks ago I posted the first of a series of articles around the library coreNLP and more specifically its sentiment analysis model. This first article was an introduction to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/intro-to-stanfords-corenlp-and-java-for-python-programmers-c2586215aab6", "anchor_text": "first of a series of articles", "paragraph_index": 0}, {"url": "https://github.com/laurabravopriegue/coreNLP_tutorial", "anchor_text": "github", "paragraph_index": 0}, {"url": "https://nlp.stanford.edu/sentiment/", "anchor_text": "official page of coreNLP\u2019s sentiment classifier", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/intro-to-stanfords-corenlp-and-java-for-python-programmers-c2586215aab6?source=your_stories_page-------------------------------------", "anchor_text": "previous post", "paragraph_index": 30}, {"url": "https://nlp.stanford.edu/pubs/2010SocherManningNg.pdf", "anchor_text": "https://nlp.stanford.edu/pubs/2010SocherManningNg.pdf", "paragraph_index": 42}], "all_paragraphs": ["Hello there! A couple of weeks ago I posted the first of a series of articles around the library coreNLP and more specifically its sentiment analysis model. This first article was an introduction to the Java package and its main features, particularly targeted at people that are used to working with Python like myself. As promised, the second article of the series will go more in depth into CoreNLP\u2019s sentiment annotator: why it is not your usual sentiment classifier, the recursive model behind it and how to implement it with some simple Java script (which you can find on my github!).", "I guess, before I start, I should warn the reader: \u26d4\ufe0f this post talks very little about sentiment analysis and a lot about sentence embeddings \u26d4\ufe0f. Fear not, hopefully this will make sense to you as you read!", "Let\u2019s start by\u2026 googling CoreNLP sentiment model! When clicking into the official page of coreNLP\u2019s sentiment classifier, we find the following description", "Most sentiment prediction systems work just by looking at words in isolation, giving positive points for positive words and negative points for negative words and then summing up these points. That way, the order of words is ignored and important information is lost. In contrast, our new deep learning model actually builds up a representation of whole sentences based on the sentence structure. It computes the sentiment based on how words compose the meaning of longer phrases. This way, the model is not as easily fooled as previous models.", "After reading this paragraph, we can already tell two things:", "Point 2 already uncovers what is going to be the central theme of this post: text representation. In order to go in depth into why CoreNLP\u2019s sentiment model is so powerful and effective, we will firstly need to understand the importance of appropriately representing input text. And this is what we are going to talk about below!", "I will firstly introduce what the complexities of text representation (which I will call semantic modelling) are, as well as the limitations of well-known word embedding models such as word2vec. I will then talk about the concept of semantic compositionality and how it was used by coreNLP for creating a very powerful recursive sentiment analysis model. Finally, I will give an example of a simple implementation in Java. Let\u2019s gooo!", "I like to think of semantic modelling like this: humans can read and understand a sequence of letters and symbols (like this sentence), however, ML algorithms can only understand numerical sequences. In order for a sentiment classifier, or any other model, to process text, it must be translated from a human-readable form to a computer-readable form.", "Turns out there are maaany ways to represent text: as a bag of words, one hot encodings, logic-based frameworks, semantic vectors on an embedding space\u2026 And it is important to choose the best possible one, since this will impact directly the performance of your model. Think about it: how can we expect the model to classify the input text if it can not even understand it!", "Out of all the semantic modelling techniques I have named above, the use of semantic vectors is regarded as one of the preferred text representation options in the NLP literature. In particular, word embeddings have become a very popular method and attracted a lot of attention in recent years. Think word2vec, GloVe, FastText\u2026", "Word embeddings were built on the idea that it is possible to infer the meaning of a given word from its linguistic context (Mitchell and Lapata, 2010).", "In this framework, words are basically represented by vectors that carry latent semantic information on the particular word (Socher, 2013). Figure 1 represents a simple semantic two-dimensional space. Note that words that are similar appear closer together.", "However, despite their widespread use, the limitations of these models becomes quite evident when we are interested in computing the representation for a phrase or sentence. Word embedding models are only able to represent words in isolation and fail to account for the syntactical and grammatical associations between them.", "One of the solutions that is more frequently used when we want to represent a sentence but we only have its word\u2019s embeddings, is to average out its word vectors in order to obtain a sentence vector. This approach has proven to work well enough in some cases, but I personally find it very reductive since the specific syntax, grammar and dependencies of the sentence are just ignored.", "As a person with a linguistics and literature background, considering syntax, grammar and word order when doing text analysis is something that I wouldn\u2019t even question!! Having experienced the limitations of word embeddings myself, I came across the Principle of Semantic Compositionality (Frege, 1980) and started thinking about cool it would be to apply it to this semantic modelling task. This principle states that:", "\u201cthe meaning of a (syntactically complex) whole is a function only of the meaning of its (syntactic) parts together with the method by which these parts were combined\u201d (Pelletier, 1994, p.11).", "In the NLP literature, the most common interpretation and use of Frege\u2019s principle is as a theoretical ground which dictates that one should be able to explain the meaning of a complete sentence in terms of its phrases and that, similarly, it should also be possible to explain these phrases in terms of its words.", "Therefore, when it comes to the representation of language, the modeller should be aware that each syntactic operation additionally implies a semantic operation (Mitchell and Lapata, 2010). Partee (1995, p. 313) formally suggested formula 1 for expressing the composition of two elements u and v, where f is the composition function acting on the two constituents and R accounts for the syntactic relationship between u and v (Mitchell and Lapata, 2010).", "Building on word embeddings, some authors have attempted to include different methods for compositionality with the aim of embedding aspects of language such as word order and syntax (Socher, 2013). An example of this is Richard Socher and his Recursive Model.", "I got super excited when I found out about Socher\u2019s work because he was basically trying to incorporate this compositional idea onto his model for building a more complete form of sentence embedding. In order to do that he presented a new approach based on Recursive Neural Networks (RecNN).", "This model is based on the idea of computing a sentence embedding starting from its simpler elements (i.e. the words), and then using the same composition function recursively in a bottom-up fashion. This way of breaking down the sentence and then building it up in a recursive, bottom-up approach would allow the final output to better capture complex information concerning semantics, syntax and sentiment of the sentence. And I find this so cool!", "I will go now into giving an overview of the intuition behind the proposed method in Socher et al (2013). Figure 2 illustrates a simplified version of the compositional idea on which the recursive model is built. Throughout this section I will use this tri-gram as an example.", "Figure 2 depicts the sentence and its internal elements as nodes of a parse tree. This kind of parse tree is said to be binary, because each parent node has only two child nodes. The basic elements of the RecNN model are the leaves of the tree, therefore the processing begins by splitting the sentence into words and computing the word embedding for each. In the example above, the first step is to compute the representations a, b and c for the words \u2018not\u2019, \u2018very\u2019 and \u2018good\u2019 respectively.", "Subsequently, two of these word representations will be paired in order to compute a higher level phrase representation. The computation will be done by a composition function. In figure 2, p1 is computed by applying a composition function g to the word embeddings b and c.", "This compositional method for pairing nodes will be repeated recursively, bottom-up, until getting to the root of the tree. In figure 2, the root node is p2, and it is computed by applying the composition to the word embedding a and the phrase embedding p1. The root node is the highest node in the tree, which usually represents the full sentence.", "It is important to note that the composition function g is the same for both compositions in this example. Similarly, for longer sentences the composition function will always remain the same and will always take as input any pair of vectors. These vectors can represent any node at any level (e.g. a word, subphrase, phrase), but they must always have the same size. When combining the vectors b and c, the output of the composition p1 will also have the same dimensionality as b and c. Similarly, the output p2 of the combination of a and p1 will also have the same size. This is fundamental in order to allow for the recursive usage of the same composition function.", "You might be wondering where the sentiment is in all of this!", "The sentiment is actually predicted at every node using a softmax classifier, which uses the node vector as input features. This is represented in figure 2 by the coloured circles that emerge from nodes c, p1 and p2.", "Furthermore, the sentiment classification is multi-label, therefore the sentiment score will range between 0\u20134: 0 being very negative, 1 being negative, 2 being neutral, 3 being positive and 4 being very positive.", "The sentiment predicted at the root node of the tree will be the final sentiment assigned to the particular sentence. In the example of figure 2 we can see that the root node has been classified as negative, therefore the whole sentence will be negative.", "I will present now a very short extension of the script of the previous post in order to run some input text through the sentiment classifier and to get some metrics about the prediction output.", "The first step would be to include parse and sentiment in our list of annotators (we need parsing in order to run sentiment analysis).", "Having done that, we know that the input text now will be passed through the sentiment predictor, and therefore we just have to retrieve the results. We would firstly be interested in knowing what is the final sentiment score of a particular sentence (the prediction at the root node).", "We do that in order to save the final score in a variable named sentimentScore. This number will always be either 0, 1, 2, 3 or 4.", "Furthermore, we would like to know what are the probabilities that the predictor assigned a sentence of belonging to each of the classes. We obtained such information by doing:", "Probabilities will be stored in variables veryneg, neg, neutral, pos and verypos.", "Let\u2019s now run the whole file coreNLP_pipeline3_LBP.java to get an example output. We will use as input the following text in order to observe the changes in prediction: \u201cThis is a terrible sentence. I love this sentence so much! This is a normal sentence\u201d. This text is saved as coreNLPinput_2.txt. Run the script using the following command:", "One hand results will be printed onto the terminal such as in the screenshot below. We can observe that the scores assigned (the number right after \u201cFinal score\u201d ) make sense for the sentences: negative, positive and neutral. We also see that the probabilities are consistent and add up to 100.", "Furthermore, all results are printed onto a .txt document coreNLP_output2.txt that can be easily imported as a DataFrame in python using the command below. Resulting DataFrame will have 13 columns: \u2018par_id\u2019, \u2018sent_id\u2019, \u2018words\u2019, \u2018lemmas\u2019, \u2018posTags\u2019, \u2018nerTags\u2019, \u2018depParse\u2019, \u2018sentiment\u2019, \u2018veryneg\u2019, \u2018neg\u2019, \u2018neu\u2019, \u2018pos\u2019 and \u2018verypos\u2019.", "That\u2019s it for now! Hope you enjoyed it and that you got as excited about including syntax in a sentence vector as I got when I first came across this model! I feel that for literature majors like myself this is a very satisfying model to go through, since it\u2019s built on actual linguistic foundations.", "Next time we will continue talking about sentence embeddings! We would go through how to extract them from the coreNLP annotation object, we will compare them with other, more basic, sentence embeddings and explore their informativeness using some feature reduction and visualisation methods. We will also further use these vectors for compute more comprehensive document embeddings in order to perform sentiment analysis at the document level! \u270c\ud83c\udffb", "Frege, G., 1980. The foundations of arithmetic: A logico-mathematical enquiry into the concept of number. Northwestern University Press.", "Socher, R., Manning, C.D. and Ng, A.Y., 2010, December. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 deep learning and unsupervised feature learning workshop (Vol. 2010, pp. 1\u20139). Available at: https://nlp.stanford.edu/pubs/2010SocherManningNg.pdf", "Socher, R., Huval, B., Manning, C.D. and Ng, A.Y., 2012, July. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning (pp. 1201\u20131211). Association for Computational Linguistics.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer @ SpoonGuru | From Madrid | Always learning \u270c\ud83c\udffb"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd88af7f7b4f5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://laurabravopriegue.medium.com/?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": ""}, {"url": "https://laurabravopriegue.medium.com/?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "Laura Bravo Priegue"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50416f861dee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&user=Laura+Bravo+Priegue&userId=50416f861dee&source=post_page-50416f861dee----d88af7f7b4f5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd88af7f7b4f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd88af7f7b4f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://towardsdatascience.com/intro-to-stanfords-corenlp-and-java-for-python-programmers-c2586215aab6", "anchor_text": "first of a series of articles"}, {"url": "https://github.com/laurabravopriegue/coreNLP_tutorial", "anchor_text": "github"}, {"url": "https://unsplash.com/@ninjason?utm_source=medium&utm_medium=referral", "anchor_text": "Jason Leung"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://nlp.stanford.edu/sentiment/", "anchor_text": "official page of coreNLP\u2019s sentiment classifier"}, {"url": "https://towardsdatascience.com/intro-to-stanfords-corenlp-and-java-for-python-programmers-c2586215aab6?source=your_stories_page-------------------------------------", "anchor_text": "previous post"}, {"url": "https://github.com/laurabravopriegue/coreNLP_tutorial", "anchor_text": "https://github.com/laurabravopriegue/coreNLP_tutorial"}, {"url": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x", "anchor_text": "https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x"}, {"url": "https://nlp.stanford.edu/pubs/2010SocherManningNg.pdf", "anchor_text": "https://nlp.stanford.edu/pubs/2010SocherManningNg.pdf"}, {"url": "https://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf", "anchor_text": "https://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf"}, {"url": "https://www.aclweb.org/anthology/D13-1170", "anchor_text": "https://www.aclweb.org/anthology/D13-1170"}, {"url": "https://stanfordnlp.github.io/CoreNLP/", "anchor_text": "OverviewHigh-performance human language analysis tools, now with native deep learning modules in Python, available in many\u2026stanfordnlp.github.io"}, {"url": "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa", "anchor_text": "Introduction to Word Embedding and Word2VecWord embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of\u2026towardsdatascience.com"}, {"url": "https://nlp.stanford.edu/sentiment/", "anchor_text": "Deeply Moving: Deep Learning for Sentiment AnalysisThis website provides a live demo for predicting the sentiment of movie reviews. Most sentiment prediction systems work\u2026nlp.stanford.edu"}, {"url": "https://medium.com/tag/corenlp?source=post_page-----d88af7f7b4f5---------------corenlp-----------------", "anchor_text": "Corenlp"}, {"url": "https://medium.com/tag/sentiment?source=post_page-----d88af7f7b4f5---------------sentiment-----------------", "anchor_text": "Sentiment"}, {"url": "https://medium.com/tag/deep-neural-networks?source=post_page-----d88af7f7b4f5---------------deep_neural_networks-----------------", "anchor_text": "Deep Neural Networks"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----d88af7f7b4f5---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----d88af7f7b4f5---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd88af7f7b4f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&user=Laura+Bravo+Priegue&userId=50416f861dee&source=-----d88af7f7b4f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd88af7f7b4f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&user=Laura+Bravo+Priegue&userId=50416f861dee&source=-----d88af7f7b4f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd88af7f7b4f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd88af7f7b4f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d88af7f7b4f5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d88af7f7b4f5--------------------------------", "anchor_text": ""}, {"url": "https://laurabravopriegue.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://laurabravopriegue.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Laura Bravo Priegue"}, {"url": "https://laurabravopriegue.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "31 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50416f861dee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&user=Laura+Bravo+Priegue&userId=50416f861dee&source=post_page-50416f861dee--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F50416f861dee%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-embeddings-and-corenlps-recursive-sentiment-model-d88af7f7b4f5&user=Laura+Bravo+Priegue&userId=50416f861dee&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}