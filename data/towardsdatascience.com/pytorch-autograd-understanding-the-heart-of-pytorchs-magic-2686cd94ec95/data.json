{"url": "https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95", "time": 1682994522.2100961, "path": "towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95/", "webpage": {"metadata": {"title": "PyTorch Autograd. Understanding the heart of PyTorch\u2019s\u2026 | by Vaibhav Kumar | Towards Data Science", "h1": "PyTorch Autograd", "description": "Taking a closer look into PyTorch\u2019s autograd engine"}, "outgoing_paragraph_urls": [{"url": "https://pytorch.org/blog/pytorch-0_4_0-migration-guide/", "anchor_text": "as of PyTorch v0.4.0 Variable class has been deprecated.", "paragraph_index": 5}, {"url": "https://github.com/pytorch/pytorch/blob/master/docs/source/notes/autograd.rst", "anchor_text": "You don\u2019t have to encode all possible paths before you launch the training \u2014 what you run is what you differentiate.", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant", "anchor_text": "Jacobian matrix", "paragraph_index": 33}, {"url": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf", "anchor_text": "compute the JVP (Jacobian vector product) directly.", "paragraph_index": 34}, {"url": "https://pytorch.org/docs/stable/autograd.html", "anchor_text": "PyTorch: Automatic differentiation package \u2014 torch.autograd", "paragraph_index": 44}, {"url": "https://www.youtube.com/watch?v=MswxJw-8PvE", "anchor_text": "Video: PyTorch Autograd Explained \u2014 In-depth Tutorial", "paragraph_index": 45}, {"url": "https://www.youtube.com/channel/UCBCWYxhwuebyRTf4v3GkBlQ", "anchor_text": "Elliot Waite", "paragraph_index": 45}], "all_paragraphs": ["Let\u2019s just agree, we are all bad at calculus when it comes to large neural networks. It is impractical to calculate gradients of such large composite functions by explicitly solving mathematical equations especially because these curves exist in a large number of dimensions and are impossible to fathom.", "To deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say \u2018fourteen\u2019 to yourself very loudly. Everyone does it \u2014Geoffrey Hinton", "This is where PyTorch\u2019s autograd comes in. It abstracts the complicated mathematics and helps us \u201cmagically\u201d calculate gradients of high dimensional curves with only a few lines of code. This post attempts to describe the magic of autograd.", "We need to know about some basic PyTorch concepts before we move further.", "Tensors: In simple words, its just an n-dimensional array in PyTorch. Tensors support some additional enhancements which make them unique: Apart from CPU, they can be loaded or the GPU for faster computations. On setting .requires_grad = True they start forming a backward graph that tracks every operation applied on them to calculate the gradients using something called a dynamic computation graph (DCG) (explained further in the post).", "In earlier versions of PyTorch, thetorch.autograd.Variable class was used to create tensors that support gradient calculations and operation tracking but as of PyTorch v0.4.0 Variable class has been deprecated. torch.Tensor and torch.autograd.Variable are now the same class. More precisely, torch.Tensor is capable of tracking history and behaves like the old Variable", "Note: By PyTorch\u2019s design, gradients can only be calculated for floating point tensors which is why I\u2019ve created a float type numpy array before making it a gradient enabled PyTorch tensor", "Autograd: This class is an engine to calculate derivatives (Jacobian-vector product to be more precise). It records a graph of all the operations performed on a gradient enabled tensor and creates an acyclic graph called the dynamic computational graph. The leaves of this graph are input tensors and the roots are output tensors. Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule.", "Neural networks are nothing more than composite mathematical functions that are delicately tweaked (trained) to output the required result. The tweaking or the training is done through a remarkable algorithm called backpropagation. Backpropagation is used to calculate the gradients of the loss with respect to the input weights to later update the weights and eventually reduce the loss.", "In a way, back propagation is just fancy name for the chain rule \u2014 Jeremy Howard", "Creating and training a neural network involves the following essential steps:", "The change in the loss for a small change in an input weight is called the gradient of that weight and is calculated using backpropagation. The gradient is then used to update the weight using a learning rate to overall reduce the loss and train the neural net.", "This is done in an iterative way. For each iteration, several gradients are calculated and something called a computation graph is built for storing these gradient functions. PyTorch does it by building a Dynamic Computational Graph (DCG). This graph is built from scratch in every iteration providing maximum flexibility to gradient calculation. For example, for a forward operation (function)Mul a backward operation (function) called MulBackwardis dynamically integrated in the backward graph for computing the gradient.", "Gradient enabled tensors (variables) along with functions (operations) combine to create the dynamic computational graph. The flow of data and the operations applied to the data are defined at runtime hence constructing the computational graph dynamically. This graph is made dynamically by the autograd class under the hood. You don\u2019t have to encode all possible paths before you launch the training \u2014 what you run is what you differentiate.", "A simple DCG for multiplication of two tensors would look like this:", "Each dotted outline box in the graph is a variable and the purple rectangular box is an operation.", "Every variable object has several members some of which are:", "Data: It\u2019s the data a variable is holding. x holds a 1x1 tensor with the value equal to 1.0 while y holds 2.0. z holds the product of two i.e. 2.0", "requires_grad: This member, if true starts tracking all the operation history and forms a backward graph for gradient calculation. For an arbitrary tensor a It can be manipulated in-place as follows: a.requires_grad_(True).", "grad: grad holds the value of gradient. If requires_grad is False it will hold a None value. Even if requires_grad is True, it will hold a None value unless .backward() function is called from some other node. For example, if you call out.backward() for some variable out that involved x in its calculations then x.grad will hold \u2202out/\u2202x.", "grad_fn: This is the backward function used to calculate the gradient.", "is_leaf: A node is leaf if :", "On calling backward(), gradients are populated only for the nodes which have both requires_grad and is_leaf True. Gradients are of the output node from which .backward() is called, w.r.t other leaf nodes.", "On turning requires_grad = True PyTorch will start tracking the operation and store the gradient functions at each step as follows:", "The code that would generate the above graph under the PyTorch\u2019s hood is :", "To stop PyTorch from tracking the history and forming the backward graph, the code can be wrapped inside with torch.no_grad(): It will make the code run faster whenever gradient tracking is not needed.", "Backward is the function which actually calculates the gradient by passing it\u2019s argument (1x1 unit tensor by default) through the backward graph all the way up to every leaf node traceable from the calling root tensor. The calculated gradients are then stored in .grad of every leaf node. Remember, the backward graph is already made dynamically during the forward pass. Backward function only calculates the gradient using the already made graph and stores them in leaf nodes.", "An important thing to notice is that when z.backward() is called, a tensor is automatically passed as z.backward(torch.tensor(1.0)). The torch.tensor(1.0)is the external gradient provided to terminate the chain rule gradient multiplications. This external gradient is passed as the input to the MulBackward function to further calculate the gradient of x. The dimension of tensor passed into .backward() must be the same as the dimension of the tensor whose gradient is being calculated. For example, if the gradient enabled tensor x and y are as follows:", "and z = x * y", "then, to calculate gradients of z (a 1x3 tensor) with respect to x or y , an external gradient needs to be passed to z.backward()function as follows: z.backward(torch.FloatTensor([1.0, 1.0, 1.0])", "z.backward() would give a RuntimeError: grad can be implicitly created only for scalar outputs", "The tensor passed into the backward function acts like weights for a weighted output of gradient. Mathematically, this is the vector multiplied by the Jacobian matrix of non-scalar tensors (discussed further in this post) hence it should almost always be a unit tensor of dimension same as the tensor backward is called upon, unless weighted outputs needs to be calculated.", "tldr : Backward graph is created automatically and dynamically by autograd class during forward pass. Backward() simply calculates the gradients by passing its argument to the already made backward graph.", "Mathematically, the autograd class is just a Jacobian-vector product computing engine. A Jacobian matrix in very simple words is a matrix representing all the possible partial derivatives of two vectors. It\u2019s the gradient of a vector with respect to another vector.", "Note: In the process PyTorch never explicitly constructs the whole Jacobian. It\u2019s usually simpler and more efficient to compute the JVP (Jacobian vector product) directly.", "If a vector X = [x1, x2,\u2026.xn] is used to calculate some other vector f(X) = [f1, f2, \u2026. fn] through a function f then the Jacobian matrix (J) simply contains all the partial derivative combinations as follows:", "Above matrix represents the gradient of f(X)with respect to X", "Suppose a PyTorch gradient enabled tensors X as:", "X = [x1, x2, \u2026.. xn] (Let this be the weights of some machine learning model)", "X undergoes some operations to form a vector Y", "Y is then used to calculate a scalar loss l. Suppose a vector v happens to be the gradient of the scalar loss l with respect the vector Y as follows", "The vector v is called the grad_tensor and passed to the backward() function as an argument", "To get the gradient of the loss l with respect to the weights X the Jacobian matrix J is vector-multiplied with the vector v", "This method of calculating the Jacobian matrix and multiplying it with a vector v enables the possibility for PyTorch to feed external gradients with ease for even the non-scalar outputs.", "PyTorch: Automatic differentiation package \u2014 torch.autograd", "Video: PyTorch Autograd Explained \u2014 In-depth Tutorial by Elliot Waite", "Thank you for reading! Feel free to express any queries in the responses.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "An actual Time Traveller from the past"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2686cd94ec95&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@timetraveller1998?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@timetraveller1998?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "Vaibhav Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30121c857f6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&user=Vaibhav+Kumar&userId=30121c857f6c&source=post_page-30121c857f6c----2686cd94ec95---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2686cd94ec95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2686cd94ec95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------", "anchor_text": "In the process it never explicitly constructs the whole Jacobian. It\u2019s usually simpler and more efficient to compute the JVP directly."}, {"url": "https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------", "anchor_text": "Source: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf"}, {"url": "http://bumpybrains.com/comics.php?comic=34", "anchor_text": "Source: http://bumpybrains.com/comics.php?comic=34"}, {"url": "https://pytorch.org/blog/pytorch-0_4_0-migration-guide/", "anchor_text": "as of PyTorch v0.4.0 Variable class has been deprecated."}, {"url": "https://github.com/pytorch/pytorch/blob/master/docs/source/notes/autograd.rst", "anchor_text": "You don\u2019t have to encode all possible paths before you launch the training \u2014 what you run is what you differentiate."}, {"url": "https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------", "anchor_text": "In the process it never explicitly constructs the whole Jacobian. It\u2019s usually simpler and more efficient to compute the JVP directly."}, {"url": "https://medium.com/@timetraveller1998/that-is-true-b02561bf9c1c?source=responses---------0---------------------", "anchor_text": "Source: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf"}, {"url": "https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant", "anchor_text": "Jacobian matrix"}, {"url": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf", "anchor_text": "compute the JVP (Jacobian vector product) directly."}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html", "anchor_text": "Backpropagation: Quick revision"}, {"url": "https://pytorch.org/docs/stable/autograd.html", "anchor_text": "PyTorch: Automatic differentiation package \u2014 torch.autograd"}, {"url": "https://github.com/pytorch/pytorch/tree/master/torch/autograd", "anchor_text": "Autograd source code"}, {"url": "https://www.youtube.com/watch?v=MswxJw-8PvE", "anchor_text": "Video: PyTorch Autograd Explained \u2014 In-depth Tutorial"}, {"url": "https://www.youtube.com/channel/UCBCWYxhwuebyRTf4v3GkBlQ", "anchor_text": "Elliot Waite"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----2686cd94ec95---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2686cd94ec95---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2686cd94ec95---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----2686cd94ec95---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/python?source=post_page-----2686cd94ec95---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2686cd94ec95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&user=Vaibhav+Kumar&userId=30121c857f6c&source=-----2686cd94ec95---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2686cd94ec95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&user=Vaibhav+Kumar&userId=30121c857f6c&source=-----2686cd94ec95---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2686cd94ec95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2686cd94ec95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2686cd94ec95---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2686cd94ec95--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2686cd94ec95--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2686cd94ec95--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@timetraveller1998?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@timetraveller1998?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vaibhav Kumar"}, {"url": "https://medium.com/@timetraveller1998/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "419 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30121c857f6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&user=Vaibhav+Kumar&userId=30121c857f6c&source=post_page-30121c857f6c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe2e0c2d3e1ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&newsletterV3=30121c857f6c&newsletterV3Id=e2e0c2d3e1ba&user=Vaibhav+Kumar&userId=30121c857f6c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}