{"url": "https://towardsdatascience.com/analyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b", "time": 1683007041.915195, "path": "towardsdatascience.com/analyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b/", "webpage": {"metadata": {"title": "Analyzing how StyleGAN works: style incorporation in high-quality image generation | by Nikolas Adaloglou | Towards Data Science", "h1": "Analyzing how StyleGAN works: style incorporation in high-quality image generation", "description": "But how far are we from generating realistic style-based images? Take a quick glance how stylish a real photo can be: To this end, in this part, we will focus on style incorporation via adaptive\u2026"}, "outgoing_paragraph_urls": [{"url": "https://theaisummer.com/gan-computer-vision/#info-gan-representation-learning-by-information-maximizing-generative-adversarial-nets-2016", "anchor_text": "InfoGAN", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1703.06868", "anchor_text": "AdaIN", "paragraph_index": 3}, {"url": "https://www.tensorflow.org/tutorials/generative/style_transfer", "anchor_text": "style transfer", "paragraph_index": 3}, {"url": "https://theaisummer.com/gan-computer-vision-incremental-training/#progressive-gan-progressive-growing-of-gans-for-improved-quality-stability-and-variation-2017", "anchor_text": "part", "paragraph_index": 3}, {"url": "https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo", "anchor_text": "spatially invariant", "paragraph_index": 4}, {"url": "https://github.com/xunhuang1995/AdaIN-style", "anchor_text": "here (official)", "paragraph_index": 12}, {"url": "https://github.com/naoto0804/pytorch-AdaIN", "anchor_text": "here (unofficial)", "paragraph_index": 12}, {"url": "http://d2l.ai/chapter_computer-vision/transposed-conv.html", "anchor_text": "transpose convolutional", "paragraph_index": 14}, {"url": "https://stats.stackexchange.com/questions/252810/in-cnn-are-upsampling-and-transpose-convolution-the-same", "anchor_text": "upsample", "paragraph_index": 14}, {"url": "https://theaisummer.com/gan-computer-vision/#info-gan-representation-learning-by-information-maximizing-generative-adversarial-nets-2016", "anchor_text": "InfoGAN", "paragraph_index": 29}, {"url": "https://tiborstanko.sk/lerp-vs-slerp.html", "anchor_text": "spherical interpolation", "paragraph_index": 32}, {"url": "https://github.com/google/compare_gan", "anchor_text": "library", "paragraph_index": 38}, {"url": "https://github.com/unit8co/vegans", "anchor_text": "one", "paragraph_index": 38}, {"url": "https://theaisummer.com/gan-computer-vision-semantic-synthesis/", "anchor_text": "The next part", "paragraph_index": 39}], "all_paragraphs": ["But how far are we from generating realistic style-based images? Take a quick glance how stylish a real photo can be:", "To this end, in this part, we will focus on style incorporation via adaptive instance normalization. To do so, we will revisit concepts of in-layer normalization that will be proven quite useful in our understanding of GANs.", "Building on our understanding of GANs, instead of just generating images, we will now be able to control their style! How cool is that? But, wait a minute. We already saw in that InfoGAN can control image generations that are dependent on disentangled representations.", "This work is heavily dependent on Progressive GANs, Adaptive Instance Normalization( AdaIN), and style transfer. We have already covered Progressive GANs in the previous part, so let\u2019s dive into the rest of them before we focus on the understanding of this work.", "The human visual system is strongly attuned to image statistics. It is known that spatially invariant statistics such as channel-wise mean and variance reliably encode the style of an image. Meanwhile, spatially varying features encode a specific instance.", "In the depicted equations, N is the number of image batch H the height and W the width. The Greek letter \u03bc() refers to mean and the Greek letter \u03c3() refers to standard deviation. Similarly, \u03b3 and \u03b2 correspond to the trainable parameters that result in the linear/affine transformation, which is different for all channels. Specifically \u03b3, \u03b2 are vectors with the channel dimensionality. The batch features are x with a shape of [N, C, H, W], where the index c denotes the per-channel mean. Notably, the spatial dimensions, as well as the image batch, are averaged. This way, we concentrate our features in a compact space, which is usually beneficial.", "However, in terms of style and global characteristics, all individual channels share the shame learned characteristics, namely \u03b3,\u03b2. Therefore, BN can be intuitively understood as normalizing a batch of images to be centered around a single style. Still, the convolutional layers are able to learn some intra-batch style differences. As such, every single sample may still have different styles. This, for example, was undesirable if you want to transfer all images to the same shared style (i.e. Van Gogh style).", "But what if we don\u2019t mix the feature batch characteristics?", "Different from the BN layer, Instance Normalization (IN) is computed only across the features spatial dimensions, but again independently for each channel (and each sample). Literally, we just remove the sum over N in the previous equation. Surprisingly, it is experimentally validated that the affine parameters in IN can completely change the style of the output image. As opposed to BN, IN can normalize the style of each individual sample to a target style (modeled by \u03b3 and \u03b2). For this reason, training a model to transfer to a specific style is easier. Because the rest of the network can focus its learning capacity on content manipulation and local details while discarding the original global ones (i.e. style information).", "In this manner, by introducing a set that consists of multiple \u03b3, one can design a network to model a plethora of finite styles, which is exactly the case of conditional instance normalization.", "The idea of style transfer of another image starts to become natural. What if \u03b3, \u03b2 is injected from the feature statistics of another image? In this way, we will be able to model any arbitrary style by just giving our desired feature image mean as \u03b2 and variance as \u03b3. AdaIN does exactly that: it receives an input x(content) and a style input y, and simply aligns the channel-wise mean and variance of x to match those of y. Mathematically:", "That\u2019s all! So what can we do with just a single layer with this minor modification? Let us see!", "In the upper part, you see a simple encoder-decoder network architecture with an extra layer of AdaIN for style alignment. In the lower part, you see some results of this amazing idea! To summarize, AdaIN performs style transfer (in the feature space) by aligning the first-order statistics (\u03bc and \u03c3), at no additional cost in terms of complexity. If you want to play around with this idea code is available here (official) and here (unofficial)", "Let\u2019s go back to our original goal of understanding Style-GAN. Basically, NVIDIA in this work totally nailed our understanding and design of the majority of the generators in GANs. Let\u2019s see how.", "In a common GAN generator, the sampled input latent space vector z is projected and reshaped, so it will be further processed by transpose convolutional or upsample with or without convolutions. Here, the latent vector is transformed by a series of fully connected layers, the so-called mapping network f! This results in another learned vector w, called intermediate latent space W. But why would somebody do that?", "The major reason for this choice is that the intermediate latent space W does not have to support sampling according to any fixed distribution. With continuous mapping, its sampling density is induced. This mapping f \u201cunwraps\u201d the space of W, so it implicitly enforces a sort of disentangled representation. This means that the factors of variation become more linear. The authors argue that it will be easier to generate realistic images based on a disentangled representation compared to entangled ones. With this totally unsupervised trick, we at least expect W to be less entangled than Z space. Let\u2019s move onto the depicted A in the figure.", "Furthermore, the authors provide G with explicit noise inputs, as a direct method to model stochastic details. B blocks are single-channel images consisting of uncorrelated Gaussian noise. They are fed as an additive noise image to each layer of the synthesis network. The single noise image is broadcasted to all feature maps.", "Except for the initial block, each layer starts with an upsampling layer to double spatial resolution. Then, a convolution block is added. After each convolution, a 2D per-pixel noise is added to model stochasticity. Finally, with the magic AdaIN layer, the learned intermediate latent space that corresponds to style is injected.", "Instead of truncating the latent vector z as in BigGAN, the use it in the intermediate latent space W. This is implemented as: w\u2019 = E( w) \u2014 \u03c8* ( w \u2014 E( w) ), where E(w)= E(f(z)). \u0395 denotes the expected value. The important parameter that controls sample quality is \u03c8. When it is closer to 0, we roughly get the sampled faces converging to the mean image of the dataset. Truncation in W space seems to work well as illustrated in the image below:", "As perfectly described by the original paper:", "\u201cIt is interesting that various high-level attributes often flip between the opposites, including viewpoint, glasses, age, coloring, hair length, and often gender. \u201c ~ Tero Karras et al", "Another trick that was introduced is the style mixing. Sampling 2 samples from the latent space Z they generate two intermediate latent vectors w1, w2 corresponding to two styles. w1 is applied before the crossover point and w2 after it. This probably is performed inside the block. This regularization trick prevents the network from assuming that adjacent styles are correlated. A percentage of generated images (usually 90%) use this trick and it is randomly applied to a different place in the network every time.", "Each injected style and noise (blocks A and B) are localized in the network. This means that when modifying a specific subset of styles/noises, it is expected to affect only certain aspects of the image.", "Style: Let\u2019s see the reason for this localization, starting from style. We extensively saw that AdaIN operation first normalizes each channel to zero mean and unit variance. Then, it applies the style-based scales and biases. In this way, the feature statistics for the subsequent convolution operation are changed. Less literally, previous statistics/styles are discarded in the next AdaIN layer. Thus, each style controls only one convolution before being overridden by the next AdaIN operation.", "Noise: In a conventional generator, the latent vector z is fed in the input of the network. This is considered sub-optimal as it consumes the generator\u2019s learning capacity. This is justified as the need of the network to invent a way to generate spatially-varying numbers from earlier activations.", "By adding per-pixel noise after each convolution, it is experimentally validated that the effect of noise appears localized in the network. New noise is available for every layer, similar to BigGAN, and thus there is no motivation to generate the stochastic effects from previous activations.", "All the above can be illustrated below:", "On the left, we have the generated image. In the middle, 4 different noises applied to a selected sub-region. The standard deviation of a big set of samples with different noise can be observed on the right.", "The awesomeness arises by the fact that they were also able to quantify the disentanglement of spaces for the first time! Because if you can\u2019t count it, it doesn\u2019t exist! To this end, they introduce two new ways of quantifying the disentanglement of spaces.", "In case you don\u2019t feel comfortable with entangled and disentangled representations you can revisit InfoGAN. In very simple terms, entangled is mixed and disentangled related to encoded but separable in some way. I like to refer to disentangled representations as a kind of decoded information of a lower dimensionality of the data.", "Let us suppose that we have two latent space vectors and we would like to interpolate between them? How could we possibly identify that the \u201cwalking of the latent space\u201d corresponds in an entangled or disentangled representation? Intuitively, a less-sharp latent space should result in a perceptually smooth transition as observed in the image.", "Interpolation of latent-space vectors can tell us a lot. For instance, non-linear, non-smooth, and sharp changes in the image may appear. How would you call such a representation? As an example, features that are absent in either endpoint can appear in the middle of a linear interpolation path. This is a sign of a messy world, namely entangled representation.", "The quantification comes in terms of a small step \u03b5 in the latent space. If we subdivide a latent space interpolation path into small segments, we can measure a distance. The latter is measured between two steps, specifically, t, where t in [0,1] and t+\u03b5. However, it is meaningful to measure the distance based on the generated images. So, one can sum the distances from all the steps to walk across two random samples of latent space z1 and z2. Note that the distance is measured in the image space. Actually, in this work, they measure the pairwise distance between two VGG network embeddings. Mathematically this can be described as (slerp denotes spherical interpolation):", "Interestingly, they found that by adding noise, the path length (average distance) is roughly halved while mixing styles is a little bit increased (+10%). Furthermore, this measurement proves that the 8-layer fully connected architecture clearly produces an intermediate latent space W, which is more disentangled than Z.", "Let\u2019s see how this works. Hold tight!", "Quantitative results can be observed in the following table:", "In essence, the conditional entropy H tells how much additional information is required to determine the true class of a sample, given the SVM label X. An ideal linear SVM would result in knowing Y with full certainty resulting in entropy of 0. A high value for the entropy would suggest high uncertainty, so the label based on a linear SVM model is not informative at all. Less literally, the lower the entropy H, the better.", "The engineering solutions that are proposed on GANs never cease to amaze me. In this article, we saw an exciting design to inject the style of a reference image via Adaptive Instance Normalization. Style-GAN is definitely one of the most revolutionary works in the field. Finally, we highlighted the proposed metrics for linear separability, which makes us dive into more and more advanced concepts in this series.", "As always, we focus on intuitions and we believe that you are not discouraged to start experimenting with GANs. If you would like to start experimenting with a bunch of models to reproduce the state-of-the-art results, you should definitely check this open-source library in Tensorflow or this one in Pytorch.", "The next part is available on AI Summer!", "[1] Karras, T., Laine, S., & Aila, T. (2019). A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4401\u20134410).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F80a29227075b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----80a29227075b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----80a29227075b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://black-adaloglou.medium.com/?source=post_page-----80a29227075b--------------------------------", "anchor_text": ""}, {"url": "https://black-adaloglou.medium.com/?source=post_page-----80a29227075b--------------------------------", "anchor_text": "Nikolas Adaloglou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c9494a3541d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&user=Nikolas+Adaloglou&userId=1c9494a3541d&source=post_page-1c9494a3541d----80a29227075b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80a29227075b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80a29227075b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://theaisummer.com/gan-computer-vision-video-synthesis/", "anchor_text": "post"}, {"url": "https://unsplash.com/@anacariteph?utm_source=medium&utm_medium=referral", "anchor_text": "anabelle carite"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "StyleGAN"}, {"url": "https://theaisummer.com/gan-computer-vision/#info-gan-representation-learning-by-information-maximizing-generative-adversarial-nets-2016", "anchor_text": "InfoGAN"}, {"url": "https://arxiv.org/abs/1703.06868", "anchor_text": "AdaIN"}, {"url": "https://www.tensorflow.org/tutorials/generative/style_transfer", "anchor_text": "style transfer"}, {"url": "https://theaisummer.com/gan-computer-vision-incremental-training/#progressive-gan-progressive-growing-of-gans-for-improved-quality-stability-and-variation-2017", "anchor_text": "part"}, {"url": "https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo", "anchor_text": "spatially invariant"}, {"url": "https://arxiv.org/abs/1607.08022", "anchor_text": "Instance normalization"}, {"url": "https://arxiv.org/abs/1703.06868", "anchor_text": "Adaptive Instance Normalization (AdaIN)"}, {"url": "https://arxiv.org/abs/1703.06868", "anchor_text": "https://arxiv.org/abs/1703.06868"}, {"url": "https://github.com/xunhuang1995/AdaIN-style", "anchor_text": "here (official)"}, {"url": "https://github.com/naoto0804/pytorch-AdaIN", "anchor_text": "here (unofficial)"}, {"url": "http://d2l.ai/chapter_computer-vision/transposed-conv.html", "anchor_text": "transpose convolutional"}, {"url": "https://stats.stackexchange.com/questions/252810/in-cnn-are-upsampling-and-transpose-convolution-the-same", "anchor_text": "upsample"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "https://arxiv.org/abs/1812.04948"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "https://arxiv.org/abs/1812.04948"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "https://arxiv.org/abs/1812.04948"}, {"url": "https://theaisummer.com/gan-computer-vision/#info-gan-representation-learning-by-information-maximizing-generative-adversarial-nets-2016", "anchor_text": "InfoGAN"}, {"url": "https://tiborstanko.sk/lerp-vs-slerp.html", "anchor_text": "spherical interpolation"}, {"url": "https://ranvir.xyz/blog/svm-support-vector-machines-in-machine-learning/", "anchor_text": "SVM"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "https://arxiv.org/abs/1812.04948"}, {"url": "https://github.com/google/compare_gan", "anchor_text": "library"}, {"url": "https://github.com/unit8co/vegans", "anchor_text": "one"}, {"url": "https://theaisummer.com/gan-computer-vision-semantic-synthesis/", "anchor_text": "The next part"}, {"url": "https://theaisummer.com/gan-computer-vision-style-gan/", "anchor_text": "https://theaisummer.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----80a29227075b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/image-generation?source=post_page-----80a29227075b---------------image_generation-----------------", "anchor_text": "Image Generation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----80a29227075b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/generative-adversarial?source=post_page-----80a29227075b---------------generative_adversarial-----------------", "anchor_text": "Generative Adversarial"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----80a29227075b---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F80a29227075b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&user=Nikolas+Adaloglou&userId=1c9494a3541d&source=-----80a29227075b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F80a29227075b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&user=Nikolas+Adaloglou&userId=1c9494a3541d&source=-----80a29227075b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80a29227075b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----80a29227075b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F80a29227075b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----80a29227075b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----80a29227075b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----80a29227075b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----80a29227075b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----80a29227075b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----80a29227075b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----80a29227075b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----80a29227075b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----80a29227075b--------------------------------", "anchor_text": ""}, {"url": "https://black-adaloglou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://black-adaloglou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nikolas Adaloglou"}, {"url": "https://black-adaloglou.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c9494a3541d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&user=Nikolas+Adaloglou&userId=1c9494a3541d&source=post_page-1c9494a3541d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1e4bf9fa33a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-how-stylegan-works-style-incorporation-in-high-quality-image-generation-80a29227075b&newsletterV3=1c9494a3541d&newsletterV3Id=1e4bf9fa33a1&user=Nikolas+Adaloglou&userId=1c9494a3541d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}