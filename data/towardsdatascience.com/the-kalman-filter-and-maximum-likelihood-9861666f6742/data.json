{"url": "https://towardsdatascience.com/the-kalman-filter-and-maximum-likelihood-9861666f6742", "time": 1682996528.8926342, "path": "towardsdatascience.com/the-kalman-filter-and-maximum-likelihood-9861666f6742/", "webpage": {"metadata": {"title": "The Kalman Filter and (Maximum) Likelihood | by Ben Ogorek | Towards Data Science", "h1": "The Kalman Filter and (Maximum) Likelihood", "description": "Use Python\u2019s statsmodels to estimate unknown parameters in the Kalman Filter, calculate the log-likelihood of individual observations, and explore the impacts of different state initializations."}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/web-traffic-time-series-forecasting", "anchor_text": "Web Traffic Time Series Forecasting", "paragraph_index": 0}, {"url": "https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43727#latest-492742", "anchor_text": "8th place with Kalman filters", "paragraph_index": 0}, {"url": "https://medium.com/@oseiskar", "anchor_text": "Otto Seiskari", "paragraph_index": 1}, {"url": "https://github.com/oseiskar/simdkalman", "anchor_text": "simdkalman", "paragraph_index": 2}, {"url": "https://simdkalman.readthedocs.io/en/latest/", "anchor_text": "simdkalman docs", "paragraph_index": 2}, {"url": "https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html", "anchor_text": "stats.arima", "paragraph_index": 3}, {"url": "https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python", "anchor_text": "Kalman and Bayesian Filters in Python", "paragraph_index": 5}, {"url": "https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/9e4ac42f796a65e5d2baffd57a0fb88391a3d956/07-Kalman-Filter-Math.ipynb", "anchor_text": "Kalman Filter Math", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/yet-another-kalman-filter-explanation-article-be0264d99937", "anchor_text": "Yet Another Kalman Filter Explanation Article", "paragraph_index": 14}, {"url": "https://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf", "anchor_text": "these notes", "paragraph_index": 19}, {"url": "https://projecteuclid.org/euclid.aos/1176348139", "anchor_text": "Piet de Jong, 1991", "paragraph_index": 20}, {"url": "https://cran.r-project.org/web/packages/KFAS/vignettes/KFAS.pdf", "anchor_text": "R package KFAS", "paragraph_index": 20}, {"url": "http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/14_state_space.pdf", "anchor_text": "these Wharton lecture notes", "paragraph_index": 26}, {"url": "https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.representation.Representation.html#statsmodels.tsa.statespace.representation.Representation", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARMA.html", "anchor_text": "documentation", "paragraph_index": 41}, {"url": "https://nousot.com/blog/how-we-won-gold/", "anchor_text": "more recent contest", "paragraph_index": 45}], "all_paragraphs": ["I realized the power of the Kalman Filter immediately after Kaggle\u2019s Web Traffic Time Series Forecasting competition, a contest requiring prediction of future web traffic volumes for thousands of Wikipedia pages. In this contest, simple heuristics like \u201cmedian-of-medians\u201d were difficult to beat, my energy spent scaling ARIMA and Prophet models went nowhere. I wondered if anything in the traditional forecasting toolbox was up for this task. Then I read a post by a user known only as \u201cos,\u201d 8th place with Kalman filters.", "Since this article was first written, the identity of os has now been revealed as Otto Seiskari, who left an illuminating response to this article. My perspective has changed also since initially writing this article, which I\u2019ll explain at the end.", "While Otto did use median-of-medians as a fallback solution, the core approach was an 8-state Kalman Filter that encoded local level and weekly seasonality. Otto also wrote a custom python package just for this contest, simdkalman. Directly from the simdkalman docs, with minor variation in boldface font, is the model specification:", "where q_t ~ N(0, Q) and r_t ~ N(0, R). In this article, measurement y_t is a scalar; the variance of r_t is denoted by the 1x1 matrix R. Here, x_t is an unobserved \u201cstate\u201d vector and y_t is an observed measurement for time period t. Together these systems form a useful representation for many time series data sets. A benefit of adopting of this framework is availability of the Kalman Filter machinery for computing likelihoods and creating 1-step-ahead predictions, among other things (search \u201cKalman\u201d in R\u2019s stats.arima documentation to see an example of its use there).", "Practically, while A and H can come from standard \u201crecipes\u201d for weekly seasonality, local trend, etc., Q and R are more likely to be unknown. Rather than estimating them, Otto used default choices for Q and R and then tuned using scalar quantity called a \u201csmoothing parameter\u201d to scale one matrix vs the other.", "Though I couldn\u2019t deny the effectiveness of the method, \u201cconstructing\u201d Q and R didn\u2019t sit well with me as a statistician. Then I dug into Roger Labbe\u2019s Jupyter-based text, Kalman and Bayesian Filters in Python, and found that it also suggests a similar procedure in the Kalman Filter Math section: \u201cIn practice,\u201d the text says, \u201cwe pick a number, run simulations on data, and choose a value that works well.\u201d", "I hear another voice from a classroom 15 years ago. A professor named David Dickey asks, rhetorically and heavily paraphrased,", "\u201cDo you like the number seven? How about three?\u201d", "\u201cLet\u2019s agree not to compute anything from the data without knowing what it\u2019s estimating.\u201d", "Maximum Likelihood Estimation, for any faults it might have, is a principled method of estimating unknown quantities, and the likelihood is a \u201cbyproduct\u201d of the Kalman Filter operations. In in the next section, we\u2019ll explore the intermediate these computations in Python\u2019s statsmodels with an ARMA(2, 1) in statespace form. Finally we\u2019ll use this likelihood to get estimates of A, H, and Q (R will be known) using maximum likelihood.", "The following product expansion of the joint pdf is especially useful in time series situations:", "In our situation, there are model matrices A, H, Q, and R, which all have potentially unknown components. Let \u0398 contain all unknown parameters necessary to construct these matrices. Then the logarithm of the likelihood becomes:", "Hence, there are two analytical tasks:", "Recall that the \u201cfiltering\u201d operations of the Kalman Filter give us the conditional distribution of the unknown state vector x_t given all observations y_t up to the present time point t (script Y_t):", "In Yet Another Kalman Filter Explanation Article, this distribution was presented as a Bayesian posterior on the unknown state at time t, but here it is presented as the sampling distribution of the latent random vector x_t.", "Unraveling the measurement model one time leads to:", "from which it follows that y_t | y_{t-1}, \u2026, y_0 must be normal since x_{t-1} | y_{t-1}, \u2026, y_0 is. It also gives us a starting point for computing the mean and the variance of this conditional normal. The conditional mean is just:", "The conditional variance is found using standard variance formulas for random vectors:", "Recall that Q, R, A and H are all fully specified by the parameters in \u0398. These parameters need starting values in a numerical maximization routine, and hence the computations above need to be made at each stage of the likelihood maximization.", "The distribution of the initial measurement requires initialization of the state mean vector and variance matrix. In the case of a stationary model, there is a correct initial (a priori) mean and variance of the state vector that are based on long run behavior of the system (see page 2 of these notes from University of Washington for more details).", "For non-stationary models, initialization is more challenging. The solution provided by statsmodels is an \u201capproximate diffuse\u201d initialization, i.e., a zero mean and a very large covariance matrix. Statisticians don\u2019t love this (\u201cThis poses numerical problems and does not answer the question of existence of diffuse constructs\u201d \u2014 Piet de Jong, 1991). While we will not handle it here, the R package KFAS does provide the \u201cexact diffuse initialization method.\u201d For simplicity and clarity, we will use a \u201cknown\u201d initialization in the example to come.", "For this article will work with one specific ARMA(1, 2) model:", "Using statsmodels to simulate data from from an ARMA(1, 2) is simple, but take note of the way that AR and MA parameter coefficients are entered:", "The statsmodels ARMA class has a fit() method for fitting the ARMA model:", "This results in the output (slightly modified):", "Note the log-likelihood of the sample as well as the coefficient estimates (in bold).", "To get this ARMA(1, 2) model in a state space framework, we have many choices. The benefit of the \u201cHarvey\u201d representation (as presented on page 8 of these Wharton lecture notes) is that it directly incorporates the AR and MA coefficients. For our model, this representation is:", "where r_t has been explicitly replaced by 0 to show that all error variability is handled in the state term, and", "Despite R being zero, this ARMA(1, 2) model fits into the Kalman Filter framework. Here is an exact coding of this model instatsmodels, albeit one that is more verbose than is necessary:", "The documentation for the statsmodels model representation is here. Below are a few notes about the implementation above.", "We\u2019ll start by creating an instance of the Kalman Filter model and initializing it with the starting values. Since the parameter values contain all necessary information for reconstructing A, H, Q, and R, the Kalman Filter machinery of statsmodels can start filtering right away, before any further optimization.", "Running interactively in iPython, it is easy to see that filtered_state contains a 3-dimensional state vector, and filtered_state_cov contains a 3x3 covariance matrix, for all 1000 time points.", "The last line shows the log-likelihood contributions of the first three measurements according to statsmodels. These are what we want to match.", "We will now use the formulas established in Section Likelihood for sequentially arriving data. Matrices A, H, Q, and R are easily accessible, and the matrix product HA is calculated and stored in HA below:", "To match the first number in the likelihood vector, -1.9201, first note that \u03bc and \u03a3 are initialized at the zero vector and the identity matrix. The measurement vector takes only the first element of the state vector without an extra error term (R is zero), so the first observation y[0] is compared to a N(0, 1) distribution.", "Since python starts with zero-indexed time, \u03bc_0 and \u03a3_0 are actually the first updated state mean and variance matrix given the initial data measurement. Following the computations laid out in the likelihood derivation,", "results in array([[-1.34946888]]), matching the second measurement\u2019s log-likelihood. One more round should be convincing that we have the pattern down.", "Again, it\u2019s a match. If we sum the loglikelihood over the 1000 measurements, we get something a little smaller (i.e., more negative) than what we saw with the ARMA fit, which should be expected given our likelihood evaluations are not at the maximum likelihood estimates.", "The Kalman Filter object created above has been waiting for Maximum Likelihood Estimation all this time (why else would it inherit from a class called \u201cMLEModel\u201d). All it takes is running the object\u2019s fit() method:", "The summary provides (slightly modified output):", "Note that the parameter estimates and log likelihood are close to the ARMA estimates, but not exact. This is due to the known initialization. As an exercise, run the following two variations, which will be discussed in the Discussion section:", "From the exercise above, you should have observed that you can exactly match the ARMA model\u2019s log-likelihood and parameter estimates with the stationary initialization. This should not be surprising, since the documentation for the ARMA class\u2019s fit()method says that it \u201cfits ARMA(p,q) model using exact maximum likelihood via Kalman filter.\u201d", "In this case, the approximate diffuse initialization leads to similar estimates as its exact and known counterparts, but it is a stationary model. The initial log-likelihood values are quite a bit smaller (< -7) so the total log-likelihood of the sample is a bit more negative. A \u201cburn in\u201d option exists to skip these first few observations in computing the likelihood by setting self.loglikelihood_burn in the class\u2019s __init__ function.", "Once the machinery is in place to get the filtered state mean and variance, the likelihood of each measurement consists of only a few linear operations. This does mean a double loop will be necessary when maximizing the likelihood numerically, which explains why fitting even a simple ARMA(1, 2) model slows down when the number of observations gets moderately large.", "My original conclusion to this article concerned the practical methods of constructing a filter that do not involve formal estimation like presented here. I said that I might use one of these methods, if I could ever stop hearing the question: \u201cDo you like the number seven?\u201d", "Since then, the original os has responded, explaining why a more engineering-based approach might outperform maximum likelihood. A stubborn person, I\u2019ve had to experience difficulties with Maximum Likelihood estimation in my own work (like this more recent contest) to fully appreciate the arguments. As Otto Seiskari explains:", "\u201cThe quality of the solution found by the MLE approach depends on how accurately the model describes the data. Kalman filters are often applier to problems where, in theory, the model assumptions are quite badly off: noise could be non-Gaussian, non-independent and the chosen hidden state model could be very simplistic compared to what it\u2019s supposed to model. However, with certain parameter choices, the filter can still work very well, but the parameters that are optimal in practice can be quite different from the ML estimates.\u201d", "I still like the idea of using Maximum Likelihood Estimation to estimate the unknown parameters of a Kalman Filter model, but I\u2019ve also opened my mind to the use of holdout-based tuning, especially when the primary objective is forecasting. In either case, I hope readers have appreciated learning more about the underlying probabilistic foundations of the Kalman Filter, and what is possible without validation sets using Maximum Likelihood Estimation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist who enjoys learning and writing about methods. @benogorek on Twitter"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9861666f6742&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9861666f6742--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9861666f6742--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@baogorek?source=post_page-----9861666f6742--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baogorek?source=post_page-----9861666f6742--------------------------------", "anchor_text": "Ben Ogorek"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7ecfd165396&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&user=Ben+Ogorek&userId=a7ecfd165396&source=post_page-a7ecfd165396----9861666f6742---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9861666f6742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9861666f6742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.researchgate.net/publication/264392229_Ignoring_Imperfect_Detection_in_Biological_Surveys_Is_Dangerous_A_Response_to_'Fitting_and_Interpreting_Occupancy_Models'", "anchor_text": "Guillera-Arroita et al (2014)"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.statsmodels.org/stable/index.html", "anchor_text": "statsmodels"}, {"url": "https://www.kaggle.com/c/web-traffic-time-series-forecasting", "anchor_text": "Web Traffic Time Series Forecasting"}, {"url": "https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43727#latest-492742", "anchor_text": "8th place with Kalman filters"}, {"url": "https://medium.com/@oseiskar", "anchor_text": "Otto Seiskari"}, {"url": "https://github.com/oseiskar/simdkalman", "anchor_text": "simdkalman"}, {"url": "https://simdkalman.readthedocs.io/en/latest/", "anchor_text": "simdkalman docs"}, {"url": "https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html", "anchor_text": "stats.arima"}, {"url": "https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python", "anchor_text": "Kalman and Bayesian Filters in Python"}, {"url": "https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/9e4ac42f796a65e5d2baffd57a0fb88391a3d956/07-Kalman-Filter-Math.ipynb", "anchor_text": "Kalman Filter Math"}, {"url": "https://towardsdatascience.com/yet-another-kalman-filter-explanation-article-be0264d99937", "anchor_text": "Yet Another Kalman Filter Explanation Article"}, {"url": "https://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf", "anchor_text": "these notes"}, {"url": "https://projecteuclid.org/euclid.aos/1176348139", "anchor_text": "Piet de Jong, 1991"}, {"url": "https://cran.r-project.org/web/packages/KFAS/vignettes/KFAS.pdf", "anchor_text": "R package KFAS"}, {"url": "http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/14_state_space.pdf", "anchor_text": "these Wharton lecture notes"}, {"url": "https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.representation.Representation.html#statsmodels.tsa.statespace.representation.Representation", "anchor_text": "here"}, {"url": "https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.representation.Representation.html", "anchor_text": "statespace representation"}, {"url": "https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARMA.html", "anchor_text": "documentation"}, {"url": "https://nousot.com/blog/how-we-won-gold/", "anchor_text": "more recent contest"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----9861666f6742---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9861666f6742---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----9861666f6742---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/time-series-analysis?source=post_page-----9861666f6742---------------time_series_analysis-----------------", "anchor_text": "Time Series Analysis"}, {"url": "https://medium.com/tag/statistics?source=post_page-----9861666f6742---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9861666f6742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&user=Ben+Ogorek&userId=a7ecfd165396&source=-----9861666f6742---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9861666f6742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&user=Ben+Ogorek&userId=a7ecfd165396&source=-----9861666f6742---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9861666f6742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9861666f6742--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9861666f6742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9861666f6742---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9861666f6742--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9861666f6742--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9861666f6742--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9861666f6742--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9861666f6742--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9861666f6742--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9861666f6742--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9861666f6742--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baogorek?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baogorek?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ben Ogorek"}, {"url": "https://medium.com/@baogorek/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "177 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7ecfd165396&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&user=Ben+Ogorek&userId=a7ecfd165396&source=post_page-a7ecfd165396--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F536bc54ba88c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-kalman-filter-and-maximum-likelihood-9861666f6742&newsletterV3=a7ecfd165396&newsletterV3Id=536bc54ba88c&user=Ben+Ogorek&userId=a7ecfd165396&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}