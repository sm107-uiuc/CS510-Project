{"url": "https://towardsdatascience.com/meta-policy-gradients-a-survey-78dc742d395d", "time": 1683018416.979546, "path": "towardsdatascience.com/meta-policy-gradients-a-survey-78dc742d395d/", "webpage": {"metadata": {"title": "Meta-Policy Gradients: A Survey. Automated Hyperparameter Tuning & RL\u2026 | by Robert Lange | Towards Data Science", "h1": "Meta-Policy Gradients: A Survey", "description": "Most learning curves plateau. After an initial absorption of statistical regularities, the system saturates and we reach the limits of hand-crafted learning rules and inductive biases. In the worst\u2026"}, "outgoing_paragraph_urls": [{"url": "http://people.idsia.ch/~juergen/", "anchor_text": "J\u00fcrgen Schmidhuber", "paragraph_index": 0}, {"url": "http://people.idsia.ch/~juergen/diploma1987ocr.pdf", "anchor_text": "Schmidhuber (1987, p.45)", "paragraph_index": 1}, {"url": "https://link.springer.com/content/pdf/10.1007/BF00992696.pdf", "anchor_text": "Williams, 1992", "paragraph_index": 4}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al. (2018)", "paragraph_index": 4}, {"url": "http://www.incompleteideas.net/papers/sutton-92a.pdf", "anchor_text": "Sutton, 1992", "paragraph_index": 7}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al., 2018", "paragraph_index": 14}, {"url": "https://roberttlange.github.io/posts/2020/12/meta-policy-gradients/#zahavy_2020", "anchor_text": "(Zahavy et al., 2020)", "paragraph_index": 14}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al. (2018)", "paragraph_index": 15}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al. (2018)", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "Zahavy et al. (2020)", "paragraph_index": 21}, {"url": "https://arxiv.org/pdf/1802.01561", "anchor_text": "Espeholt et al., 2018", "paragraph_index": 21}, {"url": "http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.pdf", "anchor_text": "Munos et al., 2016", "paragraph_index": 22}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "(Zahavy et al., 2020)", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "Zahavy et al. (2020", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "Zahavy et al. (2020)", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Richard_E._Bellman", "anchor_text": "Richard Bellman", "paragraph_index": 28}, {"url": "https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf", "anchor_text": "Houthooft et al. (2018)", "paragraph_index": 29}, {"url": "https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf", "anchor_text": "Houthooft et al. (2018)", "paragraph_index": 30}, {"url": "https://amolchanov86.github.io/pdf/bechtle19icml.pdf", "anchor_text": "Chebotar et al. (2019)", "paragraph_index": 32}, {"url": "https://arxiv.org/pdf/1910.04098", "anchor_text": "Kirsch et al. (2020)", "paragraph_index": 33}, {"url": "https://arxiv.org/pdf/1509.02971.pdf?source=post_page---------------------------", "anchor_text": "Lillicrap et al., 2015", "paragraph_index": 33}, {"url": "https://arxiv.org/pdf/1802.09477.pdf", "anchor_text": "Fujimoto et al., 2018", "paragraph_index": 33}, {"url": "https://arxiv.org/pdf/1910.04098", "anchor_text": "Kirsch et al. (2020, p.3)", "paragraph_index": 34}, {"url": "https://arxiv.org/pdf/2007.08794", "anchor_text": "(LPG, Oh et al., 2020)", "paragraph_index": 37}, {"url": "https://arxiv.org/pdf/1707.06887.pdf?source=post_page---------------------------", "anchor_text": "Bellemare et al., 2017", "paragraph_index": 39}, {"url": "https://arxiv.org/pdf/2007.08433.pdf", "anchor_text": "Xu et al. (2020)", "paragraph_index": 42}, {"url": "https://arxiv.org/pdf/2007.08433.pdf", "anchor_text": "Xu et al., 2020", "paragraph_index": 43}, {"url": "https://medium.com/@RobertTLange/membership", "anchor_text": "https://medium.com/@RobertTLange/membership", "paragraph_index": 47}], "all_paragraphs": ["Most learning curves plateau. After an initial absorption of statistical regularities, the system saturates and we reach the limits of hand-crafted learning rules and inductive biases. In the worst case, we start to overfit. But what if the learning system could critique its own learning behaviour? In a fully self-referential fashion. Learning to learn\u2026 how to learn how to learn. Introspection and the recursive bootstrapping of previous learning experiences \u2014 is this the key to intelligence? If this sounds familiar to you, you might have had the pleasure of listening to J\u00fcrgen Schmidhuber.", "\u2018Every really self-referential evolving system should accelerate its evolution.\u2019 \u2014 Schmidhuber (1987, p.45)", "Back in the 90s these ideas were visionary \u2014 but highly impractical. But with the advent of scalable automatic differentiation toolboxes, we are moving closer towards meta-meta-\u2026 learning systems. In this post we review a set of novel Reinforcement Learning (RL) algorithms, which allow us to automate much of the \u2018manual\u2019 RL design work. They come by the name of meta-policy gradients (MPG) and can tune almost all differentiable ingredients of the RL pipeline via higher-order gradients.", "Meta-learning or \u201alearning to learn\u2019 aims to automatically discover learning algorithms by decomposing the overall learning process into two stages: An inner-loop unfolding of an agents\u2019 \u2018lifetime\u2019 and an outer loop \u201aintrospection\u2019 of the experience. This general two-phase paradigm can come in many different flavours, here are only a few:", "But most of these traditional meta-learning algorithms have a hard time separating the meta-learned algorithm from the agent itself. The meta-learned algorithm is hard to interpret and limited in its meta-test generalization capabilities. In RL\u00b2 for example, the recurrent weight dynamics encode the learning algorithm by modulating effective policy. In MAML, the meta-learned initialization is inherently entangled with the network architecture of the agent. Meta-policy gradient methods, on the other hand, aim to overcome these limitations by optimizing the meta-level to provide the lower level with an objective, which maximizes the subsequent learning progress. This can range from solely optimizing single parameters to learning a black-box neural net-parametrized inner-loop objective. But what is the outer-loop objective? Most commonly it is chosen to be the Reinforcement Learning problem itself and we use pseudo-gradients resulting from a REINFORCE estimator (Williams, 1992). Hence, the name \u2014 meta-policy gradients. Next, we introduce the required mathematical background following Xu et al. (2018).", "Let\u2019s assume we want to train an agent parameterized by \u03b8 (e.g. a policy/value network). The standard learning loop (as shown in figure 1) would repeatedly perform 3 steps:", "Now let\u2019s assume that our update function explicitly depends on a set of meta-parameters \u03b7 (e.g. the discount factor \u03b3):", "Importantly, the update function is assumed to be differentiable with respect to \u03b7. We can then ask the following questions: How does the performance after the update depend on the meta-parameters used to perform the update? How should we choose \u03b7 for the next update given the current state of our agent? In order to answer this question, we need a meta-objective to evaluate a candidate \u03b7? One way to do so is to perform online cross-validation by (Sutton, 1992) using another hold-out trajectory \ud835\udf0f\u2019, collected from the post-update policy. For simplicity, we can assume that the meta-objective \u00afL, with which we evaluate \u03b7 on \ud835\udf0f\u2019, is the same as the inner loop objective L (e.g. some REINFORCE variant). Hence, we want to estimate the meta-gradient:", "Using the chain rule, we can re-write the derivative of the updated parameters \u03b8\u2019 with respect to \u03b7:", "The dependence of the updated parameters on \u03b7 boils down to a dynamical system:", "The parameters of this additive sequence and the respective Jacobians can be accumulated online as we unroll the network. This in turn provides us with all necessary ingredients to perform an outer loop gradient descent update to improve the hyperparameters \u03b7:", "Note that we have only considered the effect of the meta-parameters on the performance after a single update. Considering multiple steps is conceptually straight forward, but the math requires us to propagate gradients \u00e0 la backpropagation through time. We can use a fixed set of K steps and automatic differentiation toolboxes to do the gradient bookkeeping. The full meta-policy gradient procedure then boils down to repeating 3 essential steps (see figure 2):", "There are multiple resulting computational considerations: First, online cross-validation does not imply a decrease in sample efficiency. Instead of discarding \ud835\udf0f\u2019 after meta-evaluation, we can simply reuse it for step 1 of the next procedure iteration. Second, the derivative of the update f with respect to the meta-parameters is expensive to compute. This is due to us having to to perform \u2018unrolled\u2019 optimization through a (multi-step) inner-loop optimization procedure. There is a set of heuristic solutions which result in a semi-/pseudo-meta policy gradient update:", "And finally, the memory complexity of MPG increases with the \u2018meta-trajectory length\u2019 K. Choosing a small K introduces a truncation bias, while larger K may suffer from high variance. Furthermore, the trace of z required to compute the meta-policy gradient may be tough to fit into memory (depending on the dimensionality of \u03b8 and K). Now that we have introduced the formal MPG framework, let\u2019s see how we can exploit it.", "The RL problem postulates an artificial agent, who maximizes its expected discounted cumulative reward. This reward is sampled from a stochastic and unkwown reward function. Hence, there is no simple and explicit differentiable objective as in the supervised learning case. Instead, we have to come up with a proxy objective such as the Mean-Squared Bellman Error or a Policy Gradient objective. These objectives come with their own hyperparameters, which often are non-trivial to tune and can require dynamic scheduling. MPG methods promise to discover adaptive objectives capable of providing the best possible proxy \u2014 at any given time of the learning procedure. Throughout the next section we will focus on how the MPG setup can improve the control problem using an actor-critic (AC) objective. We cover both the standard AC setup (Xu et al., 2018) as well as the IMPALA architecture (Zahavy et al., 2020).", "Xu et al. (2018) applied the MPG framework to the return function G_\u03b7(\ud835\udf0f_t) used throughout value-based RL and policy gradient baseline corrections. The meta-parameters, \u03b7 = {\u03b3, \u03bb}, control the bias-variance trade-off in RL. How much do we rely on Monte Carlo estimation and how much on a bootstrap estimate of future returns? G defines the critic target and baseline corrects the REINFORCE term:", "where g rescales the effective learning rate of the critic and entropy loss terms. The inner loop update f for a fixed set of hyperparameters \u03b7 then boils down to:", "The next ingredient for our meta-policy gradient is the derivative of the update operation with respect to the meta-parameters \u03b7:", "Finally, the derivative of the online-validation loss wrt. the updated parameters is given by:", "We can now stitch all of these together, store our trace and update the meta-parameters \u03b3, \u03bb. Great! But there remains a fundamental challenge: Our inner-loop function approximators are shooting after a moving target. How can we learn values if the nature of the targets changes with each meta-parameter update? Here are a few solution ideas:", "After the initial paper by Xu et al. (2018), this approach was extended to many more hyperparameters. Here is a list of references, which applies MPGs to a vast set of fundamental RL design choices:", "While these projects mainly focus on tuning small sets of hyperparameters, ultimately we would like to automatically tune the entire RL pipeline. Zahavy et al. (2020) make a big step into this direction by optimizing almost all hyperparameters of the IMPALA (Espeholt et al., 2018) architecture. But that\u2019s not all: They also adapt the network architecture and objective to maximally utilize the power of MPGs. But let\u2019s first take a step back: IMPALA is a large-scale off-policy actor-critic algorithm, which allows for high data throughput using the distributed actor-learner framework. Many workers asynchronously collect trajectories, which are then sent to a centralized learner. This learner processes the experience and performs gradient updates. There is one caveat: The trajectories are most likely not collected using the most recent policy. Therefore, the algorithm is not entirely on-policy and gradients (estimated from outdated policy rollouts) will become \u2018stale\u2019. IMPALA corrects for this covariate shift by using a procedure called v-trace. V-trace combats the off-policy nature of transitions with a modified importance sampling (IS) ratio, which explicitly controls the variance of the gradient estimators and contraction speed:", "where \u03c1 controls the nature of the target value function by acting as a discount scaling. c, on the other hand, performs a form of trace cutting (\u00e0 la Retrace, Munos et al., 2016). The clipping threshold constrains the variance of the product of importance weights:", "So where do MPGs come into play? The STAC algorithm (Zahavy et al., 2020) then aims to learn the degree of correction by utilizing a convex-combination version of the V-trace parameters:", "The meta-objective can then be differentiated with respect to \u03b1 and can smoothly interpolate between the fixed point of our approximate Bellman iteration for the target policy \u03c0 and the behaviour policy \u00b5. A low \u03b1_c puts more weight on IS, which in turn implies more contraction but also high variance. A large \u03b1_c, on the other hand, emphasizes v-trace, which leads to less contraction but also lower variance. To stabilize the impact of the outer loop non-stationarity, Zahavy et al. (2020) modify the meta-objective and add a KL regularizer similar to TRPO:", "In order to assure the proper scaling between the inner and outer loss magnitude the authors propose to sigmoid-squash and scale the loss coefficients.", "The second innovation of Zahavy et al. (2020) is to add a set of auxiliary output heads on top of the shared torso corresponding different policy and critic parameterization with their own meta-parameters. The meta-controller can then control the gradient flow into the torso for potentially different timescales. Only the first of the heads is used to generate the training trajectories, while the others act as implicit regularizers. The paper shows that on ATARI the MPG improvements increase, the more parameters we optimize with the MPG setup (panel B in figure 3).", "Empirically a high discount and value loss coefficient in the outer loop appear to perform better (panel A in figure 3). Finally, when examining the online optimized meta-parameter schedules on the James Bond ATARI game, we can see significant variation across the three heads and strong non-linear dynamics over the course of training (panel C in figure 3).", "The RL problem is inherently misspecified. The agent is tasked with maximizing an expected sum of discounted rewards, without having access to the underlying reward function. In order to overcome this fundamental problem we define surrogate objectives. But it is safe to assume that for us humans the objective function changes over the course of our lives. Potentially making learning easier. Might it potentially even be possible to completely discard the ingenious legacy of Richard Bellman and to learn what to predict in order to best solve the RL problem? The ideas introduced in the following sections extend MPGs to not only optimize specific hyperparameters but to meta-optimize an entire parametrized surrogate objective function, which serves the purpose of providing the best learning curriculum to the RL agent. The first set of methods are offline and meta-learned across a large number of training inner-loops and different MDPs. Meta-learning a parametrized loss function L_\u03c6, has the advantage of potentially being more robust when varying the task distribution. The implicit characterization of the optimization problem allows for generalization beyond the meta-training distribution and tackles the core motivating problem: To disentangle learned learning algorithm from the learning agent.", "Computing higher-order gradients is expensive and truncating the unrolled optimization has the unwanted effect of introducing bias in the gradient estimates. In order to overcome this technical difficulty, Houthooft et al. (2018) propose to use evolutionary strategies for gradient estimation using a population of meta-loss parametrizations. The gradient of a (potentially non-differentiable) function L is approximated using a population-based Monte Carlo estimator, e.g. via antithetic sampling:", "where \u03f5 is multivariate Gaussian variable. P denotes the evaluated population size, \u03c3 controls the perturbation variance and \u03b2 effectively rescales the learning rate. The individual function/network/agent evaluations can be collected in parallel. Houthooft et al. (2018) introduce a parametrization of the meta-loss, which uses temporal convolutions of the agents\u2019 previous experiences. It takes into account the most recent history (see figure 4). The extended memory can help incentivize structured exploration in problems with long time horizons.", "Furthermore, the additional input may allow the loss to perform environment/task identification and to tail its target to a specific application. Furthermore, the meta-loss is fed a vector of ones, which can act as an explicit memory unit. During the inner loop optimization the agents uses the meta-loss to optimize their policy network parameters using standard SGD. The total loss of the inner loop is given by a convex combination of the parametrized loss and a standard PPO loss. A set of MuJoCo experiments reveal that EPG outperforms a plain PPO objective and that the resulting gradients are related but different (correlation of around 0.5). Interestingly, the authors find that the EPG loss has learned an adaptive gradient rescaling, which is reminiscent of policy update smoothness regularization ideas explicitly enforced in TRPO and PPO (see figure 4). One has to note that this comes at the substantial costs of an evolutionary outer-loop of meta-loss optimization. Finally, the authors show that one can combine the method with learned policy initializations such as MAML and that the loss does generalize to longer training horizons, different architectures as well as new tasks.", "The general idea of an offline learned loss was afterwards extended by Chebotar et al. (2019), who use gradient-based meta-optimization and explicitly condition the meta-loss on additional task and context information. Interestingly, the authors show that the pre-conditioning can help shape the loss landscape for enhanced smoothness in the inner loop optimization.", "While the previous work mainly considered on-policy RL agents, Kirsch et al. (2020) extend meta-policy gradients to the off-policy setting, which allows to leverage replay buffers. As in \u2018shallow\u2019 non-meta RL this can lead to sample efficiency, since transitions are re-used multiple times to construct gradients. Conceptually, the proposed MetaGenRL framework builds on the off-policy continuous control algorithm DDPG (Lillicrap et al., 2015) and its extension TD3(Fujimoto et al., 2018):", "\u2018Our key insight is that a differentiable critic Q_\u03b8: S x A \u2192 R can be used to measure the effect of locally changing the objective function parameters \u03b1 based on the quality of the corresponding policy gradients.\u2019 \u2014 Kirsch et al. (2020, p.3)", "So what does this mean on an algorithmic level? Standard DDPG alternates between minimizing a Mean-Squared Bellman Error of the critic Q_\u03b8 and the improvement of the policy using the deterministic gradient of the value function wrt. the policy parameters. In MetaGenRL an additional intermediate layer of transformation is added:", "The critic is now tasked to refine the objective function L_\u03b1, which then in turn is used to construct policy gradients. In practice, L is parametrized by a small LSTM, which processes the trajectory data in reverse order. This allows the learned objective to evaluate a specific transition based on future experiences within an episode and to emulate a form of pseudo-bootstrapping. The authors benchmark on MuJoCo and against traditional meta-learning such as RL\u00b2 and EPG. They find that MetaGenRL is not only more sample efficient and also allows for generalization to completely novel environments. Futhermore, they perform a set of ablation studies that show that the meta-objective performs well even without the timestamp input, but requires a value estimate input to induce learning progress.", "Learned Policy Gradients (LPG, Oh et al., 2020) extend upon MetaGenRL in several aspects. Instead of relying on the notion of a critic, they overcome hand-crafted semantics by learning their own characterization of what might be important to predict on a set of representative Markov Decision Processes. They do not enforce any meaning on the agent\u2019s vector-valued predictions. Instead the meta-learner decides what has to be predicted and thereby discovers its own update rules.", "The LPG framework proposes to use a recurrent neural network to emit inner loop optimization targets (see figure 6). \u03c0-hat denotes a policy-adjustment target, while y-hat represents the output of a categorical distribution related to a specific state. Again, the RNN processes an episode in reverse order and is trained by performing gradient descent on a sequence of inner-loop gradient descent updates resulting from the previous objective parameterization. But unlike MetaGenRL it does not rely upon the notion of a value function for bootstrapping. y-hat is free to learn its own semantics. The LPG targets can then be used to construct an inner loop update based on a combination of cross-entropy and KL divergence terms:", "The KL loss term in the inner loop is reminiscent of ideas in Distributional RL such as C51 (Bellemare et al., 2017) and allows for a more detailed learning signal/belief estimate than the expected value. The outer loop loss is similar to the previously introduced objectives but also includes an additional set of regularizers, which aim to ensure stability throughout the optimization procedure wrt. to \u03b7. In figure 7 panel A we can see how the different dimensions can encode different value-related estimates similar to classic value function bootstrapping.", "Something that has left quite the impression on me, is the result depicted in figure 7 panel C: The authors show that it is possible to learn a target-providing RNN on a set of toy MDPs (gridworlds and delayed chain MDPs), which is capable of generalizing to ATARI almost as well as the substantially hand-tuned DQN objective with reward clipping and all the extra sauce. Since the underlying meta-training distribution of MDPs encode a set of fundamental RL problems (e.g. long-term credit assignment and the need for propagating state knowledge), the LPG is capable of substantial out-of-distribution generalization. Furthermore, adding more types of environments to the meta-train set improves the test performance of LPGs.", "The papers reviewed in the previous section are concerned with discovering objectives or parameter schedules offline. But can we also let MPGs discover potentially even better pillars to solve the RL problem online and during single optimization run?", "Xu et al. (2020) propose the FRODO algorithm, which has nothing to do with jewelry or Goblins, but stands for Flexible Reinforcement Objective Discovered Online. What makes FRODO special, is that the interplay between naive outer loss and involved inner loss is trained online within a single task and single agent\u2019s lifetime. It does not require any task distribution or the capability to reset the environment. The target simply adapts as the agent experiences the world. Somewhat like real lifelong learning! As in the offline case, \u03b7 is no longer low dimensional, i.e. a single hyperparameter, but a neural net parametrizing the entire target return G(\ud835\udf0f) = g_\u03b7(\ud835\udf0f).", "The inner loop target values do not have to encode the crucial Bellman consistency, i.e. that the current state value may be decomposed into reward and discounted next state value. Instead, this may (or may not) be discovered by the outer loop. You might imagine that training an entire network online might be even trickier in terms of non-stationary learning dynamics. And you are right. In order to overcome this (Xu et al., 2020) propose to add a prediction consistency regularizer on the meta-level:", "The authors validate FRODO on the ATARI benchmark and panel A of figure 8 clearly indicates that the learning progress induced by FRODO improves throughout the learning process (exactly as in the cartoon sketch in the beginning of this post). Furthermore, an ablations study provides evidence for the importance of the consistency loss (panel B of figure 8). Finally, panel C shows how the learned targets vary across different ATARI games as well as the course of training. For some games FRODO learns to provide target returns close to v-trace (e.g. for simple games like Pong and Seaquest), while for other more challenging games learned return targets can differ quite significantly.", "In this post we reviewed a set algorithms, which tune large parts of the RL pipeline using higher-order gradients. This enabled not only highly non-trivial training schedules but also to let the system decide what is important to learn at different stages of the process. How far can this hierarchical optimization paradigm be pushed? Here is a set of my personal open questions:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep RL PhD Student@TU Berlin. Intelligence. \u270d\ufe0f Support my writing: https://medium.com/@RobertTLange/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F78dc742d395d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----78dc742d395d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78dc742d395d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@RobertTLange?source=post_page-----78dc742d395d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@RobertTLange?source=post_page-----78dc742d395d--------------------------------", "anchor_text": "Robert Lange"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F638b9cae9933&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&user=Robert+Lange&userId=638b9cae9933&source=post_page-638b9cae9933----78dc742d395d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78dc742d395d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78dc742d395d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/making-sense-of-big-data", "anchor_text": "Making Sense of Big Data"}, {"url": "http://people.idsia.ch/~juergen/", "anchor_text": "J\u00fcrgen Schmidhuber"}, {"url": "http://people.idsia.ch/~juergen/diploma1987ocr.pdf", "anchor_text": "Schmidhuber (1987, p.45)"}, {"url": "http://snowedin.net/tmp/Hochreiter2001.pdf", "anchor_text": "Hochreiter et al., 2001"}, {"url": "https://arxiv.org/pdf/1611.02779", "anchor_text": "Duan et al., 2016"}, {"url": "https://arxiv.org/pdf/1611.05763.pdf", "anchor_text": "Wang et al., 2016"}, {"url": "http://proceedings.mlr.press/v48/santoro16.pdf", "anchor_text": "Santoro et al., 2016"}, {"url": "https://arxiv.org/pdf/1805.09692", "anchor_text": "Ritter et al., 2018"}, {"url": "https://ora.ox.ac.uk/objects/uuid:dd8473bd-2d70-424d-881b-86d9c9c66b51/download_file?file_format=pdf&safe_filename=outline.pdf&type_of_work=Journal+article", "anchor_text": "Graves et al., 2016"}, {"url": "http://papers.nips.cc/paper/6461-learning-to-learn-by-gradient-descent-by-gradient-descent.pdf", "anchor_text": "Andrychowicz et al., 2016"}, {"url": "http://proceedings.mlr.press/v97/metz19a/metz19a.pdf", "anchor_text": "Metz et al., 2019"}, {"url": "https://arxiv.org/pdf/2009.11243", "anchor_text": "Metz et al., 2020"}, {"url": "https://arxiv.org/pdf/1703.03400", "anchor_text": "Finn et al., 2017"}, {"url": "https://pdfs.semanticscholar.org/672c/586880655dc544474280a6e086c1fc901c85.pdf", "anchor_text": "Nichol et al., 2018"}, {"url": "https://arxiv.org/pdf/1909.00025", "anchor_text": "Flennerhag et al., 2019"}, {"url": "https://link.springer.com/content/pdf/10.1007/BF00992696.pdf", "anchor_text": "Williams, 1992"}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al. (2018)"}, {"url": "http://www.incompleteideas.net/papers/sutton-92a.pdf", "anchor_text": "Sutton, 1992"}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al., 2018"}, {"url": "https://roberttlange.github.io/posts/2020/12/meta-policy-gradients/#zahavy_2020", "anchor_text": "(Zahavy et al., 2020)"}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al. (2018)"}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al. (2018)"}, {"url": "http://proceedings.mlr.press/v37/schaul15.pdf", "anchor_text": "Schaul et al., 2015"}, {"url": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning.pdf", "anchor_text": "Xu et al. (2018)"}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "Zahavy et al. (2020)"}, {"url": "https://arxiv.org/pdf/1802.01561", "anchor_text": "Espeholt et al., 2018"}, {"url": "http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.pdf", "anchor_text": "Munos et al., 2016"}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "(Zahavy et al., 2020)"}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "Zahavy et al. (2020"}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "Zahavy et al. (2020)"}, {"url": "https://arxiv.org/pdf/2002.12928", "anchor_text": "Zahavy et al. (2020)"}, {"url": "https://en.wikipedia.org/wiki/Richard_E._Bellman", "anchor_text": "Richard Bellman"}, {"url": "https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf", "anchor_text": "Houthooft et al. (2018)"}, {"url": "https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf", "anchor_text": "Houthooft et al. (2018)"}, {"url": "https://papers.nips.cc/paper/7785-evolved-policy-gradients.pdf", "anchor_text": "Houthooft et al. (2018)"}, {"url": "https://amolchanov86.github.io/pdf/bechtle19icml.pdf", "anchor_text": "Chebotar et al. (2019)"}, {"url": "https://arxiv.org/pdf/1910.04098", "anchor_text": "Kirsch et al. (2020)"}, {"url": "https://arxiv.org/pdf/1509.02971.pdf?source=post_page---------------------------", "anchor_text": "Lillicrap et al., 2015"}, {"url": "https://arxiv.org/pdf/1802.09477.pdf", "anchor_text": "Fujimoto et al., 2018"}, {"url": "https://arxiv.org/pdf/1910.04098", "anchor_text": "Kirsch et al. (2020, p.3)"}, {"url": "https://arxiv.org/pdf/1910.04098", "anchor_text": "Kirsch et al. (2020)"}, {"url": "https://arxiv.org/pdf/2007.08794", "anchor_text": "(LPG, Oh et al., 2020)"}, {"url": "https://arxiv.org/pdf/2007.08794", "anchor_text": "Oh et al. (2020)"}, {"url": "https://arxiv.org/pdf/1707.06887.pdf?source=post_page---------------------------", "anchor_text": "Bellemare et al., 2017"}, {"url": "https://arxiv.org/pdf/2007.08794", "anchor_text": "Oh et al. (2020)"}, {"url": "https://arxiv.org/pdf/2007.08433.pdf", "anchor_text": "Xu et al. (2020)"}, {"url": "https://arxiv.org/pdf/2007.08433.pdf", "anchor_text": "Xu et al., 2020"}, {"url": "https://arxiv.org/pdf/2007.08433.pdf", "anchor_text": "Xu et al. (2020)"}, {"url": "https://en.wikipedia.org/wiki/AIXI", "anchor_text": "theoretical optimal AIXI model"}, {"url": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html", "anchor_text": "\u2018The Bitter Lesson\u2019"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----78dc742d395d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----78dc742d395d---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----78dc742d395d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----78dc742d395d---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/making-sense-of-big-data?source=post_page-----78dc742d395d---------------making_sense_of_big_data-----------------", "anchor_text": "Making Sense Of Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78dc742d395d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&user=Robert+Lange&userId=638b9cae9933&source=-----78dc742d395d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78dc742d395d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&user=Robert+Lange&userId=638b9cae9933&source=-----78dc742d395d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78dc742d395d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78dc742d395d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F78dc742d395d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----78dc742d395d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----78dc742d395d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----78dc742d395d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----78dc742d395d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----78dc742d395d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----78dc742d395d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----78dc742d395d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----78dc742d395d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----78dc742d395d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@RobertTLange?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@RobertTLange?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Robert Lange"}, {"url": "https://medium.com/@RobertTLange/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.2K Followers"}, {"url": "https://medium.com/@RobertTLange/membership", "anchor_text": "https://medium.com/@RobertTLange/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F638b9cae9933&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&user=Robert+Lange&userId=638b9cae9933&source=post_page-638b9cae9933--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa35dbfb4005a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmeta-policy-gradients-a-survey-78dc742d395d&newsletterV3=638b9cae9933&newsletterV3Id=a35dbfb4005a&user=Robert+Lange&userId=638b9cae9933&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}