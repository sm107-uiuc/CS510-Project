{"url": "https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8", "time": 1683004485.390145, "path": "towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8/", "webpage": {"metadata": {"title": "Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning | by Eligijus Bujokas | Towards Data Science", "h1": "Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning", "description": "When I was writing another article that showcased how to use word embeddings in a text classification objective I realized that I always used pre-trained word embeddings downloaded from an external\u2026"}, "outgoing_paragraph_urls": [{"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "https://nlp.stanford.edu/projects/glove/", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Language_model", "anchor_text": "language modeling", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Feature_learning", "anchor_text": "feature learning", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "natural language processing", "paragraph_index": 5}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "https://nlp.stanford.edu/projects/glove/", "paragraph_index": 34}], "all_paragraphs": ["When I was writing another article that showcased how to use word embeddings in a text classification objective I realized that I always used pre-trained word embeddings downloaded from an external source (for example https://nlp.stanford.edu/projects/glove/). I started thinking about how to create word embeddings from scratch and thus this is how this article was born. My main goal is for people to read this article with my code snippets and to get an in-depth understanding of the logic behind the creation of vector representations of words.", "The whole code can be found here:", "The short version of the creation of the word embeddings can be summarized in the following pipeline:", "Read the text -> Preprocess text -> Create (x, y) data points -> Create one hot encoded (X, Y) matrices -> train a neural network -> extract the weights from the input layer", "In this article, I will briefly explain every step of the way.", "From wiki: Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. The term word2vec literally translates to word to vector. For example,", "The most important feature of word embeddings is that similar words in a semantic sense have a smaller distance (either Euclidean, cosine or other) between them than words that have no semantic relationship. For example, words like \u201cmom\u201d and \u201cdad\u201d should be closer together than the words \u201cmom\u201d and \u201cketchup\u201d or \u201cdad\u201d and \u201cbutter\u201d.", "Word embeddings are created using a neural network with one input layer, one hidden layer and one output layer.", "To create word embeddings the first thing that is needed is text. Let us create a simple example stating some well-known facts about a fictional royal family containing 12 sentences:", "The computer does not understand that the words king, prince and man are closer together in a semantic sense than the words queen, princess, and daughter. All it sees are encoded characters to binary. So how do we make the computer understand the relationship between certain words? By creating X and Y matrices and using a neural network.", "When creating the training matrices for word embeddings one of the hyperparameters is the window size of the context (w). The minimum value for this is 1 because without context the algorithm cannot work. Lets us take the first sentence and lets us assume that w = 2.", "The bolded word the is called the focus word and 2 words to the left and 2 words to the right (because w = 2) are the so-called context words. So we can start building our data points:", "Now if we scan the whole sentence we would get:", "From 6 words we are able to create 18 data points. In practice, we do some preprocessing of the text and remove stop words like is, the, a, etc. By scanning our whole text document and appending the data we create the initial input which we can then transform into a matrix form.", "The full pipeline to create the (X, Y) word pairs given a list of strings texts:", "The first entries of the created data points:", "After the initial creation of the data points, we need to assign a unique integer (often called index) to each unique word of our vocabulary. This will be used further on when creating one-hot encoded vectors.", "After using the above function on the text we get the dictionary:", "What we created up to this point is still not neural network friendly because what we have as data is the pairs of (focus word, context word). In order for the computer to start doing computations, we need a clever way to transform these data points into data points made up of numbers. One such clever way is the one-hot encoding technique.", "One-hot encoding transforms a word into a vector that is made up of 0 with one coordinate, representing the string, equal to 1. The vector size is equal to the number of unique words in a document. For example, lets us define a simple list of strings:", "There are 3 unique words: blue, sky and car. One hot representation for each word:", "Thus the list can be converted into a matrix:", "We will be creating two matrices, X and Y, with the exact same technique. The X matrix will be created using the focus words and the Y matrix will be created using the context words.", "Recall the first three data points which we created given the texts about royalties:", "The one-hot encoded X matrix (words future, future, king) in python would be:", "The one-hot encoded Y matrix (words king, prince, prince) in python would be:", "The final sizes of these matrices will be n x m, where", "n - number of created data points (pairs of focus words and context words)", "m - number of unique words", "We now have X and Y matrices built from the focus word and context word pairs. The next step is to choose the embedding dimension. I will choose the dimension to be equal to 2 in order to later plot the words and see whether similar words form clusters.", "The hidden layer dimension is the size of our word embedding. The output layers activation function is softmax. The activation function of the hidden layer is linear. The input dimension is equal to the total number of unique words (remember, our X matrix is of the dimension n x 21). Each input node will have two weights connecting it to the hidden layer. These weights are the word embeddings! After the training of the network, we extract these weights and remove all the rest. We do not necessarily care about the output.", "For the training of the network, we will use keras and tensorflow:", "After the training of the network, we can obtain the weights and plot the results:", "As we can see, there are the words \u2018man\u2019, \u2018future\u2019, \u2018prince\u2019, \u2018boy\u2019 and \u2018daughter\u2019, \u2018woman\u2019, \u2018princess\u2019 in separate corners of the plot and form clusters. All this was achieved from just 21 unique words and 12 sentences.", "Often in practice, pre-trained word embeddings are used with typical word embedding dimensions being either 100, 200 or 300. I personally use the embeddings stored here: \\https://nlp.stanford.edu/projects/glove/.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A person who tries to understand the world through data and equations"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb337d0ba17a8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://eligijus-bujokas.medium.com/?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": ""}, {"url": "https://eligijus-bujokas.medium.com/?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "Eligijus Bujokas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd61597e07b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&user=Eligijus+Bujokas&userId=d61597e07b4d&source=post_page-d61597e07b4d----b337d0ba17a8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb337d0ba17a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb337d0ba17a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "https://nlp.stanford.edu/projects/glove/"}, {"url": "https://github.com/Eligijus112/word-embedding-creation", "anchor_text": "https://github.com/Eligijus112/word-embedding-creation"}, {"url": "https://en.wikipedia.org/wiki/Language_model", "anchor_text": "language modeling"}, {"url": "https://en.wikipedia.org/wiki/Feature_learning", "anchor_text": "feature learning"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "natural language processing"}, {"url": "https://unsplash.com/@heftiba?utm_source=medium&utm_medium=referral", "anchor_text": "Toa Heftiba"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "https://nlp.stanford.edu/projects/glove/"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----b337d0ba17a8---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/python?source=post_page-----b337d0ba17a8---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----b337d0ba17a8---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/nlp?source=post_page-----b337d0ba17a8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----b337d0ba17a8---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb337d0ba17a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&user=Eligijus+Bujokas&userId=d61597e07b4d&source=-----b337d0ba17a8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb337d0ba17a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&user=Eligijus+Bujokas&userId=d61597e07b4d&source=-----b337d0ba17a8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb337d0ba17a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb337d0ba17a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b337d0ba17a8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b337d0ba17a8--------------------------------", "anchor_text": ""}, {"url": "https://eligijus-bujokas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://eligijus-bujokas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eligijus Bujokas"}, {"url": "https://eligijus-bujokas.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "359 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd61597e07b4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&user=Eligijus+Bujokas&userId=d61597e07b4d&source=post_page-d61597e07b4d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3e3cf5271629&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8&newsletterV3=d61597e07b4d&newsletterV3Id=3e3cf5271629&user=Eligijus+Bujokas&userId=d61597e07b4d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}