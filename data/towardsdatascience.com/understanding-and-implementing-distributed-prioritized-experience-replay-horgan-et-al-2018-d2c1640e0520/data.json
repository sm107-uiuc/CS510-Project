{"url": "https://towardsdatascience.com/understanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520", "time": 1683001595.8037949, "path": "towardsdatascience.com/understanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520/", "webpage": {"metadata": {"title": "Understanding and Implementing Distributed Prioritized Experience Replay (Horgan et al., 2018) | by Chris Yoon | Towards Data Science", "h1": "Understanding and Implementing Distributed Prioritized Experience Replay (Horgan et al., 2018)", "description": "In the current state-of-the-art, many reinforcement learning algorithms make use of aggressive parallelization and distribution. In this paper, we will review and implement the ApeX framework (Horgan\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/01/26/prioritizing-experience-replay.html", "anchor_text": "this blog post", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1511.05952.pdf", "anchor_text": "actual paper", "paragraph_index": 10}], "all_paragraphs": ["In the current state-of-the-art, many reinforcement learning algorithms make use of aggressive parallelization and distribution. In this paper, we will review and implement the ApeX framework (Horgan et al., 2018), also referred to as distributed prioritized experience replay. In particular, we will implement the ApeX DQN algorithm.", "In this post, I try to accomplish the following:", "The first work in generalizing the formulation of distributed reinforcement learning architectures was Gorila (General Reinforcement Learning framework; I\u2019ve also seen \u201cGoogle Reinforcement Learning framework\u201d somewhere, but the paper writes \u201cGeneral\u201d) presented in Nair et al. (2015) at Google.", "In Gorila we have a decoupled actor (data generation/collection) and learner (parameter optimization) processes. Then, we have a parameter server and a centralized replay buffer that are shared with every learner and actor processes. The actors carry out trajectory roll-outs and send it to the centralized replay buffer, from which the learner samples experiences and updates its parameters. After the learner updates its parameters, it sends its new neural network weights to the parameter server, which the actor syncs its own neural network weights. All these processes happen asynchronously, independent of each other.", "The Gorila framework seems to be the underlying structure for modern distributed reinforcement learning architectures \u2014 such as ApeX. In particular, ApeX modifies the above structure by using a prioritized replay buffer (prioritized experience replay, proposed by Schaul et al. (2015); we will have a brief discussion about this in the section below).", "The motivation for this kind of architecture comes from the fact that the majority of the computational time in RL is spent generating data, and not updating neural networks. Thus, we can perform more updates if we make these processes asynchronous with each other.", "In addition to the speed-up from the asynchronous structure, this distributed architecture has the conceptual advantage of improved exploration; we can run each actor process on distinctly-seeded environments to allow the collection of a diverse data set. For instance, A3C (Mnih et al. 2016) credits its significant improvement over the synchronous version A2C to improved exploration. As such, distributed implementations of algorithms also benefit from this effect.", "In reinforcement learning, not all experience data are equal: Intuitively, there are experiences that can contribute more to the agent\u2019s learning than other experiences. Schaul et al. (2015) suggested that we prioritized the experience by its temporal difference (TD) error:", "Recall the canonical q-learning algorithm; we are trying to minimize the distance between our current prediction of the Q value, and our bootstrapped target Q value (i.e. the temporal difference error). That is, the higher this error, the greater the degree of update there is to the neural network weights. As such, we can obtain faster learning by sampling experiences with higher TD error.", "However, sampling purely based on TD error introduces bias to the learning. For instance, experiences with low TD error might not be replayed for a long time, curtailing the limiting of replayed memories and making the model prone to over-fitting. To remedy this, Schaul et al. (2015) using importance sampling weights for bias correction. That is, for every update step, we multiply the gradient by the importance sampling weights of the experiences, computed as:", "When implementing the prioritized replay buffer, we want to have two separate data structures to store the experience data and its respective priority values. To make the sampling more efficient, Schaul et al. (2015) suggest using segment trees (or sum trees) to store the priority values. We\u2019ll use the OpenAI Baselines implementation for out ApeX implementation, but check out this blog post and the actual paper for a more detailed discussion about prioritized experience replay.", "Below is an extremely brief introduction to using Ray:", "This should suffice for following through the code. I recommend checking out their tutorial and documentation, especially If you would like to implement your own parallel/distributed programs with Ray.", "We will use Ray for multiprocessing, instead of the built-in python-multiprocessing module \u2014 from my personal experience, I had a lot of trouble debugging multiprocessing Queues, and had an easier time implementing ApeX with Ray.", "Though the implementations run and work, I\u2019m currently working to fix possible bottlenecks and memory leakages. To that end, my implementations are still work-in-progress \u2014 I recommend not to heavily parallelize tasks with them yet. I will update this post once the memory usage issues are fixed.", "We will break down the implementation process as such: we will group actor and replay buffer together, and then the learner and parameter server. The jupyter notebook files below are simplified versions of my full implementations, just to show how ApeX can be implemented; if you are interested in the full implementation details and would like to run it on your device, take a look at my GitHub repository!", "After every roll-out, the actors asynchronously send their transition data to the centralized replay buffer. The centralized buffer assigns priorities to the experiences (as outlined in prioritized experience replay), and sends batches of sampled experiences to the learner.", "We will implement the actor process below and use the OpenAI Baselines implementation of prioritized replay buffer. We will have to modify the baseline implementation of the replay buffer to make it compatible with Ray:", "We can make the actor process more memory efficient by having the actors keep a local buffer, and send everything to the centralized buffer periodically \u2014 I will update the code shortly.", "With the experiences batches sent from the central replay buffer, the learner updates its parameters. Since we\u2019re using prioritized experience replay, the learner also has to update the priority values of the used experiences and send that information to the buffer.", "Once the learner parameters are updates, it sends its neural network weights to the parameter server. A neat thing about Ray is that it supports zero-cost reading of numpy arrays; that is, we want the learner to send numpy arrays of its network weights, instead of PyTorch objects. Then, the actors periodically pull and sync their q-networks with the stored parameter.", "We will start with the parameter server, and implement the learner process:", "To find a running implementation of ApeX DQN and ApeX DDPG, check out my GitHub repository in the link below:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd2c1640e0520&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863----d2c1640e0520---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2c1640e0520&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2c1640e0520&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pemami4911.github.io/paper-summaries/deep-rl/2016/01/26/prioritizing-experience-replay.html", "anchor_text": "this blog post"}, {"url": "https://arxiv.org/pdf/1511.05952.pdf", "anchor_text": "actual paper"}, {"url": "https://github.com/cyoon1729/Distributed-Reinforcement-Learning", "anchor_text": "cyoon1729/Distributed-Reinforcement-LearningSingle-machine implementations of distributed reinforcement learning algorithms with and PyTorch. See below for a\u2026github.com"}, {"url": "https://arxiv.org/pdf/1507.04296.pdf", "anchor_text": "Massively Parallel Methods for Deep Reinforcement Learning (Nair et al., 2015)"}, {"url": "https://arxiv.org/pdf/1511.05952.pdf", "anchor_text": "Prioritized Experience Replay (Schaul et al., 2015)"}, {"url": "https://arxiv.org/pdf/1803.00933.pdf", "anchor_text": "Distributed Prioritized Experience Replay (Horgan et al., 2015)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d2c1640e0520---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----d2c1640e0520---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/dqn?source=post_page-----d2c1640e0520---------------dqn-----------------", "anchor_text": "Dqn"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----d2c1640e0520---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/distributed?source=post_page-----d2c1640e0520---------------distributed-----------------", "anchor_text": "Distributed"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2c1640e0520&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&user=Chris+Yoon&userId=b24112d01863&source=-----d2c1640e0520---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2c1640e0520&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&user=Chris+Yoon&userId=b24112d01863&source=-----d2c1640e0520---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2c1640e0520&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd2c1640e0520&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d2c1640e0520---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d2c1640e0520--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d2c1640e0520--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d2c1640e0520--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/@thechrisyoon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "631 Followers"}, {"url": "https://www.linkedin.com/in/chris-yoon-75847418b/", "anchor_text": "https://www.linkedin.com/in/chris-yoon-75847418b/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6d3234499fec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-and-implementing-distributed-prioritized-experience-replay-horgan-et-al-2018-d2c1640e0520&newsletterV3=b24112d01863&newsletterV3Id=6d3234499fec&user=Chris+Yoon&userId=b24112d01863&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}