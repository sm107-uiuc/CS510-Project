{"url": "https://towardsdatascience.com/xrl-explainable-reinforcement-learning-4cd065cdec9a", "time": 1683010322.3340201, "path": "towardsdatascience.com/xrl-explainable-reinforcement-learning-4cd065cdec9a/", "webpage": {"metadata": {"title": "XRL: eXplainable Reinforcement Learning | by Meet Gandhi | Towards Data Science", "h1": "XRL: eXplainable Reinforcement Learning", "description": "With AI technologies being deployed at scale as well as performing autonomously, it becomes imperative to instil the property of explainability in AI; which would result in the users trusting the AI\u2026"}, "outgoing_paragraph_urls": [{"url": "https://academic.oup.com/bjps/article-abstract/56/4/843/1451716", "anchor_text": "Halpern and Pearl 2005", "paragraph_index": 14}], "all_paragraphs": ["With AI technologies being deployed at scale as well as performing autonomously, it becomes imperative to instil the property of explainability in AI; which would result in the users trusting the AI technologies. A user will confidently use the technology if he/she can trust the technology, and in order for the technology to be trusted, it needs to be transparent. This transparency in AI can be achieved if a model is able to provide justifications with regards to its predictions and decision-making. Explainability is more crucial in the field of Reinforcement Learning where agents learn by itself without any human intervention.", "The objective of this article is to make the readers aware of the XRL techniques currently pursued by different research teams. An important thing to consider regarding XRL is that much of the work in the domain should be done keeping in mind the human side of the equation. As a result, in order to advance XRL (and XAI), an interdisciplinary approach should be undertaken to heed to the needs of a human user who has no particular domain expertise and is utilizing the AI technology. With respect to the article, it is assumed that the reader has an intermediate level of knowledge of Reinforcement learning theory as well as a basic understanding of eXplainable AI.", "Moving forward, let us first categorize XRL techniques. Similar to XAI methodologies, XRL techniques can be classified into the following based on the scope and timing at which information is extracted from the XRL technique.", "Below are some of the potential XRL methodologies developed by research teams showing promising leads.", "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning described in [1] extends multi-task RL by introducing modularity in the policy design through a two-layer hierarchical structure. The framework is fundamentally based on the fact that a complex task requires different skills and is composed of several simpler subtasks. [1] trained their model to perform object manipulation task in Minecraft i.e. finding, getting, putting or stacking a black in Minecraft of a certain type. The model utilized the \u2018advantage actor-critic\u2019 (A2C) for policy optimization using off-policy learning. The model possesses a hierarchical structure as every top-level policy can be decomposed into low-level actions. In the case of Minecraft, the task of stacking a cobblestone block represents a complex task which the agent then decomposes into the actions of finding cobblestone block, getting cobblestone block and putting cobblestone block. The interpretability of the framework comes from the fact that each task (for instance stack cobblestone block) is described by human instruction, and the trained agents can only access learnt skills through these human descriptions, making the agent\u2019s policies and decisions human-interpretable.", "Additionally, the framework integrated a stochastic temporal grammar (STG) methodology to model temporal relationships and priorities of tasks (for instance, before stacking a cobblestone block on top of another cobblestone block; you must first find a cobblestone block, pick it up and then find another cobblestone block to put the one on the hand on it). Hence the key idea of the framework is to disintegrate complex tasks to simpler subtasks. Thereafter if these simpler subtasks can be solved using the policies already learnt or skill already gained by the agent then no learning happens; otherwise, new skill needs to be learnt to perform a novel action. This framework can be represented pictorially as below.", "Let G be defined as the task set in which each task g represents the human instruction (for instance \u2018Get white\u2019 in the above figure). In [1], g = \u27e8skill, instruction\u27e9 two-word tuple is used to describe the object manipulation task in Minecraft. As it is a multi-task framework, reward differs for goals specified for different tasks. Hence R(s,g) is used to depict the task-specific reward function where s=state. Initially, the agent is trained a terminal policy \u03c0\u2092 for terminal basic tasks G\u2092. The task set is successively increased as the human instructor keeps on giving instruction such that G\u2092\u2282G\u2097\u2282\u2026\u2282G\u2096, which results in learning of policies as portrayed in Fig 2. Also, note that at stage k and with h=k-1, we define G\u2095 as the base task set for G\u2096 and \u03c0\u2095 is defined as the base policy for \u03c0\u2096. Weak supervision from humans in the form of tasks is required for the learning of policies and hence augmentation of the agent. Hence task augmentation from humans is required for hierarchical and interpretable skill acquisition in multi-task reinforcement learning.", "The main idea behind hierarchical policy as detailed in [1] is that the current task set G\u2096 can be fragmented into several subtasks, which can be present in the base task set G\u2095 and can be solved using base policy \u03c0\u2095. As a result, instead of mapping the current state and human instruction to perform an action as described in Fig 3(a)\u2019s flat policy structure, hierarchical policy design takes advantage by reusing the base policies for performing base tasks characterized as subtasks in the current stage. The global policy at stage k, \u03c0\u2096 consists of four sub-policies: \u201ca base policy for executing previously learned tasks, an instruction policy that manages communication between the global policy and the base policy, an augmented flat policy which allows the global policy to directly execute actions, and a switch policy that decides whether the global policy will primarily rely on the base policy or the augmented flat policy.\u201d \u2014 [1]. Instruction policy accounts for mapping the current state s and g \u2208 G\u2096 to g\u00b9 \u2208 G\u2095, hence its main function is to convey information to the base policy \u03c0\u2095 as to which base task g\u00b9 \u2208 G\u2095, it needs to perform. As mentioned earlier g consists of two things (skill and item from the phrase of an instruction) which are conditionally independent of each other, therefore", "When the base policy cannot be implemented for performing a certain task, augmented flat policy comes into action mapping state s and task g to action a, so that new skills can be learnt to solve novel tasks in G\u2096. The switch policy in the above framework plays the role of a mediator choosing as to when to implement base policy and when to implement augmented flat policy. The switch policy outputs a binary variable e such that when e=0, the global policy \u03c0\u2096 follows base policy and when e=1, \u03c0\u2096 follows augmented flat policy.", "Thus at each step, the model first samples the binary variable e\u209c from the switch policy and a new instruction g\u02e1 from the instruction policy so that our model can sample actions from the base policy. Below figure summarizes the process at each stage.", "Stochastic Temporal Grammar is used to ascertain temporal relations in distinct tasks. For example, to stack an object, we first need to find, pick and place the object. STG is used as a prior in modifying the switch and instruction policies mentioned in the hierarchical policy i.e. sampling e\u209c and g\u02e1. In [1], STG is defined at each step k>0 for a task g by the following: 1) transition probabilities,", "As a result, incorporating STG into the hierarchical policy, we get refined switch and instruction policies as follows:", "The resulting framework exhibited a higher learning efficiency, was able to generalize well in new environments and was inherently interpretable as it needs weak human supervision to give the agent instructions in order to learn new skills.", "Cognitive science proposes that in order to understand or interpret a phenomenon, humans build causal models to encode the cause-effect mapping of events happening around us. While building a causal model, we constantly ask the question of why or why not. Continuing with this logic, Explainable Reinforcement Learning through a Causal Lens detailed in [2] tries to build a structural causal model for generating causal explanations of the behaviour of model-free reinforcement learning agents through variables of interest. Counterfactual analysis is carried out of this structural causal model in order to generate explanations. Explainable Reinforcement Learning through a Causal Lens also investigates the comprehension gained by the users through the explanations, explanation satisfaction to the users as well as trust induced in the user for model-free reinforcement learning agent through the explanations. In Explainable Reinforcement Learning through a Causal Lens, action influence models are incorporated for Markov Decision Processes (MDP) based RL agents, extending structural causal models (SCMs) with the addition of actions. First, let us learn about SCMs.", "Structural Causal Models or SCMs were introduced in Halpern and Pearl 2005. SCMs portray the world using exogeneous/external and endogeneous/internal random variables; some of these variables might possess causal relationships which each other which is represented using a set of structural equations. Formally, for depicting SCMs, we have to first define a signature S which is a tuple (U, V, R) where U is the set of exogenous variables, V is the set of endogenous variables and R represents a function which designates the range of values for every variable Y \u2208 U \u222a V.", "Formal Definition: A structural causal model is a tuple M=(S, F) where F represents the set of structural equations, one for each X\u2208 V such that", "giving the value of X based on other variables in U\u222a V. In other words, F\u2093 gives the value of X in terms of other variables in the model. Additionally, a context \ud835\udd32 is defined as a vector of unique values for each exogenous variable u\u2208 U. A situation is a model-context pair (M, \ud835\udd32). Assigning values to the variables of the model according to the structural equations leads to instantiation. An actual cause of an event \u03c6 is represented by the endogenous variables and their values. If the endogenous variables differ then the event \u03c6 does not occur, hence there is some counterfactual context embedded in the actual cause of the event \u03c6.", "The main intent behind action influence models is to facilitate explanations of the agent\u2019s behaviour from how the actions influence the environment. Thus we extend the idea of SCMs to action influence models by incorporating actions in causal relationships.", "Formal Definition: An action influence model is a tuple (S\u2090, F) where S\u2090 is the signature extending the SCM\u2019s signature with a set of actions A i.e. S\u2090 in action influence models = (U, V, R, A) and F is the set of structural equations, however here F has multiple values for each X\u2208 V depending on the unique action set \ud835\udd52 applied. Thus, F_{X, \ud835\udd52} portrays the causal effect on X when action \ud835\udd52 is exerted. A set of reward variables X\u1d63\u2286 V are allocated to sink nodes.", "An explanation consists of an explanandum, the event to be explained and explanan, the cause which justifies the happening of the event. Explanation generation requires the following steps: 1) defining the action influence model; 2) learning the structural equations during reinforcement learning; and finally, 3) generating explanans for explanandum. Explainable Reinforcement Learning through a Causal Lens mainly focuses on providing explanations for questions of the form \u2018Why action A?\u2019 or \u2018Why not action A?\u2019. Also, it characterizes explanations in the context of reinforcement learning into two types: 1) Complete explanations; and 2) Minimally complete explanations. Now let us define them formally.", "Complete explanations: A complete explanation can be given for an action \ud835\udd52 under the actual instantiation M_{V\u2190S} as a tuple (X\u1d63=x\u1d63, X\u2095=x\u2095, X\u1d62=x\u1d62), where X\u1d63 is the vector comprising of reward variables achieved by moving through the causal chain of the graph to the sink nodes; X\u2095 is the vector comprising of variables present at the head node of the action \ud835\udd52; X\u1d62 is the vector comprising of intermediate variables from the head to sink node in the action influence graph; and x\u1d63, x\u2095, x\u1d62 represent the values of the associated variables given through the actual instantiation M_{V\u2190S}.", "Hence, the complete explanation according to the above definition provides the complete causal chain from an action \ud835\udd52 to any future reward variable that will be associated with the action \ud835\udd52. For instance, the causal chain for action A\u209b is shown as darkened in Fig 4. The complete explanation tuple, in this case, would be ([S=s], [A\u2099=a\u2099], [D\u1d64=d\u1d64, D_{b}=d_{b}]). The depth-first search algorithm can be used for traversing the action influence graph from the head node of the action to all the sink nodes.", "Minimally complete explanations: For larger graphs, the higher number of intermediate nodes may result in confusion. In this case, minimally complete explanations might come in handy. A minimally complete explanation is defined as a tuple (X\u1d63=x\u1d63, X\u2095=x\u2095, X\u209a=x\u209a), where X\u1d63=x\u1d63 and X\u2095=x\u2095 are similar to the earlier definition, and X\u209a represents the vector of variables which are the immediate predecessor to X\u1d63. Immediate predecessors are chosen as they depict the immediate causes of the reward. Note that the number of intermediate nodes to be considered in minimally complete explanations depends on the application as well as the knowledge of the user.", "Counterfactual explanations can be produced by comparing the causal chain of the event that occurred and explanandum (counterfactual action). First, let us define counterfactual instantiation under which we assign optimal state variable values such that the counterfactual action will be chosen.", "Counterfactual instantiation: A counterfactual instantiation pertaining to a counterfactual action \ud835\udd53 can be given from a model M_{Z\u2190S_{z}}, where Z instantiates all the predecessor variables of action \ud835\udd53 as well as successor variables falling in the causal chain of \ud835\udd53, utilizing the structural equations.", "Minimally complete contrastive explanations: Given a minimally complete explanation X=x for an action \ud835\udd52 under the actual instantiation M_{V\u2190S} and a minimally complete explanation Y=y for an action \ud835\udd53 under the counterfactual instantiation M_{Z\u2190S_{z}}, we can define a minimally complete contrastive explanation using a tuple (X\u02e1=x\u02e1, Y\u02e1=y\u02e1, X\u1d63=x\u1d63) where X\u02e1 and Y\u02e1 are the maximal sets of variables X and Y satisfying the condition (X\u02e1=x\u02e1) \u2229 (Y\u02e1=y\u02e1) \u2260 \u2205. Thereafter we contrast x\u02e1 and y\u02e1 (difference condition). Note that here X\u1d63 corresponds to the reward nodes for action \ud835\udd52.", "Hence, the minimally complete contrastive explanation is used to extract the differences between the actual causal chain for action \ud835\udd52 and the counterfactual causal chain for action \ud835\udd53.", "Examine the action influence graph for the Starcraft II agent shown in Fig 4. Using this graph, we will try to answer the question: \u2018Why build supply depots (A\u209b)?\u2019 and \u2018Why not build barracks (A_{b})?\u2019.", "Let m = [W = 12, S = 3, B = 2, A\u2099 = 22, D\u1d64 = 10, D_{b} = 7] be the actual instantiation and m\u02e1 = [W = 12, S = 1, B = 2, A\u2099 = 22, D\u1d64 = 10, D_{b} = 7] be the counterfactual instantiation. Implementing the difference condition, we get the minimally complete contrastive explanation as ([S = 3] , [S = 1] , [D\u1d64 = 10, D_{b} = 7]). On comparing [S=3] with [S=1], an explanation can be given from the NLP template: Because the goal is to increase destroyed units (D\u1d64) and destroyed buildings (D_{b}), it makes sense to build supply depots (A\u209b) to increase supply number (S). Note that the value of variables is obtained during instantiation through learning structural causal equations.", "Given a directed acyclic graph depicting the causal relationship between variables, structural equations can be learnt as multivariate regression models during the training period of the reinforcement learning agent. By saving e\u209c = (s\u209c, a\u209c, r\u209c, s_{t+1}) at each time step in a dataset D\u209c={e\u2081, \u2026, e\u209c}, experience replay can be implemented. Then a subset of structural equations F_{X, A} are updated utilizing a regression learner \ud835\udd43. Note that we update structural equations of only the variables which are associated with the specified action. For instance, referring to Fig 4, for any experience frame with the action A\u209b, only the equation F_{S, A\u209b}(W) will be updated. Note that any regression learner can be utilized as a learning model \ud835\udd43, such as MLP regressors.", "For reviewing further topics like the computational and human evaluation of the technique as well as its results, refer [2].", "[extension to Explainable Reinforcement Learning through a Causal Lens]", "Even though the explanations generated using action influence model beat the standard state-action model, structural equations performed poorly during computational task prediction accuracy. Hence substituting structural equations to model causal relationships between variables, Distal Explanations for Explainable Reinforcement Learning Agents detailed in [4] suggests using decision nodes from a decision tree to generate explanations utilizing the causal chain.", "A model for generating distal explanation consists of a decision node from a decision tree portraying the agent\u2019s complete policy and linked with the causal chain from the action influence model from the earlier section. Suppose \ud835\udd4b represents a decision tree model. Then while training the reinforcement learning agent, we implement experience replay by storing e\u209c = (s\u209c, a\u209c) at each stage t into a dataset D\u209c = {e\u2081,\u2026,e\u209c}. Thereafter we uniformly sample mini-batches from D to train \ud835\udd4b with s\u209c as input and a\u209c as output. A decision tree with no constraints on the number of its nodes would lead to confusing and overwhelming explanations hence in [4] the growth of a decision tree \ud835\udd4b is limited by setting the number of leaves equal to the number of possible actions in the domain of the agent. In [4], it is shown in the evaluation that constraining the decision tree \ud835\udd4b hardly affects the computation task prediction accuracy in comparison to the one with no constraints. In order to store the decision nodes of the decision tree \ud835\udd4b at a state S\u209c, the model traverses the decision tree from the root node until it reaches a leaf node, during which it stores the modes in its path. For instance, from Fig 6, A\u2099 and B are the decision nodes for action A\u209b. Note that the decision nodes represent the feature variables of the agent\u2019s state space.", "Now, we will define minimally complete explanations from decision trees for \u2018Why?\u2019 questions.", "Minimally complete explanations: Given a set of decision nodes X\u2098=x\u2098 for action \ud835\udd52 from the decision tree \ud835\udd4b, a minimally complete explanation for a why question can be defined as a tuple (X\u1d63=x\u1d63, X\u2099=x\u2099) where X\u1d63 is the set of reward variables; and X\u2099 is the set of maximal variables with the condition X\u2099=(X\u2098=x\u2098)\u2229(X\u2090=x\u2090); here X\u2090 represents the set of intermediate variables in the causal chain of action \ud835\udd52. Note that here x\u2098, x\u1d63 and x\u2090 depict the values of variables under the actual instantiation M_{V\u2190S}.", "Consider the Starcraft II agent, here the minimally complete explanation for the question \u2018Why A\u209b?\u2019 can be given by the tuple ([A\u2099=a\u2099], [D\u1d64=d\u1d64, D_{b}=d_{b}]); as here D\u1d64 and D_{b} represent the reward variables and A\u2099 is the intersection of the causal chain (S\u2192A\u2099\u2192[D\u1d64, D_{b}]) shown as bold in Fig 6(b) and the decision nodes (A\u2099, B) shown as grey in Fig 6(a).", "Minimally complete contrastive explanations: A minimally complete contrastive explanation for a why not question can be given by a tuple (X\u1d63=x\u1d63, X_{con}=x_{con}) where X\u1d63 is similar to the earlier definition and X_{con} is the maximal set of variables satisfying the condition X_{con}=(X_{b}=x_{b}) \u2229 (X_{c}=x_{c}); here X_{b} represents the set of intermediate nodes in the causal chain of counterfactual action \ud835\udd39 and X_{c} represents the counterfactual decision nodes. Note that the values x\u1d63 and x_{c} are contrasted using the actual instantiation M_{V\u2190S} and counterfactual instantiation M_{Z\u2190S_{Z}}. The counterfactual decision nodes X_{c}are generated utilizing the below algorithm.", "Now as an example review the Starcraft II agent task and consider the question \u2018Why not build barracks A_{b}?\u2019. The contrastive explanation is generated as follows. Initially, extract the decision boundary values from the decision nodes A\u2099 and B i.e. \u22645 for A\u2099 and >2 for B. Thereafter vary the value by a small amount \u0394 = 0.01 starting from the decision node closest to the leaf node, hence generating a new feature value in the current state of the agent. Here the feature value of the variable B will change to 1.99. In this new state predict the counterfactual action as A_{b} from the decision tree and extract the counterfactual decision nodes by traversing the tree. Subsequently, we get X_{con}=B, as B is the intersection of the nodes in the causal chain of counterfactual action A_{b} (B\u2192A\u2099\u2192[D\u1d64, D_{b}]) and X_{c} (B in this iteration). Now we contrast the actual value (3 from causal model M and training of decision tree \ud835\udd4b) and the counterfactual value (1.99) of B and append the reward variables to generate the contrastive explanation using NLP.", "Certain actions activate the execution of other actions. In the domain of reinforcement learning, we define a distal action as the one most dependent on the execution of the current action taken by the agent. For instance, in the context of Starcraft II, the action train marines A\u2098 can't be performed before building barracks A_{b}. Now to generate distal explanations, first distal action needs to be predicted. [4] utilizes a many-to-one recurrent neural network (RNN) as a prediction model \ud835\udd43 to approximate the distal action given the previous states and actions taken by the agent. The output of \ud835\udd43 is the distal action and its cumulative reward. Refer [4] Section 4.4 for information on the architecture, training and training data of \ud835\udd43. Having trained a distal action prediction model \ud835\udd43, we can now define minimally complete distal explanations for \u2018why?\u2019 and \u2018why not?\u2019 questions.", "Minimally complete distal explanation: Given a minimally complete contrastive explanation, current action \ud835\udd52 and a distal action prediction model \ud835\udd43, a minimally complete distal explanation can be defined with a tuple (X\u1d63=x\u1d63, X_{con}=x_{con}, a\u2090) where X\u1d63 and X_{con} are defined as earlier and a\u2090 is the distal action predicted using \ud835\udd43 with the condition a\u2090\u2208 A \u2229 A\u2092; here A represents the action set of the agent and A\u2092 represents the set of actions in the causal chain of current action \ud835\udd52.", "Consider the question \u2018Why to train marine A\u2098 instead of building barracks A_{b}?\u2019. On analyzing, we get A\u2099 as the counterfactual node with actual value 10 and counterfactual value 5. Also, with A\u2090 (attack) as the predicted distal action, a distal explanation can be generated using NLP.", "Causal Explanation (generated using Explainable Reinforcement Learning through a Causal Lens\u2019 minimally complete contrastive explanation)", "\u201cBecause it is more desirable to do the action train marine A\u2098 to have more ally units A\u2099 as the goal is to have more Destroyed Units D\u1d64 and Destroyed buildings D_{b}.\u201d \u2014 [4] Section 4.4", "Distal Explanation (generated using Distal Explanations for Explainable Reinforcement Learning Agents\u2019 minimally complete distal explanation for \u2018why not\u2019 questions)", "\u201cBecause ally unit number A\u2099 is less than the optimal number 18, it is more desirable do the action train marine A\u2098 to enable the action attack A\u2090 as the goal is to have more Destroyed Units D\u1d64 and Destroyed buildings D_{b}.\u201d \u2014 [4] Section 4.4", "For reviewing further topics like the computational and human evaluation of the technique as well as its results, refer [4].", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Graduate Student at the University of Michigan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4cd065cdec9a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gandhi.meet?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gandhi.meet?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "Meet Gandhi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fb7c78c2b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&user=Meet+Gandhi&userId=8fb7c78c2b4&source=post_page-8fb7c78c2b4----4cd065cdec9a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cd065cdec9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cd065cdec9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://academic.oup.com/bjps/article-abstract/56/4/843/1451716", "anchor_text": "Halpern and Pearl 2005"}, {"url": "https://doi.org/10.1093/bjps/axi147", "anchor_text": "https://doi.org/10.1093/bjps/axi147"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4cd065cdec9a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/explainable-rl?source=post_page-----4cd065cdec9a---------------explainable_rl-----------------", "anchor_text": "Explainable Rl"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4cd065cdec9a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/explainable-ai?source=post_page-----4cd065cdec9a---------------explainable_ai-----------------", "anchor_text": "Explainable Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cd065cdec9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&user=Meet+Gandhi&userId=8fb7c78c2b4&source=-----4cd065cdec9a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cd065cdec9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&user=Meet+Gandhi&userId=8fb7c78c2b4&source=-----4cd065cdec9a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cd065cdec9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4cd065cdec9a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4cd065cdec9a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4cd065cdec9a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gandhi.meet?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gandhi.meet?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Meet Gandhi"}, {"url": "https://medium.com/@gandhi.meet/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "52 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8fb7c78c2b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&user=Meet+Gandhi&userId=8fb7c78c2b4&source=post_page-8fb7c78c2b4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8fb7c78c2b4%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxrl-explainable-reinforcement-learning-4cd065cdec9a&user=Meet+Gandhi&userId=8fb7c78c2b4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}