{"url": "https://towardsdatascience.com/word-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e", "time": 1682993815.771944, "path": "towardsdatascience.com/word-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e/", "webpage": {"metadata": {"title": "Word Embeddings and Document Vectors \u2014 When in Doubt, Simplify | by Ashok Chilakapati | Towards Data Science", "h1": "Word Embeddings and Document Vectors \u2014 When in Doubt, Simplify", "description": "Word-vectors in conjunction with document vectors have been studied here for text classification."}, "outgoing_paragraph_urls": [{"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github", "paragraph_index": 0}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "large movie review data set", "paragraph_index": 2}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20-news", "paragraph_index": 2}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github", "paragraph_index": 2}], "all_paragraphs": ["This is the third and final article in this series on using word-vectors with document-vectors for NLP tasks. The focus here is text classification. We summarize the results and recommendations thereof, when word-vectors are combined with document-vectors. The code can be downloaded from github. Let us jump right in with a quick summary of the past two articles.", "With that we are ready for evaluating the impact of word-vectors for text classification and qualifying the results with options open to us in the pipeline steps. We stick to a few major options for each of the three steps.", "We work with two document repositories. The large movie review data set from Stanford for binary sentiment classication, and the reuter 20-news from scikit pages for multiclass. To keep it simple we stick to a single training set and single test set. In case of 20-news we do a stratified split with 80% for training and 20% for test. The imdb movie review data set comes with defined train and test sets. Lines 9 and 10 in the code snippet below use a Tokens class (check the github code repo) that has methods to pull tokens from the index.", "The model to run is an instance of the pipeline \u2014 a specific combination of vectorizer, transformer, and classifier. Lines 13 thru 15 define the model and run the predictions. The simulations are run via a shell script below that loops over the different options for the pipeline steps.", "The mlp classier actually consists of 9 variants with 1,2, or 3 hidden layers each with 50, 100, or 200 neurons. The results reported here are for when 2 hidden layers are used each with 100 neurons. The other mlp classifier runs basically serve to verify that the quality of the classification was not very sensitive at this level of hidden layers and neurons.", "Some combinations are not allowed however and they are skipped in the Python implementation. Those are:", "Th only metrics we look at are the F-scores for the quality of classification and the cpu time for efficiency. In case of multiclass classification for the 20-news data set, F-score is an average over all the 20 classes. The run time for a classifier+vectors combination is averaged across all the runs with the same combination.", "The results for the 20-news data set are summarized in Figure 1 below.", "There is a lot of detail crammed into the figure here so let us summarize point by point.", "1. Document Vectors vs Document+Word Vectors: A glance at 1A and 1B would tell us that the classification quality in 1A is better, not by much perhaps, but nevertheless true and across the board. That is, if classification quality is paramount then document vectors seem to have an edge in this case.", "2. Stopped vs Stemmed: Stemmed vocabulary yields shorter vectors, so better for performance for all classifiers. This is especially true for the mlp classifier where the number of input neurons equals the size of the incoming document vectors. When the words are stemmed, the number of unique words dropped by about 30% from 39k to 28k, a big reduction in the size of the pure document vectors.", "3. Frequency Counts Vs Tf-Idf: Tf-Idf vectorization allows for differential weighting for words based on how commonly they occur in the corpus. For keyword based search schemes it helps improve the relevance of search results.", "4. Pre-trained Vectors Vs Custom Vectors: This applies to Figure 1B alone. Custom word-vectors seem to have an edge.", "5. Timing Results: Figure 1C shows the average cpu time for the fit & predict runs.", "Figure 2 below shows binary classification the results obtained for the movie review data set.", "The observations here are not qualitatively different from the above so we will not spend much time on it. The overall quality of classification here is better with the F-scores being north of 0.8 (compared to a around 0.6 for the 20-news corpus). But that is just the nature of this data set.", "We cannot unfortunately draw definite conclusions based on our somewhat shallow testing on just two data sets. Plus as we noted above, there was some variance in classification quality for the same pipeline across the data sets as well. But at a high level we can perhaps conclude the following \u2014 take them with a grain of salt!", "1. When in doubt \u2014 simplify. Clearly this deserved to make it to the title of the post. Do the following and you will not be too wrong and you will get your work done in a jiffy to boot. Use:", "2. Understand the corpus. The specific pipeline (tokenization scheme => vectorization scheme => word-embedding algorithm => classifier) that works for well one corpus may not be the best for a different corpus. Even if the general pipeline may work, the details (specific stemmer, number of hidden layers, neurons etc\u2026) will need to be tuned to get the same performance on a different corpus.", "3. Word embeddings are great. For dimensionality reduction and the concurrent reduction in the run times, for sure. They work pretty well, but there is more work to do. If you have to use neural nets for document classification you should try these.", "4. Use custom vectors. Using custom word-vectors generated from the corpus at hand are likely to yield better quality classification results", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c9aaeec244e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29----8c9aaeec244e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c9aaeec244e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c9aaeec244e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github"}, {"url": "https://towardsdatascience.com/word-embeddings-and-document-vectors-part-1-similarity-1cd82737cf58", "anchor_text": "Similarity"}, {"url": "https://towardsdatascience.com/word-embeddings-and-document-vectors-part-2-order-reduction-2d11c3b5139c", "anchor_text": "Order Reduction"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html", "anchor_text": "CountVectorizer"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer", "anchor_text": "TfIdfVectozer"}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing", "anchor_text": "Word2Vec/SGNS"}, {"url": "https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip", "anchor_text": "FastText"}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "Gensim"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html", "anchor_text": "Multinomial Naive Bayes"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "anchor_text": "Linear Support Vectors,"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html", "anchor_text": "Neural Nets"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "large movie review data set"}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20-news"}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8c9aaeec244e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/classification?source=post_page-----8c9aaeec244e---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----8c9aaeec244e---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/naive-bayes?source=post_page-----8c9aaeec244e---------------naive_bayes-----------------", "anchor_text": "Naive Bayes"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----8c9aaeec244e---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c9aaeec244e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----8c9aaeec244e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c9aaeec244e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----8c9aaeec244e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c9aaeec244e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8c9aaeec244e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8c9aaeec244e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8c9aaeec244e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/@ashok.chilakapati/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "244 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ab4b71672c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-and-document-vectors-when-in-doubt-simplify-8c9aaeec244e&newsletterV3=cc37b40eae29&newsletterV3Id=5ab4b71672c9&user=Ashok+Chilakapati&userId=cc37b40eae29&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}