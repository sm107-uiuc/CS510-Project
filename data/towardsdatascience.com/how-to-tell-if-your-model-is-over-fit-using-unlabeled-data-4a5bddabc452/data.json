{"url": "https://towardsdatascience.com/how-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452", "time": 1683009783.771908, "path": "towardsdatascience.com/how-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452/", "webpage": {"metadata": {"title": "How to tell if your model is over-fit using unlabeled data | by Elan Stopnitzky | Towards Data Science", "h1": "How to tell if your model is over-fit using unlabeled data", "description": "In many settings, unlabeled data is plentiful (think images, text, etc), while sufficient labeled data for supervised learning might be harder to obtain. In these situations, it can be difficult to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/elanstop/protein-classification-and-generation", "anchor_text": "distinguish naturally occurring protein sequences from randomly shuffled ones", "paragraph_index": 12}, {"url": "https://github.com/elanstop/generalization", "anchor_text": "GitHub repository", "paragraph_index": 12}, {"url": "https://github.com/elanstop/generalization", "anchor_text": "GitHub repository", "paragraph_index": 15}, {"url": "https://mlsys.org/Conferences/2019/doc/2019/167.pdf", "anchor_text": "Data validation for machine learning", "paragraph_index": 17}, {"url": "https://ieeexplore.ieee.org/abstract/document/8812069", "anchor_text": "Guiding deep learning system testing using surprise adequacy", "paragraph_index": 18}], "all_paragraphs": ["In many settings, unlabeled data is plentiful (think images, text, etc), while sufficient labeled data for supervised learning might be harder to obtain. In these situations, it can be difficult to determine how well the model will generalize. Most methods for assessing model performance rely on labeled data alone, e.g. k-folds validation. Without enough labeled data these can be unreliable. Is there anything more we can learn about the model\u2019s ability to generalize from unlabeled data? In this article, I demonstrate how unlabeled data can frequently be used to bound test loss.", "The task we will be considering is classification, and I assume that the model outputs the probabilities of an input belonging to each of the classes. For simplicity, I will restrict the discussion to binary classification and cross-entropy loss, although the results could easily be extended. The one key assumption that we will need to invoke is that the training set is assumed to be sampled from the same distribution as the testing set.", "Our starting observation is that if the model is well-fit to the training set (i.e. attaining low loss), it must confidently and correctly predict class labels. In other words, a histogram of predictions should be approximately bimodal with one mode at p=0 and one mode at p=1.", "I hypothesize that generally, an over-fit model can be expected to return less confident predictions on the unlabeled set than it did on the training set. This, in turn, allows us to bound the loss on the unlabeled set.", "The first figure was made using a two-layer neural network with 100 neurons in the input layer and one neuron in the output layer. To simulate over-fitting, the model was trained on synthetic data using sklearn\u2019s make_classification() method with 1,000 features over 10,000 samples. The model was trained on 1,000 samples and tested on the remaining 9,000. It quickly attains a training accuracy of 100% and a loss less than 0.0001. In support of my hypothesis, this is what the histogram looks like when switching to the unlabeled set:", "Note how the region in the middle that corresponds with less confident predictions is filled in relative to the training histogram. I now describe how this behavior can be used to bound the loss on the unlabeled set.", "We begin with the lower bound on the loss. To obtain it, we assume that all the predicted labels are correct, and that loss has been incurred solely through the reduction of confidence. In this case the cross entropy breaks up into two separate sums, with one sum for the 0-labels and the other for the 1-labels:", "where the argument of L_min is the set of predictions made on the unlabeled set and we have assumed a classification threshold of 0.5. We can get an upper bound by assuming all the labels are incorrect:", "This bound is not particularly useful but we introduce it to clean up notation for the next bound.", "Since the training set is assumed to have the same class distribution as the unlabeled set, a change in the distribution of labels when moving from the training set to the unlabeled set is a signal that points are being mislabeled, and can be used to tighten the lower bound. Let M be the minimum number of mislabeled points, N the number of points in the unlabeled set, p the probability of a 1-label in the training set, and p^hat the predicted probability of a 1-label on the unlabeled set. Then we have M = N |p \u2212 p^hat|, and the refined lower bound comes from assuming that it was the M least confident predictions (i.e. the ones closest to 0.5) that led to misclassification. This gives the new function:", "where the script M refers to the set of M least confident predictions, and script M^C to its complement. In addition to these bounds, one can easily imagine a family of estimates, e.g. by assuming that it was the M most confident predictions that were mislabeled, or that the model is in fact well calibrated and the probability of a true label is equal to its predicted probability. I now detail how these ideas play out in experiments.", "For the dense neural network model trained on synthetic data I described previously, the bounds give L_min = 0.05, and a minimum of 29 mislabeled points (these are not enough for the refined bound L_tighter to come out appreciably different). The true test loss comes out to 1.79, because of course in practice the assumption that the loss is dominated by uncertainty and not bad labels is unrealistic. Yet even without access to test labels, we have deduced that the test loss must be at minimum more than 600 times greater than the training loss, and that at least 29 points would get mislabeled. In many situations, it would be critical to know that a model\u2019s minimum loss is unacceptable before using it to make real-world decisions.", "These behaviors of over-fit models were confirmed as well for a convolutional neural network performing image classification, and an LSTM-based model I trained to distinguish naturally occurring protein sequences from randomly shuffled ones. These results indicate that my hypothesis is likely to hold frequently. Further details can be found in the corresponding GitHub repository.", "Breck et al.\u00b9 suggest that one should look for changes in the distribution of features when switching between data sets. This is obviously very similar in spirit, but here we have explicitly assumed that the training and testing data are drawn from the same distribution, and focus on the model rather than the data. Kim, Feldt, and Yoo\u00b2 introduce a measure for how surprising individual test points are to the model. This measure appears different from the ones introduced here, and it would be interesting to test out our approaches against one another. One positive attribute of the measures introduced here is their extreme ease of calculation and interpretability. Finally, Grosse et al.\u00b3 use variation in the predicted class labels around epsilon-balls centered on unlabeled points as a measure of over-fitting. A very attractive feature of their approach is that it provides a local, point-by-point measure of over-fitting. However, in contrast to my approach, theirs requires a suitable metric for assessing similarity of inputs, which may be non-trivial. In high dimensions, it may also be costly to run inference on enough samples from a ball to accurately measure the variance in predicted labels. To my knowledge, the approach I have described here is the first to use unlabeled data to directly bound the test loss.", "I have introduced a new method for assessing over-fitting, which leverages the unlabeled data that is plentiful in many settings. It is applicable in classification problems where the model outputs probabilities, and the training set can reasonably be assumed to be sampled from the same distribution as the unlabeled set. The bounds I have described are rather loose, but may nevertheless provide potentially critical information about over-fitting. Moreover, visual examination of how the histogram of predictions changes when moving from the training set to the unlabeled set is an extremely simple and powerful qualitative way of recognizing over-fitting. Perhaps in the future this same principle could be used to yield more sophisticated bounds.", "Relevant code may be found in my GitHub repository.", "Special thanks to Rachael Creager, Jamie Gainer, Amber Roberts, Jeremy Mann, and Nicolas Chausseau for feedback.", "[1] E. Breck, N. Polyzotis, R. Steven, E. Whang, and M. Zinkevich, Data validation for machine learning (2019), Proceedings of the 2nd SysML Conference.", "[2] J. Kim, R. Feldt, and S. Yoo, Guiding deep learning system testing using surprise adequacy (2019), IEEE/ACM 41st International Conference on Software Engineering.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interdisciplinary scientist and ML engineer with special interests in complex systems, information theory, and biotech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4a5bddabc452&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@e.stopnitzky?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@e.stopnitzky?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "Elan Stopnitzky"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa6bc458d9869&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&user=Elan+Stopnitzky&userId=a6bc458d9869&source=post_page-a6bc458d9869----4a5bddabc452---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a5bddabc452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a5bddabc452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@bigspringsstudio?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Lauren George"}, {"url": "https://towardsdatascience.com/s/photos/bird-of-paradise?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/elanstop/protein-classification-and-generation", "anchor_text": "distinguish naturally occurring protein sequences from randomly shuffled ones"}, {"url": "https://github.com/elanstop/generalization", "anchor_text": "GitHub repository"}, {"url": "https://github.com/elanstop/generalization", "anchor_text": "GitHub repository"}, {"url": "https://mlsys.org/Conferences/2019/doc/2019/167.pdf", "anchor_text": "Data validation for machine learning"}, {"url": "https://ieeexplore.ieee.org/abstract/document/8812069", "anchor_text": "Guiding deep learning system testing using surprise adequacy"}, {"url": "https://arxiv.org/abs/2006.06721", "anchor_text": "A new measure for overfitting and its implications for backdooring of deep learning,"}, {"url": "https://arxiv.org/abs/2006.06721", "anchor_text": "https://arxiv.org/abs/2006.06721"}, {"url": "https://medium.com/tag/generalization?source=post_page-----4a5bddabc452---------------generalization-----------------", "anchor_text": "Generalization"}, {"url": "https://medium.com/tag/overfitting?source=post_page-----4a5bddabc452---------------overfitting-----------------", "anchor_text": "Overfitting"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4a5bddabc452---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4a5bddabc452---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4a5bddabc452---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a5bddabc452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&user=Elan+Stopnitzky&userId=a6bc458d9869&source=-----4a5bddabc452---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a5bddabc452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&user=Elan+Stopnitzky&userId=a6bc458d9869&source=-----4a5bddabc452---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a5bddabc452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4a5bddabc452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4a5bddabc452---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4a5bddabc452--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4a5bddabc452--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4a5bddabc452--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@e.stopnitzky?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@e.stopnitzky?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Elan Stopnitzky"}, {"url": "https://medium.com/@e.stopnitzky/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "28 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa6bc458d9869&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&user=Elan+Stopnitzky&userId=a6bc458d9869&source=post_page-a6bc458d9869--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8ab089fce212&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-tell-if-your-model-is-over-fit-using-unlabeled-data-4a5bddabc452&newsletterV3=a6bc458d9869&newsletterV3Id=8ab089fce212&user=Elan+Stopnitzky&userId=a6bc458d9869&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}