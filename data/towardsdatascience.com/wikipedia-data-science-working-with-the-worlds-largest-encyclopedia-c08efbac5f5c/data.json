{"url": "https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c", "time": 1682993740.4433222, "path": "towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c/", "webpage": {"metadata": {"title": "Wikipedia Data Science: Working with the World\u2019s Largest Encyclopedia | by Will Koehrsen | Towards Data Science", "h1": "Wikipedia Data Science: Working with the World\u2019s Largest Encyclopedia", "description": "In this article, we\u2019ll see how to programmatically download and process the world\u2019s largest encyclopedia. The techniques developed here are broadly applicable to many data science endeavors."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Wikipedia", "anchor_text": "greatest source of online knowledge", "paragraph_index": 0}, {"url": "https://www.nature.com/articles/438900a", "anchor_text": "best place to get information for writing your college papers", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia", "anchor_text": "size of Wikipedia", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9", "anchor_text": "post using neural network embeddings to build a book recommendation system", "paragraph_index": 4}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb", "anchor_text": "available on GitHub", "paragraph_index": 5}, {"url": "http://shop.oreilly.com/product/0636920097471.do", "anchor_text": "Deep Learning Cookbook", "paragraph_index": 5}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "GitHub", "paragraph_index": 5}, {"url": "https://meta.wikimedia.org/wiki/Data_dumps", "anchor_text": "dump of all of Wikipedia", "paragraph_index": 6}, {"url": "https://dumps.wikimedia.org", "anchor_text": "dumps.wikimedia.org", "paragraph_index": 6}, {"url": "http://dumps.wikimedia.org/enwiki", "anchor_text": "dumps.wikimedia.org/enwiki", "paragraph_index": 7}, {"url": "https://www.w3schools.com/html/html_intro.asp", "anchor_text": "standard markup language", "paragraph_index": 8}, {"url": "https://dumps.wikimedia.org/enwiki/20180901/", "anchor_text": "https://dumps.wikimedia.org/enwiki/20180901/", "paragraph_index": 10}, {"url": "https://www.dataquest.io/blog/web-scraping-tutorial-python/", "anchor_text": "Here\u2019s a tutorial", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia", "anchor_text": "This page", "paragraph_index": 11}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/Download%20Only.ipynb", "anchor_text": "takes a little over 2 hours", "paragraph_index": 16}, {"url": "http://www.qnx.com/developers/docs/6.5.0SP1.update/com.qnx.doc.neutrino_utilities/b/bzcat.html", "anchor_text": "system utility", "paragraph_index": 18}, {"url": "https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit", "anchor_text": "Jupyter cell magic", "paragraph_index": 18}, {"url": "https://docs.python.org/3/library/xml.sax.html#module-xml.sax", "anchor_text": "SAX parser", "paragraph_index": 24}, {"url": "https://www.w3schools.com/xml/default.asp", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://www.mediawiki.org/wiki/Differences_between_Wikipedia,_Wikimedia,_MediaWiki,_and_wiki", "anchor_text": "MediaWiki", "paragraph_index": 33}, {"url": "https://www.mediawiki.org/wiki/Manual:What_is_MediaWiki%3F", "anchor_text": "standard format", "paragraph_index": 33}, {"url": "https://github.com/earwig/mwparserfromhell", "anchor_text": "library built to work", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/KENZ_(FM)", "anchor_text": "KENZ FM", "paragraph_index": 34}, {"url": "https://mwparserfromhell.readthedocs.io/en/latest/", "anchor_text": "useful methods", "paragraph_index": 35}, {"url": "https://www.mediawiki.org/wiki/Help:Templates", "anchor_text": "MediaWiki templates", "paragraph_index": 36}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Templates", "anchor_text": "Templates", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes", "anchor_text": "list of infoboxes", "paragraph_index": 39}, {"url": "https://timber.io/blog/multiprocessing-vs-multithreading-in-python-what-you-need-to-know/", "anchor_text": "source", "paragraph_index": 49}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb", "anchor_text": "the notebook", "paragraph_index": 51}, {"url": "https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b", "anchor_text": "this article", "paragraph_index": 52}, {"url": "https://medium.com/p/3db88aec33b7?source=your_stories_page---------------------------", "anchor_text": "this project", "paragraph_index": 52}, {"url": "http://book.pythontips.com/en/latest/map_filter.html", "anchor_text": "map", "paragraph_index": 58}, {"url": "https://github.com/DOsinga/deep_learning_cookbook/blob/master/04.2%20Build%20a%20recommender%20system%20based%20on%20outgoing%20Wikipedia%20links.ipynb", "anchor_text": "entity embeddings from neural networks", "paragraph_index": 65}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 67}, {"url": "https://willk.online", "anchor_text": "willk.online", "paragraph_index": 67}], "all_paragraphs": ["Wikipedia is one of modern humanity\u2019s most impressive creations. Who would have thought that in just a few years, anonymous contributors working for free could create the greatest source of online knowledge the world has ever seen? Not only is Wikipedia the best place to get information for writing your college papers, but it\u2019s also an extremely rich source of data that can fuel numerous data science projects from natural language processing to supervised machine learning.", "The size of Wikipedia makes it both the world\u2019s largest encyclopedia and slightly intimidating to work with. However, size is not an issue with the right tools, and in this article, we\u2019ll walk through how we can programmatically download and parse through all of the English language Wikipedia.", "Along the way, we\u2019ll cover a number of useful topics in data science:", "The original impetus for this project was to collect information on every single book on Wikipedia, but I soon realized the solutions involved were more broadly applicable. The techniques covered here and presented in the accompanying Jupyter Notebook will let you efficiently work with any articles on Wikipedia and can be extended to other sources of web data.", "If you\u2019d like to see more about utilizing the data in this article, I wrote a post using neural network embeddings to build a book recommendation system.", "The notebook containing the Python code for this article is available on GitHub. This project was inspired by the excellent Deep Learning Cookbook by Douwe Osinga and much of the code is adapted from the book. The book is well worth it and you can access the Jupyter Notebooks at no cost on GitHub.", "The first step in any data science project is accessing your data! While we could make individual requests to Wikipedia pages and scrape the results, we\u2019d quickly run into rate limits and unnecessarily tax Wikipedia\u2019s servers. Instead, we can access a dump of all of Wikipedia through Wikimedia at dumps.wikimedia.org. (A dump refers to a periodic snapshot of a database).", "The English version is at dumps.wikimedia.org/enwiki. We view the available versions of the database using the following code.", "This code makes use of the BeautifulSoup library for parsing HTML. Given that HTML is the standard markup language for web pages, this is an invaluable library for working with web data.", "For this project, we\u2019ll take the dump on September 1, 2018 (some of the dumps are incomplete so make sure to choose one with the data you need). To find all the available files in the dump, we use the following code:", "Again, we parse the webpage using BeautifulSoup to find the files. We could go to https://dumps.wikimedia.org/enwiki/20180901/ and look for the files to download manually, but that would be inefficient. Knowing how to parse HTML and interact with websites in a program is an extremely useful skill considering how much data is on the web. Learn a little web scraping and vast new data sources become accessible. (Here\u2019s a tutorial to get you started).", "The above code finds all of the files in the dump. This includes several options for download: the current version of only the articles, the articles along with the current discussion, or the articles along with all past edits and discussion. If we go with the latter option, we are looking at several terabytes of data! For this project, we\u2019ll stick to the most recent version of only the articles. This page is useful for determining which files to get given your needs.", "The current version of all the articles is available as a single file. However, if we get the single file, then when we parse it, we\u2019ll be stuck going through all the articles sequentially \u2014 one at a time \u2014 a very inefficient approach. A better option is to download partitioned files, each of which contains a subset of the articles. Then, as we\u2019ll see, we can parse through multiple files at a time through parallelization, speeding up the process significantly.", "When I\u2019m dealing with files, I would rather have many small files than one large file because then I can parallelize operations on the files.", "The partitioned files are available as bz2-compressed XML (eXtended Markup Language). Each partition is around 300\u2013400 MB in size with a total compressed size of 15.4 GB. We won\u2019t need to decompress the files, but if you choose to do so, the entire size is around 58 GB. This actually doesn\u2019t seem too large for all of human knowledge! (Okay, not all knowledge, but still).", "To actually download the files, the Keras utility get_file is extremely useful. This downloads a file at a link and saves it to disk.", "The files are saved in ~/.keras/datasets/, the default save location for Keras. Downloading all of the files one at a time takes a little over 2 hours. (You can try to download in parallel, but I ran into rate limits when I tried to make multiple requests at the same time.)", "It might seem like the first thing we want to do is decompress the files. However, it turns out we won\u2019t ever actually need to do this to access all the data in the articles! Instead, we can iteratively work with the files by decompressing and processing lines one at a time. Iterating through files is often the only option if we work with large datasets that do not fit in memory.", "To iterate through a bz2 compressed file we could use the bz2 library. In testing though, I found that a faster option (by a factor of 2) is to call the system utility bzcat with the subprocess Python module. This illustrates a critical point: often, there are multiple solutions to a problem and the only way to find what is most efficient is to benchmark the options. This can be as simple as using the %%timeit Jupyter cell magic to time the methods.", "For the complete details, see the notebook, but the basic format of iteratively decompressing a file is:", "If we simply read in the XML data and append it to a list, we get something that looks like this:", "This shows the XML from a single Wikipedia article. The files we have downloaded contain millions of lines like this, with thousands of articles in each file. If we really wanted to make things difficult, we could go through this using regular expressions and string matching to find each article. Given this is extraordinarily inefficient, we\u2019ll take a better approach using tools custom built for parsing both XML and Wikipedia-style articles.", "We need to parse the files on two levels:", "Fortunately, there are good options for both of these operations in Python.", "To solve the first problem of locating articles, we\u2019ll use the SAX parser, which is \u201cThe Simple API for XML.\u201d BeautifulSoup can also be used for parsing XML, but this requires loading the entire file into memory and building a Document Object Model (DOM). SAX, on the other hand, processes XML one line at a time, which fits our approach perfectly.", "The basic idea we need to execute is to search through the XML and extract the information between specific tags (If you need an introduction to XML, I\u2019d recommend starting here). For example, given the XML below:", "We want to select the content between the <title> and <text> tags. (The title is simply the Wikipedia page title and the text is the content of the article). SAX will let us do exactly this using a parser and a ContentHandler which controls how the information passed to the parser is handled. We pass the XML to the parser one line at a time and the Content Handler lets us extract the relevant information.", "This is a little difficult to follow without trying it out yourself, but the idea is that the Content handler looks for certain start tags, and when it finds one, it adds characters to a buffer until it encounters the same end tag. Then it saves the buffer content to a dictionary with the tag as the key. The result is that we get a dictionary where the keys are the tags and the values are the content between the tags. We can then send this dictionary to another function that will parse the values in the dictionary.", "The only part of SAX we need to write is the Content Handler. This is shown in its entirety below:", "In this code, we are looking for the tags title and text . Every time the parser encounters one of these, it will save characters to the buffer until it encounters the same end tag (identified by </tag>). At this point it will save the buffer contents to a dictionary \u2014 self._values . Articles are separated by <page> tags, so if the content handler encounters an ending </page> tag, then it should add the self._values to the list of articles, self._pages. If this is a little confusing, then perhaps seeing it in action will help.", "The code below shows how we use this to search through the XML file to find articles. For now we\u2019re just saving them to the handler._pages attribute, but later we\u2019ll send the articles to another function for parsing.", "If we inspect handler._pages , we\u2019ll see a list, each element of which is a tuple with the title and text of one article:", "At this point we have written code that can successfully identify articles within the XML. This gets us halfway through the process of parsing the files and the next step is to process the articles themselves to find specific pages and information. Once again, we\u2019ll turn to a tool purpose built for the task.", "Wikipedia runs on a software for building wikis known as MediaWiki. This means that articles follow a standard format that makes programmatically accessing the information within them simple. While the text of an article may look like just a string, it encodes far more information due to the formatting. To efficiently get at this information, we bring in the powerful mwparserfromhell , a library built to work with MediaWiki content.", "If we pass the text of a Wikipedia article to the mwparserfromhell , we get a Wikicode object which comes with many methods for sorting through the data. For example, the following code creates a wikicode object from an article (about KENZ FM) and retrieves the wikilinks() within the article. These are all of the links that point to other Wikipedia articles:", "There are a number of useful methods that can be applied to the wikicode such as finding comments or searching for a specific keyword. If you want to get a clean version of the article text, then call:", "Since my ultimate goal was to find all the articles about books, the question arises if there is a way to use this parser to identify articles in a certain category? Fortunately, the answer is yes, using MediaWiki templates.", "Templates are standard ways of recording information. There are numerous templates for everything on Wikipedia, but the most relevant for our purposes are Infoboxes . These are templates that encode summary information for an article. For instance, the infobox for War and Peace is:", "Each category of articles on Wikipedia, such as films, books, or radio stations, has its own type of infobox. In the case of books, the infobox template is helpfully named Infobox book. Just as helpful, the wiki object has a method called filter_templates() that allows us to extract a specific template from an article. Therefore, if we want to know whether an article is about a book, we can filter it for the book infobox. This is shown below:", "If there\u2019s a match, then we\u2019ve found a book! To find the Infobox template for the category of articles you are interested in, refer to the list of infoboxes.", "How do we combine the mwparserfromhell for parsing articles with the SAX parser we wrote? Well, we modify the endElement method in the Content Handler to send the dictionary of values containing the title and text of an article to a function that searches the article text for specified template. If the function finds an article we want, it extracts information from the article and then returns it to the handler. First, I\u2019ll show the updated endElement :", "Now, once the parser has hit the end of an article, we send the article on to the function process_article which is shown below:", "Although I\u2019m looking for books, this function can be used to search for any category of article on Wikipedia. Just replace the template with the template for the category (such as Infobox language to find languages) and it will only return the information from articles within the category.", "We can test this function and the new ContentHandler on one file.", "Let\u2019s take a look at the output for one book:", "For every single book on Wikipedia, we have the information from the Infobox as a dictionary, the internal wikilinks, the external links, and the timestamp of the most recent edit. (I\u2019m concentrating on these pieces of information to build a book recommendation system for my next project). You can modify the process_article function and WikiXmlHandler class to find whatever information and articles you need!", "If you look at the time to process just one file, 1055 seconds, and multiply that by 55, you get over 15 hours of processing time for all files! Granted, we could just run that overnight, but I\u2019d rather not waste the extra time if I don\u2019t have to. This brings us to our final technique we\u2019ll cover in this project: parallelization using multiprocessing and multithreading.", "Instead of parsing through the files one at a time, we want to process several of them at once (which is why we downloaded the partitions). We can do this using parallelization, either through multithreading or multiprocessing.", "Multithreading and multiprocessing are ways to carry out many tasks on a computer \u2014 or multiple computers \u2014 simultaneously. We many files on disk, each of which needs to be parsed in the same way. A naive approach would be to parse one file at a time, but that is not taking full advantage of our resources. Instead, we use either multithreading or multiprocessing to parse many files at the same time, significantly speeding up the entire process.", "Generally, multithreading works better (is faster) for input / output bound tasks, such as reading in files or making requests. Multiprocessing works better (is faster) for cpu-bound tasks (source). For the process of parsing articles, I wasn\u2019t sure which method would be optimal, so again I benchmarked both of them with different parameters.", "Learning how to set up tests and seek out different ways to solve a problem will get you far in a data science or any technical career.", "(The code for testing multithreading and multiprocessing appears at the end of the notebook). When I ran the tests, I found multiprocessing was almost 10 times faster indicating this process is probably CPU bound (limited).", "Learning multithreading / multiprocessing is essential for making your data science workflows more efficient. I\u2019d recommend this article to get started with the concepts. (We\u2019ll stick to the built-in multiprocessing library, but you can also using Dask for parallelization as in this project).", "After running a number of tests, I found the fastest way to process the files was using 16 processes, one for each core of my computer. This means we can process 16 files at a time instead of 1! I\u2019d encourage anyone to test out a few options for multiprocessing / multithreading and let me know the results! I\u2019m still not sure I did things in the best way, and I\u2019m always willing to learn.", "To run an operation in parallel, we need a service and a set of tasks . A service is just a function and tasks are in an iterable \u2014 such as a list \u2014 each of which we send to the function. For the purpose of parsing the XML files, each task is one file, and the function will take in the file, find all the books, and save them to disk. The pseudo-code for the function is below:", "The end result of running this function is a saved list of books from the file sent to the function. The files are saved as json, a machine readable format for writing nested information such as lists of lists and dictionaries. The tasks that we want to send to this function are all the compressed files.", "For each file, we want to send it to find_books to be parsed.", "The final code to search through every article on Wikipedia is below:", "We map each task to the service, the function that finds the books (map refers to applying a function to each item in an iterable). Running with 16 processes in parallel, we can search all of Wikipedia in under 3 hours! After running the code, the books from each file are saved on disk in separate json files.", "For practice writing parallelized code, we\u2019ll read the separate files in with multiple processes, this time using threads. The multiprocessing.dummy library provides a wrapper around the threading module. This time the service is read_data and the tasks are the saved files on disk:", "The multithreaded code works in the exact same way, mapping tasks in an iterable to function. Once we have the list of lists, we flatten it to a single list.", "Wikipedia has nearly 38,000 articles on books according to our count. The size of the final json file with all the book information is only about 55 MB meaning we searched through over 50 GB (uncompressed) of total files to find 55 MB worth of books! Given that we are only keeping a limited subset of the book information, that makes sense.", "We now have information on every single book on Wikipedia. You can use the same code to find articles for any category of your choosing, or modify the functions to search for different information. Using some fairly simple Python code, we are able to search through an incredible amount of information.", "In this article, we saw how to download and parse the entire English language version of Wikipedia. Having a ton of data is not useful unless we can make sense of it, and so we developed a set of methods for efficiently processing all of the articles for the information we need for our projects.", "Throughout this project, we covered a number of important topics:", "The skills developed in this project are well-suited to Wikipedia data but are also broadly applicable to any information from the web. I\u2019d encourage you to apply these methods for your own projects or try analyzing a different category of articles. There\u2019s plenty of information for everyone to do their own project! (I am working on making a book recommendation system with the Wikipedia articles using entity embeddings from neural networks.)", "Wikipedia is an incredible source of human-curated information, and we now know how to use this monumental achievement by accessing and processing it programmatically. I look forward to writing about and doing more Wikipedia Data Science. In the meantime, the techniques presented here are broadly applicable so get out there and find a problem to solve!", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or on my personal website at willk.online.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc08efbac5f5c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----c08efbac5f5c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc08efbac5f5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc08efbac5f5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/photo/library-university-books-students-12064/", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Wikipedia", "anchor_text": "greatest source of online knowledge"}, {"url": "https://www.nature.com/articles/438900a", "anchor_text": "best place to get information for writing your college papers"}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia", "anchor_text": "size of Wikipedia"}, {"url": "https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9", "anchor_text": "post using neural network embeddings to build a book recommendation system"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb", "anchor_text": "available on GitHub"}, {"url": "http://shop.oreilly.com/product/0636920097471.do", "anchor_text": "Deep Learning Cookbook"}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "GitHub"}, {"url": "https://meta.wikimedia.org/wiki/Data_dumps", "anchor_text": "dump of all of Wikipedia"}, {"url": "https://dumps.wikimedia.org", "anchor_text": "dumps.wikimedia.org"}, {"url": "http://dumps.wikimedia.org/enwiki", "anchor_text": "dumps.wikimedia.org/enwiki"}, {"url": "https://dumps.wikimedia.org/enwiki/'", "anchor_text": "https://dumps.wikimedia.org/enwiki/'"}, {"url": "https://www.w3schools.com/html/html_intro.asp", "anchor_text": "standard markup language"}, {"url": "https://dumps.wikimedia.org/enwiki/20180901/", "anchor_text": "https://dumps.wikimedia.org/enwiki/20180901/"}, {"url": "https://www.dataquest.io/blog/web-scraping-tutorial-python/", "anchor_text": "Here\u2019s a tutorial"}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia", "anchor_text": "This page"}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Modelling_Wikipedia%27s_growth#/media/File:Wikipedia_article_size_in_gigabytes.png", "anchor_text": "Source"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/Download%20Only.ipynb", "anchor_text": "takes a little over 2 hours"}, {"url": "http://www.qnx.com/developers/docs/6.5.0SP1.update/com.qnx.doc.neutrino_utilities/b/bzcat.html", "anchor_text": "system utility"}, {"url": "https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit", "anchor_text": "Jupyter cell magic"}, {"url": "https://docs.python.org/3/library/xml.sax.html#module-xml.sax", "anchor_text": "SAX parser"}, {"url": "https://www.w3schools.com/xml/default.asp", "anchor_text": "here"}, {"url": "https://www.mediawiki.org/wiki/Differences_between_Wikipedia,_Wikimedia,_MediaWiki,_and_wiki", "anchor_text": "MediaWiki"}, {"url": "https://www.mediawiki.org/wiki/Manual:What_is_MediaWiki%3F", "anchor_text": "standard format"}, {"url": "https://github.com/earwig/mwparserfromhell", "anchor_text": "library built to work"}, {"url": "https://en.wikipedia.org/wiki/KENZ_(FM)", "anchor_text": "KENZ FM"}, {"url": "https://mwparserfromhell.readthedocs.io/en/latest/", "anchor_text": "useful methods"}, {"url": "https://www.mediawiki.org/wiki/Help:Templates", "anchor_text": "MediaWiki templates"}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Templates", "anchor_text": "Templates"}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes", "anchor_text": "list of infoboxes"}, {"url": "https://timber.io/blog/multiprocessing-vs-multithreading-in-python-what-you-need-to-know/", "anchor_text": "source"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb", "anchor_text": "the notebook"}, {"url": "https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b", "anchor_text": "this article"}, {"url": "https://medium.com/p/3db88aec33b7?source=your_stories_page---------------------------", "anchor_text": "this project"}, {"url": "http://book.pythontips.com/en/latest/map_filter.html", "anchor_text": "map"}, {"url": "https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia", "anchor_text": "Source"}, {"url": "https://github.com/DOsinga/deep_learning_cookbook/blob/master/04.2%20Build%20a%20recommender%20system%20based%20on%20outgoing%20Wikipedia%20links.ipynb", "anchor_text": "entity embeddings from neural networks"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://willk.online", "anchor_text": "willk.online"}, {"url": "https://medium.com/tag/programming?source=post_page-----c08efbac5f5c---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c08efbac5f5c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/education?source=post_page-----c08efbac5f5c---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c08efbac5f5c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----c08efbac5f5c---------------python-----------------", "anchor_text": "Python"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc08efbac5f5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----c08efbac5f5c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc08efbac5f5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----c08efbac5f5c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc08efbac5f5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc08efbac5f5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c08efbac5f5c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c08efbac5f5c--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}