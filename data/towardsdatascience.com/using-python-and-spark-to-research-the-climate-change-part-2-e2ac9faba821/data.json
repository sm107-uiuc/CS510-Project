{"url": "https://towardsdatascience.com/using-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821", "time": 1683017622.965528, "path": "towardsdatascience.com/using-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821/", "webpage": {"metadata": {"title": "Using Python and Spark to research the Climate Change, Part 2 | by Kaya Kupferschmidt | Towards Data Science", "h1": "Using Python and Spark to research the Climate Change, Part 2", "description": "This is the second part of a small series dedicated to perform some analytics on publicly available weather data in order to find significant indicators for the climate change, specifically for the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/dimajix/weather-analysis", "anchor_text": "GitHub", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/using-python-and-spark-to-research-the-climate-change-part-1-7616b71cefe8", "anchor_text": "The first part of this series", "paragraph_index": 5}, {"url": "https://www.noaa.gov/", "anchor_text": "National Oceanic and Atmospheric Administration", "paragraph_index": 5}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/using-python-and-spark-to-research-the-climate-change-part-1-7616b71cefe8", "anchor_text": "the first part of the series", "paragraph_index": 9}, {"url": "https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf", "anchor_text": "the official documentation", "paragraph_index": 14}], "all_paragraphs": ["This is the second part of a small series dedicated to perform some analytics on publicly available weather data in order to find significant indicators for the climate change, specifically for the global warming.", "I am by no means an expert for meteorology nor for climate models, but I have lots of experience in working with data \u2014many of us are in a similar boat since there are probably many more mathematicians, data scientists and data engineers than meteorologists. Performing an analysis based on trustworthy and publicly available data is thrilling precisely in this specific situation, such that anyone interested with some development skills can follow all steps and create their own insights, simply by applying one's knowledge in working with data combined with common sense.", "Specifically, this capability of a broad audience to reproduce scientific insights via commonly available methodologies becomes more and more important in this world, where important decisions are increasingly based on data and mathematical models while at the same time social media simplifies to spread false information. Along with that idea, this article series is my take for researching the climate change.", "Many details of processing steps are omitted in this article to keep focus on the general approach. You can find a Jupyter notebook containing the complete working code on GitHub.", "The whole journey from downloading some data until making some pictures is split up into three separate stories, since each step already contains lots of information.", "The first part of this series focused on getting some raw weather data from the National Oceanic and Atmospheric Administration (NOAA) and on transforming the data into a more convenient file format. This is one of many possible sources, but I decided to use it for several reasons:", "Since we are talking about 100GB of compressed raw data, all processing so far has been performed using PySpark, the Python binding for Apache Spark, which in turn is a very popular framework in the world of Big Data to build typical data processing pipelines.", "This time we will be focusing on making sense of the data \u2014 not so much in a technical sense (since this was addresses in the previous part), but more in a semantic sense. As we will see, weather data (and probably sensor data in general) has some very unique challenges, which are (hopefully) not to be seen within for example financial data.", "So what will we learn this time?", "I assume that you already followed the first part of the series, since we will be building upon the resulting Parquet files. Again, you need some free space left on your computer, since we will create new derived data sets. Those data sets will be smaller in size, but they will still occupy a couple of gigabytes.", "Let\u2019s pick up where we stopped last time. You should now have a set of directories (one per year from 1901 until 2020) filled with Parquet files, which have been created from the original raw weather data.", "To warm up again, let\u2019s start by reading in the data from last time and inspect its schema:", "The data contains all measurements of all weather stations from all years. Let me explain again, what all these columns actually mean:", "Remember that this is only a small subset of all the information contained in the original files, but these columns are sufficient for our purpose.", "We will discuss most of these fields in more detail later, and I invite you again to study the official documentation of the original raw data format which provides many details on the meaning of these fields. But I\u2019d already like to mention one important aspect, which might be somewhat unique to sensor data: In addition to the metrics themselves the data also contains quality indicators for each measurements. These indicators tell us if each metric of each measurement is valid or not. There are multiple different scenarios which result in partially invalid measurements:", "Of course we can also peek inside the data:", "Here we already see that the quality indicators are really important, since fortunately a wind speed of 999.9 meters per second still seems very unrealistic \u2014 even with the climate change in mind.", "Just out of curiosity, let\u2019s count the total number of records:", "As you will see, we almost have 3,5 billion records \u2014 that is really a non-trivial amount of data for a single machine, but still manageable with the right tools.", "As I explained above, the data contains many invalid measurements which we\u2019d like to ignore. You might wonder why these records are present in the original data in the first place. There are many good reasons for their presence:", "As I said, hopefully the financial systems within the bank of your trust doesn\u2019t produce data which needs to be flagged as \u201cerroneous\u201d or \u201csuspect\u201d.", "We will be mainly concerned about the air temperature, which (according to the official format documentation) can have the following quality codes:", "Well that are actually many more cases than I really understand, but this shows how much care has been taken for classifying each measurement. You will find similar descriptions for other metrics like wind speed, precipitation and so on \u2014 each metric has its own logic for determining its quality.", "Now we still need a strategy how to handle the invalid values. Since dropping full records (i.e. rows in a table) would also remove valid values in different columns, we will use a different approach. Instead we will simply replace all invalid or suspect values with NULL values. This has two advantages over dropping records:", "This replacement can be done with PySpark as follows (some columns are omitted in the example below):", "Note that we keep all wind speeds with a quality code of 1 or 5 and all air temperatures with a quality code of 1,5 or R.", "In the next step, we now aggregate all measurements to hourly values per weather station and per date/hour. You might wonder why this could make sense given that the data set is an hourly data set. The reason is that the data set actually might contain multiple measurements per weather station per hour, for example when the different sensors (for wind and temperature) are recorded in slightly different time intervals. This can be seen when closely inspecting the timestamp column ts of individual weather stations, which now also explains why they are recorded at a precision of seconds instead of precision of one hour.", "By performing an hourly aggregation, all these records within the same hour will be merged together to a single record per weather station and per hour.", "We always use the AVG aggregation function for temperature and wind speed in order to obtain the average values within a specific hour. If we were to collect precipitation, we probably would use a SUM function in order to collect the total amount of rainfall within a specific hour.", "Remember that all aggregation functions ignore any NULL values, which we used to mask suspect or invalid data.", "In a second step, we now create daily preaggregate of the data. We will also store the result as Parquet files again. While the first step guaranteed that we will have precisely a single record per hour per weather station, this step will now create daily summaries out of the hourly records.", "Note that we not only collect average values using AVG but also minimum and maximum values.", "Again this step might take a while (possibly a couple of hours), depending on the CPU power of your system.", "In order to use the preaggragted data set (which will speed up further processing significantly), we read the data back into a PySpark DataFrame.", "A simple daily_weather.count() should tell us that the daily aggregates still contain about 200 million records. But since the data is stored in Parquet files, the pain will be much less while working with the data than with the original 3,5 billion records.", "Now we have a nice data set containing daily aggregates for each weather station. In this section, we try to derive a new data set containing aggregated weather information per day and per country instead of per day and per weather station.", "Actually, if we wanted to do this correctly, this would be much more difficult than what I will be doing here. Let me try to explain some problems that normally need to be addressed:", "In order to address these issues, probably a global model would be required that describes the whole weather at uniform distances over the whole globe. This approach by far exceeds my skills and knowledge and is therefore out of scope for this article. We will have to follow a much simplified approach.", "The measurements themselves only contain two columns identifying the weather station, but no information on the country is provided. But if you remember from the first part of this series, NOAA also provides a CSV file with the master data of all weather stations containing information like each station\u2019s geo location, its lifespan and the country where it is located in.", "We therefore now need to join the aggregated daily weather measurements with the stations master data, which contains the relevant information. The join can be easily performed by PySpark and we now use both ID columns usaf and wban , which only in their combination uniquely identify each weather station.", "I reiterate that a country level aggregation of weather data (and probably of many other geo data sets) is not the most appropriate way to go. But for the sake of simplicity, we ignore all these concerns and we now aggregate all the data onto country and date. We will see later, how good or bad this approach is.", "This now really looks like a neat schema for our purpose, therefore we save the transformed data for further investigations, which we will discuss in the next (and final) part of this article series.", "Since the source data set provided in daily_country_weather is now much smaller, this step should be much faster than all the previous transformations.", "Finally, let\u2019s inspect the resulting data set by picking some country:", "In this second part of the series, we achieved two important goals: We cleaned up the data by ignoring invalid values to simplify downstream analytics and we created a semantically compressed aggregate to speed up downstream analytics. These two steps are also very common in many data science projects and are most often performed by a team of data scientists (who should understand the semantics of the data) and data engineers (who know the best tools to perform efficient processing).", "Now that we have a compact aggregate containing some minimal (but important) weather metrics per country and per year, we will be digging deeper and researching the change of the temperature over time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Freelance Big Data and Machine Learning expert at dimajix."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe2ac9faba821&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@kupferk", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kupferk.medium.com/?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": ""}, {"url": "https://kupferk.medium.com/?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "Kaya Kupferschmidt"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1b1c406b9d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=post_page-a1b1c406b9d0----e2ac9faba821---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2ac9faba821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2ac9faba821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@noaa?utm_source=medium&utm_medium=referral", "anchor_text": "NOAA"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/dimajix/weather-analysis", "anchor_text": "GitHub"}, {"url": "https://towardsdatascience.com/using-python-and-spark-to-research-the-climate-change-part-1-7616b71cefe8", "anchor_text": "Getting the data"}, {"url": "https://towardsdatascience.com/using-python-and-spark-to-research-the-climate-change-part-1-7616b71cefe8", "anchor_text": "The first part of this series"}, {"url": "https://www.noaa.gov/", "anchor_text": "National Oceanic and Atmospheric Administration"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://towardsdatascience.com/using-python-and-spark-to-research-the-climate-change-part-1-7616b71cefe8", "anchor_text": "the first part of the series"}, {"url": "https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf", "anchor_text": "the official documentation"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----e2ac9faba821---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/climate-change?source=post_page-----e2ac9faba821---------------climate_change-----------------", "anchor_text": "Climate Change"}, {"url": "https://medium.com/tag/big-data?source=post_page-----e2ac9faba821---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e2ac9faba821---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/weather?source=post_page-----e2ac9faba821---------------weather-----------------", "anchor_text": "Weather"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2ac9faba821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=-----e2ac9faba821---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2ac9faba821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=-----e2ac9faba821---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2ac9faba821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe2ac9faba821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e2ac9faba821---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e2ac9faba821--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e2ac9faba821--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e2ac9faba821--------------------------------", "anchor_text": ""}, {"url": "https://kupferk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kupferk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kaya Kupferschmidt"}, {"url": "https://kupferk.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "223 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa1b1c406b9d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=post_page-a1b1c406b9d0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F874fa4e516b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-python-and-spark-to-research-the-climate-change-part-2-e2ac9faba821&newsletterV3=a1b1c406b9d0&newsletterV3Id=874fa4e516b6&user=Kaya+Kupferschmidt&userId=a1b1c406b9d0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}