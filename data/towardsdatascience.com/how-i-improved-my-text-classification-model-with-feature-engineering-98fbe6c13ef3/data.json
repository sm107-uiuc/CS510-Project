{"url": "https://towardsdatascience.com/how-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3", "time": 1683000184.427459, "path": "towardsdatascience.com/how-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3/", "webpage": {"metadata": {"title": "How I improved my text classification model with feature engineering | by Alexandre Wrg | Towards Data Science", "h1": "How I improved my text classification model with feature engineering", "description": "In this article I will show you how to conduct a textual analysis to improve the accuracy of your model and discover several facts in your data. For that I will use a data set available on Kaggle\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TFIDF", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic Regression", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "Bag of Word", "paragraph_index": 25}], "all_paragraphs": ["In this article I will show you how to conduct a textual analysis to improve the accuracy of your model and discover several facts in your data. For that I will use a data set available on Kaggle. This dataset is composed of comments in from \u2018Wikipedia\u2019s talk page edits\u2019, it is provided with the identifiers of the users who published the comment, but we have no information on the commented page or on the users\u2019s personal data. Nevertheless, we have 6 differents labels (Toxic, Severe_Toxic, Identity hate, obscene, threat, insult) that have been handwritten.", "To carry out our analysis we will separate our corpus according to the labels. We will briefly look at the distribution of the different labels through the database and then at these sub-corpuses themselves, which correspond to the tags listed above up.", "It can be seen that the corpus of toxic comments is the largest and about 10% of the comments in the training corpus have this label, while the threatening corpus of comments is smaller. However, this does not mean that 25% of all comments are ill-intentioned, as one comment can have several labels. We can see that that just under 5% of comments have only one tag and that about 7% of malicious comments have at least 2 tags.", "In order to create relevant indicators to improve our model, we will focus on the intrinsic characteristics of our corpus, guided by our preconceptions regarding the hypothetical lexical and grammatical distribution of the different corpuses. We will ask ourselves if there are recurrent patterns between corpuses that could be used as signage to enrich our model. We will define some statistical feature that belonging to each comments.", "Before proceeding to the analysis of these textual statistics, I assumed that producing them would be interesting to compare the data with my preconceptions. That is, I assume (from experience) that a comment that is intended to be malicious will have a higher proportion of capitalized words, a less developed vocabulary and a higher number of exclamation points. With these ideas in mind, I calculated the median of some of my variables and put them on the same scale, I did not take all the variables for the sake of readability of the graph, so I kept the variables that interested me the most. I created a first radar chart with variables of interest in order to obtain a relevant signage between my corpora. The shape here is more important than the values that have been standardized (i.e., the variables have been transformed to center their means on 0 and their variances on 1) in order to obtain a consistent graph between the variables.", "Thus, on this graph we clearly see that some of my preconceptions are true. The ratio of capitalized words to sentence length is obvious compared to a distribution of comments without bad intentions. We also note that the comments without bad intentions have a higher number of unique words, more sentences and longer sentences. This confirms that the ill-intentioned comments probably present a less rich vocabulary and we will try to confirm this hypothesis later on. On the other hand, most of the comments poorlyhas a similar distribution, except for the severely toxic corpus, which is a case of this lexical poverty.", "Moreover, the real value figures reveal that on average the number of exclamation points is ten times higher in a malicious comment than in a compliant comment. There is also a less developed punctuation. The length of sentences is 20% shorter in non-compliant corpora and the number of words in capital letters is about 2.5 times higher for 43 capital letters on average compared to 14 for a compliant comment is related to the average sentence length, on average 11% of the words in a non-canonical comment are in capital letters while on average only 4% of the words in a compliant comment are. So is lexical poverty based on the number of \u201cunique\u201d words, that is, different from the others is 1.5 times lower in this type of corpus for an average of 33 unique words against about 50 in a compliant corpus. We can ask ourselves a little more profound questions on this lexical poverty, let\u2019s look at the comments with a lower unique percentage of words \u00e0 30%.", "These are mostly toxic comments, if we look at the details, we see that thesecomments are very often \u201cspam\u201d. This information is interesting for our model we will define a spam indicator.", "In a secon part, we will use a pos-tagger (an algorithm to locate the grammatical class belonging to each word in our corpus) on our textual data in order to extract information in the same way as before. I suppose that, malicious comments surely present a lexical and/or grammatical construction different from that of a conforming corpus. I used a corpus composed of comments that do not conform to any label and a corpus composed only of comments that does not present any problem.", "Firstly, let\u2019s separate our corpuses and look at the algorithmic part of our pos-tagging algorithm .", "Thanks to nltk library we don\u2019t need to write everything from scratch, we will tokenize our corpus.", "Then we will apply our pos-tagging algorithm.", "We can see that our algorithm does three things it tokenized, it lemmatized (given a transformed verb it return the verbal root) and he gave us the verbal/grammatical reference to the tokens. Then given that pos tagged corpus we can highlight the verbal/grammatical distribution of the corpus. We will count each tag and apply a weight to them.", "To my astonishment, we notice that the two corpora are quite similar, they diversify on the rate of use of the past participle and determinants in the regular corpus and generally the grammatical richness of the regular corpus which is higher. The only point where this differs very strongly is the use of the personal pronoun in construction grammatical of the sentences in the corpus. Which only confirms one of my assumptions and invalidates the others.", "Therefore, we will focus in particular on these pronouns in which cases they will characterize a corpus of malicious comments. We will now move to a more micro-level analysis of our data from the pos-tagging. Based on this graph, I think it would be interesting to look at the most frequent words among the subtags: Verbs, Names, Personal Names and Foreign words.", "Let\u2019s create a large dictionary that will contain several variables of interest for our study.", "Then we will define a function to find the 25 most common words across those verbal form and plot it.", "The micro analysis of foreign words gives us a glimpse of more points of interest, we see several things, words associated with corpora that it would be interesting to integrate as variables such as \u201cputa, Nazi, mother, fukkin\u201d. There are also symbols that appear as recurring words, indicating that the textual statistics of the number of symbols created above will surely have an impact on the future algorithm.", "Among the verbs with the highest frequency of appearance among all corpora, only kill stands out with suck otherwise the verbal occurrences are essentially the same across corpora which does not give us very interesting information at to be used here, but still allows to confirm that my preconception on the verbal difference between the corpora is wrong.", "We will deduplicate this tag analysis to find relevant variables that will fit into the final model. We will go directly to the analysis of bigrams, trigrams.", "To do this we will build a word matrix using the TFIDFmethod by restricting the number of ngram and leading to aLogistic Regression to look at the importance of the variables.", "Higher is the score more important is the feature (I sorted value by toxic that mean the first word is the higher that belong to the label sorted)", "For example, the presence of the word \u201cfuck\u201d in a commentary will increase the probability of belonging to the toxic label by 52%, and the \u201cYou\u201d has a strong impact on the entire corpus is that it is the main trigger as a unigram for the threatening label.", "In bigram analysis we always find the word \u201cfuck\u201d, what is interesting is that we find our personal pronouns that are distinguished during the tag analysis. We can therefore see that they are generally insults composed as \u201cfuck off\u201d or distinguish from a \u201cfuture\u201d personal injury \u201cyou\u2019re\u201d. Nevertheless, we notice that bigrames are already less important in the model.", "By developing towards trigrams we notice the preponderance of you and that mainly the important words in trigrams are the continuation of unigrams and bigrames. It is interesting to note that the recurring words found in the term frequency are not necessarily the ones that have the greatest influence on the probability of impacting a corpus, probably due to a lower rate of appearance in the document.", "The whole of our analysis will allow us to create new training variables that I think will have a significant impact on our model. So we will first model our data using just the text as a variable, we will create in the jargon of textual analysis a Bag of Word and used as a training basis for our classifier. This matrix is constructed in such a way that the words become columns, the comments id are the rows and each row is composed of 0 or 1 which are indicators of the presence of the word, so we are facing a sparse matrix. Then we will use our knowledge of the text to improve the understanding of our classifier and see if this has a real impact on our model.", "and used as a training basis for our classifier. This matrix is constructed in such a way that the words become columns, the comments id are the rows and each row is composed of 0 or 1 which are indicators of the presence of the word, so we are facing a sparse matrix. Then we will use our knowledge of the text to improve the understanding of our classifier and see if this has a real impact on our model.", "Let\u2019s define our feature discovered during our analysis, the first one was the statiscal feature computed on the top of this article, then we computed some pos tagged feature and finaly some feature through uni|bi|tri -grams analysis.", "Then we will create a matrix without feature engineering and another with", "We therefore applied our algorithm to our two matrices and calculated the model\u2019s accuracy rate. Firstly, the model without feature engineering.", "Without feature engineering the results are as follows :", "It can be seen that the model with the study variables performs much better. We can see that the algorithm goes up to more than 9 points on the prediction of the toxic label which is not negligible. This means that out of 9.5% of the comments labelled toxic on the entire corpus, i.e. about 15,000 comments, predicting 89.3% of the labels without making mistakes means that about 13,500 comments will be well predicted compared to just under 12,000 comments in an algorithm applied without textual analysis, i.e. 1500 additional toxic comments to avoid. This new algorithm performs on all labels and improves the detection of certain labels that were already very good, such as the hate corpus, which still gains a point and approaches a perfect prediction.", "We will now look more precisely at the impact of our study, by inspecting the coefficients of our logistic regression we can determine the variables having the most impact on our model from strongest to weakest. Below, this table summarizes, the power of each variable (words) influencing the passage from a comment from 0 to 1 for a given label. That is, the presence of these words in a sentence will have a cumulative triggering effect on the labelling performed by the algorithm.", "Then with feature engineering the features importances becomes :", "To conclude, we can say that in-depth analysis of textual data represents a significant challenge in model learning and brings a new dimension to our data set. On the one hand, it allows you to discover more of them on our data and to affirm or deny preconceptions. Another advantage that can be is the possibility of generating these indicators from the textual analysis on training and validation data and on the other hand, this study has proven us that the indicators identified were relevant for modelling purposes.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist at Auchan Retail Data"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F98fbe6c13ef3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alexandrewrg?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexandrewrg?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "Alexandre Wrg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5f80e627a47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&user=Alexandre+Wrg&userId=a5f80e627a47&source=post_page-a5f80e627a47----98fbe6c13ef3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98fbe6c13ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98fbe6c13ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@firmbee?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "William Iven"}, {"url": "https://unsplash.com/s/photos/data-analysis?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TFIDF"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic Regression"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "Bag of Word"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----98fbe6c13ef3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----98fbe6c13ef3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nltk?source=post_page-----98fbe6c13ef3---------------nltk-----------------", "anchor_text": "Nltk"}, {"url": "https://medium.com/tag/text-mining?source=post_page-----98fbe6c13ef3---------------text_mining-----------------", "anchor_text": "Text Mining"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----98fbe6c13ef3---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98fbe6c13ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&user=Alexandre+Wrg&userId=a5f80e627a47&source=-----98fbe6c13ef3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98fbe6c13ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&user=Alexandre+Wrg&userId=a5f80e627a47&source=-----98fbe6c13ef3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98fbe6c13ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F98fbe6c13ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----98fbe6c13ef3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----98fbe6c13ef3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexandrewrg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexandrewrg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alexandre Wrg"}, {"url": "https://medium.com/@alexandrewrg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "354 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5f80e627a47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&user=Alexandre+Wrg&userId=a5f80e627a47&source=post_page-a5f80e627a47--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7c3751d086ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3&newsletterV3=a5f80e627a47&newsletterV3Id=7c3751d086ca&user=Alexandre+Wrg&userId=a5f80e627a47&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}