{"url": "https://towardsdatascience.com/mechanics-of-the-roc-curve-83b10ce3887f", "time": 1683007573.596718, "path": "towardsdatascience.com/mechanics-of-the-roc-curve-83b10ce3887f/", "webpage": {"metadata": {"title": "Mechanics of the ROC Curve. An intuitive dashboard to study how the\u2026 | by Sahil Gupta | Towards Data Science", "h1": "Mechanics of the ROC Curve", "description": "Predictive models can be divided into two categories based on the task at hand, i.e. classifiers and regressors. The regression models aim at predicting continuous outcomes, and the classifiers look\u2026"}, "outgoing_paragraph_urls": [{"url": "https://stats.stackexchange.com/questions/267078/why-is-skewed-data-not-preferred-for-modelling", "anchor_text": "skewed distributions", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Huber_loss", "anchor_text": "Huber regression", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Random_forest", "anchor_text": "decision trees", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Generalized_linear_model", "anchor_text": "eneralized linear models", "paragraph_index": 1}, {"url": "https://www.sciencedirect.com/science/article/abs/pii/S016786550500303X", "anchor_text": "this", "paragraph_index": 2}, {"url": "https://www.kaggle.com/uciml/pima-indians-diabetes-database", "anchor_text": "PIMA Diabetes dataset", "paragraph_index": 3}, {"url": "https://medium.com/@sahilgupta_86549/pima-diabetes-dataset-77ee2aa67ce7", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression#Examples", "anchor_text": "logistic regression model", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Logit", "anchor_text": "log-odds", "paragraph_index": 17}, {"url": "https://github.com/sahilgupta2105/Medium-Articles/blob/master/MechanicsOfTheRocCurve.ipynb", "anchor_text": "here", "paragraph_index": 18}], "all_paragraphs": ["Predictive models can be divided into two categories based on the task at hand, i.e. classifiers and regressors. The regression models aim at predicting continuous outcomes, and the classifiers look to predict discrete outcomes.", "In the regression model, if the distribution of outcomes is skewed, it can throw off some models. For instance, using linear regression to predict skewed distributions can reduce the performance of the model. In the context of linear regression, this happens because it violates our assumption about Normally distributed noise (you can also think in terms of the underlying loss function which is influenced by some \u201coutlier\u201d outcomes and impedes the learning process; to counter this Huber regression can be helpful as the underlying loss is not influenced as much by skewness.). One way to deal with such a phenomenon can be to use a different set of models such as decision trees, generalized linear models, etc. In essence, the outcome distribution plays an important role in data modeling.", "Similar to the continuous outcome case, discrete outcomes (classification) have a similar kind of problem in the form of imbalanced outcome counts. For example, let\u2019s suppose we trying to model a dataset where the outcomes are in the form of 1/0s. For instance,1 could indicate credit card fraud. Most of the people don\u2019t commit fraud, so in such a case the counts would be skewed in the direction of 0s. In this paper, they introduce ROC curves, which as a tool, are used to compare the relative performance of different classifiers. This tool is exactly motivated by the problem of dealing with imbalanced datasets. The paper is an interesting read to understand the concept of ROC curves in-depth.", "In this article, I will try to further deconstruct the idea of the ROC curves, and hopefully give you an intuition around how these things work. I will be using the PIMA Diabetes dataset to study the ROC curve. I will start by describing the problem, show why using a simple accuracy measure is a bad choice, and finally, introduce a dashboard to look at a ROC curve in action.", "The dataset contains the following predictors: No. of times Pregnant, Plasma Glucose Conc., Blood Pressure, Skin Fold Thickness, Serum Insulin, BMI, Diabetes Pedigree Function Values, Age. These are diagnostic measurements and the aim is to build a predictive model to classify whether a patient has (1) diabetes, or not (0). In the data collection process, several constraints were placed. For instance, all patients in the dataset are females at least 21 years old. Due to such constraints, and based on the variety of predictors (in terms of their physical meaning), we might need some pre-processing before utilizing this dataset with certain models (eg. linear regression). Also, the dataset has some missing values as well. To keep this article concise and to avoid digressing from the main topic of discussion, I am not talking about the data pre-processing steps. You can find more information here.", "Assuming, we conducted the appropriate pre-processing, let\u2019s look at the counts of the outcome variables. We can observe in Figure 1 that the counts are skewed towards label 0, i.e. the number of patients who don\u2019t have diabetes is much more compared to people who have diabetes.", "The accuracy of a classifier is defined as the number of correct classifications out of all the classification performed by it. In technical terms, whenever a classified makes a classification, it can land in either of the four boxes in Figure 2. For example, this is how we would read the table for the cell \u201cFP\u201d: for a particular classification, we saw a label of 0, but the classifier predicts it as 1, i.e. a person who doesn't have diabetes is predicted to have diabetes by the predictive model, which explains why it is called a False Positive (FP). Accuracy is defined as,", "It\u2019s very easy to build a classifier that would be fairly accurate but completely useless. For example, we can build a random classifier using the class frequencies from our dataset. The proportion of True Negative (Not Diabetes) is 500 out of 768. At prediction time, this random classifier predicts a person as not having diabetes with a probability of 500/768, or 0.651. So, the accuracy of this classifier is 65.1%! But, we know this is completely useless in real-world applications. Clearly, there is something missing in our evaluation of a predictive model in terms of accuracy. Intuitively, accuracy is focusing on the actual truth values the model is able to capture (i.e. TP and TN). However, in Figure 1, we saw that counts are skewed. So, it would make sense to also account for the false predictions made by the model (i.e. FP and FN), because the real-life cost of missing a rarer event can be much more (which probably goes further up as the skewness in counts increase).", "Until this point, we have convinced ourselves that the usual way of evaluating predictive classification models (i.e. based on accuracy) doesn\u2019t seem to work with imbalanced datasets. The ROC curve is one tool to help us with this.", "ROC curve, as the name suggests, is a graph between True Positive Rate (TPR) and False Positive Rate (FPR). These are defined as,", "Let\u2019s try to understand these terms and then see why the ROC curve might be a good choice. Let\u2019s start with the denominators. Notice that the denominators in TPR and FPR are actual positives and actual negatives, respectively (positive refers to label 1 and negative to label 0, similar to Figure 2). Based on Figure 2 (above), for instance, we know that a diabetic person can be predicted as diabetic (TP) or not diabetic (FN) by the model. So, TPR indicates the proportion of true positives (predicted by the model) among all the positive labels. Similarly, FPR indicates the proportion of false positives (predicted by the model) among all the negative labels. The crucial thing to note here is that these two quantities are looking at the predictions of a classifier at a more granular level. We can also interpret the complement of both these quantities in a similar fashion. The complement of TPR would be FNR (false-negative rate), and for FPR it would be TNR (true negative rate). TPR is also called Sensitivity, and 1-FPR is called Selectivity. So, we can also say that the ROC curve is a graph of Sensitivity v/s 1-Selectivity.", "As a side note, Sensitivity (TPR) and Selectivity (1-FPR) can also be interpreted as conditional probabilities. For instance, Sensitivity is the probability of classifying a data point (with a positive label in the PIMA dataset) as positive (notice that denominator, TP+FN, are the data points with true labels as 1, and TP are the actual positives in the data set). In mathematical terms, this is how sensitivity and selectivity are defined,", "This is exactly a conditional probability description. Here, s\u2092 is a threshold, which is used as follows: if the score of our classifier is above the threshold (s\u2092), then it is classified as a positive label (1), else negative label (0). The terms sensitivity and specificity are widely used in medicine and thinking about the definitions in terms of diseases make them a bit more intuitive: with the PIMA diabetes dataset, our aim is to build classification models that can identify diabetic vs non-diabetic patients. So, sensitivity defines the ability of our model (/test) to correctly identify diabetic patients. Similarly, specificity defines how specific our model (/test) is in terms of detecting non-diabetic patients (usually, in case of diseases, the number of patients who don\u2019t have the disease is much more vs the patients who do).", "Now, let\u2019s observe the plane where the ROC curve lives and gain some intuition. The dotted line, where TPR = FPR, represents a random classifier.", "The reason is that a random classifier based on prior (from train set) class frequencies would on average get similar conditional frequencies, and hence equal TPR and FPR. Let\u2019s suppose the prior class proportions are \u03b1 and 1- \u03b1 for positive and negative labels, respectively. A random classifier based on this would on average predict \u03b1 fraction of samples as a diabetic. This also means that in a sample of all diabetic patients, it would on average predict \u03b1 fraction of patients as diabetic, which says that the sensitivity is \u03b1. Using a similar argument, we can say that the selectivity is 1-\u03b1, which implies that FPR is \u03b1. Hence, the line TPR=FPR indeed represents a random classifier.", "We can gain some further intuition by looking at the extreme points in the curve. The lower left green point (FPR=0 and TPR=0) represents a classifier that has 0 sensitivity and 1 selectivity, i.e. it is extremely good at identifying the non-diabetic patients and extremely bad at identifying diabetic patients. On the other hand, the top right corner (FPR=1 and TPR=1) represents a classifier really good at identifying diabetic patients, but bad at identifying non-diabetic patients. In other words, the classifier is super-sensitive at the cost of being non-selective. Finally, the top left corner (FPR=0 and TPR=1) represents a classifier which is ideal: it has perfect sensitivity (=1) and perfect selectivity (=1).", "In a similar way, we can compare the two classifiers marked as red crosses. The left one has better selectivity and worse sensitivity compared to the right one. In general, our aim would be to build a classifier that is as close to the top left corner as possible.", "Now, we are in a position to look at an actual ROC curve. Just to clear up any confusion about the score thing I mentioned in the definition of sensitivity and selectivity, these can be thought of values before we assign a label to the prediction. For example, in a logistic regression model, it\u2019s the value of the log-odds for the data point. Commonly, an implicit threshold of 0.5 (for probability) is used (in our heads, or on paper). In the context of log-odds, this basically means that we are selecting the more probable event (remember, log-odds is the log ratio of odds of an event). Like we saw earlier, sometimes this threshold of 0.5 is not a good idea (accuracy), and we need a better threshold. The ROC Curve helps us with exactly that. Without further ado, here\u2019s the dashboard you can play with and make these things a bit more intuitive.", "You can find the Jupyter notebook for this dashboard here. The dashboard pops up once you run all the cells in the notebook. The dashboard uses Plotly and Python widgets, which makes it a bit tricky to embed it directly on medium.", "Before leaving you with the dashboard, let me say a few words about how to interpret the dashboard. At the top, you can find two threshold sliders that control the thresholds (as defined in the sensitivity and selectivity definition) for Logistic Regression and Random Forest. The top-left plot is the logistic distribution of scores generated using the logistic regression model. I have included this in the dashboard to make things explicit. However, in the case of Random Forest, the functional form of distribution is not known, which is why the distribution in the case of random forest is omitted. Next, to the right of that, we have a plot of ROC curves for both models and markers which indicate the current values for the thresholds. As the sliders are moved, the markers travel on there respective ROC curves.", "Further below, we have the conditional probability distribution for both models, i.e. conditional on knowing the predicted labels by the classifier. For instance, probability conditional on Pred=1 defines the probability that a person is diabetic (TP) or not (FP), given that the classifier predicted a person as a diabetic. In other words, the two plots at the bottom give us a complementary view of what happens when the threshold is varied. Notice that sensitivity (and selectivity) are conditional probabilities conditioned on knowing the true label of the data point is C=1 (or C=0).", "An interesting thing that you can notice here is that for a particular model (and hyperparameter values), the ROC curve is fixed, and changing the threshold is helping us tune the model further to align better with the business objective at hand (think sensitivity and selectivity). Usually, the threshold values are selected using cross-validation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F83b10ce3887f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sahilgupta_86549?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahilgupta_86549?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "Sahil Gupta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fafb6a5fd07b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&user=Sahil+Gupta&userId=afb6a5fd07b4&source=post_page-afb6a5fd07b4----83b10ce3887f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b10ce3887f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b10ce3887f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@sweetmangostudios?utm_source=medium&utm_medium=referral", "anchor_text": "Ricky Kharawala"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://stats.stackexchange.com/questions/267078/why-is-skewed-data-not-preferred-for-modelling", "anchor_text": "skewed distributions"}, {"url": "https://en.wikipedia.org/wiki/Huber_loss", "anchor_text": "Huber regression"}, {"url": "https://en.wikipedia.org/wiki/Random_forest", "anchor_text": "decision trees"}, {"url": "https://en.wikipedia.org/wiki/Generalized_linear_model", "anchor_text": "eneralized linear models"}, {"url": "https://www.sciencedirect.com/science/article/abs/pii/S016786550500303X", "anchor_text": "this"}, {"url": "https://www.kaggle.com/uciml/pima-indians-diabetes-database", "anchor_text": "PIMA Diabetes dataset"}, {"url": "https://medium.com/@sahilgupta_86549/pima-diabetes-dataset-77ee2aa67ce7", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression#Examples", "anchor_text": "logistic regression model"}, {"url": "https://en.wikipedia.org/wiki/Logit", "anchor_text": "log-odds"}, {"url": "https://github.com/sahilgupta2105/Medium-Articles/blob/master/MechanicsOfTheRocCurve.ipynb", "anchor_text": "here"}, {"url": "https://medium.com/tag/data-science?source=post_page-----83b10ce3887f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----83b10ce3887f---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/classification?source=post_page-----83b10ce3887f---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----83b10ce3887f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----83b10ce3887f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83b10ce3887f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&user=Sahil+Gupta&userId=afb6a5fd07b4&source=-----83b10ce3887f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83b10ce3887f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&user=Sahil+Gupta&userId=afb6a5fd07b4&source=-----83b10ce3887f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b10ce3887f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F83b10ce3887f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----83b10ce3887f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----83b10ce3887f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----83b10ce3887f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----83b10ce3887f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahilgupta_86549?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahilgupta_86549?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sahil Gupta"}, {"url": "https://medium.com/@sahilgupta_86549/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "106 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fafb6a5fd07b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&user=Sahil+Gupta&userId=afb6a5fd07b4&source=post_page-afb6a5fd07b4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb14b0a1adbc5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmechanics-of-the-roc-curve-83b10ce3887f&newsletterV3=afb6a5fd07b4&newsletterV3Id=b14b0a1adbc5&user=Sahil+Gupta&userId=afb6a5fd07b4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}