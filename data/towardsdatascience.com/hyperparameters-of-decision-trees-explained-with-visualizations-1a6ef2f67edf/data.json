{"url": "https://towardsdatascience.com/hyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf", "time": 1683011726.436907, "path": "towardsdatascience.com/hyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf/", "webpage": {"metadata": {"title": "Hyperparameters of Decision Trees Explained with Visualizations | by Soner Y\u0131ld\u0131r\u0131m | Towards Data Science", "h1": "Hyperparameters of Decision Trees Explained with Visualizations", "description": "Decision tree is a widely-used supervised learning algorithm which is suitable for both classification and regression tasks. Decision trees serve as building blocks for some prominent ensemble\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Decision_tree_learning", "anchor_text": "wikipedia", "paragraph_index": 10}, {"url": "http://linkedin.com/in/soneryildirim/", "anchor_text": "linkedin.com/in/soneryildirim/", "paragraph_index": 26}, {"url": "http://twitter.com/snr14", "anchor_text": "twitter.com/snr14", "paragraph_index": 26}], "all_paragraphs": ["Decision tree is a widely-used supervised learning algorithm which is suitable for both classification and regression tasks. Decision trees serve as building blocks for some prominent ensemble learning algorithms such as random forests, GBDT, and XGBOOST.", "A decision tree builds upon iteratively asking questions to partition data. For instance, the following figure represents a decision tree used as a model to predict customer churn.", "Decision trees are prevalent in the field of machine learning due to their success as well as being straightforward. Some of the features that make them highly efficient:", "On the downside, decision trees are prone to overfitting. They can easily become over-complex which prevents them from generalizing well to the structure in the dataset. In that case, the model is likely to end up overfitting which is a serious issue in machine learning.", "To overcome this issue, we need to carefully adjust the hyperparameters of decision trees. In this post, we will try to gain a comprehensive understanding of these hyperparameters using tree visualizations.", "We will use one of the built-in datasets of scikit-learn. The wine dataset contains 13 features (i.e.columns) on three different wine classes. There are 178 samples (i.e. rows) in the dataset.", "Let\u2019s start with a decision tree classifier without any hyperparameter tuning.", "All of the hyperparameters are set with the default settings. We can plot our model using plot_tree function.", "The model keeps splitting the nodes until all the nodes are pure (i.e. contain samples from only one class). Let\u2019s first understand what the information in a", "box tells us. The first line indicates the name of the feature (i.e. column). Since we did not name the columns, the index of the column is shown. Samples indicates the number of observations (i.e. rows) and value shows the distribution of these samples according to the target variable.", "Gini is a measure of impurity. As stated on wikipedia, \u201cGini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset\u201d. It basically means that impurity increases with randomness. For instance, let\u2019s say we have a box with ten balls in it. If all the balls are same color, we have no randomness and impurity is zero. However, if we have 5 blue balls and 5 red balls, impurity is 1. If you take a look the leaf nodes (the nodes at the end of tree), you will see that gini is equal to zero.", "The other function to evaluate the quality of a split is entropy which is a measure of uncertainty or randomness. The more randomness a variable has, the higher the entropy is.", "We can select gini or impurity using criterion parameter. The default value is gini.", "We usually do not want a tree with all pure leaf nodes. It would be too specific and likely to overfit.", "When the algorithm performs a split, the main goal is to decrease impurity as much as possible. The more the impurity decreases, the more informative power that split gains. As the tree gets deeper, the amount of impurity decrease becomes lower. We can use this to prevent the tree from doing further splits. The hyperparameter for this task is min_impurity_decrease. It is set to zero by default. Let\u2019s change it and see the difference.", "We now have a much smaller tree. Consider the green node at the bottom. It contains 65 samples and 61 of them belong to one class. There is no need to further split that node because we can afford to have 4 misclassified samples out of 65. If we keep splitting that node, the model will probably be overfitting. Min_impurity_split parameter can be used to control the tree based on impurity values. It sets a threshold on gini. For instance, if min_impurity_split is set to 0.3, a node needs to have a gini value that is more then 0.3 to be further splitted.", "Another hyperparameter to control the depth of a tree is max_depth. It does not make any calculations regarding impurity or sample ratio. The model stops splitting when max_depth is reached.", "Max_depth is less flexible compared to min_impurity_decrease. For instance,", "we probably should not make the split on the left. It only distinguishes 2 samples and decreases the impurity by less than 0.1. This actually brings us to another hyperparameter which is min_samples_leaf. It indicates the minimum number of samples required to be at a leaf node. We need to be careful when using hyperparameters together. For instance, if we set min_samples_leaf to 3 in the previous tree, the split that separates 2 samples will not occur because we cannot have a leaf node with less than 3 samples. Well, it\u2019s not quite right. Let\u2019s see what really happens.", "In this case, min_samples_leaf is actually harmful for the model. It did not prevent the model from doing that final split. Furthermore, it caused extra misclassifications. Thus, it is not wise to use min_samples_leaf in this way.", "We can also limit the number of leaf nodes using max_leaf_nodes parameter which grows the tree in best-first fashion until max_leaf_nodes reached. The best split is decided based on impurity decrease.", "We end up having a tree with 5 leaf nodes.", "Another important hyperparameter of decision trees is max_features which is the number of features to consider when looking for the best split. If not specified, the model considers all of the features. There are 13 features in our dataset. If we set max_features as 5, the model randomly selects 5 features to decide on the next split. Max_features parameter also helps preventing the model from overfitting but it is not enough just to use max_features. If we let the model to become too deep, it will end up using all the features anyway.", "The hyperparameters need to be carefully adjusted in order to have a robust decision tree with a high out-of-sample accuracy. We do not have to use all of them. Depending on the task and the dataset, a couple of them could be enough. Please pay extra attention if you use multiple hyperparameters together because one may negatively effect the other.", "Thank you for reading. Please let me know if you have any feedback.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Top 10 Writer in AI and Data Science | linkedin.com/in/soneryildirim/ | twitter.com/snr14"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a6ef2f67edf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sonery.medium.com/?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": ""}, {"url": "https://sonery.medium.com/?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "Soner Y\u0131ld\u0131r\u0131m"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2cf6b549448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=post_page-2cf6b549448----1a6ef2f67edf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a6ef2f67edf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a6ef2f67edf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning", "anchor_text": "wikipedia"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1a6ef2f67edf---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1a6ef2f67edf---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1a6ef2f67edf---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----1a6ef2f67edf---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a6ef2f67edf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=-----1a6ef2f67edf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a6ef2f67edf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=-----1a6ef2f67edf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a6ef2f67edf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1a6ef2f67edf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1a6ef2f67edf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1a6ef2f67edf--------------------------------", "anchor_text": ""}, {"url": "https://sonery.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sonery.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Soner Y\u0131ld\u0131r\u0131m"}, {"url": "https://sonery.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "21K Followers"}, {"url": "http://linkedin.com/in/soneryildirim/", "anchor_text": "linkedin.com/in/soneryildirim/"}, {"url": "http://twitter.com/snr14", "anchor_text": "twitter.com/snr14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2cf6b549448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=post_page-2cf6b549448--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7cdf5377373a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameters-of-decision-trees-explained-with-visualizations-1a6ef2f67edf&newsletterV3=2cf6b549448&newsletterV3Id=7cdf5377373a&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}