{"url": "https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d", "time": 1683003558.475761, "path": "towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d/", "webpage": {"metadata": {"title": "Understanding Latent Space in Machine Learning | by Ekin Tiu | Towards Data Science", "h1": "Understanding Latent Space in Machine Learning", "description": "Imagine a large dataset of handwritten digits (0\u20139) like the one shown above. Handwritten images of the same number (i.e. images that are 3\u2019s) are the most similar to each other compared to other\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["If I have to describe latent space in one sentence, it simply means a representation of compressed data.", "Imagine a large dataset of handwritten digits (0\u20139) like the one shown above. Handwritten images of the same number (i.e. images that are 3\u2019s) are the most similar to each other compared to other images of different numbers (i.e. 3s vs. 7s). But can we train an algorithm to recognize these similarities? How?", "If you have trained a model to classify digits, then you have also trained the model to learn the \u2018structural similarities\u2019 between images. In fact, this is how the model is able to classify digits in the first place- by learning the features of each digit.", "If it seems that this process is \u2018hidden\u2019 from you, it\u2019s because it is. Latent, by definition, means \u201chidden.\u201d", "The concept of \u201clatent space\u201d is important because it\u2019s utility is at the core of \u2018deep learning\u2019 \u2014 learning the features of data and simplifying data representations for the purpose of finding patterns.", "Intrigued? Let\u2019s break latent space down bit by bit:", "Why do we compress data in ML?", "Data compression is defined as the process of encoding information using fewer bits than the original representation. This is like taking a 19D data point (need 19 values to define unique point) and squishing all that information into a 9D data point.", "More often than not, data is compressed in machine learning to learn important information about data points. Let me explain with an example.", "Say we would like to train a model to classify an image using a fully convolutional neural network (FCN). (i.e. output digit number given image of digit). As the model \u2018learns\u2019, it is simply learning features at each layer (edges, angles, etc.) and attributing a combination of features to a specific output.", "But each time the model learns through a data point, the dimensionality of the image is first reduced before it is ultimately increased. (see Encoder and Bottleneck below). When the dimensionality is reduced, we consider this a form of lossy compression.", "Because the model is required to then reconstruct the compressed data (see Decoder), it must learn to store all relevant information and disregard the noise. This is the value of compression- it allows us to get rid of any extraneous information, and only focus on the most important features.", "This \u2018compressed state\u2019 is the Latent Space Representation of our data.", "What do I mean by space?", "You may be wondering why we call it a latent space. After all, compressed data, at first glance, may not evoke any sort of \u201cspace.\u201d", "In this rather simplistic example, let\u2019s say our original dataset are images with dimensions 5 x 5 x 1. We will set our latent space dimensions to be 3 x 1, meaning our compressed data point is a vector with 3-dimensions.", "Now, each compressed data point is uniquely defined by only 3 numbers. That means we can graph this data on a 3D Plane (One number is x, the other y, the other z).", "This is the \u201cspace\u201d that we are referring to.", "Whenever we graph points or think of points in latent space, we can imagine them as coordinates in space in which points that are \u201csimilar\u201d are closer together on the graph.", "A natural question that arises is how would we imagine space of 4D points or n-dimensional points, or even non-vectors (since the latent space representation is NOT required to be 2 or 3-dimensional vectors, and is oftentimes not since too much information would be lost).", "The unsatisfying answer is, we can\u2019t. We are 3-dimensional creatures that cannot fathom n-dimensional space (such that n > 3). However, there are tools such as t-SNE which can transform our higher dimensional latent space representations into representations that we can visualize (2D or 3D). (See Visualizing Latent Space section below.)", "But you may be wondering, what are \u2018similar\u2019 images, and why does reducing the dimensionality of our data make similar images \u2018closer\u2019 together in space?", "What do I mean by similar?", "If we look at three images, two of a chair and one of a desk, we would easily say that the two chair images are the most similar whereas the desk is the most different from either of the chair images.", "But what makes these two chair images \u201cmore similar?\u201d A chair has distinguishable features (i.e. back-rest, no drawer, connections between legs). These can all be \u2018understood\u2019 by our models by learning patterns in edges, angles, etc.", "As explained, such features are packaged in the latent space representation of data.", "Thus, as dimensionality is reduced, the \u2018extraneous\u2019 information which is distinct to each image (i.e. chair color) is \u2018removed\u2019 from our latent space representation, since only the most important features of each image are stored in the latent space representations.", "As a result, as we reduce dimensionality, the representations of both chairs become less distinct and more similar. If we were to imagine them in space, they would be \u2018closer\u2019 together.", "*Please note that the \u2018closeness\u2019 metric I have referred to throughout the article is an ambiguous term and not a definitive Euclidian distance, because there are multiple definitions of distance in space.", "The latent space concept is definitely intriguing. But how is it used? When do we use it? And most importantly, why?", "What we\u2019ll find is that the latent space is \u2018hidden\u2019 in many of our favorite image processing networks, generative models, etc.", "Although the latent space is hidden from most, there are certain tasks in which understanding the latent space is not only helpful, but necessary.", "The latent space representation of our data contains all the important information needed to represent our original data point.", "This representation must then represent the features of the original data.", "In other words, the model learns the data features and simplifies its representation to make it easier to analyze.", "This is at the core of a concept called Representation Learning, defined as a set of techniques that allow a system to discover the representations needed for feature detection or classification from raw data.", "In this use case, our latent space representations are used to transform more complex forms of raw data (i.e. images, video), into simpler representations which are \u2018more convenient to process\u2019 and analyze.", "Listed below are specific instances of representation learning.", "The latent space is an essential concept in manifold learning, a subfield of representation learning.", "Manifolds in data science can be understood as groups or subsets of data that are \u2018similar\u2019 in some way.", "These similarities, usually imperceptible or obscured in higher-dimensional space, can be discovered once our data has been represented in the latent space.", "Take the example of a \u2018swiss roll\u2019 below.", "In 3D, we know that there are groups of similar data points that exist, but it is much more difficult to delineate such groups with higher dimensional data.", "By reducing the dimensionality of our data to 2D, which in this case could be considered a \u2018latent space\u2019 representation, we are able to more easily distinguish the manifolds (groups of similar data) in our dataset.", "To learn more about manifolds and manifold learning, I recommend the following articles:", "A common type of deep learning model that manipulates the \u2018closeness\u2019 of data in the latent space is the autoencoder \u2014 a neural network that acts as an identity function. In other words, an autoencoder learns to output whatever is inputted.", "Now, if you\u2019re new to the field, you may be wondering, why in the world would we need a model that does this? It seems rather useless if all it outputs is itself\u2026", "Though this reasoning is valid, we don\u2019t care so much about what the model outputs. We care more about what the model learns in the process.", "When we force a model to become an identity function, we are forcing it to store all of the data\u2019s relevant features in a compressed representation so that there is enough information in that compressed form such that the model can \u2018accurately\u2019 reconstruct it. Sound familiar? It should, because this compressed representation is our latent space representation (red block in image above).", "We have seen how patterns can be more easily discovered in the latent space since similar data points will tend to cluster together, but we have not yet seen how we can sample points from this latent space to seemingly generate \u2018new\u2019 data.", "In the example above, we can generate different facial structures by interpolating on latent space, and using our model decoder to reconstruct the latent space representation into a 2D image with the same dimensions as our original input.", "What do I mean by interpolating on latent space?", "Let\u2019s say that I have compressed the chair images from the previous section into the following 2D vectors, [0.4, 0.5] and [0.45, 0.45]. Let\u2019s say the desk is compressed to [0.6, 0.75]. If I were to interpolate on latent space, I would sample points in latent space between the \u2018chair\u2019 cluster and the \u2018desk\u2019 cluster.", "We can feed these sampled 2D vectors into the model\u2019s decoder, and voila! We get \u2018new\u2019 images that look like a morph between a chair and a desk. *new is in quotes because these generated images are not technically independent of the original data sample.", "Below is an example of linear interpolation between two types of chairs in latent space.", "Image generation is still an active area of research, and the latent space is an essential concept that must be understood. See the following articles for more use cases of generative models, and a hands-on example of latent space interpolation using a GAN (Generative Adversarial Network), another generative model that uses latent space representations.", "For more on latent space visualization, I recommend Hackernoon\u2019s article which provides a hands-on example of visualizing similarities between digit images in a 2D space with the t-SNE algorithm.", "While learning about the latent space, I was fascinated by this \u2018hidden,\u2019 yet essential concept. I hope that this article demystified the latent space representation, and provided the \u2018deeper understanding\u2019 of deep learning that I longed for as a novice.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "CS @ Stanford University | Stanford ML Group"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fde5a7c687d8d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ekintiu.medium.com/?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": ""}, {"url": "https://ekintiu.medium.com/?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "Ekin Tiu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F684d91ee6205&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&user=Ekin+Tiu&userId=684d91ee6205&source=post_page-684d91ee6205----de5a7c687d8d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde5a7c687d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde5a7c687d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df", "anchor_text": "https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df"}, {"url": "https://datascience.stackexchange.com/a/5698", "anchor_text": "https://datascience.stackexchange.com/a/5698"}, {"url": "https://towardsdatascience.com/manifolds-in-data-science-a-brief-overview-2e9dde9437e5", "anchor_text": "Manifolds in Data Science \u2014 A Brief OverviewManifolds are an essential tool for representing data in higher dimensions. But what are manifolds, and how are they\u2026towardsdatascience.com"}, {"url": "https://scikit-learn.org/stable/modules/manifold.html", "anchor_text": "2.2. Manifold learning \u2014 scikit-learn 0.22.1 documentationLook for the bare necessities The simple bare necessities Forget about your worries and your strife I mean the bare\u2026scikit-learn.org"}, {"url": "https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/", "anchor_text": "18 Impressive Applications of Generative Adversarial Networks (GANs)A Generative Adversarial Network, or GAN, is a type of neural network architecture for generative modeling. Generative\u2026machinelearningmastery.com"}, {"url": "https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/", "anchor_text": "How to Explore the GAN Latent Space When Generating FacesHow to Use Interpolation and Vector Arithmetic to Explore the GAN Latent Space. Generative Adversarial Networks, or\u2026machinelearningmastery.com"}, {"url": "https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df", "anchor_text": "Latent space visualization \u2014 Deep Learning bits #2Featured: interpolation, t-SNE projection (with gifs & examples!)hackernoon.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----de5a7c687d8d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----de5a7c687d8d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/representation-learning?source=post_page-----de5a7c687d8d---------------representation_learning-----------------", "anchor_text": "Representation Learning"}, {"url": "https://medium.com/tag/latent-space?source=post_page-----de5a7c687d8d---------------latent_space-----------------", "anchor_text": "Latent Space"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----de5a7c687d8d---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde5a7c687d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&user=Ekin+Tiu&userId=684d91ee6205&source=-----de5a7c687d8d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde5a7c687d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&user=Ekin+Tiu&userId=684d91ee6205&source=-----de5a7c687d8d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde5a7c687d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fde5a7c687d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----de5a7c687d8d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----de5a7c687d8d--------------------------------", "anchor_text": ""}, {"url": "https://ekintiu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ekintiu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ekin Tiu"}, {"url": "https://ekintiu.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F684d91ee6205&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&user=Ekin+Tiu&userId=684d91ee6205&source=post_page-684d91ee6205--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F70d9799552f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-latent-space-in-machine-learning-de5a7c687d8d&newsletterV3=684d91ee6205&newsletterV3Id=70d9799552f&user=Ekin+Tiu&userId=684d91ee6205&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}