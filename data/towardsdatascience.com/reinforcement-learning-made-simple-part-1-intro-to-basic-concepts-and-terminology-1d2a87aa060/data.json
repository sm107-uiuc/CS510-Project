{"url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060", "time": 1683015246.8041658, "path": "towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060/", "webpage": {"metadata": {"title": "Reinforcement Learning Made Simple (Part 1): Intro to Basic Concepts and Terminology | by Ketan Doshi | Towards Data Science", "h1": "Reinforcement Learning Made Simple (Part 1): Intro to Basic Concepts and Terminology", "description": "A Gentle Guide to applying Markov Decision Processes, in Plain English. Introduce Rewards, Discounts, Returns, State Action Value, and Optimal Policy."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["You\u2019ve probably started hearing a lot more about Reinforcement Learning in the last few years, ever since the AlphaGo model, which was trained using reinforcement-learning, stunned the world by beating the then reigning world champion at the complex game of Go.", "Over a series of articles, I\u2019ll go over the basics of Reinforcement Learning (RL) and some of the most popular algorithms and deep learning architectures used to solve RL problems. We\u2019ll try to focus on understanding these principles in as intuitive a way as possible without going too much into mathematical theory.", "Here\u2019s a quick summary of the articles in the series. My goal throughout will be to understand not just how something works but why it works that way.", "Typically when people provide an overview of ML, the first thing they explain is that it can be divided into two categories, Supervised Learning and Unsupervised Learning. However, there is a third category, viz. RL although it isn\u2019t mentioned as often as its two more glamorous siblings.", "Supervised Learning uses labeled data as input, and predicts outcomes. It receives feedback from a Loss function acting as a \u2018supervisor\u2019.", "Unsupervised Learning uses unlabeled data as input and detects hidden patterns in the data such as clusters or anomalies. It receives no feedback from a supervisor.", "Reinforcement Learning gathers inputs and receives feedback by interacting with the external world. It outputs the best actions that it needs to take while interacting with that world.", "Rather than the typical ML problems such as Classification, Regression, Clustering and so on, RL is most commonly used to solve a different class of real-world problems, such as a Control task or Decision task, where you operate a system that interacts with the real world.", "It is useful for a variety of applications like:", "With RL the learning happens from experience by trial and error, similar to a human eg. A baby can touch fire or milk and then learns from negative or positive reinforcement.", "Let\u2019s say you want to train a robot. How would you use RL to solve a problem like this?", "To apply RL, the first step is to structure the problem as something called a Markov Decision Process (MDP). If you haven\u2019t worked with RL before, chances are that the only thing you know about an MDP is that it sounds scary \ud83d\ude04.", "So let\u2019s try to understand what an MDP is. An MDP has five components that work together in a well-defined way.", "Agent: it\u2019s the system that you operate eg. the robot. This is the model that you want to build and train using RL.", "Environment: the real-world environment with which the agent interacts as part of its operation. eg. The terrain that the robot has to navigate, its surroundings, factors such as wind, friction, lighting, temperature and so on.", "State: this represents the current \u2018state of the world\u2019 at any point. eg. it could capture the position of the robot relative to its terrain, the position of objects around it, and perhaps the direction and speed of the wind.", "There could be a finite or infinite set of states.", "Action: these are the actions that the agent takes to interact with the environment. eg. The robot can turn right, left, move forward, go backward, bend, raise its hand and so on.", "There could be a finite or infinite set of possible actions.", "Reward: is the positive or negative reinforcement that the agent receives from the environment as a result of its actions. It is a way to evaluate the \u2018good-ness\u2019 or \u2018bad-ness\u2019 of a particular action.", "eg. If moving in a particular direction causes the robot to run into a wall, that would have a negative reward. On the other hand, if turning left causes the robot to locate the object it needs to pick up, it would get a positive reward.", "Agent and Environment: Obviously, the first step is to decide the role and scope of your agent and the environment for the problem you are trying to solve.", "State: Next, you have to define what data points the state contains and how they are represented.", "The important thing is that it captures everything that is needed for your problem to represent the current world situation so that the agent can reason about the future without requiring information about the past or any additional knowledge.", "In other words, the definition of the state should be self-contained. So, for instance, if you need to know something about how you arrived at this state, that history should be encapsulated within your state definition itself.", "Actions: What is the set of the potential actions your agent can take?", "Reward: This is how the agent learns from experience. So this is something that you need to give a fair amount of thought to because it is critical that you define the rewards in a way that truly reflects the behavior that you want your agent to learn.", "Now that we\u2019ve seen what an MDP is, we\u2019ll go into how it works. Let\u2019s use the game of Tic-Tac-Toe (aka Noughts and Crosses) as a simple example. Two players play this game by placing their tokens on a 3x3 grid. One player places Noughts (the donut-shape) while the other places Crosses. The objective is to win the game by placing three of your tokens in a line.", "The agent interacts with its environment over a sequence of time-steps. A set flow of operations occurs in each time-step and that flow then repeats in each time-step.", "The sequence starts with an initial state, which becomes the current state. For instance, your opponent, the environment has placed their token in a particular position, and that is the starting state for the game.", "Now, starting with the first time-step, the following steps occur at each time-step:", "This completes one time-step and moves us to the next time-step. This next state now becomes the current state which is then provided to the agent as input, and the cycle repeats.", "Throughout this process, it is the agent\u2019s goal to maximize the total amount of rewards that it receives from taking actions in given states. It wants to maximize not just the immediate reward, but the cumulative rewards it receives over time. We will return to this topic shortly.", "Here is another view of the MDP\u2019s operation which shows the progression of time-steps.", "In each time-step, three things occur \u2014 state, action and reward, which fully describe what happened in that time-step.", "So the execution of the MDP can be described as a trajectory of occurrences (in terms of state, action, reward) over a sequence of time-steps, as below.", "For RL tasks that have a well-defined end or Terminal state, a complete sequence from the starting state to the end state is called an episode. eg. Each round of a game is an episode.", "Therefore an RL system\u2019s operation repeats over multiple episodes. Within each episode, it repeats over multiple time-steps.", "On the other hand, RL tasks which have no end are known as Continuing Tasks, and can go on forever (or till you stop the system). Eg. A robot that continuously manages manufacturing or warehouse automation.", "As we just saw, the MDP operates by alternating between the agent doing something and then the environment doing something, in each time-step:", "Given a current state, and the action picked by the agent, how does the environment figure out the outcome ie. the next state and reward?", "For most realistic RL problems that we will deal with, the answer will usually be that 'it just does\u2019. Most environments have complex internal dynamics that control how they behave when an action is taken from a particular state.", "For instance, in a stock-trading RL application, the stock market environment has a range of unseen factors that determine how stock prices move. Or the environment in a drone navigation RL application depends on the laws of physics that control air flows, motion, thermodynamics, visibility and so on in a variety of terrains and micro-weather conditions.", "Our focus is on training the agent and we can usually treat the environment as an external black box.", "Note that this external black box might be a simulator for the environment. In many cases, it might not be practical to build a simulator and we would interact with the real environment directly.", "However, just for completeness, let me briefly mention that if we did build such an environment model, an MDP would represent it as a large transition probability matrix or function.", "This matrix maps a given state and action pair to:", "On the other hand, we are very interested in how the agent decides what action to pick in a given state. That is, in fact, precisely the RL problem that we want to solve.", "For that it uses three concepts, which we will explore next:", "As the agent executes time-steps, it accumulates reward at each time-step.", "However, rather than any individual reward, what we really care about is the cumulative rewards.", "We call this the Return. It is the total reward that the agent accumulates over the duration of a task.", "When we calculate Return, rather than simply adding up all the rewards, we apply a discount factor \u03b3 to weight later rewards over time. These are known as Discounted Rewards.", "This way, cumulative rewards do not grow infinitely as the number of time-steps becomes very large (like for continuing tasks, or for very long episodes).", "It also encourages the agent to care more about the immediate reward compared to later rewards since later rewards will be more heavily discounted.", "The ultimate goal of the agent is to get the maximum Return, not just over one episode, but over many, many episodes.", "Based on this discount, we can see that there are two factors the agent considers when evaluating rewards.", "The first point is that if the agent had to choose between getting some amount of reward now versus later, the immediate reward is more valuable. Since the discount factor, \u03b3, is less than 1, we will discount later rewards more than immediate rewards.", "The second point is that if the agent had to choose between getting some reward now versus getting a much bigger reward later, the bigger reward is most likely preferable. This is because we want the agent to look at total Returns rather than individual rewards. eg. In a game of chess, the agent has to pick the better of two paths. In the first, it can kill off a few pieces early on by playing aggressively. That gives it some immediate reward. However in the long run that puts it in a disadvantaged position, and it loses the game. Hence it gets a large negative reward at the end. Alternately it can play a different set of moves which yields lower rewards at first but where it ultimately wins the game. And thus gets a large positive reward. Clearly, the second approach is better since it gives a higher total Return as opposed to a bigger immediate reward.", "The second concept for us to cover is Policy. Earlier we had deferred one very important question, which was, how the agent decides which action to pick in a given state. There can be many different strategies that an\u00a0agent\u00a0might\u00a0use:", "Any strategy that the agent follows to decide which action to pick in a given state, is called a Policy. Although that sounds abstract, a Policy is simply something that maps a given state to an action to be taken.", "You could think of a Policy as a (huge) Lookup Table which maps a state to an action.", "So given the current state, the agent looks up that state in the table to find the action that it should pick.", "In practice, for real-world problems, there are so many states and so many actions, that a function is used, not a Lookup Table, that maps a state to an action.", "However, the intuition is the same \u2014 think of a function as a \u2018huge lookup table\u2019.", "Policies can be either Deterministic or Stochastic.", "A Deterministic Policy is a Policy where the agent always chooses the same fixed action when it reaches a particular state.", "Alternately, a Stochastic Policy is a Policy where the agent varies the actions it chooses for a state, based on some probability for each action.", "It might do this while playing a game, for instance, so that it doesn\u2019t become completely predictable. Eg while playing Rock Paper Scissors, if it always played the same move, opponents can figure this out and easily defeat it.", "We\u2019ve been talking about the Policy as though the agent already had one readily available for it to use. But that is not really the case.", "Much like a human baby, the agent doesn\u2019t really have a useful policy when it starts out and has no idea what action it should take from any given state. Then, by using the Reinforcement Learning algorithm, it slowly learns a helpful policy that it can use.", "The action that the agent takes from a given state determines the reward it obtains, and therefore over time, the eventual total Return. Hence the goal of the agent is to pick the action that maximizes its Return.", "Put another way, the agent\u2019s goal is to follow a Policy (which is how it picks its actions) that maximizes its Return.", "So, out of all the Policies the agent could follow, it wants to pick the best one ie. the one which gives it the highest Return.", "In order to do that, the agent needs to compare two Policies to decide which is better. For that, we need to understand the notion of Value.", "Let\u2019s say the agent is in a particular state. Also, let\u2019s say that the agent has somehow been given a policy, \u03c0. Now, if it starts from that state, and always picks actions based on that policy, what is the Return it could expect to get?", "This is the same as saying, if the agent starts from that state, and always picks actions based on that policy, what would its average Return be over many, many episodes?", "This average long-term Return, or expected Return, is known as the Value of that particular state, under policy \u03c0.", "Alternately, the agent could start from a state-action pair ie. it has already taken a particular action from a particular state. If going forward from that state-action, it always picks actions based on the given policy \u03c0, what is the Return it could expect to get?", "As discussed earlier for the Policy Table, we can think of Value as a (huge) Lookup Table which maps a State, or a State-Action pair, to a Value.", "Thus we have two types of Value:", "Think of Reward as immediate pleasure and Value as long-lasting happiness \ud83d\ude03.", "Intuitively one can think of Value as follows. Like a human, the agent learns from experience. As it interacts with the environment and completes episodes, it obtains the Returns for each episode.", "As it accumulates more experience (ie. obtains Returns for more and more episodes), it gets a sense of which states, and which actions in those states yield the most Return.", "It stores this \u2018experience\u2019 as \u2018Value\u2019.", "Clearly, the rewards we get (and hence the Return and therefore the Value) depends on the action we take from a given state. And since the action depends on the chosen Policy, it follows that the Value depends on the Policy.", "Eg. if our policy was to choose entirely random actions (i.e., sample actions from a uniform distribution), the Value (expected reward) of a state would probably be pretty low, since we\u2019re definitely not choosing the best possible actions.", "Eg. Instead, if our policy was to choose actions from a probability distribution that produces the maximum rewards when sampled, the Value (expected reward) of a state would be much higher.", "Now that we understand Value, let\u2019s go back to our earlier discussion about comparing two policies to see which is better. How do we evaluate what \u2019better\u2019 means?", "Given two policies, we can determine the corresponding State-Value or State-Action Value functions for each of those policies, by following the policy and evaluating the Returns.", "Once we have the respective Value Functions, we can use those Value Functions to compare the policies. The policy, whose Value Function is higher, is better because that means it will yield higher Returns.", "Since we can now compare policies to figure out which ones are 'good' and which ones are 'bad\u2019, we can also use that to find the 'best' policy. This is known as the Optimal Policy.", "The Optimal Policy is the policy that will yield more Returns to the agent than all other policies.", "So now we have the approach to solve an RL problem.", "We structure our problem as an MDP and we can then solve this problem by building an agent viz. the brains of the MDP, in such a way that it can make decisions about which action to take. It should do this in a way that maximizes Returns.", "In other words, we need to find the Optimal Policy for the agent. Once it has the Optimal Policy it simply uses that policy to pick actions from any state.", "We will apply a Reinforcement Learning algorithm to build an agent model and train it to find the Optimal Policy. Finding the Optimal Policy essentially solves the RL problem.", "In the next article in this series, we will look at the solution approach used by these RL algorithms.", "And finally, if you liked this article, you might also enjoy my other series on Transformers as well as Audio Deep Learning.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d2a87aa060&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47----1d2a87aa060---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d2a87aa060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d2a87aa060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://unsplash.com/@pmpietsch?utm_source=medium&utm_medium=referral", "anchor_text": "Philippe Murray-Pietsch"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/reinforcement-learning-made-simple-part-2-solution-approaches-7e37cbf2334e", "anchor_text": "Solution Approaches"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-3-model-free-solutions-step-by-step-c4bbb2b72dcf", "anchor_text": "Model-free algorithms"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e", "anchor_text": "Q-Learning"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b", "anchor_text": "Deep Q Networks"}, {"url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-6-policy-gradients-step-by-step-f9f448e73754", "anchor_text": "Policy Gradient"}, {"url": "https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452", "anchor_text": "Transformers Explained Visually (Part 1): Overview of FunctionalityA Gentle Guide to Transformers for NLP, and why they are better than RNNs, in Plain English. How Attention helps\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504", "anchor_text": "Audio Deep Learning Made Simple (Part 1): State-of-the-Art TechniquesA Gentle Guide to the world of disruptive deep learning audio applications and architectures. And why we all need to\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1d2a87aa060---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1d2a87aa060---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1d2a87aa060---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----1d2a87aa060---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----1d2a87aa060---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d2a87aa060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----1d2a87aa060---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d2a87aa060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&user=Ketan+Doshi&userId=54f9ca55ed47&source=-----1d2a87aa060---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d2a87aa060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d2a87aa060&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d2a87aa060---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d2a87aa060--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d2a87aa060--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d2a87aa060--------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ketanhdoshi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ketan Doshi"}, {"url": "https://ketanhdoshi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54f9ca55ed47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&user=Ketan+Doshi&userId=54f9ca55ed47&source=post_page-54f9ca55ed47--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fae94feabe1c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060&newsletterV3=54f9ca55ed47&newsletterV3Id=ae94feabe1c9&user=Ketan+Doshi&userId=54f9ca55ed47&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}