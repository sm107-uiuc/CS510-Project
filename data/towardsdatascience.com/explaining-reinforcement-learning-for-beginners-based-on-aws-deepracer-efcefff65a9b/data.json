{"url": "https://towardsdatascience.com/explaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b", "time": 1683009432.888507, "path": "towardsdatascience.com/explaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b/", "webpage": {"metadata": {"title": "Explaining Reinforcement Learning for Beginners based on AWS DeepRacer | by Marc Cervera | Towards Data Science", "h1": "Explaining Reinforcement Learning for Beginners based on AWS DeepRacer", "description": "Reinforcement Learning is a type of machine learning algorithm that learns through experience. This algorithm tells an agent which set of actions it should take within a closed environment to fulfill\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188", "anchor_text": "Go", "paragraph_index": 2}, {"url": "https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-how-it-works-reinforcement-learning-algorithm.html", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk&t=2s", "anchor_text": "3Blue1Brown", "paragraph_index": 12}, {"url": "https://openai.com/blog/openai-baselines-ppo/", "anchor_text": "Proximal Policy Optimization (PPO)", "paragraph_index": 20}, {"url": "https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12", "anchor_text": "this article", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/an-advanced-guide-to-aws-deepracer-2b462c37eea", "anchor_text": "advanced guide", "paragraph_index": 22}, {"url": "https://www.linkedin.com/in/natalia-korchagina/", "anchor_text": "Natalia Korchagina", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/dg4/", "anchor_text": "Daniel Gonzalez", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/jnin/", "anchor_text": "Jordi Nin", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/arubiomunoz/", "anchor_text": "Alberto Rubio", "paragraph_index": 23}], "all_paragraphs": ["In this article, we will cover the following topics:", "Reinforcement Learning is a type of machine learning algorithm that learns through experience. This algorithm tells an agent which set of actions it should take within a closed environment to fulfill a task. Afterward, the agent receives a quantifiable reward for those actions. This reward is a numerical score that measures how adjusted are these actions with the overall objective. Roughly speaking, the agent, guided by the algorithm, repeatedly tries different sets of actions, attempting to maximize the total reward received. After each group of trials, the algorithm \u201clearns\u201d which actions work better than others and adjusts its next trials to those findings. This process continues for an allocated time or number of trials. As a result, the algorithm will be able to tell the agent which actions to take to aim for a higher reward.", "If this sounds like a game, you are not wrong; Reinforcement Learning follows the same principles! This is why it was used successfully to play games such as Go and Chess, or videogames such as Super Mario, or StarCraft, beating professional players. Other applications include algorithmic trading, robotics, or dynamic pricing.", "For example, imagine we decided to use Reinforcement Learning to play Super Mario. The agent (the Mario icon) observes the environment (part of the level the player can see on the screen) and performs an action (go forward, backward or jump). After each trial, it receives a reward based on the progress it achieved in the level.", "Let\u2019s look at how principles of Reinforcement Learning are introduced in AWS DeepRacer. Amazon Web Services (AWS) DeepRacer is an autonomous racing car that is trained with reinforcement learning in a digital simulator in the Cloud. Reinforcement Learning \u201cteaches\u201d the car (which is the agent) how to drive in a racing track. The goal of the agent is to complete multiple laps in the fastest time possible. This brief background will be the basis for the following chapters. If you need further information, you can visit here.", "DeepRacer applies a specific method of reinforcement learning using two neural networks. This algorithm is what helps the agent (the car) choose the best actions within every trial. In reinforcement learning, each trial is referred to as an episode. Formally, an episode is a combination of actions in specific states. Think of action as a decision taken by the car. A state is the position of the car within the circuit. Each episode finishes once the car leaves the delimited track or finishes the lap.", "In a DeepRacer circuit, its (x,y) coordinates describe the millions of possible car states. Each action is what moves the agent from one state to another. In our case, an action is a combination of steering angle and throttle.", "The Reinforcement Learning algorithm performs some calculations (which we are going to explain later) and tells the car to take a specific action (e.g., going at 0.5 m/s and steering 30 degrees left). The car performs this action for several milliseconds and arrives at another state. In this new state, the agent performs another calculation and decides an action. This process repeatedly happens until the car finishes the episode.", "The algorithm decides which actions to take based on a reward function. This reward function rates the agent\u2019s performance in each action, the same way a student is graded for a test. We have to set the reward function, which leads the agent to fulfill the desired objective(s). In our case, we want the car to finish the track (objective #1) and do it fast (objective #2).", "The reward function reflects these objectives using parameters, which are measurable factors in each state. A simple reward function could include the progress (% of track completion) and the speed parameter. In this way, the closest the car gets to the finish line (objective #1), and the faster it does this (objective #2) the higher the reward. In DeepRacer, the algorithm receives these and other parameters (e.g., steering or distance from the center of the track) from the virtual environment. The algorithm measures these parameters before telling the car which action to take, reflecting the interaction with the environment. Then the values of these parameters would be placed selectively in a reward function, which would return a number, rating the agent\u2019s action. Depending on our goal, we have to choose which parameters to add in the reward function and what influence they are going to have on the size of the reward.", "Maximizing a reward function should sound familiar to those of you who learned about optimization problems. For some optimization problems, you can arrive at a global set of actions, which maximizes the amount of reward the agent can receive within a given set of constraints. But in the case of DeepRacer we do not have a standard optimization problem, as there is an almost infinite number of combinations of actions and states. That is why it is impossible to brute-force the best answer by performing all combinations possible and seeing which one is the best.", "In a way, reinforcement learning is about teaching the agent how to perform the best \u201cguesses\u201d of what actions should it take, which are likely to give a high reward. Usually, the most we can obtain from reinforcement learning algorithms are local reward maxima. As we cannot efficiently try all combinations, we are not able to conclude that this is the global maximum reward the agent can receive.", "When we talk about reinforcement learning there are several distinct algorithms. In this article, we are going to focus on the neural network application of Reinforcement Learning. This algorithm combines two neural networks to learn which actions to take: the policy network and the value network. If you do not understand what a neural network is, then I highly recommend watching the videos from 3Blue1Brown, although they are not necessary to understand what comes next.", "The value network\u2019s goal is to generate \u201cguesses\u201d based on previous trials. In this case, a \u201cguess\u201d is the expected reward received after a particular action. Imagine that the car can take three possible actions. Then this network, based on past experiences, estimates what is the expected reward from taking each of these actions. Understanding how the value network does this is beyond the scope of this article. For simplicity, let\u2019s define action space as the combination of all possible actions the car can take in a given state. For example, imagine the car is in a state (S39) and has an action space of 3. Then, an output of the value network could be: (A1, A2 ,A3) = (10, 100, 5).", "The policy network transforms the generated \u201cguesses\u201d from the value network and turns them into actions. It will use the expected reward for taking each action and transform them into probabilities, applying a method called policy gradients. These probabilities are a distribution of actions in a given state. The higher the expected reward for taking an action, the higher the probability. We will refer to these probabilities as policies. Going back to our previous example in S39, the policy could be: (A1, A2, A3) = (0.2 , 0.7 , 0. 1). A2 has a higher probability because the expected reward for taking A2 is higher compared to the rest of the actions.", "The process of learning consists of two phases: the exploration phase and the adjustment phase, which are repeated several times sequentially.", "During the exploration phase, the policy network is going to use the \u201cguesses\u201d provided by the value network to estimate policies in a given state. Using those same guesses, it will run x amount of trials (e.g., 10.000 times), doing different combinations of actions and receiving a reward after each episode. In each state, the agent might perform one action or the other, following the distribution of the probabilities set by the policy.", "Let\u2019s go back to our old example: (A1,A2,A3) = (0.2 , 0.7 , 0. 1) in S39. In all the episodes the agent arrives at S39, 20% of those it takes decision A1, 70% A2, and 10% A3. Note that the policy network will never output certain actions (e.g. (A1,A2,A3) = (0,1,0)) but a probability, to allow the agent to explore the circuit and try every action even if the model predicts a low expected reward. This is because these expected rewards are nothing more than \u201cguesses\u201d based on experience and, thus, might be wrong.", "Note that during the first exploration phase, the value network will have no experience and will give the same expected reward to all actions. Then the policy network will transform these expected rewards into equal probabilities. Within each state, the car will perform any action with equal probabilities and move to another state. Because it is the first exploration and there is no accumulated experience, most episodes end up out of the track and comparably low rewards.", "After this exploration phase has ended, the adjustment phase will begin. The value network will compile and aggregate all the rewards received from those trials and adjust the expected payoffs for performing each specific action in a given state. These new \u201cguesses\u201d will be fed back to the policy network and change the probabilities of taking particular actions. It will increase the probability of actions that gave a higher reward while decreasing the probability of actions that led to poor rewards. Then the agent will start the second iteration of exploration and adjustment phase. This process can go over several times.", "Technical note: the size of the increase/decrease of the probabilities will depend on: (a) the difference between the expected reward and obtained reward in those experiences and (b) the policy gradient. The policy gradient is a fundamental part of reinforcement learning. The policy network creates a surrogate function, which approximates the behavior of the reward function dependent on the change in policies. In a few words, it estimates the impact on the expected reward caused by a change of policy. Then the algorithm calculates the policy, which maximizes the expected reward. To do this, we compute the gradient of this surrogate function, which will indicate the direction and the size of the change in the old policies during the adjustment phase. In the case of DeepRacer, the policy gradient applies a method called Proximal Policy Optimization (PPO). What differentiates this method from others is its simplicity and easy implementation. Additionally, PPO sets a cap to avoid significant policy adjustments based on a few episodes. For more information on how PPO works in detail, you can read this article from Jonathan Hui.", "The main idea is that once we do this several times, we will increase the probability of actions that lead to higher rewards. Then with each iteration of exploration and adjustment phase, the algorithm keeps improving its rewards by taking actions that lead to better rewards. Consequently, after some hours of training, we got our car to finish a fast lap!", "If you want read more about autonomous racing and DeepRacer, you can take a look at this advanced guide, my team member, Daniel built. He gathered all the insights we took out from participating in the AWS-Formula 1 event, achieving a top 1% placement.", "Wrapping up, I would like to thank Natalia Korchagina and Daniel Gonzalez for helping me create this article and our professors Jordi Nin and Alberto Rubio for proof-reading it. Finally, thank you to ESADE Business School for allowing content creation like this to be part of our Master in Business Analytics.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fefcefff65a9b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marccervera_54553?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marccervera_54553?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "Marc Cervera"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F80156c8beb2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&user=Marc+Cervera&userId=80156c8beb2c&source=post_page-80156c8beb2c----efcefff65a9b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fefcefff65a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fefcefff65a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jerrykavan?utm_source=medium&utm_medium=referral", "anchor_text": "Jarom\u00edr Kavan"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188", "anchor_text": "Go"}, {"url": "https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-how-it-works-reinforcement-learning-algorithm.html", "anchor_text": "here"}, {"url": "https://unsplash.com/@lillynwilson?utm_source=medium&utm_medium=referral", "anchor_text": "Caitlyn Wilson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk&t=2s", "anchor_text": "3Blue1Brown"}, {"url": "https://unsplash.com/@anniespratt?utm_source=medium&utm_medium=referral", "anchor_text": "Annie Spratt"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://openai.com/blog/openai-baselines-ppo/", "anchor_text": "Proximal Policy Optimization (PPO)"}, {"url": "https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12", "anchor_text": "this article"}, {"url": "https://towardsdatascience.com/an-advanced-guide-to-aws-deepracer-2b462c37eea", "anchor_text": "advanced guide"}, {"url": "https://www.linkedin.com/in/natalia-korchagina/", "anchor_text": "Natalia Korchagina"}, {"url": "https://www.linkedin.com/in/dg4/", "anchor_text": "Daniel Gonzalez"}, {"url": "https://www.linkedin.com/in/jnin/", "anchor_text": "Jordi Nin"}, {"url": "https://www.linkedin.com/in/arubiomunoz/", "anchor_text": "Alberto Rubio"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----efcefff65a9b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/aws-deepracer?source=post_page-----efcefff65a9b---------------aws_deepracer-----------------", "anchor_text": "Aws Deepracer"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----efcefff65a9b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/self-driving-cars?source=post_page-----efcefff65a9b---------------self_driving_cars-----------------", "anchor_text": "Self Driving Cars"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----efcefff65a9b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fefcefff65a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&user=Marc+Cervera&userId=80156c8beb2c&source=-----efcefff65a9b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fefcefff65a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&user=Marc+Cervera&userId=80156c8beb2c&source=-----efcefff65a9b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fefcefff65a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fefcefff65a9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----efcefff65a9b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----efcefff65a9b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----efcefff65a9b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----efcefff65a9b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marccervera_54553?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marccervera_54553?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marc Cervera"}, {"url": "https://medium.com/@marccervera_54553/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13 Followers"}, {"url": "https://www.linkedin.com/in/marc-cervera-castro/", "anchor_text": "https://www.linkedin.com/in/marc-cervera-castro/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F80156c8beb2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&user=Marc+Cervera&userId=80156c8beb2c&source=post_page-80156c8beb2c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F80156c8beb2c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplaining-reinforcement-learning-for-beginners-based-on-aws-deepracer-efcefff65a9b&user=Marc+Cervera&userId=80156c8beb2c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}