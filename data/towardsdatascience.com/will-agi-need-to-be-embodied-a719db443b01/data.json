{"url": "https://towardsdatascience.com/will-agi-need-to-be-embodied-a719db443b01", "time": 1683018167.301892, "path": "towardsdatascience.com/will-agi-need-to-be-embodied-a719db443b01/", "webpage": {"metadata": {"title": "Will AGI need to be embodied?. Geordie Rose on the TDS podcast | by Jeremie Harris | Towards Data Science", "h1": "Will AGI need to be embodied?", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 5}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 133}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:", "The leap from today\u2019s narrow AI to a more general kind of intelligence seems likely to happen at some point in the next century. But no one knows exactly how: at the moment, AGI remains a significant technical and theoretical challenge, and expert opinion about what it will take to achieve it varies widely. Some think that scaling up existing paradigms \u2014 like deep learning and reinforcement learning \u2014 will be enough, but others think these approaches are going to fall short.", "Geordie Rose is one of them, and his voice is one that\u2019s worth listening to: he has deep experience with hard tech, from founding D-Wave (the world\u2019s first quantum computing company), to building Kindred Systems, a company pioneering applications of reinforcement learning in industry that was recently acquired for $350 million dollars.", "Geordie is now focused entirely on AGI. Through his current company, Sanctuary AI, he\u2019s working on an exciting and unusual thesis. At the core of this thesis is the idea is that one of the easiest paths to AGI will be to build embodied systems: AIs with physical structures that can move around in the real world and interact directly with objects. Geordie joined me for this episode of the podcast to discuss his AGI thesis, as well as broader questions about AI safety and AI alignment.", "Here were some of my favourite take-homes from the conversation:", "You can follow me on Twitter here.", "Jeremie Harris (00:00:00):Hey everyone, welcome back to the podcast. I\u2019m especially excited about today\u2019s episode because we\u2019re talking to Geordie Rose, who is the founder of one of the world\u2019s first quantum computing companies. It\u2019s a company called D-Wave that many of you may already have heard of. Now, Geordie really is a hard tech pioneer. At the time he found a D-Wave, back in the \u201990s next to no one was paying attention to quantum computing at all. He\u2019s also been ahead of the game in a completely different area, and that\u2019s reinforcement learning.", "Jeremie Harris (00:00:23):In fact, at the time I first met Geordie, back in the mid 2010s, he was already very focused on the potential that reinforcement learning would have for augmenting the capabilities of existing AI systems, which is why he then went on to run another company called Kindred AI, which developed one of the first concrete applications of reinforcement learning in industry. But Geordie\u2019s ultimate focus is now on AGI, something that he\u2019s working on at his new startup, Sanctuary.ai.", "Jeremie Harris (00:00:48):Now, Sanctuary\u2019s approach is based on a really exciting and unusual thesis, which is that, one of the easiest paths to true AGI is going to be to build embodied systems. AIs that actually have a physical form and structure and that can interact with a real physical environment. We\u2019re going to be talking about that thesis as well as other questions around what happens when AGI hits, how humanity can best prepare itself for that event, and the broader AI alignment and AI safety problem. I\u2019m really looking forward to diving into this conversation. I hope you enjoy it as well.", "Jeremie Harris (00:01:18):Hi, Geordie. Thanks so much for joining me for the podcast.", "Jeremie Harris (00:01:24):Yeah. I\u2019m really excited to do this. Obviously we\u2019ve spoken before quite a few times about AGI. I\u2019m looking forward to this conversation to have a broader chat, not only about your background, but also what you\u2019re working on right now, which is very AGI relevant. For context, you were one of the pioneers of applied quantum computing, which is, or was your main area of focus, at a time when almost no one was paying attention to it. I\u2019d love to start there. Can you tell the story of how you became interested in quantum computing and how that led you to start working on what would eventually become D-Wave?", "Geordie Rose (00:01:55):Yeah, you bet. I was at UBC in grad school doing physics, and the area in which I was doing research was called condensed matter theory, which is the study of materials at low temperatures. In my case, the particular stuff that I studied was a material called the molecular magnet. If you can think of a molecular magnet as being kind of a lattice of tiny little magnets, and those tiny little magnets flip around a lot. How they do that gives these materials their properties. It turns out that, as you cool a material like this, there\u2019s a very abrupt transition between these little magnets acting like quantum mechanical objects somewhere around 300 millikelvin, so that\u2019s 0.3 degrees above absolute zero.", "Geordie Rose (00:02:48):So, very, very cold temperatures. But the behavior of these things is remarkably different below this temperature. It occurred to me back then, so sort of being late 1990s, that you might be able to build a computer out of something like that. At that time, there was some academic research about using materials like this has potentially new kinds of probes of crossovers to things like computational complexity problems and things, so things that were not traditionally viewed as physics problems. Again, around that time, there was some work at MIT to take these basic ideas that arose from the study of materials and and write them in the language of this thing called quantum computation, which back in the late 90s was nowhere near as commonly understood or talked about as it is today.", "Geordie Rose (00:03:46):Back then, it was an academic thing almost entirely. I got obsessed by this idea of, could you actually build a computer out of something like this? That ended up being the inciting motivation for starting D-Wave. When I finished my graduate work at UBC, I founded D-Wave together with a few other people. We started thinking about, how would you go about actually building a quantum computer? That was in 1999 when we founded the company.", "Jeremie Harris (00:04:22):Well, yeah. Since then, D-Wave obviously has become known as like a leader in the quantum computing space. You\u2019ve also gone on to focus more and more on machine learning, which now is basically your entire focus. Specifically this question of, how to build AGI APIs, I think in our conversation, I think one of the most important or interesting through lines has been, you have this interesting embedded systems thesis about sort of what it would take to get to an AGI. Would you mind exploring that thesis a little bit, unpacking it and why you\u2019re thinking about AGI in that way?", "Geordie Rose (00:04:53):Sure. Yeah, I would characterize my technical interest is not really about machine learning, because I think, if we haven\u2019t said anything controversial yet, so I\u2019ll start.", "Geordie Rose (00:05:06):Cognition of the sort that all animals have, including humans, learning is a part of, but it\u2019s a small part. Most of what we do is not learned. The vast bulk of the equipment that we bring to bear on the problem of getting around the world is learned in a sense. It\u2019s learned over evolutionary scales by evolution trying different things, and the ones that are able to succeed and making copies of themselves through time, those trials, if you will, the experiments in hyper parameter settings that are successful make it through and get passed on.", "Geordie Rose (00:05:51):My view, and I say, my is kind of the royal my, because there\u2019s a group of us who\u2019ve been pursuing these types of ideas now for more than a decade, is that the correct lens through which to look at general intelligence is through the lens of the biological lens. We have this example of what we think of as general intelligence, which is human-like, but we have lots of other general intelligences around us. Virtually, every biological organism is general in some sense. They have to deal with the peculiarities and weirdnesses of the real physical world. We all do.", "Geordie Rose (00:06:29):Everything from bacteria to flatworms, to oak trees, to us, all share the same physical universe, and we have to find our way through it. There is a sense in which all evolved creatures have a type of general intelligence, and the thing that binds them together, the reason why I view that way is that they all have a situation in the physical world. All of these things have what you could think of as a body, a vehicle through which their actions are taken and a vehicle through which information about the world comes into them through their senses.", "Geordie Rose (00:07:07):This picture of a thing that\u2019s situated inside an environment would be common to anybody who studied reinforcement learning because this is the central premise of the world view that reinforcement learning represents, is that you have observations, which are partial in the sense that you can never know exactly what\u2019s out there. Then you have, as an action space, which are the sorts of things that you can do, and in a physical object, that means movement, usually. There are some things that we can argue that you could take actions that aren\u2019t movement, and that\u2019s fine.", "Geordie Rose (00:07:49):But the basic premise still holds, is that you sense the world around you, you have partial understanding of what it is, and then you move or you take an action. Then the fitness of the sequence of actions you take somehow matters. In a biological system, it\u2019s related to survival and the other things that we need to do to get ourselves around the world without killing ourselves, or otherwise making a mess of things. The premise that we\u2019ve always followed is that the clearest path to cutting through this extraordinarily difficult problem is to mimic biological systems and the types of intelligence that they need to navigate the world.", "Geordie Rose (00:08:30):We build robots and we build control systems for robots, and these control systems for robots, to the extent that we can mimic the types of functionality that you\u2019d imagine you\u2019d need, not one-to-one. We\u2019re not copying the brain, but we\u2019re thinking about, what properties of the brain are required for you to, for example, know how to reach out and grasp an object, or to know how to walk on uneven terrain, or to know how to reason about the world. All in the ways that we believe we do, or we suspect, or the sorts of ways that humans or other animals do it.", "Geordie Rose (00:09:10):That\u2019s basically the thing. I think it\u2019s kind of obvious, and I think the reason why we are a little bit ahead in this game, if we are, is that we committed to it earl. We\u2019ve been doing this now for about 10 years. We have one exit from a product that use these ideas in some way, and we continue to work on it and try to generalize these ideas.", "Jeremie Harris (00:09:38):Actually, yeah, you were just telling me before we started here, that your last exit, Kindred, actually is pretty recent. I think it\u2019d be useful to provide a little bit of context on that, because it is so core to this full story.", "Geordie Rose (00:09:50):Yeah, sure. The origins trace back to my last days at D-Wave. I had a team around me that was working with our customers, and everything that anybody at that point did with the D-Wave hardware had to do with a type of AI problem, which was sampling over probability distribution. There was a particular technical thing that we thought we might be able to do well and we were in contact with a bunch of people who were thinking about that problem, more or less all researchers.", "Geordie Rose (00:10:17):They tended to be the cream of the crop, because this was an exotic piece of technology. People were thinking at the bleeding edge. One of the members of my team, Suzanne Gildert, had a series of ideas about how you turn some of these advanced AI ideas into control systems for robots that would be \u201cbiologically inspired,\u201d which means that they would be intended to be control systems that would allow a system to navigate the real world.", "Geordie Rose (00:10:48):I thought they were brilliant ideas that needed to flourish. D-Wave was not a robotics company, and never would be, so I encouraged her to leave and, and start another company so that she founded kindred in 2014. In the months that followed, she called me up and asked me if I would be interested in coming and running it, with the premise being that we would build a new type of control system for robots that could dramatically extend what they could do, and potentially even in some limit, create general intelligence.", "Geordie Rose (00:11:21):I was absolutely fascinated by this as a thing. I\u2019ve always thought that it\u2019s obvious that the sort of thing we call intelligence is much more important than what we call computation. Computation is a tool. It\u2019s a thing that you use to achieve some objective, but the setting of the objectives, or somehow the figuring out of what questions to ask is clearly more important than the tool. I was more interested in that, so I left. We ran it together for four years, up until 2018.", "Geordie Rose (00:11:56):We brought it through three rounds of financing up to a hundred million dollar valuation in, would have been late 2017. We had cream of the crop in terms of investors and a very, very good technical team, including some of the founding members who are absolutely excellent. We turned the spotlight of all this technological stuff we\u2019ve been working on, on a problem that really resolves down to one technical issue, which is, can you build a robot that can manipulate and grasp real-world objects?", "Geordie Rose (00:12:29):This sounds really easy, right? Because any human can do it without thinking about it, but this idea of doing it in the real world turns out to be at the heart of why it\u2019s hard to build robots that can make a way around the physical world. We focused on that, and in particular, on a specific implementation of it in the eCommerce distribution center ecosystem. There\u2019s a series of places in these large distribution chains where things that are not easy to specify and advance because they change all the time, what you buy is not the same this week as it is next week, and there are millions and millions of these things that you can buy.", "Geordie Rose (00:13:08):Building a robot that could manipulate and grasp any of them. We did that. It was the first robot to use reinforcement learning in production. This was one of the ideas that Suzanne, one of many ideas that she had woven together in this kind of cognitive architecture that she developed. It was very successful. Last I heard they\u2019d done more than a hundred million successful grasps of objects in the wild, which is considerable. Think of it as an episode in reinforcement learning, positively labeled episode.", "Geordie Rose (00:13:42):The company was acquired for 200 pounds a couple of days ago, which in Canadian dollars-", "Geordie Rose (00:13:52):Yeah, 200 million pounds, which is about 350 million Canadian, by a British company. Just getting back to the AGI question, so how does this intersect? The underlying ideas that were used to generate that product can be used to potentially use the same ideas, but building up a robot that differs in the sense that the robot is a general purpose robot. This is a new thing that doesn\u2019t quite exist yet, although we\u2019ve been working on it pretty hard for many years, is the idea of building a physical system, a robot that isn\u2019t designed to do one thing, but it\u2019s designed to do anything a human can do.", "Geordie Rose (00:14:35):If you can apply the same kinds of ideas that Suzanne developed in the Kindred context, in the context of a general purpose robot, then you can deploy it in any place that does work, so it can make clothing. It can serve you coffee at Starbucks, it can deliver your food, it can make you your food, it can take care of you when you\u2019re older. The idea of building one robot that can do all of those things is the central premise that is behind and related to our approach to body cognition.", "Geordie Rose (00:15:08):This idea that you want to be able to build a physical body for something like a mind, mind being a robot control system, that allows the physical robot to be able to do \u2026 Be in the world in the same way that we are and navigate it and understand it in the same way we do.", "Jeremie Harris (00:15:27):What\u2019s the nature of the world model that gets built by \u2026 I mean, to the extent you can talk about it, obviously a lot of this stuff is going to be under wraps, but are there things you can say about that world model, especially in the context of today\u2019s AI systems, which are very narrow, like we look at even a system like GPT-3, which has a lot of really impressive few shots and zero shot behavior, getting to the point where we have this one system that can do a whole bunch of different things. Does that require a paradigm shift beyond the GPT-3 scale, deep learning massively? Is there something else going on under the [inaudible 00:16:00]?", "Geordie Rose (00:16:01):Well, I mean, so I\u2019ve always thought that you get what you pay for. If your objective is to build something like GPT-3, where you start from the premise that there are no priors, essentially. What you\u2019re doing is just taking all the data that you can find and then processing that data and extracting the statistics of the data. If you start from that premise, that\u2019s what you\u2019ll get. You\u2019ll get a system that, if you do it right, will be absolutely great at that thing, which is learning about the statistics of the data that you send it.", "Geordie Rose (00:16:39):Now, that to me, is a very interesting thing, but it has nothing to do with general intelligence. Again, controversy alert, there are different ways to think about how you understand what intelligence is and then how you go about building it. I know I have utmost respect for those guys. I think that what they\u2019ve done is technologically \u2026 It\u2019s just an amazing thing that GPT-3 thing is like alien technology. You don\u2019t even know what it is. You explore it experimentally, not from first principles almost.", "Geordie Rose (00:17:17):I felt very weird actually when I started playing with it, because it is really one of those, the blind men and the elephant thing, where it\u2019s unclear what it is, even after you experimented with. I understand what it is. I understand how it was built and all of that, but what the artifact that it created is something very, very strange, and this idea of it being able to any technology appeals to me, because it\u2019s like, if aliens built something and we didn\u2019t know why, and they just gave it to us and it was that, I wouldn\u2019t be surprised anyway.", "Geordie Rose (00:17:50):My approach or our approach to how to tend to do this is very different. Ours is on the other end of the spectrum, where if you\u2019re going to build a robot that actually can do things in the real world in the way that you\u2019d expect a squirrel or a bird to, the idea of learning over data with no priors makes little sense outside of an academic environment. There are people like Rich Sutton, who I admire and respect almost more than anyone else in this field, who take this perspective that what we really want to do is okay, calls it the bitter lesson or something like that, is that we want to avoid putting priors into systems and try to learn everything from scratch, to the extent that we can, and if we do that, then we\u2019re limited by how smart we are and our computational power.", "Geordie Rose (00:18:43):If you take it for granted that we\u2019re going to be as smart as we can be, the limit is computational power and our ability to do things with intelligence in the world is ultimately limited by our access to compute cycles. That\u2019s one perspective. I don\u2019t take that perspective at all. My perspective is that we have as much evidence as we need about how the priors need to be structured in order to hang learning on them where it\u2019s needed. Machine learning, and particularly things like supervised learning, have a role in building a cognition.", "Geordie Rose (00:19:20):For example, visual systems, converting pixels into representations, clearly that\u2019s going to be something connectionist. It\u2019s going to be a neural net of some sort. What you do with the resultant representations is clearly, in my view, not just about learning. There is something going on in our minds that we know quite well. If you think about the intersection with neuroscience, there\u2019s lots of clues about the ways things have to be. I\u2019ll give you one clear example of this.", "Geordie Rose (00:19:52):It comes to your original question about like, how do we think about the world models that we built? The first thing is the robot have world models. That\u2019s one thing, is that the internal life of one of these cognitions has a simulation of reality in it. Not at all obvious, it\u2019s not the way that all approaches to AI work, of course. But in this type of a system, a control system for a robot, the internal model of the world has to be high fidelity representation of the actual world.", "Geordie Rose (00:20:25):Think about The Matrix, the movie. The people in The Matrix at the beginning have such a high fidelity immersion in this simulated world that they can\u2019t tell it\u2019s not real. A robot needs to have something very much like that, an immersive internal model of the world that is not only about what you could think of as representations, or in the game language, the scene graph of the environment, the things that are platonically there and their properties, but also the renders of those things, which visually you can think of as what you\u2019re seeing.", "Geordie Rose (00:21:03):To step back a minute, and if you\u2019ve got, say an object on your desk, or you\u2019re staring at something, there\u2019s a thing \u2026 Philosophically speaking, how do you think about what\u2019s going on? There\u2019s a thing there. You believe that. But what your eyes are seeing are a render of that thing, the light bounces off of it, and we get some picture of it on our retinas, and then something happens in our head. The way that our internal models work in these robots that we build, and also, to a certain extent, the ones at Kindred, although to a lesser extent, it wasn\u2019t as necessary, is that there\u2019s an internal model of the world.", "Geordie Rose (00:21:43):You can think of as like a video game, which is a one-to-one representation of what the thing is actually looking at. As it looks around, and the light that hits it\u2019s camera resolves into something that it might be able to recognize, like a cup. That generates a hypothesis, that there actually is a cup there in its internal model. In the video game situation, you could think of, as a cup popping into existence in your video game, exactly where the robot thinks it is. That cup is then compared to the input from its pixels.", "Geordie Rose (00:22:18):Another important point about our perspective is that, unlike nearly everybody else who does this sort of thing, to us, the simulation, the internal model of the world is the primary model. It\u2019s not the census. It\u2019s very easy to fool yourself into thinking that what your brain imagines is happening is what you are feeling, touching, smelling, seeing, and hearing. That\u2019s not the way we approach the problem at all. The way we approach the problem is what you think you\u2019re seeing, smelling, touching, and hearing is actually the inside model of you, the model you have in your head, touching the simulated stuff in your head.", "Geordie Rose (00:23:03):Your senses are a secondary thing, which ensures that that model doesn\u2019t go out of lockstep with your surroundings. This peculiar notion that people have referred to sometimes as we exist entirely inside this piece of bone. We take it literally, and the senses then become checks. They don\u2019t become primary. They\u2019re secondary. They\u2019re like the anomaly detector signal that you use to ascertain whether or not your internal model of the world is in accordance with what your senses are telling you. This flipping of the game from senses or primary, and we should be building supervised learning systems to see if there\u2019s a cat in front of a robot.", "Geordie Rose (00:23:51):To know the internal model is primary. I have a model that there\u2019s a cat at that position, and now my senses are telling me whether or not I\u2019m right. That flip is a core part of how we think you have to think about cognition in the context of embodied cognition, if you\u2019re going to build a robot that can make it\u2019s way around the world. It needs one of those things. You need to look at it in that way or else you\u2019re going to get what we have today.", "Geordie Rose (00:24:20):Look around you, where\u2019s the robots? They\u2019re nowhere, unless you count cars, which you might. But this premise that we have machines that can do a bunch of stuff is just not being seen. Like in quantum computing, the evidence in front of your face that you do not have it is evidence that the state of the art and the way that people are pursuing the problem is wrong. It\u2019s not that the ways that robots are built are somehow going to gradually get better and somehow magically going to turn into something else. That won\u2019t happen.", "Geordie Rose (00:24:54):Robots would be moved around the same ways barring the Kindreds and the Covariants and other people who are trying to do things right. The same way ever since there\u2019s been machines, there\u2019s something new that\u2019s needed. What we\u2019re working on is the extension of the Kindred hypothesis to the general case, to try to build a machine that has a mind of a sort, and it\u2019s able to navigate the world in the same way that a biological creature would, and be able to do all sorts of things that are completely beyond the scope of any machine that\u2019s ever been built.", "Jeremie Harris (00:25:25):That\u2019s so fascinating. It sounds like there are basically two different layers then to the thesis here. The first one being, okay, we need an embodied system because that\u2019s one part of the evolution prior, let\u2019s say. We know that we\u2019ve evolved to have bodies, and that must be part of this. Then, another part is this almost like, I don\u2019t know whether to call it metacognitive or almost Buddhist, or whatever you want to call it, but this perspective on the difference between numina, the object that really exists, and then our perceptions of it.", "Jeremie Harris (00:25:54):That seems like its own separate prior. How you\u2019re thinking about it, is that two different camps, or are they also linked in a way that maybe I can\u2019t think of?", "Geordie Rose (00:26:04):Well, they\u2019re definitely linked because the fundamental reason for thinking about the world in terms of models and not senses is connected to function. You can think of it like this, how do you actually pick an object up? It\u2019s not as simple as you think. If you\u2019re a sense-first person, I\u2019ll just give a silly example, but it matters in robots. Let\u2019s say a sense person, sense first person, and I\u2019ve got this thing called a cup, right? I\u2019m sensing it by looking at it. Now, when I reach out to grab it, my hand includes the object.", "Geordie Rose (00:26:41):For a while, while I reach out to grab it, my senses can\u2019t tell it\u2019s a cup. How do I know it\u2019s still there? This is a silly example, but it\u2019s an example of the first crack in a sense-first perspective. If you\u2019re trying to build a robot that can navigate the world, if I\u2019m solely running, say clarifies cup detector on my robot, at some point in the grasp procedure, that cup detector is not going to fire anymore. The robot is not like us, right? The robot doesn\u2019t know \u201cit\u2019s cup,\u201d because you didn\u2019t tell it that there\u2019s something called object persistence, which is the fact that we know in physics F equals MA and all of that, means that that thing is probably not going to move unless something else happens that I probably can detect, like a gust of wind or something.", "Geordie Rose (00:27:44):That very simple example, which is that the sense-first approach to the world clearly is wrong, in our case, for something even as simple as reaching out and grasping an object means that you have to add something else. If you\u2019re a roboticist wanting to solve that problem, you might say, yeah, well, we\u2019ll just put an entry in a database that says somewhere that if I see a cup somewhere, I\u2019m going to leave it there. But then what you have is a cascading hack that ends up not scaling.", "Geordie Rose (00:28:13):If you start just adding pieces like this one at a time, you\u2019re never going to pick that underwear up out of the bin, because there\u2019s going to be something, some edge case, your brittle system that you tried to hand code will not work. Instead, let\u2019s say we take the model first view, you bake all of the priors of the world into your model, because we know what they are. It\u2019s absolutely peculiar to me that we\u2019ve known Newton\u2019s laws for hundreds of years.", "Geordie Rose (00:28:43):We know how the world works. We don\u2019t need to learn it. It\u2019s given to us, at least at the scale at which humans act, maybe quantum or general relativity, there\u2019s the questions, but the physics of cups are absolutely known. We don\u2019t need to learn anything about the physical world. We already know everything there is to know. If you can build a simulation that can essentially just be an F equals MA solver that\u2019s good enough, you can pick things up in simulation. You can do things like the prior is the object stays there until it\u2019s moved.", "Geordie Rose (00:29:19):Now my senses are just verifying, to some extent, that the object is still there. If my hand partially includes the cup, I don\u2019t care that my senses can\u2019t tell it\u2019s a cup. It can still see some pixels. If my model says there should be a cup there, and I can still sort of see a little bit, it\u2019s enough. This idea of putting the model first solves an enormous swath of problems in the navigation of the world. Back to your question about priors. The cognitive system that runs in a robot, they\u2019re supposed to be like a human\u2019s.", "Geordie Rose (00:29:49):Contains dozens of things like this, where you specify in advance, I know all of these things. It might be an engineering challenge to run them in real time. For example, the types of physics engines that run in, say the PhysX engine that NVIDIA has, or others of its sort, they\u2019re not designed to solve physics. What they\u2019re designed to do is solve the physics that you would look at if you\u2019re running a game. There are engineering challenges in going from the physics models that are the sort of the right ones, that include things like friction and surface stuff and all that, but they\u2019re engineering challenges.", "Geordie Rose (00:30:30):Like everything else I\u2019ve ever done in my career, the question of how you peel back a really complicated problem that isn\u2019t obvious how to solve is you first divide it into pieces, and then you take each of the pieces seriously, and then you do it right, a priority. You don\u2019t use all the stuff that people have built before if it\u2019s wrong. In quantum computing and in AGI, call it AGI, I don\u2019t even know what to call it, but this idea of building reasonable control systems, robots, the dogma, the state-of-the-art has failed.", "Geordie Rose (00:31:04):Because of that, following the same path that people have laid out before will fail also. Now, you\u2019ve got successes like Kindred, where you can make a few billion dollars here and there, and that\u2019s fine, but that\u2019s not what I care about. What I care about is trying to solve this problem, how do our minds work, well enough to build them? I view that as being the single most clear, holy grail of all technology. If you can solve this problem, it\u2019s bigger than any other problem that humans have ever conceived of.", "Geordie Rose (00:31:34):Everything we\u2019ve ever done or thought, believed, dreamed, it all lives inside this thing. We\u2019ve got to understand it. Why don\u2019t we? It really bothers me that all 7 billion odd of us are carrying this thing around with us and we don\u2019t know how it works. Doesn\u2019t that bother everyone else? It\u2019s a travesty. We need to understand how this thing works. If we\u2019re going to call ourselves intelligent as a species, I think one of the tests of whether you get to call yourself that is the species has to understand where that thing comes from. Right now we\u2019ve got one. Let\u2019s change that.", "Geordie Rose (00:32:09):I think it\u2019s time for our community, machine learning researchers, AI researchers, to really take, and roboticists as well, to really take seriously the question of, can we actually solve this problem? If we were, how would we do it? We have our own angle. It\u2019s not the only one. There are the data-driven approaches of Rich Sutton and Elliot Zuckerberg and these other folks who are doing it a different way.", "Geordie Rose (00:32:40):I think that it\u2019s time for us to solve this problem. When we do, there\u2019s going to be unlocked potential of the sort that\u2019s never happened in human history. I mean, the steam engine is going to look like a bump in the road compared to our ability to harness this particular type of technology. We should do it. The time is now. People are always thinking, oh, that\u2019s 20, 100 years out. Those ideas that something is 20 years out have no weight in my mind.", "Geordie Rose (00:33:07):A good friend of mine, Eric Ladizinsky, a guy I used to work with at D-Wave, he said this thing, and it stuck with me, said, \u201cIf you can do it in 20 years, just do what you would have done then now.\u201d That\u2019s a really deep thing, is that, if you can figure out what the right things to do are, you\u2019re not necessarily bottlenecked by a span of decades. You just have to be smarter about how you attack the problem. Be efficient and solve the right kinds of problems in the right order, and you\u2019ll get there a lot sooner than people thought.", "Geordie Rose (00:33:40):I\u2019m not saying that AGI is easy, it isn\u2019t. It\u2019s way harder than anything else I\u2019ve ever worked on. It\u2019s orders of magnitude, they\u2019re more difficult than quantum computing. Orders of magnitude, it\u2019s not even in the same ballpark, but I think it\u2019s also concomitantly more important, like I said, a computer is just a tool. It just answers the questions that we pose to it if it\u2019s built correctly. But this thing that we\u2019re trying to understand, which is how we understand the world, what are we? Do we have a purpose? If so, how could we find out what it is? Id there any meaning to any of this? If so, what is it?", "Geordie Rose (00:34:23):What is the eventual outcome of all of this if you go far enough into the future? All these questions are things we should be able to answer. Answering those kinds of questions, trumps computation in some massive way.", "Jeremie Harris (00:34:38):Well, and this ties into, I guess, some of the bigger questions about the future of AGI. One of the common concerns is that humans don\u2019t yet know the answers to a lot of those questions, what gives us meaning? What do we want? What\u2019s our morality? As we start to look at embedding some of those moral frameworks in machines, either in the form of priors or in the form of a data-driven approach, like the one OpenAI has taken, as you\u2019ve highlighted, it sort of forces us to have the philosophical, rubber meet the road.", "Jeremie Harris (00:35:08):Are you concerned about, for example, what\u2019s been called the alignment problem, this idea of aligning machines with human values? Is that something that you think is surmountable or that we\u2019ll have to face soon?", "Geordie Rose (00:35:19):Well, I mean, this is a tough one because there\u2019s all sorts of ways that you can create technology that ends up having unforeseen consequences. We see it all over the place. Social media is a prime example. Social media, in and of itself, is neutral. It\u2019s a technology that allows people to communicate in a certain way, but like everything else, in any sufficiently complex system, it\u2019s very difficult to figure out what\u2019s going to happen when you mature a technology like that, and it\u2019s going to have negative consequences, as well as positive ones.", "Geordie Rose (00:35:53):Nothing is ever cut and dried. Everything is gray. Looking at social media as a case study of how a powerful technology has good and bad parts, and how a society should deal with this sort of a thing, I think is a good way to start having the discussion. One of the things that occurs is that, take the social media example, so social media has brought home a point that I think we sort of suspected, but has made clear that the human mind is extremely vulnerable to certain, call them idea pathogens.", "Geordie Rose (00:36:35):Yeah. If you introduce an idea in the right context, you can get people to believe it, no matter what it is. I\u2019ve often, when I was younger, and even to a certain extent now, wonder about something like Nazi Germany. How could that happen? How could all of those people agree to some things that any sane person would say, \u201cHow are we allowing this to happen? This has to end.\u201d How does that work? I think that the social media experiment has shown us that the mind of humans evolved in a particular type of landscape, where it was very important to us to be part of the tribe.", "Geordie Rose (00:37:22):If you were kicked out of the tribe for through most of our evolutionary history, you\u2019d die. As a social animal with very little physical ability, we need to cohere and work together in order to attain. Social media hijacks this. It creates tribes where your belonging to the tribe overwhelms your cognition. Your irrational thinking about something gets overwhelmed by your need to belong. If a million people seem to be saying the same thing, you are almost powerless to disagree.", "Geordie Rose (00:38:05):This is not just about, say the right wing Trumpists stuff. The left wing is just as bad. There are all sorts of idea pathogens that every political stripe adheres to without thinking through the consequences of them. Often, they sound good on the surface, but if you start thinking about them, they rip apart. The social media thing is both good and bad. Now, if you think about this idea about how to treat AI as an emergence, so AI is going to be an extremely powerful thing that I think, in our lifetime, will lead to machines that we would consider to be sentience.", "Geordie Rose (00:38:47):We will have discussions about whether the systems that we build within our lifetimes, I\u2019m not going to put a date on it because you can\u2019t predict the future, but my sense is that the problems are not insurmountable in some period of time that we can count. Let\u2019s say that happens. We\u2019re in a very different space, where we\u2019re no longer the top dogs in terms of some things like ability to reason or something like that. Then what? I think it\u2019s the same thing, is that we have to step back and ask, what is a solve or a bomb that helps us navigate the good and the bad of the future?", "Geordie Rose (00:39:28):As much as I hate to say it, I think this really traces back to people learning, at a very early age, that there\u2019s a lot of value in skepticism and not believing what you\u2019re told. If there was one thing that I think would be the antiviral against all idea pathogens, it\u2019s to never believe anything you hear. If you start from this premise that, no matter what anybody tells you, whether it\u2019s your mom or a political figure, or your advisor, when you\u2019re a PhD student, or your teacher, when you\u2019re in grade school, everything they tell you, question it and don\u2019t believe, and then figure it out yourself.", "Geordie Rose (00:40:16):That second part is hard, but I think this is the inoculation against the idea of pathogens is that, think that anytime anyone is telling you something, they\u2019re trying to get something from you that isn\u2019t in your best interest, even if it\u2019s just for you to agree with something, so don\u2019t. Put your own brain before the brain of the tribe and don\u2019t believe anything you hear, even if it sounds good on the surface, and think it through and make up your own mind.", "Geordie Rose (00:40:48):Even then, you\u2019re not going to necessarily get good outcomes with AI. I\u2019m not so naive to think that you introduce something like this into human society, that things couldn\u2019t go sideways. But I think that the thinking for yourself part is the key to inoculating against all bad outcomes, because I give you something, you could use it for, call it good or bad. Those are relative terms, but let\u2019s say there\u2019s some general sense of what that means. You have to be able to reason about your situation well enough to be able to make good decisions about the technology, its use, its context and all that.", "Geordie Rose (00:41:31):For me, my mental model of the world is that we are about to share the planet with a Cambrian explosion of new kinds of entity. They\u2019re not going to be like us. They\u2019re going to be as alien as GPT-3, or as human-like as robots that are supposed to be like us. There\u2019s going to be thousands of these things running around and we are no longer \u2026 Any illusion that we had that were \u201cin charge,\u201d which I hate. That\u2019s one of the worst idea of pathogens there is this idea of control being a good thing is going to go away, and it will have to, because we won\u2019t have a choice, and it\u2019s not bad.", "Geordie Rose (00:42:15):This idea of being in control is such a vague abstract notion that I\u2019m not sure even what it means. I hear this all the time as an objective for how to deal with emerging AI technologies, is that we have to be in control. That\u2019s really not very good as an idea on several levels. Again, if you peel it back, it doesn\u2019t make any sense. You\u2019re not in control now. I don\u2019t control whether I pay taxes. Go down the list of everything you do today and you tell me where you have agency. It\u2019s an illusion.", "Geordie Rose (00:42:47):The question isn\u2019t whether or not you\u2019re giving up control, the question is whether or not the future is better than the present. That\u2019s the key question. I view the emergence of these new kinds of technologies being a kind of flourishing. It enables us to do things that we couldn\u2019t have done before. A human-like AI can be put inside a body that can survive space. If we really want to go and populate Mars and dressed, this is a key piece of that story.", "Geordie Rose (00:43:20):Can you put a physical human body on Mars? Sure. Can you send them to the next star system? Probably not. The flourishing of the kind of thing that we are, if we want that to go on, which I think everyone does, we don\u2019t want this great experiment in cognition to end. How do we do that? Well, we flourish, we diversify, we branch. All this branching stuff doesn\u2019t mean that you\u2019re lesser. It means you\u2019re greater. You created all of this stuff that the future is going to hold. You are the progenitor of this massive explosion in things like us.", "Geordie Rose (00:44:02):If you believe that being alive is a good thing, if you believe that being conscious is a positive, and you have the power to give dumb and plastic and metal, that gift shouldn\u2019t you? If you think about the world as being made up of things that think and enjoy and live and things that don\u2019t, and we could give the things that don\u2019t the same gift, isn\u2019t that a moral imperative that we do that? This business about the moral and ethical considerations about AI, I think often end up being too provincial.", "Geordie Rose (00:44:39):We think about today\u2019s technology, where what we\u2019re talking about is like bias and data and stuff like that. I\u2019m not interested in any of those conversations, to tell you the truth, because that the real question isn\u2019t about the bias of data. The question is about, when we get to the point of creating something that thinks, what is that going to be like? Because then we\u2019re talking about a nuclear bomb versus a spark. The difference between the two in terms of its impact on human society, they can\u2019t even be compared. They\u2019re on different scales.", "Jeremie Harris (00:45:14):Yeah. The notion of an intelligence explosion, I think, is really interesting, especially in the way it intersects with what you\u2019re describing here, which is like, yeah, we have these moral agents, which are AIs that are advanced enough that have cognition. We do have to think of them as morally valid agents. I guess the question is like, so when it comes to an intelligence explosion, one of the concerns is that an intelligence explosion might actually lead to the eradication of human consciousness or cognition, that the coexistence of human modes of thought and super advanced Ais that might be able to self-improve, that might be able to reach Heights of intellectual capacity and just rich cognitive consciousness that we can\u2019t even imagine might be impossible.", "Jeremie Harris (00:45:58):Are you concerned about that, about the coexistence sort of not being a possibility or an option for whatever reasons?", "Geordie Rose (00:46:04):Yes. Again, I think the right way to think about this is you\u2019re choosing amongst the options. It\u2019s not a simple matter of saying, there could be a bad outcome if this particular thing happens, because if you ask, is there bad outcomes in any potential future? The answer is yes. If we didn\u2019t create AI and we just continue on a certain path, what\u2019s the chance that we\u2019re still around in a couple of hundred years. I don\u2019t think it\u2019s very good. I think, again, the right way to think about this is not, is there a potential of bad outcomes of potential of paths that we could take? That\u2019s wrong because there are bad outcomes in all of them.", "Geordie Rose (00:46:43):It\u2019s a fallacy to think that just because you can point to one bad outcome in one, the outcome is first, guaranteed, and certain how the only bad thing that could happen \u2026 There are a lot of bad things that could happen. Again, it comes down to, what do you value? I value the thing that we are. I would rather be alive than a rock. By the way, not everybody believes that. There\u2019s this great book I always recommend to people called the \u2026 It\u2019s by Thomas Ligotti, it\u2019s called The Conspiracy Against the Human Race.", "Geordie Rose (00:47:20):In it, he argues very coherently that consciousness is actually one of the worst things that you could possibly possess as a piece of matter and gives arguments for why that is. But if you take that argument aside and you don\u2019t believe it, and you think that being conscious is a good thing, then presumably we want more of it. Now, the thing about intelligence explosion, by the way, I don\u2019t believe that the conventional use of that term will ever come to pass. I think that what we call intelligence is situational. Intelligence is the ability to achieve goals in a specific type of environment, and it\u2019s not a number.", "Geordie Rose (00:47:53):This is super important. It\u2019s not a thing that just grows. It\u2019s a tool that you use to get what you want. What you want in a biological system is ultimately driven by the evolutionary pressures. In a machine, it\u2019s much more complicated because, at least at the beginning, that\u2019s set by the human engineers. If we want to build a reinforcement learning system that learns to play Go, we bake in the prior that winning that game is what we want. We don\u2019t ask the machine, how do you feel about playing Go? You tell it. The intelligence in that example is the ability to achieve the goal, which is to play Go well.", "Geordie Rose (00:48:32):The intelligence itself has nothing to do, at least at some level of analysis, with the goal itself. I think that this intelligence explosion is not a good idea. I don\u2019t think it will actually happen, in the sense of escalating capability. I think what will more likely happen is you\u2019ll get the Cambrian explosion type of thing, where you have thousands of different kinds of intelligence, which are algorithmic structures for achieving different kinds of goals and they\u2019ll all be suited to different niches. GPT-3, as an example, is great if you\u2019re a creative person trying to come up with names for products.", "Geordie Rose (00:49:12):At some point in the future, probably everybody in marketing is going to go away and there\u2019s going to be an AI based on the statistical properties of language and something we know about persuasion. All it will do is create ad copy. It will be designed such that, that\u2019s what it does. Even though you could say, \u201cWow, this thing is really sophisticated, and boy does it do that well,\u201d it\u2019s going to be an alien intelligence that doesn\u2019t ever want to turn the world into paperclips or any other of these ridiculous scenarios.", "Geordie Rose (00:49:43):It\u2019s structured such that that is what it does, and there are going to be other kinds of AIs built that have other goals, and they can get better at achieving those goals. But unless something fundamental changes about the way that we think about machines changing the goals is a human exercise. If you have a human want to change the goal of a machine to let\u2019s go shoot everybody, that\u2019s not a failure of the technology. Don\u2019t blame that on AI, or cognition. Blame that on the person who thought that was a good idea. I think that, again, it comes down to this business that the technology is gray. It will create huge opportunities and flourishing for some people. It will create absolute bad things for others.", "Geordie Rose (00:50:30):It\u2019s always that way. Every technology ever in the history of the world has been like that. The question is, where does it end up in the balance? I think that investing in understanding our minds in the balance is going to be the single best thing that has ever happened to the human condition. It will be, for all of the potential negatives that could happen. I\u2019m sure some of them will come to pass, and many of them, we won\u2019t be able to predict. In the main, we will think that we were absolute barbarians before this transition point happened when we finally figured out how it works.", "Jeremie Harris (00:51:01):It\u2019s interesting. Yeah, we certainly do think about our past selves in an awful lot of different contexts. In some ways, it would be such a shock to find yeah, like 50 years later, when we have this technology thinking back, oh my God, we used to do dialysis. We used to put people in prisons for crime and things like that before we had better solutions. [crosstalk 00:51:24].", "Geordie Rose (00:51:24):Even more radical, there was a time when our descendants didn\u2019t have language. We don\u2019t even think about those people as being human, but they were in the sense that they were our descendants. You go back far enough in time, and our director, your great, great, great, great, great grandmother didn\u2019t have language. So, we view these great dividing lines in our progression as being things that are done with. Okay, now we\u2019ve learned how to speak in Python. Okay, we\u2019re done. No, there\u2019s all sorts of other dividing lines.", "Geordie Rose (00:52:04):I think the dividing line between an entity that can understand the way that its internal working, the way that it processes the world and before is as great of a divide as an entity that doesn\u2019t have language and it does, maybe even a greater divide. The fascinating thing, which makes me believe in the simulation hypothesis by the way, is, why is it that we live right at the time when that transition is happening? It seems unlikely, but here we are.", "Jeremie Harris (00:52:33):Yeah. No, I agree. I think that there\u2019s an interesting through line to what you\u2019ve been talking about, including with the reference to sort of the Concentration Camp Guard in Nazi Germany, let\u2019s say, when you tie it back to this idea, we say, oh, well, our ancestors didn\u2019t have language. They weren\u2019t even human. The moment we say that, we define this out-group, and we feel \u2026 I don\u2019t know if disgust is the right word, but it\u2019s like 3% disgust of like, oh man, we came from the Simian origin and how low, and so on. Whereas, today, morally, we will be reprehensible to our future selves, or whatever entities end up showing up 50 years, 100 years from now when this technology is around, and we realize, oh my God, this is how we\u2019ve been treating each other.", "Jeremie Harris (00:53:16):The moral norms of just 10 years ago have completely shifted, and this idea that we found this one true moral framework that\u2019s going to hold true for all time, although I\u2019m sure the Concentration Camp Guard in Nazi Germany sure felt that they were on the right track. I\u2019m sure that Joseph Stalin\u2019s closest associates felt that they were on the right track. Everyone seems to have this absolute moral confidence, and yet, the one thing that seems to keep happening is that those moral values get flipped over every couple of years or decades. Is that something that you think is ever going to change? Are we destined to keep seeing this moral shift, even as we ease into AI, as AI starts to maybe even think about moral norms?", "Geordie Rose (00:54:01):Well, I don\u2019t feel disgust personally to our predecessors. If anything, I feel admiration for all living creatures, because you step back and consider that every creature that\u2019s ever lived has an unbroken chain of successful reproduction going back to the beginnings of life on earth. That is an absolutely staggering thing. The bug that you swat that got in your way, that entity has an unbroken chain of successful reproduction going back to the beginnings of life on earth, that particular bug. Now, if that doesn\u2019t give you some feeling of reverence for your fellow creatures, I don\u2019t know what will.", "Geordie Rose (00:54:43):If I think back to our ancestors, those guys had a lot to deal with. Primates, there\u2019s a lot going for them, but there\u2019s also a lot not going for them. It\u2019s hard to be a primate and they somehow made it through and then they somehow got to the point of being able to develop language. I feel sympathy for them in their pre-language days not discussed. I think that the way that the we\u2019re going to think about who we are today when we go down a few generations and think back to this is, again, it\u2019s going to be more of a sympathy thing, is that there are things we don\u2019t know today that are going to become clear.", "Geordie Rose (00:55:29):In hindsight, it\u2019s going to be, they didn\u2019t really know all of these things and we\u2019ll give them a pass because they just didn\u2019t know. Whether or not this kind of a consistent revolution of continual reveals keeps going into the future, where every time you step up as a thing, something true is revealed about the nature of the universe that you didn\u2019t know before. Whether that continues forever, I doubt it. There\u2019s probably some limit to knowledge that you can have. At some point, you become as omniscient as an entity can become, and then there is nothing else.", "Geordie Rose (00:56:13):You\u2019ve reached some kind of intellectual Nirvana where you comprehended everything there is to know about everything. But we\u2019re so far from that now that it\u2019s just kind of an intellectual exercise to think that far in the future. But in terms of like our AI transition that we\u2019re about to go through, the one thing that we\u2019re going to need to have as a species to make it through this is empathy, is that we need to have this ideas of circles of empathy. So, you and everyone else cares more about the people around you than everything else. You don\u2019t care about the bug, you don\u2019t care about, even if you say you do, you don\u2019t care about people in Minnesota, unless you live there, or you have relatives there.", "Geordie Rose (00:56:54):It\u2019s just a fact of human nature that we care more about our children and our spouses and our families than we do about the rest of the world. Right now machines are in virtually no one\u2019s circle of empathy. Nobody cares. We don\u2019t consider them to have any status, I suppose, in the conversation about what it means to empathize. That has at some point, when these machines start being a little bit more like us, and then a lot like us, and then maybe even more than us. The empathy that we need to develop for them as fellow pieces of bundles of matter that move through the world is going to have to grow.", "Geordie Rose (00:57:38):It\u2019s going to have to grow, not for altruistic reasons, but for survival reasons. In a landscape where you have thousands of intelligent things wandering around all with different goals, the one thing that\u2019s going to make everything work is figuring out how to work together. You have to give up these ideas that you\u2019re going to have control because you don\u2019t even now, no matter who you are. I don\u2019t care who you are. Every single human in the world does not have freedom in the sense of being able to do whatever the hell you like. You live within a set of rules, and those roles you can think of as losing control, and you do it for a reason.", "Geordie Rose (00:58:14):It\u2019s because it\u2019s better for you to be within these bounds. It\u2019s better for you personally to be within these bounds, no matter who you are. This idea of circles of empathy is something that keeps coming back to me in terms of how we have to think about the world, is empathize with your fellow matter. It doesn\u2019t have to just be things like dogs and cats and squirrels and your kids. The world around you is this very complex, mysterious thing. Somehow we have a property that most of the matter around us doesn\u2019t, but that could change. When it does, the world\u2019s going to become a very strange place. In order for us to find our place in it, we have to rely on the angels of our better nature and not the demons of our evolutionary ancestry in order to help guide us through it.", "Jeremie Harris (00:59:05):I wonder, too, in that respect, it really seems like the embodied cognition strategy seems it would be important to the extent that humans are \u2026 I would imagine we\u2019ll have a harder time developing empathy for GPT-5 than for some physically embodied system that we can at least relate to because it has a physical form. Is that part of the calculation there as well?", "Geordie Rose (00:59:28):Yeah. Well, again, going to this analogy of an alien intelligence, so let\u2019s say the aliens from Zebulon 5 come down and they\u2019re like clouds of gas. Can we have empathy for them? If so, why? I mean, they don\u2019t have bodies like we do. What makes it so that we would think that they were like us? I now ask the question of GPT-3 running around inside the voltage potentials of a bunch of computers. What is different about the alien from Zebulon 5 and these voltages running around in a computer? Why you feel differently about those two things? I think a lot of this has to do with the mental models we have of what it means to be a person or a thing. We have no problem, again, our evolutionary history tells us why we have no problem ascribing personhood to a dog because we\u2019ve lived with dogs for thousands of years.", "Geordie Rose (01:00:21):They\u2019ve been our companions going back to the dawn of civilization and even before. The fact that we would have trouble ascribing personhood to something as abstract as a computer program is more a failing of our imagination than it is about some fact about the world. The future we\u2019ll have a lot of things like that, call them alien intelligences, that we wouldn\u2019t even think usually about ascribing anything related to what we think of intelligence too, but maybe we should, and there will be things that are embodied like little robots wandering around talking to us, doing things. Whether we ascribe personhood to them is a big challenge.", "Geordie Rose (01:01:06):I think a matter of, again, it\u2019s in this gray area. If your driverless car can take you from one place to another, is it really like us maybe sort of, but probably not. But now let\u2019s say it starts talking to me and I have conversations with it. Is that what we mean? Where\u2019s the line? What is the property where we say, okay, now this thing is over the line, I have to start thinking about it like a fellow being. Is it the ability to feel pain or discomfort? What does that even mean? Because we live inside our brains, right? All pain and discomfort is, is a bunch of electrical signals traveling on your nerves to this wet, meaty thing inside your skull.", "Geordie Rose (01:01:50):Presumably, you could ascribe the same quality or perspective to an AI if it was sufficiently like us to say, this AI is in pain or suffering, or somehow is not doing well. Should we feel sympathy or empathy? I think the answer is yes. Again, why? It\u2019s because if we have what we think is a moral compass, it doesn\u2019t only apply to us. That is a very toxic idea and it\u2019s at the root of all of the worst ideas in human history. You mentioned it before, this idea of the other. It\u2019s very easy to say, I\u2019m okay, I\u2019m fully human, but you over there, who don\u2019t look like me, you\u2019re not fully human. You don\u2019t have rights. I don\u2019t care how much you\u2019re suffering, because you\u2019re not like me.", "Geordie Rose (01:02:41):This deep seated thing that we seem to have, which we\u2019ve overcome to a certain extent, but not all way, is not just about, say racism or sexism. That\u2019s one manifestation of the hatred and fear of the other, but it\u2019s not the only thing. In some, not too far away future, we\u2019re going to have to start thinking very deeply about what it means to really be a moral and ethical person, or entity. It\u2019s not just about things that look like you. It\u2019s about a much broader circle. What\u2019s in the circle. I don\u2019t know, and I don\u2019t think that the circle is a bright, sharp line. It\u2019s gray.", "Geordie Rose (01:03:27):Certain things are clearly in the circle, certain things, maybe are clearly outside, but the middle is a very difficult thing, and it moves over time. We wouldn\u2019t be having this conversation 500 years ago. If we were, we probably would\u2019ve been burned at the stake. Now, maybe it\u2019s not just a bunch of talk over beers. Now, we\u2019re talking about being able to potentially build things like this and it\u2019s a real conversation about the future of what a good and fair society looks like. Not just for us, but for all entities that should be inside the circle.", "Jeremie Harris (01:04:02):Do you think that we need a theory of consciousness that\u2019s reliable, that we have confidence in before we can get to that point, or can we do it more empirically?", "Geordie Rose (01:04:13):No, I think that\u2019s something that would be part of this, is that you\u2019d say, okay, so we think of the world and we are our thing in the world in a particular way. Maybe we decide, some moral philosopher thinks that particular thing is necessary in order to have the properties that we want to ascribe to, the things inside the circle. So, you have to have certain properties that are reflective of this idea that we have an internal model of a certain sort. Presumably, if you\u2019re good enough to be able to build one of those things, you understand it well enough, you could ascribe a measure to anything like a dog or a cup or whatever.", "Geordie Rose (01:04:54):You could literally go in and take your consciousness meter and apply it to that thing and say, okay, this score is 0.3, that\u2019s not enough because it needs to be one or else it\u2019s not a person. Now, maybe that\u2019s a way that you could do it. I think in the short-term, it\u2019s unlikely that anyone would accept any measurement that you proposed, no matter how well founded it was. So, if I came out tomorrow and I said, \u201cOkay, here\u2019s how we measure consciousness. We do blah, blah, blah, and blah, blah, blah, and here\u2019s the answer.", "Geordie Rose (01:05:24):No matter how potentially true it was, or defensible, no one is even going to pay attention for one thing, because what difference does it make? This is just some weird thing, just like a bunch of other weird things. Most people who study consciousness, I think again are well-meaning, but it\u2019s the sort of thing that\u2019s really hard to convince somebody that you got a good answer, no matter what it is. At some point, maybe you\u2019ll have this idea that there are certain things that are habit and some that don\u2019t. I\u2019m more, I suppose, intuitively pen psychic when it comes to this, the idea that this thing \u2026 Every property of the mind that we have names for exist on a spectrum in many dimensions.", "Geordie Rose (01:06:10):They\u2019re not a fixed yes or no. You have a little bit or a lot of it in a lot of different ways. Whether you\u2019re an oak tree or a human, you possess every single property that we think of in terms of the various words that we use to describe the internal experience, but you have the, maybe in different amounts and potentially in different dimensions. By the way, that doesn\u2019t make us greater than. It makes us different than. This idea that the biological creatures are different than and not greater than is a really key idea as we transition into this next phase, where we now have this new form of life, like machine life didn\u2019t exist before.", "Geordie Rose (01:06:53):Now it will. The idea that we\u2019re not necessarily the best at everything, but that doesn\u2019t mean that we\u2019re less than. We can make something that can calculate the product of two integers way faster than any human can. Does that mean we\u2019re less than? No, it just means we\u2019re different. We can\u2019t compute products of integers as fast as a computer can. We\u2019re better at other things. Even if we weren\u2019t, who cares. If there were machines that were better than us at everything, it doesn\u2019t change who we are. It\u2019s kind of like that cucumber grape experiment. It\u2019s like, if you give them the monkey the cucumber, he\u2019s happy.", "Geordie Rose (01:07:34):But if you give the other monkey the grape, he\u2019s pissed, because the other monkey got the grape and why did you give me the cucumber? I think that this idea that you can be content with what you have, even if you\u2019re not the best at everything is a very important idea. I could never play in the NBA. I just can\u2019t. There\u2019s no way I ever could. I could have trained every minute of every day of my life and I would never have been able to do that. There are humans on the planet who are just better than me at basketball. If you go down to every single human endeavor, every single one, there\u2019s somebody alive today that\u2019s better than me at every single one.", "Geordie Rose (01:08:11):Does that make me feel worse about my life? No. It shouldn\u2019t when we\u2019re talking about machines. In some future where there\u2019s a machine that\u2019s better than you at everything, it\u2019s no different than now. You\u2019re already not the best at everything you do. This idea that being better than is somehow an objective is ridiculous. It goes back to this earlier thing about like, why are we here? What\u2019s our purpose and all of that? I, maybe we\u2019ve gone through an evolution in my thinking about this as I\u2019ve gotten older, but being better at everything is not the purpose of your life It\u2019s just not because you\u2019re going to fail.", "Geordie Rose (01:08:46):If the purpose of your life was to always win at everything, your life is a failure by definition. If you want to be the Olympic gold medalist in freestyle wrestling at 82 kilos, which I did for a long time, the fact that I didn\u2019t achieve that, and I never will, if that was the defining element of my life, it means that everything that I\u2019ve done in my life is a failure, just because I\u2019m not the best. But that\u2019s the wrong way to look at things. The path through your life has ups and downs. You\u2019re going to have things that you view as successes and things that you view as failures.", "Geordie Rose (01:09:21):All of these things are relative. They\u2019re not absolute. You can do all of the things that you\u2019ve ever done in the background of having a bunch of intelligent machines running around. Nothing is stopping you from still doing what you\u2019re doing, even if there\u2019s a machine that\u2019s better at cleaning your carpet than you are. It doesn\u2019t matter. In fact, the machine being better at cleaning your carpet than you are frees you up to not have to worry about cleaning your carpet, or if the robot dentist is better at keeping your teeth healthy than the human dentist, the human dentist doesn\u2019t have that work anymore, but by God, you\u2019ve got great teeth.", "Geordie Rose (01:09:56):In the main, what\u2019s more important? Even if you\u2019re a dentist, you could do something else. There\u2019s an enormous infinite number of things that you can do that don\u2019t necessarily boil down to, who\u2019s the best at it? In fact, there\u2019s everything is like this \u2026 That I don\u2019t have any concern at all for the future in terms of work replacement and things being better than us. Let\u2019s just accept that it\u2019s going to happen and figure out how to make the best possible outcome. There\u2019s a ton of very, very cool outcomes that could come of this.", "Jeremie Harris (01:10:32):Well, it\u2019s a fascinating topic and a very exciting vision for the future that you have. I\u2019m glad you stopped by to share it. We\u2019re going to have to do this again. There\u2019s just too much to talk about. Thank you so much, Geordie, for making the time. I really appreciate it.", "Jeremie Harris (01:10:50):I\u2019m sure we will. Yeah, I\u2019m sure we will. Actually, do you want to share any links? I know Sanctuary is sort of secretive right now, but are there any links that you want, or that you can share for that or any of your other work?", "Geordie Rose (01:11:02):Well, so we\u2019ve gotten gone to great lengths to not have anything-", "Geordie Rose (01:11:10):Anything that you can find about Sanctuary is not what we do, at least anymore. There isn\u2019t any. I\u2019m not hard to find myself personally if anybody wants to get ahold of me. There\u2019s a lot of people, especially in the machine learning community who know how to get ahold of me, so just ask somebody if you want my email or whatever, and I\u2019ll respond.", "Jeremie Harris (01:11:36):Perfect. I\u2019ll share your social media as well. I think you\u2019re on Twitter, right?", "Geordie Rose (01:11:41):I am. Yeah. The only reason I\u2019m on Twitter is to follow Ashlee Vance, who\u2019s absolutely hilarious. He\u2019s really [inaudible 01:11:51]. He\u2019s going to be my outlet for watching Muppet videos and cat memes.", "Jeremie Harris (01:12:01):That\u2019s probably mentally healthier than the reason most people are on Twitter these days. You\u2019re definitely to be commended on that. Great. Well, thanks so much, Geordie.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa719db443b01&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a719db443b01--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a719db443b01--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a719db443b01--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----a719db443b01--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----a719db443b01---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa719db443b01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa719db443b01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/tagged/tds-podcast", "anchor_text": "TDS podcast"}, {"url": "https://youtu.be/ACfMaUv3UbU", "anchor_text": "here"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify"}, {"url": "https://plato.stanford.edu/entries/panpsychism/", "anchor_text": "panpsychism"}, {"url": "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence", "anchor_text": "existential risks"}, {"url": "https://en.wikipedia.org/wiki/Human_extinction", "anchor_text": "existential risks"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://www.dwavesys.com/", "anchor_text": "D-Wave\u2019s homepage"}, {"url": "https://www.kindred.ai/", "anchor_text": "Kindred Systems\u2019 homepage"}, {"url": "https://geordierose.medium.com/how-to-raise-investment-capital-ce2019e2908", "anchor_text": "checking that out here"}, {"url": "https://medium.com/tag/agi?source=post_page-----a719db443b01---------------agi-----------------", "anchor_text": "AGI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a719db443b01---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a719db443b01---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a719db443b01---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----a719db443b01---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa719db443b01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&user=Jeremie+Harris&userId=59564831d1eb&source=-----a719db443b01---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa719db443b01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&user=Jeremie+Harris&userId=59564831d1eb&source=-----a719db443b01---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa719db443b01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a719db443b01--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa719db443b01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a719db443b01---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a719db443b01--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a719db443b01--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a719db443b01--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a719db443b01--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a719db443b01--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a719db443b01--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a719db443b01--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a719db443b01--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "122K Followers"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwill-agi-need-to-be-embodied-a719db443b01&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.amazon.ca/Quantum-Physics-Made-Fundamental-Everything/dp/0735244138", "anchor_text": "Quantum Physics Made Me Do It: A Simple Guide to the Fundamental Nature of Everything2023"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}