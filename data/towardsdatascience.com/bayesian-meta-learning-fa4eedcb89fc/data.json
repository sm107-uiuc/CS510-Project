{"url": "https://towardsdatascience.com/bayesian-meta-learning-fa4eedcb89fc", "time": 1683007498.4747388, "path": "towardsdatascience.com/bayesian-meta-learning-fa4eedcb89fc/", "webpage": {"metadata": {"title": "Bayesian meta-learning. This story covers the reason why we\u2026 | by Qiurui Chen | Towards Data Science", "h1": "Bayesian meta-learning", "description": "This story covers the reason why we need Bayesian approaches, and how to implement and evaluate these approaches. It is a summary of the course \u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 |\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=QY8JXpnllb0&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=5", "anchor_text": "\u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 5 \u2014 Bayesian Meta-Learning\u2019", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1312.6114", "anchor_text": "One popular approach", "paragraph_index": 12}, {"url": "https://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf", "anchor_text": "Another approach", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1505.05424", "anchor_text": "Another approach", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1410.8516.pdf", "anchor_text": "Normalizing flows", "paragraph_index": 15}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "nergy-based models and GAN", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1801.08930", "anchor_text": "We can recast gradient-based meta-learning as hierarchical Bayes.", "paragraph_index": 26}, {"url": "https://openreview.net/pdf?id=rkgpy3C5tX", "anchor_text": "In the paper (Amortized Bayesian Meta-learning)", "paragraph_index": 27}, {"url": "https://papers.nips.cc/paper/7963-bayesian-model-agnostic-meta-learning.pdf", "anchor_text": "Ensemble of MAMLs (aka EMAML)", "paragraph_index": 29}, {"url": "https://arxiv.org/pdf/1806.03836.pdf", "anchor_text": "BMAML", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "paper", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "We can also use likelihood or a combination of accuracy as well as coverage of the distribution to measure performance", "paragraph_index": 43}, {"url": "https://openreview.net/pdf?id=rkgpy3C5tX", "anchor_text": "Or we can plot the confidence of the predictor versus the accuracy of the predictor", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "There are also active learning evaluations", "paragraph_index": 43}], "all_paragraphs": ["This story covers the reason why we need Bayesian approaches, and how to implement and evaluate these approaches. It is a summary of the course \u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 5 \u2014 Bayesian Meta-Learning\u2019.", "For meta-learning algorithms, 3 algorithmic properties are important: expressive power, consistency, and uncertainty awareness. Expressive power is the ability for f to represent a range of learning procedures, it measures scalability and applicability to a range of domains. Consistency means learned learning procedure will solve tasks with enough data, this property reduces reliance on meta-training tasks, which leads to good out-of-distribution performance. Uncertainty awareness is the ability to reason about ambiguity during learning. It allows us to think about how we might explore new environments in a reinforcement learning context in order to reduce our uncertainty. It also thinks about if we are in safety-critical settings, we want to calibrate uncertainty estimates. It also allows us to think about, from the Bayesian perspective of Meta-learning, what sort of principle approaches can be derived from those graphical models?", "This story covers 1. Why be Bayesian? 2. Bayesian meta-learning approaches 3. How to evaluate Bayesians", "The principle of multi-task and meta-learning principles are training and the test must match, and tasks must share \u201cstructure\u201d. What does \u201cstructure\u201d mean? it means statistical dependence on shared latent information theta.", "The graphical model, where theta is the meat parameters that are shared across all tasks. \u03a6i is the task-specific parameters for each task i. In particular, this graphical model shows some dependency on \u03b8. Basically, each of the parameters \u03d5i has an arrow coming from \u03b8. If your condition on the information in \u03b8, the task parameters (\u03d5s) become independent, which is \u03d5i is independent of another \u03d5i conditioned on \u03b8. If you do condition on \u03b8 on the latent information, then they are not otherwise independent. These properties indicate that the distribution over \u03d5 given \u03b8 has lower entropy than the distribution of the marginal of \u03d5. Essentially, \u03b8 tells you information about task-specific parameter \u03d5.", "What if you could identify \u03b8 (for example, through meta-learning), in which situations would it not be faster to learn \u03d5 in comparison to learning from scratch? If \u03d5s are independent before knowing to condition on \u03b8, which means their entropies will be the same, then learning from scratch will just as fast as learning from the shared information \u03b8. If a single point carries all the information of \u03b8, then the shared information basically doesn\u2019t tell you that much because the information also exists in a single data point. The entropy conditioned on \u03b8 is going to be fairly high as well as the marginal entropy. What if the entropy of \u03d5 given \u03b8 is zero? It means there isn\u2019t any additional information in \u03d5 that isn\u2019t captured in \u03b8, then \u03b8 can solve all of the tasks, and you don\u2019t need to do anything to learn from \u03b8.", "What information might theta contain? If the family of tasks corresponds to sinusoids with different amplitudes and different phases, then Theta will correspond to that family of sinusoid functions. In machine translation example, \u03b8 corresponds to the family of all language pairs, and the information in \u03d5 that isn't present in theta is going to correspond to things that are specific to a particular language. Note that in both of these examples, theta is narrower than the space of all possible functions (this why we can get benefit from using meta-learning in principle)", "If you have an infinite number of tasks, you should be able to recover theta exactly, or basically recover that family with high precision. What if you meta-learn without a lot of tasks? if you have a space of tasks, it won't necessarily cover the true distribution of tasks but will potentially meta overfitting to that space of tasks, such that actually doesn\u2019t recover \u03b8 that corresponds to all language pairs for example, but it finds the \u03b8 that corresponds to a set of language pairs that looks like the things in your training data, and not something that captures the full distribution. As a result, you won\u2019t be as effective at adapting to new things from that distribution unless the test data are very close to the training examples.", "For the parametric approaches, what we recovered was the deterministic estimate of the task-specific parameters \u03d5, given the dataset and meta-parameters \u03b8. So you\u2019d essentially get a point estimate for this distribution. Where/why is this a problem? We need more than just a point estimate in some situations. For example, some few-shot learning problems may not be fully determined by their data, maybe the underlying functions are ambiguous given the evidence that you have your prior information. Here in this example, ambiguity is because the dataset is small. To reconcile this problem, what if we can generate hypotheses about the underlying function? If we can sample from this distribution, we can then reason about our uncertainty, and this is important for safety-critical few-shot learning. It allows us to learn active learn and explore in meta-RL. That\u2019s where Bayesian meta-learning approaches come in.", "If we care about generating a distribution over our predictions, one approach is to let function f output the parameters of distribution over labels. For example, you let f output something that corresponds to the probability of discrete categorical distribution if your label y is discrete; you can also output the mean and variance of the Gaussian distribution to represent a Gaussian distribution over your labels; or if you have a more multi-modal distribution, you could try to represent the means and variances and mixture weights of a mixture of Gaussian; or if you have a multidimensional output label y, then you could output the parameters of a sequence of conditional distributions to allow you to represent the joint distribution over those variables, which is an autoregressive model. Once you output the parameters of the distribution, you can then train any of these approaches with the maximum likelihood estimate optimizing both over the mean of the distribution as well as the variance or basically all the full probability values of that distribution.", "The benefits of this approach are they are simple and can combine with a variety of methods. The downside is this approach can not reason about uncertainty over the underlying function to determine how uncertainty across datapoints relate. It also tends to produce poorly-calibrated uncertainty estimates. For example, you cant disentangle model uncertainty versus label noise. It is also limited to a particular class of distributions over y.", "Can we do the same maximum likelihood training for \u03d5? We can not do it exactly the same way because we don\u2019t have labels corresponding to \u03d5, unless we generated labels.", "The bayesian deep learning aims to represent distribution with neural networks. There are numbers of approaches to representing distributions with neural networks. One popular approach is to use latent variable models and then optimize them with variational inference. So in particular, we have a graphical model where we have latent variable Z and observed variables X. For example, the left graphical model for a variational auto-encoder. You can formulate a lower bound of the likelihood objective and use that to optimize over a distribution over X where that distribution over X may be non-Gaussian because it has this latent variable.", "Another approach that has been fairly popular in some regard and is quite simple is to use a particle-based representation of your distribution. And in particular, you can train separate models on different bootstraps of your data. Each of those models will correspond to a particular of your distribution, and then you together those particles represent samples from that distribution.", "Another approach is to represent an explicit distribution over the weights of neural network parameters. And then practice these distributions tend to be Gaussian with a diagonal covariance matrix, so you have basically an independent variance for each neural network parameter. This allows you to represent a distribution over functions with the caveat that this independent assumption that two parameters are independent is violated in practice all the time.", "Normalizing flows try to represent a function over a data distribution by inverting some latent distribution into your data distribution or transforming from latent space into your data space and back into your latent space.", "In energy-based models and GAN, the way to estimate unnormalized density is to have low-energy for your data and higher energy for everything else where everything else is approximated by your generator in a GAN.", "This graphical model contains observed variable x, and latent variable z. The evidence lower bound or ELBO wants to be able to estimate a lower bound on the likelihood of data so that the likelihood of data can be optimized. ELBO can be represented by the expectation with respect to q(z|x), plus an entropy term that is regularizing or operating on q(z|x). Q is representing the variational distribution of the probability of x and z. This can be rewritten two terms plus each other, the first term corresponds to the reconstruction loss of the decoder, basically the likelihood of data according to the decoder after sampling from the inference network q, and the second term corresponds to the KL divergence between the inference network and the prior. P corresponds to model and q corresponds to a variational distribution that is introduced in order to approximate the likelihood objective. There are a couple of design choices to how to represent p: p(z) and p(x|z). P(x|z) can be represented as a neural network, for example, in the case of a variational autoencoder, it represented a neural network that takes in the latent variable and outputs an image or whatever datatype you are modeling. P(z) is typically represented as a diagonal Gaussian with unit variance. It could also be represented by a neural network as well or as a learned mean and a learned variance. In practical variational autoencoders typically don\u2019t learn it, because the layer afterward can transform it into a learned mean and variance. q(z|x) is also represented by a neural network, which is inferred to as the inference network or your variational distribution. When using variational inference in the context of deep learning, \u03b8 represents model parameters and \u03d5 represents the parameters of inference networks.", "But there is a problem to optimize ELBO. ELBO contains an expectation with respect to q, which means we need to able to back-propagate into the q distribution, but sampling is not differentiable. We can use the reparameterization trick. For a gaussian q(z|x), it can be represented in a differentiable way, which is reparametrized in terms of the noise that is sampled from a unit Gaussian. In particular, if we represent the distribution of latent variable as a Gaussian that corresponds to the output of the neural network q, which is outputting both the mean and variance, then you can represent the output of q (Gaussian distributed) as being reparameterized by the noise rather than sampling from that particular distribution of the mean plus the variance times that noise. Fortunately, this equation is something that\u2019s differentiable. If your inference network is expressive enough, it should be able to transform your data distribution into this Gaussian distribution over latent variables.", "This is often called amortized variational inference, we have an inference network that\u2019s predicting what the variational distribution will be as basically amortizing the process of estimating that distribution. Can we use amortized variational inference for meta-learning?", "First, let\u2019s think about black-box meta-learning approaches for simplicity. In particular, what we want to have a neural network that takes as input a training dataset and produces a distribution over our parameters \u03a6. Then applying \u03d5 to parametrize a neural network that takes as input x and outputs y. The goal is to be able to maximize the likelihood of the test data points given task-specific parameters when sampling task-specific parameters as a function of our training dataset.", "In standard VAE, the observed variable is x and the latent variable is z. While in meta-learning, the observed variable is dataset and the latent variable is \u03d5. ELBO is taken into mat-learning. It has a variational distribution over the task-specific parameters \u03d5, where we will sample from and estimate the likelihood of the data given \u03d5. The KL term indicates that the variational distribution and the prior over phi should be similar.", "There are two design options for q condition on. If you want to be able to sample parameters as a function of our dataset, then we should condition q on our training data (this is shown in the first equation). How does this training data differ from the probability of test data points? So you want to be able to maximize the likelihood of the test data points given our task-specific parameters when sampling our task-specific parameters as a function of our training dataset (this is shown in the second equation).", "What about the meta-parameters \u03b8? We will have a prior over the task-specific parameters \u03d5 be conditioned on \u03b8. The distribution over test data points is a function only of \u03a6. We could also condition on \u03b8.", "The final objective maximizes w.r.t. the meta parameters and expectations over all of the tasks. Task-specific parameters are sampled from the neural network q, and then they are applied to maximize the likelihood of p. The term one the right encourages q to stay close to some prior distribution.", "To summarize bayesian black-box meta-learning, some benefits and downsides are listed. One benefit is that it can represent non-Gaussian distributions over y, and it also generates distribution over task-specific parameters \u03d5, rather than just generating distribution over labels (y^{ts}). This allows it to represent uncertainty over the underlying function, and not just about the underlying data points. The downside is that it can only represent Gaussian distributions of p of \u03d5 given \u03b8. There are 2 reasons for that. One is the reparameterization trick, which holds for Gaussian. The second thing is that the KL terms that come up in the objective is something that can be evaluated in closed form for Gaussians but can not be evaluated in closed form for other non-Gaussian objectives.", "We can recast gradient-based meta-learning as hierarchical Bayes. This is nice in that it provides a Bayesian interpretation of MAML as a kind of learning meta parameters such that at test time you are doing map inference under a Gaussian prior represented by your meta parameters. It\u2019s using a map estimate of task-specific parameters \u03d5 as a function of \u03b8, which means that it\u2019s representing a point estimate of this distribution and it\u2019s only giving you one set of parameters for this distribution and you can\u2019t sample from this distribution or more easily represent the full distribution of your task parameters for example.", "Q is an arbitrary function, it can include a gradient operator. It\u2019s so you can have an inference network that is basically performing gradient descent inside your inference network. Because q doesn\u2019t need to be a neural network, it could be anything that outputs a distribution over \u03d5. In the paper (Amortized Bayesian Meta-learning), we are going to be running gradient descent w.r.t. the mean of a set of parameters and the variance of the set of parameters w.r.t. some training data, and then everything else is the same. In contrast to things like MAML, this is not just running a gradient descent on a parameter, it\u2019s running gradient descent on the mean and the variance of the parameters and at the end of this procedure, you get both the mean and variance of the parameter instead of just a mean. The benefit is you running gradient descent at test time, so we get an optimization-based meta-learning approach by basically stuffing gradient descent into the reference network. The downside is that p(\u03d5i|\u03b8) is modeled as a Gaussian.", "We can model non-Gaussian posterior with maintaining separate model instances. There are several papers about ensembling MAMLs.", "Ensemble of MAMLs (aka EMAML) trains M independent MAML models, it doesn't work well if ensemble members are too similar. We can take this idea to ensemble black-box and non-parametric methods.", "BMAML is a more diverse ensemble of mammals, basically, it does is it pushes the different ensemble members away from each other encourages them to represent different models. The way that you accomplish this is that when you are running gradient descent, you have your typical likelihood term, but you also have a term that encourages by different ensemble members to be different from one another. And there\u2019s different kernel like functions that you can do to represent the similarity between models.", "Instead of just pushing the particles away from one another, they also optimized such that the ensemble gives you a distribution of particles that produce high likelihood. And so instead of completely separately training these different models, they took their ensemble and they optimized a term that they optimized them all jointly together. So you are optimizing for the likelihood that\u2019s averaged over those M particles.", "The benefits of these ensemble models are they are simple, tend to work well, and also they can model non-gaussian posterior. The downside is you need to maintain M models instances or do gradient-based inference on the last layer only. Can we model non-Gaussian posterior over all parameters without having to maintain separate model instances?", "Can we sample parameter vectors with a procedure like Hamiltonian Monte Carlo? in particular what Hamiltonian Monte Carlo does is it adds noise and then runs gradient descent, repeatedly in order to be able to sample from some distributions. The intuition is learning a prior where a random kick can put us in different modes.", "In this loss landscape, there are different modes of solutions. One corresponds to a classifier that classifies on \u201csmiling\u201d and \u201cwearing a hat\u201d, one that classifies on \u201csmiling\u201d and \u201cbeing young\u201d. We want essentially parameter vector in the middle such that if we add some noise to that parameter vector and then run gradient descent, we get 2 different modes of this solution. We get different functions that represent different modes of the correct answer.", "In this case, theta would be a distribution instead of a single parameter vector. Our goal will be to sample different task-specific parameters given all of the observed variables at test time, which are x train, y train, and x test. We can cross over x testbecause y testis not given and others remain conditionally independent, so you can just sample from the function \u03d5 given x train and y train. The exact solution for this distribution is completely intractable because of its integral. If we knew how to sample phi given theta x train and y train, then sampling becomes a lot easier. We can crudely approximate this distribution using a point estimate for \u03d5 by MAP inference. We can still actually sample from \u03d5 where we first sample from \u03b8 then we run a few steps of gradient descent. This is what happens at test time where we want to do inference. Training can be done with amortized variational inference, you can check out the paper for details.", "What does ancestral sampling look like? There are 2 steps: first, we sample from P of \u03b8 which has a mean and variance; then we run gradient descent starting from the sample theta, running gradient descent w.r.t our theta. It allows us to represent these types of multimodal distributions.", "The benefits of this approach are it has non-gaussian posterior, simple at test time, and has only one model instance (in contrast to ensembles of MAML). The downside is it has a more complex training procedure.", "Three methods are covered. The first is train a function f to output a distribution over y test. The benefits are it is simple and can combine with a variety of methods. But it can not reason about uncertainty over the underlying function and also a limited class of distribution over y test can be expressed. The second approach is black-box approaches, where using latent variable models and amortized variational inference. The benefit is it can represent non-gaussian distribution over y test but it can only represent gaussian distribution p(\u03d5i |\u03b8), it is okay when \u03d5i is a latent vector. For optimization-based approaches, we mentioned 3 types: amortized inference; ensembles, and hybrid inference. Amortized inference is simple but it model p(\u03d5i |\u03b8) as a Gaussian. Ensembles approach is simple, tends to work well, and has non-Gaussian distribution but it contains M model instance. The hybrid inference has a non-gaussian posterior, it\u2019s simple at test time and only has one model instance, but it has more complex training procedure.", "How about using the standard benchmarks, such as Minilmagenet accuracy? The benefit is it\u2019s standardized so we can easier compare papers. Minilmagene also has real images. But metrics like accuracy do not evaluate uncertainty. Tasks may not exhibit ambiguity, and uncertainty may not be useful on this dataset. What are better problems and metrics? It depends on the problem you care about.", "One way to check the performance of these algorithms is to look at varied toy examples. For example, visualize the underlying function that you are sampling from.", "This example corresponds to the underlying meta-learning functions are both sinusoids and linear functions where there\u2019s noise in the labels to make the task ambiguous. The types of functions that it\u2019s reasoning about in some situations, it\u2019s ambiguous even if it\u2019s sinusoid function or a linear function. And you see this sort of multimodal output from the sampling distribution. It shows that the function can actually represent something that\u2019s multimodal.", "The second example is a classification example where you are just given one data point, all of the tasks corresponding to circular decision boundaries. And you are trying to classify between positive points and negative points. The D train only contains one positive point and D test contains both positive and negative data points. So basically you can see a visualization of the decision boundaries here shown in the dash-lines, where the neural network is representing these different decision boundaries that kind of represent the structure of the class of functions that it was trained on.", "We can also use likelihood or a combination of accuracy as well as coverage of the distribution to measure performance. Or we can plot the confidence of the predictor versus the accuracy of the predictor. There are also active learning evaluations.", "MSc in Computer Science at UT."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa4eedcb89fc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://www.youtube.com/watch?v=QY8JXpnllb0&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=5", "anchor_text": "\u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 5 \u2014 Bayesian Meta-Learning\u2019"}, {"url": "https://medium.com/@rachel_95942?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Qiurui Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbd32a3f303d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&user=Qiurui+Chen&userId=cbd32a3f303d&source=post_page-cbd32a3f303d----fa4eedcb89fc---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa4eedcb89fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----fa4eedcb89fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa4eedcb89fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&source=-----fa4eedcb89fc---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://wordart.com/create", "anchor_text": "WordArt"}, {"url": "https://www.youtube.com/watch?v=QY8JXpnllb0&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=5", "anchor_text": "\u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 5 \u2014 Bayesian Meta-Learning\u2019"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "https://arxiv.org/abs/1312.6114", "anchor_text": "One popular approach"}, {"url": "https://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf", "anchor_text": "Another approach"}, {"url": "https://arxiv.org/abs/1505.05424", "anchor_text": "Another approach"}, {"url": "https://arxiv.org/pdf/1410.8516.pdf", "anchor_text": "Normalizing flows"}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "nergy-based models and GAN"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "https://arxiv.org/abs/1801.08930", "anchor_text": "We can recast gradient-based meta-learning as hierarchical Bayes."}, {"url": "https://openreview.net/pdf?id=rkgpy3C5tX", "anchor_text": "In the paper (Amortized Bayesian Meta-learning)"}, {"url": "https://papers.nips.cc/paper/7963-bayesian-model-agnostic-meta-learning.pdf", "anchor_text": "Ensemble of MAMLs (aka EMAML)"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "https://arxiv.org/pdf/1806.03836.pdf", "anchor_text": "BMAML"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "Probabilistic MAML"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "paper"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "the course slide"}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "We can also use likelihood or a combination of accuracy as well as coverage of the distribution to measure performance"}, {"url": "https://openreview.net/pdf?id=rkgpy3C5tX", "anchor_text": "Or we can plot the confidence of the predictor versus the accuracy of the predictor"}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "There are also active learning evaluations"}, {"url": "https://www.youtube.com/watch?v=QY8JXpnllb0&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=5", "anchor_text": "Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 5 \u2014 Bayesian Meta-Learning"}, {"url": "http://cs330.stanford.edu/slides/cs330_bayesian_metalearning.pdf", "anchor_text": "The course slide"}, {"url": "https://arxiv.org/abs/1807.05960", "anchor_text": "Meta-Learning with Latent Embedding Optimization"}, {"url": "https://arxiv.org/abs/1903.03096", "anchor_text": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples"}, {"url": "https://cs.stanford.edu/~woodward/papers/active_one_shot_learning_2016.pdf", "anchor_text": "Active One-shot Learning \u2014 Stanford Computer Science"}, {"url": "https://arxiv.org/abs/1708.00088", "anchor_text": "Learning Algorithms for Active Learning"}, {"url": "https://arxiv.org/abs/1312.6114", "anchor_text": "Auto-Encoding Variational Bayes"}, {"url": "https://arxiv.org/abs/1612.01474", "anchor_text": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"}, {"url": "https://arxiv.org/abs/1806.02817", "anchor_text": "Probabilistic Model-Agnostic Meta-Learning"}, {"url": "https://papers.nips.cc/paper/8161-probabilistic-model-agnostic-meta-learning.pdf", "anchor_text": "Probabilistic Model-Agnostic Meta-Learning \u2014 NIPS Proceedings"}, {"url": "https://medium.com/tag/meta-learning?source=post_page-----fa4eedcb89fc---------------meta_learning-----------------", "anchor_text": "Meta Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fa4eedcb89fc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fa4eedcb89fc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----fa4eedcb89fc---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa4eedcb89fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----fa4eedcb89fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa4eedcb89fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----fa4eedcb89fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa4eedcb89fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbd32a3f303d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&user=Qiurui+Chen&userId=cbd32a3f303d&source=post_page-cbd32a3f303d----fa4eedcb89fc---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5d77130b77c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&newsletterV3=cbd32a3f303d&newsletterV3Id=5d77130b77c3&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----fa4eedcb89fc---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Written by Qiurui Chen"}, {"url": "https://medium.com/@rachel_95942/followers?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "77 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbd32a3f303d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&user=Qiurui+Chen&userId=cbd32a3f303d&source=post_page-cbd32a3f303d----fa4eedcb89fc---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5d77130b77c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-meta-learning-fa4eedcb89fc&newsletterV3=cbd32a3f303d&newsletterV3Id=5d77130b77c3&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----fa4eedcb89fc---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51?source=author_recirc-----fa4eedcb89fc----0---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=author_recirc-----fa4eedcb89fc----0---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=author_recirc-----fa4eedcb89fc----0---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Qiurui Chen"}, {"url": "https://medium.com/analytics-vidhya?source=author_recirc-----fa4eedcb89fc----0---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Analytics Vidhya"}, {"url": "https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51?source=author_recirc-----fa4eedcb89fc----0---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "T5: a detailed explanationGiven the current landscape of transfer learning for NLP, Text-to-Text Transfer Transformer (T5) aims to explore what works best, and how\u2026"}, {"url": "https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51?source=author_recirc-----fa4eedcb89fc----0---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "8 min read\u00b7Jun 8, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-vidhya%2Fa0ac9bc53e51&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ft5-a-detailed-explanation-a0ac9bc53e51&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----a0ac9bc53e51----0-----------------clap_footer----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51?source=author_recirc-----fa4eedcb89fc----0---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa0ac9bc53e51&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ft5-a-detailed-explanation-a0ac9bc53e51&source=-----fa4eedcb89fc----0-----------------bookmark_preview----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fa4eedcb89fc----1---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----fa4eedcb89fc----1---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----fa4eedcb89fc----1---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----fa4eedcb89fc----1---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fa4eedcb89fc----1---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fa4eedcb89fc----1---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fa4eedcb89fc----1---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----fa4eedcb89fc----1-----------------bookmark_preview----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fa4eedcb89fc----2---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----fa4eedcb89fc----2---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----fa4eedcb89fc----2---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----fa4eedcb89fc----2---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fa4eedcb89fc----2---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fa4eedcb89fc----2---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fa4eedcb89fc----2---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----fa4eedcb89fc----2-----------------bookmark_preview----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b?source=author_recirc-----fa4eedcb89fc----3---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=author_recirc-----fa4eedcb89fc----3---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=author_recirc-----fa4eedcb89fc----3---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Qiurui Chen"}, {"url": "https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b?source=author_recirc-----fa4eedcb89fc----3---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "Language models and RNNThis story covers topics: Language models(LM) and RNN. In detail, for LM, this story goes from the N-gram language model to neural LM; for\u2026"}, {"url": "https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b?source=author_recirc-----fa4eedcb89fc----3---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": "11 min read\u00b7May 12, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc516fab9545b&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40rachel_95942%2Flanguage-models-and-rnn-c516fab9545b&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----c516fab9545b----3-----------------clap_footer----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b?source=author_recirc-----fa4eedcb89fc----3---------------------c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc516fab9545b&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40rachel_95942%2Flanguage-models-and-rnn-c516fab9545b&source=-----fa4eedcb89fc----3-----------------bookmark_preview----c512e0db_d71d_4cb2_ac3a_92f4a5f40928-------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "See all from Qiurui Chen"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Babar M Bhatti"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Essential Guide to Foundation Models and Large Language ModelsThe term Foundation Model (FM) was coined by Stanford researchers to introduce a new category of ML models. They defined FMs as models\u2026"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "\u00b714 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&user=Babar+M+Bhatti&userId=10dee34829b&source=-----27dab58f7404----0-----------------clap_footer----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&source=-----fa4eedcb89fc----0-----------------bookmark_preview----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----1-----------------clap_footer----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----fa4eedcb89fc----1-----------------bookmark_preview----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://kargarisaac.medium.com/?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://kargarisaac.medium.com/?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Isaac Kargar"}, {"url": "https://medium.com/aiguys?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "AIGuys"}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Reinforcement Learning from Human Feedback, InstructGPT, and ChatGPTNote: some parts of this blog post are generated by ChatGPT! :)"}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "\u00b79 min read\u00b7Jan 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Faiguys%2F693d00cb9c58&operation=register&redirect=https%3A%2F%2Fmedium.com%2Faiguys%2Freinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58&user=Isaac+Kargar&userId=bf5ea8e11f80&source=-----693d00cb9c58----0-----------------clap_footer----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58?source=read_next_recirc-----fa4eedcb89fc----0---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F693d00cb9c58&operation=register&redirect=https%3A%2F%2Fmedium.com%2Faiguys%2Freinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58&source=-----fa4eedcb89fc----0-----------------bookmark_preview----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----1-----------------clap_footer----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----fa4eedcb89fc----1---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----fa4eedcb89fc----1-----------------bookmark_preview----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758?source=read_next_recirc-----fa4eedcb89fc----2---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=read_next_recirc-----fa4eedcb89fc----2---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=read_next_recirc-----fa4eedcb89fc----2---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Lu\u00eds Roque"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----fa4eedcb89fc----2---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758?source=read_next_recirc-----fa4eedcb89fc----2---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Deterministic vs. Probabilistic Deep LearningProbabilistic Deep Learning"}, {"url": "https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758?source=read_next_recirc-----fa4eedcb89fc----2---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "\u00b79 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5325769dc758&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeterministic-vs-probabilistic-deep-learning-5325769dc758&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----5325769dc758----2-----------------clap_footer----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758?source=read_next_recirc-----fa4eedcb89fc----2---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5325769dc758&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeterministic-vs-probabilistic-deep-learning-5325769dc758&source=-----fa4eedcb89fc----2-----------------bookmark_preview----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----fa4eedcb89fc----3---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----fa4eedcb89fc----3---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----fa4eedcb89fc----3---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "Amy @GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo?source=read_next_recirc-----fa4eedcb89fc----3---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----fa4eedcb89fc----3---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "The Ultimate Guide to Evaluating Your Recommendation SystemUnderstand the key metrics to measure the performance of your recommender engine"}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----fa4eedcb89fc----3---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": "\u00b720 min read\u00b7Apr 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgrabngoinfo%2Fd4fc8d4423cc&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fthe-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc&user=Amy+%40GrabNGoInfo&userId=ef6171ffb4ed&source=-----d4fc8d4423cc----3-----------------clap_footer----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----fa4eedcb89fc----3---------------------0969d5db_d374_4478_b602_a369d5e3c7b5-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4fc8d4423cc&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fthe-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc&source=-----fa4eedcb89fc----3-----------------bookmark_preview----0969d5db_d374_4478_b602_a369d5e3c7b5-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----fa4eedcb89fc--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}