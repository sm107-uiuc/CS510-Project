{"url": "https://towardsdatascience.com/can-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0", "time": 1682996813.8383439, "path": "towardsdatascience.com/can-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0/", "webpage": {"metadata": {"title": "Can AI Read Chest X-rays like Radiologists? | by David W. Dai | Towards Data Science", "h1": "Can AI Read Chest X-rays like Radiologists?", "description": "This is Part 2 of a two part series. See Part 1 for challenges and clinical applications of chest x-ray (CXR) segmentation, and how medical imaging, and CXRs specifically, critically need AI to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/can-machine-learning-read-chest-x-rays-like-radiologists-part-1-7182cf4b87ff", "anchor_text": "Part 1", "paragraph_index": 0}, {"url": "https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf", "anchor_text": "ere", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1409.1556", "anchor_text": "VGG-based", "paragraph_index": 8}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/10628457", "anchor_text": "Japanese Society of Radiological Technology (JSRT) dataset", "paragraph_index": 10}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/15919232", "anchor_text": "study", "paragraph_index": 10}, {"url": "https://www.bmj.com/rapid-response/2011/10/28/re-doctors-are-subjective-not-objective", "anchor_text": "exact science", "paragraph_index": 11}, {"url": "https://lhncbc.nlm.nih.gov/system/files/pub9356.pdf", "anchor_text": "Montgomery", "paragraph_index": 12}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "Generative Adversarial Networks (GAN)", "paragraph_index": 17}, {"url": "https://arxiv.org/pdf/1703.08770.pdf", "anchor_text": "our paper", "paragraph_index": 18}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "Candemir et. al., TMI (2014)", "paragraph_index": 24}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "Candemir et. al., TMI (2014)", "paragraph_index": 25}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "Candemir et. al., TMI (2014)", "paragraph_index": 26}, {"url": "https://lukeoakdenrayner.wordpress.com/2017/11/18/quick-thoughts-on-chestxray14-performance-claims-and-clinical-tasks/", "anchor_text": "fraught with suspicion from radiologists", "paragraph_index": 28}, {"url": "https://www.apple.com/", "anchor_text": "Apple", "paragraph_index": 29}, {"url": "http://wayfinder.ai", "anchor_text": "Wayfinder AI", "paragraph_index": 29}, {"url": "http://petuum.com", "anchor_text": "Petuum", "paragraph_index": 29}, {"url": "https://twitter.com/daiwei89", "anchor_text": "@daiwei89", "paragraph_index": 29}, {"url": "https://medium.com/@davidwdai", "anchor_text": "Medium", "paragraph_index": 29}, {"url": "http://Wayfair.ai", "anchor_text": "Wayfair.ai", "paragraph_index": 31}, {"url": "http://Petuum.com", "anchor_text": "Petuum.com", "paragraph_index": 31}], "all_paragraphs": ["This is Part 2 of a two part series. See Part 1 for challenges and clinical applications of chest x-ray (CXR) segmentation, and how medical imaging, and CXRs specifically, critically need AI to scale.", "The task of chest X-ray (CXR) segmentation is to recognize the lung fields and the heart regions in CXRs:", "Among a number of clinical applications, lung segmentation directly leads to a key clinical indicator cardiothoracic ratio (CTR), which leads to diagnosis of cardiomegaly.", "Given the challenges in working with CXR (see Part 1), we first design the segmentation model based on Fully Convolutional Network (FCN). We then augment with adversarial training in the Structure Correcting Adversarial Network (SCAN) framework, which achieves human-level performance.", "Let\u2019s deep dive into the models and the thought processes leading to the model designs.", "The input to the segmentation model is an image of dimension H x W x C (height, width, channels), where C = 3 for RGB values, or C = 1 for grayscale images like CXR. The model then outputs per-pixel class probability H x W x T where T is the number of classes. In our case T = 4 for [left lung, right lung, heart, background] and T=3 when heart segmentation label is not available (such as in one of the dataset).", "We design the network to be fully convolutional, which replaces fully connected layers with 1x1 convolution. (See here for more details). We started off with VGG-like architecture, with about 16 weight layers and many feature maps (or convolutional channels): 64 feature maps in the first convolution, then doubling till 512 channels in the final layers. The resulted model has large capacity (>100 million parameters) that it overfits the training data perfectly, but performs poorly on the test data. This is a clear indication that our dataset is too small to support a large model like this.", "Since CXR images are grayscale with standardized structures, we reduce the number of filters and find that using 8 feature maps for the first convolution, instead of 64 in VGG, gives much improved results. However we quickly runs into the model capacity limit. To increase model capacity, we go deeper. Eventually we arrive at a \u201cskinny\u201d and deep network with 21 weight layers:", "The total number of parameters in the model is 271k, which is 500x smaller than the VGG-based segmentation models.", "Because the model is so small (very few parameters), we can train it from scratch on just the 209 CXR examples. We use Intersection over Union (IoU) metrics to evaluate the quality of lung and heart segmentations. (See left image for a pictorial definition.) IoU ranges between 0 (no overlap between predicted mask and ground truth) to 1 (perfect match).", "We use CXRs from Japanese Society of Radiological Technology (JSRT) dataset and labels from another study to prepare the JSRT dataset, consisting of 247 CXRs (209 for training and validation, 38 for evaluation). This skinny and tall segmentation network (which we call FCN for fully convolutional network) performs pretty well:", "Notice that human performance is not perfect, limited by inherent subjective interpretation needed to draw out the boundaries. The low heart IoU by human observers indicates that heart boundaries are especially difficult to infer (see challenges in Part 1). This is just one of the many places where medicine isn\u2019t an exact science.", "It\u2019s often helpful to visualize what happens in the low performing samples and do a failure analysis. Below we apply our model trained on JSRT dataset to both JSRT and another dataset (which we call Montgomery):", "Aside: In the image above notice that CXRs from different dataset look quite different due to factors like different equipment, medical operators, and population. Therefore it\u2019s a much more difficult task to adapt to a new dataset domain. Knowing that, our segmentation model already performs surprisingly well on the Montgomery dataset for the lung segmentation without ever seeing an image from that population.", "These failure cases reveal the difficulties arising from CXR images\u2019 varying contrast across samples. For example, in the image above, the apex of the ribcage of the rightmost patient\u2019s is mistaken as an internal rib bone, resulting in the mask \u201cbleeding out\u201d to the black background, which has a similar intensity as the lung field. Vascular structures around mediastinum (the \u201cwhite stuff\u201d between the two lungs) and anterior rib bones (the criss-crossing lines in the lung fields) can also have similar intensity and texture as exterior boundary, resulting in the drastic mistakes as can be seen in the middle two columns.", "The failure cases tell us that the model needs to have a sense of global structures to avoid drastic failure like the earlier examples. For example, anyone with a basic training knows that the heart should be more or less elliptical, while the apex of the lung fields should be smooth and the angle where the diaphragm meets the ribcage should be sharp. But just how should we teach this knowledge to the FCN segmentation model?", "While it\u2019s not easy to mathematically encode the knowledge (for example, exactly how sharp is a sharp angle?), it\u2019s pretty easy to tell whether the predicted segmentation looks natural or not. In machine learning lingo that\u2019s called a binary classification problem. This naturally leads to the following adversarial framework:", "The key addition here is that segmentation network\u2019s prediction is evaluated not only by the per-pixel loss (i.e. how well the predicted mask matches the ground truth pixel by pixel), but an \u201coverall look and feel\u201d evaluation given by the critic network (i.e., how well the predicted mask looks real enough to fool the critic network). Astute readers might notice that this is very similar to Generative Adversarial Networks (GAN). Indeed, this framework can be viewed as conditional GAN, where we generate the masks based on an input CXR image instead of a random noise vector in the original GAN.", "In our work we design the critic network to largely mirror the segmentation network\u2019s architecture. Details such as training objectives, hyperparameters of the model, and experiment setups can be found in our paper.", "Before we dive into the numbers, we should clarify that the critic network in SCAN is only involved during the training stage. During testing, we only use the segmentation network, which has an identical architecture as FCN. In other words, our hope is that with the addition of critic network we can somehow train the same segmentation network better, using the guidance from the critic network to encourage the same segmentation network towards more \u201cnatural\u201d predictions. With that in mind, we repeat the evaluation on the JSRT dataset:", "Notice that without any change in FCN architecture, SCAN improves FCN by 1.8% absolutely to human level performance, at around 94.6% lung IoU! Let\u2019s revisit the 4 difficult patients in our failure cases:", "As you can see, all 4 cases are \u201cfixed\u201d pretty satisfactorily. Furthermore, notice that SCAN produces the more realistic sharp angle at the outer lower corner of each lung field (the costophrenic angle) compared with SCAN. The corners generally don\u2019t affect the per-pixel performance, but can be important in downstream diagnostic tasks (e.g., detecting the blunting of costophrenic angle).", "In the clinical settings it\u2019s not enough to just have a good average performance, but it\u2019s important to avoid outrageous errors in prediction as they can affect doctors\u2019 trust in AI. By using the adversarial learning framework, SCAN improves the per-pixel metrics as well as \u201coverall look and feel\u201d of the prediction. Both of which are important in the clinical settings.", "The evaluation table above shows that our method outperforms the prior state of the art for CXR lung field segmentation (\u201cregistration-based\u201d method) by a large margin. Since our work is the first deep learning solution for CXR segmentation, it\u2019s helpful to have a perspective of how complex non-deep learning solutions can be:", "The approach in Candemir et. al., TMI (2014) involves a series of sift feature extraction, shape transformation, finding patients with similar lung shape profiles as candidate CXR segmentations, graph cut etc to produce the final segmentation. Each stage requires various tuning parameters, and since the prediction is based on deforming patients with similar lung profiles, when the new patient\u2019s lung is sufficiently different from the existing training data, the performance suffers, as we will see later.", "The complex pipeline in Candemir et. al., TMI (2014) stands in stark contrast with the simplicity of neural networks, where the network learns both the features and shapes on its own. Gone are the days for handcrafted features like SIFT and delicate shape manipulations in a series of stages.", "It\u2019s helpful to have some qualitative comparison to understand how SCAN outperforms Candemir et. al., TMI (2014):", "For the left two columns SCAN produces more realistic contours around the sharp costophrenic angles. This may be a challenge in registration-based models where detecting and matching the costophrenic point is difficult. For the right two columns (Candemir et. al., TMI (2014)) struggles due to the mismatch between test patient lung profiles (from Montgomery dataset) and the existing lung profiles in the JSRT dataset, leading to the unnatural mask shapes.", "There\u2019s been much hype around AI\u2019s diagnostic accuracy on CXRs. However, AI-based diagnosis on CXR can be fraught with suspicion from radiologists. While there are exciting results, it\u2019s often easier to make inroad to hospitals with smaller improvements like cardiothoracic ratio (CTR) calculation that can be derived from lung segmentations (see Part 1). We were able to go into trials with our CTR engine quickly. Automated CTR calculation is easy to interpret, and generally very accurate. We\u2019ve found that sometimes it\u2019s more important to gain trust from the doctors and domain experts by supporting their existing workflow well with robust AI, instead of changing their workflow with less mature AI solutions. I hope that this case study can serve as a helpful example for the development of other healthcare AI solutions.", "About the author: David Dai is Senior Machine Learning Engineer at Apple, advisor at Wayfinder AI, and former Senior Director of Engineering at Petuum. He holds PhD in Machine Learning from Carnegie Mellon University, and was named Pittsburgh\u2019s 30 Under 30. @daiwei89 | Medium | david@wayfinder.ai.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Sr. Machine Learning Engineer @ Apple, Advisor at Wayfair.ai, PhD in ML (Carnegie Mellon U), Former Sr. Director of Engineering @ Petuum.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faa77dba219f0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@davidwdai?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@davidwdai?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "David W. Dai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f430f00f54c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&user=David+W.+Dai&userId=5f430f00f54c&source=post_page-5f430f00f54c----aa77dba219f0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa77dba219f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa77dba219f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/can-machine-learning-read-chest-x-rays-like-radiologists-part-1-7182cf4b87ff", "anchor_text": "Part 1"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/10628457", "anchor_text": "Japanese Society of Radiology Technology"}, {"url": "https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf", "anchor_text": "ere"}, {"url": "https://arxiv.org/abs/1409.1556", "anchor_text": "VGG-based"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/10628457", "anchor_text": "Japanese Society of Radiological Technology (JSRT) dataset"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/15919232", "anchor_text": "study"}, {"url": "https://www.bmj.com/rapid-response/2011/10/28/re-doctors-are-subjective-not-objective", "anchor_text": "exact science"}, {"url": "https://lhncbc.nlm.nih.gov/system/files/pub9356.pdf", "anchor_text": "Montgomery"}, {"url": "https://lhncbc.nlm.nih.gov/system/files/pub9356.pdf", "anchor_text": "Montgomery dataset"}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "Generative Adversarial Networks (GAN)"}, {"url": "https://arxiv.org/pdf/1703.08770.pdf", "anchor_text": "our paper"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "(Candemir et. al., TMI (2014))"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "Candemir et. al., TMI (2014)"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "Candemir et. al., TMI (2014)"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "Candemir et. al., TMI (2014)"}, {"url": "https://lukeoakdenrayner.wordpress.com/2017/11/18/quick-thoughts-on-chestxray14-performance-claims-and-clinical-tasks/", "anchor_text": "fraught with suspicion from radiologists"}, {"url": "https://www.apple.com/", "anchor_text": "Apple"}, {"url": "http://wayfinder.ai", "anchor_text": "Wayfinder AI"}, {"url": "http://petuum.com", "anchor_text": "Petuum"}, {"url": "https://twitter.com/daiwei89", "anchor_text": "@daiwei89"}, {"url": "https://medium.com/@davidwdai", "anchor_text": "Medium"}, {"url": "https://arxiv.org/abs/1703.08770", "anchor_text": "SCAN: Structure Correcting Adversarial Network for Organ Segmentation in Chest X-rays"}, {"url": "https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf", "anchor_text": "Fully Convolutional Networks for Semantic Segmentation"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/10628457", "anchor_text": "Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists\u2019 detection of pulmonary nodules."}, {"url": "https://arxiv.org/abs/1409.1556", "anchor_text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/15919232", "anchor_text": "Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database."}, {"url": "https://lhncbc.nlm.nih.gov/system/files/pub9356.pdf", "anchor_text": "Two public chest X-ray datasets for computer-aided screening of pulmonary diseases"}, {"url": "https://www.bmj.com/rapid-response/2011/10/28/re-doctors-are-subjective-not-objective", "anchor_text": "Doctors are subjective not objective"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/24239990", "anchor_text": "Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration"}, {"url": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "anchor_text": "Generative Adversarial Nets"}, {"url": "https://lukeoakdenrayner.wordpress.com/2017/11/18/quick-thoughts-on-chestxray14-performance-claims-and-clinical-tasks/", "anchor_text": "Quick thoughts on ChestXray14, performance claims, and clinical tasks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----aa77dba219f0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/medical-imaging?source=post_page-----aa77dba219f0---------------medical_imaging-----------------", "anchor_text": "Medical Imaging"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----aa77dba219f0---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----aa77dba219f0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/generative-adversarial?source=post_page-----aa77dba219f0---------------generative_adversarial-----------------", "anchor_text": "Generative Adversarial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa77dba219f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&user=David+W.+Dai&userId=5f430f00f54c&source=-----aa77dba219f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa77dba219f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&user=David+W.+Dai&userId=5f430f00f54c&source=-----aa77dba219f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa77dba219f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faa77dba219f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----aa77dba219f0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----aa77dba219f0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----aa77dba219f0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----aa77dba219f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@davidwdai?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@davidwdai?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David W. Dai"}, {"url": "https://medium.com/@davidwdai/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "178 Followers"}, {"url": "http://Wayfair.ai", "anchor_text": "Wayfair.ai"}, {"url": "http://Petuum.com", "anchor_text": "Petuum.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f430f00f54c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&user=David+W.+Dai&userId=5f430f00f54c&source=post_page-5f430f00f54c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff1ec85144ed6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-machine-learning-read-chest-x-rays-like-radiologists-part-2-aa77dba219f0&newsletterV3=5f430f00f54c&newsletterV3Id=f1ec85144ed6&user=David+W.+Dai&userId=5f430f00f54c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}