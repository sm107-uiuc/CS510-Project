{"url": "https://towardsdatascience.com/understanding-neural-networks-19020b758230", "time": 1682996462.3898098, "path": "towardsdatascience.com/understanding-neural-networks-19020b758230/", "webpage": {"metadata": {"title": "Understanding Neural Networks. We Explore How Neural Networks Function\u2026 | by Tony Yiu | Towards Data Science", "h1": "Understanding Neural Networks", "description": "Deep learning is a hot topic these days. But what is it that makes it special and sets it apart from other aspects of machine learning? That is a deep question (pardon the pun). To even begin to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understanding-logistic-regression-using-a-simple-example-163de52ea900", "anchor_text": "I wrote about that here", "paragraph_index": 9}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "gradient descent optimization", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "Mean Squared Error (MSE)", "paragraph_index": 30}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "gradient descent", "paragraph_index": 32}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "great textbook (online and free!) for the detailed math (if you want to understand neural networks more deeply, definitely check it out)", "paragraph_index": 37}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "If you liked this article and my writing in general, please consider supporting my writing by signing up for Medium via my referral link here. Thanks!", "paragraph_index": 55}, {"url": "http://neuralnetworksanddeeplearning.com/index.html", "anchor_text": "Neural Networks and Deep Learning by Michael A. Nielsen", "paragraph_index": 56}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "https://tonester524.medium.com/membership", "paragraph_index": 58}], "all_paragraphs": ["Deep learning is a hot topic these days. But what is it that makes it special and sets it apart from other aspects of machine learning? That is a deep question (pardon the pun). To even begin to answer it, we will need to learn the basics of neural networks.", "Neural networks are the workhorses of deep learning. And while they may look like black boxes, deep down (sorry, I will stop the terrible puns) they are trying to accomplish the same thing as any other model \u2014 to make good predictions.", "In this post, we will explore the ins and outs of a simple neural network. And by the end, hopefully you (and I) will have gained a deeper and more intuitive understanding of how neural networks do what they do.", "Let\u2019s start with a really high level overview so we know what we are working with. Neural networks are multi-layer networks of neurons (the blue and magenta nodes in the chart below) that we use to classify things, make predictions, etc. Below is the diagram of a simple neural network with five inputs, 5 outputs, and two hidden layers of neurons.", "Starting from the left, we have:", "The arrows that connect the dots shows how all the neurons are interconnected and how data travels from the input layer all the way through to the output layer.", "Later we will calculate step by step each output value. We will also watch how the neural network learns from its mistake using a process known as backpropagation.", "But first let\u2019s get our bearings. What exactly is a neural network trying to do? Like any other model, it\u2019s trying to make a good prediction. We have a set of inputs and a set of target values \u2014 and we are trying to get predictions that match those target values as closely as possible.", "Forget for a second the more complicated looking picture of the neural network I drew above and focus on this simpler one below.", "This is a single feature logistic regression (we are giving the model only one X variable) expressed through a neural network (if you need a refresher on logistic regression, I wrote about that here). To see how they connect we can rewrite the logistic regression equation using our neural network color codes.", "Not too bad right? So let\u2019s recap. A super simple neural network consists of just the following components:", "And these two objects are the fundamental building blocks of the neural network. More complex neural networks are just models with more hidden layers and that means more neurons and more connections between neurons. And this more complex web of connections (and weights and biases) is what allows the neural network to \u201clearn\u201d the complicated relationships hidden in our data.", "Now that we have our basic framework, let\u2019s go back to our slightly more complicated neural network and see how it goes from input to output. Here it is again for reference:", "The first hidden layer consists of two neurons. So to connect all five inputs to the neurons in Hidden Layer 1, we need ten connections. The next image (below) shows just the connections between Input 1 and Hidden Layer 1.", "Note our notation for the weights that live in the connections \u2014 W1,1 denotes the weight that lives in the connection between Input 1 and Neuron 1 and W1,2 denotes the weight in the connection between Input 1 and Neuron 2. So the general notation that I will follow is Wa,b denotes the weight on the connection between Input a (or Neuron a) and Neuron b.", "Now let\u2019s calculate the outputs of each neuron in Hidden Layer 1 (known as the activations). We do so using the following formulas (W denotes weight, In denotes input).", "We can use matrix math to summarize this calculation (remember our notation rules \u2014 for example, W4,2 denotes the weight that lives in the connection between Input 4 and Neuron 2):", "For any layer of a neural network where the prior layer is m elements deep and the current layer is n elements deep, this generalizes to:", "Where [W] is your n by m matrix of weights (the connections between the prior layer and the current layer), [X] is your m by 1 matrix of either starting inputs or activations from the prior layer, [Bias] is your n by 1 matrix of neuron biases, and [Z] is your n by 1 matrix of intermediate outputs. In the previous equation, I follow Python notation and use @ to denote matrix multiplication. Once we have [Z], we can apply the activation function (sigmoid in our case) to each element of [Z] and that gives us our neuron outputs (activations) for the current layer.", "Finally before we move on, let\u2019s visually map each of these elements back onto our neural network chart to tie it all up ([Bias] is embedded in the blue neurons).", "By repeatedly calculating [Z] and applying the activation function to it for each successive layer, we can move from input to output. This process is known as forward propagation. Now that we know how the outputs are calculated, it\u2019s time to start evaluating the quality of the outputs and training our neural network.", "This is going to be a long post so feel free to take a coffee break now. Still with me? Awesome! Now that we know how a neural network\u2019s output values are calculated, it is time to train it.", "The training process of a neural network, at a high level, is like that of many other data science models \u2014 define a cost function and use gradient descent optimization to minimize it.", "First let\u2019s think about what levers we can pull to minimize the cost function. In traditional linear or logistic regression we are searching for beta coefficients (B0, B1, B2, etc.) that minimize the cost function. For a neural network, we are doing the same thing but at a much larger and more complicated scale.", "In traditional regression, we can change any particular beta in isolation without impacting the other beta coefficients. So by applying small isolated shocks to each beta coefficient and measuring its impact on the cost function, it is relatively straightforward to figure out in which direction we need to move to reduce and eventually minimize the cost function.", "In a neural network, changing the weight of any one connection (or the bias of a neuron) has a reverberating effect across all the other neurons and their activations in the subsequent layers.", "That\u2019s because each neuron in a neural network is like its own little model. For example, if we wanted a five feature logistic regression, we could express it through a neural network, like the one on the left, using just a singular neuron!", "So each hidden layer of a neural network is basically a stack of models (each individual neuron in the layer acts like its own model) whose outputs feed into even more models further downstream (each successive hidden layer of the neural network holds yet more neurons).", "So given all this complexity, what can we do? It\u2019s actually not that bad. Let\u2019s take it step by step. First, let me clearly state our objective. Given a set of training inputs (our features) and outcomes (the target we are trying to predict):", "We want to find the set of weights (remember that each connecting line between any two elements in a neural network houses a weight) and biases (each neuron houses a bias) that minimize our cost function \u2014 where the cost function is an approximation of how wrong our predictions are relative to the target outcome.", "For training our neural network, we will use Mean Squared Error (MSE) as the cost function:", "The MSE of a model tell us on average how wrong we are but with a twist \u2014 by squaring the errors of our predictions before averaging them, we punish predictions that are way off much more severely than ones that are just slightly off. The cost functions of linear regression and logistic regression operate in a very similar manner.", "OK cool, we have a cost function to minimize. Time to fire up gradient descent right?", "Not so fast \u2014 to use gradient descent, we need to know the gradient of our cost function, the vector that points in the direction of greatest steepness (we want to repeatedly take steps in the opposite direction of the gradient to eventually arrive at the minimum).", "Except in a neural network we have so many changeable weights and biases that are all interconnected. How will we calculate the gradient of all of that? In the next section, we will see how backpropagation helps us deal with this problem.", "The gradient of a function is the vector whose elements are its partial derivatives with respect to each parameter. For example, if we were trying to minimize a cost function, C(B0, B1), with just two changeable parameters, B0 and B1, the gradient would be:", "So each element of the gradient tells us how the cost function would change if we applied a small change to that particular parameter \u2014 so we know what to tweak and by how much. To summarize, we can march towards the minimum by following these steps:", "I will defer to this great textbook (online and free!) for the detailed math (if you want to understand neural networks more deeply, definitely check it out). Instead we will do our best to build an intuitive understanding of how and why backpropagation works.", "Remember that forward propagation is the process of moving forward through the neural network (from inputs to the ultimate output or prediction). Backpropagation is the reverse. Except instead of signal, we are moving error backwards through our model.", "Some simple visualizations helped a lot when I was trying to understand the backpropagation process. Below is my mental picture of a simple neural network as it forward propagates from input to output. The process can be summarized by the following steps:", "And the objective of forward propagation is to calculate the activations at each neuron for each successive hidden layer until we arrive at the output.", "Now let\u2019s just reverse it. If you follow the red arrows (in the picture below), you will notice that we are now starting at the output of the magenta neuron. That is our output activation, which we use to make our prediction, and the ultimate source of error in our model. We then move this error backwards through our model via the same weights and connections that we use for forward propagating our signal (so instead of Activation 1, now we have Error1 \u2014 the error attributable to the top blue neuron).", "Remember we said that the goal of forward propagation is to calculate neuron activations layer by layer until we get to the output? We can now state the objective of backpropagation in a similar manner:", "We want to calculate the error attributable to each neuron (I will just refer to this error quantity as the neuron\u2019s error because saying \u201cattributable\u201d again and again is no fun) starting from the layer closest to the output all the way back to the starting layer of our model.", "So why do we care about the error for each neuron? Remember that the two building blocks of a neural network are the connections that pass signals into a particular neuron (with a weight living in each connection) and the neuron itself (with a bias). These weights and biases across the entire network are also the dials that we tweak to change the predictions made by the model.", "The magnitude of the error of a specific neuron (relative to the errors of all the other neurons) is directly proportional to the impact of that neuron\u2019s output (a.k.a. activation) on our cost function.", "So the error of each neuron is a proxy for the partial derivative of the cost function with respect to that neuron\u2019s inputs. This makes intuitive sense \u2014 if a particular neuron has a much larger error than all the other ones, then tweaking the weights and bias of our offending neuron will have a greater impact on our model\u2019s total error than fiddling with any of the other neurons.", "And the partial derivatives with respect to each weight and bias are the individual elements that compose the gradient vector of our cost function. So basically backpropagation allows us to calculate the error attributable to each neuron and that in turn allows us to calculate the partial derivatives and ultimately the gradient so that we can utilize gradient descent. Hurray!", "That\u2019s a lot to digest so hopefully this analogy will help. Almost everyone has had a terrible colleague at some point in his or her life \u2014 someone who would always play the blame game and throw coworkers or subordinates under the bus when things went wrong.", "Well neurons, via backpropagation, are masters of the blame game. When the error gets backpropagated to a particular neuron, that neuron will quickly and efficiently point the finger at the upstream colleague (or colleagues) who is most at fault for causing the error (i.e. layer 4 neurons would point the finger at layer 3 neurons, layer 3 neurons at layer 2 neurons, and so forth).", "And how does each neuron know who to blame, as the neurons cannot directly observe the errors of other neurons? They just look at who sent them the most signal in terms of the highest and most frequent activations. Just like in real life, the lazy ones that play it safe (low and infrequent activations) skate by blame free while the neurons that do the most work get blamed and have their weights and biases modified. Cynical yes but also very effective for getting us to the optimal set of weights and biases that minimize our cost function. To the left is a visual of how the neurons throw each other under the bus.", "And that in a nutshell is the intuition behind the backpropagation process. In my opinion, these are the three key takeaways for backpropagation:", "If you have read all the way here, then you have my gratitude and admiration (for your persistence).", "We started with a question, \u201cWhat makes deep learning special?\u201d I will attempt to answer that now (mainly from the perspective of basic neural networks and not their more advanced cousins like CNNs, RNNs, etc.). In my humble opinion, the following aspects make neural networks special:", "Author\u2019s Note: Neural networks and deep learning are extremely complicated subjects. I am still early in the process of learning about them. This blog was written as much to develop my own understanding as it was to help you, the reader. I look forward to all of your comments, suggestions, and feedback. Cheers!", "If you liked this article and my writing in general, please consider supporting my writing by signing up for Medium via my referral link here. Thanks!", "Neural Networks and Deep Learning by Michael A. Nielsen", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist. Founder Alpha Beta Blog. Doing my best to explain the complex in plain English. Support my writing: https://tonester524.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F19020b758230&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----19020b758230--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----19020b758230--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tonester524.medium.com/?source=post_page-----19020b758230--------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=post_page-----19020b758230--------------------------------", "anchor_text": "Tony Yiu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840a3210fbe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&user=Tony+Yiu&userId=840a3210fbe7&source=post_page-840a3210fbe7----19020b758230---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19020b758230&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19020b758230&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/understanding-logistic-regression-using-a-simple-example-163de52ea900", "anchor_text": "I wrote about that here"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid activation function"}, {"url": "https://towardsdatascience.com/understanding-logistic-regression-using-a-simple-example-163de52ea900", "anchor_text": "to go from log-odds to probability (do a control-f search for \u201csigmoid\u201d in my previous post)"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "Sigmoid"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "gradient descent optimization"}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "Mean Squared Error (MSE)"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "gradient descent"}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "great textbook (online and free!) for the detailed math (if you want to understand neural networks more deeply, definitely check it out)"}, {"url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)", "anchor_text": "regularization"}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "If you liked this article and my writing in general, please consider supporting my writing by signing up for Medium via my referral link here. Thanks!"}, {"url": "http://neuralnetworksanddeeplearning.com/index.html", "anchor_text": "Neural Networks and Deep Learning by Michael A. Nielsen"}, {"url": "https://en.wikipedia.org/wiki/Backpropagation#Finding_the_derivative_of_the_error", "anchor_text": "Wikipedia: Backpropagation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----19020b758230---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----19020b758230---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----19020b758230---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----19020b758230---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----19020b758230---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F19020b758230&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&user=Tony+Yiu&userId=840a3210fbe7&source=-----19020b758230---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F19020b758230&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&user=Tony+Yiu&userId=840a3210fbe7&source=-----19020b758230---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19020b758230&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----19020b758230--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F19020b758230&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----19020b758230---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----19020b758230--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----19020b758230--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----19020b758230--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----19020b758230--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----19020b758230--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----19020b758230--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----19020b758230--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----19020b758230--------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tonester524.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tony Yiu"}, {"url": "https://tonester524.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "102K Followers"}, {"url": "https://tonester524.medium.com/membership", "anchor_text": "https://tonester524.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840a3210fbe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&user=Tony+Yiu&userId=840a3210fbe7&source=post_page-840a3210fbe7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F78d3e392d884&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-neural-networks-19020b758230&newsletterV3=840a3210fbe7&newsletterV3Id=78d3e392d884&user=Tony+Yiu&userId=840a3210fbe7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}