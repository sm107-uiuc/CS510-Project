{"url": "https://towardsdatascience.com/xg-boost-the-reigning-champion-a993cb1896e2", "time": 1683017085.9695308, "path": "towardsdatascience.com/xg-boost-the-reigning-champion-a993cb1896e2/", "webpage": {"metadata": {"title": "XG Boost -The Reigning Champion. The base and Mathematics behind it | by Namrata Kapoor | Towards Data Science", "h1": "XG Boost -The Reigning Champion", "description": "XG Boost is one of the best machine learning algorithms known. It has high performance and speed due to various factors like parallelization, cache optimization, out of memory computation. Its\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["XG Boost is one of the best machine learning algorithms known. It has high performance and speed due to various factors like parallelization, cache optimization, out of memory computation. Its features like regularization, auto pruning, also missing values treatment makes it best in the class.", "Before going into mathematics and intuition of XG Boost it is necessary that reader has an idea of regularization, ensemble, bagging, boosting techniques like Ada boost and Gradient Boost.", "If you want an idea of it all description is given in easy language below.", "If you already have an idea of above terms you can straight away jump to XG Boost heading and get started.", "Ensemble-Ensemble technique is one which combines various algorithms to form one optimized predictive algorithm. Making small decision trees instead of one from random batch of data and aggregate them into one final, strong predictor.", "Bagging \u2014 Random Forest, All trees have equal say in final outcome. It is parallel learning model, all models are independent. The tress are expanded till whole length but still are weak learners.", "Boosting- Ada boost, It is sequential learning model, the model built afterwards are smarter and have more say in the final outcome. The trees are not expanded fully and are just one root of 2 leaf nodes known as stumps.", "Ada boost- It is derived from Adaptive Boosting. It is a training process by which only those features are selected in model by the predictive power is improved, reducing dimensionality and improving execution time as irrelevant features are removed gradually. This model learns through updating weights in sequence, i.e. models are dependent on each other. Boosting happens by increasing weights of misclassified dataset and decreasing weights of correctly classified dataset.", "In next model the misclassified from last model is tried to be classified correctly and hence adaptive learning happens in each sequential model. End model is one built by all previous learnings.", "AdaBoost is sensitive to noisy data and outliers", "1) The initial weights are one divided by the number of records in the dataset. Like 1/3 for above example.", "2) The weights are increased for misclassified or wrongly predicted record, example record 2 above and decreased for the rightly predicted ones so that their sum remains same. It is done so that the algorithm concentrates and emphasizes more on the data which is difficult to predict or classify.", "3) In iteration 2 weights are again updated based on results and wrongly predicted ones are tried to be predicted perfectly.", "4) In Final iteration Learning happens and weights are updated.", "Amount of say or weight in model is determined by formula", "When stump is good then total error is near to Zero, amount of say will be near to 1, when stump is horrible the total error is near to one, Amount of say will be large negative value in this case.", "If total error is 0.5 that is half misclassified the Amount of say will be 0.", "New Sample Weight for wrongly Classified=Sample weight * e amount of say", "New Sample Weight for correctly classified=Sample weight * e -amount of say", "Increase of weight means the misclassified sample is added more than once in new data set giving it more weightage or priority, it means if this sample will be misclassified again the penalty will be high as the samples are more than once in new data.", "GradientBoost-It is a machine learning technique for regression as well as for classification problems and uses sequential learning through ensemble techniques. It uses optimization of Loss Function for better learning, i.e . Difference between Actual and predicted will be tried to bring to zero, i.e to the most close value to actual. Trees are better grown and not stumps like in Adaboost. Leaf nodes range from 8\u201332.", "1) It calculates the average of the target column for regression problem, example 160 in above case. For classification problem probability will come under picture.", "2) Calculates residual i.e. Actual -Predicted values", "3) Residual becomes the target column and model is fit on it.", "4) Residual values are then predicted with model, predictions will change.", "Iteration 1: New Predicted Value =Average Value(step 1) + learning rate(0.1)* New residual Value.", "1) Ridge \u2013Ridge regression works when most of the features in a model are useful. Its shrinks the parameters but do not remove any of them.", "Suppose data is less to train the model i.e. blue dots are train data. If we have only 2 points we try to make a linear regression line y=mx+b , m is slope and b is intercept here.", "Now if we see test data then we see that there is a lot of variance and the line crossing over train data was an overfitting model.", "Now if we add bias and decrease the slope, we will see it better fits the test data, and not an overfit for train data. Here we see how a ridge makes a model better.", "If \u019b=0 Line = Line of least squares.", "If \u019b=1, Line introduces bias and variance is decreased, slope is lesser, i.e. line is less sensitive to x.", "With increase of \u019b the slope gets smaller and smaller till it reaches near to zero but not equal to zero. Hence getting lesser sensitive to x.", "We use 10 fold cross validations to determine which value of \u019b gives least variation.", "We always need more data than parameters to determine the model, but if the data set is small we use Ridge regression, which uses cross validation and reduces variance by making predictions less sensitive to the training data which is done by adding penalty i.e. \u019b * m**2 and lambda is determined by using cross validation.", "Ridge regression works well if all parameters are important in prediction.", "2) Lasso-It is similar to Ridge regression. When model contains lots of useless features then Lasso regression is used. It removes useless features and make the model that is easy to interpret.", "Lasso lambda is used to regularize and to avoid overfitting if records of data are less. It uses absolute value of errors instead of squared values as in ridge regression.", "Lasso Regression Line = \u019b * |m | + Sum of squared residuals", "If \u019b=0 Line = Line of least squares.", "If \u019b=1, Line introduces bias and variance is decreased, slope is lesser, i.e. line is less sensitive to x.", "With increase of \u019b the slope gets smaller and smaller till it reaches to zero, i.e almost insensitive to x data. Which means all the features which are unimportant in predicting the values in a model are removed totally.", "It is hence better than Ridge regression in the sense that it reduces variance and removes unimportant features from predicting.", "3) Elastic Net: When there are millions of features and it is nearly impossible to know all of them we use Elastic Net model, which is a mixture of both Lasso and Ridge models and uses their best features to give an optimized model.", "It is especially good when there is correlation between parameters. Lasso regression just picks one of the correlated parameter and eliminates the others which are less correlated. Ridge regression shrinks the parameters of all the correlated features together but do not remove them.", "By combining both penalties of Lasso and Ridge it gives the best of both.", "If \u019b1 , \u019b2 both are zero it is least square method.", "If both If \u019b1 >0 \u019b2 >0 , It is elastic regression which will take best of both.", "XGBoost is a sequential Ensemble model, it is extreme gradient boosting model. It is extension of gradient boost. Now understand the Maths behind it.", "1) We make a base model first with average of target field, and calculating loss.", "2) Next Model is made on loss errors and tries to minimize the errors.", "b) \u03b5 (Learning rate) which is measure of how fast we want to converge values to actual.", "Whenever gamma value is less than gain value then split will happen otherwise split will not happen", "d) Similarity Score= Sum of square of residual/ Number of rows + \u019b", "e) Gain= Similarity Score after split \u2014 Similarity Score of node above(branch before split)", "Lesser gain at each level means that to control the tree to overfit, \u019b value can be increased.", "Impact of outlier on prediction will come down significantly if \u019b is more.", "Hence regularization is used to control Overfitting.", "As the similarity score and gain value will also come down because of it.", "Suppose here \u0264 value given is 60, then again split will happen and gain is calculated else if \u0264 value given is 70, no further split will happen. That is how auto-pruning will happen.", "Let\u2019s take example of row 3 i.e. 179 cms since age is more than 16 its gain is 67.5", "It is done in same way for all records. And iterations are done when the values are near to actual ones.", "There are lot other things that make XG Boost the best is its features like:", "Hardware Optimization: This algorithm is designed to make use of hardware resources efficiently. This is done by cache awareness by allocating internal buffers in each thread to store gradient statistics. Out-of-core computing optimize available disk space while handling big data-frames that do not fit into memory.", "Missing Values Handling: Missing values are handled in XG Boost in the best possible way, by using various techniques. And Cross Validation is done at each step to be sure.", "XG Boost is by far deemed as the best algorithm for Supervised learning. CatBoost and Light GBM are very near in race and its a matter of time when a new algorithm will come and will be better than XG Boost.", "This algorithm is almost perfect in terms of performance and predictive capabilities, however one needs to study it in depth to know about it and its techniques of internal functioning.", "If there are a lot of features which are categorical in nature then CatBoost of Light GBM is recommended as XG Boost may convert them to One Hot encoding resulting in sparse dataset. which is not good for decision trees.", "However in Classification and Regression models it is still reigning the Machine learning World with its extra Ordinary capabilities to deal with data.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science Professional | Technical Blogger | Artificial Intelligence | NLP | Chatbots and more"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa993cb1896e2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://namratakapoor1.medium.com/?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": ""}, {"url": "https://namratakapoor1.medium.com/?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "Namrata Kapoor"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae90d36721ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&user=Namrata+Kapoor&userId=ae90d36721ac&source=post_page-ae90d36721ac----a993cb1896e2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa993cb1896e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa993cb1896e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/IkYuzPneQWs", "anchor_text": "Source"}, {"url": "https://www.numpyninja.com/post/xg-boost-the-base-and-mathematics-behind-it", "anchor_text": "https://www.numpyninja.com"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----a993cb1896e2---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a993cb1896e2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a993cb1896e2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a993cb1896e2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/boosting?source=post_page-----a993cb1896e2---------------boosting-----------------", "anchor_text": "Boosting"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa993cb1896e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&user=Namrata+Kapoor&userId=ae90d36721ac&source=-----a993cb1896e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa993cb1896e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&user=Namrata+Kapoor&userId=ae90d36721ac&source=-----a993cb1896e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa993cb1896e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa993cb1896e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a993cb1896e2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a993cb1896e2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a993cb1896e2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a993cb1896e2--------------------------------", "anchor_text": ""}, {"url": "https://namratakapoor1.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://namratakapoor1.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Namrata Kapoor"}, {"url": "https://namratakapoor1.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "211 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae90d36721ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&user=Namrata+Kapoor&userId=ae90d36721ac&source=post_page-ae90d36721ac--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fccc842effc8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxg-boost-the-reigning-champion-a993cb1896e2&newsletterV3=ae90d36721ac&newsletterV3Id=ccc842effc8c&user=Namrata+Kapoor&userId=ae90d36721ac&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}