{"url": "https://towardsdatascience.com/a-different-kind-of-deep-learning-part-2-b447ff469255", "time": 1682994307.8675368, "path": "towardsdatascience.com/a-different-kind-of-deep-learning-part-2-b447ff469255/", "webpage": {"metadata": {"title": "A different kind of (deep) learning: part 2 | by Gidi Shperber | Towards Data Science", "h1": "A different kind of (deep) learning: part 2", "description": "In the previous post, we\u2019ve discussed some self supervised learning articles, along with some attempts to strive towards the \u201choly grail\u201d: exploiting the almost unlimited number of un-annotated\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/a-different-kind-of-deep-learning-part-1-90fe6c52f1ab", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "https://youtu.be/U2mhZ9E8Fk8?t=2425", "anchor_text": "talk", "paragraph_index": 1}, {"url": "https://youtu.be/8881p8p3Guk?t=3004", "anchor_text": "talk", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "GAN", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1411.1784", "anchor_text": "conditional GAN", "paragraph_index": 11}, {"url": "http://richzhang.github.io/colorization/", "anchor_text": "work", "paragraph_index": 13}, {"url": "https://phillipi.github.io/pix2pix/", "anchor_text": "Pix2pix", "paragraph_index": 13}, {"url": "https://medium.com/@ManishChablani/cyclegans-and-pix2pix-5e6a5f0159c4", "anchor_text": "post", "paragraph_index": 13}, {"url": "https://affinelayer.com/pixsrv/", "anchor_text": "highly", "paragraph_index": 16}, {"url": "https://vimeo.com/260612034", "anchor_text": "creative", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1605.09782", "anchor_text": "BiGAN", "paragraph_index": 19}, {"url": "https://junyanz.github.io/CycleGAN/", "anchor_text": "CycleGAN", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1611.09842.pdf", "anchor_text": "work", "paragraph_index": 29}, {"url": "https://openreview.net/forum?id=S1v4N2l0-", "anchor_text": "work", "paragraph_index": 33}], "all_paragraphs": ["In the previous post, we\u2019ve discussed some self supervised learning articles, along with some attempts to strive towards the \u201choly grail\u201d: exploiting the almost unlimited number of un-annotated images available wherever to generalize for other tasks. And hopefully, get closer to the currently unmet benchmark of ImageNet pre-training.", "Surprisingly, or perhaps not so surprisingly, we\u2019ve got some extra tailwind from Yan Lecun, which devoted a few minutes of his NeurIPS talk (\u201cThe Next Step Towards Artificial Intelligence\u201d) to self supervised learning. He described self supervised learning as \u201cthe body of the cake\u201d when the topping is supervised learning, and the cherry is reinforcement learning (because the sparsity of the reward in RL). Lecun also takes some slides from our favorite self supervised researcher, Aloysha Efros, so I\u2019m happy we share interests.", "Additionally, a few readers have pointed out that there are also prominent self supervised body of work dealing with videos, which is evident. However, videos will be discussed in a later post, since we have another topic, which is the generative models.", "In his talks about self supervision, Efros (yes, he is going to be dominant in this post as well) frequently discusses the difficulty in finding the right loss function for self supervised tasks.", "In the previous post we examined the special classification loss used for the colorization task, and emphasized the difficulty of finding the right loss function for them.", "In the talk, Efros described a method for finding such loss functions. He called it: \u201cgraduate student descent\u201d. In other words, there is a lot of trial and error in finding a good loss function for these models. So can we have some better way, more universal, of finding them?", "Additionally, there is the colorization Turing test thing: to evaluate the results, researchers use mechanical Turks to tell between real and fake photos. So wishfully, we would like to have some kind of mechanism to tell between these two types of images.", "If you were into deep learning back in 2014, you probably remember that when Ian Goodfellow presented his groundbreaking GAN work for the first time, the community was very excited about the promising generational abilities, but many researchers were skeptical about the purpose of this work. To them, it was merely a toy, at least until some significant progress to be made.", "The self-supervised researchers had some different thoughts: The GAN, in their eyes, was potentially a custom loss function for the self-supervised tasks.", "Let\u2019s think about it for a second: in the colorization work, we\u2019ve used standard deep learning paradigm for predicting color for each pixel. Can we use the power of GAN discriminator as a custom loss? if so, it will require structuring the problem in a different way.", "We know that GAN in its essence generates images from a completely random distribution. What if we can make it generate a colored image given a black and white image, using the discriminator to evaluate the result?", "This requires some change in paradigm: generating images from something different then a completely random distribution was done by the conditional GAN: adding a feature to the generator, making it produce some sub set of the target space. E.g, a specific number from Mnist dataset. But if we can use a scalar (digit)as the \u201cconditional\u201d, we can use a vector as well. And if we can use a vector, we can also use a tensor. And an image is merely a tensor, isn\u2019t it?", "So here is the idea:train a conditional-GAN-like network, which the condition (as well as the input to the Generator) is a black and white image, which will constrain the output to be a colored image.", "Phillip Isola, a student of Efros that was also involved in the previously discussed colorization work, took on this task in the paper \u201cImage-to-Image Translation with Conditional Adversarial Networks\u201d that was nicknamed Pix2pix. This required a serious tweaking with GAN architecture: first using an encoder-decoder architecture for the generator. Second, the discriminator can\u2019t just get randomly paired images from dataset and generator. It should be fed with strict pairs of images, one is the original RGB, and the other is generated form black and white. The discriminator architecture and training schedule are also different from standard. You can read a nice explanation about it in this post. It is evident that a significant amount of hard work was put into this paper.", "But Isola went one step further: He probably said to himself: well, if I succeeded in building a colorizing-GAN, which learns from pairs of images, why can\u2019t I apply it to different pairs? what about:", "And so on. And it all worked.", "This became one of the most interesting deep learning works in last years (which means, ever), and it triggered something that Efros called: \u201cTwitter-driven research\u201d. Since the code of the paper was readily available on GitHub, many people trained it on various pairings of images, and reached some highly creative results. And these as well:", "You may also find more by looking for #pix2pix on twitter. Efros said that these projects amazed him, brought many new ideas and took their research many steps forward.", "OK, we got a bit carried away \u2014 all this GAN excitement opened a variety of new options. However, in the route to innovation a small detail was lost: the self supervised paradigm, which intended to use a self supervised model for transfer learning, was neglected along the way, and wasn\u2019t even mentioned in in this paper. Perhaps the Generator architecture was too different to try this, or perhaps the importance of the generative results overshadowed the potential of yet one more partially-successful self supervised attempt.", "Well, the pix2pix work is \u201cnatural successor\u201d of the colorization and context works from the first post. But there was another work that actually did try to apply transfer learning on a GAN network: the bi-directional GAN \u2014 BiGAN.", "BiGAN presented a new concept (back then, 2016): along with standard GAN architecture, an Encoder is added to the architecture (see bellow). This encoder is intended to learn the inverse of the generator, for different purposes.", "The work takes a very interesting approach: taking a standard GAN architecture, and instead of presenting the discriminator with x (a the real image) and G(z) (the generated image, when z is the random input to generator) the discriminator is fed with 2 pairs: (x,E(x)),(G(z),z), which means the random input z and E(x), which is the encoder function tries to replicate the random input. Interestingly, there is no shared knowledge between the Generator and the Encoder.", "This is a bit tricky to grasp, so you can read further details in the paper - formal (and intuitive) explanations about the encoder and generator must learn to invert one another to fool the discriminator.", "Although there is no conditional element here \u2014 z is not a label, the Encoder can be used for classification (and hence detection and segmentation, after switching some layers) with transfer learning. Results may be described as \u201creasonable\u201d.", "If you feel that similar ideas appear in the BiGan and in the Pix2Pix works (I should mention that BiGan came out earlier) it is not by chance. The successive paper of pix2pix, the CycleGAN, was a combination of the two, and allowed training such networks without \u201cpaired\u201d training images, and significantly expand the transferable objects, and create the famous transfer of zebra to horse (and back).", "So we\u2019ve seen GANs have great potential (somehow yet to be fulfilled) in self supervised learning. But what about their older, currently less popular counterpart, the autoencoder?", "Indeed, autoencoders have reached good results on some tasks, but they always suffer from information losses through their layers.", "In self supervised learning they had some success though.", "In our discussion of colorization in previous post, we\u2019ve mentioned that colorization is actually a cross channel encoder. Meaning, using some channels to predict others. What if we give a chance here to real auto-encoders and define channels bit differently?", "More specifically, by trying to reconstruct half of the image by the other half. The following work, named \u201csplit-brain\u201d does exactly this. It defines the task of bisecting an image diagonally, and using the auto-encoder to predict one half, using the other.", "Going further, every image can be used twice, using every half for predicting the other.", "Seeing this method work reasonably well for diagonal bisection, researchers went back to color, doing back and forth predicting: RBG by B&W and vice versa, b&w by color (using specifically Lab space)", "Pursuant the previous post I\u2019ve got some questions regarding the actual transfer learning results of the self supervised models. As said there, the important feature is the generalization on different tasks, e.g detection and segmentation. Intentionally, I don\u2019t put too much emphasis on these results since they are quite fragile, and always keep a quite stable difference of 10% from their goal: ImageNet pretraining.", "The above table is taken from the rotation work, which is perhaps surprisingly current \u201cstate of the art\u201d in self-supervised transfer learning. Most other discussed papers are there as well. However, this should be taken with a grain of salt since:", "So once again, It seems there is a lot of potential, especially in the custom-loss function idea, but results are not \u201cthere\u201d yet. However, we still have some reasons to be optimistic: fortunately, visual signal is not limited to images, but is also found in\u2026 videos. Videos add the important the dimension of time, which in its turn adds an immense number of new possible tasks, paradigms and options, and eventually, some real results(!). This and more will be discussed in the next post so stay tuned.", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @shgidi", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb447ff469255&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b447ff469255--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b447ff469255--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://gidishperber.medium.com/?source=post_page-----b447ff469255--------------------------------", "anchor_text": ""}, {"url": "https://gidishperber.medium.com/?source=post_page-----b447ff469255--------------------------------", "anchor_text": "Gidi Shperber"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1dbbeb01604b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&user=Gidi+Shperber&userId=1dbbeb01604b&source=post_page-1dbbeb01604b----b447ff469255---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb447ff469255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb447ff469255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/a-different-kind-of-deep-learning-part-1-90fe6c52f1ab", "anchor_text": "previous post"}, {"url": "https://youtu.be/U2mhZ9E8Fk8?t=2425", "anchor_text": "talk"}, {"url": "https://youtu.be/8881p8p3Guk?t=3004", "anchor_text": "talk"}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "GAN"}, {"url": "https://arxiv.org/abs/1411.1784", "anchor_text": "conditional GAN"}, {"url": "http://richzhang.github.io/colorization/", "anchor_text": "work"}, {"url": "https://phillipi.github.io/pix2pix/", "anchor_text": "Pix2pix"}, {"url": "https://medium.com/@ManishChablani/cyclegans-and-pix2pix-5e6a5f0159c4", "anchor_text": "post"}, {"url": "https://affinelayer.com/pixsrv/", "anchor_text": "highly"}, {"url": "https://vimeo.com/260612034", "anchor_text": "creative"}, {"url": "https://arxiv.org/abs/1605.09782", "anchor_text": "BiGAN"}, {"url": "https://junyanz.github.io/CycleGAN/", "anchor_text": "CycleGAN"}, {"url": "https://arxiv.org/pdf/1611.09842.pdf", "anchor_text": "work"}, {"url": "https://openreview.net/forum?id=S1v4N2l0-", "anchor_text": "work"}, {"url": "https://towardsdatascience.com/a-different-kind-of-deep-learning-part-1-90fe6c52f1ab", "anchor_text": "An intro to self supervised learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b447ff469255---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b447ff469255---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b447ff469255---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb447ff469255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&user=Gidi+Shperber&userId=1dbbeb01604b&source=-----b447ff469255---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb447ff469255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&user=Gidi+Shperber&userId=1dbbeb01604b&source=-----b447ff469255---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb447ff469255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b447ff469255--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb447ff469255&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b447ff469255---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b447ff469255--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b447ff469255--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b447ff469255--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b447ff469255--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b447ff469255--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b447ff469255--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b447ff469255--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b447ff469255--------------------------------", "anchor_text": ""}, {"url": "https://gidishperber.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://gidishperber.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gidi Shperber"}, {"url": "https://gidishperber.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.8K Followers"}, {"url": "http://Shibumi.AI", "anchor_text": "Shibumi.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1dbbeb01604b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&user=Gidi+Shperber&userId=1dbbeb01604b&source=post_page-1dbbeb01604b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Faa67cb16739c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-different-kind-of-deep-learning-part-2-b447ff469255&newsletterV3=1dbbeb01604b&newsletterV3Id=aa67cb16739c&user=Gidi+Shperber&userId=1dbbeb01604b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}