{"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86", "time": 1683011547.706021, "path": "towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86/", "webpage": {"metadata": {"title": "Choosing the right GPU for deep learning on AWS | by Shashank Prasanna | Towards Data Science", "h1": "Choosing the right GPU for deep learning on AWS", "description": "Just a decade ago, if you wanted access to a GPU to accelerate your data processing or scientific simulation code, you\u2019d either have to get hold of a PC gamer or contact your friendly neighborhood\u2026"}, "outgoing_paragraph_urls": [{"url": "https://aws.amazon.com/ec2/instance-types/p4/", "anchor_text": "P4 instances", "paragraph_index": 18}, {"url": "https://www.nvidia.com/en-us/data-center/a100/", "anchor_text": "NVIDIA A100 GPUs", "paragraph_index": 18}, {"url": "https://aws.amazon.com/blogs/compute/amazon-ec2-p4d-instances-deep-dive/", "anchor_text": "record setting training performance", "paragraph_index": 18}, {"url": "https://www.tensorflow.org/guide/mixed_precision", "anchor_text": "TensorFlow", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/notes/amp_examples.html", "anchor_text": "PyTorch", "paragraph_index": 21}, {"url": "https://mxnet.apache.org/api/python/docs/tutorials/performance/backend/amp.html", "anchor_text": "MXNet", "paragraph_index": 21}, {"url": "https://cloud.google.com/tpu/docs/bfloat16", "anchor_text": "supported BF16", "paragraph_index": 22}, {"url": "https://www.nvidia.com/en-us/data-center/nvlink/", "anchor_text": "3rd generation NVLink", "paragraph_index": 25}, {"url": "https://aws.amazon.com/ec2/instance-types/p3/", "anchor_text": "P3 instances", "paragraph_index": 37}, {"url": "https://www.nvidia.com/en-us/data-center/v100/", "anchor_text": "NVIDIA V100 GPUs", "paragraph_index": 37}, {"url": "https://github.com/horovod/horovod", "anchor_text": "libraries like Horovod", "paragraph_index": 42}, {"url": "https://www.tensorflow.org/guide/distributed_training", "anchor_text": "tf.distribute.Strategy", "paragraph_index": 42}, {"url": "https://pytorch.org/docs/stable/distributed.html", "anchor_text": "torch.distributed", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/a-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e?source=friends_link&sk=0a1f6a2e7716d4272c79156cf7c5f294", "anchor_text": "A quick guide to distributed training with TensorFlow and Horovod on Amazon SageMaker", "paragraph_index": 43}, {"url": "https://aws.amazon.com/ec2/instance-types/g4/", "anchor_text": "G4 instances", "paragraph_index": 47}, {"url": "https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html", "anchor_text": "NVIDIA\u2019s support matrix", "paragraph_index": 50}, {"url": "https://aws.amazon.com/ec2/instance-types/g4/", "anchor_text": "product G4 instance page", "paragraph_index": 52}, {"url": "https://aws.amazon.com/ec2/instance-types/p2/", "anchor_text": "P2 instances", "paragraph_index": 54}, {"url": "https://aws.amazon.com/ec2/instance-types/g3/", "anchor_text": "G3 instances", "paragraph_index": 59}, {"url": "https://towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c?source=friends_link&sk=adf7b78cea61ed9b8415c8371777ba3d", "anchor_text": "A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference", "paragraph_index": 65}, {"url": "https://aws.amazon.com/ec2/spot/bid-advisor/", "anchor_text": "Spot Instance Advisor", "paragraph_index": 67}, {"url": "https://aws.amazon.com/sagemaker/?trk=el_a134p000006vgXgAAI&trkCampaign=NA-FY21-GC-400-FTSA-SAG-Overview&sc_channel=el&sc_campaign=Y21-SageMaker_shshnkp&sc_outcome=AIML_Digital_Marketing", "anchor_text": "Amazon SageMaker", "paragraph_index": 68}, {"url": "https://towardsdatascience.com/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68?source=friends_link&sk=937b581bef3605c84c07cfd32c5842d1", "anchor_text": "A quick guide to using Spot instances with Amazon SageMaker", "paragraph_index": 70}, {"url": "https://towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c?source=friends_link&sk=adf7b78cea61ed9b8415c8371777ba3d", "anchor_text": "A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference", "paragraph_index": 71}, {"url": "https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html", "anchor_text": "AWS Deep Learning AMIs", "paragraph_index": 73}, {"url": "https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html", "anchor_text": "AWS Deep Learning Containers (DLC)", "paragraph_index": 73}, {"url": "https://medium.com/@shashankprasanna", "anchor_text": "medium", "paragraph_index": 77}, {"url": "https://twitter.com/shshnkp", "anchor_text": "@shshnkp", "paragraph_index": 77}, {"url": "https://www.linkedin.com/in/shashankprasanna/", "anchor_text": "LinkedIn", "paragraph_index": 77}, {"url": "http://shashankprasanna.com", "anchor_text": "shashankprasanna.com", "paragraph_index": 79}], "all_paragraphs": ["Just a decade ago, if you wanted access to a GPU to accelerate your data processing or scientific simulation code, you\u2019d either have to get hold of a PC gamer or contact your friendly neighborhood supercomputing center. Today, you can log on to your AWS console and choose from a range of GPU based Amazon EC2 instances.", "What GPUs can you access on AWS you ask? You can launch GPU instances with different GPU memory sizes (8 GB, 16 GB, 24 GB, 32 GB, 40 GB), NVIDIA GPU generations (Ampere, Turing, Volta, Maxwell, Kepler) different capabilities (FP64, FP32, FP16, INT8, Sparsity, TensorCores, NVLink), different number of GPUs per instance (1, 2, 4, 8, 16), and paired with different CPUs (Intel, AMD, Graviton2). You can also select instances with different vCPUs (core thread count), system memory and network bandwidth and add a range of storage options (object storage, network file systems, block storage, etc.) \u2014 in summary, you have options.", "My goal with this blog post is to provide you with guidance on how you can choose the right GPU instance on AWS for your deep learning projects. I\u2019ll discuss key features and benefits of various EC2 GPU instances, and workloads that are best suited for each instance type and size. If you\u2019re new to AWS, or new to GPUs, or new to deep learning, my hope is that you\u2019ll find the information you need to make the right choice for your projects.", "In a hurry? just want the final recommendation without the deep dive? I got you covered. Here are 5 GPU instance recommendations that should serve majority of deep learning use-cases. However, I do recommend you come back and review the rest of the article so you can make a more informed decision.", "Instance: p4d.24xlargeWhen to use it: When you need all the performance you can get. Use it for distributed training on large models and datasets.What you get: 8 x NVIDIA A100 GPUs with 40 GB GPU memory per GPU. Based on the latest NVIDIA Ampere architecture. Includes 3rd generation NVLink for fast multi-GPU training.", "Instance: p3.2xlargeWhen to use it: When you want the highest performance Single GPU and you\u2019re fine with 16 GB of GPU memory.What you get: 1 x NVIDIA V100 GPU with 16 GB of GPU memory. Based on the older NVIDIA Volta architecture. The best performing single-GPU is still the NVIDIA A100 on P4 instance, but you can only get 8 x NVIDIA A100 GPUs on P4. This GPU has a slight performance edge over NVIDIA A10G on G5 instance discussed next, but G5 is far more cost-effective and has more GPU memory.", "Instance: g5.xlargeWhen to use it: When you want high-performance, more GPU memory at lower cost than P3 instanceWhat you get: 1 x NVIDIA A10G GPU with 24 GB of GPU memory, based on the latest Ampere architecture. NVIDIA A10G can be seen as a lower powered cousin of the A100 on the p4d.24xlarge so it\u2019s easy to migrate and scale when you need more compute. Consider larger sizes withg5.(2/4/8/16)xlarge for the same single-GPU with more vCPUs and higher system memory if you have more pre or post processing steps.", "Instance: p3.(8/16)xlargeWhen to use it: Cost-effective multi-GPU model development and training.What you get: p3.8xlarge has 4 x NVIDIA V100 GPUs and p3.16xlarge has 8 x NVIDIA V100 GPUs with 16 GB of GPU memory on each GPU, based on the older NVIDIA Volta architecture. For larger models, datasets and faster performance consider P4 instances.", "Instance: g4dn.xlargeWhen to use it: Lower performance than other options at lower cost for model development and training. Cost effective model inference deployment. What you get: 1 x NVIDIA T4 GPU with 16 GB of GPU memory. Based on the previous generation NVIDIA Turing architecture. Consider g4dn.(2/4/8/16)xlarge for more vCPUs and higher system memory if you have more pre or post processing.", "With that you should have enough information to get started with your project. If you\u2019re still itching to learn more, let\u2019s dive deep and geek out on every instance type, GPU type, their features on AWS and discuss when and why you should consider each of them.", "Or why you should look at the whole system and not just the type of GPU", "A GPU is the workhorse of a deep learning system, but the best deep learning system is more than just a GPU. You have to choose the right amount of compute power (CPUs, GPUs), storage, networking bandwidth and optimized software that can maximize utilization of all available resources.", "Some deep learning models need higher system memory or a more powerful CPU for data pre-processing, others may run fine with fewer CPU cores and lower system memory. This is why you\u2019ll see many Amazon EC2 GPU instances options, some with the same GPU type but different CPU, storage and networking options. If you\u2019re new to AWS or new to deep learning on AWS, making this choice can feel overwhelming.", "Let\u2019s start with high-level EC2 GPU instance nomenclature on AWS. There are two families of GPU instances \u2014 the P family and the G family of EC2 instances and the chart below shows the various instance generations and instance sizes.", "Historically P instance type represented GPUs better suited for High-performance computing (HPC) workloads, characterized by their higher performance (higher wattage, more cuda cores) and support for double precision (FP64) used in scientific computing. G instance types had GPUs better suited for graphics and rendering, characterized by their lack of double precision and lower cost/performance ratio (Lower wattage, smaller number of cuda cores).", "All this has started to change as the amount of machine learning workloads on GPUs are growing rapidly in recent years. Today, the newer generation P and G instance types are both suited for machine learning. P instance type is still recommended for HPC workloads and demanding machine learning training workloads and I recommend G instance type for machine learning inference deployments and less compute intensive training. All this will become clearer in the following section when we discuss specific GPU instance types.", "Each instance size has a certain vCPU count, GPU memory, system memory, GPUs per instance, and network bandwidth. The number next to the letter (P3, G5) represent the instance generation. Higher the number, the newer the instance type is. Each instance generation can have GPUs with different architecture and the timeline image below shows NVIDIA GPU architecture generations, GPU types and the corresponding EC2 instance generations.", "Now let\u2019s take a look at each of these instances by family, generation and sizes in the order listed below.", "P4 instances provide access to NVIDIA A100 GPUs based on NVIDIA Ampere architecture. It only comes in one size \u2014 a multi-GPUs per instance with 8 A100 GPUs with 40 GB of GPU memory per GPU, 96 vCPU, and 400 Gbps network bandwidth for record setting training performance.", "Every new GPU generation is faster than the previous generation, and there\u2019s no exception here. NVIDIA A100 is significantly faster than NVIDIA V100 (found on P3 instances discussed later) but also includes newer precision types suited for deep learning, particularly BF16 and TF32.", "Deep learning training is typically done in single precision or FP32. The choice of FP32 IEEE standard format pre-dates deep learning, so hardware and chip manufacturers have started to support newer precision types that work better for deep learning. This is a perfect example of hardware evolving to suit the needs of application vs. developers having to change applications to work on existing hardware.", "The NVIDIA A100 includes special cores for deep learning called Tensor Cores to run mixed-precision training, which was first introduced in the Volta architecture. Rather than training the model in single precision (FP32), your deep learning framework can use Tensor Cores to perform matrix multiplication in half-precision (FP16) and accumulate in single precision (FP32). This often requires updating your training scripts, but can lead to much higher training performance. Each framework handles this differently, so refer to your framework\u2019s official guides (TensorFlow, PyTorch and MXNet) for using mixed-precision.", "The NVIDIA A100 GPU supports two new precision formats \u2014 BF16 and TensorFloat-32 (TF32). The advantage of TF32 is that the TF32 Tensor Cores on the NVIDIA A100 can read FP32 data from the deep learning framework and use and produces a standard FP32 output, but internally it uses reduced internal precision. This means that unlike mixed precision training which often required code changes to your training scripts, frameworks like TensorFlow and PyTorch can support TF32 out of the box. BF16 is an alternative to IEEE FP16 standard that has a higher dynamic range, better suited for processing gradients without loss in accuracy. TensorFlow has supported BF16 for a while, and you can now take advantage of BF16 precision on NVIDIA A100 GPU when using p4d.24xlarge instances.", "P4 instance come in only 1 size: p4d.24xlarge. Let\u2019s take a closer look.", "If you need the absolutely fastest training GPU instance in the cloud then look no further than the p4d.24xlarge. This title was previously held by the p3dn.24xlarge, which had 8 Volta architecture based NVIDIA V100 GPU.", "You get access to 8 NVIDIA A100 GPUs with 40 GB GPU memory, interconnected with 3rd generation NVLink that theoretically double the inter-GPU bandwidth compared to the 2nd generation NVLink on the NVIDIA V100 available on the P3 instance type we\u2019ll discuss in the next section. This makes p4d.24xlarge instance type ideal for distributed data parallel training as well as model-parallel training of large models that don\u2019t fit on a single GPU. The instance also gives you access to 96 vCPUs, 1152 GB system memory (highest ever on a EC2 GPU instance) and 400 Gbps network bandwidth (highest ever on a EC2 GPU instance) which becomes important for achieving near-linear scaling for large-scale distributed training jobs.", "Runnvidia-smi on this instance and you can see that the GPU memory is 40 GB. This is the largest GPU memory per GPU, you\u2019ll find on AWS today. If your models are large or you\u2019re working on 3D images or other large data batches, then this is the instance to consider. Run nvidia-smi topo -matrix and you\u2019ll see that NVLink is used for between-GPU communication. NVlink provides much higher inter-GPU bandwidth compared to PCIe and this means that multi-GPU and distributed training jobs will run much faster.", "G5 instances are interesting as there are two types of NVIDIA GPUs under this instance type. This is a departure from all other instance types which have a 1:1 relationship between EC2 instance type and GPU architecture type.", "And they each come in different instance sizes that include single and multi-GPU instances.", "First let\u2019s take a look at G5 instance type and particularly g5.xlargeinstance size which I discussed in the Key takeaway/recommendation list at the beginning.", "The GPU instances:g5.(2/4/8/16)xlarge offer the best performance per cost ratio for single-GPU instance on AWS. Start with g5.xlarge as your single GPU model development, prototyping and training instance. You can increase the size to g5.(2/4/8/16).xlarge for more vCPUs and system memory to better handle data pre and post processing that rely on CPU power. You can get access to single and multi-GPU instance sizes (4 GPUs, 8 GPUs). The single GPU options g5.(2/4/8/16)xlarge with NVIDIA A10G offer the best performance/cost profile for training and inference deployment.", "If you take a look at the output of nvidia-smi for g5.xlarge instance you\u2019ll see that the Thermal Design Power (TDP), which is the maximum power the GPU can draw, is 300W. Compare this with the output of nvidia-smi shown in the P4 section above which shows a TDP of 400W. This makes NVIDIA A10G in the G5 instances is a lower powered cousin of the NVIDIA A100 found on the P4 instance type. Since it\u2019s also based on the same NVIDIA Ampere architecture and this means it includes all the features supported on the P4 instance type.", "This makes G5 instances perfect for single GPU training and migrating your training workload to P4 if your models and data size grows and you need to do distributed training or if you want to run multiple parallel training experiements on a faster GPU.", "Although the you get access to multi-GPU instance sizes, I do not recommend them for multi-GPU distributed training, since there is no NVIDIA high-bandwidth NVLink GPU interconnect, and communication will fall back to PCIe which is significantly slower. The multi-GPU options on G5 are meant to host multiple models on each GPU for inference deployment use-cases.", "Unlike G5 instances, G5g instance offers NVIDIA T4G GPUs which are based on the older NVIDIA Turing architecture. NVIDIA T4G GPU\u2019s closest cousin is the NVIDIA T4 GPU available on the Amazon EC2 G4 instance that I\u2019ll discuss in the next section. The key difference between the G5g instance and G4 instance is interestingly the choice of CPU.", "The G5g instance offers an ARM based AWS Graviton2 CPU, andThe G4 instance offers x86 based Intel Xeon Scalable CPU.The GPUs (T4 and T4g) are very similar in performance profiles", "our choice between these two should come down to the CPU architecture you prefer. My personal preference for machine learning today would be the G4 instance over the G5g instance since more open-source frameworks are designed to run on Intel CPUs vs ARM based CPUs.", "P3 instances provide access to NVIDIA V100 GPUs based on NVIDIA Volta architecture and you can launch a single GPU per instance or multiple GPUs per instance (4 GPUs, 8 GPUs). A single GPU instance p3.2xlarge can be your daily driver for deep learning training. And the most capable instance p3dn.24xlarge gives you access to 8 x V100 with 32 GB GPU memory, 96 vCPUs, 100 Gbps networking throughput ideal for distributed training.", "The NVIDIA V100 also includes Tensor Cores to run mixed-precision training, but doesn\u2019t offer TF32 and BF16 precision types introduced in the NVIDIA A100 offered on the P4 instance. P3 instances however, come in 4 different sizes from single GPU instance size up to 8 GPU instance size making it the ideal choice flexible training workloads. Let\u2019s take a look at each of the following instance sizesp3.2xlarge, p3.8xlarge, p3.16xlarge and p3dn.24xlarge.", "This should be your go-to instance for most of your deep learning training work if you need a single GPU and performance is a priority. G5 instances are more cost effective for slightly lower performance than P3. With p3.2xlarge you get access to one NVIDIA V100 GPU with 16 GB GPU memory, 8 vCPUs, 61 GB system memory and up to 10 Gbps network bandwidth. V100 is the fastest GPU available in the cloud at the time of this writing and supports Tensor Cores that can further improve performance if your scripts can take advantage of mixed-precision training.", "If you spin up an Amazon EC2 p3.2xlarge instance and run the nvidia-smi command you can see that the GPU on the instances is a V100-SXM2 version which supports NVLink (we\u2019ll discuss this in the next section). Under Memory-Usage you\u2019ll see that it has 16 GB GPU memory. If you need more than 16 GB of GPU memory for large models or large data sizes then you should consider p3dn.24xlarge (more details below).", "If you need more GPUs for experimenting, more vCPUs for data pre-processing and data augmentation, or higher network bandwidth consider p3.8xlarge (with 4 GPUs) and p3.16xlarge (with 8GPUs). Each GPU is an NVIDIA V100 with 16 GB memory. They also include NVLink interconnect for high-bandwidth between-GPU communication which will come in handy for multi-GPU training. With p3.8xlarge you get access to 32 vCPUs and 244 GB system memory, and with p3.16xlarge you get access to 64 vCPUs and 488 GB system memory. This instance is ideal for couple of use cases:", "Multi-GPU training jobs: If you\u2019re just getting started with multi-GPU training, 4 GPUs on p3.8xlarge or 8 GPUs on a p3.16xlarge can give you a nice speedup. You can also use this instance to prepare your training scripts for much larger-scale multi-node training jobs, which often require you to modify your training scripts using libraries like Horovod, tf.distribute.Strategy or torch.distributed. Refer to my step-by-step guide to using Horovod for distributed training:", "Blog post: A quick guide to distributed training with TensorFlow and Horovod on Amazon SageMaker", "Parallel experiments: Multi-GPU instances also come in handy when you have to run variations of your model architecture and hyperparameters in parallel, to experiment faster. With p3.16xlarge you can run training on up to 8 variants of your model. Unlike multi-GPU training jobs, since each GPU is running training independently and doesn\u2019t block the use of other GPUs, you can be more productive during the model exploration phase.", "This instance previously held the fastest GPU instance in the cloud title, which now belongs to p4d.24xlarge. That doesn\u2019t make p3dn.24xlarge is a slouch. It is still one of the fastest instance types you can find on the cloud today and is much more cost-effective compared to P4 instance. You get access to 8 NVIDIA V100 GPUs, but unlike p3.16xlarge which have 16 GB of GPU memory, the GPUs on p3dn.24xlarge have 32 GB GPU memory. This means you can fit much larger models and train on much larger batch sizes. The instance gives you access to 96 vCPUs, 768 GB system memory and 100 Gbps network bandwidth which becomes important for achieving near-linear scaling for large-scale distributed training jobs.", "Run nvidia-smi on this instance and you can see that the GPU memory is 32 GB. The only instance with more GPU memory than this is the p4d.24xlarge which as am A100 GPU with 40 GB of GPU memory. If your models are large or you\u2019re working on 3D images or other large data batches, then this is the instance to consider. Run nvidia-smi topo \u2014 matrix and you\u2019ll see that NVLink is used for between-GPU communication. NVlink provides much higher inter-GPU bandwidth compared to PCIe and this means that multi-GPU and distributed training jobs will run much faster.", "G4 instances provide access to NVIDIA T4 GPUs based on NVIDIA Turing architecture. You can launch a single GPU per instance or multiple GPUs per instance (4 GPUs, 8 GPUs). In the timeline diagram below, you\u2019ll see that right below G4 instance is G5g instance, which are both based on GPUs with NVIDIA Turing architecture. We already discussed G5g instance type in the earlier section and the GPU in G4 (NVIDIA T4)and G5g (NVIDIA T4G) are very similar in performance. You choice will come down to choice of CPU type on these instances.", "The G5g instance offers an ARM based AWS Graviton2 CPU, andThe G4 instance offers x86 based Intel Xeon Scalable CPU.The GPUs (T4 and T4g) are very similar in performance profiles", "In the GPU timeline diagram you can see that NVIDIA Turing architecture came after the NVIDIA Volta architecture and introduced several new features for machine learning like the next generation Tensor Cores and integer precision support which make them ideal for cost effective inference deployments and graphics.", "NVIDIA Turing was the first to introduce support for integer precision (INT8) data type, that can significantly accelerate inference throughput. During training, model weights and gradients are typically stored in single precision (FP32). As it turns out, to run predictions on a trained model, you don\u2019t actually need full precision, and you can get away with reduced precision calculations in either half precision (FP16) or 8 bit integer precision (INT8). Doing so gives you a boost in throughput, without sacrificing too much accuracy. There will be a some drop in accuracy, and how much depends on various factors specific to your model and training. Overall, you get the best inference performance/cost with G4 instances compared to other GPU instances. NVIDIA\u2019s support matrix shows what neural network layers and GPU types support INT8 and other precision for inference.", "NVIDIA T4 (and NVIDIA T4G) are the lowest powered GPUs on any EC2 instance on AWS. Run nvidia-smi on this instance and you can see that the g4dn.xlarge has a NVIDIA T4 GPU with 16 GB of GPU memory. You\u2019ll also notice that the power cap is 70W compared to 300W on NVIDIA A10G.", "The following instance sizes all give you access to single NVIDIA T4 GPU with increasing number of vCPUs, system memory, storage and network bandwidth: g4dn.xlarge (4 vCPU, 16 GB system memory), g4dn.2xlarge (8 vCPU, 32 GB system memory), g4dn.4xlarge (16 vCPU, 64 GB system memory), g4dn.8xlarge (32 vCPU, 128 GB system memory), g4dn.16xlarge (64 vCPU, 256 GB system memory). You can find the full list of differences on the product G4 instance page under the Product Details section.", "G4 instance sizes also include two multi-GPU configurations: g4dn.12xlarge with 4 GPUs and g4dn.metal with 8 GPUs. However, if your use case is multi-GPU or multi-node/distributed training, you should consider using P3 instances. Run nvidia-smi topo --matrix on a multi-GPU g4dn.12xlarge instance and you\u2019ll see that the GPUs are not connected by high-bandwidth NVLink GPU interconnect. P3 multi-GPU instances include high-bandwidth NVLink interconnects that can speed up multi-GPU training.", "P2 instances give you access to NVIDIA K80 GPUs based on the NVIDIA Kepler architecture. Kepler architecture is a few generations old (Kepler -> Maxwell -> Pascal -> Volta -> Turing), therefore they\u2019re not the fastest GPUs around. They do have some specific features such as full precision (FP64) support that makes them attractive and cost-effective for high-performance computing (HPC) workloads that rely on the extra precision. P2 instances come in 3 different sizes: p2.xlarge (1 GPU), p2.8xlarge (8 GPUs), p2.16xlarge (16 GPUs).", "The NVIDIA K80 is an interesting GPU. A single NVIDIA K80 is actually two GPUs on a physical board, which NVIDIA calls dual-GPU design. What this means is that, when you launch an instance of p2.xlarge, you\u2019re only getting one of those two GPUs on a physical K80 board. Similarly, when you launch a p2.8xlarge you\u2019re getting access to eight GPUs on four K80 GPUs, and with p2.16xlarge you\u2019re getting access to sixteen GPUs on eight K80 GPUs. Run nvidia-smi on a p2.xlarge and what you see is one of the two GPUs on an NVIDIA K80 board and it has 12 GB of GPU memory", "P2 instance features at a glance:", "No, there are better options discussed above. Prior to the launch of Amazon EC2 G4 and G5 instances, the P2 instances were the recommended cost-effective deep learning training instance type. Since the launch of G4 instances, I recommend G4 as the go-to cost-effective training and prototyping GPU instance for deep learning training. P2 continues to be cost-effective for HPC workloads in scientific computing, but you\u2019ll miss out on several new features such as support for mixed-precision training (Tensor Cores) and reduced precision inference, which have become a standard on newer generations.", "If you run nvidia-smi on the p2.16xlarge GPU instance, since NVIDIA K80 has a dual-GPU design, you\u2019ll see 16 GPUs which are part of 8 NVIDIA K80 GPUs. This is the most number of GPUs you can get on a single instance on AWS. If you runnvidia-smi topo --matrix, you\u2019ll see that the all inter-GPU communications are through PCIe, unlike P3 multi-GPU instances which use the much faster NVLink.", "G3 instances give you access to NVIDIA M60 GPUs based on the NVIDIA Maxwell architecture. NVIDIA refers to the M60 GPUs as virtual workstations and positions them for professional graphics. However, with much more powerful and cost-effective options for deep learning with P3, G4, G5, G5g instances, G3 is not a recommended option for deep learning. I\u2019ve only included it here for some history and sake of completeness.", "G3 instance features at a glance:", "Prior to the launch of Amazon EC2 G4 instances, single GPU G3 instances were cost effective to develop, test and prototype. And although the Maxwell architecture is more recent than NVIDIA K80\u2019s Kepler architectures found on P2 instances, you should still consider P2 instances before G3 for deep learning. Your choice order should be P3 > G4 > P2 > G3.", "G3 instances come in 4 sizes, g3s.xlarge and g3.4xlarge (2 GPU, different system configuration) g3.8xlarge (2 GPUs) and g3.16xlarge (4 GPUs). Run nvidia-smi on a g3s.xlarge and you\u2019ll see that this instances gives you access to an NVIDIA M60 GPU with 8 GB GPU memory.", "NVIDIA GPUs are no doubt a staple for deep learning, but there are other instance options and accelerators on AWS that may be the better option for your training and inference workloads.", "For a detailed discussion on inference deployment options please refer to blog post on choosing the right AI accelerators for inference:", "Blog Post: A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference", "You have a few different options to optimize the cost of your training and inference workloads.", "Spot-instance pricing makes high-performance GPUs much more affordable and allows you to access spare Amazon EC2 compute capacity at a steep discount compared to on-demand rates. For an up-to-date list of prices by instance and Region, visit the Spot Instance Advisor. In some cases you can save over 90% on your training costs, but your instances can be preempted and be terminated with just 2 mins notice. Your training scripts must implement frequent checkpointing and ability to resume training once Spot capacity is restored.", "During the development phase much of your time is spent prototyping, tweaking code and trying different options in your favorite editor or IDE (which is obvious VIM) \u2014 all of which don\u2019t need a GPU. You can save costs by simply decoupling your development and training resources and Amazon SageMaker will let you do this easily. Using the Amazon SageMaker Python SDK you can test your scripts locally on your laptop, desktop, EC2 instance or SageMaker notebook instance.", "When you\u2019re ready to train, specify what GPU instance type you want to train on and SageMaker will provision the instances, copy the dataset to the instance, train your model, copy results back to Amazon S3, and tear down the instance. You are only billed for the exact duration of training. Amazon SageMaker also supports managed Spot Training for additional convenience and cost savings.", "Here\u2019s my guide: A quick guide to using Spot instances with Amazon SageMaker", "Save costs for inference workloads by leveraging EI to add just the right amount of GPU acceleration to your CPU instances discussed in this blog post: A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference", "Downloading your favourite deep learning framework is easy right? Just pip install XXX or conda install XXX or docker pull XXX and you\u2019re all set right? Not really, frameworks you install from upstream repositories are often not optimized for the target hardware they\u2019ll run on. These frameworks are built to support a wide range of diverse CPU and GPU types, so they will support lowest common denominator of features and performance optimization, which can lead to substantially degraded performance on your AWS GPU instance.", "For this reason, I highly recommend using AWS Deep Learning AMIs or AWS Deep Learning Containers (DLC) instead. AWS qualifies and tests them on all Amazon EC2 GPU instances, and they include AWS optimizations for networking, storage access and the latest NVIDIA and Intel drivers and libraries. Deep learning frameworks have upstream and downstream dependencies on higher level schedulers and orchestrators and lower-level infrastructure services. By using AWS AMIs and AWS DLCs you know it\u2019s been tested end-to-end and is guaranteed to give you the best performance.", "High-performance Computing (HPC) is another scientific domain that relies on GPUs to speed up computation for simulation, data processing and visualization. While deep learning training can be done on lower precision arithmetic from FP32 (single precision) down to FP16 (half precision) and variations such as Bfloat16 and TF32, HPC applications need to high-precision arithmetic up to FP64 (double precision). The NVIDIA A100, V100 and K80 GPUs support FP64 precision and these are available on P4, P3 and P2 instances respectively.", "In today\u2019s \u201cI put this together because I couldn\u2019t find one already\u201d contribution, I present to you a GPUs on AWS features list. I often want to know how much memory is on a specific GPU or if a specific precision type is supported on a GPU, or if the instance has an Intel, AMD or Graviton CPU etc. before I launch a GPU instance. To avoid having to go through various webpages and NVIDIA white papers, I\u2019ve painstakingly compiled all the information into a table. You can use the image below or go right to the markdown table embedded at the end of the post and hosted on GitHub, your choice. Enjoy!", "Do you prefer consuming content in a graphical format, I got you covered there too! The following image shows all the GPU instance types and sizes on AWS. There isn\u2019t enough space for all the features, for that I still recommend the spreadsheet.", "Thank you for reading. If you found this article interesting, consider giving this an applause and following me on medium. Please also check out my other blog posts on medium or follow me on twitter (@shshnkp), LinkedIn or leave a comment below. Want me to write on a specific machine learning topic? I\u2019d love to hear from you!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Talking Engineer. Runner. Coffee Connoisseur. Formerly Machine learning @Meta, AWS, NVIDIA, MATLAB, posts are my own opinions. website: shashankprasanna.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd69c157d8c86&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "Shashank Prasanna"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5----d69c157d8c86---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://aws.amazon.com/ec2/instance-types/p4/", "anchor_text": "P4 instances"}, {"url": "https://www.nvidia.com/en-us/data-center/a100/", "anchor_text": "NVIDIA A100 GPUs"}, {"url": "https://aws.amazon.com/blogs/compute/amazon-ec2-p4d-instances-deep-dive/", "anchor_text": "record setting training performance"}, {"url": "https://www.tensorflow.org/guide/mixed_precision", "anchor_text": "TensorFlow"}, {"url": "https://pytorch.org/docs/stable/notes/amp_examples.html", "anchor_text": "PyTorch"}, {"url": "https://mxnet.apache.org/api/python/docs/tutorials/performance/backend/amp.html", "anchor_text": "MXNet"}, {"url": "https://cloud.google.com/tpu/docs/bfloat16", "anchor_text": "supported BF16"}, {"url": "https://www.nvidia.com/en-us/data-center/nvlink/", "anchor_text": "3rd generation NVLink"}, {"url": "https://aws.amazon.com/ec2/instance-types/p3/", "anchor_text": "P3 instances"}, {"url": "https://www.nvidia.com/en-us/data-center/v100/", "anchor_text": "NVIDIA V100 GPUs"}, {"url": "https://github.com/horovod/horovod", "anchor_text": "libraries like Horovod"}, {"url": "https://www.tensorflow.org/guide/distributed_training", "anchor_text": "tf.distribute.Strategy"}, {"url": "https://pytorch.org/docs/stable/distributed.html", "anchor_text": "torch.distributed"}, {"url": "https://towardsdatascience.com/a-quick-guide-to-distributed-training-with-tensorflow-and-horovod-on-amazon-sagemaker-dae18371ef6e?source=friends_link&sk=0a1f6a2e7716d4272c79156cf7c5f294", "anchor_text": "A quick guide to distributed training with TensorFlow and Horovod on Amazon SageMaker"}, {"url": "https://aws.amazon.com/ec2/instance-types/g4/", "anchor_text": "G4 instances"}, {"url": "https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html", "anchor_text": "NVIDIA\u2019s support matrix"}, {"url": "https://aws.amazon.com/ec2/instance-types/g4/", "anchor_text": "product G4 instance page"}, {"url": "https://aws.amazon.com/ec2/instance-types/p2/", "anchor_text": "P2 instances"}, {"url": "https://aws.amazon.com/ec2/instance-types/g3/", "anchor_text": "G3 instances"}, {"url": "https://towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c?source=friends_link&sk=adf7b78cea61ed9b8415c8371777ba3d", "anchor_text": "discussed in this blog post"}, {"url": "https://towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c?source=friends_link&sk=adf7b78cea61ed9b8415c8371777ba3d", "anchor_text": "further detail on this blog post"}, {"url": "https://towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c?source=friends_link&sk=adf7b78cea61ed9b8415c8371777ba3d", "anchor_text": "A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference"}, {"url": "https://aws.amazon.com/ec2/spot/bid-advisor/", "anchor_text": "Spot Instance Advisor"}, {"url": "https://aws.amazon.com/sagemaker/?trk=el_a134p000006vgXgAAI&trkCampaign=NA-FY21-GC-400-FTSA-SAG-Overview&sc_channel=el&sc_campaign=Y21-SageMaker_shshnkp&sc_outcome=AIML_Digital_Marketing", "anchor_text": "Amazon SageMaker"}, {"url": "https://towardsdatascience.com/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68?source=friends_link&sk=937b581bef3605c84c07cfd32c5842d1", "anchor_text": "A quick guide to using Spot instances with Amazon SageMaker"}, {"url": "https://towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c?source=friends_link&sk=adf7b78cea61ed9b8415c8371777ba3d", "anchor_text": "A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference"}, {"url": "https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-gpu-opt-training.html", "anchor_text": "enabling mixed-precision training"}, {"url": "https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimizing_int8_python", "anchor_text": "provides examples"}, {"url": "https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html", "anchor_text": "AWS Deep Learning AMIs"}, {"url": "https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html", "anchor_text": "AWS Deep Learning Containers (DLC)"}, {"url": "https://medium.com/@shashankprasanna", "anchor_text": "medium"}, {"url": "https://twitter.com/shshnkp", "anchor_text": "@shshnkp"}, {"url": "https://www.linkedin.com/in/shashankprasanna/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d69c157d8c86---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/aws?source=post_page-----d69c157d8c86---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d69c157d8c86---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/gpu?source=post_page-----d69c157d8c86---------------gpu-----------------", "anchor_text": "Gpu"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d69c157d8c86---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----d69c157d8c86---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----d69c157d8c86---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d69c157d8c86---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d69c157d8c86--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d69c157d8c86--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d69c157d8c86--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shashank Prasanna"}, {"url": "https://medium.com/@shashankprasanna/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "681 Followers"}, {"url": "http://shashankprasanna.com", "anchor_text": "shashankprasanna.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd48ce4d9cb5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&newsletterV3=e0c596ca35b5&newsletterV3Id=d48ce4d9cb5c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}