{"url": "https://towardsdatascience.com/building-your-own-self-attention-gans-e8c9b9fe8e51", "time": 1683011068.003774, "path": "towardsdatascience.com/building-your-own-self-attention-gans-e8c9b9fe8e51/", "webpage": {"metadata": {"title": "Building your own Self-attention GANs | by Frank Xu | Towards Data Science", "h1": "Building your own Self-attention GANs", "description": "GANs, as known as Generative Adversarial Networks, is one of the most popular topics in the Machine Learning fields recently. It consists of two different Neural Network models, one called Generator\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "CNN", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1805.08318", "anchor_text": "Self-Attention Generative Adversarial Networks", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "Spectral Normalization for Generative Adversarial Networks", "paragraph_index": 11}, {"url": "https://github.com/heykeetae/Self-Attention-GAN/blob/master/spectral.py", "anchor_text": "SAGAN\u2019s spectral.py", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Hinge_loss", "anchor_text": "hinge loss", "paragraph_index": 14}, {"url": "https://github.com/heykeetae/Self-Attention-GAN", "anchor_text": "SAGAN", "paragraph_index": 18}, {"url": "https://github.com/franknb/Self-attention-DCGAN/blob/master/model_64.py", "anchor_text": "model structure", "paragraph_index": 18}, {"url": "https://github.com/franknb/Self-attention-DCGAN/blob/master/SAGAN_celeba.ipynb", "anchor_text": "training function", "paragraph_index": 18}, {"url": "https://github.com/franknb/Self-attention-DCGAN", "anchor_text": "GitHub", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1805.08318", "anchor_text": "SAGAN paper", "paragraph_index": 20}, {"url": "https://github.com/franknb/Self-attention-DCGAN", "anchor_text": "GitHub", "paragraph_index": 20}], "all_paragraphs": ["GANs, as known as Generative Adversarial Networks, is one of the most popular topics in the Machine Learning fields recently. It consists of two different Neural Network models, one called Generator, and one called Discriminator. It sounds hard to understand, but let me try to put it in this way: Let\u2019s say we want to forge famous paintings starting without any knowledge of painting, what should we do? Most would say, just look at the paintings and learn how to do it. But it\u2019s not a one-man job, to some point, I am sure you will be better and better in painting. You will need to have your friend come in front of one real painting and one that you forged and let him guess which one is the real one. It will be pretty easy for him to guess in the beginning, but keep it going and you will eventually confuse your friend.", "In GANs, the generator is like you who forge paintings, and the discriminator is the friend who specializes in telling which painting is fake. Think about the goal here, you want to make it hard for your friend to tell real or fake. If your friend were to give out a probability of being real from 0 to 1 for each painting, you would want him to give 0.5 to any paintings you show him, either real or forged. This will also be the objective of GANs, as reflected in loss functions.", "We also see DCGAN a lot, which stands for Deep Convolutional GAN. It is a GAN design that specialized in image generation, using convolution layers for both generator and discriminator. It works just like a CNN. A Self-attention GAN is a DCGAN that utilizes self-attention layers. The idea of self-attention has been out there for years, also known as non-local in some researches. Think about how does convolution works: they convolve nearby pixels and extract features out of local blocks. They work \u201clocally\u201d in each layer. In contrast, self-attention layers learn from distant blocks. In 2017, Google published a paper \u201cAttention Is All You Need\u201d, bringing more hypes about the topic. For a single image input, it works like this:", "1. Using a kernel size 1 convo to generate Query, Key and Value layers, with the shape of (Channels * N), where N = Width * Height.", "2. Generate attention map by the matrix dot product of Query and Key, with the shape of (N * N). The N * N attention map describes each pixel\u2019s attention score on every other pixel, hence the name \u201cself-attention\u201d. Pixels here mean data points in input matrices.", "3. Get attention weights by the matrix dot product of Value and attention map, with the shape of (C * N). The attention weights describe each pixel\u2019s total attention score throughout all pixels. We then reshape the attention weights into (C * W * H).", "4. Add the attention weights back onto input layer itself with a weight of Gamma, a learning parameter initializing at 0. It means that the self-attention module does not do anything initially.", "To conclude, a Self-attention GAN is just a DCGAN with self-attention layers. The 2018 paper \u201cSelf-Attention Generative Adversarial Networks\u201d states that DCGANs could fail in capturing geometric or structural patterns that occur consistently with multi-class datasets, e.g. dogs are drawn without separate feet. After all, how to build a Self-attention GAN? Let\u2019s get our hands dirty!", "We will be using the MNIST digits dataset. After downloaded with PyTorch, we then load the data with DataLoader.", "First, we will construct the self-attention module, which will be used in both generator and discriminator later. You can check out the previous self-attention module structure for better understanding.", "The generator and discriminator\u2019s structure are shown in this graph:", "Spectral Normalization is a novel weight normalization technique proposed in \u201cSpectral Normalization for Generative Adversarial Networks\u201d for a more stablized training process. Borrowed from SAGAN\u2019s spectral.py.", "This is usually the most confusing part. Unlike CNNs or other simple machine learning models, GANs does not have a ready-to-use fit function. We need to write the whole training function.", "We start by initializing our models and optimizers. Then within a loop, we first read in a batch of data from the DataLoader.", "Next step, we feed our discriminator the batch of real images and calculate the hinge loss. Then we use an array of random numbers as latent variables, generate a batch of fake images with the generator, feed them into discriminator again, and get the loss.", "As mentioned before, the goal for our discriminator is to give out predictions closer to 1 for real images, and 0 for fake images. Therefore, we want to reduce the sum of (1-predictions for real images) and predictions for fake images. We can achieve this using backward and step, and update all learning parameters for discriminator within a few lines of code thanks to PyTorch.", "We then update parameters for the generator using the output of fake images from the discriminator. Voila, one training loop is done and we trained our model for the first batch of images. Print out the log info and save images/models if you want.", "After the training function is done, simply run the function to start training. Taking samples from the generator every 100 batches, we can see the process of our model learning to draw digits. On the left-hand side, we have the result without attention, and on the right with attention.", "With some slight changes to the model structure, we can have our model running on other sizes of inputs. Using the same structure as SAGAN, we can also run on the CelebA dataset. The specific model structure and training function can be found at my GitHub.", "As you might observe from the gifs above, there is no clear visual difference between the model with and without self-attention layers. Moreover, self-attention layers work by calculating multiple matrix dot products, which would contribute to a 10% ~ 30% longer training times depending on your specific model structure.", "However, in the SAGAN paper (Han et al. 2018), the author did report getting better inception and FID scores with self-attention modules. My next step would be to adopt either one of these metrics, and see if Self-attention GAN is bringing better performance. Check out all the source code and demo at my GitHub.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe8c9b9fe8e51&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://franknb.medium.com/?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": ""}, {"url": "https://franknb.medium.com/?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "Frank Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F545cf0b2e26c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&user=Frank+Xu&userId=545cf0b2e26c&source=post_page-545cf0b2e26c----e8c9b9fe8e51---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8c9b9fe8e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8c9b9fe8e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "CNN"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need"}, {"url": "https://arxiv.org/abs/1805.08318", "anchor_text": "Self-Attention Generative Adversarial Networks"}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "Spectral Normalization for Generative Adversarial Networks"}, {"url": "https://github.com/heykeetae/Self-Attention-GAN/blob/master/spectral.py", "anchor_text": "SAGAN\u2019s spectral.py"}, {"url": "https://en.wikipedia.org/wiki/Hinge_loss", "anchor_text": "hinge loss"}, {"url": "https://github.com/heykeetae/Self-Attention-GAN", "anchor_text": "SAGAN"}, {"url": "https://github.com/franknb/Self-attention-DCGAN/blob/master/model_64.py", "anchor_text": "model structure"}, {"url": "https://github.com/franknb/Self-attention-DCGAN/blob/master/SAGAN_celeba.ipynb", "anchor_text": "training function"}, {"url": "https://github.com/franknb/Self-attention-DCGAN", "anchor_text": "GitHub"}, {"url": "https://arxiv.org/abs/1805.08318", "anchor_text": "SAGAN paper"}, {"url": "https://github.com/franknb/Self-attention-DCGAN", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e8c9b9fe8e51---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e8c9b9fe8e51---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e8c9b9fe8e51---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e8c9b9fe8e51---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----e8c9b9fe8e51---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8c9b9fe8e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&user=Frank+Xu&userId=545cf0b2e26c&source=-----e8c9b9fe8e51---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8c9b9fe8e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&user=Frank+Xu&userId=545cf0b2e26c&source=-----e8c9b9fe8e51---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8c9b9fe8e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe8c9b9fe8e51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e8c9b9fe8e51---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e8c9b9fe8e51--------------------------------", "anchor_text": ""}, {"url": "https://franknb.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://franknb.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Frank Xu"}, {"url": "https://franknb.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "186 Followers"}, {"url": "https://www.linkedin.com/in/frank-xu-huaze/", "anchor_text": "https://www.linkedin.com/in/frank-xu-huaze/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F545cf0b2e26c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&user=Frank+Xu&userId=545cf0b2e26c&source=post_page-545cf0b2e26c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F630ffaf7a481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-your-own-self-attention-gans-e8c9b9fe8e51&newsletterV3=545cf0b2e26c&newsletterV3Id=630ffaf7a481&user=Frank+Xu&userId=545cf0b2e26c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}