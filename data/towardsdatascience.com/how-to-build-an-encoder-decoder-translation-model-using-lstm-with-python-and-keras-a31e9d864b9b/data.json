{"url": "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b", "time": 1683015438.216474, "path": "towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b/", "webpage": {"metadata": {"title": "How to build an encoder decoder translation model using LSTM with Python and Keras | by Nechu BM | Towards Data Science", "h1": "How to build an encoder decoder translation model using LSTM with Python and Keras", "description": "Prerequisites: to understand this article previous knowledge about recurrent neural network (RNN) and encoder decoder is valuable. This article is a practical guide on how to develop an encoder\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630", "anchor_text": "recurrent neural network (RNN)", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a", "anchor_text": "encoder decoder", "paragraph_index": 0}, {"url": "http://www.manythings.org/bilingual/", "anchor_text": "manythings.org", "paragraph_index": 6}, {"url": "https://tatoeba.org/spa", "anchor_text": "Tatoeba", "paragraph_index": 6}, {"url": "http://www.manythings.org/anki/spa-eng.zip", "anchor_text": "Spanish-English pairs", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/how-to-build-a-translation-pipeline-with-rnn-and-keras-57c1cf4a8a7", "anchor_text": "previous tutorial.", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa", "anchor_text": "embedding layer", "paragraph_index": 19}, {"url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "anchor_text": "Keras documentation", "paragraph_index": 35}, {"url": "https://bit.ly/36vajnu", "anchor_text": "https://bit.ly/36vajnu", "paragraph_index": 37}], "all_paragraphs": ["Prerequisites: to understand this article previous knowledge about recurrent neural network (RNN) and encoder decoder is valuable.", "This article is a practical guide on how to develop an encoder decoder model, more precisely a Sequence to Sequence (Seq2Seq) with Python and Keras. In the previous tutorial we develop a many to many translation model similar to the image below:", "This structure has one important limitation, the sequence length. As we can see in the image, the input sequence and the output sequence must have the same length. What if we need to have different lengths? Models with different sequences lengths are, for example, sentiment analysis that receives a sequence of words and outputs a number, or Image captioning models where the input is an image and the output is a sequence of words.", "If we want to develop models were inputs and outputs lengths are different we need to develop an encoder decoder model. Through this tutorial we are going to see how to develop the model, applying it to a translation exercise. The representation of the model looks as the following.", "We have split the model into two parts, first, we have an encoder that inputs the Spanish sentence and produces a hidden vector. The encoder is built with an Embedding layer that converts the words into a vector and a recurrent neural network (RNN) that calculates the hidden state, here we will be using Long Short-Term Memory (LSTM) layer.", "Then the output of the encoder will be used as input for the decoder. For the decoder, we will be using LSTM layer again, as well as a dense layer that predicts the English word.", "The sample data can be downloaded at manythings.org and comes from Tatoeba. It is composed of sentences pairs in the language you need. In our case we will work with Spanish-English pairs.", "The first thing we need in order to build the model is to pre-process the data and get the maximum length of Spanish and English sentences.", "Prerequisites: understanding of the class \u2018tokenizer\u2019 and \u2018pad_sequences\u2019 from Keras. If you would like to review them in detail, we covered the topic in the previous tutorial.", "First, we will import the libraries and then read the downloaded data.", "Once we have read the data, we will keep the first examples for faster training. In case we want to develop a higher performance model we need to use the full data set. Then we must clean the data by removing capital letters and punctuation.", "Next, we tokenize the sentences and analyse the data.", "Once we are done with creating the functions we can do the pre-processing:", "The above code prints the following results", "From the previous code we have a maximum length of 12 words for Spanish sentences and 6 words for English. Here we can see the advantage of using an encoder decoder model, previously we had the limitation of working with equal length sentences, so we needed to apply padding to the English sentences up to 12, now it is half. Consequently, and more importantly, it also reduces the number of LSTM time steps, reducing computation needs and complexity.", "We apply padding to make the maximum length of the sentences in each language equal.", "Now that we have the data ready let\u2019s build the model.", "In the following section we will create the model and explain each layer as we add it in our python code.", "As we saw in the image of the model, the first layer to be defined is the embedding layer. For this, we first must add an Input Layer, the only parameter to consider here is \u2018shape\u2019, which is the maximum length of the Spanish sentences, in our case 12.", "We will then connect it to the embedding layer, here the parameters to take into account are \u2018input_dim\u2019 which is the length of the Spanish vocabulary and \u2018output_dim\u2019 which is the shape of the embedding vector. This layer will convert any of the Spanish words into a vector of the shape of the output dimension.", "The concept behind this is to extract the meaning of the word in a form of a spatial representation where each dimension will be a characteristic defining the word. For example, the world \u2018sol\u2019 will be converted into a vector of shape 128. The higher the output dimension the more semantic meaning you can extract from each word, but also the higher the calculations required and the processing time. Finding a balance between speed and performance is required.", "Next we will add the LSTM layer of size 64. Even though each time step of the LSTM outputs a hidden vector, we will focus our attention on the last one, therefore the parameter return_sequences is \u2018False\u2019. We will see how the LSTM layer works with return_sequences=True for the decoder.", "The output of the encoder layer will be the hidden state of the last time step. We will then need to feed this vector into the decoder. Let\u2019s look more precisely at the decoder part and understand how it works.", "As we can see in the image the hidden vector is repeated n times, so each time step of the LSTM receives the same vector. In order to have this same vector for every time step we need to use the layer RepeatVector, as its names implies its role is to repeat the vector it is receiving, the only parameter we need to define is n, the number of repetitions. This number is equal to the number of time step of the decoder part, in other words the maximum English sentence length, 6.", "Once we have the input ready, we will continue with the decoder. This is also built with a LSTM layer, the difference is the parameter return_sequences, which in this case is \u2018True\u2019. What is this parameter for? In the encoder part we were expecting only one vector in the last time step and neglecting all the others, here we are expecting an output vector at every time step so the Dense layer can make a prediction.", "We have one last step, to predict the translated word. For this we need to use a Dense Layer. The parameter we need to define is the number of units, this number of units is the shape of the output vector and it needs to be the same as the length of the English vocabulary. Why? The vector will be all values close to zero, except one of the units that will be close to 1. We then need to map the index of the unit that outputs a 1 with a dictionary where we map each unit to a word. For example, if the input is the word \u2018sol\u2019 and the output is a vector where all are zeros and then the unit 472 is 1, we map this index against the dictionary containing the English words and we get the value \u2018sun\u2019.", "We have just seen how to apply the Dense layer and predict one word, but how do we make the prediction for the whole sentence? Because we are using return_sequence=True, LSTM layer outputs a vector at every time step, so we need to apply the previous explained Dense layer at every time step and predict one word at a time. To do this, Keras has developed a specific layer called TimeDistributed, it applies the same Dense layer to every time step.", "Lastly, we stack the layers to create the model and add a function loss.", "Once we define the model, we just need to train it.", "When the model is trained we can make our first translation. You will also find the function \u2018logits_to_sentence\u2019 that maps the output of the dense layer with the English vocabulary.", "An encoder decoder structure allows for a different input and output sequence length. First, we use an Embedding layer to create a spatial representation of the word and feed it into a LSTM layer that outputs a hidden vector, because we just focus on the output of the last time step we use return_sequences=False.", "This output vector needs to be repeated the same number of times as the number of time step in the decoder part, for that we use the RepeatVector layer. The decoder will be built with LSTM layer and the parameter return_sequences=True, so each output of the time steps is used by the Dense layer.", "Even though this model is already a nice improvement from the previous tutorial we still can increase the accuracy. We could increase the number of LSTM layers in the model, instead of just one layer in the encoder and one layer in the decoder. We could also use a pre-trained embedding layer like word2vec or Glove. Finally, we could use the attention mechanism which is one of the major improvements in the natural language processing field. We will cover this concept in the next tutorial.", "Appendix: encoder-decoder without using repeat vector", "In this tutorial we have seen how to build an encoder decoder using the RepeatVector Layer. There is a second option where instead of repeating the hidden vector we use the output of the model as input for the next time step as we can see in this image.", "The code for implementing this model can be found in Keras documentation, it requires a deeper understanding of Keras library and the development is considerably more complex.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist & Entrepreneur! Learn Artificial Intelligence and Machine Learning to become a Linchpin \u279c https://bit.ly/36vajnu"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa31e9d864b9b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "Nechu BM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56da16d481ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&user=Nechu+BM&userId=56da16d481ba&source=post_page-56da16d481ba----a31e9d864b9b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lazycreekimages?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Michael Dziedzic"}, {"url": "https://unsplash.com/@lazycreekimages?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630", "anchor_text": "recurrent neural network (RNN)"}, {"url": "https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a", "anchor_text": "encoder decoder"}, {"url": "http://www.manythings.org/bilingual/", "anchor_text": "manythings.org"}, {"url": "https://tatoeba.org/spa", "anchor_text": "Tatoeba"}, {"url": "http://www.manythings.org/anki/spa-eng.zip", "anchor_text": "Spanish-English pairs"}, {"url": "https://towardsdatascience.com/how-to-build-a-translation-pipeline-with-rnn-and-keras-57c1cf4a8a7", "anchor_text": "previous tutorial."}, {"url": "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa", "anchor_text": "embedding layer"}, {"url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "anchor_text": "Keras documentation"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a31e9d864b9b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a31e9d864b9b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a31e9d864b9b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----a31e9d864b9b---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/ai?source=post_page-----a31e9d864b9b---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&user=Nechu+BM&userId=56da16d481ba&source=-----a31e9d864b9b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&user=Nechu+BM&userId=56da16d481ba&source=-----a31e9d864b9b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa31e9d864b9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a31e9d864b9b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a31e9d864b9b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nechu BM"}, {"url": "https://medium.com/@dbenzaquenm/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "154 Followers"}, {"url": "https://bit.ly/36vajnu", "anchor_text": "https://bit.ly/36vajnu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56da16d481ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&user=Nechu+BM&userId=56da16d481ba&source=post_page-56da16d481ba--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4f6c9f72bf01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b&newsletterV3=56da16d481ba&newsletterV3Id=4f6c9f72bf01&user=Nechu+BM&userId=56da16d481ba&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}