{"url": "https://towardsdatascience.com/model-selection-assessment-bb2d74229172", "time": 1683007236.089517, "path": "towardsdatascience.com/model-selection-assessment-bb2d74229172/", "webpage": {"metadata": {"title": "Model Selection & Assessment. Going beyond the train-val-test split | by Micha\u0142 Oleszak | Towards Data Science", "h1": "Model Selection & Assessment", "description": "A standard modeling workflow would see you partitioning your data into the training, validation, and testing sets. You would then fit your models to the training data, then use the validation set to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "linear", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb", "anchor_text": "logistic", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/non-linear-regression-basis-expansion-polynomials-splines-2d7adb2cc226", "anchor_text": "non-linear splines", "paragraph_index": 6}, {"url": "https://robjhyndman.com/hyndsight/aic/", "anchor_text": "excellent post by Rob Hyndman", "paragraph_index": 12}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles", "paragraph_index": 26}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com", "paragraph_index": 28}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal", "paragraph_index": 28}], "all_paragraphs": ["A standard modeling workflow would see you partitioning your data into the training, validation, and testing sets. You would then fit your models to the training data, then use the validation set to perform model selection, and finally, evaluate the very best selected model on the test data to see what generalization performance can be expected of it (model assessment). This flow is presumably your best bet to make sure you have selected the right model and that you won\u2019t be startled once you deploy it to production.", "That being said, one cannot always afford the luxury of setting data aside to form validation and testing sets. If you have very little data, you\u2019d rather use it all for training. In this article, we will discuss methods for selecting and assessing models that let you do exactly this \u2014 no validation nor test sets required!", "This article is based on a chapter from Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.", "While comparing competing models, you would like to pick the one performing best on new, unseen data. That\u2019s what you would normally use the validation set for: validation data are not seen by the model while it\u2019s being fit, so choosing a model that works best for these data is a good strategy. Alas, in our setting you have no validation data to check different models against! To see what to do about it, let\u2019s first introduce a couple of error measures:", "Now, consider this quantity: in-sample error \u2014 training error. It is typically positive: the training error is smaller because it is based on the very same data the model was optimized for. But what does it amount to exactly? Well, it can be shown that (up to an expectation) it holds that", "where \ud835\udc41 is the number of observations and the last term is the covariance between the training set response and its predictions. The larger this covariance, the stronger we fit the model to training data (to the point of overfitting), so the training error goes down, increasing the left-hand side of the equation.", "Let\u2019s assume (and this an important assumption) that we are dealing with a model that is linear in its parameters. This means we are talking linear or logistic regression models, non-linear splines, or auto-regressive models, for instance. When that\u2019s the case, then the covariance term above simplifies to d * \u03c3\u03f5\u00b2, where \ud835\udc51 is a measure of model complexity (number of features in linear regression, number of basis functions in regression splines) and \u03c3\u03f5\u00b2 is the error variance. After substituting this simplification and rearranging terms, we get", "What does this give us? If we could estimate the right-most term, we could then add it to the training error to obtain an estimate of the in-sample error. And the in-sample error is all we need for model selection! Sure, it doesn\u2019t give us information about the models\u2019 generalization performance (that\u2019s the role of model assessment, read on). Also, we don\u2019t really care about the in-sample error\u2019s specific value \u2014 it\u2019s rather uncommon to see the same feature values after deployment as in the training data. But the relative magnitude of in-sample errors across different models allows us to pick the best one.", "The above formula is also known as the Mallows\u2019s \ud835\udc36\ud835\udc5d:", "and when the models are fit under squared loss, it can be used for model selection \u2014 we simply pick the model with the lowest \ud835\udc36\ud835\udc5d.", "If the models are not necessarily fit under squared loss, we need to adjust Mallows\u2019s \ud835\udc36\ud835\udc5d slightly. Recall we are discussing models linear in their parameters: think linear or logistic regression, regression splines, or ARIMA models. They are typically estimated by maximum likelihood, and under the Gaussian model, it holds (up to a constant) that", "Solving for \ud835\udc52_\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b and substituting this into the formula for \ud835\udc36\ud835\udc5d we arrive at the Akaike Information Criterion or AIC:", "The AIC is basically a penalized likelihood. It goes up as the model complexity \ud835\udc51 increases and goes down as the model\u2019s fit to the data (the loglikelihood) increases, trading-off these two. We pick the model with the lowest AIC. The best thing about it is that even though AIC is computed using only the training data, minimizing it is asymptotically equivalent to minimizing the leave-one-out cross-validation mean squared error, which makes it great for model selection. For more caveats on the AIC (also in the context of time series forecasting) check out this excellent post by Rob Hyndman, the author of the R\u2019s forecast package.", "Let us now look at a Python example of model selection with AIC. We will use the infamous Boston housing data from scikit-learn\u2019s datasets. Let\u2019s fit two linear regression models explaining house prices. We will employ the statsmodels package for this, as it conveniently computes the AIC for us. Both models will use the number of rooms and house age as features. Model 1 will use the neighborhood crime rate on top of that, while model 2 will use distance to large employment centers instead. Which of the two models is better?", "As far as the AIC is concerned, Model 1 (the one using the crime rate) is better, as it scores a lower AIC value.", "AIC is not the only information criterion. Another one is the BIC, or Bayesian Information Criterion, also known as the Schwarz criterion. Similarly to the AIC, the BIC is also a penalized likelihood, but with a different penalty term:", "This penalty tends to penalize more complex models more heavily than the AIC. Let\u2019s see how our two housing price models score on BIC.", "Again, Model 1 is preferred, as it scores a lower BIC. In our examples, both criteria where unanimous, but it does not have to be the case. Which criterion should we then base our model selection on? There is no one-size-fits-all answer, but here are a few things to keep in mind:", "Now that we have selected our model to be the one including the crime rate feature, it would be great to know what kind of performance can we expect of it once deployed. That\u2019s the job of model assessment \u2014 to estimate the testing error of a model.", "One way to do so is via the well-known procedure of cross-validation. We split the data randomly into \ud835\udc58 subsets, or folds, and then iterate through them, leaving the current fold out and fitting the model the remaining \ud835\udc58\u22121 folds. Then, we evaluate the model\u2019s error on the left-out fold and proceed to the next iteration. This way, we obtain \ud835\udc58 error estimates. Once averaged, they form the cross-validated estimate of the testing error. Pretty simple, right? But what should be the value of \ud835\udc58?", "The choice of \ud835\udc58, like so many other choices in machine learning, is between the bias and the variance. Going to the extreme of setting \ud835\udc58=\ud835\udc41 results in the so-called leave-one-out cross-validation. In this setting, each observation constitutes its own fold. As a result, the training sets will be very similar across the folds \u2014 indeed, they will only differ by one observation. Consequently, the CV estimate of the testing error might suffer from high variance. On the other hand, when \ud835\udc58 is small, we risk high bias. This is because the smaller the \ud835\udc58, the fewer observations constitute the \ud835\udc58\u22121 training folds. For instance, consider \ud835\udc41=100 observations. With \ud835\udc58=10, each fold has 10 observations and so each training is based on 90 observations. With \ud835\udc58=4, each training uses only 75 observations. If the model performance decreases with less training data, too low \ud835\udc58 will lead to an over-estimated error.", "Ultimately, the choice of \ud835\udc58 should depend on your application. \ud835\udc58=\ud835\udc41 is rarely a good idea \u2014 it\u2019s also quite computationally expensive to run (\ud835\udc41 models need to be trained)! You know your data best \u2014 if you are willing to assume (or have proved) that less data won\u2019t make much of a difference to your model\u2019s performance, then you\u2019re good to go with a small \ud835\udc58 of say 3 or 5. But that\u2019s probably not the case if you have only little data (should you have big data, you could just follow the standard train/validation/test split and ignore this article). Hence, a slightly larger \ud835\udc58 of say 10 might be worth a shot. One more thing to consider: if your \ud835\udc58 will be too small, the error will be over-estimated, meaning that the true testing error is likely to be less than what your CV tells you. If you\u2019re satisfied with the cross-validated error estimate, you\u2019re likely to be even happier with the production performance.", "Let\u2019s assess our selected model with 10-fold cross-validation. To do this, we\u2019ll use the scikit-learn API.", "To sum up: what has just happened? In case you don\u2019t have enough data to set aside a validation and a test set, you need other ways to do your model selection and assessment. We have shown what are information criteria and how to use them for model selection, and then how to estimate the expected real-world performance of the selected model with cross-validation.", "Why not use CV for model selection, I hear you ask. You could! However, with small data, information criteria tend to be more reliable. And if your CV will be biased due to too little data, at least you will know the model you\u2019ve selected is the right one \u2014 even if its cross-validated testing error estimate was far from perfect.", "Thanks for reading! I hope you have learned something useful that will benefit your projects \ud83d\ude80", "If you liked this post, try one of my other articles. Can\u2019t choose? Pick one of these:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer & Data Science Instructor | Top Writer in AI & Statistics | michaloleszak.com | Book 1:1 @ hiretheauthor.com/michal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbb2d74229172&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bb2d74229172--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bb2d74229172--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaloleszak.medium.com/?source=post_page-----bb2d74229172--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=post_page-----bb2d74229172--------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8----bb2d74229172---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb2d74229172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb2d74229172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/illustrations/cubes-choice-one-yellow-light-2492010/", "anchor_text": "https://pixabay.com/illustrations/cubes-choice-one-yellow-light-2492010/"}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "linear"}, {"url": "https://towardsdatascience.com/linear-classifiers-an-overview-e121135bd3bb", "anchor_text": "logistic"}, {"url": "https://towardsdatascience.com/non-linear-regression-basis-expansion-polynomials-splines-2d7adb2cc226", "anchor_text": "non-linear splines"}, {"url": "https://robjhyndman.com/hyndsight/aic/", "anchor_text": "excellent post by Rob Hyndman"}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles"}, {"url": "https://towardsdatascience.com/boost-your-grasp-on-boosting-acf239694b1", "anchor_text": "Boost your grasp on boostingDemystifying the famous competition-winning algorithm.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "A Comparison of Shrinkage and Selection Methods for Linear RegressionA detailed look at 7 popular shrinkage & selection methods.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/non-linear-regression-basis-expansion-polynomials-splines-2d7adb2cc226", "anchor_text": "Non-linear regression: basis expansion, polynomials & splinesHow to capture non-linear relations with polynomials and splines.towardsdatascience.com"}, {"url": "https://robjhyndman.com/hyndsight/aic/", "anchor_text": "https://robjhyndman.com/hyndsight/aic/"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bb2d74229172---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bb2d74229172---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/statistics?source=post_page-----bb2d74229172---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb2d74229172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----bb2d74229172---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb2d74229172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----bb2d74229172---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb2d74229172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bb2d74229172--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbb2d74229172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bb2d74229172---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bb2d74229172--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bb2d74229172--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bb2d74229172--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bb2d74229172--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bb2d74229172--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bb2d74229172--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bb2d74229172--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bb2d74229172--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://michaloleszak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com"}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38bf302f5b56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-assessment-bb2d74229172&newsletterV3=c58320fab2a8&newsletterV3Id=38bf302f5b56&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}