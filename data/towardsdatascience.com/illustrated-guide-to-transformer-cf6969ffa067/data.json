{"url": "https://towardsdatascience.com/illustrated-guide-to-transformer-cf6969ffa067", "time": 1683008306.443467, "path": "towardsdatascience.com/illustrated-guide-to-transformer-cf6969ffa067/", "webpage": {"metadata": {"title": "Illustrated Guide to Transformers | by Jingles (Hong Jing) | Towards Data Science", "h1": "Illustrated Guide to Transformers", "description": "The Transformer model is the evolution of the encoder-decoder architecture, proposed in the paper Attention is All You Need. While encoder-decoder architecture has been relying on recurrent neural\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is All You Need", "paragraph_index": 0}, {"url": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735", "anchor_text": "Hochreiter & Schmidhuber", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "Bahdanau", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Transformer", "paragraph_index": 16}, {"url": "https://www.tensorflow.org/tutorials/text/transformer", "anchor_text": "TensorFlow", "paragraph_index": 40}, {"url": "https://pytorch.org/tutorials/beginner/transformer_tutorial.html", "anchor_text": "PyTorch", "paragraph_index": 40}, {"url": "https://huggingface.co/transformers/quickstart.html", "anchor_text": "Hugging Face", "paragraph_index": 40}, {"url": "http://linkedin.com/in/jingles", "anchor_text": "linkedin.com/in/jingles", "paragraph_index": 41}], "all_paragraphs": ["The Transformer model is the evolution of the encoder-decoder architecture, proposed in the paper Attention is All You Need. While encoder-decoder architecture has been relying on recurrent neural networks (RNNs) to extract sequential information, the Transformer doesn\u2019t use RNN. Transformer based models have primarily replaced LSTM, and it has been proved to be superior in quality for many sequence-to-sequence problems.", "Transformer relies entirely on Attention mechanisms to boost its speed by being parallelizable. It has produced state-of-the-art performance in machine translation. Besides significant improvements in language translation, it has provided a new architecture to solve many other tasks, such as text summarization, image captioning, and speech recognition.", "Before the Transformer model, recurrent neural networks (RNNs) have been the go-to method for sequential data, where the input data has a defined order. RNNs work like a feed-forward neural network that unrolls the input over its sequence, one after another.", "This process of unrolling each symbol in the input is done by the Encoder, whose objective is to extract data from the sequential input and encode it into a vector, a representation of the input.", "Examples of sequential input data are words (or characters) in a product review where the RNN will extract each word sequentially, forming a sentence representation. This representation will be used as a feature for the classifier to output a fixed-length vector, such as a sentiment label indicating positive/negative, or on a 5-point scale.", "In machine translation and image captioning, instead of a classifier that outputs a fixed-length vector, we can replace it with a Decoder. Like the Encoder that consumes each symbol in the input individually, the Decoder produces each output symbol over several time steps.", "For example, in machine translation, the input is an English sentence, and the output is the French translation. The Encoder will unroll each word in sequence and forms a fixed-length vector representation of the input English sentence. Then, the Decode will take the fixed-length vector representation as input, and produce each French word one after another, forming the translated English sentence.", "However, RNN models have some problems, they are slow to train, and they can\u2019t deal with long sequences.", "The input data needs to be processed sequentially one after the other. Such a recurrent process does not make use of modern graphics processing units (GPUs), which were designed for parallel computation. RNNs are so slow that truncated backpropagation was introduced to limit the number of timesteps in the backward pass\u2013estimating gradients to update the weights rather than backpropagation fully. Even with truncated backpropagation, RNNs are still slow to train.", "Secondly, RNNs also can\u2019t deal with long sequences very well as we get vanishing and exploding gradients if the input sequence is too long. Generally, you will see NaN (Not a Number) in the loss during the training process. These are also known as the long-term dependency problems in RNNs.", "In 1997, Hochreiter & Schmidhuber introduced the Long Short Term Memory (LSTM) networks, which are explicitly designed to avoid long-term dependency problems. Each LSTM cell allows past information to skip all the processing of the current cell and move to the next cell; this allows the memory to be retained longer and will enable data to flow along with it unchanged. LSTM consists of an input gate that decides what new information to be stored and a forget gate that determines what information to remove.", "Certainly, LSTMs have improved memory, able to deal with longer sequences than RNNs. However, LSTM networks are even slower as they are more complex.", "Another drawback of an RNN based encoder-decoder architecture is the fixed-length vector. Using a fixed-length vector to represent the input sequence to decode an entirely new sentence is difficult. The context vector cannot store all the information if the input sequence is large. Furthermore, it is challenging to differentiate sentences with similar words but with different meanings.", "Imagine this, selects a paragraph above (input) and memorize it (fixed-length vector). Then, translate that entire paragraph (output) without referring to it. It is difficult, and that is not how we do it. Instead, when we translate a sentence from one language to another, we look at the sentence part by part, by paying attention to a particular phrase of the sentence each time.", "Bahdanau proposed a method to search for parts of a source sentence relevant to predicting a target word in an encoder-decoder model. This is the beauty of the Attention mechanism; we can translate comparatively longer sentences without affecting its performance using Attention. For example, translating to \u201cnoir\u201d (which means \u201cblack\u201d in French), the Attention mechanism will focus on the word \u201cblack\u201d and possibly \u201ccat,\u201d ignoring other words in the sentence.", "The attention mechanism has increased encoder-decoder networks\u2019 performance, but the bottleneck in speed is still due to RNN having to process word by word sequentially. Can we remove RNN for sequential data?", "Yes! Attention is all you need. The Transformer architecture was introduced in 2017. Like the encoder-decoder architectures, where input sequences are fed into the Encoder, and Decoder will predict each word after another. The Transformer improves its time complexity and performance by eliminating RNN and utilizing the attention mechanism.", "Considering translating a sentence from English to French. In RNN, each hidden state has dependencies on the previous words\u2019 hidden state. Thus the embeddings of the current step are generated one time step at a time. With Transformer, there is no concept of the time step; the input sequence can be passed into the Encoder in parallel.", "Let\u2019s assume we are training a model that translates the English sentence to French. The Transformer architecture has two parts, the Encoder (left) and the Decoder (right). Let\u2019s examine the Transformer architecture.", "In the Encoder, it inputs an English sentence, and the output will be a set of encoded vectors for every word. Each word in the input English sentence is converted into an embedding to represent meaning. Then we add a positional vector to add the context of the word in the sentence. These word vectors are fed into Encoder attention block, which computes the attention vectors for every word. These attention vectors are passed in through a feed-forward network in parallel, and the output will be a set of encoded vectors for every word.", "The Decoder receives input of the French word(s) and attention vectors of the entire English sentence, to generate the next French word. It encodes each word\u2019s meaning with the embedding layer. Then positional vectors are added to represent the context of the word in the sentence. These word vectors are fed into the first attention block, the masked attention block. The masked attention block computes the Attention vectors for current and prior words. Attention vectors from the Encoder and Decoder are fed into the next attention block, which generates attention mapping vectors for every English and French word. These vectors are passed into the feed-forward layer linear layer and the softmax layer to predict the next French word. We repeat this process to generate the next word until the \u201cend of sentence\u201d token is generated.", "This is the high-level details of how Transformer works. Let\u2019s dive deeper and examine each component.", "Since computers don\u2019t understand words\u2014its meanings and its relationship between words like we do; we need to replace the words with vectors. Word vectors (or embeddings) allow every word to be mapped in a high dimensional embedding space, where words with similar meanings are closer to each other.", "Even though we can reference and represent each word\u2019s meaning with a vector, the true meaning of a word relies on the context in the sentence, as the same word in different sentences may have different meanings. As RNN was designed to capture sequence information, how the Transformer handles word order without RNN? We need positional encoders.", "The positional encoders receive inputs from the input embeddings layer and apply relative positional information. This layer outputs word vectors with positional information; that is the word\u2019s meaning and its context in the sentence.", "Consider the following sentences, \u201cThe dog bit Johnny\u201d and \u201cJohnny bit the dog.\u201d Without context information, both sentences would have almost identical embeddings. But we know this is not true, definitely not true for Johnny.", "The authors proposed using multiple sine and cosine functions to generate positional vectors. This way, we can use this positional Encoder for sentences of any length. The frequency and offset of the wave are different for each dimension, representing each position, with values between -1 and 1.", "This binary encoding method also allows us to determine if two words are near each other. For example, by referencing the low-frequency sine wave, if one word has \u201chigh\u201d while another is \u201clow,\u201d we know that they are further apart, one located at the beginning, another at the end.", "The key purpose of Attention is answering, \u201cwhat part of the input should I focus on?\u201d If we\u2019re encoding an English sentence, the question we want to answer is, \u201chow relevant is a word in the English sentence relevant to other words in the same sentence?\u201d This is represented in the Attention vector. For every word, we can have an Attention vector generated that captures contextual relationships between words in a sentence. For example, for the word \u201cblack,\u201d the Attention mechanism focus on \u201cblack\u201d and \u201ccat.\u201d", "As we are interested in the interactions between different words, the Attention vector for each word may weight itself too highly. As such, we need a way to normalize the vector. The Attention block accepts inputs V, K, and Q\u2013these are abstract vectors that extract different components of an input word. We use these to compute the Attention vectors for every word.", "Why is it called \u201cMulti-Head Attention\u201d? That is because we are using multiple Attention vectors per word and take a weighted average to compute the final Attention vector for every word.", "As the multi-head Attention block output multiple Attention vectors, we need to convert these vectors into a single Attention vector for every word.", "This feed-forward layer receives Attention vectors from the Multi-Head Attention. We apply normalization to transform it into a single Attention vector. Thus we get a single vector is digestible by the next encoder block or decoder block. In the paper, the author stacks six encoder blocks before output to the decoder block.", "Since we\u2019re translating from English to French, we input French word(s) to the Decoder. We replace the words with word embeddings, and then we add the positional vector to get the notion of context of the word in a sentence. We can feed these vectors that contain the word\u2019s meaning and its context in the sentence into the Decoder block.", "Similar to the Multi-Head Attention in the Encoder block. The Attention block generates Attention vectors for every word in the French sentence to represent how much each word is related to every word in the same output sentence.", "Unlike the Attention block in the Encoder that receiving every word in the English sentence, only the previous words of the French sentence are fed into this Decoder\u2019s Attention block. Thus, we mask the words appearing later using a vector and representing it in zeros, so the attention network can\u2019t use them while performing matrix operations.", "This Attention block acts as the Encoder-Decoder, which receives vectors from the Encoder\u2019s multi-head Attention and Decoder\u2019s Masked Multi-Head Attention. This Attention block will determine how related each word vector is with respect to each other, and this is where the mapping from English to French word happens. The output of this block is Attention vectors for every word in English and French sentences, where each vector representing the relationships with other words in both languages.", "Like the Encoder\u2019s feed-forward layer, this layer normalized each word consisting of multiple vectors into a single Attention vector for the next decoder block or a linear layer. In the paper, the author stacks six decoder blocks before output to the linear layer.", "As the purpose of the Decoder is to predict the following word, the output size of this feed-forward layer is the number of French words in the vocabulary. Softmax transforms the output into a probability distribution, which outputs a word corresponding to the highest probability of the next word.", "For each word generated, we repeat this process, including the French word, and used it to generate the next until the end of sentence tokens generated.", "Both TensorFlow and PyTorch has a step-by-step tutorial that can help you understand and train a sequence-to-sequence model with Transformer. If you need something fast for production, probably the most popular option is by Hugging Face.", "Alibaba PhD in machine learning | write about machine learning, neuroscience, healthcare & blockchain | reach me at linkedin.com/in/jingles"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcf6969ffa067&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://jinglesnote.medium.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Jingles (Hong Jing)"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F641197e9ee36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=post_page-641197e9ee36----cf6969ffa067---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf6969ffa067&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----cf6969ffa067---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf6969ffa067&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&source=-----cf6969ffa067---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is All You Need"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "source"}, {"url": "https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735", "anchor_text": "Hochreiter & Schmidhuber"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "Bahdanau"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Transformer"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "source"}, {"url": "https://www.tensorflow.org/tutorials/text/transformer", "anchor_text": "TensorFlow"}, {"url": "https://pytorch.org/tutorials/beginner/transformer_tutorial.html", "anchor_text": "PyTorch"}, {"url": "https://huggingface.co/transformers/quickstart.html", "anchor_text": "Hugging Face"}, {"url": "https://www.tensorflow.org/tutorials/text/transformer", "anchor_text": "Transformer model for language understanding | TensorFlow CoreThis tutorial trains a Transformer model to translate Portuguese to English. This is an advanced example that assumes\u2026www.tensorflow.org"}, {"url": "https://pytorch.org/tutorials/beginner/transformer_tutorial.html", "anchor_text": "Sequence-to-Sequence Modeling with nn.Transformer and TorchText - PyTorch Tutorials 1.5.0\u2026PyTorch 1.2 release includes a standard transformer module based on the paper Attention is All You Need. The\u2026pytorch.org"}, {"url": "https://huggingface.co/transformers/quickstart.html", "anchor_text": "Quickstart - transformers 2.10.0 documentationTransformers is an opinionated library built for NLP researchers seeking to use/study/extend large-scale transformers\u2026huggingface.co"}, {"url": "https://towardsdatascience.com/how-convolutional-layers-work-in-deep-learning-neural-networks-2913af333b72", "anchor_text": "How Convolutional Layers Work in Deep Learning Neural Networks?An animated ELI5 way to understand convolutions and its parameterstowardsdatascience.com"}, {"url": "https://www.linkedin.com/in/jingles/", "anchor_text": ""}, {"url": "https://towardsdatascience.com/@jinglesnote", "anchor_text": ""}, {"url": "https://jingles.substack.com/subscribe", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cf6969ffa067---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/technology?source=post_page-----cf6969ffa067---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cf6969ffa067---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----cf6969ffa067---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----cf6969ffa067---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf6969ffa067&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----cf6969ffa067---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcf6969ffa067&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----cf6969ffa067---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf6969ffa067&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F641197e9ee36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=post_page-641197e9ee36----cf6969ffa067---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb9c0df6b9368&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&newsletterV3=641197e9ee36&newsletterV3Id=b9c0df6b9368&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----cf6969ffa067---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Written by Jingles (Hong Jing)"}, {"url": "https://jinglesnote.medium.com/followers?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "1.7K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://linkedin.com/in/jingles", "anchor_text": "linkedin.com/in/jingles"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F641197e9ee36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=post_page-641197e9ee36----cf6969ffa067---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb9c0df6b9368&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-transformer-cf6969ffa067&newsletterV3=641197e9ee36&newsletterV3Id=b9c0df6b9368&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----cf6969ffa067---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28?source=author_recirc-----cf6969ffa067----0---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=author_recirc-----cf6969ffa067----0---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=author_recirc-----cf6969ffa067----0---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Jingles (Hong Jing)"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cf6969ffa067----0---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/why-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28?source=author_recirc-----cf6969ffa067----0---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Why Linear Regression is not suitable for Binary ClassificationWhy logistic regression better than linear regression for classification problems, and 2 reasons why linear regression is not suitable"}, {"url": "https://towardsdatascience.com/why-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28?source=author_recirc-----cf6969ffa067----0---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "\u00b76 min read\u00b7May 7, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc64457be8e28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----c64457be8e28----0-----------------clap_footer----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28?source=author_recirc-----cf6969ffa067----0---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc64457be8e28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28&source=-----cf6969ffa067----0-----------------bookmark_preview----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cf6969ffa067----1---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----cf6969ffa067----1---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----cf6969ffa067----1---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cf6969ffa067----1---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cf6969ffa067----1---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cf6969ffa067----1---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----cf6969ffa067----1---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----cf6969ffa067----1-----------------bookmark_preview----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cf6969ffa067----2---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----cf6969ffa067----2---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----cf6969ffa067----2---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cf6969ffa067----2---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cf6969ffa067----2---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cf6969ffa067----2---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----cf6969ffa067----2---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----cf6969ffa067----2-----------------bookmark_preview----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/parametric-vs-nonparametric-machine-learning-algorithms-5bf31393d944?source=author_recirc-----cf6969ffa067----3---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=author_recirc-----cf6969ffa067----3---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=author_recirc-----cf6969ffa067----3---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Jingles (Hong Jing)"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----cf6969ffa067----3---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/parametric-vs-nonparametric-machine-learning-algorithms-5bf31393d944?source=author_recirc-----cf6969ffa067----3---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "Parametric vs. Nonparametric Machine Learning AlgorithmsWhat\u2019s the difference, when to use it, and a few examples"}, {"url": "https://towardsdatascience.com/parametric-vs-nonparametric-machine-learning-algorithms-5bf31393d944?source=author_recirc-----cf6969ffa067----3---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": "\u00b74 min read\u00b7Mar 7, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5bf31393d944&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparametric-vs-nonparametric-machine-learning-algorithms-5bf31393d944&user=Jingles+%28Hong+Jing%29&userId=641197e9ee36&source=-----5bf31393d944----3-----------------clap_footer----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/parametric-vs-nonparametric-machine-learning-algorithms-5bf31393d944?source=author_recirc-----cf6969ffa067----3---------------------c88cc58c_a767_4097_bc82_b9280939ce2a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5bf31393d944&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparametric-vs-nonparametric-machine-learning-algorithms-5bf31393d944&source=-----cf6969ffa067----3-----------------bookmark_preview----c88cc58c_a767_4097_bc82_b9280939ce2a-------", "anchor_text": ""}, {"url": "https://jinglesnote.medium.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "See all from Jingles (Hong Jing)"}, {"url": "https://towardsdatascience.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----0-----------------clap_footer----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----cf6969ffa067----0-----------------bookmark_preview----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Albers Uzila"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Wanna Break into Data Science in 2023? Think Twice!It won\u2019t be smooth sailing for you"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "\u00b711 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&user=Albers+Uzila&userId=159e5ce51250&source=-----26842e9a87fe----1-----------------clap_footer----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&source=-----cf6969ffa067----1-----------------bookmark_preview----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----cf6969ffa067----0---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----cf6969ffa067----0-----------------bookmark_preview----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----cf6969ffa067----1---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----cf6969ffa067----1-----------------bookmark_preview----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----cf6969ffa067----2---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----cf6969ffa067----2---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----cf6969ffa067----2---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----cf6969ffa067----2---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----cf6969ffa067----2---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----cf6969ffa067----2---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----2-----------------clap_footer----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----cf6969ffa067----2---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----cf6969ffa067----2-----------------bookmark_preview----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----cf6969ffa067----3---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----cf6969ffa067----3---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----cf6969ffa067----3---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----cf6969ffa067----3---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----cf6969ffa067----3---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----3-----------------clap_footer----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----cf6969ffa067----3---------------------cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----cf6969ffa067----3-----------------bookmark_preview----cf70aaeb_a731_4e28_8688_875bbe4d7ec0-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----cf6969ffa067--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}