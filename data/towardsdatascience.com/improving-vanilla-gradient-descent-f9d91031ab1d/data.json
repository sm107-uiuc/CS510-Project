{"url": "https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d", "time": 1682993250.571365, "path": "towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d/", "webpage": {"metadata": {"title": "Improving Vanilla Gradient Descent | by Devin Soni | Towards Data Science", "h1": "Improving Vanilla Gradient Descent", "description": "When we train neural networks with gradient descent, we risk the network falling into local minima, in which the network stops somewhere along the error surface that is not the lowest point on the\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["When we train neural networks with gradient descent, we risk the network falling into local minima, in which the network stops somewhere along the error surface that is not the lowest point on the overall surface. This is because the error surfaces are not inherently convex, so the surface may contain many independent local minima separate from the global minimum. Additionally, while the network may reach a global minimum and converge to a desirable point for the training data, there is no guarantee as to how well it will generalize what it has learned. This means that they are prone to overfitting on the training data.", "There are several things that we may use in order to help mitigate these issues, although there is no way to definitively prevent them from occurring, as the error surfaces for these networks tend to be quite difficult to traverse, and neural networks as a whole are rather difficult to interpret.", "These adaptations of the standard gradient descent algorithm use a subset of the training data for each iteration of the algorithm. SGD will use one example per weight update, and mini-batch SGD will use a predefined number (typically much smaller than the total number of training examples). This allows training to occur much faster, as it requires far fewer computations since we do not use the entire data-set in each iteration. It also, hopefully, leads to better performance, as the network\u2019s jerkier movements during training should allow it to better avoid local minima, and using only small portions of the dataset should help prevent overfitting.", "Regularization, in general, is a mechanism that penalizes model complexity by adding a term to the loss function that represents model complexity. In the case of a neural network, it penalizes large weights, which may indicate that the network has overfit to the training data.", "Using L2 regularization, we may rewrite the loss function as follows, denoting the network\u2019s original loss function as L(y, t) and the regularization constant as \u03bb:", "Regularization adds the sum of the squares of every weight in the network to the loss function, penalizing the model for attributing too much weight to any one connection, and hopefully reducing overfitting.", "Momentum, simply put, adds a fraction of the past weight update to the current weight update. This helps prevent the model from getting stuck in local minima, as even if the current gradient is 0, the past one most likely was not, so it will as easily get stuck. By using momentum, the movements along the error surface are also smoother in general and the network can move more quickly throughout it.", "With simple momentum, we may rewrite the weight update equation as follows, denoting \u03b1 as the momentum factor:", "There are also other, more advanced, forms of momentum such as the Nesterov method.", "Rather than using a constant learning rate throughout training, we may anneal the learning rate, and have it decline as time progresses.", "The most common schedule has a 1/t relationship as follows, where T and \u03bc_0 are provided hyper-parameters, and \u03bc is the current learning rate:", "This is often referred to as the \u201csearch-then-converge\u201d annealing schedule, as until t hits T, the network is in the \u201csearch\u201d phase and the learning rate is not decreased much, and afterwards, the learning rate slows down and the network reaches the \u201cconverge\u201d phase. This loosely pertains to the balance between exploitation and exploration. In the beginning we prioritize exploring the search space and expanding our overall knowledge of the space, and as time progresses, we transition into exploiting the good areas in the search space that we have already found and narrowing into a specific minimum.", "These are some ways to improve upon the standard gradient descent algorithm. Of course, each of these methods will add hyper-parameters to your model, and will thus increase the amount of time spent tuning the network. Recently, newer algorithms such as Adam, Adagrad, and Adadelta have sprung up that use some of these techniques as well as many others. They tend to optimize on a per-parameter basis, rather than globally, so they can fine tune the learning rate based on individual circumstances. They tend to work much faster and better in practice; however, they are far more difficult to implement properly. The graphic below illustrates each of the mentioned gradient descent variations working simultaneously. Observe that the more complex versions converge much more quickly than the simple momentum or SGD versions do.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff9d91031ab1d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@devins?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@devins?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "Devin Soni"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f4d2b8b896d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&user=Devin+Soni&userId=5f4d2b8b896d&source=post_page-5f4d2b8b896d----f9d91031ab1d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9d91031ab1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9d91031ab1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f9d91031ab1d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f9d91031ab1d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----f9d91031ab1d---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f9d91031ab1d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f9d91031ab1d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff9d91031ab1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&user=Devin+Soni&userId=5f4d2b8b896d&source=-----f9d91031ab1d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff9d91031ab1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&user=Devin+Soni&userId=5f4d2b8b896d&source=-----f9d91031ab1d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9d91031ab1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff9d91031ab1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f9d91031ab1d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f9d91031ab1d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@devins?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@devins?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Devin Soni"}, {"url": "https://medium.com/@devins/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6.9K Followers"}, {"url": "https://medium.com/@devins/membership", "anchor_text": "https://medium.com/@devins/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f4d2b8b896d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&user=Devin+Soni&userId=5f4d2b8b896d&source=post_page-5f4d2b8b896d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa0d2763ed2eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-vanilla-gradient-descent-f9d91031ab1d&newsletterV3=5f4d2b8b896d&newsletterV3Id=a0d2763ed2eb&user=Devin+Soni&userId=5f4d2b8b896d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}