{"url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "time": 1683009891.6232672, "path": "towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a/", "webpage": {"metadata": {"title": "Reinforcement Learning for Formula 1 Race Strategy | by Ashref Maiza | Towards Data Science", "h1": "Reinforcement Learning for Formula 1 Race Strategy", "description": "This article presents my experiment prototyping a deep reinforcement learning agent to help Formula 1 teams optimize their strategy decisions in real-time during the race. A neural network is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "dynamic programming", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)", "anchor_text": "model-free reinforcement learning", "paragraph_index": 17}, {"url": "https://sbgsportssoftware.com/", "anchor_text": "SBG Sports Software", "paragraph_index": 20}, {"url": "https://gym.openai.com/", "anchor_text": "Open AI Gym", "paragraph_index": 24}, {"url": "https://www.formula1.com/en/results.html/2019/races/1005/monaco/qualifying.html", "anchor_text": "Monaco 2019", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Q-learning", "anchor_text": "Q-learning", "paragraph_index": 26}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind", "paragraph_index": 27}, {"url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf", "anchor_text": "playing Atari Games", "paragraph_index": 27}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "MSE", "paragraph_index": 32}, {"url": "https://deepmind.com/blog/article/alphago-zero-starting-scratch", "anchor_text": "AlphaGo Zero", "paragraph_index": 35}, {"url": "https://en.wikipedia.org/wiki/Nash_equilibrium", "anchor_text": "Nash Equilibrium", "paragraph_index": 36}, {"url": "https://www.formula1.com/en/results.html/2019/races/1005/monaco/qualifying.html", "anchor_text": "Monaco 2019", "paragraph_index": 40}], "all_paragraphs": ["This article presents my experiment prototyping a deep reinforcement learning agent to help Formula 1 teams optimize their strategy decisions in real-time during the race. A neural network is designed as a mapping function from observed lap data to control actions and instantaneous reward signals. The conceptual framework introduced here is analyzed in the context of Monaco GP, but the same approach can be improved and generalized to all Grand Prix events.", "Formula 1 is one kind of war without bullets and the main weapon in this war is innovation. Deciding a race strategy is the dark art in this sport. Fans usually enjoy the high speed maneuvers performed by brilliant drivers they get to see on TV. What they do not see is how much preparation is required beforehand to achieve better results, involving the use of mathematical models and probability theory. By racing strategy we mainly refer to these two crucial decisions:", "Tyres degrade during the race which may lead to slower lap times. This process is inevitable and can be quicker or smoother depending on the compound (soft, medium, hard, etc.) and the track that holds the race.If the lap time is slow to a level that the car loses more time over a number of laps than a pit stop would cost, then probably the tyres need to be changed.", "Nowadays, Formula 1 teams use Monte Carlo simulations which they run for many hours before the race in order to decide a strategy for each of their two drivers. The way these simulations work is by sampling the race events lap by lap thousands of times in different configurations (pit stop lap, tyre compounds, positions, etc.) for the driver in question but also for all other opponents. They can only run a limited number of simulations due to the large space of possible scenarios and they pick the strategies that, among those simulations, revealed to lead to the best average outcomes.", "During the race event, many new scenarios could happen and race strategists sometimes find themselves obliged to adapt their decisions based on human judgements. Sometimes these decisions work well and lead to scoring more championship points. In other cases, it leads to frustration - losing important positions to other competitors who performed better decisions.", "This situation motivated me to think of how we could teach an AI system to play the game of Race Strategy. The goal is to help a Formula 1 driver cross the finish line winning places (or not losing any) against other competitive team strategies.", "Reinforcement Learning (RL) is an advanced machine learning (ML) technique which takes a very different approach to training models than other machine learning methods. Its super power is that it learns very complex behaviors without requiring any labeled training data, and can make short term decisions while optimizing for a longer term goal.", "In RL, an agent learns the optimal behavior to perform a certain task by interacting directly with the environment and maximizing the total reward it gets. In Formula 1 racing, the total reward (return) could be thought of as the number of places gained or lost by a specific driver at the end of the race. So, the agent can decide to pit at a particular lap and lose some positions if it thinks that the driver would achieve a better position by the end of the race.", "For simplification, we will consider the set of possible actions at each lap as follows:", "The first interesting idea to introduce by applying RL for Formula 1 race strategy is the concept of \u201cControl\u201d.", "A prediction task in Reinforcement Learning is where a policy is being given, and the goal is to measure how well it performs at any given state. This is somehow similar to what the simulations run by F1 teams try to achieve. Before the race, they want to predict how good a certain strategy would be if it was applied starting from the first lap.", "But, things get really interesting in RL when we start performing control! A control task is when the policy is not fixed, and the goal is to find the optimal behavior. That is to find the optimal policy that given any state, always provides the best decision that maximizes the expected total reward. In Formula 1, this would be very interesting as it offers a way to learn how to exploit very complex patterns and come up with strategies that adapt in real time to each lap and that hopefully beats the best strategy experts from the competition.", "Let\u2019s consider that one state in our Formula 1 racing environment corresponds to the information we get on the following TV screen.", "A state is represented mainly by the lap number, drivers ranking, time interval between drivers and their pace (last lap time) as well as their tyre compounds (Soft, Medium or Hard) and age (in number of laps). A safety car flag could also be considered. Besides, there is a rule that imposes at least two different dry compounds during the race for each car. So, we can imagine a flag for each driver indicating wether or not a second dry compound was used.", "A Formula 1 race can then be formulated as a Markov Decision Process (MDP) where the probability of transitioning from a state to another relies only on the last observed state. At lap 11, all we need to decide for the next lap is the situation we observe at lap 11 (lap 1 to 10 become much less important).", "\u201cThe future is independent of the past given the present.\u201d", "Planning a strategy for the next laps in the race may require a perfect model of the environment where we know exactly what would be the reward from taking an action a when in state s and the probability of transitioning from state s to state s\u2019 under action a. But in reality, that perfect model of the environment is hard to obtain and many events happening during the race may be stochastic. Think of how the reward depends on the behavior of other cars, the continuous change in lap times and driver positions. So, we could not use a methodology like dynamic programming. By definition, a state value V(s) is the cumulative reward estimated for being in state s and following the policy moving forward. Dynamic programming is an iterative approach that relies on full-width backups of these state values. It is too expensive when the number of possible states is too large like in the case of Formula 1 racing.", "For these reasons, we will rely on model-free reinforcement learning where the main idea is to sample particular trajectories from a state to evaluate actions in a trial-and-error setup by interacting with the environment.", "State of the art reinforcement learning has been usually demonstrated on classic games like Atari, Chess or Go. These are perfectly observable environments and we like studying them because they can be formulated using simple rules. They also allow to easily benchmark AI performance against human-level performance.", "Formula 1 racing, though, is a real-world issue. It is an imperfect information game because partially observable. The main challenge consists in building an emulator that would respect the logic and complex rules of the game.", "We previously discussed the set of possible actions (pit stop and tyre compound) and the environment state (information we see on TV). This information could be provided to all teams by the F1 broadcast center and platforms like SBG Sports Software. We will use a Pandas dataframe to represent every state in the environment. Further information is included such as the potential pace of the car [potential_pace] and a flag indicating if a second dry compound has already been used [second_dry].", "The potential pace is the estimation of car pace in free air when not blocked by other cars. It is computed using a specific function taking into account fuel mass, tyre compound and their age.", "Each lap is a new step in the environment which introduces changes in the observable measures (pace, tyre age, interval, etc.) and can lead to new drivers ranking. The agent decides a strategy to only one car at a time. After each step in the environment, a reward is calculated as the number of places gained or lost by that particular car.", "Some of the important tools necessary to compute a step is the overtake model. This model provides a probability of overtake for each driver. It takes into account the interval to driver in front, potential pace of driver in front, potential pace of driver in question and a parameter representing the difficulty of overtake which is specific to the race track. Another important tool is the time spent in pit stop, this can be learned from past races or estimated during the weekend and has major impact on where a car will end up after pit stop.", "Open AI Gym is an open source framework that provides significant help in structuring and implementing custom environments. Building a reinforcement learning environment that emulates well the dynamics of a Formula 1 Race is very important and challenging. This requires a deep understanding of the sport and many efforts in coding and testing the implementation. In order to develop and evaluate the approach, we decided to parametrize the emulator as for Monaco GP, a track where overtaking is known to be difficult. We used the qualifying results of Monaco 2019 to initialize the starting grid and train the system for that particular race.", "After implementing the environment, we need to design the agent responsible of recommending a pit stop decision at each lap. The agent, when associated to a specific car has one ultimate goal which is maximizing the total reward that could be obtained by that car. Remember that total reward is defined as the number of places gained or lost across an entire race episode.", "Q-learning is one of the techniques used in reinforcement learning to find the optimal policy according to which the agent should adapt its behavior. For each state, it is possible to estimate the total reward that would be obtained by taking a specific action and continuously following the policy. This total reward obtained from a (state, action) pair is called the Q-value. If we can estimate the Q-value for each (state, action) pair, the agent will behave optimally at each state by deciding the action that has the largest estimated Q-value maximizing the total reward.", "Because the space of possible states in a Formula1 race is infinite, we cannot store all states in memory and compute a Q-value for each (state, action) combination. We need a neural network to approximate the Q-value function. Usually, it is called a Deep Q-Network (DQN) and the idea was first used by DeepMind to build an artificially intelligent system capable of playing Atari Games better than the best human experts. Without this approach, it becomes very hard to maintain computation and memory efficiency especially in cases like Formula 1 racing which comes with continuous and high cardinality feature spaces.", "We implement a dense neural network in TensorFlow. It takes as input a vectorized representation of the environment state and outputs the estimated Q-value for each action. The next action is then determined by the maximum output from this network.", "There are two main ideas that make this approach stable compared to naive Q-learning: experience replay and fixed Q-targets .", "Experience replay: We store the agents experience (state, action, reward, next_state) in a replay memory so that we can randomly sample batches of transitions and use them as training data. The major advantage of applying this technique is that it stabilizes the Q-learning method as it decorrelates the trajectories and uses efficient updates of the neural network. Usually in the literature, it is recommended to use a replay memory of size ~1M transitions. Nevertheless, the best results on this use case were obtained by using a reduced size for the replay memory ~15000 transitions and a batch size of 32 transitions. This forces the network to learn from each transition frequently and faster for a short time period until it definitely disappears from the replay memory and gets replaced by new unseen transitions that are sampled by following an improved policy.", "Fixed Q-targets: The target values for training this model are obtained by using a target network which is a past version of the main behavior network. We keep this target network frozen for a certain number of steps in the mini-batch learning process and teach the behavior network to estimate Q-values towards those frozen targets. By choosing the right number of steps after which we update the target network ~760 steps, we could achieve a stabilized training.", "The objective of the optimization is to minimize the MSE loss between the target values generated using the target network and the estimated values generated using the behavior network.", "We may think that the network would be learning its own predictions, but because the reward r is the unbiased true reward, the network will update its weights using backpropagation to finally converge to the optimal Q-values.", "In the following example, we can observe the behavior of the Q-network applied to Pierre Gasly\u2019s car (GAS). It estimates the total reward from each combination of a given state (at lap 3) and all possible actions (action 0: pit for soft, action 1: pit for medium, action 2: pit for hard, action 3: continue). The agent estimates a total reward of +0.69 places gained by the end of the race if action 3 (continue) is applied. Because this Q-value is the highest outcome, it will then recommend action 3 as a decision for that particular state. If Gasly was to pit for the soft compound at that lap, this would put him at the risk of losing 3 positions by the end of the race according to the list of Q-values estimated below.", "In 2017, DeepMind introduced AlphaGo Zero, a reinforcement learning algorithm that learned to master the game of Go without seeing any previous human play. The system starts off without knowing any technique and learns to improve its ability only by playing itself. The essence of this idea is that AI systems, in order to beat human-level performance, should not be constrained by the limits of human knowledge.", "By analogy, we can think of a Formula 1 race as a multi-player game involving 20 cars where each car is trying to maximize its ranking by the end of the race and beat other opponent strategies. If all other players fix their policies, then the best response is the optimal policy against those policies. Thus, it is interesting to optimize the system towards the Nash Equilibrium which is a joint policy for all players such that every player\u2019s policy is a best response. The best response is then a solution to a single-agent RL problem where other players become part of the environment. We populate the replay memory by generating RL experience where the agent is playing an older version of itself. In addition to the behavior network and the target network, we decided to use a third network responsible of generating decisions for other cars when optimizing for a specific one. Just like the target network, this opponent network is a past version of the behavior network but updated 10 times less frequently than the target network (every ~7600 steps).", "While training the system to play 15000 races against itself, we could notice the convergence towards the Nash Equilibrium as the total reward obtained at each episode oscillated around zero for different cars and race trajectories. During that process, the system was learning to design a race strategy and improve its ability only by self-play!", "This plot shows the decisions made by the system for each car at each lap in the early stages of the learning process. We can see that it was pitting too often which is not common sense in Formula 1 as so much time was wasted.", "However, The following graphs show exactly how the system learned it should continue more often and optimize the use of tyres. For each episode, we count the number of laps where each action has been taken except for the first two laps (used for environment initialization). Notice how the system is learning to increase the frequency of the continue decision (action 3) and make less use of the hard compound (action 2) as compared to other faster compounds (medium and soft).", "A nice way to evaluate the final system outside the multi-agent training process is by using it for a specific car and make it compete against an older version of itself applied to other cars. We will use the agent trained for 15000 episodes as the control agent for Perez (PER) and use same agent trained only for 12000 episodes (80%) to decide opponent strategies for all other cars. Knowing that Perez started Monaco 2019 race in the 16th position, it is interesting to see how he could gain +1 position by the end of the race following the latest agent\u2019s behavior policy.", "In this case, we are evaluating the agents performance against an older version of itself that also appears to be a strong strategist. Notice how the opponent strategy system learned to decide a one-stop strategy for many drivers with no prior knowledge. It decided the pit stop lap and tyre choice depending on the specific situation of each car it controls and managed to maintain the driver\u2019s position as part of the Nash Equilibrium.", "For Raikkonen (RAI), it decided two sprints on the fastest soft compound, then pit stop at the very end of the race for a hard compound. The reason of this last pit stop is to respect the rule of using two different dry compounds in the same race. It clearly demonstrates the ability of the agent to optimize its decisions while autonomously learning the rules of the game.", "What would be more interesting is to compare the performance of the agent against predefined strategies decided by Formula 1 teams while interacting with a real race event.", "In this work, we introduced a deep reinforcement learning approach that offers the advantage of optimizing the Formula 1 race strategy live during the race. This solution differs from classic simulation approaches in some notable ways:", "What is exciting about RL is that the system doesn\u2019t mainly rely on history data to improve decision making. It learns directly by interacting with the environment. Surprisingly, for many real world applications, interacting with an environment may be possible. We define a set of actions, the space of observations and focus the effort on engineering the most convenient reward function. This AI framework for Formula 1 race strategy can be enhanced by taking into account the probability of safety car and the weather conditions that may change the course of a race.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7f29c966472a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7f29c966472a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7f29c966472a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashrefm?source=post_page-----7f29c966472a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashrefm?source=post_page-----7f29c966472a--------------------------------", "anchor_text": "Ashref Maiza"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F80c2f1871537&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&user=Ashref+Maiza&userId=80c2f1871537&source=post_page-80c2f1871537----7f29c966472a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f29c966472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f29c966472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@gustavocpo?utm_source=medium&utm_medium=referral", "anchor_text": "gustavo Campos"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.fia.com/news/fia-and-pirelli-announce-2020-f1-tyre-specification", "anchor_text": "FIA 2019 & 2020"}, {"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "dynamic programming"}, {"url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)", "anchor_text": "model-free reinforcement learning"}, {"url": "https://sbgsportssoftware.com/", "anchor_text": "SBG Sports Software"}, {"url": "https://gym.openai.com/", "anchor_text": "Open AI Gym"}, {"url": "https://www.formula1.com/en/results.html/2019/races/1005/monaco/qualifying.html", "anchor_text": "Monaco 2019"}, {"url": "https://en.wikipedia.org/wiki/Q-learning", "anchor_text": "Q-learning"}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind"}, {"url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf", "anchor_text": "playing Atari Games"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://en.wikipedia.org/wiki/Mean_squared_error", "anchor_text": "MSE"}, {"url": "https://deepmind.com/blog/article/alphago-zero-starting-scratch", "anchor_text": "AlphaGo Zero"}, {"url": "https://en.wikipedia.org/wiki/Nash_equilibrium", "anchor_text": "Nash Equilibrium"}, {"url": "https://www.formula1.com/en/results.html/2019/races/1005/monaco/qualifying.html", "anchor_text": "Monaco 2019"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----7f29c966472a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-q-learning?source=post_page-----7f29c966472a---------------deep_q_learning-----------------", "anchor_text": "Deep Q Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7f29c966472a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/formula-1?source=post_page-----7f29c966472a---------------formula_1-----------------", "anchor_text": "Formula 1"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----7f29c966472a---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f29c966472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&user=Ashref+Maiza&userId=80c2f1871537&source=-----7f29c966472a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f29c966472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&user=Ashref+Maiza&userId=80c2f1871537&source=-----7f29c966472a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f29c966472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7f29c966472a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7f29c966472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7f29c966472a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7f29c966472a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7f29c966472a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7f29c966472a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7f29c966472a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7f29c966472a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7f29c966472a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7f29c966472a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7f29c966472a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashrefm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashrefm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashref Maiza"}, {"url": "https://medium.com/@ashrefm/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "130 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F80c2f1871537&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&user=Ashref+Maiza&userId=80c2f1871537&source=post_page-80c2f1871537--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3a2ae36b9258&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-formula-1-race-strategy-7f29c966472a&newsletterV3=80c2f1871537&newsletterV3Id=3a2ae36b9258&user=Ashref+Maiza&userId=80c2f1871537&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}