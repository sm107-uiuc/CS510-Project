{"url": "https://towardsdatascience.com/a-guide-to-text-classification-and-sentiment-analysis-2ab021796317", "time": 1683010699.789653, "path": "towardsdatascience.com/a-guide-to-text-classification-and-sentiment-analysis-2ab021796317/", "webpage": {"metadata": {"title": "A Guide to Text Classification and Sentiment Analysis | by Abhijit Roy | Towards Data Science", "h1": "A Guide to Text Classification and Sentiment Analysis", "description": "Motivation: Text Classification and sentiment analysis is a very common machine learning problem and is used in a lot of activities like product predictions, movie recommendations, and several\u2026"}, "outgoing_paragraph_urls": [{"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv", "paragraph_index": 104}], "all_paragraphs": ["Motivation: Text Classification and sentiment analysis is a very common machine learning problem and is used in a lot of activities like product predictions, movie recommendations, and several others. Currently, for every machine learner new to this field, like myself, exploring this domain has become very important. After exploring the topic, I felt, if I share my experience through an article, it may help some people trying to explore this field. So, I will try to build the whole thing from a basic level, so this article may look a little long, but it has some parts you can skip if you want.", "Text classification problems like sentimental analysis can be achieved in a number of ways using a number of algorithms. These are majorly divided into two main categories:", "This is the very basic difference between the two approaches. Now, let\u2019s see in detail.", "For the purpose, we will use, the IMDB review dataset. It has 50,000 reviews and their corresponding sentiments marked as \u201cPositive\u201d and \u201cNegative\u201d. So, first, we need to check and clean our dataset.", "These are the preprocessing required to clean the reviews part and the Convert_to_binary is used to convert \u201cPositive\u201d sentiment to 1 and \u201cNegative\u201d to 0. So, after preprocessing we obtain:", "If we count plot our target class division.", "We have 25k positive sentiments and 25k negative sentiments. So, let\u2019s split up the data.", "In this method, we create a single feature vector using all the words in the vocabulary, that we obtain from tokenizing the sentences in the train sets. This feature vector is used to represent all the reviews in our set. Each word is basically regarded as a feature. So, the number of features is equal to the number of unique words in the vocabulary. Now, each sentence or review is regarded as a sample or record. If the word is present in that sample it has some values for the word which is regarded as the feature, and if the word is not present it is zero. So, if \u2018It is a sunny day\u2019, and \u2018The Sun rises in east\u2019 are two sentences. and their bag will have 10 words which will be the feature vector size. Say vector is: [\u2018It\u2019, \u2018is\u2019, \u2018a\u2019, \u2018sunny\u2019, \u2018day\u2019, \u2018The\u2019, \u2018Sun\u2019, \u2018rises\u2019, \u2018in\u2019, \u2018east\u2019]. So, the first sentence is represented by [a,b,c,d,e,0,0,0,0,0], a,b,c,d,e are values depending on the scheme we use", "So, each sample has the same feature set size which is equal to the size of the vocabulary. Now, the vocabulary is basically made of the words in the train set. All the samples of the train and test set are transformed using this vocabulary only. So, there may be some words in the test samples which are not present in the vocabulary, they are ignored.", "They form very sparse matrices or feature sets. Similar to a normal classification problem, the words become features of the record and the corresponding tag becomes the target value. If we consider as a dataset, the samples or reviews will be the rows or records, the feature set of each record, or the feature columns corresponding to each record will be equal to the size of the vocabulary, where each word will be a feature. So, it is actually like a common classification problem with the number of features being equal to the distinct tokens in the training set.", "This can be done in two ways:", "We will be using scikit-learn\u2019s feature extraction libraries here. But one thing to notice here is, this can also be done using TensorFlow's Tokenizer function. It can be done in mainly three ways using tokenizer:", "Now, if we notice, the vector is fit only to X_train. Here is where the vocabulary is formed. So vocabulary contains only the words in the train set. Then we transform on both train and test set.", "This is our model for the count vectorization approach. We can see that the input dimension is of size equal to the number of columns for each sample which is equal to the number of words in our vocabulary. The output is 1 i.e, sentiment Positive or Negative.", "The model gives an accuracy of 87.3%", "Now, here we call the TF-IDF vectorizer and the data is transformed. Here we use a logistic regression model.", "This model gives an accuracy of 89.4%", "In this method, the words are individually represented as a vector. In the case of the bag of words, all of the words in the vocabulary made up a vector. Say, there are 100 words in a vocabulary, so, a specific word will be represented by a vector of size 100 where the index corresponding to that word will be equal to 1, and others will be 0.", "So, Each sample having a different number of words will basically have a different number of vectors, as each word is equal to a vector. Now, to feed a model we will need to have the same dimension for each sample, and as a result, padding is needed to make the number of words in each sample equal to each other.", "Basically in the bag of words or vectorizer approach, if we have 100 words in our total vocabulary, and a sample with 10 words and a sample with 15 words, after vectorization both the sample sizes would be an array of 100 words, but here for the 10 words it will be a (10 x 100) i.e, 100 length vector for each of the 10 words and similarly for 15th one size will be (15 x 100). So, we need to find the longest sample and pad all others up to match the size.", "We can do this in some ways:", "One-Hot encoding: It is just taking the size of the vocabulary and making an array of that size with 0\u2019s at all indices and 1 at only the index of the word, as we have discussed above. But these things provide us with very little information and create a very sparse matrix.", "The Second Choice is the word embeddings.", "The one-hot encoder is a pretty hard-coded approach. It is of a very high dimension and sparse with a very low amount of data. Embedding is a way to create a dense vector representation. It is of a lower dimension and helps to capture much more information. It more like captures the relationships and similarities between words using how they appear close to each other. For example, the king, queen, men, and women will have some relations.", "Say, we are having 10k words are being embedded in a 300-dimensional embedding space. To do this, we declare the number of nodes in the embedding layer=300. Now, each word of the 10k words enters the embedding layer. Each of the words will be placed in a 300-dimensional plane based on their similarities with one another which is decided by several factors, like the order in which the words occur. Now, being placed in 300 Dimensional planes the words will have a 300 length tuple to represent it which are actually the coordinates of the point on the 300-dimensional plane. So, this 300-dimensional tuple becomes the new feature set or representing a vector for the word.", "So, the vector for the word decreased from 10k to 300. The tuples serve as feature vectors between two words and the cosine angle between the vectors represents the similarity between the two words.", "We can do this in two ways:", "Now, for the embedding, we need to send each sample through an embedding layer first then move to make them dense using embedding. These embedding layers see how the words are used, i.e, it tries to see if two words always occur together or are used in contrast. After judging all these factors the layer places the word in a position one the n-dimensional embedding space.", "We can use pre-trained word embeddings like word2vec by google and GloveText by Standford. They are trained on huge corpora with billions of examples and words. Now, they have billions of words we have only say, a 10k so, training our model with a billion words will be very inefficient. We need to just select out our required word\u2019s embeddings from their pre-trained embeddings.", "Now, How are these embeddings found?", "For google\u2019s word2vec implementations, there are two ways:", "Both of these algorithms actually use a Neural Network with a single hidden layer to generate the embedding.", "For CBOW, the context of the words, i.e, the words before, and after the required words are fed to the neural network, and the model is needed to predict the word.", "For the Skip-Gram, the words are given and the model has to predict the context words.", "In both cases, the feature vectors or encoded vectors of the words are fed to the input. The output has a softmax layer with a number of nodes equal to the vocabulary size, which gives the percentage of prediction for each word, i.e, it tells what is the probability that the required word is the word, the node in the softmax layer is representing. Though we don\u2019t use the output layer actually.", "We go for the weight matrix produced in the hidden layer. The number of nodes in the hidden layer is equal to the embedding dimension. So, say if there are 10k words in vocabulary and 300 nodes in the hidden layer, each node in the hidden layer will have an array of weights of the dimension of 10k for each word after training.", "Because the neural network units work on", "Here x1, x2\u2026\u2026 xn are the words and so n= number of words in vocabulary=10k for our case.", "So for 10k x\u2019s, there will be 10k w\u2019s. Now for 1 node, there is a 10k length-weight matrix. For 300 combined we have a matrix of 300 x 10k weights. Now, if we concatenate, we will have 300 rows and 10k columns. Let\u2019s transpose the matrix. We will get 300 columns and 10k rows. Each row represents a word, and the 300 column values represent a 300 length-weight vector for that word.", "This weight vector is the obtained embedding of length 300.", "Num_words indicates the size of the vocabulary", "1) tokenize.fit_on_text() \u2192> Creates the vocabulary index based on word frequency. For example, if you had the phrase \u201cMy dog is different from your dog, my dog is prettier\u201d, word_index[\u201cdog\u201d] = 0, word_index[\u201cis\u201d] = 1 (\u2018dog\u2019 appears 3 times, \u2018is\u2019 appears 2 times)", "2) tokenize.text_to_sequence() \u2192> Transforms each text into a sequence of integers. Basically, if you had a sentence, it would assign an integer to each word from your sentence. You can access tokenizer.word_index() (returns a dictionary) to verify the assigned integer to your word.", "We are padding all sentences to a length of max length 100.", "Max pool layer is used to pick out the best-represented features to decrease sparsity.", "This model gives an accuracy of 77.4%", "We will be using Standford's Glove embedding which is trained over 6Billion words.", "It has four files each with a different embedding space, we will be using the 50d one, which is a 50-Dimensional Embedding space.", "So, let\u2019s see how to extract the embedding we require from the given embedding file.", "This is an extractor for the task, so we have the embeddings and the words in a line. So, we just compare the words to pick out the indices in our dataset. Take the vectors and place them in the embedding matrix at an index corresponding to the index of the word in our dataset.", "We have used an *emb because the embedding matrix is variant in size.", "So, we here have a feature set with a vocabulary of 10k words and each word represented by a 50 length tuple embedding which we obtained from the Glove embedding.", "This model gives an accuracy of 67% probably due to the decreased embedding size. The larger the embedding size more the information contained.", "Now, how these approaches are beneficial over to the bag of words model? As we can see these Bag of Words models just saw how a word behaves in the document, i.e, what can we tell about the frequency of occurrence of the word or any pattern in which the word occurs? While these approaches also take into consideration the relationship between two words using the embeddings.", "In mathematics (in particular, functional analysis) convolution is a mathematical operation on two functions (f and g) that produces a third function expressing how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it.", "So, Convolutional is best for extracting special features and behavior of feature values from the 2D pixels of images. Convolutional layers have a set of kernels which helps to extract several important features from the data samples. Now here, in case of text classifications, our feature matrices are 1Dimensional. So, here Conv1D is used. Basically it moves as a sliding window of size decided by the user. We have chosen 5.", "Now, initially after embedding, we get 100 Dimensional embeddings. Next using 1D convolutions we try to make our feature set smaller and let the feature set discover the best features relations for the classification. The max-pooling layer also helps to pick the features or words which have the best performance.", "The convolutional layer is always used after an embedding layer after it provides its embedded feature vectors.", "Here we have trained our own embedding matrix of dimension 100. We have used two convolutional layers with 64 and 128 kernel filters respectively. We have kept our sliding window =5.", "This model has given a test accuracy of 77%", "Until now we have tried to extract some features from all the words in a sample at a time. So, all of them are non-temporal approaches. Now, let\u2019s see how a person will judge a sentiment. He/she will not only consider what were the words used, but humans will also consider how they are used, that is, in what context, and what are the preceding and succeeding words? So, until now we have focused on what were the words used only, so, now let\u2019s look at the other part of the story.", "So, for this part, we need a Recurrent neural network to give a memory to our models. If we think about telling something about someone's statements, we will generally listen to the whole statement word by word and then make a comment. This is what the Recurrent Neural networks will accomplish. It will look at each word in a temporal manner one by one and try to correlate to the context using the embedded feature vector of the word.", "We know RNN suffers from the vanishing and exploding gradient problem we will be using LSTM.", "LSTM operates on two things a hidden state that is sent from a previous timestamp and a cell state that actually maintains the weight neutralizing the vanishing gradient effect.", "The LSTM layer basically has 4 components: A Forget gate, An input gate, a cell state, and an output gate.", "Now, let\u2019s talk a bit about the working and dataflow in an LSTM, as I think this will help to show how the feature vectors are actually formed and what it looks like.", "These are the set of equations LSTM operates upon.", "LSTM provides a feature set on the last timestamp for the dense layer, to use the feature set to produce results. We can see the above equations are the equations for the Gates of LSTM. Here, each gate acts as a neural network individually. So, they have their individual weight matrices that are optimized when the recurrent network model is trained. Using these weight matrices only the gates learn their tasks, like which data to forget and what part of the data is needed to be updated to the cell state. So, the gates optimize their weight matrices and decide the operations according to it.", "Say we have a 100-dimensional vector space. a batch size of 16, each sample length = 10. and the number of nodes in each layer= 64.", "INPUT SIZE = batch_size * Embedding so, here it is 16 x 100 matrix = x(t)", "The timestamp 0 that is the first word of every sample or record enters.", "The x0 represents the first word of the samples, x1 represents second, and so on. So, each time 1 word from 16 samples and each word is represented by a 100 length vector. So, the size of each input is (16 x 100).", "PREVIOUS HIDDEN STATE (0 vector for tiemstamp 0) = Batch size x Hidden Units So, Here it is 16 x 64 matrix.= h(t-1)", "First, the Forget Gate Weight matrix W{hf} of the hidden state is of dimension 64 x 64 because in the hidden state for each of the 16 words of timestamp (t-1) there were 64 values from each of the 64 nodes of the RNN.", "So, actually our matrix from the hidden state with shape (16 x 64):16 rows which are records, and for each record there are 64 columns or 64 features.", "where the x\u2019s are the features or the column values. So, there must a maintained array of 64 weights, one corresponding to each x, for each node or unit of the network. Now there are 64 such units so a total of (64 x 64) matrix.", "Again, now for the input vector of shape (16 x 100), there are 16 rows or records for each of the 100 columns or 100 features. so, the weight matrix of one hidden unit must have 100 values. A total of 64 units are there. So, the dimension of the weight matrix W{xf} is. (100 x 64)", "Sigmoid gives a value between 0 and 1. If the value is close to 0 the value is forgotten else added to the cell state after passing through F{t}.", "Now, the cell state is also of the same dimension (16 x 64) as it is also having the weights of the 16 sample word\u2019s by 64 nodes So, they can easily be added.", "Next, is the input or update gate it decides what part of the data should enter, which means the actual update function of the Recurrent Neural Network. It decides whether the cell state should be updated.", "These gate\u2019s weight matrices are also the same as the forget gate\u2019s matrices with (64 x 64) values in the last hidden layer and (100 x 64) values for the input.", "W{hu} is of dimension (64x 64) and W{xu} is of dimension (100 x 64). The Weight matrix with \u2018h\u2019 in subscript are multiplied with the h(t-1) portion and correspondingly weight matrices with \u2018x\u2019 in subscript are multiplied with the x(t) portion of the concatenated {x(t) +h(t-1)} vector obtained by adding the previously hidden layer with current input.", "Every weight matrix with h has dimension (64 x 64) and Every weight matrix with x has dimension (100 x 64).", "So, Input gate also gives sigmoid(16 x 64) as a result", "One thing to notice here is there is a tanh layer also. The tanh is here to squeeze the value between 1 to -1 to deal with the exploding and vanishing gradient. So, it basically works like and regularized value that represents the value of the temporary cell state on that timestep. The sigmoid is the switch.", "According to equation 3, the temporary cell state is calculated.", "The C1{t} and the U{t} vector matrices are of the same dimensions. So, dot product can be applied.", "So, after that, the obtained vectors are just multiplied to obtain 1 result.", "One thing to notice about this is, though the weight matrices are of the same dimensions, but, they are not the same. They belong to different gates and their values and optimizations are all different.", "Now in the next step, the cell step is updated", "It is basically a simple sum.", "The new c or cell state is formed by removing the unwanted information from the last step + accomplishments of the current time step. as shown according to equation 5.", "Next, comes the output gate. This decides what should be the next steps hidden layer be.", "For this, the new cell state is passed through a tanh gate and the h(t-1) + x(t) is passed through another sigmoid. Both of the results are multiplied. That\u2019s the next.", "According to equation 4, the output gate which decides the next hidden layer.", "So here we can see the dimension of O(t) and the h(t-1) matches.", "Now, these weights get updated at every timestep with every word, and after the 10th word or timestamp, the final timestamp in our case the model has gone through all the words in the samples, so we get a matrix of size 16 x 64, which is basically the weight values of the 64 internal nodes corresponding to each sample. But, what we don\u2019t see are the weight matrices of the gates which are also optimized. These 64 values in a row basically represent the weights of an individual sample in the batch produced by the 64 nodes, one by each .", "For all the samples we obtain a value. These values act as a feature set for the dense layers to perform their operations.", "Here, we have used a pre-trained word embedding. We have used 1 LSTM layer with 64 hidden unit nodes.", "This model provides an accuracy of 78%", "It is a hybrid model formed by combining an LSTM layer followed by a CNN layer.", "The LSTM layer is generating a new encoding for the original input. The output of the LSTM layer is then fed into a convolution layer which we expect will extract local features. Finally, the convolution layer\u2019s output will be pooled to a smaller dimension and ultimately outputted as either a positive or negative label.", "We have talked about a number of approaches that can be used for text classification problems like the sentimental analysis.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Computer Science and Technology Graduate from NIT, Durgapur. Find Me at https://abhijitroy1998.wixsite.com/abhijitcv"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2ab021796317&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab021796317--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab021796317--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----2ab021796317--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----2ab021796317--------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----2ab021796317---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ab021796317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ab021796317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tengyart?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Tengyart"}, {"url": "https://unsplash.com/s/photos/emotion?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/", "anchor_text": "Source"}, {"url": "https://github.com/abr-98/text_classification_all_network", "anchor_text": "here"}, {"url": "https://medium.com/tag/sentiment-analysis?source=post_page-----2ab021796317---------------sentiment_analysis-----------------", "anchor_text": "Sentiment Analysis"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----2ab021796317---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/tag/lstm?source=post_page-----2ab021796317---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ab021796317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----2ab021796317---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ab021796317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----2ab021796317---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ab021796317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab021796317--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2ab021796317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2ab021796317---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ab021796317--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2ab021796317--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2ab021796317--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2ab021796317--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2ab021796317--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2ab021796317--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2ab021796317--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2ab021796317--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/@myac.abhijit/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "458 Followers"}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ba8066c30a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-text-classification-and-sentiment-analysis-2ab021796317&newsletterV3=4c235a4f4b95&newsletterV3Id=2ba8066c30a7&user=Abhijit+Roy&userId=4c235a4f4b95&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}