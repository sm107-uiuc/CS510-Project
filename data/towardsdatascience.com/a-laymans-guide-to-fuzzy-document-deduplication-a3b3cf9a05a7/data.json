{"url": "https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7", "time": 1683002952.2355928, "path": "towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7/", "webpage": {"metadata": {"title": "A Layman\u2019s Guide to Fuzzy Document Deduplication | by Ryan Basques | Towards Data Science", "h1": "A Layman\u2019s Guide to Fuzzy Document Deduplication", "description": "Fully-explained NLP concepts for eradicating near duplicate documents from your unstructured data, including visuals and Python code sample."}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/avito-duplicate-ads-detection/data", "anchor_text": "Kaggle competition", "paragraph_index": 9}], "all_paragraphs": ["In our era of exponentially growing data, complex projects, large teams, and a desire to move on to the next deadline, small things often fall through the cracks.", "Our team of data analysts saw this at a large law firm, where important documents were often shared, edited, shared again, and stored away on a dedicated document server. Twenty years and a million documents later, the server was a labyrinth of nested folders, unintelligible filenames, and corrupted files.", "Our goal was to sift through the mass of storage for important documents, but we had a problem to solve first.", "We proposed a new, clean location for very important documents. However this required documents to be searchable, relevant, and notably, unique. From a quick peek at the documents, we could see that there were many duplicates in the data.", "We couldn\u2019t rely on file names, dates or any other unique identifier. All that we had was the content in the text documents themselves.", "Why is this problem more complicated than writing a simple \u201cexactly equal\u201d script that looks for documents where the texts are completely identical? The wrinkle is that in practice, searching for 100% similarity misses many duplicate documents. The reasons are varied, but include:", "By allowing for a \u201cfuzzy\u201d deduplication where the script identifies mostly similar documents, we can sidestep any issues that might cause two otherwise duplicate documents to fall out of step with each other like stray punctuation, capitalization, whitespace, etc. As you will see, we will have a lever to adjust just how similar a document has to be to be classified as a duplicate.", "While this use-case is quite specific, there are many situations when mass deduplication may be desirable, including:", "First, let\u2019s look at the solution from a theoretical perspective. Afterward, we can show a brief implementation in Python for those interested in the code behind the theory.", "For illustration, we\u2019ll consider a real-life use case. We\u2019ll use a dataset from a Kaggle competition where the goal is to detect duplicate advertisements in an online marketplace. For this guide, we\u2019ll focus on the texts of the advertisements.", "Folks submitting ads often make tweaks to their existing ad and re-submit it to have a higher chance of people finding their product.", "A quick trip to Craigslist shows us an example of this right away:", "See how some of these are worded slightly differently? An exact match system would have never caught these.", "We want to reduce the clutter for users by flagging and removing duplicate ads (even if they are not the same word-for-word).", "Some basic information about the dataset:", "Let\u2019s take a look at the top most duplicated advertisements in our dataset, using the \u201cexactly equal\u201d script:", "We can see right away some of the downside of strict duplicate detection. Notice how \u201cin a good condition\u201d is broken out into three separate categories because of capitalization or punctuation. Our simple script erroneously classifies these as completely independent, original texts.", "Another frequent pattern in the data is more suspicious \u2014 much longer, specific ads with slightly different wording:", "Below we will develop a more sophisticated solution to detect near duplicates in addition to exact duplicates.", "Before we dive further into this case, let\u2019s talk about the concepts that will support our endeavor.", "To deduplicate these redundant ads in a \u201cfuzzier\u201d way, we\u2019ll need to find a way to turn text into numbers to analyze similarity quantitatively.", "There are a few different methods, but one of the most popular and effective ways to measure \u201cstring similarity\u201d, or the sameness of text, is called cosine similarity.", "The logic behind it is quite straightforward. If we can convert text into a vector (line on a graph), we can compare it numerically to other vectors (more lines on the same graph). The placement of the line depends on the contents of the document.", "For example, consider the following three phrases:", "Intuitively, we can see some similarity among these phrases, or documents. Two of them are buttery, two of them are popcorn. However, in order to scale to comparing millions of documents, we need to make the problem understandable by a machine.", "Let\u2019s take a simple count of each time a word occurs in each of the phrases. The result will look something like below:", "Each row is what we call a document vector, representing each respective document quantitatively.", "We\u2019ve only included two features (columns) here, although you could have as many features as you have unique words in your corpus (entire collection of documents). We could add a third dimension for croissant, for example.", "Because we only have buttery and popcorn, we can graph this dataset in 2D to illustrate how cosine similarity works:", "The y-axis measures how often we see \u201cbuttery\u201d, while the x-axis measures how often we see \u201cpopcorn\u201d. We can see our three documents plotted with blue dots.", "The green text indicates our cosine similarity calculation. We can see that buttery croissant and salty popcorn have a very large angle sitting between them of 90 degrees. Buttery popcorn and salty popcorn have a smaller angle of only 45 degrees.", "We could stop here, but taking a cosine of these angles will normalize them for us, to 0.0 and 0.7 respectively. Handily, a cosine calculation will always have a minimum of -1 (for 180 degrees), and a maximum of 1 (for 0 degrees).", "We\u2019ve successfully quantified how different these pieces of text are with two features. Considering that our documents contain hundreds or thousands of unique words, we will have just as many features. It would be impossible to visualize this many document vectors on a graph like above, but rest assured that the logic is exactly the same in thousands of dimensions. The computer will handle it for us.", "However, there\u2019s still one wrinkle. If we\u2019re just counting the words in a document, our method doesn\u2019t take into account word order. For example, how would our cosine similarity method above account for the difference between:", "Not very well, because they contain the exact same words, just in a different order. We can see that these phrases have very different meanings, but our cosine similarity would simply see that they contain all the same words, for a score of 100% similar!", "An n-gram simply describes a sequence of n items from a given sample. In text, this may be a sequence of n words from a given sentence. So, let\u2019s say you want all 2-grams (or bigrams) from the document \u201cgood condition, not negotiable\u201d \u2014 the following are the three bigrams you\u2019d receive:", "Similarly, if you want 3-grams (or trigrams), you\u2019d receive:", "Can you see why having these additional features might be useful?", "Instead of creating a document vector on only single words, we create it on all bigrams and trigrams, too! Now \u201cgood condition, not negotiable\u201d and \u201cnot good condition, negotiable\u201d will justifiably receive a lower similarity score since their bigrams and trigrams do not line up.", "One question you may ask is \u2014 why go through the effort of measuring the angles between document vectors? We can see from the graph above that you can plot a point for each document \u2014 why not just look at the distance between the dots (Euclidean distance)?", "It\u2019s a great question. To answer it, let\u2019s check out the below graph:", "Now we\u2019ve added magnitude to the documents, where some words occur multiple times. We have one document that is very buttery (x4), one that is buttery and popcorn (x2, x1) and one that is just popcorn (x1).", "While intuition would tell us that the second document is closer to the first since they both overload on butter, the Euclidean distance A (2.2) is actually larger than Euclidean distance B (2.0). Using these distances, we would say the documents sharing one \u201cpopcorn\u201d term are more similar than the documents sharing two \u201cbuttery\u201d terms!", "However, angles A and B (measured with cosine similarity) support our intuition that the buttery documents are more similar than the popcorn documents.", "In fact, when magnitude is normalized (all dots are equal length from the origin of the graph), Euclidean distance and cosine similarity will return the same results.", "One additional benefit of cosine similarity is that it is less expensive for a computer to calculate, which can result in some serious time savings when we\u2019re talking about millions or billions of comparisons.", "The field of natural language processing is deep and wide, and there are many additional ways to increase the quality of your data through preprocessing, depending on your use-case.", "For example, stemming will normalize all words to their most basic form (running becomes run, delivered becomes deliver).", "Another transformation takes the document vectors we\u2019ve created, called bag-of-words vectors (because we\u2019re simply counting how many words are in each document), and converting it into term frequency \u2014 inverse document frequency vectors (TF-IDF). This is a very powerful transformation that can help you in tasks like document classification. You discount words that occur frequently across your entire corpus (the, he, her, it, etc.) and focus on rarer words that occur in specific documents but not across the entire corpus, because these latter words are much more likely to be meaningful.", "These transformations are worth knowing about, but we won\u2019t be using them here because duplicates will generally use the same word forms (no need for stemming) and term frequency (no need to specially weight different terms/documents).", "Finally, if you\u2019re on the cutting edge of the field, you\u2019ll look into using document embeddings, which is another method of mapping textual meaning into a vector of numbers. This is outside the scope of this article (and may be overkill for our deduplication application).", "Nicely done! You now have a grasp on a very useful tool. Let\u2019s see how we can apply it to our earlier duplicate-ad-detection example.", "The first step is to create a document-term matrix. You\u2019ve already seen one of these earlier with the popcorn example \u2014 it\u2019s simply a structure that keeps track of documents on the rows, and specific terms (note: no longer \u201cwords\u201d since we could also have n-grams thrown in) on the columns. Each time a term occurs in a document, we\u2019ll increment the appropriate cell in the matrix by one.", "Let\u2019s see what this matrix looks like for a few examples in our data. I\u2019ve translated the Russian examples to English for better illustration, although the method applies in any language.", "Orange cells represent an intersection between our documents and our terms. We can see that multiple documents contain the same term \u2014 like condition belonging to four documents. This will contribute to a higher cosine similarity score between these documents.", "Now that we have transformed our textual data into document vectors, we can apply a pairwise cosine calculation to evaluate the similarity between all of the documents. Pairwise just means that we\u2019ll receive a number for every two examples in our dataset \u2014 every document needs to be compared with every other document to see if there are any similarities.", "Note that this can become computationally expensive \u2014 in the technical section we\u2019ll discuss a great package that saves on resources while performing this calculation.", "Let\u2019s see what a cosine similarity calculation on the above examples provides us:", "Each document is placed along the rows and the columns, with the pairwise cosine similarities filling in the cells.", "We see 100% similarity along the diagonal as we would expect \u2014 all documents are identical to themselves. For example, \u201c14.5 cm in excellent condition\u201d is in the first row and the first column, resulting in an orange 100% in the first cell. Of course, when we perform our analysis, we will not count these self-similar cells as \u201cduplicates\u201d in our dataset.", "Right away, we see some interesting results:", "We might argue that the first match is not a case of duplication, whereas the second is a case of duplication. We can start to see an intuition for forming a duplication cutoff between 71% and 100% \u2014 but more on choosing this threshold later.", "Note that I included the original Russian text in the graphic as well. This helps explain why \u201c14.5 cm in excellent condition\u201d and \u201cin good condition\u201d did not receive any similarity. In Russian, the two condition terms are spelled slightly differently (one meaning \u201ccondition\u201d, the other technically meaning \u201cstate\u201d), resulting in completely different/independent meanings behind the term.", "These results are from a small sample of 6 documents to provide us with some intuition. Let\u2019s see what the results look like on our dataset of 100,000 documents.", "For comparison, let\u2019s see the findings side-by-side with the results from doing an exact/strict deduplication criteria earlier in the article:", "Our fuzzy deduplication found 2,244 duplicate documents, or about 2% of the total dataset. When accounting for the bloating effect of multiple copies of these duplicate ads, these duplicates account for 7.5% of our data!", "By allowing fuzzy deduplication, we\u2019ve found twice as many duplicate documents as before.", "We\u2019ll looks at three examples of the fuzzy findings that we\u2019re now picking up.", "In this first example, we pick up variations on \u201cgreat condition\u201d. While interesting, this doesn\u2019t strike as an example of duplicitous advertising \u2014 it\u2019s just a common phrase for folks selling items!", "This second example represents a pattern of many ads seen throughout the dataset. While the word orders are different, the contents are the same! We can be confident that this is a duplicitous advertisement selling the same pair of sneakers.", "As our final example, we see dozens of advertisements for trampolines, where the only variation is the dimensions of the trampoline itself. A user of this website who is not interested in trampolines certainly doesn\u2019t need to see such variety. As the website owner you may choose to take action on this type of redundancy.", "Other examples fitting this pattern are duplicate ads where the only difference is in the address, phone number, or product color.", "Now that we\u2019ve fully developed a cosine similarity deduplication method and have seen some results, we\u2019re almost finished with the theory part of this article. There is one very important outstanding item to discuss.", "One important area for consideration is the cosine similarity threshold we choose. Remember, when we compare two document vectors, we\u2019ll get a cosine similarity between 0% and 100%. It\u2019s up to us to decide whether the result passes or fails a predefined set-point.", "A 0% cosine similarity cutoff would flag everything as duplicative of everything else, and a 100% cutoff would require exact duplicates (non-fuzzy) for a duplicate flag to be thrown.", "There is no universally \u201ccorrect\u201d answer, since the cutoff we set will depend on our specific use-case.", "On one extreme, let\u2019s say you are deduplicating old manuscripts that are rare and very valuable. You\u2019d be in trouble for deleting even one original manuscript because you thought it was a duplicate of a very similar but different manuscript. If some duplicates do slip by and stay in the dataset, it\u2019s not as big of a deal. Here, you\u2019d err toward a higher deduplication cutoff that is stricter on detecting duplicates.", "On the other extreme, you are deduplicating millions of tweets as a preprocessing step for a larger project. Each tweet in itself doesn\u2019t matter much, and your main goal is shrinking the data. You want to get rid of bulky redundant tweets by spambots, for example. It\u2019s perfectly fine if you accidentally remove original tweets \u2014 you\u2019re just trying to shrink the size of your corpus by removing low-hanging fruit.", "Here is where the concepts of precision and recall become helpful \u2014 these are the opposing forces that will help you set a good deduplication cutoff.", "Precision asks \u2014 of the \u201cpositive\u201d documents we\u2019ve flagged (in our case, positive is synonymous with duplicate), how many were true positive/duplicate documents?", "Keep in mind that if we flag a document as \u201cpositive\u201d/duplicate, but it is not a true \u201cpositive\u201d/duplicate, we just erroneously flagged an original document as a duplicate! Depending on our use-case, this may mean that we\u2019ve cued an original document for deletion.", "In the former rare manuscript example, we\u2019re optimizing for precision. We want to make sure that all of the documents we\u2019ve flagged as duplicates are truly duplicates. Nobody would blame us for having a 99% deduplication cutoff to be safe.", "On the other hand, recall asks \u2014 of all the true positive/duplicate documents out there, how many did we correctly identify as duplicates?", "How is this different than precision? In precision, we care about how many true positives we got vs false positives. In recall, we care about how many true positives we got vs false negatives.", "In the tweet deduplication example, we care more about deduplicating the entire universe of tweets much more than we do making sure that each tweet we flag is truly a duplicate. We\u2019re willing to sacrifice precision for recall. Because of this trade-off, we\u2019re OK with a lower cosine similarity cutoff of say, 70%.", "Let\u2019s say we have a dataset of 100 documents. We\u2019ve manually looked at the data, and know that there are 20 duplicate documents that should be deleted.", "At a cosine similarity cutoff of 70%, we delete 30 documents. We certainly deleted the 20 true duplicates, for a very good recall score (20/20\u2013100%). However, we also deleted 10 documents that were not truly duplicates. This lends to a poor precision score (20/30\u201366%).", "See how precision and recall oppose each other? In practice, it makes sense to break out a sample of your dataset, label originals and duplicates, and try to optimize the cutoff threshold for your specific situation.", "Additionally, cosine similarity could be just one feature of a larger statistical model. In the advertisement deduplication example, it could make sense to include information about the product images, usernames, posting timestamp, and so on, ultimately feeding into a binary classification model for flagging duplicate ads.", "For illustration purposes, we used a 95% similarity cutoff. Relatively strict, but as we saw above, we still gain significant performance in fuzzy deduplication.", "That\u2019s it for the theory! Thank you if you\u2019ve stuck around this long \u2014 you now have a practical familiarity with a valuable method to help you tame duplicative data.", "If you have a technical bent and want to know how to code something like the above examples, we\u2019ll continue on with some Python code-snippets that can perform the methodology we just discussed.", "Gensim is a Python library popularly used for topic modeling. However it also has very valuable utilities for deduplication.", "While there are several efficient ways to calculate cosine similarity in Python, including use of the popular SKLearn library, Gensim\u2019s major advantage comes when your dataset grows very large.", "When your corpus grows beyond your computer\u2019s ability to store data in-memory, Gensim automatically shards (splits) your corpus into computer-readable chunks and saves it to disk. By placing your data on disk, you can work with much larger datasets before running into memory issues. You can also load and re-use the shards stored on your disk.", "Relatedly, once you generate a similarity index (which we\u2019ll see below), you can save it to disk, and incrementally add more documents to it instead of recalculating from scratch. This allows for a one-time time-intensive lift to load your existing corpus, with very fast updates whenever you add new data to your corpus and want to recalculate similarities.", "First, we import the modules we\u2019ll need from Gensim. Note that to run the code yourself, you\u2019ll need to have downloaded Gensim with pip or conda.", "Let\u2019s define some test data to work with.", "In general, we work with the documents at word-level. Here\u2019s a simple way to break documents into the correct format:", "simple_preprocess is a helpful Gensim function for parsing a document into individual words. You can pass it additional arguments depending on what you need. Here, we pass \u201cdeacc=True\u201d to strip our punctuations, accents and numbers.", "Here is \u201cUsed SpaceX rocket as-is, buyer must transport.\u201d after simple_preprocess:", "We\u2019ll use the word-level structures to train a Gensim n-gram phraser.", "Whereas a normal bigram generator would produce a bigram for each word pair (used_spacex, spacex_rocket, rocket_as), the Gensim phraser is pretrained to select collocations, or words that would commonly be found together. For example, only spacex_rocket would be kept because the phraser deems it a likely collocation of words.", "This selectivity is a great way to save on program runtime by not storing a feature for every possible permutation of words.", "The min_count argument specifies how many times the phrase must be found in the corpus to be kept. We\u2019re keeping it at 1 since our dataset is so small.", "Now that we have a trained Gensim Phraser, we\u2019ll use it during conversion of sentences into terms. This looks the same as before, except we\u2019re including the bigram_phraser variable.", "For example, \u201cSpaghetti noodles for sale \u2014 100lbs bulk pack\u201d now becomes:", "Gensim uses package-specific structures to run more efficiently. Specifically, we need to create a Gensim dictionary and Gensim corpus.", "The Gensim dictionary simply maps each specific term (key) to a unique ID (value). A normal Python dictionary representation looks like:", "The Gensim corpus uses the dictionary we just created to convert our original text documents to numeric representation. By replacing text with numbers, the computation becomes much faster.", "For example, the first document in our data \u2014 \u201cUsed SpaceX rocket as-is, buyer must transport.\u201d \u2014 now looks like:", "Each set of parenthesis (tuples) represents a word. The first number is where the word lies in our dictionary, and the second number is how many times it occurs in the document.", "For (0, 1), 0 represents our bigram \u201cas_is\u201d, which occurs once in the document.", "These numbers really just represent a Bag of Words \u2014 a term we used earlier meaning the deconstructed, independent collection of terms from a document.", "Each one of these tuples represents one dimension in our document-term vector.", "We\u2019re ready to calculate cosine similarity for our collection of document-term vectors!", "Gensim does all the heavy lifting for us, all we have to use is the right syntax:", "This instantiates the similarity index, which computes and tracks the cosine similarity of each document against all the others \u2014 a pairwise calculation. Gensim is optimized to perform this calculation relatively quickly. The output_prefix argument is simply the name of the file that will be saved to your hard disk \u2014 this can be loaded back later if needed.", "Let\u2019s break open the index and see what\u2019s inside:", "similar_docs is now a Python dictionary that tracks each document, and the similarity scores between it and all other documents in the corpus.", "For example, the first entry looks like:", "So the first document (\u201c0\u201d \u2014 Python is zero-indexed) in our collection (\u201cUsed SpaceX rocket as-is, buyer must transport.\u201d) is exactly the same as itself, and exactly the same as the second document.", "We\u2019ll use a threshold of 90% similar. Depending on the use-case, we could move this up or down, as discussed above.", "The below helper code will return to us the pairs of documents that exceed our threshold:", "There we have it. We were able to validate that the first pair of documents are exact duplicates, and the second are fuzzy duplicates. Feel free to use this test code to play with some of your own data.", "That\u2019s it for this guide! You now are familiar with valuable text analytics techniques, and are ready to implement a sophisticated method for fuzzy deduplication on your next project.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Working in data analytics, studying philosophy. Striving to find a better path in both."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa3b3cf9a05a7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ryanbasques?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ryanbasques?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "Ryan Basques"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F36ee3faf569f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&user=Ryan+Basques&userId=36ee3faf569f&source=post_page-36ee3faf569f----a3b3cf9a05a7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa3b3cf9a05a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa3b3cf9a05a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/photos/stormtrooper-star-wars-lego-storm-1343772/", "anchor_text": "Pixabay"}, {"url": "https://www.kaggle.com/c/avito-duplicate-ads-detection/data", "anchor_text": "Kaggle competition"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a3b3cf9a05a7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----a3b3cf9a05a7---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/nlp?source=post_page-----a3b3cf9a05a7---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/python?source=post_page-----a3b3cf9a05a7---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----a3b3cf9a05a7---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa3b3cf9a05a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&user=Ryan+Basques&userId=36ee3faf569f&source=-----a3b3cf9a05a7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa3b3cf9a05a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&user=Ryan+Basques&userId=36ee3faf569f&source=-----a3b3cf9a05a7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa3b3cf9a05a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa3b3cf9a05a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a3b3cf9a05a7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a3b3cf9a05a7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ryanbasques?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ryanbasques?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ryan Basques"}, {"url": "https://medium.com/@ryanbasques/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "132 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F36ee3faf569f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&user=Ryan+Basques&userId=36ee3faf569f&source=post_page-36ee3faf569f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3fc603bcfd0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7&newsletterV3=36ee3faf569f&newsletterV3Id=3fc603bcfd0&user=Ryan+Basques&userId=36ee3faf569f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}