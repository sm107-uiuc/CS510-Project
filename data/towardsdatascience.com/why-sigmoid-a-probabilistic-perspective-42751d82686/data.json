{"url": "https://towardsdatascience.com/why-sigmoid-a-probabilistic-perspective-42751d82686", "time": 1683006356.08964, "path": "towardsdatascience.com/why-sigmoid-a-probabilistic-perspective-42751d82686/", "webpage": {"metadata": {"title": "Why Sigmoid: A Probabilistic Perspective | by Logan Yang | Towards Data Science", "h1": "Why Sigmoid: A Probabilistic Perspective", "description": "If you have taken any machine learning courses before, you must have come across logistic regression at some point. There is this sigmoid function that links the linear predictor to the final\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Logistic_distribution", "anchor_text": "logistic distribution", "paragraph_index": 35}, {"url": "https://www.youtube.com/watch?v=X-ix97pw0xY", "anchor_text": "MIT 18.650 Statistics for Applications lectures by Philippe Rigollet", "paragraph_index": 63}, {"url": "https://medium.com/@loganyang", "anchor_text": "Medium", "paragraph_index": 64}, {"url": "https://twitter.com/logancyang", "anchor_text": "Twitter", "paragraph_index": 64}, {"url": "https://twitter.com/logancyang", "anchor_text": "https://twitter.com/logancyang", "paragraph_index": 66}], "all_paragraphs": ["If you have taken any machine learning courses before, you must have come across logistic regression at some point. There is this sigmoid function that links the linear predictor to the final prediction.", "Depending on the course you took, this sigmoid function is often pulled out of thin air and introduced as the function that maps the number line to the desired range [0, 1]. There is an infinite number of functions that could do this mapping, why this one?", "One critical point to focus on is that the output of the sigmoid is interpreted as a probability. It\u2019s obvious that not any number between 0 and 1 can be interpreted as a probability. The interpretation must come from the model formulation and the set of assumptions that come with it.", "If you don\u2019t want to read the full article, you can watch the video version here:", "This question of \u201cwhy sigmoid\u201d used to bug me for a long time. Many answers online are not to the point. The kind of answers I found most frequently mentioned the keywords \u201clogit\u201d and \u201clog odds\u201d and simply transformed the sigmoid to its inverse, which not only explains nothing about why we chose the log odds as the thing our linear predictor aims for, it also says nothing about the implications such a choice has. Some better ones mentioned \u201cgeneralized linear models\u201d, but they share the same weakness as introductory classes where concepts are mentioned but the inner connections that really answer the \u201cwhy\u201d aren\u2019t there. The real answer should help you get to the point where you can design this algorithm from scratch without knowing anything about it beforehand. When you face a binary classification problem with only basic probability and statistics knowledge, you should be able to think \u201cokay, one of the most logical ways to tackle this problem is to follow this exact model design\u201d.", "In this post, I will try my best to arrange the flow of logic in an easy to read manner so it becomes clear that the sigmoid is a natural design choice for probabilistic binary classification with some important assumptions. To tell it like a story, the logic is not necessarily nice and linear, some points may appear to be parallel but they all contribute to the design motivation of the logistic model. So if you care about this topic, sit back and bear with me for a moment. This is going to be a long post with an amount of information comparable to an entire chapter in a machine learning book.", "The reason for mentioning linear regression here is to see how we can look at it as a probabilistic model of the data and whether we can apply similar ideas on classification.", "We assume our target variable y and the inputs x are related via (superscript i is the index of the data point)", "where epsilon is an error term that captures either unmodeled effects or random noise. We assume the noise comes from different sources and is not correlated, so it should be Gaussian based on Central Limit Theorem. We can write out the distribution and express the error as the difference between the target and the linear predictor,", "We call this the distribution of y given x parametrized by \u03b8. We are not conditioning on \u03b8 because it\u2019s not a random variable, it is the parameter to learn. Next, we define the likelihood as", "The likelihood is a function of \u03b8. When viewed as a function of y and X with a fixed \u03b8, it is just the probability density function. But when viewed as a function of \u03b8, it means that by varying \u03b8 we can \u201cfit\u201d a distribution to the data observed. The process of finding that best fit is called maximum likelihood estimation (MLE). In other words, MLE is the attempt to find the distribution that maximizes the probability of observing the data, with the assumption of the type of distribution (in this case a Gaussian) and parameters (in this case, \u03b8, notice we only care about the mean and not the variance/covariance matrix here). We further write it out as a product for individual data points in the following form because we assume independent observations,", "Since the log transformation is monotonic, we use the log-likelihood below for the optimization of MLE.", "To find the best Gaussian that describes the true underlying model which generates our data, in other words, the best \u03b8, we need to find the peak that gives us the maximum log-likelihood. Maximizing the expression above is equivalent to minimizing the term below,", "Now we see the magic: this is exactly least-squares!", "In short, why does linear regression fit the data using least-squares?", "Because it tries to find the best model in the form of a linear predictor plus a Gaussian noise term that maximizes the probability of drawing our data from it.", "The probabilistic formulation of linear regression is not only an inspiring example for our formulation of logistic regression later, but it also shows what a proper justification for model design looks like. We mapped a linear predictor with Gaussian noise to the target variable. For binary classification, it would be nice if we can do something similar, i.e. map a linear predictor with something to the probability of being in one of the two classes (the posterior p(y=1|x)), and use MLE to justify the model design by saying it\u2019s maximizing the probability of drawing the observed data out of our parametrized distribution. I will show how to do that in section 3, but next, let\u2019s look at a motivating example.", "Let\u2019s consider a binary classification task on 1D data where we already know the underlying generative distribution for the two classes: Gaussians with the same variance 1 and different means 3 and 5. Both Gaussians have 50k data points, i.e. equal priors, p(C0) = 0.5 = p(C1). (Ck represents the class of y)", "Since we only have 1 dimension in the data, the best we can do is to draw a vertical boundary somewhere that separates the two classes as much as it can. It\u2019s visually obvious that the boundary should be around 4. Using a generative approach where we know the class conditionals p(X|Ck), which are the two Gaussians, and the priors p(Ck), we can use Bayes rule to get the posterior", "We can clearly see the boundary in the posterior, i.e. the final probability prediction of our algorithm. The red region is classified as class 0, the blue region is class 1. This approach is a generative model called Gaussian Discriminant Analysis (GDA). It models continuous features. You may have heard of its sibling for discrete features: the Naive Bayes classifier.", "Now look at the S shape of the posterior around the boundary, it describes the transition of uncertainty between the two classes.", "Wouldn\u2019t it be cool if we can model this S shape directly without knowing the class conditionals beforehand?", "But how? Let\u2019s work through some math.", "Notice that the red and blue curves are symmetric, and they always sum to 1 because they are normalized in Bayes theorem. Let\u2019s look at the red one. It\u2019s simply p(C0|X) which is a function of X. We massage the previous equation a bit by dividing the top and bottom with the top to the following form,", "For the bottom right term, we can cancel the priors because they are equal, and plug the Gaussians in for the class conditionals.", "Okay, this is nice! We have a linear function of x inside the exp(), if we set z = -2x + 8, write it out for the posterior, it becomes,", "This is the logistic sigmoid function! If you ask why we have that negative sign for z, it\u2019s because we want p and z to be monotonic in the same direction for convenience, meaning increasing z will increase p. The inverse of this is called the log odds or logit, which is the part that we can use a linear function to model.", "Looking back at the flow of logic above, what really happened that made it possible to have a sigmoid form posterior and a linear function of x for z? That will give us some insights to decide when we can model the classification this way.", "For the sigmoid form, you see that it came naturally from the Bayes rule for two classes, i.e. a Bernoulli distribution of the target variable. It doesn\u2019t require the class conditionals to be Gaussians! There can be a family of distributions that have a similar exponential form, fitting into the same derivation we came through above! As long as the outcome y is binary, the input X can have some flexibility in their class conditional distribution.", "Next, the linear form of z. In this example, we had two Gaussians with the same variance and prior. These are the facts that let us cancel out the priors and the quadratic term of X in the derivation. This requirement looks quite strict. Indeed, if we change the shape of our Gaussians, the decision boundary can no longer be a straight line. Consider the 2D examples below. If the two Gaussians have the same covariance matrix, the decision boundary is linear; in the second graph they have different covariance matrices, the decision boundary is parabolic.", "What this tells us is that if we model the posterior directly (the discriminative approach) with the sigmoid function and a linear boundary which is also known as logistic regression, it has some pros and cons compared to the generative approach of GDA.", "There is an extensive comparison for GDA and logistic regression in section 8.6.1 of Machine Learning: a Probabilistic Perspective by Kevin Murphy. I discussed GDA here only to show that", "The sigmoid function can arise naturally when we try to model a Bernoulli target variable along with some assumptions.", "Now coming back to the thread of point 1. We designed linear regression by defining the linear predictor with a Gaussian noise term. Can we do something similar in the case of binary classification? Yes, we can! Let\u2019s look at it this way,", "The linear predictor plus the error here evaluates to what we call a latent variable because it is unobserved and computed from the observed variable x. The binary outcome is determined by whether the latent variable exceeds a threshold, 0 in this case. (Note that the decision threshold is set to 0 and not 0.5 as usual for the convenience of the cumulative distribution interpretation later. Mathematically, it doesn\u2019t matter if it\u2019s 0 or 0.5 here because the linear predictor can update a bias term to compensate.)", "If we assume the error term has a logistic distribution, whose cumulative distribution is the logistic sigmoid function (shown side by side below), we then get the logistic regression model!", "Denote the latent random variable as Y*, the linear predictor as z, the cumulative distribution as F, then the probability of observing outcome y = 1 is,", "We made F the sigmoid function so it is symmetric around 0,", "Now we reached the goal where the probability of our Bernoulli outcome is expressed as the sigmoid of the linear predictor!", "The above gives us the relationship between the linear predictor z and the prediction p. The function F, or the activation function in the context of machine learning, is the logistic sigmoid. The inverse of the activation function is called the link function which maps the prediction back to z. It is the logit in logistic regression.", "To recap, the derivation is essentially saying that if we assume the error term to have a logistic distribution, the probability of our Bernoulli outcome is the sigmoid of a linear predictor.", "If you look at the derivation closely, this formulation doesn\u2019t require a logistic distribution to work. It just requires a symmetric distribution around 0. What is a reasonable alternative? A Gaussian!", "What if we assume the error to be Gaussian?", "It actually gives us another model that works similarly as logistic regression and also does the job. It is called Probit Regression.", "Comparing with an alternative model that is designed to solve the same task is a great way to gain insight into our subject: logistic regression and its assumptions.", "As the previous section mentioned, the probit model for binary classification can be formulated with the same latent variable formulation but with Gaussian error. You may wonder why it\u2019s not as widely used as logistic regression since it seems more natural to assume Gaussian error. One reason is that the Gaussian distribution does not have a closed-form CDF and its derivative is harder to compute during training. The logistic distribution has a very similar shape as Gaussian but its CDF, aka the logistic sigmoid, has a closed-form and easy-to-compute derivative.", "\u03a6 is the CDF of Gaussian. Notice we divided by \u03c3 to obtain a standard normal variate and used the symmetry to obtain the last result. This shows that we can\u2019t identify \u03b8 and \u03c3 separately because p depends only on their ratio. It means the scale of the latent variable is not identified. Hence, we set \u03c3 = 1 and interpret \u03b8\u2019s in units of standard deviations of the latent variable.", "The only difference between the derivation above and the one for logistic regression is that the activation function is set as the Gaussian CDF rather than the logistic sigmoid, i.e. the logistic distribution\u2019s CDF. The inverse of the Gaussian CDF is called the probit and it is used as the link function here.", "Probit regression is used more in biological and social sciences as a convention. It generally produces similar results as logistic regression and is harder to compute. If you are not a statistician specialized in this area, logistic regression is the go-to model.", "There is another link function called the complementary log-log that can be used for the Bernoulli response, I won\u2019t go into details here but you can read about it if you are interested.", "We have seen linear, logistic, and probit regressions so far. One of their main differences is the link function. If we abstract that out and make some additional assumptions, we can define a broader class of models called Generalized Linear Models.", "A GLM models the expected value of p(y|x), i.e. \u03bc = E[y|x; \u03b8]. For linear regression, \u03bc is just the linear predictor, in other words, its link function is the identity function. But for other cases, p(y|x) can be of an exponential form or some other form, if we still want to use a linear predictor somehow, we have to transform it to match the output.", "To make the leap to GLM, we first take advantage of a nice mathematical form that groups some of the most widely-used distributions together so we can study their shared properties. Instead of looking at each distribution with their own parameters, we can look at a shared form as shown below,", "Distributions that can be massaged into this form are called the Exponential Family (note it is not the same as the exponential distribution). Here, y is the target response variable we are trying to predict. Statisticians developed some fancy names for these terms. But what I\u2019m focusing on here is the term \u03b7, also called the natural parameter. For our purpose, we can assume T(y) (called sufficient statistics) is just y. Hence, the natural parameter \u03b7 is just mapping the outcome y in that exp() to the probability on the left. Let\u2019s use a concrete example to show what I mean.", "For a Bernoulli target variable with mean \u03bc, we can write", "The natural parameter \u03b7 turned out to be the logit!", "The logit is also called the canonical link function for the Bernoulli distribution because of this formulation of the exponential family.", "As we have seen before, the probit is also a link function, but it is not canonical because it doesn\u2019t fall into the exponential family setting here.", "Now we are equipped to leap over to GLM. With the exponential family and its natural parameter, we can define a canonical link function for our linear predictor according to the distribution of the outcome y. In the case of a Bernoulli outcome, this approach gives us the logit link and logistic regression.", "The exponential family gives us a lot of nice properties. It is shown that their log-likelihood is always concave (equivalently, the negative log-likelihood is always convex), and their gradient-based optimization shares the same form so we can always use some iterative algorithm to find the best fit.", "Besides Bernoulli, some other famous distributions in the exponential family include Gaussian, Poisson, Gamma, the exponential distribution, Beta, and Dirichlet.", "To pick the GLM for your machine learning task, consider the type of your target variable y. For example,", "In introductory classes and books, solutions are often imposed on the readers without full justifications. Finding leads from many different resources and making sense of them is not easy. Hopefully, this article can serve as a somewhat comprehensive and intuitive answer to the question \u201cwhy sigmoid\u201d for the people who had doubts. The goal of learning is not just knowing the how, but also the why so that we can generalize our learning in real applications.", "This topic led to the broader topic of Generalized Linear Models. GLMs are a powerful class of models that don\u2019t get the same spotlight as deep learning. In many cases, the correct application of GLMs may get the job done and make your life easier at the same time. Compared to deep learning techniques, GLM has the advantage of mathematical simplicity and well-studied interpretability. A solid understanding of the underlying theory could also help machine learning researchers and practitioners develop new approaches. If you have interests in further pursuing this topic, I recommend MIT 18.650 Statistics for Applications lectures by Philippe Rigollet and the resources in my references. Keep on learning!", "See my other posts on Medium, or follow me on Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Engineer, builder, writer. Follow me here and on Twitter for future content https://twitter.com/logancyang"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F42751d82686&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----42751d82686--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----42751d82686--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://loganyang.medium.com/?source=post_page-----42751d82686--------------------------------", "anchor_text": ""}, {"url": "https://loganyang.medium.com/?source=post_page-----42751d82686--------------------------------", "anchor_text": "Logan Yang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59aa671cf125&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&user=Logan+Yang&userId=59aa671cf125&source=post_page-59aa671cf125----42751d82686---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F42751d82686&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F42751d82686&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/video-tutorial", "anchor_text": "Video Tutorial"}, {"url": "https://en.wikipedia.org/wiki/Logistic_distribution", "anchor_text": "logistic distribution"}, {"url": "https://en.wikipedia.org/wiki/Logistic_distribution", "anchor_text": "Wikipedia"}, {"url": "https://www.youtube.com/watch?v=X-ix97pw0xY", "anchor_text": "MIT 18.650 Statistics for Applications lectures by Philippe Rigollet"}, {"url": "https://medium.com/@loganyang", "anchor_text": "Medium"}, {"url": "https://twitter.com/logancyang", "anchor_text": "Twitter"}, {"url": "http://data.princeton.edu/wws509/notes/", "anchor_text": "http://data.princeton.edu/wws509/notes/"}, {"url": "https://www.youtube.com/watch?v=X-ix97pw0xY", "anchor_text": "https://www.youtube.com/watch?v=X-ix97pw0xY"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----42751d82686---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/statistics?source=post_page-----42751d82686---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----42751d82686---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/sigmoid?source=post_page-----42751d82686---------------sigmoid-----------------", "anchor_text": "Sigmoid"}, {"url": "https://medium.com/tag/video-tutorial?source=post_page-----42751d82686---------------video_tutorial-----------------", "anchor_text": "Video Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F42751d82686&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&user=Logan+Yang&userId=59aa671cf125&source=-----42751d82686---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F42751d82686&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&user=Logan+Yang&userId=59aa671cf125&source=-----42751d82686---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F42751d82686&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----42751d82686--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F42751d82686&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----42751d82686---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----42751d82686--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----42751d82686--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----42751d82686--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----42751d82686--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----42751d82686--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----42751d82686--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----42751d82686--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----42751d82686--------------------------------", "anchor_text": ""}, {"url": "https://loganyang.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://loganyang.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Logan Yang"}, {"url": "https://loganyang.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "307 Followers"}, {"url": "https://twitter.com/logancyang", "anchor_text": "https://twitter.com/logancyang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59aa671cf125&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&user=Logan+Yang&userId=59aa671cf125&source=post_page-59aa671cf125--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff079e6cea9be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-sigmoid-a-probabilistic-perspective-42751d82686&newsletterV3=59aa671cf125&newsletterV3Id=f079e6cea9be&user=Logan+Yang&userId=59aa671cf125&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}