{"url": "https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28", "time": 1683007257.942775, "path": "towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28/", "webpage": {"metadata": {"title": "Word2Vec Implementation. How to implement Word2Vec using numpy\u2026 | by Rahuljha | Towards Data Science", "h1": "Word2Vec Implementation", "description": "How to implement Word2Vec using numpy and python."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Tomas_Mikolov", "anchor_text": "Tomas Mikolov", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Google", "anchor_text": "Google", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "NLP", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Jeffrey_Archer", "anchor_text": "Jeffery Archer", "paragraph_index": 17}, {"url": "https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Memory_comparison.ipynb", "anchor_text": "code", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Neural_network", "anchor_text": "neural network", "paragraph_index": 27}, {"url": "http://www.claudiobellei.com/2018/01/06/backprop-word2vec/#skipgram", "anchor_text": "Here", "paragraph_index": 32}, {"url": "https://www.machinelearningplus.com/nlp/cosine-similarity/", "anchor_text": "here", "paragraph_index": 38}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "T-SNE", "paragraph_index": 41}, {"url": "https://github.com/rahul1728jha/Word2Vec_Implementation/tree/master/output_images_bigger_data", "anchor_text": "here", "paragraph_index": 48}, {"url": "http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/", "anchor_text": "negative sampling", "paragraph_index": 49}, {"url": "https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08", "anchor_text": "here", "paragraph_index": 49}], "all_paragraphs": ["This article is about the implementation of a very popular word embedding technique known as Word2Vec. It was implemented by Tomas Mikolov at Google.", "The objective of this article to show the inner workings of Word2Vec in python using numpy. I will not be using any other libraries for that. This implementation is not an efficient one as the purpose here is to understand the mechanism behind it. You can find the official paper here.", "Computers only understand the language of numbers. The way we encode our text data to numbers influences the outcome a lot. In general there are 3 techniques which are used to perform this task.", "Out of these, word2vec performs incredibly well in NLP tasks. The core idea behind the concept is very simple yet it produces amazing results.", "\u201cA man is known by the company he keeps\u201d", "This is a very well know saying. And word2vec also works primarily on this idea. A word is known by the company it keeps. This sounds so strange and funny but it gives amazing results. Let us try to understand it a bit more.", "Here we can easily see that the words \u201chard\u201d and \u201cwork\u201d occur in close vicinity. This seems very easy as a human being to observe, but for computers it is a very difficult task. So when we vectorize (turn words to numbers) these words, it seems obvious that there representation as numbers should be similar or close. This is exactly what word2vec achieves and with a very good result. Enough said, now time to get our hands dirty!", "So with the core idea understood, we understand that the algorithm is based on recognizing words in vicinity of each other. In other words we can say that if the computer tries to learn that the words \u201chard\u201d and \u201cwork\u201d occur in close proximity of each other then it will learn the vectors according to that.", "If we say that our \u201ctarget\u201d word is \u201chard\u201d for which we need to learn a good vector, we provide the computer with its nearby word or the \u201ccontext\u201d word which is \u201cwork\u201d in this case amongst \u201cthe, began, is etc\u201d.", "There are two main architectures which try to learn the above. Skip gram and CBOW", "Skip-gram : So we understood about the concept of target and context words. This model, tries to learn the context words for each of the target words.", "Text : ['Best way to success is through hard work and persistence']", "So for this model our input will be like :", "So we see here there is an input layer, a hidden layer and a output layer. We can also see there are two sets of weights (W, W`).", "CBOW: Context bag of words. Conceptually this is just the opposite of skip-gram. Here we try to predict the target word from a list of context words. So for our example, we will have the input as (Best,way,success,is) and we will need to predict \u201cto\u201d from it.", "We can see here it is just the opposite of skip-gram model.", "In this article, I will be implementing the skip-gram model.", "To train a model to learn good vectors for words, we will need a huge amount of data. But in this article i will try to showcase the workings on a very small data set. The data set consists of plots of various stories of Jeffery Archer taken from Wikipedia.", "Step 1: Read data from a file", "We will need some variables which will come handy to us in further sections.", "The output of this code will give:", "Before seeing the code let us first understand some concepts.", "The following code generates one-hot-vectors for our data:", "There is an alternate way to generate the context_word_one_hot_vector.", "Instead of having the indices present in a single list, we have two different list. But the problem with this approach is that it will take a lot of space if the size of our data increases. I have illustrated that in a python script. Refer to the code to see the difference.", "Now we have the vectors generated for target word and context word. To train a model, we need to have the data in the form of (X,Y) i.e (target_words, context_words).", "This is achieved by the following code:", "The above image shows how a neural network is trained, in this case a skip-gram model. Here we can see there is only one hidden layer. When we train a neural network in deep learning, we tend to have several hidden layers. This is where this skip-gram works so well. In spite of just having a single hidden layer it is a state-of-the-art algorithm.", "Here we have the following parameters:", "Now once we have done 1 round of forward propagation we get some output value. So obviously we will have some error in our prediction as compared to the original value.", "Explanation: This is back-propagated to update the weights for the next iteration", "With the error calculated above we need to update the weights (W_1 and W_2) so that our network tries to rectify the error.", "Here is a good link that explains the derivative equations for back propagation.", "If we look a bit deep into the loss function E, we see that we are trying to optimize the probability of finding correct context words p(WO,1, WO,2, \u00b7 \u00b7 \u00b7 , WO,C) given our WI (the input word). So the loss will decrease as we come nearer to a distribution that finds the correct context words for each given target words.", "The loss function is comprised of two parts.", "Till now we can see that a lot of variables are involved in the process. So a very big part of training is finding the right set of variables that gives the best result. We will go by these variables one by one.", "I have done some fun experiments with window size, epochs ,dimensions ,stopwords while keeping the learning rate as 0.01", "To get the word vectors for each word, after the the final training, the first set of weights i.e weight_1 is used to retrieve the vector for each word.", "To find the similar set of words, cosine similarity is used. It is a metric that measures how similar two vectors are in a multidimensional space. Basically it measures the angle between two vectors to find how similar they are. A good explanation of it can be found here.", "Now i used two different sets of data, one a single line of text and the other a text corpus.", "Inference with a single line of text as input", "The scatter plots show below are of 2 dimension. This is done through dimensionality reduction. I have used T-SNE for it. The figure below does not give an exact picture of as we are compressing several dimensions into 2.", "We can see here that how close \u201cbest\u201d and \u201cway\u201d are present. Even though a single line of text is very very scarce for a model to train, yet it learns good embedding.", "Scatter plot with varying window size:", "Inference with a relatively larger corpus", "To see how well the model is working, i printed a similarity matrix for some words with a larger corpus", "One thing to observe here is that for higher dimension, the numbers for similarity is low. The reason behind this is that the corpus is very small with only around 700 unique words. So to learn embedding for a larger dimension, a huge corpus is needed. The word2vec algorithm is trained on a corpus of size in millions.", "The model seemed to perform better without stopwords as the loss curve was better for each epoch. To have a better understanding of it we need to try it on a bigger corpus.", "The scatter plot for above can be found at my github link here.", "Training of word2vec is a very computationally expensive process. With millions of word the training may take a lot of time. Some methods to counter this are negative sampling and Hierarchical softmax. A good link to understand both can be found here.", "The entire code can be found at my github repository :", "Please leave comments for any clarifications or questions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd256cf0e5f28&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rahul1728jha?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rahul1728jha?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "Rahuljha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2a7b38158936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&user=Rahuljha&userId=2a7b38158936&source=post_page-2a7b38158936----d256cf0e5f28---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd256cf0e5f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd256cf0e5f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Tomas_Mikolov", "anchor_text": "Tomas Mikolov"}, {"url": "https://en.wikipedia.org/wiki/Google", "anchor_text": "Google"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "Bag-of-words"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "Tf-Idf"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "NLP"}, {"url": "https://www.researchgate.net/figure/The-architecture-of-Skip-gram-model-20_fig1_322905432", "anchor_text": "Credits"}, {"url": "https://en.wikipedia.org/wiki/Jeffrey_Archer", "anchor_text": "Jeffery Archer"}, {"url": "https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Memory_comparison.ipynb", "anchor_text": "code"}, {"url": "https://en.wikipedia.org/wiki/Neural_network", "anchor_text": "neural network"}, {"url": "https://towardsdatascience.com/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932", "anchor_text": "here"}, {"url": "http://www.claudiobellei.com/2018/01/06/backprop-word2vec/#skipgram", "anchor_text": "Here"}, {"url": "https://arxiv.org/pdf/1411.2738.pdf", "anchor_text": "Credits"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent"}, {"url": "https://www.machinelearningplus.com/nlp/cosine-similarity/", "anchor_text": "here"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "T-SNE"}, {"url": "https://github.com/rahul1728jha/Word2Vec_Implementation/tree/master/output_images_bigger_data", "anchor_text": "here"}, {"url": "http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/", "anchor_text": "negative sampling"}, {"url": "https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08", "anchor_text": "here"}, {"url": "https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb", "anchor_text": "https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d256cf0e5f28---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d256cf0e5f28---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----d256cf0e5f28---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/numpy?source=post_page-----d256cf0e5f28---------------numpy-----------------", "anchor_text": "Numpy"}, {"url": "https://medium.com/tag/implementation?source=post_page-----d256cf0e5f28---------------implementation-----------------", "anchor_text": "Implementation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd256cf0e5f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&user=Rahuljha&userId=2a7b38158936&source=-----d256cf0e5f28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd256cf0e5f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&user=Rahuljha&userId=2a7b38158936&source=-----d256cf0e5f28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd256cf0e5f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd256cf0e5f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d256cf0e5f28---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d256cf0e5f28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rahul1728jha?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rahul1728jha?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rahuljha"}, {"url": "https://medium.com/@rahul1728jha/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "37 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2a7b38158936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&user=Rahuljha&userId=2a7b38158936&source=post_page-2a7b38158936--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Faf2dab38ca44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-word2vec-implementation-using-numpy-and-python-d256cf0e5f28&newsletterV3=2a7b38158936&newsletterV3Id=af2dab38ca44&user=Rahuljha&userId=2a7b38158936&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}