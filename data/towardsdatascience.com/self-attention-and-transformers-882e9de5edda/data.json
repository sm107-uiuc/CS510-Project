{"url": "https://towardsdatascience.com/self-attention-and-transformers-882e9de5edda", "time": 1682997116.4652338, "path": "towardsdatascience.com/self-attention-and-transformers-882e9de5edda/", "webpage": {"metadata": {"title": "Self Attention and Transformers. From Attention to Self Attention to\u2026 | by Mahendran Venkatachalam | Towards Data Science", "h1": "Self Attention and Transformers", "description": "This is really a continuation of an earlier post on \u201cIntroduction to Attention\u201d, where we saw some of the key challenges that were addressed by the attention architecture introduced there (and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1503.08895.pdf", "anchor_text": "in this paper by Sukhbaatar et al", "paragraph_index": 4}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "\u201cAttention Is All You Need\u201d by Vaswani et al.", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1803.02155.pdf", "anchor_text": "paper by Shaw et al here", "paragraph_index": 15}], "all_paragraphs": ["This is really a continuation of an earlier post on \u201cIntroduction to Attention\u201d, where we saw some of the key challenges that were addressed by the attention architecture introduced there (and referred in Fig 1 below).", "The challenge of one single context vector, the final hidden state in the encoder RNN, holding the meaning of the entire sentence/input sequence was addressed by replacing that with an attention based context vector generated for every decoder step as seen in Figure 1.", "But it introduced the challenge of increased computational complexity of computing a separate context vector for every step of decoder. And this was over and above the already existing parallelization related challenge. i.e. The sequential nature of processing that is required in RNNs \u2014 give that hidden state h1 is required to compute the next hidden state h2, these operations cannot be done in parallel. Both of these challenges are represented by the dashed red lines in the left half of Figure 1 (and also called out in Figure 3).", "Talking about sequential processing, you might also be wondering, given that we are replacing the one final hidden state with a context vector generated for every output step \u2014 do we need the \u201ch\u201d states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and \u201ch\u201d is only an indirect representation of \u201cx\u201d. It represents the context of all input steps until \u201cx\u201d and not just \u201cx\u201d alone. Wouldn\u2019t using \u201cx\u201d directly make more sense?", "End to End Memory networks introduced in this paper by Sukhbaatar et al proposes. Pasted in Figure 2 is just a single layer version of the proposed model. The proposed model has \u201cinput memory\u201d or \u201ckey\u201d vectors representing all inputs, a \u201cquery\u201d vector to which the model needs to respond to (like the last decoder hidden state) and \u201cvalue\u201d or \u201coutput memory\u201d vectors \u2014 again a representation of the inputs. The inner product between \u201cquery\u201d and \u201ckeys\u201d give the \u201cmatch\u201d (akin to attention) probability. The sum of \u201cvalue\u201d vectors weighted by the probability gives the final response. While producing good results, this eliminated sequential processing of the inputs and replaced it with a \u201cmemory query\u201d paradigm.", "Compare this with the base attention model we have seen earlier and the \u201csimilarities\u201d will start to emerge. While there are differences between the two \u2014 \u201cEnd to End Memory Networks\u201d proposed a memory across sentences and multiple \u201chops\u201d to generate an output, we can borrow the concepts of \u201cKey\u201d, \u201cQuery\u201d and \u201cValue\u201d to get a generalized view of our base model. Figure 3 calls out these concepts as it applies to the base model.", "Figure 3 also highlights the two challenges we would love to resolve. For challenge #1, we could perhaps just replace the hidden state (h) acting as keys with the inputs (x) directly. But this wouldn\u2019t be a rich representation - if we directly use word embeddings. The end-to-end memory network used different embedding matrices for input and output memory representations, which is better but they are still independent representations of the word. Compare this with the hidden state (h) which represents not just the word, but the word in context of the given sentence.", "Is there a way to eliminate the sequential nature of generating hidden states, but still produce a richer, context representing vector?", "Figure 4 illustrates a possible way to do this. What if, instead of using attention to connect encoder and decoder, we use attention within encoder and decoder respectively? Attention, after all, is a rich representation \u2014 as it considers all keys and values. So instead of deriving hidden states using a RNN, we can use an attention based replacement where inputs (x) are used as \u201cKeys\u201d and \u201cValues\u201d. (i.e. \u201ch\u201ds are replaced by \u201cx\u201ds as illustrated in Figure 5 below) ?", "On the encoder side, we can use self attention to generate a richer representation of a given input step xi, with respect to all other items in the input x1, x2\u2026xn. This can be done for all input steps in parallel, unlike hidden state generation in a RNN based encoder. We are basically moving the criss-crossed lines on the left half of Figure 4 downwards as seen in the right half, thereby eliminating the dashed red lines between the representative vectors.", "On the decoder side, we can do something similar. We replace a RNN based decoder to an attention based decoder. i.e. there are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, we do self attention on all outputs generated so far and along with it consume the entirety of encoder output. In other words, we are applying attention to whatever we know so far. (Side note \u2014 this is strictly not how it happens in Transformers, where attention over generated outputs and encoder output are done in two separate layers one after the other).", "The \u201cTransformer\u201d model, introduced in the paper \u201cAttention Is All You Need\u201d by Vaswani et al. and seen in Figure 6 below, does what we discussed above.", "The Transformer model uses a \u201cScaled Dot Product\u201d attention mechanism. It is illustrated in right side of Figure 6 and also in Figure 7. Compare Figure 7 and Figure 1 to get a sense of differences in \u201chow\u201d attention is computed between the two models. (Note: the \u201cwhere\u201d also differs, we\u2019ll get to that next). The transformer model also uses what is called as \u201cMulti-Head Attention\u201d \u2014 instead of calculating just one \u201cai\u201d (attention) for a give \u201cxi\u201d, multiple attention scores \u201cai\u201ds are calculated \u2014 using different sets of Ws. This allows the model to attend to different \u201crepresentation sub-spaces\u201d at different positions, akin to using different filters to create different features maps in a single layer in a CNN.", "The encoder in the proposed Transformer model has multiple \u201cencoder self attention\u201d layers. Each layer is constructed as follows:", "The decoder will also have multiple layers. Each layer is constructed as follows:", "While getting rid of the sequential nature was helpful in many ways, it took of one key advantage \u2014 of knowing the order of words in the input sequence. Without it, the same word occurring in different positions within the same sentence might end up with the same output representation (since it will have the same key, value etc). So the model uses \u201cPositional Encodings\u201d \u2014 basically a vector that represents position which is added to the input embeddings at the bottom of the encoder and decoder stack. There\u2019s another paper by Shaw et al here that proposes a relational position based alternative that achieves better result than absolute positional encoding suggested in the original Transformer model paper \u2014 I recommend looking into that if you can spend time on the positional embeddings.", "Hope this was helpful in some way.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F882e9de5edda&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----882e9de5edda--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----882e9de5edda--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----882e9de5edda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----882e9de5edda--------------------------------", "anchor_text": "Mahendran Venkatachalam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d2735a047ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=post_page-4d2735a047ae----882e9de5edda---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F882e9de5edda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F882e9de5edda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "paper by Bahdanau et al"}, {"url": "https://arxiv.org/pdf/1503.08895.pdf", "anchor_text": "in this paper by Sukhbaatar et al"}, {"url": "https://arxiv.org/pdf/1503.08895.pdf", "anchor_text": "by Sukhbaatar et al"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "paper by Bahdanau et al"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "\u201cAttention Is All You Need\u201d by Vaswani et al."}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "\u201cAttention Is All You Need\u201d by Vaswani et al."}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "\u201cAttention Is All You Need\u201d by Vaswani et al."}, {"url": "https://arxiv.org/pdf/1803.02155.pdf", "anchor_text": "paper by Shaw et al here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----882e9de5edda---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/attention-network?source=post_page-----882e9de5edda---------------attention_network-----------------", "anchor_text": "Attention Network"}, {"url": "https://medium.com/tag/data-science?source=post_page-----882e9de5edda---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----882e9de5edda---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----882e9de5edda---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F882e9de5edda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----882e9de5edda---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F882e9de5edda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----882e9de5edda---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F882e9de5edda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----882e9de5edda--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F882e9de5edda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----882e9de5edda---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----882e9de5edda--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----882e9de5edda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----882e9de5edda--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----882e9de5edda--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----882e9de5edda--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----882e9de5edda--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----882e9de5edda--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----882e9de5edda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mahendran Venkatachalam"}, {"url": "https://medium.com/@mahendran.venkatachalam/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "296 Followers"}, {"url": "https://gotensor.com/", "anchor_text": "https://gotensor.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d2735a047ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=post_page-4d2735a047ae--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8b9125d53ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-attention-and-transformers-882e9de5edda&newsletterV3=4d2735a047ae&newsletterV3Id=b8b9125d53ec&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}