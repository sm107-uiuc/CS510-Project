{"url": "https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1", "time": 1683017206.47248, "path": "towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1/", "webpage": {"metadata": {"title": "Essential Math for Data Science: Information Theory | by Hadrien Jean | Towards Data Science", "h1": "Essential Math for Data Science: Information Theory", "description": "The field of information theory studies the quantification of information in signals. In the context of machine learning, some of these concepts are used to characterize or compare probability\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science", "paragraph_index": 11}, {"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science", "paragraph_index": 19}, {"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science", "paragraph_index": 27}, {"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science", "paragraph_index": 36}, {"url": "https://hadrienj.github.io/about/", "anchor_text": "https://hadrienj.github.io/about/", "paragraph_index": 58}], "all_paragraphs": ["The field of information theory studies the quantification of information in signals. In the context of machine learning, some of these concepts are used to characterize or compare probability distributions. The ability to quantify information is also used in the decision tree algorithm, to select the variables associated with the maximum information gain. The concepts of entropy and cross-entropy are also important in machine learning because they lead to a widely used loss function in classification tasks: the cross-entropy loss or log loss.", "The first step to understanding information theory is to consider the concept of the quantity of information associated with a random variable. In information theory, this quantity of information is denoted as I and is called the Shannon information, information content, self-information, or surprisal. The main idea is that likely events convey less information than unlikely events (which are thus more surprising). For instance, if a friend from Los Angeles, California tells you: \u201cIt is sunny today\u201d, this is less informative than if she tells you: \u201cIt is raining today\u201d. For this reason, it can be helpful to think of the Shannon information as the amount of surprise associated with an outcome. You\u2019ll also see in this section why it is also a quantity of information, and why likely events are associated with less information.", "Bits represent variables that can take two different states (0 or 1). For instance, 1 bit is needed to encode the outcome of a coin flip. If you flip two coins, you\u2019ll need at least two bits to encode the result. For instance, 00 for HH, 01 for HT, 10 for TH, and 11 for TT. You could use other codes, such as 0 for HH, 100 for HT, 101 for TH, and 111 for TT. However, this code uses a larger number of bits on average (considering that the probability distribution of the four events is uniform, as you\u2019ll see)", "Let\u2019s take an example to see what a bit describes. Erica sends you a message containing the result of three coin flips, encoding \u2018heads\u2019 as 0 and \u2018tails\u2019 as 1. There are 8 possible sequences, such as 001, 101, etc. When you receive a message of one bit, it divides your uncertainty by a factor of 2. For instance, if the first bit tells you that the first roll was \u2018heads\u2019, the remaining possible sequences are 000, 001, 010, and 011. There are only 4 possible sequences instead of 8. Similarly, receiving a message of two bits will divide your uncertainty by a factor of 2\u00b2; a message of three bits, by a factor of 2\u00b3, and so on.", "Note that we talk about \u201cuseful information\u201d, but it is possible that the message is redundant and convey less information with the same number of bits.", "Let\u2019s say that we want to transmit the result of a sequence of eight tosses. You\u2019ll allocate one bit per toss. You thus need eight bits to encode the sequence. The sequence might be for instance \u201c00110110\u201d, corresponding to HHTTHTTH (four \u201cheads\u201d and four \u201ctails\u201d).", "However, let\u2019s say that the coin is biased: the chance to get \u201ctails\u201d is only 1 over 8. You can find a better way to encode the sequence. One option is to encode the index of the outcomes \u201ctails\u201d: it will take more than one bit, but \u2018tails\u2019 occurs only for a small proportion of the trials. With this strategy, you allocate more bits to rare outcomes.", "This example illustrates that more predictable information can be compressed: a biased coin sequence can be encoded with a smaller amount of information than a fair coin. This means that Shannon information depends on the probability of the event.", "Shannon information encodes this idea and converts the probability that an event will occur into the associated quantity of information. Its characteristics are that, as you saw, likely events are less informative than unlikely events and also that information from different events is additive (if the events are independent).", "Mathematically, the function I(x) is the information of the event X=x that takes the outcome as input and returns the quantity of information. It is a monotonically decreasing function of the probability (that is, a function that never increases when the probability increases). Shannon information is described as:", "The result is a lower bound on the number of bits, that is, the minimum amount of bits needed to encode a sequence with an optimal encoding.", "The logarithm of a product is equal to the sum of the elements: log2(ab)=log2(a)+log2(b). This property is useful to encode the additive property of the Shannon information. The probability of occurrence of two events is their individual probabilities multiplied together (because they are independent, as you saw in Essential Math for Data Science):", "Let\u2019s plot this function for a range of probability between 0 and 1 to see the shape of the curve:", "Figure 1: The quantity of information is given by the negative logarithm of the probability.", "As you can see in Figure 1, the negative logarithm function encodes the idea that a very unlikely event (probability around 0) is associated with a large quantity of information and a likely event (probability around 1) is associated with a quantity of information around 0.", "Since you used a base-two logarithm np.log2(), the information I(x) is measured in bits.", "Consider for instance a biased coin, where you have a probability of 0.8 of getting \u2018heads\u2019.", "The average number of bits required to describe a series of events from this distribution is 0.72 bits.", "To summarize, you can consider the entropy as a summary of the information associated with the probabilities of the discrete distribution:", "The entropy is the expectation of the information with respect to the probability distribution. Remember from Essential Math for Data Science that the expectation is the mean value you\u2019ll get if you draw a large number of samples from the distribution:", "with the random variable X having n possible outcomes, x_i being the ith possible outcome corresponding to a probability of P(x_i). The expected value of the information of a distribution corresponds to the average of the information you\u2019ll get.", "Following the formula of the expectation and the Shannon information, the entropy of the random variable X is defined as:", "The entropy gives you the average quantity of information that you need to encode the states of the random variable X.", "Note that the input of the function H(X) is the random variable X while I(x) denotes the Shannon information of the event X=x. You can also refer to the entropy of the random variable X which is distributed with respect to P(x) as H(P).", "Let\u2019s take an example: as illustrated in Figure 2 in the bottom panel, you have a discrete distribution with four possible outcomes, associated with probabilities 0.4, 0.4, 0.1, and 0.1, respectively. As you saw previously, the information is obtained by log transforming the probabilities (top panel). This is the last part of the entropy formula: log2 P(x).", "Figure 2: Illustration of the entropy as the weighted sum of the Shannon information. (Image by author)", "Each of these transformed probabilities is weighted by the corresponding raw probability. If an outcome occurs frequently, it will give more weight into the entropy of the distribution. This means that a low probability (like 0.1 in Figure 2) gives a large amount of information (3.32 bits) but has less influence on the final result. A larger probability (like 0.4 in Figure 2) is associated with less information (1.32 bits as shown in Figure 2) but has more weight.", "In the example of a biased coin, you calculated the entropy of a Bernoulli process (more details about the Bernoulli distribution in Essential Math for Data Science). In this special case, the entropy is called the binary entropy function.", "To characterize the binary entropy function, you\u2019ll calculate the entropy of a biased coin described by various probability distributions (from heavily biased in favor of \u201ctails\u201d to heavily biased in favor of \u201cheads\u201d).", "Let\u2019s start by creating a function to calculate the entropy of a distribution that takes an array with the probabilities as input and returns the corresponding entropy:", "You can also use entropy() from scipy.stats, where you can specify the base of the logarithm used to calculate the entropy. Here, I have used the base-two logarithm.", "Let\u2019s take the example of a fair coin, with a probability of 0.5 of landing \u2018heads\u2019. The distribution is thus 0.5 and 1\u20130.5 = 0.5. Let\u2019s use the function we just defined to calculate the corresponding entropy is:", "The function calculates the sum of P * np.log2(P) over each element of the array you use as input. Using an array as input As you saw in the previous section, you can expect a lower entropy for a biased coin. Let's plot the entropy for various coin biases, from a coin landing only as 'tails' to a coin landing only as 'heads':", "Figure 3: Entropy as a function of the probability to land \u201cheads\u201d.", "Figure 3 shows that the entropy increases until you reach the more uncertain condition: that is, when the probability of landing \u2018heads\u2019 equals the probability of landing \u2018tails\u2019.", "The entropy of a continuous distribution is called differential entropy. It is an extension of the entropy for discrete distribution, but it doesn\u2019t satisfy the same requirements. The issue is that values have probability tending to zero with continuous distributions, and encoding this would require a number of bits tending to infinity.", "Differential entropy can be negative. The reason is that, as you saw in Essential Math for Data Science, continuous distributions are not probabilities but probability densities, meaning that they don\u2019t satisfy the requirements of probabilities. For instance, they are not constrained to be lower than", "The concept of entropy can be used to compare two probability distributions: this is called the cross-entropy between two distributions, which measures how much they differ.", "You can also consider cross-entropy as the expected quantity of information of events drawn from P(x) when you use Q(x) to encode them.", "Figure 4: Illustration of the cross-entropy as the Shannon information of Q(x) weighted according to the distribution of P(x). (Image by author)", "Figure 4 shows two different situations to illustrate the cross-entropy. On the left, you have two identical distributions P(x) (in blue) and Q(x) (in red). Their cross-entropy is equal to the entropy because the information of Q(x) is weighted according to the distribution of P(x), which is similar to Q(x).", "However, in the right panel, P(x) and Q(x) are different. This results in a larger cross-entropy, because probabilities associated with a large quantity of information have a small weight, while probabilities associated with a small quantity of information have large weights.", "The cross-entropy can\u2019t be smaller than the entropy. Still in the right panel, you can see that, when the probability Q(x) is larger than P(x) (and thus associated with a lower amount of information), it is counterbalanced by the low weights (resulting in low weights and low information). These low weights will be compensated with larger weights in other probabilities from the distribution (resulting in large weights and large information).", "To summarize, the cross-entropy is minimum when the distributions are identical. As you\u2019ll see in 0.1.4, this property makes the cross-entropy a useful metric. Note also that the result is different according to the distribution you choose as a reference: H(P, Q) \u2260 H(Q, P).", "Figure 5: Cross entropy can be used to compare the true distribution (probability of 1 for the correct class and 0 otherwise) and the distribution estimated by the model.(Image by author)", "Say you want to build a model that classifies three different bird species from audio samples. As illustrated in Figure 5, the audio samples are converted in features (here spectrograms) and the possible classes (the three different birds) are one-hot encoded, that is, encoded as 1 for the correct class and 0 otherwise. Furthermore, the machine learning model outputs probabilities for each class.", "Figure 5 shows that the true class corresponding to the sample you consider in this example is \u201cEuropean Green Woodpecker\u201d. The model outputs a probability distribution and you\u2019ll compute the cross-entropy loss associated with this estimation. Figure 6 shows both distributions.", "Figure 6: Comparison of the true distribution P(x) and the estimated distribution Q(x).(Image by author)", "Let\u2019s manually calculate the cross-entropy between these two distributions:", "Since you one-hot encoded the classes (1 for the true class and 0 otherwise), the cross-entropy is simply the negative logarithm of the estimated probability for the true class.", "In machine learning, the cross-entropy is widely used as a loss for binary classification: the log loss.", "which is the formula of the log loss.", "Intuitively, the KL divergence is the supplemental amount of information associated with the encoding of the distribution Q(x) compared to the true distribution P(x). It tells you how different the two distributions are.", "Mathematically, the KL divergence between two distributions P(x) and Q(x), denoted as D_KL(P||Q), is expressed as the difference between the cross-entropy of P(x) and Q(x) and the entropy of P(x):", "Replacing with the expressions of the cross-entropy and the entropy, you get:", "The KL divergence is always non-negative. Since the entropy H(P) is identical to the cross-entropy H(P, P), and because the smallest cross-entropy is between identical distributions ( H(P, P)), H(P, Q) is necessarily larger than H(P). In addition, the KL divergence is equal to zero when the two distributions are identical.", "This post is a sample of my book Essential Math for Data Science!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning scientist. PhD in cognitive science at the \u00c9cole Normale Sup\u00e9rieure (ENS), Paris, France. More info: https://hadrienj.github.io/about/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5d0380232ca1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hadrienj?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hadrienj?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "Hadrien Jean"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a54d892cde4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&user=Hadrien+Jean&userId=6a54d892cde4&source=post_page-6a54d892cde4----5d0380232ca1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d0380232ca1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d0380232ca1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/essential-math", "anchor_text": "ESSENTIAL MATH FOR DATA SCIENCE"}, {"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science"}, {"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science"}, {"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science"}, {"url": "https://www.essentialmathfordatascience.com", "anchor_text": "Essential Math for Data Science"}, {"url": "https://hadrienj.github.io/posts/Essential-Math-information_theory/", "anchor_text": "https://hadrienj.github.io/posts/Essential-Math-information_theory/"}, {"url": "https://bit.ly/34ZXUsy", "anchor_text": ""}, {"url": "https://bit.ly/3odUglO", "anchor_text": "Get the book here: https://bit.ly/3odUglO!"}, {"url": "https://medium.com/tag/essential-math?source=post_page-----5d0380232ca1---------------essential_math-----------------", "anchor_text": "Essential Math"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----5d0380232ca1---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----5d0380232ca1---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5d0380232ca1---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5d0380232ca1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d0380232ca1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&user=Hadrien+Jean&userId=6a54d892cde4&source=-----5d0380232ca1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d0380232ca1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&user=Hadrien+Jean&userId=6a54d892cde4&source=-----5d0380232ca1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d0380232ca1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5d0380232ca1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5d0380232ca1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5d0380232ca1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5d0380232ca1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5d0380232ca1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hadrienj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hadrienj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hadrien Jean"}, {"url": "https://medium.com/@hadrienj/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "846 Followers"}, {"url": "https://hadrienj.github.io/about/", "anchor_text": "https://hadrienj.github.io/about/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a54d892cde4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&user=Hadrien+Jean&userId=6a54d892cde4&source=post_page-6a54d892cde4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F13cd9155f2bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-math-for-data-science-information-theory-5d0380232ca1&newsletterV3=6a54d892cde4&newsletterV3Id=13cd9155f2bd&user=Hadrien+Jean&userId=6a54d892cde4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}