{"url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "time": 1682993765.025375, "path": "towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526/", "webpage": {"metadata": {"title": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data Science", "h1": "Neural Network Embeddings Explained", "description": "Applications of neural networks have expanded significantly in recent years from image segmentation to natural language processing to time-series forecasting. One notably successful use of deep\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1705.03127", "anchor_text": "machine translation", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1604.06737", "anchor_text": "entity embeddings for categorical variables", "paragraph_index": 0}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "real problem I\u2019m working on", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/guide/embedding", "anchor_text": "In the context of neural networks, embeddings", "paragraph_index": 2}, {"url": "https://stats.stackexchange.com/questions/182775/what-is-an-embedding-layer-in-a-neural-network", "anchor_text": "embeddings form the parameters", "paragraph_index": 13}, {"url": "https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=embeddings-colab&hl=en", "anchor_text": "this Google Colab Notebook", "paragraph_index": 14}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "complete notebook here", "paragraph_index": 16}, {"url": "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "anchor_text": "cosine distance", "paragraph_index": 21}, {"url": "https://github.com/lmcinnes/umap", "anchor_text": "Uniform Manifold Approximation and Projection, UMAP,", "paragraph_index": 27}, {"url": "https://projector.tensorflow.org", "anchor_text": "projector", "paragraph_index": 31}, {"url": "http://fast.ai", "anchor_text": "learn deep learning", "paragraph_index": 33}, {"url": "http://shop.oreilly.com/product/0636920097471.do", "anchor_text": "build deep learning solutions", "paragraph_index": 33}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 34}, {"url": "https://willk.online", "anchor_text": "willk.online", "paragraph_index": 34}], "all_paragraphs": ["Applications of neural networks have expanded significantly in recent years from image segmentation to natural language processing to time-series forecasting. One notably successful use of deep learning is embedding, a method used to represent discrete variables as continuous vectors. This technique has found practical applications with word embeddings for machine translation and entity embeddings for categorical variables.", "In this article, I\u2019ll explain what neural network embeddings are, why we want to use them, and how they are learned. We\u2019ll go through these concepts in the context of a real problem I\u2019m working on: representing all the books on Wikipedia as vectors to create a book recommendation system.", "An embedding is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space.", "Neural network embeddings have 3 primary purposes:", "This means in terms of the book project, using neural network embeddings, we can take all 37,000 book articles on Wikipedia and represent each one using only 50 numbers in a vector. Moreover, because embeddings are learned, books that are more similar in the context of our learning problem are closer to one another in the embedding space.", "Neural network embeddings overcome the two limitations of a common method for representing categorical variables: one-hot encoding.", "The operation of one-hot encoding categorical variables is actually a simple embedding where each category is mapped to a different vector. This process takes discrete entities and maps each observation to a vector of 0s and a single 1 signaling the specific category.", "The one-hot encoding technique has two main drawbacks:", "The first problem is well-understood: for each additional category \u2014 referred to as an entity \u2014 we have to add another number to the one-hot encoded vector. If we have 37,000 books on Wikipedia, then representing these requires a 37,000-dimensional vector for each book, which makes training any machine learning model on this representation infeasible.", "The second problem is equally limiting: one-hot encoding does not place similar entities closer to one another in vector space. If we measure similarity between vectors using the cosine distance, then after one-hot encoding, the similarity is 0 for every comparison between entities.", "This means that entities such as War and Peace and Anna Karenina (both classic books by Leo Tolstoy) are no closer to one another than War and Peace is to The Hitchhiker\u2019s Guide to the Galaxy if we use one-hot encoding.", "Considering these two problems, the ideal solution for representing categorical variables would require fewer numbers than the number of unique categories and would place similar categories closer to one another.", "To construct a better representation of categorical entities, we can use an embedding neural network and a supervised task to learn embeddings.", "The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. The embeddings form the parameters \u2014 weights \u2014 of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories \u2014 relative to the task \u2014 are closer to one another.", "For example, if we have a vocabulary of 50,000 words used in a collection of movie reviews, we could learn 100-dimensional embeddings for each word using an embedding neural network trained to predict the sentimentality of the reviews. (For exactly this application see this Google Colab Notebook). Words in the vocabulary that are associated with positive reviews such as \u201cbrilliant\u201d or \u201cexcellent\u201d will come out closer in the embedding space because the network has learned these are both associated with positive reviews.", "In the book example given above, our supervised task could be \u201cidentify whether or not a book was written by Leo Tolstoy\u201d and the resulting embeddings would place books written by Tolstoy closer to each other. Figuring out how to create the supervised task to produce relevant representations is the toughest part of making embeddings.", "In the Wikipedia book project (complete notebook here), the supervised learning task is set as predicting whether a given link to a Wikipedia page appears in the article for a book. We feed in pairs of (book title, link) training examples with a mix of positive \u2014 true \u2014 and negative \u2014 false \u2014 pairs. This set-up is based on the assumption that books which link to similar Wikipedia pages are similar to one another. The resulting embeddings should therefore place alike books closer together in vector space.", "The network I used has two parallel embedding layers that map the book and wikilink to separate 50-dimensional vectors and a dot product layer that combines the embeddings into a single number for a prediction. The embeddings are the parameters, or weights, of the network that are adjusted during training to minimize the loss on the supervised task.", "In Keras code, this looks like the following (don\u2019t worry if you don\u2019t completely understand the code, just skip to the images):", "Although in a supervised machine learning task the goal is usually to train a model to make predictions on new data, in this embedding model, the predictions can be just a means to an end. What we want is the embedding weights, the representation of the books and links as continuous vectors.", "The embeddings by themselves are not that interesting: they are simply vectors of numbers:", "However, the embeddings can be used for the 3 purposes listed previously, and for this project, we are primarily interested in recommending books based on the nearest neighbors. To compute similarity, we take a query book and find the dot product between its vector and those of all the other books. (If our embeddings are normalized, this dot product is the cosine distance between vectors that ranges from -1, most dissimilar, to +1, most similar. We could also use the Euclidean distance to measure similarity).", "This is the output of the book embedding model I built:", "(The cosine similarity between a vector and itself must be 1.0). After some dimensionality reduction (see below), we can make figures like the following:", "We can clearly see the value of learning embeddings! We now have a 50-number representation of every single book on Wikipedia, with similar books closer to one another.", "One of the coolest parts about embeddings are that they can be used to visualize concepts such as novel or non-fiction relative to one another. This requires a further dimension reduction technique to get the dimensions to 2 or 3. The most popular technique for reduction is itself an embedding method: t-Distributed Stochastic Neighbor Embedding (TSNE).", "We can take the original 37,000 dimensions of all the books on Wikipedia, map them to 50 dimensions using neural network embeddings, and then map them to 2 dimensions using TSNE. The result is below:", "(TSNE is a manifold learning technique which means that it tries to map high-dimensional data to a lower-dimensional manifold, creating an embedding that attempts to maintain local structure within the data. It\u2019s almost exclusively used for visualization because the output is stochastic and it does not support transforming new data. An up and coming alternative is Uniform Manifold Approximation and Projection, UMAP, which is much faster and does support transform new data into the embedding space).", "By itself this isn\u2019t very useful, but it can be insightful once we start coloring it based on different book characteristics.", "We can clearly see groupings of books belonging to the same genre. It\u2019s not perfect, but it\u2019s still impressive that we can represent all books on Wikipedia using just 2 numbers that still capture the variability between genres.", "The book example (full article coming soon) shows the value of neural network embeddings: we have a vector representation of categorical objects that is both low-dimensional and places similar entities closer to one another in the embedded space.", "The problem with static graphs is that we can\u2019t really explore the data and investigate groupings or relationships between variables. To solve this problem, TensorFlow developed projector, an online application that lets us visualize and interact with embeddings. I\u2019ll release an article on how to use this tool shortly, but for now, here\u2019s the results:", "Neural network embeddings are learned low-dimensional representations of discrete data as continuous vectors. These embeddings overcome the limitations of traditional encoding methods and can be used for purposes such as finding nearest neighbors, input into another model, and visualizations.", "Although many deep learning concepts are talked about in academic terms, neural network embeddings are both intuitive and relatively simple to implement. I firmly believe that anyone can learn deep learning and use libraries such as Keras to build deep learning solutions. Embeddings are an effective tool for handling discrete variables and present a useful application of deep learning.", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or on my personal website at willk.online.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4d028e6f0526&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----4d028e6f0526---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d028e6f0526&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d028e6f0526&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/photo/milky-way-illustration-1169754/", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1705.03127", "anchor_text": "machine translation"}, {"url": "https://arxiv.org/abs/1604.06737", "anchor_text": "entity embeddings for categorical variables"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "real problem I\u2019m working on"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "From Jupyter Notebook on GitHub"}, {"url": "https://www.tensorflow.org/guide/embedding", "anchor_text": "In the context of neural networks, embeddings"}, {"url": "https://stats.stackexchange.com/questions/182775/what-is-an-embedding-layer-in-a-neural-network", "anchor_text": "embeddings form the parameters"}, {"url": "https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=embeddings-colab&hl=en", "anchor_text": "this Google Colab Notebook"}, {"url": "https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=embeddings-colab&hl=en", "anchor_text": "source"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "complete notebook here"}, {"url": "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "anchor_text": "cosine distance"}, {"url": "https://github.com/lmcinnes/umap", "anchor_text": "Uniform Manifold Approximation and Projection, UMAP,"}, {"url": "https://projector.tensorflow.org", "anchor_text": "projector"}, {"url": "http://fast.ai", "anchor_text": "learn deep learning"}, {"url": "http://shop.oreilly.com/product/0636920097471.do", "anchor_text": "build deep learning solutions"}, {"url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "anchor_text": "Google-Produced tutorial on embeddings"}, {"url": "https://www.tensorflow.org/guide/embedding", "anchor_text": "TensorFlow Guide to Embeddings"}, {"url": "https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb", "anchor_text": "Book Recommendation System Using Embeddings"}, {"url": "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/", "anchor_text": "Tutorial on Word Embeddings in Keras"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://willk.online", "anchor_text": "willk.online"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4d028e6f0526---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4d028e6f0526---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4d028e6f0526---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/education?source=post_page-----4d028e6f0526---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4d028e6f0526---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d028e6f0526&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----4d028e6f0526---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d028e6f0526&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----4d028e6f0526---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d028e6f0526&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4d028e6f0526&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4d028e6f0526---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4d028e6f0526--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4d028e6f0526--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4d028e6f0526--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-embeddings-explained-4d028e6f0526&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}