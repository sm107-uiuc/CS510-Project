{"url": "https://towardsdatascience.com/ci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e", "time": 1683013023.686898, "path": "towardsdatascience.com/ci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e/", "webpage": {"metadata": {"title": "CI/CD Pipeline with Azure DevOps for Data Science project. | by Andrii Shchur | Towards Data Science", "h1": "CI/CD Pipeline with Azure DevOps for Data Science project.", "description": "In this article, I would like to show how to build Continuous Integration and Continuous Delivery pipelines for a Machine Learning project with Azure DevOps. First of all, let\u2019s define the CI/CD. As\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/CI/CD", "anchor_text": "Wiki", "paragraph_index": 1}, {"url": "https://www.infoworld.com/article/3271126/what-is-cicd-continuous-integration-and-continuous-delivery-explained.html", "anchor_text": "Continuous integration", "paragraph_index": 3}, {"url": "https://www.infoworld.com/article/3271126/what-is-cicd-continuous-integration-and-continuous-delivery-explained.html", "anchor_text": "Continuous delivery", "paragraph_index": 4}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data", "anchor_text": "link", "paragraph_index": 9}], "all_paragraphs": ["In this article, I would like to show how to build Continuous Integration and Continuous Delivery pipelines for a Machine Learning project with Azure DevOps.", "First of all, let\u2019s define the CI/CD. As Wiki said \u201cCI/CD bridges the gaps between development and operation activities and teams by enforcing automation in building, testing, and deployment of applications. Modern-day DevOps practices involve continuous development, continuous testing, continuous integration, continuous deployment, and continuous monitoring of software applications throughout its development life cycle. The CI/CD practice or CI/CD pipeline forms the backbone of modern-day DevOps operations.\u201d", "Ok, and let\u2019s find out CI and CD separately.", "Continuous integration is a coding philosophy and set of practices that drive development teams to implement small changes and check-in code to version control repositories frequently. Because most modern applications require developing code in different platforms and tools, the team needs a mechanism to integrate and validate its changes.", "Continuous delivery picks up where continuous integration ends. CD automates the delivery of applications to selected infrastructure environments. Most teams work with multiple environments other than the production, such as development and testing environments, and CD ensures there is an automated way to push code changes to them.", "So, why it is important? Machine Learning applications are becoming popular in our industry, however, the process for developing, deploying, and continuously improving them is more complex compared to more traditional software, such as a web service or a mobile application.", "Ok, let\u2019s try to build a simple pipeline for my project. The project is about predictive analytics with the following Azure services: Azure SQL, Azure Data Factory, Azure Storage Account V2, Azure Data Bricks.", "This solution consists of three steps. The first step \u2014 run data ADF Data flow to get data from SQL DB, convert and select several columns format from the table and that save these results to Stage folder in Azure Data Lake. The second step \u2014 run Azure Databriks notebook from ADF with specified parameter about storage account to prepare history dataset and run train model. At this step, we can use MLflow rack experiments to record and compare parameters and results. And the last step \u2014 run Azure Databriks notebook from ADF also with specified parameter about storage account to score our model and save results to Azure Data Lake.", "To start this project I need to create 3 resource groups in the Azure portal. These resource groups would be responsible for the different environments \u2014 Dev, Staging, and Production. In these environments, I created the following services \u2014 Azure SQL, Azure Data Factory, Azure Storage Account V2, Azure Data Bricks, and Azure Key Vault.", "In all Azure SQL, I uploaded two table \u2014 history data and score data. The description of this data you can find by this link. In the Azure storage account, I created a container with three folders \u2014 RawData, PreprocData, and Results. The next step is the creation of Secrets in Azure Key Vaults. I need a secret for Azure Storage Account, where I used Access key and Connection string for my Azure SQL DB. The one important thing, to get access from Azure Data Factory to your secrets you need to make some configuration in Access policy in Azure storage account \u2014", "As you can see I added one more Access policy with an appropriate Azure Data factory service and Azure Databricks, which I created early and permit to get a secret.", "The next step is to configure the Azure Databricks. To reference secrets stored in an Azure Key Vault, I created a secret scope backed by Azure Key Vault. To create it go to https://<databricks-instance>#secrets/createScope. This URL is case sensitive; scope in createScope must be uppercase and fill this form:", "The next step in Azure Databricks configuration is \u2014 connect my notebooks to the Azure DevOps repository. To do it, just open your notebook, push to Sync and fill the form with information about your Azure DevOps repository. In this way, you can commit updates in your Notebook to the repository.", "The base configuration Azure services are over, lets start to create a pipeline in Azure Data Factory. The first step is the connection of ADF to the Azure DevOps repository. I can connect it in two ways \u2014 during service creation and in ADF configuration. I need to config this connection only for the Dev environment.", "Ok, now we can choose the branch in which we would like to develop our pipeline. The next step \u2014 create Linked services to all Azure services.", "To create Azure Databricks Linked service I filled the next form:", "To create Azure SQL DB Linked service I filled the next form:", "To create Azure Storage Account Linked service I filled the next form:", "To create Azure Key Vault Linked service I filled the next form:", "To control Linked services names between different environment I create use Global parameters in ADF, were created three parameters \u2014 dev, stg, prd.", "The first step of my pipeline is Dataflow:", "In this Dataflow, I get data from Azure SQL DB, convert data format, select some column from the table and upload results to Azure Data Lake in the appropriate folder. This process was created for history and score(new data) tables.", "The next steps of the pipeline are run data_preparation_model_train and then score_new_data script. To do it we need:", "The same make with the score_new_data script.", "This all stages of my ADF pipeline, the next steps are validated, create pull requests to join my branch with masters, and then publish all updates to the master branch from ADF. The results:", "Let\u2019s start to config CI/CD in Azure DevOps.", "The first step \u2014 create a new release pipeline (In Azure DevOps portal navigate to Pipelines -> Releases and click on a New pipeline)", "Select a template window that shows various pre-configured templates. In the case, of Data Factory, the choice is to create an Empty job and name it for example \u2014 ADF-DevOps2020-CICD:", "Create a new stage \u201cStaging\u201d (test env):", "Now we have a blank release pipeline created. The next step is to create variable groups and variables that will be used by tasks. For this we need to Navigate to Pipelines -> Library and create a new variable group. In the new form fill the next information and then clone it to create the same for the Production stage.", "As a result, I have got two variable groups. Later I will be mapped to the corresponding stages.", "The next step \u2014 create release pipeline variables. Variables will contain environment-independent values, because of the inheritance of values of variable groups. Therefore, the variable value will be adjusted to a stage where it is used. To create it return to Pipelines -> Releases and then open tab Variables. I created the next variables:", "It is time to create and configure the development stage.", "These actions will create a Development stage. Every time we will update our master branch it will trigger Continuous Delivery Pipeline to deliver updated ARM template to other environments, like staging and production. We also can automate this process creating the trigger:", "Now it is time to config out the Staging or Test stage. Time to create several Tasks for this stage. For my first pipeline, I created two jobs \u2014 Deploy pipeline in Azure Data Factory And Copy Python Notebooks to Azure Databricks.", "Let\u2019s go into more detail with Task ARM template deployment. The main thing I would like to describe -", "In the results, we have got the next form:", "My next Task is Databricks Deploy Notebooks. Let\u2019s go into more detail with it. We need to fill:", "As a result, I have got two tasks at this stage:", "I also need the Production stage. Click clone Staging environment to a Production:", "The production stage was created just by cloning a staging without extra adjustments or additions. This is because the underlying job was configured by variables which hold pointers to all external references, like resource groups, key vaults, storage accounts. Also for this stage recommend creating \u201cPre-deployment\u201d approval. Therefore, when Staging deployment is over the execution flow will be waiting for action from an assigned approver. I can react directly from a notification email or by using DevOps portal UI.", "And the last step of all configuration is mapping variable groups to stages. This step is necessary to map stage-specific variable values to certain stages. As an example, a variable $(Environment) has a value \u201cprd\u201d if the pipeline a job in a production stage and it set the value to \u201cstg\u201d in case if the job is triggered in staging. To make it open tab Variables -> Variable Groups and click on \u201cLink variable group\u201d. Map variable group \u201cProduction\u201d to stage \u201cProduction\u201d and select also scope \u201cstage Production\u201d instead of \u201cRelease\u201d and then repeat the same action for \u201cStaging\u201d.", "Time to run and check this simple pipeline.", "As a result, I found the ADF pipeline and Azure Databricks Notebooks on appropriate services in the Stage and Production Resource group. I also can analyze the detail of all task at every stage, for example:", "To make this pipeline more complete, I also can add to the Test stage some tasks with tests. These tests, for example, can run the ADF pipeline and analyze its results \u2014 table after copy in Azure Data Lake, MLflow logs, and results in the table after the score. After passing all these tests we can approve the Production stage.", "In this story, I would like to illustrate in a detailed step-by-step way of how release pipelines can be created and configured to enable CI/CD practices in Azure Data Factory environments. It shows limitations and also possibilities that Azure DevOps, Azure Data Factory, Azure SQL DB, Azure Data Lake, and Azure Databricks can bring if they are used together.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff263586c266e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f263586c266e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f263586c266e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andriishchur?source=post_page-----f263586c266e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andriishchur?source=post_page-----f263586c266e--------------------------------", "anchor_text": "Andrii Shchur"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e94c5fb3ede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&user=Andrii+Shchur&userId=7e94c5fb3ede&source=post_page-7e94c5fb3ede----f263586c266e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff263586c266e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff263586c266e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.clipartkey.com/view/iRihwiw_ci-cd/", "anchor_text": "CI/CD pipeline"}, {"url": "https://en.wikipedia.org/wiki/CI/CD", "anchor_text": "Wiki"}, {"url": "https://www.infoworld.com/article/3271126/what-is-cicd-continuous-integration-and-continuous-delivery-explained.html", "anchor_text": "Continuous integration"}, {"url": "https://www.infoworld.com/article/3271126/what-is-cicd-continuous-integration-and-continuous-delivery-explained.html", "anchor_text": "Continuous delivery"}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data", "anchor_text": "link"}, {"url": "https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes", "anchor_text": "Create an Azure Key Vault-backed secret scope"}, {"url": "https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app", "anchor_text": "link"}, {"url": "https://docs.microsoft.com/en-us/azure/devops/?view=azure-devops", "anchor_text": "Azure DevOps documentation"}, {"url": "https://docs.microsoft.com/en-us/azure/data-factory/introduction", "anchor_text": "Azure Data Factory"}, {"url": "https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/", "anchor_text": "Azure Databricks MLflow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f263586c266e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f263586c266e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/devops?source=post_page-----f263586c266e---------------devops-----------------", "anchor_text": "DevOps"}, {"url": "https://medium.com/tag/ci-cd-pipeline?source=post_page-----f263586c266e---------------ci_cd_pipeline-----------------", "anchor_text": "Ci Cd Pipeline"}, {"url": "https://medium.com/tag/azure?source=post_page-----f263586c266e---------------azure-----------------", "anchor_text": "Azure"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff263586c266e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&user=Andrii+Shchur&userId=7e94c5fb3ede&source=-----f263586c266e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff263586c266e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&user=Andrii+Shchur&userId=7e94c5fb3ede&source=-----f263586c266e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff263586c266e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f263586c266e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff263586c266e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f263586c266e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f263586c266e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f263586c266e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f263586c266e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f263586c266e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f263586c266e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f263586c266e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f263586c266e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f263586c266e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andriishchur?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andriishchur?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrii Shchur"}, {"url": "https://medium.com/@andriishchur/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "320 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e94c5fb3ede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&user=Andrii+Shchur&userId=7e94c5fb3ede&source=post_page-7e94c5fb3ede--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3fcad9e401a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-pipeline-with-azure-devops-for-data-science-project-f263586c266e&newsletterV3=7e94c5fb3ede&newsletterV3Id=3fcad9e401a7&user=Andrii+Shchur&userId=7e94c5fb3ede&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}