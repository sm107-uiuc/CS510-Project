{"url": "https://towardsdatascience.com/predicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48", "time": 1682997721.18987, "path": "towardsdatascience.com/predicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48/", "webpage": {"metadata": {"title": "Predicting the Effect of More Training Data, by Using Less | by Daniel Hogan | Towards Data Science", "h1": "Predicting the Effect of More Training Data, by Using Less", "description": "A step-by-step guide to estimating the value of increased training data, using a case study from geospatial deep learning"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/the-downlinq/robustness-of-limited-training-data-for-building-footprint-identification-part-1-8c55810b5ef9", "anchor_text": "test case", "paragraph_index": 2}, {"url": "https://registry.opendata.aws/spacenet/", "anchor_text": "SpaceNet", "paragraph_index": 3}, {"url": "https://medium.com/the-downlinq/the-spacenet-challenge-off-nadir-buildings-introducing-the-winners-b60f2b700266", "anchor_text": "SpaceNet4 Challenge", "paragraph_index": 4}, {"url": "https://github.com/SpaceNetChallenge/SpaceNet_Off_Nadir_Solutions/tree/master/XD_XD", "anchor_text": "fifth-place submission to the SpaceNet4 Challenge", "paragraph_index": 5}, {"url": "https://medium.com/the-downlinq/robustness-of-limited-training-data-part-2-f51eb783823f", "anchor_text": "elsewhere", "paragraph_index": 5}, {"url": "https://medium.com/the-downlinq/the-spacenet-metric-612183cc2ddb", "anchor_text": "quantify", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/F1_score", "anchor_text": "F1 score", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Jaccard_index", "anchor_text": "IoU", "paragraph_index": 6}, {"url": "https://journals.aps.org/pra/abstract/10.1103/PhysRevA.45.6056", "anchor_text": "1", "paragraph_index": 16}, {"url": "http://papers.nips.cc/paper/803-learning-curves-asymptotic-values-and-rate-of-convergence.pdf", "anchor_text": "2", "paragraph_index": 16}, {"url": "https://www.liebertpub.com/doi/10.1089/106652703321825928", "anchor_text": "3", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1511.06348", "anchor_text": "4", "paragraph_index": 16}], "all_paragraphs": ["Here\u2019s the scenario: You\u2019ve gone to great difficulty and expense to collect some training data, and you\u2019ve used that data to train a deep neural net. Still, you find yourself wondering, \u201cDo I have enough?\u201d Would more training data cause a meaningful performance boost, or just waste time and money for an inconsequential gain? At first, it seems almost paradoxical: You can\u2019t make an informed decision about gathering more data without an idea of how much it will help, but you can\u2019t measure how much it will help unless you\u2019ve made the decision and acquired the additional data already.", "There is a solution, however. By repeatedly retraining a deep neural net with different-sized subsets of the training data that\u2019s already in hand, it may be possible to extrapolate performance out to quantities of training data beyond what\u2019s presently available.", "We\u2019ll walk through the procedure using an important test case from the field of computer vision: tracing out the outlines of buildings in satellite photos. The ability to automatically map buildings, without relying on extensive manual labor every time things change, has applications ranging from disaster relief to urban planning.", "To undertake a study of satellite imagery, we need to start by getting some images! For that, we turn to SpaceNet, which maintains a free, open source, always-available data store of thousands of square kilometers of labeled high-resolution satellite imagery.", "We\u2019ll use SpaceNet\u2019s latest (as of this writing) data release: a series of collects taken over Atlanta, Georgia by Maxar\u2019s WorldView-2 satellite. Included with the imagery are high-quality labels of building footprints (outlines). This dataset was the basis of the now-completed SpaceNet4 Challenge. The imagery shows the same area from 27 different viewing angles, making it ideal for studying how viewing angle affects the performance of geospatial deep learning algorithms. For simplicity, we\u2019ll organize our results into three categories: \u201cnadir\u201d with viewing angles within 25 degrees of nadir (straight down), \u201coff-nadir\u201d with viewing angles from 26 to 40 degrees off nadir, and \u201cfar-off-nadir,\u201d with viewing angles exceeding 40 degrees off nadir. We also define \u201coverall\u201d performance as a simple average of performance for the three categories. Fig. 1 shows an example of imagery from different angles.", "The model we\u2019ll train to find buildings also comes from SpaceNet. We\u2019ll use the fifth-place submission to the SpaceNet4 Challenge. Although a few other submissions slightly outperformed this model, this one is desirable for its fast inference time and straightforward architecture. The model\u2019s inference speed is more than ten times faster that the top SpaceNet4 winner. It carries out pixel segmentation using a U-Net with a VGG-16 encoder, then generates building footprint polygons based on the pixel map. To expedite training, the original submission\u2019s ensemble of three neural nets is pared down to a single net, causing only a modest performance degradation discussed elsewhere.", "To quantify model performance, an F1 score is calculated for the building footprints identified by the model. For the purpose of the calculation, a footprint is considered correct if it has an IoU (intersection over union) of at least 0.5 with a ground truth building footprint.", "To see how model performance depends on the amount of training data, the same model is trained with different amounts of data.", "The data is chipped into tiles that are 900 pixels on a side, each showing an area of 450m on a side. There is an average of 63 buildings per tile. Within the training data are 1064 unique tile locations, with 27 views of each location. Since our algorithm sets aside a quarter of the data for validation, training on the full dataset gives us 1064 * 27 * (3/4) = 21546 images. Fig. 2 shows what happens if we train with less data. The training and evaluation process is repeated ten times. But each time, we start from scratch and use only about half as much data as the time before. When data is scarce, performance rises quickly with new data, but there are diminishing returns when data is abundant.", "Now that we\u2019re all on the same page for our case study, it\u2019s time to use it as the context in which to address our main question: How can we make an educated guess about model performance with lots of data, before that data is even available? To do that, there are a few steps.", "For this process, we will need to train the model from scratch many times, with different amounts of data. If the model is hard-coded to train for a certain number of epochs or certain amount of time (whether it uses the final weights after that period or the best intermediate weights), it is worth the effort to study whether all that time is really necessary. Fig. 3 shows model performance as a function of training time for different amounts of data. The top line uses the full data set, and each subsequent line uses about half the training data of the line above it.", "Recall that when the training data set size is cut in half, the number of training epochs must be doubled to get the same total number of image views. However, the number of image views that are needed to reach maximum performance may go down with less data. For our geospatial case study, about 60 epochs are needed to reach maximum performance with the full training data set of 21546 images. But when only a few hundred images are used, the number of necessary image views falls below the equivalent of 5 epochs with the full training data set. It takes about four GPU-days (on an Nvidia Titan Xp) to train with the full dataset. While that\u2019s unavoidable, knowing that we can at least get away with less training for the reduced-data cases brings some welcome time savings.", "Once we\u2019ve trained with different amounts of data and evaluated each version of the model, we can plot the points in Fig. 2. But we\u2019re not ready to fit a curve just yet.", "Many deep learning papers quote performance figures without error bars. However, in the absence of error bars, it is unclear whether observed performance improvements are statistically significant or whether results are even repeatable. For our extrapolation task, error bars are necessary. In the case study, we have ten data points, each representing a different amount of training data. To save time, we\u2019ll calculate the error bars for only four of them, and logarithmically interpolate the rest. To calculate an error bar, we just repeat the model training/testing process multiple times with the given amount of data, and take the standard deviation of the results. Fig. 4 shows the same information as Fig. 2, but with a logarithmic x-axis to better display the error bars for low amounts of data.", "A better approach, which I\u2019ll be using in a follow-up study, would be to repeatedly train the model for every amount of training data under consideration, and then make a plot not of individual results but instead of the means of the results for each amount of training data.", "Next, to understand the dependence of F1 score on the amount of training data, we want to be able to model the relationship of these variables, ideally with a simple curve.", "For ideas, we turn to the literature. There hasn\u2019t been much written about our specific type of case study (data size dependence of the F1 score for semantic segmentation in a geospatial context). However, there has been a lot of work on deep learning classification problems [1, 2, 3, 4]. For those, the data set size dependence of the accuracy was found to scale as a constant minus an inverse power law term (Fig. 5). In a pleasant turn of events, the same functional form shows an excellent fit to our test case even though it\u2019s not a classification problem. In fact, the fitted curves shown as dotted lines in Figs. 2 and 4 were generated using this functional form.", "Once we have a fitted curve, we can use it to extrapolate performance given higher amounts of data. To summarize the process: the same model was trained with different amounts of data, error bars were estimated for each of those amounts, and a weighted regression was use to generate a simple model of how performance scales with the amount of the training data. Now it\u2019s time to try out the procedure and see how well it does when the amount of training data is limited.", "Suppose that instead of having the full training data set, we only had one-sixteenth of it. With only 1,323 images to train on instead of 21,546, we want to estimate the improvement from acquiring sixteen times more training data. Fig. 6 shows what happens when we follow the three steps above. Here we\u2019ve simply ignored the points with more than 1,323 images when fitting the curve. This test is somewhat artificial in that the low-training-data samples were not restricted to all be drawn from the same one-sixteenth of the full data set. However, that is not expected to materially change the result.", "From starting with 1,323 images, Table 1 shows the actual and predicted performance increases from doubling the training data and from increasing it by a factor of 16. In each case the prediction was correct to within a factor of two, and in most cases it was correct to within 25%. These figures refer to the estimated improvements in the F1 score, whereas the percent errors in the estimated F1 scores themselves are much lower. The measured and predicted results tend to gradually diverge as training data is increased. However, the F1 scores increase with more data, which constrains the growth of the percentage error. As a result, for this case study the method gives an estimate for a 16-fold increase in training data that\u2019s about as good as its estimate for a mere 2-fold increase. For a point of comparison, fitting the data with a simple logarithmic curve, instead of the constant minus a power law, produces a maximum percent error that\u2019s almost twice as large as seen here.", "In this building footprint case study, we could predict the performance improvement from a 16-fold increase in training data to within a factor of 2. Although there are no one-size-fits-all solutions in data science, this approach can help inform your decision the next time you find yourself wondering whether to procure more training data.", "A couple caveats bear mentioning. Implicit in this extrapolation approach is an assumption that the training data we already have and the new training data we might acquire are drawn from the same distribution. This would not hold true if, for example, the new training data we sought to acquire was from a different city, or a different part of the same city, which differed in overall appearance from the source of our already-available data. Even if that assumption is met, there is no firm guarantee that the F1 score will have the same dependence on training data set size as seen in this case study. No one recipe can be a replacement for thoughtful judgement and deep understanding of what makes your data unique. However, it can be a constructive starting point to making the best decisions for your data science project.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Daniel Hogan, PhD, is a data scientist at CosmiQ Works, an IQT Lab."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc3dde2f9ae48&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@daniel_hogan?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@daniel_hogan?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "Daniel Hogan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fecbbd25b9fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&user=Daniel+Hogan&userId=ecbbd25b9fd2&source=post_page-ecbbd25b9fd2----c3dde2f9ae48---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3dde2f9ae48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3dde2f9ae48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/the-downlinq/robustness-of-limited-training-data-for-building-footprint-identification-part-1-8c55810b5ef9", "anchor_text": "test case"}, {"url": "https://registry.opendata.aws/spacenet/", "anchor_text": "SpaceNet"}, {"url": "https://medium.com/the-downlinq/the-spacenet-challenge-off-nadir-buildings-introducing-the-winners-b60f2b700266", "anchor_text": "SpaceNet4 Challenge"}, {"url": "https://github.com/SpaceNetChallenge/SpaceNet_Off_Nadir_Solutions/tree/master/XD_XD", "anchor_text": "fifth-place submission to the SpaceNet4 Challenge"}, {"url": "https://medium.com/the-downlinq/robustness-of-limited-training-data-part-2-f51eb783823f", "anchor_text": "elsewhere"}, {"url": "https://medium.com/the-downlinq/the-spacenet-metric-612183cc2ddb", "anchor_text": "quantify"}, {"url": "https://en.wikipedia.org/wiki/F1_score", "anchor_text": "F1 score"}, {"url": "https://en.wikipedia.org/wiki/Jaccard_index", "anchor_text": "IoU"}, {"url": "https://journals.aps.org/pra/abstract/10.1103/PhysRevA.45.6056", "anchor_text": "1"}, {"url": "http://papers.nips.cc/paper/803-learning-curves-asymptotic-values-and-rate-of-convergence.pdf", "anchor_text": "2"}, {"url": "https://www.liebertpub.com/doi/10.1089/106652703321825928", "anchor_text": "3"}, {"url": "https://arxiv.org/abs/1511.06348", "anchor_text": "4"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c3dde2f9ae48---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----c3dde2f9ae48---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/training-data?source=post_page-----c3dde2f9ae48---------------training_data-----------------", "anchor_text": "Training Data"}, {"url": "https://medium.com/tag/geospatial?source=post_page-----c3dde2f9ae48---------------geospatial-----------------", "anchor_text": "Geospatial"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c3dde2f9ae48---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3dde2f9ae48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&user=Daniel+Hogan&userId=ecbbd25b9fd2&source=-----c3dde2f9ae48---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3dde2f9ae48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&user=Daniel+Hogan&userId=ecbbd25b9fd2&source=-----c3dde2f9ae48---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3dde2f9ae48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc3dde2f9ae48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c3dde2f9ae48---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c3dde2f9ae48--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@daniel_hogan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@daniel_hogan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daniel Hogan"}, {"url": "https://medium.com/@daniel_hogan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "316 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fecbbd25b9fd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&user=Daniel+Hogan&userId=ecbbd25b9fd2&source=post_page-ecbbd25b9fd2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8b5876e38f79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-the-effect-of-more-training-data-by-using-less-c3dde2f9ae48&newsletterV3=ecbbd25b9fd2&newsletterV3Id=8b5876e38f79&user=Daniel+Hogan&userId=ecbbd25b9fd2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}