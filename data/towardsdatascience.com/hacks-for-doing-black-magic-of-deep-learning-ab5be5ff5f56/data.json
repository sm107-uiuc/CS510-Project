{"url": "https://towardsdatascience.com/hacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56", "time": 1683000928.443077, "path": "towardsdatascience.com/hacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56/", "webpage": {"metadata": {"title": "Hacks for Doing Black Magic of Deep Learning | by Lusine Abrahamyan | Towards Data Science", "h1": "Hacks for Doing Black Magic of Deep Learning", "description": "Deep neural networks are known as \u201cblack boxes\u201d, where it\u2019s hard to do debugging. And after writing the training scripts, you cant be sure that you don\u2019t have any mistakes in the script or foresee\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1609.04836.pdf", "anchor_text": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "paragraph_index": 25}], "all_paragraphs": ["Deep neural networks are known as \u201cblack boxes\u201d, where it\u2019s hard to do debugging. And after writing the training scripts, you cant be sure that you don\u2019t have any mistakes in the script or foresee whether your model has enough parameters to learn the transformation that you need.", "And that is where the advice about overfiting\u00b9 from Andrej Karpathy is coming.", "At the beginning of the training, before feeding all the data to your network, try to overfit it on the one fixed batch, without any augmentation, and with a very small learning rate. If it will not be overfitted, it means, that, either your model doesn\u2019t have enough learning power for transformation that you need, or you have a bug in your code.", "Only after successful overfitting, it\u2019s reasonable to start training on the whole data.", "Normalization is a strong technique for overcoming vanishing gradients and train network with higher learning rates, without careful parameter initialization. Originally in the paper of S.Ioffe\u00b2, it\u2019s proposed to normalize features across the batch and turn activations toward the unit Gaussian distribution, to learn one, universal mean and variance, for the all data distribution(test data included). This approach is valid for all classification tasks when you need to predict one(or several in case of multilabel classification) label for the image. But the picture is different when you are working on image-to-image translation tasks. Here, learning one moving-average and one moving- mean, for the whole dataset may lead to failure. In this case, for each image, as an output of the network, you want to obtain, unique result.And that\u2019s where instance normalization is coming. In contrast, in instance normalization, the statistics are being computed independently for each image in the batch. And this independence is helping to successfully train networks for such tasks as image super-resolution, neural style transfers, image inpainting and much more. So be careful and not use the common practice of transfer learning, with the most famous pre-trained networks as ResNet, MobileNet, Inception, in image transformation tasks.", "It is known, that in the process of training deep neural networks, the bigger batch size is, the faster convergence will be. But also, it is empirically has been shown, that after a certain point, an increase of the batch size can harm the final performance of the model. In the work, N.S. Keskar et. al.\u00b3 stated, that it is connected with the fact that in case of the large batches, training tends to converge to sharp minimizers of the training function, and in case of smaller batches, to flat minimizers. As a result, in the first case, there will be high sensitivity from the training function, and little change in data distribution will harm the performance on the test stage.", "But further P. Goyal et. al.\u2074 in the paper \u201cAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour\u201d, showed that it is possible to train ImageNet with batch size up to 8K, without degradation in performance. As the authors state, optimization difficulty is the main issue with large minibatches, rather than poor generalization (at least on ImageNet). The authors proposed a linear scaling rule for the learning rate, depending on batch size. The rule is following", "when the minibatch size is multiplied by k, multiply the learning rate by k.", "Small batch size also can be considered as the form of regularization, because in this case, you will have noisy updates, which can help to avoid fast convergence to a local minimum and improve generalization.", "In recent years, with the increase of performance, the number of parameters in the neural networks increased drastically, and the design of the efficient and less costly neural networks turn out to be an issue of the day. One of the solutions, proposed by Google as a part of the Tensorflow\u2075 framework, is a depthwise separable convolution, which is a modification of the conventional convolutional layer, where you need fewer parameters.", "Let us suppose, we have a layer with", "In the case of the convolution, the number of parameters in the layer will be", "We are convolving each input filter by the number of times of output filters and then summing them up.", "And in case of depthwise separable convolution, it will be", "We are convolving each input filter one time, with the kernel (kh, kw), and then, convolving these intermediate filters with the kernel (1, 1), by the number of times of output filters.", "Now, let\u2019s have a look at two examples.", "Suppose we have following values for the layer", "Number of parameters in the convolutional layer will be", "Number of parameters in depthwise separable convolution will be", "Advantage in case of the depthwise separable convolution is obvious!!!", "Now let\u2019s suppose that we have other values for the layer", "Number of parameters in the convolutional layer will be", "Number of parameters in depthwise separable convolution will be", "So, as we can see, in the second case, instead of having a reduction, we increased the number of parameters.", "[2]S.Ioffe, C. Szegedy Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 2015, Proceedings of The 32nd International Conference on Machine Learning,", "[3]N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima, 2017, ICLR", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning for Computer Vision, PhD Vrije University Brussels"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fab5be5ff5f56&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@lusinlu?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lusinlu?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "Lusine Abrahamyan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb59a3af7e3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&user=Lusine+Abrahamyan&userId=b59a3af7e3b7&source=post_page-b59a3af7e3b7----ab5be5ff5f56---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab5be5ff5f56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab5be5ff5f56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@grakozy?utm_source=medium&utm_medium=referral", "anchor_text": "Greg Rakozy"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://karpathy.github.io/2019/04/25/recipe/", "anchor_text": "http://karpathy.github.io/2019/04/25/recipe/"}, {"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"url": "https://arxiv.org/pdf/1609.04836.pdf", "anchor_text": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"url": "https://arxiv.org/pdf/1706.02677.pdf", "anchor_text": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow\u2019s official website"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ab5be5ff5f56---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ab5be5ff5f56---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ab5be5ff5f56---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----ab5be5ff5f56---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ab5be5ff5f56---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab5be5ff5f56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&user=Lusine+Abrahamyan&userId=b59a3af7e3b7&source=-----ab5be5ff5f56---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab5be5ff5f56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&user=Lusine+Abrahamyan&userId=b59a3af7e3b7&source=-----ab5be5ff5f56---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab5be5ff5f56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fab5be5ff5f56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ab5be5ff5f56---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ab5be5ff5f56--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lusinlu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lusinlu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lusine Abrahamyan"}, {"url": "https://medium.com/@lusinlu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "15 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb59a3af7e3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&user=Lusine+Abrahamyan&userId=b59a3af7e3b7&source=post_page-b59a3af7e3b7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fb59a3af7e3b7%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56&user=Lusine+Abrahamyan&userId=b59a3af7e3b7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}