{"url": "https://towardsdatascience.com/deep-learning-to-jump-e507103ab8d3", "time": 1683014617.059016, "path": "towardsdatascience.com/deep-learning-to-jump-e507103ab8d3/", "webpage": {"metadata": {"title": "Deep Learning to Jump. Maxime Bergeron & Ivan Sergienko | by Maxime Bergeron | Towards Data Science", "h1": "Deep Learning to Jump", "description": "Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms", "paragraph_index": 1}, {"url": "https://thismatter.com/money/bonds/bond-pricing.htm", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://pricer.riskfuel.com/", "anchor_text": "Bermudan swaption", "paragraph_index": 5}, {"url": "https://github.com/Riskfuel-Collaborations/jump-unit/blob/main/notebook/JumpUnit.ipynb", "anchor_text": "this Jupyter notebook", "paragraph_index": 6}, {"url": "https://github.com/Riskfuel-Collaborations/jump-unit/blob/main/notebook/JumpUnit.ipynb", "anchor_text": "this Jupiter notebook!", "paragraph_index": 25}], "all_paragraphs": ["In this short note, we describe a Jump Unit that can be used to fit a step function with a simple neural network. Our motivation comes from quantitative finance problems where discontinuities often appear.", "Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should not rely on an author\u2019s works without seeking professional advice. See our Reader Terms for details.", "Discontinuous functions are a common occurrence in financial instruments. For instance, the graph below shows the price of a typical five-year fixed rate bond with a semi-annual coupon. We have set the coupon rate higher than the discount rate, so the value of the bond stays above its Par Value of $100. If you are not familiar with bond pricing, a good primer is available here.", "The important things to notice for our purposes are the jumps that occur at each coupon payment date. This happens simply because money cannot be made out of thin air. The \u201cwealth\u201d of the security owner remains the same immediately before and after the coupon. As such, we have:", "value before coupon = value after coupon + coupon cash.", "Similar jumps occur in the values of more complicated path-dependent financial derivatives on exercise dates. A classic example here is a Bermudan swaption, a popular instrument used to manage mortgage prepayment risk. Bermudan-style options can be exercised on a predetermined schedule of dates where the value may jump.", "To keep things concrete, we will focus on the sub-problem of learning a piece-wise constant function with a single downward jump. We generate the training data as shown in the graph above. The reader is invited to follow along with our code in this Jupyter notebook.", "So how do we machine learn a jump? A natural approach here is to use a sigmoid. However, in our problem, the steps are sharp. All the points to the left of a coupon payment date are strictly above the points to the right of it resulting in a true discontinuity. Fitting a simple 1-dimensional neural network", "comprised of a sigmoidal activation function sandwiched between two linear layers requires an unbounded coefficient (weight) in the initial linear layer. Quantities growing without bound are always bad news for numerical methods, but it gets even worse with neural nets. Indeed, it leads to the infamous exploding gradients problem which makes optimal parameter values nearly impossible to learn. We would have to compromise by keeping the weight large yet finite, leading to a region of the inputs (just around the jump) where it is difficult to get rid of a significant error.", "Why not simply learn a jump with\u2026 a jump? It is tempting to replace the sigmoid function in our one-dimensional network above with a discontinuous activation. The simplest candidate here is the Heaviside step function H(x) which equals 0 for x < 0, and 1 otherwise:", "This is an obvious idea, which immediately leads to a failure like the one shown in the graph below:", "To understand the problem here, let\u2019s look at the math. The loss function corresponding to our little neural network is:", "Here, y\u1d62 are the training data values corresponding to times t\u1d62 , the sub-scripted w term is the weight of the last linear layer and the sub-scripted b terms are biases from the two linear layers. The astute reader will notice that without loss of generality we have set the weight of the first linear layer to 1 and that the remaining weights and biases determine the size and position of the step. The gradient descent methods on which deep learning relies for minimizing the error function require the use of first derivatives. In this context, one of them is rather problematic:", "The problem here is the second factor, which blows up when the b and t terms cancel out. The following plot clearly illustrates the issue: the function is piece-wise constant!", "While the actual minimum is where we expect it to be, gradient based methods never reach it. Instead the process gets stuck in one of the little plateaus.", "In fact, this problem is one of the reasons why sigmoid functions became a staple of machine learning. Replacing Heaviside functions with sigmoidal functions allowed complex neural networks such as the multi-layer perceptron to be successfully trained via gradient descent.", "At this point, it may start to feel like we are going in circles. Sigmoidal functions are of limited use to fit sharp jumps, yet they were introduced to fix the obvious problems with Heaviside functions.", "The following table summarizes the previous two sections and shows what we like and dislike about the two activation functions when faced with the task of learning a sharp step function:", "A natural question is whether we can combine the two functions, keeping the features that we like and discarding those we dislike. Enter the Jump Unit illustrated below:", "It consists of three linear nodes along with a sigmoid and Heaviside activation functions arranged in parallel. To understand how this unit works, consider the equation that it encodes:", "In order to simplify the equation, we omit the bias term in the linear layer following the sigmoid activation since it is accounted for by the bias term of the linear layer following the Heaviside activation. Let\u2019s now take a look at the derivative which was problematic before:", "Since the troublesome bias term now appears in the argument of both S(-) and H(-), its gradient no longer vanishes at most points and the network is able to learn. Notice that at the end of the process we also want the weight of the linear layer following the sigmoid activation to vanish, so that only the Heaviside contribution remains.", "The plot above shows the MSE error of our Jump Unit as a function of its two key parameters. We see that the plateaus between the steps along the bias axis are now slopped, allowing gradient-based algorithms to learn the global minimum.", "Finally, we plot the resulting function, voil\u00e0!", "To conclude, we have shown how the benefits and drawbacks of sigmoidal and Heaviside activation functions can be combined to produce a Jump Unit capable of learning a discontinuous step function via gradient descent.", "We encourage readers to try it out for themselves in this Jupiter notebook!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Director of Research & Development at Riskfuel"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe507103ab8d3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bergeronmax?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bergeronmax?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "Maxime Bergeron"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda21ec0960ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&user=Maxime+Bergeron&userId=da21ec0960ed&source=post_page-da21ec0960ed----e507103ab8d3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe507103ab8d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe507103ab8d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines"}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms"}, {"url": "https://thismatter.com/money/bonds/bond-pricing.htm", "anchor_text": "here"}, {"url": "https://pricer.riskfuel.com/", "anchor_text": "Bermudan swaption"}, {"url": "https://github.com/Riskfuel-Collaborations/jump-unit/blob/main/notebook/JumpUnit.ipynb", "anchor_text": "this Jupyter notebook"}, {"url": "https://github.com/Riskfuel-Collaborations/jump-unit/blob/main/notebook/JumpUnit.ipynb", "anchor_text": "this Jupiter notebook!"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e507103ab8d3---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----e507103ab8d3---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e507103ab8d3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/finance?source=post_page-----e507103ab8d3---------------finance-----------------", "anchor_text": "Finance"}, {"url": "https://medium.com/tag/modeling?source=post_page-----e507103ab8d3---------------modeling-----------------", "anchor_text": "Modeling"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe507103ab8d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&user=Maxime+Bergeron&userId=da21ec0960ed&source=-----e507103ab8d3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe507103ab8d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&user=Maxime+Bergeron&userId=da21ec0960ed&source=-----e507103ab8d3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe507103ab8d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe507103ab8d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e507103ab8d3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e507103ab8d3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e507103ab8d3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e507103ab8d3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bergeronmax?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bergeronmax?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Maxime Bergeron"}, {"url": "https://medium.com/@bergeronmax/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "7 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda21ec0960ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&user=Maxime+Bergeron&userId=da21ec0960ed&source=post_page-da21ec0960ed--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fda21ec0960ed%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-to-jump-e507103ab8d3&user=Maxime+Bergeron&userId=da21ec0960ed&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}