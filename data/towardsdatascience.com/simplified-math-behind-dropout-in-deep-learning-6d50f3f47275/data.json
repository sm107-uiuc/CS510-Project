{"url": "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275", "time": 1682996119.1908538, "path": "towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275/", "webpage": {"metadata": {"title": "Understanding Dropout with the Simplified Math behind it | by Chitta Ranjan | Towards Data Science", "h1": "Understanding Dropout with the Simplified Math behind it", "description": "In spite of the groundbreaking results reported, little is known about Dropout from a theoretical standpoint. Likewise, the importance of Dropout rate as 0.5 and how it should be changed with layers\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.understandingdeeplearning.com/", "anchor_text": "Understanding Deep Learning", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/4878-understanding-dropout.pdf", "anchor_text": "2", "paragraph_index": 12}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "1", "paragraph_index": 26}, {"url": "http://www.understandingdeeplearning.com", "anchor_text": "www.understandingdeeplearning.com", "paragraph_index": 37}], "all_paragraphs": ["<<Download the free book, Understanding Deep Learning, to learn more>>", "In spite of the groundbreaking results reported, little is known about Dropout from a theoretical standpoint. Likewise, the importance of Dropout rate as 0.5 and how it should be changed with layers are not evidently clear. Also, can we generalize Dropout to other approaches? The following will provide some explanations.", "Deep Learning architectures are now becoming deeper and wider. With these bigger networks, we are able to achieve good accuracies. However, this was not the case about a decade ago. Deep Learning was, in fact, infamous due to overfitting issue.", "Then, around 2012, the idea of Dropout emerged. The concept revolutionized Deep Learning. Much of the success that we have with Deep Learning is attributed to Dropout.", "In this post, our objective is to understand the Math behind Dropout. However, before we get to the Math, let\u2019s take a step back and understand what changed with Dropout. This will be a motivation to touch the Math.", "Before Dropout, a major research area was regularization. Introduction of regularization methods in neural networks, such as L1 and L2 weight penalties, started from the early 2000s [1]. However, these regularizations did not completely solve the overfitting issue.", "One major issue in learning large networks is co-adaptation. In such a network, if all the weights are learned together it is common that some of the connections will have more predictive capability than the others.", "In such a scenario, as the network is trained iteratively these powerful connections are learned more while the weaker ones are ignored. Over many iterations, only a fraction of the node connections is trained. And the rest stop participating.", "This phenomenon is called co-adaptation. This could not be prevented with the traditional regularization, like the L1 and L2. The reason is they also regularize based on the predictive capability of the connections. Due to this, they become close to deterministic in choosing and rejecting weights. And, thus again, the strong gets stronger and the weak gets weaker.", "A major fallout of this was: expanding the neural network size would not help. Consequently, neural networks\u2019 size and, thus, accuracy became limited.", "Then came Dropout. A new regularization approach. It resolved the co-adaptation. Now, we could build deeper and wider networks. And use the prediction power of all of it.", "With this background, let\u2019s dive into the Mathematics of Dropout. You may skip directly to Dropout equivalent to regularized Network section for the inferences.", "Consider a single layer linear unit in a network as shown in Figure 4 below. Refer [2] for details.", "This is called linear because of the linear activation, f(x) = x. As we can see in Figure 4, the output of the layer is a linear weighted sum of the inputs. We are considering this simplified case for a mathematical explanation. The results (empirically) hold for the usual non-linear networks.", "For model estimation, we minimize a loss function. For this linear layer, we will look at the ordinary least square loss,", "Eq. 1 shows loss for a regular network and Eq. 2 for a dropout network. In Eq. 2, the dropout rate is \ud835\udeff, where \ud835\udeff ~ Bernoulli(p). This means \ud835\udeff is equal to 1 with probability p and 0 otherwise.", "The backpropagation for network training uses a gradient descent approach. We will, therefore, first look at the gradient of the dropout network in Eq. 2, and then come to the regular network in Eq. 1.", "Now, we will try to find a relationship between this gradient and the gradient of the regular network. To that end, suppose we make w\u2019 = p*w in Eq. 1. Therefore,", "Taking the derivative of Eq. 4, we find,", "Now, we have the interesting part. If we find the expectation of the gradient of the Dropout network, we get,", "If we look at Eq. 6, the expectation of the gradient with Dropout, is equal to the gradient of Regularized regular network E\u0274 if w\u2019 = p*w.", "This means minimizing the Dropout loss (in Eq. 2) is equivalent to minimizing a regularized network, shown in Eq. 7 below.", "That is, if you differentiate a regularized network in Eq. 7, you will get to the (expectation of) gradient of a Dropout network as in Eq. 6.", "This is a profound relationship. From here, we can answer:", "This is because the regularization parameter, p(1-p) in Eq. 7, is maximum at p = 0.5.", "In Keras, the dropout rate argument is (1-p). For intermediate layers, choosing (1-p) = 0.5 for large networks is ideal. For the input layer, (1-p) should be kept about 0.2 or lower. This is because dropping the input data can adversely affect the training. A (1-p) > 0.5 is not advised, as it culls more connections without boosting the regularization.", "Because the expected value of a Dropout network is equivalent to a regular network with its weights scaled with the Dropout rate p. The scaling makes the inferences from a Dropout network comparable to the full network. There are computational benefits as well, which is explained with an Ensemble modeling perspective in [1].", "Before we go, I want to touch upon Gaussian-Dropout.", "As we saw before, in Dropout we are dropping a connection with probability (1-p). Put mathematically, in Eq. 2 we have the connection weights multiplied with a random variable, \ud835\udeff, where \ud835\udeff ~ Bernoulli(p).", "This Dropout procedure can be looked at as putting a Bernoulli gate on each connection.", "We can replace the Bernoulli gate with another gate. For example, a Gaussian Gate. And this gives us a Gaussian-Dropout.", "The Gaussian-Dropout has been found to work as good as the regular Dropout and sometimes better.", "With a Gaussian-Dropout, the expected value of the activation remains unchanged (see Eq. 8). Therefore, unlike the regular Dropout, no weight scaling is required during inferencing.", "This property gives the Gaussian-Dropout a computational advantage as well. We will explore the performance of Gaussian-Dropout in an upcoming post. Until then, a word of caution.", "Although the idea of Dropout Gate can be generalized to distributions other than Bernoulli, it is advised to understand how the new distribution will affect the expectation of the activations. And based on this, appropriate scaling of the activations should be done.", "In this post, we went through the Mathematics behind Dropout. We worked the Maths under some simplified conditions. However, the results extend to general cases in Deep Learning. In summary, we understood,", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Director of Science at ProcessMiner | Book Author | www.understandingdeeplearning.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6d50f3f47275&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://cranjan.medium.com/?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": ""}, {"url": "https://cranjan.medium.com/?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "Chitta Ranjan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c9fae27a83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&user=Chitta+Ranjan&userId=1c9fae27a83&source=post_page-1c9fae27a83----6d50f3f47275---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d50f3f47275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d50f3f47275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.understandingdeeplearning.com/", "anchor_text": "Understanding Deep Learning"}, {"url": "https://papers.nips.cc/paper/4878-understanding-dropout.pdf", "anchor_text": "2"}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "1"}, {"url": "http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "anchor_text": "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929\u20131958."}, {"url": "https://papers.nips.cc/paper/4878-understanding-dropout.pdf", "anchor_text": "Baldi, P., & Sadowski, P. J. (2013). Understanding dropout. In Advances in neural information processing systems (pp. 2814\u20132822)."}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6d50f3f47275---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/dropout?source=post_page-----6d50f3f47275---------------dropout-----------------", "anchor_text": "Dropout"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6d50f3f47275---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----6d50f3f47275---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d50f3f47275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&user=Chitta+Ranjan&userId=1c9fae27a83&source=-----6d50f3f47275---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d50f3f47275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&user=Chitta+Ranjan&userId=1c9fae27a83&source=-----6d50f3f47275---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d50f3f47275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6d50f3f47275&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6d50f3f47275---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6d50f3f47275--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6d50f3f47275--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6d50f3f47275--------------------------------", "anchor_text": ""}, {"url": "https://cranjan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://cranjan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chitta Ranjan"}, {"url": "https://cranjan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "http://www.understandingdeeplearning.com", "anchor_text": "www.understandingdeeplearning.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c9fae27a83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&user=Chitta+Ranjan&userId=1c9fae27a83&source=post_page-1c9fae27a83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffa1c3bb004a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplified-math-behind-dropout-in-deep-learning-6d50f3f47275&newsletterV3=1c9fae27a83&newsletterV3Id=fa1c3bb004a0&user=Chitta+Ranjan&userId=1c9fae27a83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}